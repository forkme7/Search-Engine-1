Lower Boundaries of Motoneuron  Desynchronization via Renshaw Interneurons  Mitchell Gii Maitenfort*  Dept. of Biomedical Engineering  Northwestern University  Evanston, IL 60201  C. J. Heckman  V. A. Research Service  Lakeside Hospital  and Dept. of Physiology  Northwestern University  Chicago, IL 60611  Robert E. Druzinsky  Dept. of Physiology  Northwestern University  Chicago, IL 60611  W. Zev Rymer  Dept. of Physiology  and Biomedical Engineering  Northwestern University  Chicago, IL 60611  Abstract  Using a quasi-realistic model of the feedback inhibition of motoneurons  (MNs) by Renshaw cells, we show that weak inhibition is sufficient to  maximally desynchronize MNs, with negligible effects on total MN  activity. MN synchrony can produce a 20 - 30 Hz peak in the force  power spectrum, which may cause instability in feedback loops.  1  INTRODUCTION  The structure of the recurrent inhibitory connections from Renshaw cells (RCs) onto  motoneurons (MNs) (Figure 1) suggests that the RC forms a simple negative feedback  * send mail to: Mitchell G. Maltenfort, SMPP room 1406, Rehabilitation Insitute of  Chicago, 345 East Superior Street, Chicago, IL 60611. Eraall address is mgm@nwu.edu  535  536 Maltenfort, Druzinsky, Heckman, and Rymer  loop. Past theoretical work has examined possible roles of this feedback in smoothing or  gain regulation of motor output (e.g., Bullock and Contreras-Vidal, 1991; Graham and  Redman, 1993), but has assumed relatively strong inhibitory effects from the RC.  Experimental observations (Granit et al.,1961) show that maximal RC activity can only  reduce MN fh-ing rates by a few impulses per second, although this weak inhibition is  sufficient to affect the timing of MN firings, reducing the probability that any two MNs  will fire simultaneously (Adam et al., 1978; Windhorst et al., 1978). In this study,  simulations were used to examine the impact of RC inhibition on MN fh-ing synchrony  and to predict the effects of such synchrony on force output.  Figure 1: Simplified Schematic of Recurrent Inhibition  2  CONSTRUCTION OF THE MODEL  2.1 MODELING OF INDIVIDUAL NEURONS  The integrate-and-fire neuron model of MacGregor (1987) adequately mimics specific  fh-ing patterns. Coupled first-order differential equations govern membrane potential and  afterhyperpolarization (AHP) based on injected current and synaptic inputs. A spike is  fired when the membrane potential crosses a threshold. The model was modified to  include a membrane resistance in order to model MNs of varying current thresholds.  Membrane resistance and time constants of model MNs were set to match published data  (Gustaffson and Pinter, 1984). The parameters governing AHPs were adjusted to agree  with observations from single action potentials and steady-state current-rate plots  (Heckman and Binder, 1991). Realistic faring behavior could be generated for MNs with  current thresholds of 4 - 40 hA.  Although there are no direct measurements of RC membrane properties available,  appropriate parameters were estimated by extrapolation from the MN parameter set. The  simulated RC has a 30 ms AHP and a current-rate plot matching that reported by  Hultbom and Pierrot-Deseilligny (1979). Spontaneous firing of 8 pps is produced in the  model by setting the RC firing threshold to 0.01 mV below resting potential; in vivo  Lower Boundaries of Motoneuron Desynchronization via Renshaw Interneurons 537  this fu'ing is likely due to descending inputs (Hamm et al., 1987a), but there is no  quantitative description of such inputs. The RCs are assumed to be homgeneous.  2.2 CONNECTIVITY OF THE POOL  Simulated neurons were arranged along a 16 by 16 grid. The network consists of 256  MNs and 64 RCs, with the RCs ordered on even-numbered rows and columns; as a result,  the MN - RC connections are inhomogeneous along the pool. For each trial, MN pools  were randomly generated following the distribution of MN current thresholds for a model  of the cat medial gastrocnemius motor pool (Heckman and Binder, 1991).  Communication between neurons is mediated by synaptic conductances which open when  a presynaptic cell fires, then decay exponentially. MN excitation of Res was set to  produce Re firing rates < 190 pps (Cleveland et al., 1981) which linearly increase with  MN activity (Cleveland and Ross, 1977). MN activation of RCs scales inversely with  MN current threshold (Hultborn et al., 1988).  Connectivity is based on observations t_hat synapses from RCs to MNs have a longer  spatial range than the reverse (reviewed in Windhorst, 1990). The IPSPs produced by  single MN firings are 4 - 6 times larger than those produced by single Re firings  (Hamm et al., 1987b; van Kuelen, 1981). In the model, each MN excites RCs within  one column or row of itself, and each Re inhibits MNs up to two rows or columns  away; thus, each MN excites 1 - 4 Res (mean 2.25) and receives feedback from 4 - 9 RCs  (mean 6.25).  2.3 ACTIVATION OF THE POOL  The MNs are activated by applied step currents. Although this is not realistic, it is  computationally efficient. An option in the simulation program allows for the addition  of bandlimited noise to the activation current, to simulate a synchronizing common  synaptic input. This signal has an rms value of 3% of the mean applied current and is  low-pass filtered with a cutoff of 30 Hz. This allows us look at the effects due purely to  Re activity and to establish which effects persist when the MN pool is being actively  3  EFFECTS OF RC STRENGTH ON MN SYNCHRONY  3.1 DEFINITION OF SYNCHRONY COEFFICIENT  Consider the total number of spikes fired by the MN pool as a time series. During  synchronous firing, the MN spikes will clump together and the time series will have  regions of very many or very few MN spikes. When the MNs are desynchronized, the  range of spike counts in each time bin will contract towards the mean. It follows that a  simple measure of MN synchrony is the the coefficient of variation (c.v. = s.d. / mean) of  the time series formed by the summed MN activity. Figure 2 shows typical MN pool  firing before and after RC feedback inhibition is added; the changes in "clumping"  described above are quite visible in the two plots.  538 Maltenfort, Druzinsky, Heckman, and Rymer  3.2 "PLATEAU" OF DESYNCHRONIZATION  The magnitude of the synaptic conductance from RCs onto MNs was changed from zero  to twice physiological in order to compare the effects of 'weak' and 'strong' recurrent  inhibition. At activation levels sufficient to recruit at least 70% of MNs in the pool  (mean firing rate > 15 pps), a surprising plateau effect was seen. The synchrony  coefficient fell off with RC synapfic conductance until the physiological level was  reached, and then no further alesynchronization was seen. The effect persisted when  synchronizing noise was added (Figure 3). At activation levels sufficient to show this  plateau, this "comer" inhibition level was always the same.  50  Synchronized Firing (no RC inhibition)  0 50  100 150  Desynchronized Firing (RC inhibition added)  20  0  0  50 100 150 200  Time (ms)  Figure 2: Comparison of Synchronous and Asynchronous MN Firing  At this comer level, the decrease in mean MN firing rate was < 1 pps and not  statistically significant. There was also no discernible change in the percentage of the  MN pool active. The c.v. of the interspike interval of single MN firings during constant  activation is < 2.5 % even with RCs active - this implies that the RC system finds an  optimal arrangement of the MN firings and then performs few if any further shifts. When  synchronizing noise is added, the RC effect on the interspike interval is swamped by the  effect of the synchronizing random input.  Figure 4 shows the effect of increasing MN activation on the synchrony coefficients  before and after RC inhibition is added. The change is statistically significant at all  levels, but is only large at higher levels as discussed above. As activation of the MNs  Lower Boundaries of Motoneuron Desynchronization via Renshaw Interneurons 539  increases, the "before" level of synchrony increases while the "after" level seems to move  asymptotically towards a minimum level of about 0.35. This minimum level of MN  synchrony, as well as the dependence of the effect on the activation level of the pool,  suggests that a certain amount of synchrony becomes inevitable as more MNs are  activated and fLre at higher rates.  8  1.4  1.2  1  0.8  0.6  0.4  synchronizing noise added  no synchronizing noise  I I I I I I  0 0.002 0.004 0.006 0.008 0.01  RC Synaptic Conductance (laSiemens)  Figure 3: MN Firing Synchrony vs. RC Strength  4  EFFECTS OF MN SYNCHRONY ON MUSCLE FORCE  4.1 MODELING OF FORCE OUTPUT  Single twitches of motor units are modeled with a second-order model, f(t) = F-et -t/x,  where the amplitude F and time constant  are matched to MN current threshold according  to the model of Heckman and Binder (1991). A rate-based gain factor adapted from  Fuglevand (1989) produces fused tetanus at high fh-ing rates. The tenfold difference in  current thresholds maps to a fifty-fold difference in twitch forces. Twitch time constants  range 30-90 ms.  540 Maltenfort, Druzinsky, Heckman, and Rymer  4.2 EFFECTS OF RECURRENT INHIBITION ON FORCE  The force model sharply low-pass f'fiters the neural input signal (< 5 Hz). As a result, the  c.v. of the force output is much lower than that of the associated MN input (< 0.01).  Although the plot of force c.v. vs. RC strength during constant activation follows the  curve in Figure 3, adding synchronizing noise removes any correlation between force .c.v.  and magnitude of recurrent inhibition. The effect of recurrent inhibition on mean force is  similar to that on the mean firing rate: small (<5 % decrease) and generally not  statistically significant.  1.6  1.4  1.2  1  0.8  0.6 after  0.4  5 10 15 20 25 30 35  Activation Current (nA)  Figure 4: Effects of MN Activation on Synchrony Before and After Recurrent Inhibition  When the change in synchrony due to RCs is large, a peak appears in the force power  spectrum in the range 20 - 30 Hz. This peak is reduced by RCs even when the MN pool  is being actively synchronized (Figure 5). Peaks in the force spectrum match peaks in the  spectrum of pooled MN activity, suggesting the effect is due to synchronous MN firing.  Although the magnitude of this peak is small (< 0.5% of mean force), its relatively high  frequency suggests that in derivative feedback - where spectral components are multiplied  by 2Jr times their frequency - its impact could be substantial. The feedback loop which  measures muscle stretch contains such a derivative component (Houk and Rymer, 1981).  Lower Boundaries of Motoneuron Desynchronization via Renshaw Intemeurons 541  5  DISCUSSION  The preceding shows that the ostensibly weak recurrent inhibition is sufficient to sharply  reduce the maximum number of synchronous firings of a neuron population, while  having a negligible effect on the total population activity. This has a broad implication  for neural networks in t_hat it suggests the existence of a "switching mechanism" which  forces the peaks in the output of an ensemble of neurons to remain below a threshold  level without significantly suppressing the total ensemble activity.  One possible role for such a mechanism would be in the accommodation to a step or  ramp increase in a stimulus. The initial increase synchronizes the neural signal from the  receptor, which is then alesynchronized by the recurrent inhibition. The synchronized  f'u-ing phase would be sufficient to excite a target neuron past its f'uSng threshold, but after  that, the desynchronized neural signal would remain well below the target's threshold.  0.2  0.15  0.05  0  0  10 20 30 40  Frequency (Hz)  Figure 5: Recurrent Inhibition Reduces Spectral Peak. 95% confidence limit of means  plotted, solid lines before recurrent inhibition and dashed lines after.  Acknowledgments  The authors are indebted to Dr. Tom Buchanan for use of his IBM RS/6000 workstation.  This work was supported by NIH grants NS28076-02 and NS30295-01.  542 Maltenfort, Druzinsky, Heckman, and Rymer  References  Adam D, Windhorst U, Inbar GF: The effects of recurrent inhibition on the cross-  correlated firing patterns of motoneurons (and their relation to signal transmission in the  spinal cord-muscle channel). Biol. Cybern., 29: 229-235, 1978.  Bullock D, Contreras-Vidal J: How spinal neural networks reduce discrepancies between  motor intention and motor realization. Tech.Report CAS/CNS-91-023, Boston U., 1991.  Cleveland S, Kuschmierz A, Ross H-G: Static input-output relations in the spinal  recurrent inhibitory pathway. Biol. Cybern., 40: 223-231, 1981.  Cleveland S, Ross H-G: Dynamic properties of Renshaw cells: Frequency response  characteristics. Biol. Cybern., 27: 175-184, 1977.  Fuglevand AJ: A motor unit pool model: relationship of neural control properties to  isometric muscle tension and the electromyogram. Ph.D. Thesis, U. of Waterloo, 1989.  Graham BP, Redman S J: Dynamic behaviour of a model of the muscle stretch reflex.  Neural Networks, 6: 947-962, 1993.  Granit R, Haase J, Rutledge LT: Recurrent inhibition in relation to frequency of fh-ing  and limitation of discharge rate of extensor motoneurons. J. Physiol., 158: 461-475,  1961.  Gustaffson B, Pinter MJ: An investigation of threshold properties among cat spinal a-  motoneurons. J. Physiol., 357: 453-483, 1984.  Harem TM, Sasaki S, Stuart DG, Windhorst U, Yuan C-S: Distribution of single-axon  recurrent inhibitory post-synaptic potentials in the cat. J. Physiol., 388: 631-651,1987a.  Harem TM, Sasaki S, Stuart DG, Windhorst U, Yuan C-S: The measurement of single  motor-axon recurrent inhibitory post-synaptic potentials in a single spinal motor nucleus  in the cat. J. Physiol., 388: 653-664, 1987b.  Heckman CJ, Binder MD: Computer simulation of the steady-state input-output function  of the cat roedial gastrocnemius motoneuron pool. J. Neurophysiol., 65: 952-967, 1991.  Honk JC, Rymer WZ: Chapter 8: Neural control of muscle length and tension. In  Handbook of Physiology: the Nervous System H pt. 1, ed.VB Brooks. Am. Physiol.  Soc., Bethesda, MD, 1981.  Hultborn H, Peirrot-Deseillgny E: Input-output relations in the pathway of recurrent  inhibition to motoneurons in the cat. J. Physiol., 297: 267-287, 1979.  MacGregor RJ: Neural and Brain Modeling. Academic Press, San Diego, 1987.  Van Kuelen LCM: Autogenetic recurrent inhibition of individual spinal motoneurons of  the cat. Neurosci. Lett., 21: 297-300, 1981.  Windhorst U: Activation of Renshaw cells. Prog. in Neurobiology, 35: 135-179, 1990.  Windhorst U, Adam D, Inbar GF: The effects of recurrent inhibitory feedback in shaping  discharge pauerns of motoneurones excited by phasic muscle stretches. Biol. Cybern.,  29: 221-227, 1978.  
Non-linear Statistical Analysis and  Self-Organizing Hebbian Networks  Jonathan L. Shapiro and Adam Priigel-Bennett  Department of Computer Science  The University, Manchester  Manchester, UK  M13 9PL  Abstract  Neurons learning under an unsupervised Hebbian learning rule can  perform a nonlinear generalization of principal component analysis.  This relationship between nonlinear PCA and nonlinear neurons is  reviewed. The stable fixed points of the neuron learning dynamics  correspond to the maxima of the statist, ic optimized under non-  linear PCA. However, in order to predict, what the neuron learns,  knowledge of the basins of attractions of the neuron dynamics is  required. Here the correspondence between nonlinear PCA and  neural networks breaks down. This is shown for a simple model.  Methods of statistical mechanics can be used to find the optima  of the objective function of non-linear PCA. This determines what  the neurons can learn. In order to find how the solutions are parti-  tioned amoung the neurons, however, one must solve the dynamics.  1 INTRODUCTION  Linear neurons learning under an unsupervised Hebbian rule can learn to perform a  linear statistical analysis of the input data. This was first shown by Oja (1982), who  proposed a learning rule which finds the first principal component of the variance  matrix of the input data. Based on this model, Oja (1989), Sanger (1989), and  many others have devised numerous neural networks which find many components  of this matrix. These networks perform principal component analysis (PCA), a  well-known method of statistical analysis.  407  408 Shapiro and PrOgel-Bennett  Since PCA is a form of linear analysis, and the neurons used in the PCA networks  are linear - the output of these neurons is equal to the weighted sum of inputs;  there is no squashing function of sigmoid - it is obvious to ask whether non-linear  Hebbian neurons compute some form of non-linear PCA7 Is this a useful way to  understand the performance of the networks? Do these networks learn to extract  features of the input data which are different from those learned by linear neurons?  Currently in the literature, the phrase "non-linear PCA" is used to describe what  is learned by any non-linear generalization of Oja neurons or other PCA networks  (see for example, Oja, 1993 and Taylor, 1993).  In this paper, we discuss the relationship between a particular form of non-linear  Hebbian neurons (Priigel-Bennett and Shapiro, 1992) and a particular generaliza-  tion of non-linear PCA (Softky and Karomen 1991). It is clear that non-linear neu-  rons can perform very differently from linear ones. This has been shown through  analysis (Priigel-Bennett and Shapiro, 1993) and in application (Karhuenen and  Joutsensalo, 1992). It can also be very useful way of understanding what the neu-  rons learn. This is because non-linear PCA is equivalent to maximizing some objec-  tive function. The features that this extracts from a data set can be studied using  techniques of statistical mechanics. However, non-linear PCA is ambiguous because  there are multiple solutions. What the neuron can learn is given by non-linear PCA.  The likelihood of learning the different solutions is governed by the dyanamics cho-  sen to implement non-linear PCA, and may differ in different implementations of  the dynamics.  2 NON-LINEAR HEBBIAN NEURONS  Neurons with non-linear activation functions can learn to perform very different  tasks from those learned by linear neurons. Nonlinear Hebbian neurons have been  analyzed for general non-linearities by Oja (1991), and was applied to sinusoidal  signal detection by Karhuenen and Joutsensalo (1992).  Previously, we analysed a simple non-linear generalization of Oja's rule (Priigel-  Bennett and Shapiro, 1993). We showed how the shape of the neuron activation  function can control what a neuron learns. Whereas linear neurons learn to a  statistic mixture of all of the input patterns, non-linear neurons can learn to become  tuned to individual patterns, or to small clusters of closely correlated patterns.  In this model, each neuron has weights, wi is the weight from the i th input, and  responds to the usual sum of input times weights through an activation function  A(y). This is assumed a simple power-law above a threshold and zero below it. I.e.  A(V p)= { (VP-05) V p>05  0 Vp _< 05 (1)  P is the ith compo-  rtere 05 is the threshold, b controls the power of the power-law, x i  nent of the pth pattern, and V p = i xwi. Curves of these functions are shown  in figure la; if b = i the neurons are threshold-linear. For b > i the curves can be  thought of as low activation approximations to a sigmoid which is shown in figure  lb. The generalization of Oja's learning rule is that the change in the weights Swi  Non-Linear Statistical Analysis and Self-Organizing Hebbian Networks 409  Neuron Activation Function  b>l  b=l  b<l   PSP  A Sigmoid Activation Function  Figure 1' a) The form of the neuron activation function. Control by two parameters  b and . When b > 1, this activation function approximates a sigmoid, which is  shown in b).  is given by  6wi =  A(V p) [xf - VPwi] . (2)  p  If b < 1, the neuron learns to average a set of patterns. If b = 1, the neuron finds  the principal component of the pattern set. When b > 1, the neuron learns to  distinguish one of the patterns in the presence of the others, if those others are not  too correlated with the pattern. There is a critical correlation which is determined  by b; the neuron learns to individual patterns which are less correlated than the  critical value, but learns to something like the center of the cluster if the patterns  are more correlated. The threshold controls the size of the subset of patterns which  the neuron can respond to.  For these neurons, the relationship between non-PCA and the activation function  was not previously discussed. That is done in the next section.  3 NON-LINEAR PCA  A non-linear generalization of PCA was proposed by Softky and Kammen (1991).  In this section, the relationship between non-linear PCA and unsupervised Hebbian  learning is reviewed.  410 Shapiro and Prtigel-Bennett  3.1 WHAT IS NON-LINEAR PCA  The principal component of a set of data is the direction which maximises the  variance. I.e. to find the principal component of the data set, find the vector  of  unit length which maximises  Here, xi denotes the ith component of an input pattern and < .-. > denotes  the average over the patterns. Sofky and Kammen suggested that an appropriate  generalization is to find the vector  which maximizes the d-dimensional correlation,  (4)  They argued this would give interesting results if higher order correlations are im-  portant, or if the shape of the data cloud is not second order. This can be generalized  further, of course, maximizing the average of any non-linear function of the input  U(y),  =<  i  The equations for the principal components are easily found using Lagrange multi-  pliers. The extremal points are given by  < >= (6)  k  These points will be (local) maxima if the Hessian 7/ij,  tj =< v"( wx)xx >-, (7)  k  Here, A is a Lagrange multiplier chosen to make 1[2 _ 1.  3.2 NEURONS WHICH LEARN PCA  A neuron learning via unsupervised Hebbian learning rule can perform this opti-  mization. This is done by associating wi with the weight from the ith input to  P The  the neuron, and the data average <  > as the sum over input patterns x i.  nonlinear function which is optimized is determined by the integral of the activation  function of the neuron  A(y) = U'(y).  In their paper, Softky and Kammen propose a learning rule which does not perform  this optimization in general. The correct learning rule is a generalization of Oja's  rule (equation (2) above), in this notation,  ,w = (A(V) [- Vwd). (8)  Non-Linear Statistical Analysis and Self-Organizing Hebbian Networks 411  This fixed points of this dynamical equation will be solutions to the extremal equa-  tion of nonlinear PCA, equation (6), when the associations  .X = (A(V)V),  and  are made.  A(y) = Ut(y)  Here (.) is interpreted as sum over patterns; this is batch learning. The rule can also  be used incrementally, but then the dynamics are stochastic and the optimization  might be performed only on average, and then maybe only for small enough learning  rates. These fixed points will be stable when the Hessian 7iij is negative definite at  the fixed point. This is now,  -/ij =< U"(V)xixj > - wwA- < VU"(V)x > wi.  (9)  which is the same as the previous, equation (7),in directions perpendicular to the  fixed point, but contains additional terms in direction of the fixed point which  normalize it.  The neurons described in section 2 would perform precisely what Softky and Kam-  men proposed if the activation function was pure power-law and not thresholded;  as it is they maximize a more complicated objective function.  Since there is a one to one correspondence between the stable fixed points of the  dynamics and the local maxima of the non-linear correlation measure, one says that  these non-linear neurons compute non-linear PCA.  3.3 THEORETICAL STUDIES OF NONLINEAR PCA  In order to understand what these neurons learn, we have studied the networks  learning on model data drawn from statistical distributions. For very dense clusters  P - c, N fixed, the stable fixed point equations are algebraic. In a few simple  cases they can be solved. For example, if the data is Gaussian or if the data cloud is  a quadratic cloud (a function of a quadratic form), the neuron learns the principal  component, like the linear neuron. Likewise, if the patterns are not random, the  fixed point equations can be solved in some cases.  For large number of patterns in high dimensions fluctuations in the data are im-  portant (N and P goes to c together in some way). In this case, methods of  statistical mechanics can be used to average over the data. The objective function  of the non-linear PCA acts as (minus) the energy in statistical mechanics. The free  energy is formally,  i=1 i=1  exp U(V) > . (10)  In the limit that fi is large, this calculation finds the local maxima of U. In this  form of analysis, the fact that the neuron optimizes an objective function is very  important. This technique was used to produce the results outlined in section 2.  412 Shapiro and Pgel-Bennett  3.4 WHAT NON-LINEAR PCA FAILS TO REVEAL  In the linear PCA, there is one unique solution, or if there are many solutions  it is because the solutions are degenerate. However, for the non-linear situation,  there are many stable fixed points of the dynamics and many local maxima of the  non-linear correlation measure.  This has two effects. First, it means that you cannot predict what the neuron will  learn simply by studying fixed point equations. This tells you what the neuron  might learn, but the probability that this solution will be can only be ascertained if  the dynamics are understood. This also breaks the relationship between non-linear  PCA and the neurons, because, in principle, there could be other dynamics which  have the same fixed point structure, but do not have the same basins of attraction.  Simple fixed point analysis would be incapable of predicting what these neurons  would learn.  4 PARTITIONING  An important question which the fixed-point analysis, or corresponding statistical  mechanics cannot address is: what is the likelihood of learning the different solu-  tions? This is the essential ambiguity of non-linear PCA - there are many solutions  and the size of the basin of attractions of each is determined by the dynamics, not  by local maxima of the nonlinear correlation measure.  As an example, we consider the partitioning of the neurons described in section 2.  These neurons act much like neurons in competitive networks, they become tuned to  individual patterns or highly correlated clusters. Given that the density of patterns  in the input set is p('), what is the probability p(') that a neuron will become  tuned to this pattern. It is often said that the desired result should be i0(') -- p(),  although for Kohonen 1-d feature maps has been shown to be p() - p()2/3 (see  for example, Hertz, Krogh, and Palmer 1991).  We have found that he partitioning cannot be calculated by finding the optima  of the objective function. For example, in the case of weakly correlated patterns,  the global maxima is the most likely pattern, whereas all of the patterns are local  maxima. To determine the partitioning, the basin of attraction of each pattern  must be computed. This could be different for different dynamics with the same  fixed point structure.  In order to determine the partitioning, the dynamics must be understood. The  details will be described elsewhere (Priigel-Bennett and Shapiro, 1994). For the  case of weakly correlated patterns, a neuron will learn a pattern for which  p(;)(V) b-' > p()(v0q) b-1 Vq  p.  Here V is the initial overlap (before learning) of the neuron's weights with the pth  pattern. This defines the basin of attraction for each pattern.  In the large P limit and for random patterns  p(Z')  p() (11)  where a m 21og(P)/(b- 1), P is the number of patterns, and where b is a parameter  that controls the non-linearity of the neuron's response. If b is chosen so that a = 1,  Non-Linear Statistical Analysis and Self-Organizing Hebbian Networks 413  then the probability of a neuron learning a pattern will be proportional to the  frequency with which the pattern is presented.  5 CONCLUSIONS  The relationship between a non-linear generalization of Oja's rule and a non-linear  generalization of PCA was reviewed. Non-linear PCA is equivalent to maximizing a  objective function which is a statistical measure of the data set. The objective func-  tion optimized is determined by the form of the activation function of the neuron.  Viewing the neuron in this way is useful, because rather than solving the dynamics,  one can use methods of statistical mechanics or other methods to find the maxima  of the objective function. Since this function has many local maxima, however,  these techniques cannot determine how the solutions are partitioned amoung the  neurons. To determine this, the dynamics must be solved.  Acknowledgement s  This work was supported by SERC grant GRG20912.  References  J. Hertz, A. Krogh, and R.G. Palmer. (1991). Introduction to the Theory of Neural  Computation. Addison-Wesley.  J. Karhunen and J. Joutsensalo. (1992) Nonlinear Hebbian algorithms for sinusoidal  frequency estimation, in Artificial Neural Networks, 2, I. Akeksander and J. Taylor,  editors, North-Holland.  Erkki Oja. (1982) A simplified neuron model as a principal component analyzer.  em J. Math. Bio., 15:267-273.  Erkki Oja. (1989) Neural networks, principal components, and subspaces. Int. J.  of Neural Systems, 1(1):61-68.  E. Oja, H. Ogawa, and J. Wangviwattan. (1992) Principal Component Analysis  by homogeneous neural networks: Part II: analysis and extension of the learning  algorithms IEICE Trans. on Information and Systems, E75-D, 3, pp 376-382.  E. Oja. (1993) Nonlinear PCA: algorithms and applications, in Proceedings of  World Congress on Neural Networks, Portland, Or. 1993.  A. Prugel-Bennett and Jonathan L. Shapiro. (1993) Statistical Mechanics of Unsu-  pervised Hebbian Learning. J. Phys. A: 26, 2343.  A. Prugel-Bennett and Jonathan L. Shapiro. (1994) The Partitioning Problem for  Unsupervised Learning for Non-linear Neurons. J. Phys. A to appear.  T. D. Sanger. (1989) Optimal Unsupervised Learning in a Single-Layer Linear  Feedforward Neural Network. Neural Networks 2,459-473.  Jonathan L. Shapiro and A. Prugel-Bennett (1992), Unsupervised Hebbian Learning  and the Shape of the Neuron Activation Function, in Artificial Neural Networks, 2,  I. Akeksander and J. Taylor, editors, North-Holland.  414 Shapiro and Prtigel-Bennett  W. Soffky and D. Kammen (1991). Correlations in High Dimensional or Asymmet-  ric Data Sets: Hebbian Neuronal Processing. Neural Networks 4, pp 337-347.  J. Taylor, (1993) Forms of Memory, in Proceedings of World Congress on Neural  Networks, Portland, Or. 1993.  
Signature Verification using a "Siamese"  Time Delay Neural Network  Jne Bromley, Isabelle Guyon, Yann LeCun,  Eduard Scklnger and Roopak Shah  AT&T Bell Laboratories  Holmdel, NJ 07733  jbromley@big.att.com  Copyright. 1994, American Telephone and Telegraph Company used by permission.  Abstract  This paper describes an algorithm for verification of signatures  written on a pen-input tablet. The algorithm is based on a novel,  artificial neural network, called a "Siamese" neural network. This  network consists of two identical sub-networks joined at their out-  puts. During training the two sub-networks extract features from  two signatures, while the joining neuron measures the distance be-  tween the two feature vectors. Verification consists of comparing an  extracted feature vector with a stored feature vector for the signer.  Signatures closer to this Stored representation than a chosen thresh-  old are accepted, all other signatures are rejected as forgeries.  I INTRODUCTION  The aim of the project was to make a signature verification system based on the  NCR 5990 Signature Capture Device (a pen-input tablet) and to use 80 bytes or  less for signature feature storage in order that the features can be stored on the  magnetic strip of a credit-card.  Verification using a digitizer such as the 5990, which generates spatial coordinates  as a function of time, is known as dynamic verification. Much research has been  carried out on signature verification. Function-based methods, which fit a func-  tion to the pen trajectory, have been found to lead to higher performance while  parameter-based methods, which extract some number of parameters from a signs-  737  738 Bromley, Guyon, Le Cun, Sickinger, and Shah  ture, make a lower requirement on memory space for signature storage (see Lorette  and Plamondon (1990) for comments). We chose to use the complete time extent  of the signature, with the preprocessing described below, as input to a neural net-  work, and to allow the network to compress the information. We believe that it is  more robust to provide the network with low level features and to allow it to learn  higher order features during the training process, rather than making heuristic de-  cisions e.g. such as segmentation into balistic strokes. We have had success with  this method previously (Guyon et al., 1990) as have other authors (Yoshimura and  Yoshimura, 1992).  2 DATA COLLECTION  All signature data was collected using 5990 Signature Capture Devices. They consist  of an LCD overlayed with a transparent digitizer. As a guide for signing, a 1 inch by  3 inches box was displayed on the LCD. However all data captured both inside and  outside this box, from first pen down to last pen up, was returned by the device.  The 5990 provides the trajectory of the signature in Cartesian coordinates as a  function of time. Both the trajectory of the pen on the pad and of the pen above  the pad (within a certain proximity of the pad) are recorded. It also uses a pen  pressure measurement to report whether the pen is touching the writing screen or  is in the air. Forgers usually copy the shape of a signature. Using such a tablet  for signature entry means that a forger must copy both dynamic information and  the trajectory of the pen in the air. Neither of these are easily available to a forger  and it is hoped that capturing such information from signatures will make the task  of a forger much harder. Strangio (1976), Herbst and Liu (1977b) have reported  that pen up trajectory is hard to imitate, but also less repeatable for the signer.  The spatial resolution of signatures from the 5990 is about 300 dots per inch, the  time resolution 200 samples per second and the pad's surface is 5.5 inches by 3.5  inches. Performance was also measured using the same data treated to have a lower  resolution of 100 dots per inch. This had essentially no effect on the results.  Data was collected in a university and at Bell Laboratories and NCR cafeterias.  Signature donors were asked to sign their signature as consistently as possible or  to make forgeries. When producing forgeries, the signer was shown an example  of the genuine signature on a computer screen. The amount of effort made in  producing forgeries varied. Some people practiced or signed the signature of people  they knew, others made little effort. Hence, forgeries varied from undetectable to  obviously different. Skilled forgeries are the most difficult to detect, but in real life a  range of forgeries occur from skilled ones to the signatures of the forger themselves.  Except at Bell Labs., the data collection was not closely monitored so it was no  surprise when the data was found to be quite noisy. It was cleaned up according to  the following rules:  Genuine signatures must have between 80% and 120% of the strokes of  the first signature signed and, if readable, be of the same name as that  typed into the data collection system. (The majority of the signatures were  donated by residents of North America, and, typical for such signatures,  were readable.) The aim of this was to remove signatures for which only  Signature Verification Using a "Siamese" Time Delay Neural Network 739  some part of the signature was present or where people had signed another  name e.g. Mickey Mouse.   Forgeries must be an attempt to copy the genuine signature. The aim of  this was to remove examples where people had signed completely different  names. They must also have 80% to 120% of the strokes of the signature.   A person must have signed at least 6 genuine signatures or forgeries.  In total, 219 people signed between 10 and 20 signatures each, 145 signed genuines,  74 signed forgeries.  3 PREPROCESSING  A signature from the 5990 is typically 800 sets of z, y and pen up-down points. z(t)  and y(t) were originally in absolute position coordinates. By calculating the linear  estimates for the z and y trajectories as a function of time and subtracting this from  the original z and y values, they were converted to a form which is invariant to the  position and slope of the signature. Then, dividing by the y standard deviation  provided some size normalization (a person may sign their signature in a variety  of sizes, this method would normalize them). The next preprocessing step was to  resample, using linear interpolation, all signatures to be the same length of 200  points as the neural network requires a fixed input size. Next, further features were  computed for input to the network and all input values were scaled so that the  majority fell between +1 and -1. Ten different features could be calculated, but a  subset of eight were used in different experiments:  feature  feature  feature  feature  feature  feature  feature  feature  feature  feature  1 pen up = -1; pen down = +1, (pud)  2 x position, as a difference from the linear estimate for a:(t), normalized using  the standard deviation of y, (x)  3 y position, as a difference from the linear estimate for y(t), normalized using  the standard deviation of y, (y)  4 speed at each point, (spd)  5 centripetal acceleration, (acc-c)  6 tangential acceleration, (acc-t)  7 the direction cosine of the tangent to the trajectory at each point, (cos)  8 the direction sine of the tangent to the trajectory at each point, (sin)  9 cosine of the local curvature of the trajectory at each point, (cosb)  10 sine of the local curvature of the trajectory at each point, (sinb)  In contrast to the features chosen for character recognition with a neural network  (Guyon et al., 1990), where we wanted to eliminate writer specific information, the  features such as speed and acceleration were chosen to carry information that aids  the discrimination between genuine signatures and forgeries. At the same time we  still needed to have some information about shape to prevent a forger from breaking  the system by just imitating the rhythm of a signature, so positional, directional  amd curvature features were also used. The resampling of the signatures was such  as to preserve the regular spacing in time between points. This method penalizes  forgers who do not write at the correct speed.  740 Bromley, Guyon, Le Cun, Sickinger, and Shah  TARGET  Figure 1' Architecture 1 consists of two identical time delay neural networks. Each  network has an input of 8 by 200 units, first layer of 12 by 64 units with receptive  fields for each unit being 8 by 11 and a second layer of 16 by 19 units with receptive  fields 12 by 10.  4 NETWORK ARCHITECTURE AND TRAINING  The Siamese network has two input fields to compare two patterns and one output  whose state value corresponds to the similarity between the two patterns. Two  separate sub-networks based on Time Delay Neural Networks (Lang and Hinton,  1988, Guyon et al. 1990) act on each input pattern to extract features, then the  cosine of the angle between two feature vectors is calculated and this represents the  distance value. Results for two different subnetworks are reported here.  Architecture i is shown in Fig 1. Architecture 2 differs in the number and size  of layers. The input is 8 by 200 units, the first convolutional layer is 6 by 192  units with each unit's receptive field covering 8 by 9 units of the input. The first  averaging layer is 6 by 64 units, the second convolution layer is 4 by 57 with 6 by 8  receptive fields and the second averaging layer is 4 by 19. To achieve compression in  the time dimension, architecture 1 uses a sub-sampling step of 3, while architecture  2 uses averaging. A similar Siamese architecture was independently proposed for  fingerprint identification by Baldi and Chauvin (1992).  Training was carried out using a modified version of backpropagation (LeCun, 1989).  All weights could be learnt, but the two sub-networks were constrained to have  identical weights. The desired output for a pair of genuine signatures was for a  small angle (we used cosine=l.0) between the two feature vectors and a large angle  Signature Verification Using a "Siamese" Time Delay Neural Network 741  Table 1: Summary of the Training.  Note: GA is the percentage of genuine signature pairs with output greater than 0, FR  the percentage of genuine:forgery signature pairs for which the output was less than 0.  The aim of removing all pen up points for Network 2 was to investigate whether the pen  up trajectories were too variable to be helpful in verification. For Network 4 the training  simulation crashed after the 42nd iteration and was not restarted. Performance may have  improved if training had continued past this point.  Network Input Features Best Performance on:  Training Set Validation Set  1, arch 1 pud acc-c acc-t spd GA 97.0% FR 65.3%, 26 GA 90.3% FR 74.8%, 6  cos0 sin0 cosb sinb passes through set passes through set  2, arch 1 same as 1, but pen up GA 97.8% FR 60.0%, 11 GA 93.2% FR 75.2%, 2  trajectory removed passes through set passes through set  3, arch 1 x y pud spd cos0 sin0 GA 99.8% FR 88.8%, GA 91.7% FR 74.2%, 32  cosb sinb 100 passes through set passes through set  4, arch 1 same as network 3, GA 98.2% FR 81.7%, 42 GA 99.4% FR 80.5%, 42  but a larger training passes through set passes through set  set  5, arch 2 same as 4, except ar- GA 98.6% FR 81.5%, 69 GA 99.6% FR 80.1%, 44  chitecture 2 was used passes through set passes through set  (we used cosine= -0.9 and -1.0) if one of the signatures was a forgery. The training  set consisted of 982 genuine signatures from 108 signers and 402 forgeries of about  40 of these signers. We used up to 7,701 signature pairs; 50% genuine:genuine pairs,  40% genuine:forgery pairs and 10% genuine:zero-effort pairs. I The validation set  consisted of 960 signature pairs in the same proportions as the training set. The  network used for verification was that with the lowest error rate on the validation  See Table 1 for a summary of the experiments. Training took a few days on a  SPARC 1+.  5 TESTING  When used for verification, only one sub-network is evaluated. The output of this is  the feature vector for the signature. The feature vectors for the last six signatures  signed by each person were used to make a multivariate normal density model of the  person's signature (see pp. 22-27 of Pattern Classification and Scene Analysis by  Duda and Hart for a fuller description of this). For simplicity, we assume that the  features are statistically independent, and that each feature has the same variance.  Verification consists of comparing a feature vector with the model of the signature.  The probability density that a test signature is genuine, p-yes, is found by evaluating  zero-effort forgeries, also known as random forgeries, are those for which the forger  makes no effort to copy the genuine signature, we used genuine signatures from other  signers to simulate such forgeries.  742 Bromley, Guyon, Le Cun, Sickinger, and Shah  Percentage of Genuine Signatures Accepted  Figure 2: Results for Networks 4 (open circles) and 5 (closed circles). The training  of Network 4 was essentially the same as for Network 3 except that more data  was used in training and it had been cleaned of noise. They were both based on  architecture 1. Network 5 was based on architecture 2. The signature feature vector  from this architecture is just 4 by 19 in size.  the normal density function. The probability density that a test signature is a  forgery, p-no, is assumed, for simplicity, to be a constant value over the range of  interest. An estimate for this value was found by averaging the p-yes values for all  forgeries. Then the probability that a test signature is genuine is p-yes/(p-yes + p-  no). Signatures closer than a chosen threshold to this stored representation are  accepted, all other signatures are rejected as forgeries.  Networks 1, 2 and 3, all based on architecture 1, were tested using a set of 63  genuine signatures and 63 forgeries for 18 different people. There were about 4  genuine test signatures for each of the 18 people, and 10 forgeries for 6 of these  people. Networks 1 and 2 had identical training except Network 2 was trained  without pen up points. Network I gave the better results. However, with such a  small test set, this difference may be hardly significant.  The training of Network 3 was identical to that of Network 1, except that x and y  were used as input features, rather than acc-c and acc-t. It had somewhat improved  performance. No study was made to find out whether the performance improvement  came from using x and y or from leaving out acc-c and acc-t. Plamondon and  Parizeau (1988) have shown that acceleration is not as reliable as other functions.  Figure 2 shows the results for Networks 4 and 5. They were tested using a set of  532 genuine signatures and 424 forgeries for 43 different people. There were about  12 genuine test signatures for each person, and 30 forgeries for 14 of the people.  This graph compares the performance of the two different architectures.  It takes 2 to 3 minutes on a Sun SPARC2 workstation to preprocess 6 signatures,  Signature Verification Using a "Siamese" Time Delay Neural Network 743  collect the 6 outputs from the sub-network and build the normal density model.  6 RESULTS  Best performance was obtained with Network 4. With the threshold set to detect  80% of forgeries, 95.5% of genuine signatures were detected (24 signatures rejected).  Performance could be improved to 97.0% genuine signatures detected (13 rejected)  by removing all first and second signature from the test set 5. For 9 of the remaining  13 rejected signatures pen up trajectories differed from the person's typical signa-  ture. This agrees with other reports (Strangio, 1976 Herbst and Liu, 1977b) that  pen up trajectory is hard to imitate, but also a less repeatable signature feature.  However, removing pen up trajectories from training and test sets did not lead to  any improvement (Networks i and 2 had similar performance), leading us to be-  lieve that pen up trajectories are useful in some cases. Using an elastic matching  method for measuring distance may help. Another cause of error came from a few  people who seemed unable to sign consistently and would miss out letters or add  new strokes to their signature.  The requirement to represent a model of a signature in 80 bytes means that the  signature feature vector must be encodable in 80 bytes. Architecture 2 was specif-  ically designed with this requirement in mind. Its signature feature vector has 76  dimensions. When testing Network 5, which was based on this architecture, 50% of  the outputs were found (surprisingly) to be redundant and the signature could be  represented by a 38 dimensional vector with no loss of performance. One explana-  tion for this redundancy is that, by reducing the dimension of the output (by not  using some outputs), it is easier for the neural network to satisfy the constraint that  genuine and forgery vectors have a cosine distance of -1 (equivalent to the outputs  from two such signatures pointing in opposite directions).  These results were gathered on a Sun SPARC2 workstation where the 38 values  were each represented with 4 bytes. A test was made representing each value in one  byte. This had no detrimental effect on the performance. Using one byte per value  allows the signature feature vector to be coded in 38 bytes, which is well within  the size constraint. It may be possible to represent a signature feature vector with  even less resolution, but this was not investigated. For a model to be updatable  (a requirement of this project), the total of all the squares for each component of  the signature feature vectors must also be available. This is another 38 dimensional  vector. From these two vectors the variance can be calculated and a test signature  verified. These two vectors can be stored in 80 bytes.  7 CONCLUSIONS  This paper describes an algorithm for signature verification. A model of a person's  signature can easily fit in 80 bytes and the model can be updated and become more  accurate with each successful use of the credit card (surely an incentive for people  to use their credit card as frequently as possible). Other beneficial aspects of this  verification algorithm are that it is more resistant to forgeries for people who sign  2people commented that they needed to sign a few time to get accustomed to the pad  744 Bromley, Guyon, Le Cun, Sickinger, and Shah  consistently, the algorithm is independent of the general direction of signing and is  insensitive to changes in size and slope.  As a result of this project, a demonstration system incorporating the neural network  signature verification algorithm was developed. It has been used in demonstrations  at Bell Laboratories where it worked equally well for American, European and  Chinese signatures. This has been shown to commercial customers. We hope that  a field trial can be run in order to test this technology in the real world.  Acknowledgements  All the neural network training and testing was carried out using SN2.6, a neu-  ral network simulator package developed by Neuristique. We would like to thank  Bernhard Boser, John Denker, Donnie Henderson, Vic Nalwa and the members of  the Interactive Systems department at AT&T Bell Laboratories, and Cliff Moore  at NCR Corporation, for their help and encouragement. Finally, we thank all the  people who took time to donate signatures for this project.  References  P. Baldi and Y. Chauvin, "Neural Networks for Fingerprint Recognition", Neural Compu-  tation, 5 (1993).  R. Duda and P. Hart, Pattern Classification and Scene Analysis, John Wiley and Sons,  Inc., 1973.  I. Guyon, P. Albrecht, Y. LeCun, J. S. Denker and W. Hubbard, "A Time Delay Neural  Network Character Recognizer for a Touch Terminal", Pattern Recognition, (1990).  N.M. Herbst and C. N. Liu, "Automatic signature verification based on accelerometry",  IBM J. Res. Develop., 21 (1977)245-253.  K. J. Lang and G. E. Hinton, "A Time Delay Neural Network Architecture for Speech  Recognition", Technical Report CMU-cs-88-152, Carnegie-Mellon University, Pittsburgh,  PA, 1988.  Y. LeCun, "Generalization and Network Design Strategies", Technical Report CRG-TR-  89-4 University of Toronto Connectionist Research Group, Canada, 1989.  G. Lorette and R. Plamondon, "Dynamic approaches to handwritten signature verifica-  tion", in Computer processing of handwriting, Eds. R. Plamondon and C. G. Leedham,  World Scientific, 1990.  R. Plamondon and M. Parizeau, "Signature verification from position, velocity and accel-  eration signals: a comparative study", in Proc. 9th Int. Con. on Pattern Recognition,  Rome, Italy, 1988, pp 260-265.  C. E. Strangio, "Numerical comparison of similarly structured data perturbed by random  variations, as found in handwritten signatures", Technical Report, Dept. of Elect. Eng.,  1976.  I. Yoshimura and M. Yoshimura, "On-line signature verification incorporating the direction  of pen movement - an experimental examination of the effectiveness", in From pixels to  features III: frontiers in Handwriting recognition, Eds. S. Impedova and J. C. Sim6n,  Elsevier, 1992.  
WATTLE: A Trainable Gain Analogue  VLSI Neural Network  Richard Coggins and Marwan Jabri  Systems Engineering and Design Automation Laboratory  Department of Electrical Engineering J03,  University of Sydney, 2006.  Australia.  Email: richardc@sedal.su.oz.au  marwan@sedal.su.oz.au  Abstract  This paper describes a low power analogue VLSI neural network  called Wattle. Wattle is a 10:6:4 three layer perceptron with multi-  plying DAC synapses and on chip switched capacitor neurons fabri-  cated in 1.2um CMOS. The on chip neurons facillitate variable gain  per neuron and lower energy/connection than for previous designs.  The intended application of this chip is Intra Cardiac Electrogram  classification as part of an implantable pacemaker/defibrillator sys-  tem. Measurements of the chip indicate that 10pJ per connection  is achievable as part of an integrated system. Wattle has been suc-  cessfully trained in loop on parity 4 and ICEG morphology classi-  fication problems.  I INTRODUCTION  A three layer analogue VLSI perceptron has been previously developed by  [Leong and Jabri, 1993]. This chip named Kakadu uses 6 bit digital weight storage,  multiplying DACs in the synapses and fixed value off chip resistive neurons. The  chip described in this paper called Wattle has the same synapse arrays as Kakadu,  however, has the neurons implemented as switched capacitors on chip. For both  Kakadu and Wattle, analogue techniques have been favoured as they offer greater  opportunity to achieve a low energy and small area design over standard digital  874  WATTLE: A Trainable Gain Analogue VLSI Neural Network 875  Iout+ md font- opt <    rn oth sps  ...... ,  oonnt h  w,aoi-rr sTooE v+ tl-  ' WR  Figure 1' Wattle Synapse Circuit Diagram  SYNAPSE R CUlT  techniques since the transistor count for the synapse can be much lower and the  circuits may be biased in subthreshold. Some work has been done in the low en-  ergy digital area using subthreshold and optimised threshold techniques, however  no large scale circuits have been reported so far. [Burr and Peterson, 1991] The cost  of using analogue techniques is however, increased design complexity, sensitivity to  noise, offsets and component tolerances. In this paper we demonstrate that difficult  nonlinear problems and real world problems can be trained despite these effects.  At present, commercially available pacemakers and defibrillators use timing deci-  sion trees implemented on CMOS microprocessors for cardiac arrythmia detection  via peak detection on a single ventricular lead. Even when atrial leads are used, In-  tra Cardiac Electrogram (ICEG) morphology classification is required to separate  some potentially fatal rhythms from harmless ones. [Leong and Jabri, 1992] The  requirements of such a morphology classifier are:   Adaptable to differing morphology within and across patients.   Very low power consumption. ie. minimum energy used per classification.   Small size and high reliability.  This paper demonstrates how this morphology classification may be done using a  neural network architecture and thereby meet the constraints of the implantable  arrythmia classification system. In addition, in loop training results will also be  given for parity 4, another difficult nonlinear training problem.  876 Coggins and Jabri  reset  clock  Vdd  ctk  charging  clock  synapse  row  connects  cop  COM  fan out to  F=i next laye[  Figure 2: Wattle Neuron Circuit Diagram  [ Row Address ]  Column  Address  ur  10x6 Synapse Array  neurons ]  [ hi mutipbxor I [nclk demux ]  IJ   $X4 Synapse    Array [--"] [  ,.uron. I--I I--1  Data Register I Buffers  Figure 3: Wattle Floor Plan  WATI'LE: A Trainable Gain Analogue VLSI Neural Network 877  l,  'L  Figure 4: Photomicrograph of Wattle  2 ARCHITECTURE  Switched capacitors were chosen for the neurons on Wattle after a test chip was fab-  ricated to evaluate three neuron designs. [Coggins and Jabri, 1993] The switched  capacitor design was chosen as it allowed flexible gain control of each neuron, in-  vestigation of gain optimisation during limited precision in loop training and the  realisation of very high effective resistances. The wide gain range of the switched  capacitor neurons and the fact that they are implemented on chip has allowed Wat-  tle to operate over a very wide range of bias currents from lpA LSB DAC current  to 10nA LSB DAC current.  Signalling on Wattle is fully differential to reduce the effect of common mode noise.  The synapse is a multiplying digital to analogue convertor with six bit weights. The  synapse is shown in figure 1. This is identical to the synapse used on the Kakadu  chip [Leong and Jabri, 1993]. The MDAC synapses use a weighted current source  to generate the current references for the weights. The neuron circuit is shown in  figure 2. The neuron requires reset and charging clocks. The period of the charging  clock determines the gain. Buffers are used to drive the neuron outputs off chip to  avoid the effects of stray pad capacitances.  Figure 3 shows a floor plan of the wattle chip. The address and data for the weights  access is serial and is implemented by the shift registers on the boundary of the chip.  The hidden layer multiplexor allows access to the hidden layer neuron outputs. The  neuron demultiplexor switches the neuron clocks between the hidden and output  layers. Figure 4 shows a photomicrograph of the wattle die.  3 ELECTRICAL CHARACTERISTICS  Tests have been performed to verify the operation of the weighted current source  for the MDAC synapse arrays, the synapses, the neurons and the buffers driving the  neuron voltages off chip. The influences of noise, offsets, crosstalk and bandwidth  of these different elements have been measured. In particular, the system level noise  measurement showed that the signal to noise ratio was 40dB. A summary of the  electrical characteristics appears in table 1.  878 Coggins and Jabri  Table 1: Electrical Characteristics and Specifications  Parameter  Area  Technology  Resolution  Energy per connection  LSB DAC current  Feedforward delay  Synapse Offset  Gain cross talk delta  Value  2.2 x 2.2ram 2  1.2urn Nwell CMOS 2M2P  weights 6bit, gains 7bit  43pJ  200pA  1.5ms  5mV  20%  Comment  standard process  weights on chip, gains off  all weights maximum  typical  @200pA, 3V supply  typical maximum  maximum  A gain cross talk effect between the neurons was discovered during the electrical  testing. The mechanism for this cross talk was found to be transients induced on the  current source reference lines going to all the synapses as individual neuron gains  timed out. The worst case cross talk coupled to a hidden layer neuron was found  to be a 20% deviation from the singularly activated value. However, the training  results of the chip do not appear to suffer significantly from this effect.  A related effect is the length of time for the precharging of the current summation  lines feeding each neuron due to the same transients being coupled onto the current  source when each neuron is active. The implication of this is an increase in energy  per classification for the network due to the transient decay time. However, one of  the current reference lines was available on an outside pin, so the operation of the  network free of these transients could also be measured. For this design, including  the transient conditions, an energy per connection of 43pJ can be achieved. This  may be reduced to 10pJ by modifying the current source to reduce transients and  neglecting the energy of the buffers. This is to be compared with typical digital  10nJ per connection and analogue of 60pJ per connection appearing in the literature.  [Delcorso et. al., 1993], Table 1.  4 TRAINING BOTH GAINS AND WEIGHTS  A diagram of the system used to train the chip is shown in figure 5. The training  software is part of a package called MUME [Jabri et. al., 1992], which is a multi  module neural network simulation environment. Wattle is interfaced to the work  station by Jiggle, a general purpose analogue and digital chip tester developed by  SEDAL. Wattle, along with gain counter circuitry, is mounted on a separate daugh-  ter board which plugs into Jiggle. This provides a software configurable testing  environment for Wattle. In loop training then proceeds via a hardware specific  module in MUME which writes the weights and reads back the analogue output of  the chip. Wattle can then be trained by a wide variety of algorithms available in  MUME.  Wattle has been trained in loop using a variation on the Combined Search Algorithm  (CSA) for limited precision training. [Xie and Jabri, 1992] (Combination of weight  perturbation and axial random search). The variation consists of training the gains  WATI'LE: A Trainable Gain Analogue VLSI Neural Network 879  SUN IPC  Running  MUME  (CSA)  SBUS to VME  Convertor  Figure 5: The in loop training of Wattle  in exactly the same way that the weights are trained rather than scaling the gains  based on the average value of the weights connected to a neuron. In this case, the  gains are 7 bits and are implemented off chip. The in loop training results of the  chip are compared with that of a numerical model. The numerical model used does  not model noise, cross talk, offsets or common mode saturation and in this sense is  an idealised model of the chip. A comparison is done between the training of each  neuron gain versus leaving the neuron gain fixed.  5  TRAINING AND GENERALISATION  PERFORMANCE  The parity 4 problem converged 8 out of 10 times training the gains within a max-  imum of 300 iterations of CSA. Leaving the gains fixed, convergence was obtained  3 out 6 times within 500 iterations. On the numerical model, parity 4 did not  converge within 300 iterations. This suggests that the influence of noise may have  assisted the training as in annealing based algorithms. In all cases a 5:5:1 network  was used where the 5th input is a bias.  The ICEG training problem consisted of separating the ICEG morphology of pa-  tients with Retrograde Ventricular Tachycardia. The morphologies to be separated  were Normal Sinus Rhythm and Ventricular Tachycardia on an individual patient  basis. In this case, 10 morphology samples are used as inputs to 10:H:2 network  where H was varied from 1 to 6. The chip trained a patient using trainable gains.  The 8 training patterns were selected and the network trained twice on each archi-  tecture.  Table 2 summarises the classification performance on data not previously seen by  880 Coggins and Jabri  4.4O  40  4.00  :].so  3.60  3,40  3.20  3.00  2.80  2.60.  2.4o  2.20  2.oo  1.8o  1.6o  1.4o  1.2o  r,t o  0.40  o..20  oo  Tralntng_51d_units  t.I  Figure 6: An example training run of Wattle  the network for simulated and in loop chip training. Figure 6 shows an example  of the CSA algorithm training the chip in loop. The CSA algorithm should always  reduce the error, however the effect of the noise in the hardware is evident once the  training reaches smaller error values.  6 CONCLUSION  The results from the Wattle chip have shown the combination of switched capacitor  and analogue design techniques have produced a very low power trainable neural  network for the ICEG classifica[ion problem. When used with the CSA training  algorithm the parity 4 problem is trained routinely. Work is continuing to assess  trainable gain requirements and advantages for different problems. With some  design modification 10pJ per connection energy useage is achievable.  Acknowledgement s  The work presented in this paper was jointly funded by:   The Department of Industry, Technology and Regional Development.   Telectronics Pacing Systems Ltd., Australia.  The authors also acknowledge the contribution of Philip Leong, Barry Flower and  Steve Pickard for their help in laying out and testing wattle and Jeevan Gagarin for  his assistance with the test jig hardware.  WATTLE: A Trainable Gain Analogue VLSI Neural Network 881  Table 2: ICEG Generalisation Performance  No. Of Hidden Testing Patterns Correct %  Units  Sim. Chip.  NSR VT NSR VT  1 24 100 91 99  24 100 90 98  2 94 99 97 93  94 99 96 93  3 97 87 100 84  97 87 100 82  4 96 97 90 97  96 97 91 99  5 100 99 100 97  100 99 97 99  6 94 93 94 95  94 93 96 97  References  [Leong and Jabri, 1993] Leong, P.H.W. and Jabri, M.A., A Low Power Analogue  Neural Network Classifier Chip, Proceedings of the IEEE Custom Integrated  Circuits Conference, p4.5.1-4.5.4, San Diego, USA, May 1993.  [Leorig and Jabri, 1992] Leong, P.It.W. and Jabri, M.A., MATIC - An Intracardiac  Tachycardia Classification System, PACE, September, 1992.  [Burr and Peterson, 1991] Burr, J. and Peterson A., Ultra Low Power CMOS Tech-  nology, 3rd NASA Symposium on VLSI Design, 1991.  [Coggins and Jabri, 1993] R.J. Coggins, M.A. Jabri, S.J. Pickard, A Comparison  of Three On Chip Neuron Designs For a Low Power Analogue VLSI MLP, Mi-  cronemro '93, Edinburgh, Scotland, UK., April, 1993.  [Delcorso et. al., 1993] D. Del Corso, F. Gregoretti, L.M. Reyneri, An Artificial  Neural System Using Coherent Pulse Width and Edge Modulations, Micronemro  '93, Edinburgh, Scotland, UK., April, 1993.  [Xie and Jabri, 1992] Xie, Yun and Jabri, Marwan, Training Limited Precision  Feedforward Neural Networks, ACNN92, p68-71, Canberra, Australia, February  1992.  [Tsividis ,1989] Yannis P. Tsividis, "Operation and Modelling of the MOS Transis-  tor", McGraw-Hill, 1988.  [Jabri et. al., 1992] Jabri M., Tinker E., Leerink L. (1992)"MUME- A Multi-NET-  Multi- Architecture Neural Simulation Environment". Neural Network Simula-  tion Environments, Kluwer Academic Publications.  
The Role of MT Neuron Receptive Field  Surrounds in Computing Object Shape from  Velocity Fields  G.T.Buracas & T.D.Albright  Vision Center Laboratory, The Salk Institute,  P.O.Box 85800, San Diego, California 92138-9216  Abstract  The goal of this work was to investigate the role of primate  MT neurons in solving the structure from motion (SFM)  problem. Three types of receptive field (RF) surrounds  found in area MT neurons (K. Tanaka et a/.,1986; Allman et  a/.,1985) correspond, as our analysis suggests, to the 0 th, I st  and 2 nd order fuzzy space-differential operators. The large  surround/center radius ratio (> 7) allows both  differentiation of smooth velocity fields and discontinuity  detection at boundaries of objects. The model is in  agreement with recent psychophysical data on surface  interpolation involvement in SFM. We suggest that area  MT partially segregates information about object shape  from information about spatial relations necessary for  navigation and manipulation.  1 INTRODUCTION  Both neurophysiological investigations [8] and lesioned human patients'  data show that the Middle Temporal (MT) cortical area is crucial to  perceiving three-dimensional shape in moving stimuli. On the other hand,  969  970 Buracas and Albright  a solid body of data (e.g. [1]) has been gathered about functional properties  of neurons in the area MT. Hoever, the relation between our ability to  perceive structure in stimuli, simulating 3-D objects, and neuronal  properties has not been addressed up to date. Here we discuss a  possibility, that area MT RF surrounds might be involved in shape-from-  motion perception. We introduce a simplifying model of MT neurons and  analyse the implications to SFM problem solving.  2 REDEFINING THE SFM PROBLEM  2.1 RELATIVE MOTION AS A CUE FOR RELATIVE DEPTH  Since Helmholtz motion parallax is known to be a powerful cue providing  information about both the structure of the surrounding environment and  the direction of self-motion. On the other hand, moving objects also induce  velocity fields allowing judgement about their shapes. We can capture both  cases by assuming that an observer is tracking a point on a surface of  interest. The velocity field of an object then is (fig. l): V = t z + w x (R - R o )  =-tz+WXZ, where w=[Wx,Wy,0] is an effective rotation vector of a surface  z=[x,y,z(x,y)]; Ro=[0,0,z0 ] is a positional vector of the fixation point; t z is a  translational component along Z axis.  x  z(x,y)  w  y  Fig. 1: The coordinate system assumed in this paper. The origin is set at  the fixation point. The observer is at Z 0 distance from a surface.  The Role of MT Neuron Receptive Field Surrounds in Computing Object Shape 971  The component velocities of a retinal velocity field under perspective  projection can be calculated from:  WyZ --Xtz -wxxy+Wy x2 wZ -Ytz +WyXy--wxY 2  ZO +z (Zo +Z) 2 , V----  Z 0 + Z (Z 0 + Z) 2  In natural viewing conditions the distance to the surface z 0 is usually much  larger than variation in distance on the surface z : z0>>z. In such the  second term in the above equations vanishes. In the case of translation  tangential to the ground, to which we confine our analysis, w=[0,Wy,0] -  [0,w,0], and the retinal velocity reduces to  u = -wz/(zo+z) -- -wz/zo, v=O (1).  The latter relation allows the assumption of orthographic projection, which  approximates the retinal velocity field rather well within the central 20 deg  of the visual field.  2.2 SFM PERCEPTION INVOLVES SURFACE INTERPOLATION  Human SFM perception is characterized by an interesting peculiarity --  surface interpolation [7]. This fact supports the hypothesis that an  assumption of surface continuity is embedded in visual system. Thus, we  can redefine the SFM problem as a problem of characterizing the  interpolating surfaces. The principal normal curvatures are a local  measure of surface invariant with respect to translation and rotation of the  coordinate system. The orientation of the surface (normal vector) and its  distance to the observer provide the information essential for navigation  and object manipulation. The first and second order differentials of a  surface function allow recovery of both surface curvature and orientation.  3 MODEL OF AREA MT RECEPTIVE FIELD SURROUNDS  3.1 THREE TYPES OF RECEPTIVE FIELD SURROUNDS  The Middle Temporal (MT) area of monkeys is specialized for the  systematic representation of direction and velocity of visual motion [1,2].  MT neurons are known to posess large, silent (RFS, the "nonclassical RF".  Born and Tootell [4] have very recently reported that the RF surrounds of  neurons in owl monkey MT can be divided into antagonistic and synergistic  types (Fig.2a).  972 Buracas and Albright  25  g2o   10  0  a)  0 10  Annulus diameter deg  Fig. 2: Top left (a): an example of a  synergistic RF surround, redrawn from  [4] (no velocity tuning known). Bottom  left (b): a typical V-shaped tuning curve  for RF surround The horizontal axis  represents the logarithmic scale of ratio  between stimulus speeds in the RF  center and surround, redrawn from [9].  Bottom (c,d): monotonically increasing  and decreasing tuning curves for RF  surrounds, redrawn from [9].  1  0.8  0.6  o,4  0.2  0  03  1 10  Ratio of C/S s pc. eds  1    80.4  ergO. 2  c)  0.1 1 10  Ratio of C/S speeds  01 1  Ralio f C4S speeds  About 44% of the owl monkey neuron RFSs recorded by Allman et al. [3]  showed antagonistic properties. Approximately 33% of these demonstrated  V(or U)oshaped (Fig. 2b), and 66% - quasi-linear velocity tuning curves  (Fig. 2c, d). One half of Macaca fuscata neurons with antagonistic RFS  found by Tanaka et al [9] have had V(U)-shaped velocity tuning curves,  and 50% monotonically increasing or decreasing velocity tuning curves.  The RFS were tested for symmetry [9] and no asymmetrical surrounds  were found in primate MT.  3.2 CONSTRUCTING IDEALIZED MT FILTERS  The surround (S) and center (C) responses seem to be largely independent  (except for the requirement that the velocity in the center must be nonzero)  and seem to combine in an additive fashion [5]. This property allows us to  combine C and S components in our model independently. The resulting  filters can be reduced to three types, described below.  3.2.1 Discrete Filters  The essential properties of the three types of RFSs in area MT can be  captured by the following difference equations. We choose the slopes of  velocity tuning curves in the center to be equal to the ones in the surround;  this is essential for obtaining the desired properties for 12 but not 10. The 0-  order (or low-pass) and the 2nd order (or band-pass) filters are defined by:  The Role of MT Neuron Receptive Field Surrounds in Computing Object Shape 973  l o=goyy(uc+us(i,j))+ConstO ,; 12=g2yy(uc-us(i,j))+Const2, (2)  i j i j  where g is gain, wi j =1, ije [-r,r] (r = radius of integration). Speed scalars  u(ij) at points [ij] replace the velocity vectors 17 due to eq. (1). Constants  correspond to spontaneous activity levels.  In order to achieve the V(U) -shaped tuning for the surround in Fig.2b, a  nonlinearity has to be introduced:  li =gi(Uc-Us(i,j)) 2 +Constl. (3)  i j  The responses of 11 and 12 filters to standard mapping stimuli used in [3,9]  are plotted together with their biological correlates in Fig. 3.  3.2.2 Continuous analogues of MT filters  We now develop continuous, more biologicaly plausible, versions of our  three MT filters. We assume that synaptic weights for both center and  surround regions fall off with distance from the RF center as a Gaussian  function G(x,y,(), and ( is different for center and surround: (c  Cs- Then,  by convolving with Gaussians equation (2) can be rewritten:  L o (i,j)= u(i,j)*G(o c )+u(i,j)*G(o s ),  L (i,j) = +[u(i,j)*G({J c )-u(i,j)*G({J s )].  The continuous nonlinear L1 filter can be defined if equivalence to l (eq. 3)  is observed only up to the second order term of power series for u(ij):  Li (i,j)=u 2 (i,j)*G({ c )+u 2 (i,j)*G(c s )-C.[u(i,j)*G({ c )].[u(i,j)*G(c s )];  u2(ij) corresponds to full-wave rectification and seems to be common in  area V1 complex neurons; C = 2/Erf2(n/2 /2) is a constant, and Erf0 is an  error function.  3.3 THE ROLE OF MT NEURONS IN SFM PERCEPTION.  Expanding z(x,y) function in (1) into power series around an arbitrary  point and truncating above the second order term yields:  u(x,y)=w(ax2+by2+cxy+dx+ey+f)/zo, where a,b,c,d,e,f are expansion  coefficients. We assume that w is known (from proprioceptive input) and  =1. Then z 0 remans an unresolved scaling factor and we omit it for  simplicity.  974 Buracas and Albright  DATA MODEL  1  O. L 2  1/4 1/2 I 2 4 1/4 1/2 I 2 4  Surround/Center speed ratio  L1 response in slant space  Fig. 3: The comparison between data  [9] and model velocity tuning curves  for RF surrounds. The standard  mapping stimuli (optimaly moving bar  in the center of RF, an annulus of  random dots with varying speed) were  applied to L 1 and L 2 filters. Thee  output of the filters was passed  through a sigmoid transfer function to  accout for a logarithmic compresion in  the data.  Fig. 4: Below, left: the response profile  of the L1 filter in orientation space (x  and y axes represent the components of  normal vector ). Right: the response  profile of the L 2 filter in curvature  space. x and y axes represent the two  normal principal curvatures.  L2 response in curvature space  1C  -10  -5  -10  -15 -15  -15 -10 -5 0 5 10 15 -15 -10 -5 0 5 10 15  Applying L 0 on u(x,y), high spatial frequency information is filtered out,  but otherwise u(x,y) does not change, i.e. L0*u covaries with lower  frequencies of u(x,y). L 2 applied on u(x,y) yields:  2 ((c 2 -(s ) v2u, (4)  L2 *u=(2a+2b)C2(lJc 2 -{Js )=C2 2  that is, L 2 shows properties of the second order space-differential operator -  Laplacian; C2((c 2 - (s 2) is a constant depending only on the widths of the  center and surround Gaussians. Note that L2*u -- c I + c 2 , (cL2 are  principal normal curvatures) at singular points of surface z(x,y).  The Role of MT Neuron Receptive Field Surrounds in Computing Object Shape 975  When applied on planar stimuli Up(X,y) = d x + e y, L 1 has properties of a  squared first order differential operator:  2 2 2 I x +(y) lUp ' (5)  L 1 *Up =(d 2 +e 2 )C 1 (Oc 2 -o s )- C 1 (o c -o s ) ( )2 2  where C2(Oc 2 - Os2) is a function of O c and o s only. Thus the output of L1 is  monotonically related to the norm of gradient vector. It is straightforward  to calculate the generic second order surface based on outputs of three Lo,  four L and one L 2 filters.  Plotting the responses of L and L 2 filters in orientation and curvature  space can help to estimate the role they play in solving the SFM problem  (Fig.4). The iso-response lines in the plot reflect the ambiguity of MT filter  responses. However, these responses covary with useful geometric  properties of surfaces -- norm of gradient (L l) and mean curvature (L2).  3.4 EXTRACTING VECTOR QUANTITIES  Equations (4) and (5) show, that only averaged scalar quantities can be  extracted by our MT operators. The second order directional derivatives  for estimating vectorial quantities can be computed using an oriented RFs  with the following profile: O2=G(x,o s) [G(y,o s) - G(y,oc)]. O1 then can be  defined by the center - surround relationship of L1 filter. The outputs of  MT filters L 1 and L 2 might be indispensible in normalizing responses of  oriented filters. The normal surface curvature can be readily extracted  using combinations of MT and hypothetical O filters. The oriented spatial  differential operators have not been found in primate area MT so far.  However, preliminary data from our lab indicate that elongated RFs may  be present in areas FST or MST [6].  3.5 L2: LAPLACIAN VS. NAKAYAMA'S CONVEXITY OPERATOR  The physiologically tested ratio of standard deviations for center and sur-  round Gaussians Os/O c > 7. Thus, besides performing the second order  differentiation in the low frequency domain, L9. can detect discontinuities  in optic flow.  4. CONCLUSIONS  We propose that the RF surrounds in MT may enable the neurons to  function as differential operators. The described operators can be thought  of as providing a continuous interpolation of cortically represented  surfaces.  Our model predicts that elongated RFs with flanking surrounds will be  found (possibly in areas FST or MST [6]). These RFs would allow extraction  976 Buracas and Albright  of the directional derivatives necessary to estimate the principal curvatures  and the normal vector of surfaces.  From velocity fields, area MT extracts information relevant to both the  "where" stream (motion trajectory, spatial orientation and relative distance  of surfaces) and the "what" stream (curvature of surfaces).  Acknowledgements  Many thanks to George Carman, Lisa Croner, and Kechen Zhang for  stimulating discussions and Jurate Bausyte for helpful comments on the  poster. This project was sponsored by a grant from the National Eye  Institute to TDA and by a scholarship from the Lithuanian Foundation to  GTB. The presentation was supported by a travel grant from the NIPS  foundation.  References  [1] Albright, T.D. (1984) Direction and orientation selectivity of neurons in  visual area MT of the macaque. J. Neurophysiol., 52: 1106-1130.  [2] Albright, T.D., R. Desimone. (1987) Local precision of visuotopic  organization in the middle temporal area (MT) of the macaque. Exp. Brain  Res., 65, 582-592.  [3] Allman, J., Miezin, F., McGuinnes. (1985) Stimulus specific responses  from beyond the classical receptive field. Ann. Rev. Neurosci., 8, 407-430.  [4] Born R.T. & Tootell R.B.H. (1992) Segregation of global and local motion  processing in primate middle temporal visual area. Nature, 357, 497-499.  [5] Born R.T. & Tootell R.B.H. (1993) Center - surround interactions in  direction - selective neurons of primate visual area MT. Neurosci. Abstr.,  19, 315.5.  [6] Carman G.J., unpublished results.  [7] Hussain M., Treue S. & Andersen R.A. (1989) Surface interpolation in  three-dimensional Structure-from-Motion perception. Neural Computation,  1, 324-333.  [8] Siegel, R.M. and R.A. Andersen. (1987) Motion perceptual deficits  following ibotenic acid lesions of the middle temporal area in the behaving  rhesus monkey. Soc. Neurosci.Abstr., 12, 1183.  [9]Tanaka, K., Hikosaka, K., Saito, H.-A., Yukie, M., Fukada, Y., Iwai, E.  (1986) Analysis of local and wide-field movements in the superior temporal  visual areas of the macaque monkey. J. Neurosci., 6, 134-144.  
Convergence of Indirect Adaptive  Asynchronous Value Iteration Algorithms  Vijaykumar Gullapalll  Department of Computer Science  University of Massachusetts  Amherst, MA 01003  vijaycs.umass.edu  Andrew G. Barto  Department of Computer Science  University of Massachusetts  Amherst, MA 01003  barto@cs.umass.edu  Abstract  Reinforcement Learning methods based on approximating dynamic  programming (DP) are receiving increased attention due to their  utility in forming reactive control policies for systems embedded  in dynamic environments. Environments are usually modeled as  controlled Markov processes, but when the environment model is  not known a priori, adaptive methods are necessary. Adaptive con-  trol methods are often classified as being direct or indirect. Direct  methods directly adapt the control policy from experience, whereas  indirect methods adapt a model of the controlled process and com-  pute control policies based on the latest model. Our focus is on  indirect adaptive DP-based methods in this paper. We present a  convergence result for indirect adaptive asynchronous value itera-  tion algorithms for the case in which a look-up table is used to store  the value function. Our result implies convergence of several ex-  isting reinforcement learning algorithms such as adaptive real-time  dynamic programming (ARTDP) (Barto, Bradtke, & Singh, 1993)  and prioritized sweeping (Moore & Atkeson, 1993). Although the  emphasis of researchers studying DP-bascd reinforcement learning  has been on direct adaptive methods such as Q-Learning (Watkins,  1989) and methods using TD algorithms (Sutton, 1988), it is not  clear that these direct methods are preferable in practice to indirect  methods such as those analyzed in this paper.  695  696 Gullapalli and Barto  1 INTRODUCTION  Reinforcement learning methods based on approximating dynamic programming  (DP) are receiving increased attention due to their utility in forming reactive con-  trol policies for systems embedded in dynamic environments. In most of this work,  learning tasks are formulated as Markovian Decision Problems (MDPs) in which  the environment is modeled as a controlled Markov process. For each observed  environmental state, the agent consults a policy to select an action, which when  executed causes a probabilistic transition to a successor state. State transitions  generate rewards, and the agent's goal is to form a policy that maximizes the ex-  pected value of a measure of the long-term reward for operating in the environment.  (Equivalent formulations minimize a measure of the long-term cost of operating in  the environment.) Artificial neural networks are often used to store value functions  produced by these algorithms (e.g., (Tesauro, 1992)).  Recent advances in reinforcement learning theory have shown that asynchronous  value iteration provides an important link between reinforcement learning algo-  rithms and classical DP methods for value iteration (VI) (Barto, Bradtke, & Singh,  1993). Whereas conventional VI algorithms use repeated exhaustive "sweeps" of the  MDP's state set to update the value function, asynchronous VI can achieve the same  result without proceeding in systematic sweeps (Bertsekas & Tsitsiklis, 1989). If the  state ordering of an asynchronous VI computation is determined by state sequences  generated during real or simulated interaction of a controller with the Markov pro-  cess, the result is an algorithm called Real-Time DP (RTDP) (Barto, Bradtke, &  Singh, 1993). Its convergence to optimal value functions in several kinds of prob-  lems follows from the convergence properties of asynchronous VI (Barto, Bradtke,  & Singh, 1993).  2 MDPS WITH INCOMPLETE INFORMATION  Because asynchronous VI employs a basic update operation that involves computing  the expected value of the next state for all possible actions, it requires a complete  and accurate model of the MDP in the form of state-transition probabilities and ex-  pected transition rewards. This is also true for the use of asynchronous VI in RTDP.  Therefore, when state-transition probabilities and expected transition rewards are  not completely known, asynchronous VI is not directly applicable. Problems such  as these, which are called MDPs with incomplete information, x require more com-  plex adaptive algorithms for their solution. An indirect adaptive method works by  identifying the underlying MDP via estimates of state transition probabilities and  expected transition rewards, whereas a direct adaptive method (e.g., Q-Learning  (Watkins, 1989)) adapts the policy or the value function without forming an ex-  plicit model of the MDP through system identification.  In this paper, we prove a convergence theorem for a set of algorithms we call indirect  adaptive asynchronous VI algorithms. These are indirect adaptive algorithms that  result from simply substituting current estimates of transition probabilities and ex-  pected transition rewards (produced by some concurrently executing identification  These problems should not be confused with MDPs with incomplete ,tate information,  i.e., partially observable MDPs.  Convergence of Indirect Adaptive Asynchronous Value Iteration Algorithms 697  algorithm) for their actual values in the asynchronous value iteration computation.  We show that under certain conditions, indirect adaptive asynchronous VI algo-  rithms converge with probability one to the optimal value function. Moreover, we  use our result to infer convergence of two existing DP-based reinforcement learning  algorithms, adaptive real-time dynamic programming (ARTDP) (Barto, Bradtke,  & Singh, 1993), and prioritized sweeping (Moore & Atkeson, 1993).  3  CONVERGENCE OF INDIRECT ADAPTIVE  ASYNCHRONOUS VI  Indirect adaptive asynchronous VI algorithms are produced from non-adaptive algo-  rithms by substituting a current approximate model of the MDP for the true model  in the asynchronous value iteration computations. An indirect adaptive algorithm  can be expected to converge only if the corresponding non-adaptive algorithm, with  the true model used in the place of each approximate model, converges. We therefore  restrict attention to indirect adaptive asynchronous VI algorithms that correspond  in this way to convergent non-adaptive algorithms. We prove the following theorem:  Theorem 1 For any finite state, finite action MDP with an infinite-horizon dis-  counted performance measure, any indirect adaptive asynchronous VI algorithm (for  which the corresponding non-adaptive algorithm converges) converges to the optimal  value function with probability one if  1) the conditions for convergence of the non-adaptive algorithm are met,  ) in the limit, every action is executed from every state infinitely often, and  3) the estimates of the state-transition probabilities and the expected transition re-  wards remain bounded and converge in the limit to their true values with probability  one.  Proof The proof is given in Appendix A.2.  4 DISCUSSION  Condition 2 of the theorem, which is also required by direct adaptive methods  to ensure convergence, is usually unavoidable. It is typically ensured by using a  stochastic policy. For example, we can use the Gibbs distribution method for se-  lecting actions used by Watkins (1989) and others. Given condition 2, condition 3 is  easily satisfied by most identification methods. In particular, the simple maximum-  likelihood identification method (see Appendix A.1, items 6 and 7) converges to the  true model with probability one under this condition.  Our result is valid only for the special case in which the value function is explicitly  stored in a look-up table. The case in which general function approximators such  as neural networks are used requires further analysis.  Finally, an important issue not addressed in this paper is the trade-off between  system identification and control. To ensure convergence of the model, all actions  have to be executed infinitely often in every state. On the other hand, on-line control  objectives are best served by executing the action in each state that is optimal  according to the current value function (i.e., by using the certainty equivalence  698 Gullapalli and Barto  optimal policy). This issue has received considerable attention from control theorists  (see, for example, (Kumar, 1985), and the references therein). Although we do not  address this issue in this paper, for a specific estimation method, it may be possible  to determine an action selection scheme that makes the best trade-off between  identification and control.  5  EXAMPLES OF INDIRECT ADAPTIVE  ASYNCHRONOUS VI  One example of an indirect adaptive asynchronous VI algorithm is ARTDP (Barto,  Bradtke, & Singh, 1993) with maximum-likelihood identification. In this algorithm,  a randomized policy is used to ensure that every action has a non-zero probability  of being executed in each state. The following theorem for ARDTP follows directly  from our result and the corresponding theorem for RTDP in (Barto, Bradtke, &  Singh, 1993):  Theorem 2 For any discounted MDP and any initial value function, trial-based 2  ARTDP converges with probability one.  As a special case of the above theorem, we can obtain the result that in similar prob-  lems the prioritized sweeping algorithm of Moore and Atkeson (Moore & Atkeson,  1993) converges to the optimal value function. This is because prioritized sweeping  is a special case of ARTDP in which states are selected for value updates based  on their priority and the processing time available. A state's priority reflects the  utility of performing an update for that state, and hence prioritized sweeping can  improve the efficiency of asynchronous VI. A similar algorithm, Queue-Dyna (Peng  & Williams, 1992), can also be shown to converge to the optimal value function  using a simple extension of our result.  CONCLUSIONS  We have shown convergence of indirect adaptive asynchronous value iteration un-  der fairly general conditions. This result implies the convergence of several existing  DP-based reinforcement learning algorithms. Moreover, we have discussed possi-  ble extensions to our result. Our result is a step toward a better understanding  of indirect adaptive DP-based reinforcement learning methods. There are several  promising directions for future work.  One is to analyze the trade-off between model estimation and control mentioned  earlier to determine optimal methods for action selection and to integrate our work  with existing results on adaptive methods for MDPs (Kumar, 1985). Second, anal-  ysis is needed for the case in which a function approximation method, such as a  neural network, is used instead of a look-up table to store the value function. A  third possible direction is to analyze indirect adaptive versions of more general DP-  based algorithms that combine asynchronous policy iteration with asynchronous  2As in (Barto, Bradtke, & Singh, 1993), by trial-ba, ed execution of an algorithm we  mean its use in an infinite series of trials such that every state is selected infinitely often  to be the start state of a trial.  Convergence of Indirect Adaptive Asynchronous Value Iteration Algorithms 699  policy evaluation. Several non-adaptive algorithms of this nature have been pro-  posed recently (e.g., (Williams & Baird, 1993; Singh & Gullapalli)).  Finally, it will be useful to examine the relative efficacies of direct and indirect  adaptive methods for solving MDPs with incomplete information. Although the  emphasis of researchers studying DP-based reinforcement learning has been on di-  rect adaptive methods such as Q-Learning and methods using TD algorithms, it is  not clear that these direct methods are preferable in practice to indirect methods  such as the ones discussed here. For example, Moore and Atkeson (1993) report sev-  eral experiments in which prioritized sweeping significantly outperforms Q-learning  in terms of the computation time and the number of observations required for  convergence. More research is needed to characterize circumstances for which the  various reinforcement learning methods are best suited.  APPENDIX  A.1 NOTATION  1. Time steps are denoted t: 1, 2,..., and zt denotes the last state observed  before time t. zt belongs to a finite state set S -- {1, 2,...,  2. Actions in a state are selected according to a policy 7r, where 7r(i) E A, a  finite set of actions, for 1 _< i _< n.  3. The probability of making a transition from state i to state j on executing  action a is pa(i, j).  4. The expected reward from executing action a in state i is r(i, a). The  reward received at time t is denoted rt(zt, at).  5. 0 _< 'y < 1 is the discount factor.  6. Let p(i,j) denote the estimate at time t of the probability of transition  from state i to j on executing action a E A. Several different methods can be  used for estimating i5'(i, 5)- For example, if n'(i, j) is the observed number  of times before time step t that execution of action a when the system was  in state i was followed by a transition to state j, and r,(i) = YjEs r,(i,j)  is the number of times action a was executed in state i before time step  t, then, for I < i < r, and for all a G A, the maximum-likelihood state-  transition probability estimates at time t are  n(i) ' l_< j <_n.  Note that the maximum-likelihood estimates converge to their true values  with probability one if r,(i) -- o as t -- o, i.e., every action is executed  from every state infinitely often.  Let pa(i) = (i, 1),...,p(i, n)] G [0, 1] n, and similarly,/5(i) = 9(i, 1),  ...,p(i,r*)]  [0,1] . We will denote the ISI x IA[ matrix of transition  probabilities associated with state i by P(i) and its estimate at time t by  Pt(i). Finally, P denotes the vector of matrices [P(1),...,P(r,)], and Pt  denotes the vector [Pt(l),..., Pt(r*)].  700 Gullapalli and Barto  7. Let t(i,a) denote the estimate at time t of the epected reward v(i,a),  and let t denote all the ISI x IAI estimates at time t. Again, if maximum-  likelihood estimation is used,  '  where Ii,: S x A -- {0, 1) is the indicator function for the state-action pair  i a.  8. Vt* denotes the optimal value function for the MDP defined by the estimates  Pt and t of P and r at time t. Thus, Vi  $,  = + v  j$  Similarly, V* denotes the optimal value function for the MDP defined by  P and r.  9. Bt C_ $ is the subset of states whose values are updated at time t. Usually,  at least  A.2 PROOF OF THEOREM I  In indirect adaptive asynchronous VI algorithms, the estimates of the MDP param-  eters at time step t, Pt and t, are used in place of the true parameters, P and r, in  the asynchronous VI computations at time t. Hence the value function is updated  at time t as  { max,e.,i{t(i,a) +'y.esl(i,j)Vt(j)} if i  Bt  Vt+ (i) - Vt(i) otherwise,  where B(t) C_ S is the subset of states whose values are updated at time t.  First note that because Pt and t are assumed to be bounded for all t, Vt is also  bounded for all t. Next, because the optimal value function given the model Pt and  t, Vt*, is a continuous function of the estimates Pt and t, convergence of these  estimates w.p. 1 to their true values implies that  where V* is the optimal value function for the original MDP. The convergence w.p.  1 of l/t* to V* implies that given an e > 0 there exists an integer T > 0 such that  for all t _> T,  (1 -if)  IIV?- v'11 < w.p. 1. (1)  Here, II' II can be any norm on R', although we will use the oo or max norm.  In algorithms based on asynchronous VI, the values of only the states in Bt C_ S are  updated at time t, although the value of each state is updated infinitely often. For  an arbitrary z  S, let us define the infinite subsequence {t,)=o to be the times  when the value of state z gets updated. Further, let us only consider updates at,  or after, time T, where T is from equation (1) above, so that t >_ T for all z G S.  Convergence of Indirect Adaptive Asynchronous Value Iteration Algorithms 701  By the nature of the VI computation we have, for each t >_ 1,  IV,+(i)- V/(i)l _< 'll v, - V/I I if i e B,.  Using inequality (2), we can get a bound for IVtg+(z)- V(z)l as  (2)  (3)  We can verify that the bound in (3) is correct through induction. The bound is  clearly valid for k = 0. Assuming it is valid for k, we show that it is valid for k + 1:  - '+=11v,, - v,11 q- (1 -q,a+)e.  Taking the limit as k -- oo in equation (3) and observing that for each  limk-,oo V (): V'() w.p. 1, we obtain  lira IV,:+()- v'()l <  w.p. 1.  k-+oo  Since e and a axe arbitrary, this implies that Vt -- V* w.p. 1.  Acknowledgement s  We gratefully acknowledge the significant contribution of Peter Dayan, who pointed  out that a restrictive condition for convergence in an earlier version of our result  was actually unnecessary. This work has also benefited from several discussions  with Satinder Singh. We would also like to thank Chuck Anderson for his timely  help in preparing this material for presentation at the conference. This material  is based upon work supported by funding provided to A. Barto by the AFOSR,  Bolling AFB, under Grant AFOSR-F49620-93-1-0269 and by the NSF under Grant  ECS-92-14866.  References  [1] A.G. Barto, S.J. Bradtke, and S.P. Singh. Learning to act using real-time  dynamic programming. Technical Report 93-02, University of Massachusetts,  Amherst, MA, 1993.  [2] D. P. Bertsekas and J. N. Tsitsiklis. Parallel and Distributed Computation:  Numerical Methods. Prentice-Hall, Englewood Cliffs, N J, 1989.  [3] P. R. Kumax. A survey of some results in stochastic adaptive control. SIAM  Journal of Control and Optimization, 23(3):329-380, May 1985.  702 Gullapalli and Barto  [9]  [10]  [4] A. W. Moore and C. G. Atkeson. Memory-based reinforcement learning: Ef-  ficient computation with prioritized sweeping. In S. J. Hanson, J. D. Cowan,  and C. L. Giles, editors, Advances in Neural Information Processing Systems  5, pages 263-270, San Mateo, CA, 1993. Morgan Kaufmann Publishers.  [5] J. Peng and R. J. Williams. Efficient learning and planning within the dyna  framework. In Proceedings of the Second International Conference on Simula-  tion of Adaptive Behavior, Honolulu, HI, 1992.  [6] S. P. Singh and V. Gullapalli. Asynchronous modified policy iteration with  single-sided updates. (Under review).  [7] R. S. Sutton. Learning to predict by the methods of temporal differences.  Machine Learning, 3:9-44, 1988.  [8] G. J. Tesauro. Practical issues in temporal difference learning. Machine Learn-  ing, 8(3/4):257-277, May 1992.  C. J. C. H. Watkins. Learning from delayed rewards. PhD thesis, Cambridge  University, Cambridge, England, 1989.  R. J. Williams and L. C. Baird. Analysis of some incremental variants of  policy iteration: First steps toward understanding actor-critic ]earning sys-  tems. TechnicM Report NU-CCS-93-11, Northeastern University, College of  Computer Science, Boston, MA 02115, September 1993.  
Learning Complex Boolean Functions:  Algorithms and Applications  Arlindo L. Oliveira and Alberto Sangiovanni-Vincentelli  Dept. of EECS  UC Berkeley  Berkeley CA 94720  Abstract  The most commonly used neural network models are not well suited  to direct digital implementations because each node needs to per-  form a large number of operations between floating point values.  Fortunately, the ability to learn from examples and to generalize is  not restricted to networks of this type. Indeed, networks where each  node implements a simple Boolean function (Boolean networks) can  be designed in such a way as to exhibit similar properties. Two  algorithms that generate Boolean networks from examples are pre-  sented. The results show that these algorithms generalize very  well in a class of problems that accept compact Boolean network  descriptions. The techniques described are general and can be ap-  plied to tasks that are not known to have that characteristic. Two  examples of applications are presented: image reconstruction and  hand-written character recognition.  I Introduction  The main objective of this research is the design of algorithms for empirical learning  that generate networks suitable for digital implementations. Although threshold  gate networks can be implemented using standard digital technologies, for many  applications this approach is expensive and inefficient. Pulse stream modulation  [Murray and Smith, 1988] is one possible approach, but is limited to a relatively  small number of neurons and becomes slow if high precision is required. Dedicated  911  912 Oliveira and Sangiovanni-Vincentelli  boards based on DSP processors can achieve very high performance and are very  flexible but may be too expensive for some applications.  The algorithms described in this paper accept as input a training set and generate  networks where each node implements a relatively simple Boolean function. Such  networks will be called Boolean networks. Many applications can benefit from  such an approach because the speed and compactness of digital implementations  is still unmatched by its analog counterparts. Additionally, many alternatives are  available to designers that want to implement Boolean networks, from full-custom  design to field programmable gate arrays. This makes the digital alternative more  cost effective than solutions based on analog designs.  Occam's razor [Blumer e! al., 1987; Rissanen, 1986] provides the theoretical founda-  tion for the development of algorithms that can be used to obtain Boolean networks  that generalize well. According to this paradigm, simpler explanations for the avail-  able data have higher predictive power. The induction problem can therefore be  posed as an optimization problem: given a labeled training set derive the  less complex Boolean network that is consistent  with the training set.  Occam's razor, however, doesn't help in the choice of the particular way of mea-  suring complexity that should be used. In general, different types of problems may  require different complexity measures. The algorithms described in section 3.1 and  3.2 are greedy algorithms that aim at minimizing one specific complexity measure:  the size of the overall network. Although this particular way of measuring com-  plexity may prove inappropriate in some cases, we believe the approach proposed  can be generalized and used with minor modifications in many other tasks. The  problem of finding the smallest Boolean network consistent with the training set is  NP-hard [Garey and Johnson, 1979] and cannot be solved exactly in most cases.  Heuristic approaches like the ones described are therefore required.  2 Definitions  We consider the problem of supervised learning in an attribute based description  language. The attributes (input variables) are assumed to be Boolean and every  exemplar in the training set is labeled with a value that describes its class. Both  algorithms try to maximize the mutual information between the network output  and these labels.  Let variable X take the values {z, z2, ...z,} with probabilities p(z),p(z2)...p(z,).  The entropy of X is given by H(X) - -j p(cj)logp(cj) and is a measure  of the uncertainty about the value of X. The uncertainty about the value  of X when the value of another variable Y is known is given by H(XIY ) -  -- i P(Yi ) Ej P( Xj lYi ) log p( xj lYi ).  The amount by which the uncertainty of X is reduced when the value of variable Y  is known, I(Y,X) - H(X) - H(XIY) is called the mutual information between Y  and X. In this context, Y will be a variable defined by the output of one or more  nodes in the network and X will be the target value specified in the training set.  1Up to sortie specified level.  Learning Complex Boolean Functions: Algorithms and Applications 913  3 Algorithms  3.1 Muesli - An algorithm for the design of nmlti-level logic networks  This algorithm derives the Boolean network by performing gradient descent in the  mutual information between a set of nodes and the target values specified by the  labels in the training set.  In the pseudo code description of the algorithm given in figure 1, the function  computes the mutual information between the nodes in S (viewed as a multi-valued  variable) and the target output.  muesli(ntist) {  nlist - sort_nlist_by_T(nlist,1);  sup *- 2;  while (not_done(Mist) ^ sup < max_sup) {  act +- 0;  do {  act q- q- ;  success +- improve_mi(act, nlist, sup);  ) while (success -- FALSE A act  max_act);  if (success - TRUE) {  sup *- 2;  while (success = TRUE)  success +- improve_mi(act, nlist, sup);  else sup q- q- ;  improve_mi(act, nlist, sup) {  nlist +- sort_nlist_by_27(nlist, act);  f - best_function(nlist, act, sup);  if (I(nlist[l:act-1] t_J f) > I(nlist[l:act]))  nlist .- nlist t_J f;  return(TRUE);  else return(FALSE);  Figure 1: Pseudo-code for the Muesli algorithm.  The algorithm works by keeping a list of candidate nodes, nlist, that initially con-  tains only the primary inputs. The act variable selects which node in nlist is active.  Initially, act is set to 1 and the node that provides more information about the out-  put is selected as the active node. Function improve_mi() tries to combine the  active node with other nodes as to increase the mutual information.  Except for very simple functions, a point will be reached where no further improve-  914 Oliveira and Sangiovanni-Vincentelli  merits can be made for the single most informative node. The value of act is then  increased (up to a pre-specified maximum) and improve_rot is again called to select  auxiliary features using other nodes in nlist as the active node. If this fails, the  value of sup (size of the support of each selected function) is increased until no  further improvements are possible or the target is reached.  The function sort_nlist_by_T(nlist, act) sorts the first act nodes in the list by de-  creasing value of the information they provide about the labels. More explicitly, the  first node in the sorted list is the one that provides maximal information about the  labels. The second node is the one that will provide more additional information  after the first has been selected and so on.  Function improve_rot() calls best_function(nlist, act,sup) to select the Boolean  function f that takes as inputs node nlist[act] plus sup-1 other nodes and maximizes  I(nlist[1: act- 1] U f). When sup is larger than 2 it is unfeasible to search all 22p  possible functions to select the desired one. However, given sup input variables,  finding such a function is equivalent to selecting a partition 2 of the 2 sup points in  the input space that maximizes a specific cost function. This partition is found using  the Kernighan-Lin algorithm [Kernighan and Lin, 1970] for graph-partitioning.  Figure 2 exemplifies how the algorithm works when learning the simple Boolean  function f - ab + cde from a complete training set. In this example, the value of  sup is always at 2. Therefore, only 2 input Boolean functions arc generated.  mi([ ]) = 0.0  Fails to find fix,?) with mi([f]) > 0.52  Select x = ab Set act = 2;  a a [ a  x  nlist = [a,b,c,d,e] nlist = [x,c,d,e,a,b] nlist = [x,c,d,e,a,b]  act = 1 act = 1 act = 2  mi([a]) = 0.16 mi([x]) = 0.52 mi([x,c]) = 0.63  Select y = cd  a  nlist = [x,y,e,a,b,c,d]  act = 2  mi([x,y]) = 0.74  Select w = ye  a   d  d  nlist = [x,y,e,a,b,c,d]  act = 2  mi([x,w]) = 0.93  Fails to find f(w,?) with mi([x,f]) > 0.93  Set act = 0; Select z = x+w  a  z  nlist = [z,x,y,a,b,c,d,e]  act = 1  mi([z]) = 0.93  Figure 2: The muesli algorithm, illustrated  2A single output Boolean function is equivalent to a partition of the input space in two  sets.  Learning Complex Boolean Functions: Algorithms and Applications 915  3.2 Fulfringe - a network generation algorithm based on decision trees  This algorithm uses binary decision trees [Quinlan, 1986] as the basic underlying  representation. A binary decision tree is a rooted, directed, acyclic graph, where  each terminal node (a node witl no outgoing edges) is labeled with one of the  possible output labels and each non-terminal node has exactly two outgoing edges  labeled 0 and 1. Each non-terminal node is also labeled with the name of the  attribute that is tested at that node. A decision tree can be used to classify a  particular example by starting at the root node and taking, until a terminal is  reached, the edge labeled with the value of the attribute tested at the current node.  Decision trees are usually built in a greedy way. At each step, the algorithm greedily  selects the attribute to be tested as the one that provides maximal informa6on about  the label of the examples that reached that node in the decision tree. It then recurs  after splitting these examples according to the value of the tested attribute.  Fulltinge works by identifying patterns near the fringes of the decision tree and  using them to build new features. The idea was first proposed in [Pagallo and  Haussler, 1990].  p&~g ~p&~g P&g ~p&g  p+g ~p+g p-g  4- 4- + + + 4- 4-  Figure 3: Fringe patterns identified by fulfringe  Figure 3 shows the patterns that fulfringe identifies. Dcfringe, proposed in [Yang  el al., 1991], identifies the patterns shown in the first two rows. These patterns  correspond to 8 Boolean functions of 2 variables. Since there are only 10 distinct  Boolean functions that depend on two va.riables 3, it is natural to add the patterns  in the third row and identify all possible functions of 2 variables. As in ticfringe  and fringe, these new composite features are added (if they have not yet been  generated) to the list of available features and a new decision tree is built. The  3The remaining 6 functions of 2 variables depend on only one or none of the variables.  916 Oliveira and Sangiovanni-Vincentelli  process is iterated until a decision tree with ouly one decisiou node is built. The  attribute tested at this node is a complex feature and can be viewed as the output  of a Boolean network that matches the training set data.  3.3 Encoding nmltivalued outputs  Both muesli and fulfringe generate Boolean networks with a single binary valued  output. When the target label can have more than 2 values, some encoding must be  used. The prefered solution is to encode the outputs using an error correcting code  [Dietterich and Bakiri, 1991]. This approach preserves most of the compactness of  a digital encoding while beeing much less sensitive to errors in one of the output  variables. Additionally, the Hamming distance between an observed output and the  closest valid codeword gives a measure of the certainty of the classification. This  can be used to our advantage in problems where a failure to classify is less serious  than the output of a wrong classification.  4 Performance evaluation  To evaluate the algorithms, we selected a set of 11 functions of variable complexity.  A complete description of these finctions can be found in [Oliveira, 1994]. The first  6 functions were proposed as test cases in [Pagallo and Haussler, 1990] and accept  compact disjoint normal form descriptions. The remaining ones accept compact  multi-level representations but have large two level descriptions. The algorithms  described in sections 3.1 and 3.2 were compared with the cascade-correlation algo-  rithm [Fahlman and Lebiere, 1990] and a standard decision tree algorithm analog  to ID3 [Quinlan, 1986]. As in [Pagallo and Haussler, 1990], the nmnber of examples  1  in the training set was selected to be equal to 7 times the description length of the  function under a fixed encoding scheme, where e was set equal to 0.1. For each  function, 5 training sets were randomly selected. The average accuracy for the 5  runs in an independent set of 4000 examples is listed in table 1.  Table 1: Accuracy of the four algorithms.  Function # inputs # examples Accuracy  muesli fulfringe ID3 CasCor  dnfl 80 3292 99.91 99.98 82.09 75.38  dnf2 40 2185 99.28 98.89 88.84 73.11  dnf3 32 1650 99.94 100.00 89.98 79.19  dnf4 64 2640 100.00 100.00 72.61 58.41  xor4_16 16 1200 98.35 100.00 75.20 99.91  xor5_32 32 4000 60.16 100.00 51.41 99.97  sm12 12 1540 99.90 100.00 99.81 98.98  sm18 18 2720 100.00 99.92 91.48 91.30  str18 18 2720 100.00 100.00 94.55 92.57  str27 27 4160 98.64 99.35 94.24 93.90  carry8 16 2017 99.50 98.71 96.70 99.22  Average 95.97 99.71 85.35 87.45  The results show that the performance of muesli and fulltinge is consistently su-  Learning Complex Boolean Functions: Algorithms and Applications 917  perior to the other two algorithms. Muesli performs poorly in examples that have  many xor functions, duc the greedy nature of the algorithm. In particular, muesli  failed to find a solution in the alloted time for 4 of the 5 runs of or5_32 and found  the exact solution in only one of the runs.  ID3 was the fastest of the algorithms and C, ascadc-Corrclation the slowest. Fulfringe  and muesli exhibited similar running times for these tasks. Wc observed, however,  that for larger problems the runtime for fulfring becomes prohibitively high and  musli is comparatively much faster.  5 Applications  To evaluate the techniques described in real problems, experiments were performed  in two domains: noisy image reconstruction and handwritten character recognition.  The main objective was to investigate whether the approach is applicable to prob-  lems that are not known to accept a compact Boolean network representation. The  outputs were encoded using a 15 bit Hadamard error correcting code.  5.1 hnage reconstruction  The speed required by applications in image processing makes it a very interesting  field for this type of approach. In this experiment, 16 level gray scale images wcrc  corrupted by random noise by switching each bit with 5% probability. Samples of  this image wcrc used to train a network in the reconstruction of the original image.  The training set consisted of 5x5 pixel regions of corrupted images (100 binary  variables per sample) labeled with the value of the center pixel. Figure 4 shows a  detail of the reconstruction performed in an independent test image by the network  obtained using fulfringe.  Original inege  Corrupted imge  Reconstructed iage  Figure 4: Image reconstruction experiment  5.2 Handwritten character recognition  The NIST database of handwritten characters was used for this task. Individually  segmented digits were normalized to a 16 by 16 binary grid. A set of 53629 digits  was used for training and the resulting network was tested in a different set of 52467  918 Oliveira and Sangiovanni-Vincentelli  digits. Training was performed using muesli. The algorithm was stopped after a pre-  specified time (48 hours on a DECstation 5000/260) ellapsed. The resulting network  was placed and routed using the TimberWolf [Sechen and Sangiovanni-Vincentelli,  1986] package and occupies an area of 78.8 sq. ram. using 0.8/ technology.  The accuracy on the test set was 93.9%. This value compares well with the per-  formance obtained by alternative approaches that use a similarly sized training set  and little domain knowledge, but fails short of the best results published so far.  Ongoing research on this problem is concentrated on the use of domain knowledge  to restrict the search for compact networks and speed up the training.  A cknowledgemen[ s  This work was supported by Joint Services Electronics Program grant F49620-93-C-0014.  References  [Blumer et al., 1987] A. Blumer, A. Ehrenfeucht, D. Haussler, and M. Warmuth. Occam's  razor. Information Processing Letters, 24:377-380, 1987.  [Dietterich and Bakiri, 1991] T. G. Dietterich and G. Bakiri. Error-correcting output  codes: A general method for improving multiclass inductive learning programs. In Pro-  ceedings of the Ninth National Conference on Artificial Intelligence (AAAL91), pages  572-577. AAAI Press, 1991.  [Fahlman and Lebiere, 1990] S.E. Fahlman and C. Lebiere. The cascade-correlation learn-  ing architecture. In D.S. Touretzky, editor, Advances in Neural Information Processing  Systems, volume 2, pages 524-532, San Mateo, 1990. Morgan Kaufmann.  [Garey and Johnson, 1979] M.R. Garey and D.S. Johnson. Computers and Intractability:  A Guide to the Theory of NP-Completeness. Freeman, New York, 1979.  [Kernighan and Lin, 1970] B. W. Kernighan and S. Lin. An efficient heuristic procedure  for partitioning graphs. The Bell System Technical Journal, pages 291-307, February  1970.  [Murray and Smith, 1988] Alan F. Murray and Anthony V. W. Smith. Asynchronous vlsi  neural networks using pulse-stream arithmetic. IEEE Journal of Solid-State Circuits,  23:3:688-697, 1988.  [Oliveira, 1994] Arlindo L. Oliveira. Inductive Learning by Selection of Minimal Repre-  sentations. PhD thesis, UC Berkeley, 1994. In preparation.  [Pagallo and Haussler, 1990] G. PagMlo and D. Haussler. Boolean feature discovery in  empirical learning. Machine Learning, 1, 1990.  [Quinlan, 1986] J. R. Quinlan. Induction of decision trees. Machine Learning, 1:81-106,  1986.  [Rissanen, 1986] J. Rissanen. Stochastic complexity and modeling. Annals of Statistics,  14:1080-1100, 1986.  [Sechen and Sangiovanni-Vincentelli, 1986]  Carl Sechen and Alberto Sangiovanni-Vincentelli. TimberWolf3.2: A new standard cell  placement and global routing package. In Proceedings of the 23rd Design Automation  Uonference, pages 432-439, 1986.  [Yang et al., 1991] D. S. Yang, L. Rendell, and G. Blix. Fringe-like feature construction:  A comparative study and a unifying scheme. In Proceedings of the Eight International  Uonference in Machine Learning, pages 223-227, San Mateo, 1991. Morgan Kaufmann.  
Efficient Simulation of Biological Neural  Networks on Massively Parallel  Supercomputers with Hypercube  Architecture  Ernst Niebur  Computation and Neural Systems  California Institute of Technology  Pasadena, CA 91125, USA  Dean Brettle  Booz, Allen and Hamilton, Inc.  8283 Greensboro Drive  McLean, VA 22102-3838, USA  Abstract  We present a neural network simulation which we implemented  on the massively parallel Connection Machine 2. In contrast to  previous work, this simulator is based on biologically realistic neu-  rons with nontrivial single-cell dynamics, high connectivity with a  structure modelled in agreement with biological data, and preser-  vation of the temporal dynamics of spike interactions. We simulate  neural networks of 16,384 neurons coupled by about 1000 synapses  per neuron, and estimate the performance for much larger systems.  Communication between neurons is identified as the computation-  ally most demanding task and we present a novel method to over-  come this bottleneck. The simulator has already been used to study  the primary visual system of the cat.  I INTRODUCTION  Neural networks have been implemented previously on massively parallel supercom-  puters (Fujimoto et al., 1992, Zhang et al., 1990). However, these are implemen-  tations of artificial, highly simplified neural networks, while our aim was explicitly  to provide a simulator for biologically realistic neural networks. There is also at  least one implementation of biologically realistic neuronal systems on a moderately  904  Efficient Simulation of Biological Neural Networks 905  parallel but powerful machine (De Schutter and Bower, 1992), but the complexity  of the used neuron model makes simulation of larger numbers of neurons impracti-  cal. Our interest here is to provide an efficient simulator of large neural networks  of cortex and related subcortical structures.  The most important characteristics of the neuronal systems we want to simulate  are the following:   Cells are highly interconnected (several thousand connections per cell) but  far from fully interconnected.   Connections do not follow simple deterministic rules (like, e.g., nearest  neighbor connections).   Cells communicate with each other via delayed spikes which are binary  events ("all-or-nothing").   Such communication events are short (1 ms) and infrequent (1 to 100 per  second).   The temporal fine structure of the spike trains may be an important  information carrier (Kreiter and Singer, 1992, Richmond and Optican,  1990, Softky and Koch, 1993).  2 IMPLEMENTATION  The biological network was modelled as a set of improved integrate-and-fire neurons  which communicate with each other via delayed impulses (spikes). The single-cell  model and details of the connectivity have been described in refs. (Wehmeier et al.,  1989, WSrgStter et al., 1991).  Despite the rare occurrence of action potentials, their processing accounts for the  major workload of the machine. The efficient implementation of inter-neuron com-  munication is therefore the decisive factor which determines the efficacy of the sim-  ulator implementation. By "spike propagation" we denote the process by which a  neuron communicates the occurrence of an action potential to all its postsynaptic  partners. While the most efficient computation of the neuronal equations is ob-  tained by mapping each neuron on one processor, this is very inefficient for spike  propagation. This is due to the fact that spikes are rare events and that in the SIMD  architecture used, each processor has to wait for the completion of the current tasks  of all other processors. Therefore, only very few processors are active at any given  time step. A more efficient data representation than provided by this "direct" algo-  rithm is shown in Fig. 1. In this "transposed" scheme, a processor changes its role  from simulating one of the neurons to simulating one synapse, which is, in general,  not a synapse of the neuron simulated by the processor (see legend of Fig. 1). At  any given time step, the addresses of the processors representing spiking neurons are  broadcast along binary trees which are implemented efficiently (in time complexity  log2 for M processors) in a hypercube architecture such as the CM-2. We obtain  further computational efficiency by dividing the processor array into "partitions" of  size M and by implementing partially parallel I/O scheduling (both not discussed  here).  906 Niebur and Brettle  I 2 3 4 5  1,1 1,2 1,3 1,4   2,1 2,2 2,3 2,4  3,1 3,2 3,3 3,4  i,1 i,2 i,3 i,4  M,1 M,2 M,3 M,4  1,i   2,i  3,i  i,i  M,i  M-1 M  1,M  2,M   3,M  M,M  Figure 1: Transposed storage method for connections. The storage space for each  of the N processors is represented by a vertical column. A small part of this space  is used for the time-dependent variables describing each of the N neurons (upper  part of each column, "Cell data"). The main part of the storage is used for datasets  consisting of the addresses, weights and delays of the synapses ("Synapse data"),  represented by the indices i, j in the figure. For instance, "1, 1" stands for the first  synapse of neuron 1, "1, 2" for the second synapse of this neuron and so on. Note  that the storage space of processor i does not hold the synapses of neuron i. If  neuron i generates a spike, all M processors are used for propagating the spike  (black arrows)  Efficient Simulation of Biological Neural Networks 907  3 PERFORMANCE ANALYSIS  In order to accurately compare the performance of the described spike propagation  algorithms, we implemented both the direct algorithm and the transposed algorithm  and compared their performances with analytical estimates.  O. 1 -+'  O.Ol ___ - -  o. o o 1 .+___  0.0001 0.001 0.01 0.1 1  p  Figure 2: Execution time for the direct algorithm (diamonds) and the transposed  algorithm (crosses) as function of the spiking probability p for each cell. If all cells  fire at each time step, there is no advantage for the transposed algorithm; in fact,  it is at a disadvantage due to the overhead discussed in the text. Therefore, the  two curves cross at a value just below p - 1. As expected, the largest difference  between them is found for the smallest values of p.  Figure 2 compares the time required for the direct algorithm to the time required  for the transposed algorithm as a function of p, the average number of spikes per  neuron per time step. Note that while the time required rises much more rapidly for  the transposed algorithm than the direct algorithm, it takes significantly less time  for p < 0.5. The peak speedup was a factor of 454 which occurred at p - 0.00012  (or 1.2 impulses per second at a timestep of 0.1ms, corresponding approximately to  spontaneous spike rates). The absolutely highest possible speedup, obtained if there  is exactly one spike in every partition at every time step, is equal to M (M - 1024  in this simulation). The average speedup is determined by the maximal number of  spiking neurons per time step in any partition, since the processors in all partitions  have to wait until the last partition has propagated all of its spikes. The average  maximal number of spikes in a system of N partitions, each one consisting of M  908 Niebur and Brettle  neurons is  M N  k=O m:l  where p is the spiking probability of one cell, H(k) is the probability that a given  partition has k spikes and  k-1  n(i)  i=0  (2)  1000  100  10  1  0.0001 0.001 0.01 0.1 1  p  Figure 3: Speedup of the transposed algorithm over the direct algorithm as a func-  tion of p for different VP ratios; M = 1024. The ideal speedup (uppermost curve;  diamonds), computed in eq. 3 essentially determines the observed speedup. (lower  curves; "+" signs: VP-ratio=l, diamonds: VP-raio=2, crosses: VP-ratio=4.). The  difference between the ideal and the effectively obtained speedup is due to commu-  nication and other overhead of the transposed algorithm. Note that the difference  in speedup for different VP ratios (difference between lower curves) is relatively  small, which shows that the penalty for using larger neuron numbers is not large.  As expected, the speedup approaches unity for p m 1 in all cases.  It can be shown that for independent neurons and for low spike rates, II(k) is the  Poisson distribution and I(k) the incomplete F function. The average maximal  Efficient Simulation of Biological Neural Networks 909  number of spikes for M - 1024 and different values of p (eq. 1) can be shown to be  a mildly growing function of the number of partitions which shows that the perfor-  mance will not be limited crucially by changing the number of partitions. Therefore,  the algorithm scales well with increasing network size and the performance-limiting  factor is the activity level in the network and not the size of the network. This is  also evident in Fig. 3 which shows the effectively obtained speedup compared to the  ideal speedup, which would be obtained if the transposed algorithm were limited  only by eq. 1 and would not require any additional communication or other over-  head. Using N,,:(p, M, N) from eq. 1 it is clear that this ideal speedup is given  by  M  (3)  The difference between theory and experiment can be attributed to the time re-  quired for the spread operation and other additional overhead associated with the  transposed algorithm. At P = 0.0010 (or 10 ips) the obtained speedup is a factor  of 106.  4 VERY LARGE SYSTEMS  Using the full local memory of the machine and the "Virtual Processor" capabil-  ity of the CM-2, the maximal number of neurons that can be simulated without  any change of algorithm is as high as 4,194,304 ("4M"). Figure 3 shows that the  speedup is reduced only slightly as the number of neurons increases, when the addi-  tional neurons are simulated by virtual processors. The performance is essentially  limited by the mean network activity, whose effect is expressed by eq. 3, and the  additional overhead originating from the higher "VP ratio" is small. This corrob-  orates our earlier conclusion that the algorithm scales well with the size of the  simulated system. Although we did not study the scaling of execution time with  the size of the simulated system for more than 16,384 real processors, we expect the  total execution time to be basically independent of the number of neurons, as long  as additional neurons are distributed on additional processors.  Acknowlegdements  We thank U. Wehmeier and F. WSrgStter who provided us with the code for gen-  erating the connections, and G. Holt for his retina simulator. Discussions with C.  Koch and F. WSrgStter were very helpful. We would like to thank C. Koch for  his continuing support and for providing a stimulating research atmosphere. We  also acknowledge the Advanced Computing Laboratory of Los Alamos National  Laboratory, Los Alamos, NM 87545. Some of the numerical work was performed  on computing resources located at this facility. This work was supported by the  National Science Foundation, the Office of Naval Research, and the Air Force Office  of Scientific Research.  910 Niebur and Brettle  References  De Schutter E. and Bower J.M. (1992). Purk.nje cell simulation on the Intel Touch-  stone Delta with GENESIS. In Mihaly T. and Messina P., editors, Proceedings  of the Grand Challenge Computing Fair, pages 268-279. CCSF Publications,  Caltech, Pasadena CA.  Fujimoto Y., Fukuda N., and Akabane T. (1992). Massively parallel architectures for  large scale neural network simulations. IEEE Transactions on Neural Networks,  3(6):876-888.  Kreiter A.K. and Singer W. (1992). Oscillatory neuronal responses in the visual-  cortex of the awake macaque monkey. Europ. J. Neurosci., 4(4):369-375.  Richmond B.J. and Optican L.M. (1990). Temporal encoding of two-dimensional  patterns by single units in primate primary visual cortex. II: Information trans-  mission. J. Neurophysiol., 64:370-380.  Softky W. and Koch C. (1993). The highly irregular firing of cortical-cells is incon-  sistent with temporal integration of random epsps. J. Neurosci., 13(1):334-350.  Webmeier U., Dong D., Koch C., and van Essen D. (1989). Modeling the visual  system. In Koch C. and Segev I., editors, Methods in Neuronal Modeling, pages  335-359. MIT Press, Cambridge, MA.  WSrgStter F., Niebur E., and Koch C. (1991). Isotropic connections generate  functional asymmetrical behavior in visual cortical cells. J. Neurophysiol.,  66(2):444-459.  Zhang X., Mckenna M., Mesirov J., and Waltz D. (1990). An efficient implemen-  tation of the back-propagation algorithm on the Connection Machine CM-2.  In Touretzky D.S., editor, Neural Information Processing Systems 2, pages  801-809. Morgan-Kaufmann, San Mateo, CA.  
Coupled Dynamics of Fast Neurons and  Slow Interactions  A.C.C. Coolen R.W. Penney D. Sherrington  Dept. of Physics - Theoretical Physics  University of Oxford  i Keble Road, Oxford OX1 3NP, U.K.  Abstract  A simple model of coupled dynamics of fast neurons and slow inter-  actions, modelling self-organization in recurrent neural networks,  leads naturally to an effective statistical mechanics characterized  by a partition function which is an average over a replicated system.  This is reminiscent of the replica trick used to study spin-glasses,  but with the difference that the number of replicas has a physi-  cal meaning as the ratio of two temperatures and can be varied  throughout the whole range of real values. The model has inter-  esting phase consequences as a function of varying this ratio and  external stilnuli, and can be extended to a range of other models.  1  A SIMPLE MODEL WITH FAST DYNAMIC  NEURONS AND SLOW DYNAMIC INTERACTIONS  As the basic archetypal model we consider a system of Ising spin neurons rri C  {-1, 1}, i C {1,..., N}, interacting via continuous-valued symmetric interactions,  Jij, which themselves evolve in response to the states of the neurons. The neurons  are taken to have a stochastic field-alignment dynamics which is fast compared with  the evolution rate of the interactions Jij, such that on the time-scale of Jij-dynamics  the neurons are effectively in equilibrium according to a Boltzmann distribution,  P{.,} ({o'i}) cr exp [-H{.,} ({o'i})]  (1)  447  448 Coolen, Penney, and Sherrington  where  - (2)  <j  and the subscript {Ji} indicates that the {Jj} are to be considered as quenched  variables. In practice, several specific types of dynamics which obey detailed balance  lead to the equilibrium distribution (1), such as a Markov process with single-spin  flip Glauber dynamics [1]. The quantity  is an inverse temperature characterizing  the stochastic gain.  For the Jij dynamics we choose the form  d 1 1  rJij = {aiaj}{ao} - yJij + ?ij(t) (i < j) (3)  where {..-){ao} refers to a thermodynamic average over the distribution (1) with  the effectively instantaneous {Jj}, and ?j(t) is a stochastic Gaussian white noise  of zero mean and correlation  (rlij(t)rlkt(t')} - 2r/-l S(ij),(kl)5(t - t')  The first term on the right-hand side of (3) is inspired by the Hebbian process in  neural tissue in which synaptic efficacies are believed to grow locally in response to  the simultaneous activity of pre- and post-synaptic neurons [2]. The second term  acts to limit the magnitude of Zij; / is the characteristic inverse temperature of  the interaction system. (A related interaction dynamics without the noise term,  equivalent to/ = c, was introduced by Shinomoto [3]; the anti-Hebbian version of  the above coupled dynamics was studied in layered systems by Jonker et al. [4, 5].)  Substituting for Io'io'j) in terms of the distribution (1) enables us to re-write (3) as  d c  NrJii = -c2Ji-'7/ ( {Jii)) + x/rlii(t) (4)  where the effective Hamiltonian 7/({Ji}) is given by  1 N  _1 lnZ; ({Jij}) + Y  Ji} (5)  where Z ({Jij}) is the partition function associated with (2):  ({Jij})= Tr exp .  {o'i}  2 COUPLED SYSTEM IN THERMAL EQUILIBRIUM  We now recognise (4) as having the form of a Langevin equation, so that the equilib-  rium distribution of the interaction system is given by a Boltzmann form. Hence-  forth, we concentrate on this equilibrium state which we can characterize by a  partition function  and an associated 'free energy' /:  /i< j ' [ 1~ I --/-lln2  2 --- dJij [Z({Jij})] exp -ttN Ji} 10 = (6)  <j  Coupled Dynamics of Fast Neurons and Slow Interactions 449  where n --//. We may use  as a generating functional to produce thermody-  namic averages of state variables (I> ({rri}; {Jq}) in the combined system by adding  suitable infinitesimal source terms to the neuron Itamiltonian (2):  lira  -0 0h  fl-[cidJi  -- (7)  where the bar refers to an average over the asymptotic {Jij) dynamics.  The form (6) with n - 0 is immediately reminiscent of the effective partition  function which results from the application of the replica trick to replace In Z by  lim,_0 (Z ' - 1) in dealing with a quenched average for the infinite-ranged spin-  glass [6], while n = i relates to the corresponding annealed average, although we  note that in the present model the time-scales for neuron and interaction dynamics  remain completely disparate. These observations correlate with the identification  of n with//3, which implies that n - 0 corresponds to a situation in which the  interaction dynamics is dominated by the stochastic term rli j (t), rather than by the  behaviour of the neurons, while for n - i the two characteristic temperatures are  the same. For n - oe the influence of the neurons on the interaction dynamics  dominates. In fact, any real n is possible by tuning the ratio between the two fi's.  In the formulation presented in this paper n is always non-negative, but negative  values are possible if the Itebbian rule of (3) is replaced by an anti-Itebbian form  with (rrrrj) replaced by -(rrirrj) (the case of negative n is being studied by Mzard  and co-workers [7]).  The model discussed above is range-free/infinite-ranged and can therefore be an-  alyzed in the thermodynamic limit N  oc by the replica mean-field theory as  devised for the Sherrington-Kirkpatrick spin-glass [6, 8, 9]. This can be developed  precisely for integer n [6, 8, 9, 10] and analytically continued. In the usual manner  there enters a spin-glass order parameter  where the superscripts are replica labels. qV* is given by the extremum of  F({qV*}): 2fi_rt2 y [qV*] 2 +in Tr exp  while 2 is proportional to exp [NextrF ({qV*})]. In the replica-symmetric region  (or ansatz) one assumes qV* = q.  We will first choose as the independent variables n and/ and briefly discuss the  phase picture of our model (full details can be found in [11]). The system exhibits  a transition from a paramagnetic state (q = 0) to an ordered state (q > 0) at a  critical/(n). For n _< 2 this transition is second order at/ = 1, down to the SK  450 Coolen, Penney, and Sherrington  spin-glass limit, n - 0, but for n > 2 the coupled dynamics leads to a qualitative,  as well as quantitative, change to first order. Replica symmetry is stable above a  critical value no(/3), at which there is a de Almeida-Thouless (AT) transition (c.f.  Kondor [12]). As expected from spin-glass studies, no(/3) goes to zero as /3  1  but rises for larger /7, having a maximum of order 0.3 at /3 of order 2. Thus, for  n > n(max) m 0.3 there is no instability against small replica-symmetry breaking  fluctuations, while for smaller n there is re-entrance in this stability. The transition  from a paramagnetic to an ordered state and the onset of local RS instability for  various temperatures is shown in Figure 1.  3 EXTERNAL FIELDS  Several simple modifications of the above model are possible. One consists of adding  external fields to the spin dynamics and/or to the interaction dynamics, by making  the substitutions  in (2) and (5) respectively. These external fields may be viewed as generating fields  in the sense of (7); for example  Ob:ij - Jij OKijOKkt -/3 [JijJk - Jij  For neural network models a natural first choice for the external fields would be  Oi  hi and Kij -- Kij, i  {-1, 1}, where the i are quenched random vari-  ables corresponding to an imposed pattern. Without loss of generality all the i  can be taken as +1, via the gauge transformation o'i  o'ii, Jij  Jijij. Hence-  forth we shall make this choice. The neuron perturbation field h induces a finite  'magnetization' characterized by a new order parameter  which is independent of a in the replica-symmetric assumption (which turns out  to be stable with respect to variation in this parameter). As in the case of the  spin-glass, there is now a critical surface in (h, n,/3) space characterizing the onset  of replica symmetry breaking. In introducing the interaction perturbation field K  we find that K/lu is the analogue of the mean exchange J0 in the SK spin-glass  model, f2 -- (/3nit) -t being the analogue of the variance. If large enough, this field  leads to a spontaneous 'ferromagnetic' order.  Again we find further examples of both second and first order transitions (details  can be found in [11]). For the paramagnetic (P; m = 0, q = 0) to ferromagnetic  (F; m  0, q  0) case, the transition is second order at the SK value fiJ0 = 1 so  long as (/3f)-2 _> 3n - 2. Only when (/3f)-2 < 3n - 2 do the interaction dynamics  Coupled Dynamics of Fast Neurons and Slow Interactions 451  PARAMAGNET .. .. ......... ' ......  1  0.6 MATTIB GLABB  T  0.6  0.4  0.2  SPllq  O0 1 2 3  Figure 1: Phasediagram for j = 1. Dotted line: first order transition, solid line:  second order transition. The separation between Mattis-glass and spin-glass phase  is defined by the de Almeida-Thouless instability  452 Coolen, Penney, and Sherrington  influence the transition, changing it to first order at a lower temperature. Regarding  the ferromagnetic to spin-glass (SG; m = 0, q  0) transition, this exhibits both  second order (lower Jo) and first order (higher J0) sections separated by a tricritical  point for n less than a critical value of the order of 3.3. This tricritical point exhibits  re-entrance as a function of n.  4  COMPARISON BETWEEN COUPLED DYNAMICS  AND SK MODEL  In order to clarify the differences, we will briefly summarize the two routes that  lead to an SK-type replica theory:  Coupled Dynamics:  Fast Ising spin neurons + slow dynamic interactions,  d i K  j = (j)j,j) +  - j +GWN  Free energy:  Define:  Thermodynamics:  1  /3nextr G ({qV*}; {my}) + const.  SK spin-glass:  Ising spins + fixed random interactions,  P(Jij) -= [2rJ2]-e -['J'-]/  Free energy:  Sell-averaging:  Physical scaling:  Thermodynamics:  N --- o<>:  1 1 lim 1 [Z_ 1]  f --= 3N log Z = -/3- ,-0 n  J0 = o/V, J = /v  - 1  f = - lim -extr G ({qV*}; {my}) + const.  r--+ 0  Coupled Dynamics of Fast Neurons and Slow Interactions 453  5 DISCUSSION  We have obtained a solvable model with which a coupled dynamics of fast stochas-  tic neurons and slow dynamic interactions can be studied analytically. Furthermore  it presents the replica method from a novel perspective, provides a direct inter-  pretation of the replica dimension n in terms of parameters controlling dynamical  processes and leads to new phase transition characters. As a model for neural learn-  ing the specific example analyzed here is however only a first step, with h and K  as introduced corresponding to only a single pattern. Its adaptation to treat many  patterns is the next challenge.  One type of generalization is to consider the whole system as of lower connectivity  with only pairs of connected sites being available for interaction upgrade. For  example, the system could be on a lattice, in which case the corresponding coupled  partition function will have the usual greater complication of a finite-dimensional  system, or randomly connected with each bond present with a probability C/N,  in which case there results an analogue of the Viand-Bray [13] spin-glass. In each  of these cases the explicit factors involving N in the {Jij} dynamics (3) should  be removed (their presence or absence being determined by the need for statistical  relevance and physical scaling).  Yet another generalization is to higher order interactions, for example to p-neuron  ones:  S{j} ({(Ti})"-" -- Z Jil,...:ip(Til(Ti9...(Tip  ii,...,ip  with corresponding interaction dynamics  or to more complex neuron types.  If the symmetry-breaking fields Kij in the interaction dynamics are choosen at  random, we obtain a curious theory in which we find replicas on top of replicas (the  replica trick would be used to deal with the quenched disorder of the Kij, for a  model in which replicas are already present due to the coupled dynamics).  Finally, our approach can in fact be generalized to any statistical mechanical system  which in equilibrium is described by a Boltzmann distribution in which the Hamilto-  nian has (adiabatically slowly) evolving parameters. By choosing these parameters  to evolve according to an appropriate Langevin process (involving the free energy  of the underlying fast system) one always arrives at a replica theory describing the  coupled system in equilibrium.  Acknowledgements  Financial support from the U.K. Science and Engineering Research Council under  grants 9130068X and GR/H26703, from the European Community under grant  S/SC1'915121, and from Jesus College, Oxford, is gratefully acknowledged.  454 Coolen, Penney, and Sherrington  References  [1] Glauber R.J. (1963) J. Math. Phys. 4 294  [2] Hebb D.O. (1949) 'The Organization of Behaviour' (Wiley, New York)  [3] Shinomoto S. (1987) J. Phys. A: Math. Gen. 20 L1305  [4] Jonker It.J.J. and Coolen A.C.C. (1991) J. Phys. A: Math. Gen. 24 4219  [5] Jonker H.J.J., Coolen A.C.C. and Denier van der Gon J.J. (1993) J. Phys. A:  Math. Gen. 26 2549  [6] Sherrington D. and Kirkpatrick S. (1975) Phys. Rev. Left. 35 1792  [7] M6zard M. private communication  [8] Kirkpatrick S. and Sherrington D. (1978) Phys. Rev. B 17 4384  [9] M6zard M., Parisi G. and Virasoro M.A. (1987) 'Spin Glass Theory and Be-  yond' (World Scientific, Singapore)  [10] Sherrington D. (1980) J. Phys. A: Math. Gen. 13 637  [11] Penney R.W., Coolen A.C.C. and Sherrington D. (1993) J. Phys. A: Math.  Gen. 26 3681-3695  [12] Kondor I. (1983) J. Phys. A: Math. Gen. 16 L127  [13] Viana L. and Bray A.J. (1983) J. Phys. C 16 6817  
Learning in Compositional Hierarchies:  Inducing the Structure of Objects from Data  Joachim Utans  Oregon Graduate Institute  Department of Computer Science and Engineering  P.O. Box 91000  Portland, OR 97291-1000  utans @cse.ogi.edu  Abstract  I propose a learning algorithm for learning hierarchical models for ob-  ject recognition. The model architecture is a compositional hierarchy  that represents part-whole relationships: parts are described in the lo-  cal context of substructures of the object. The focus of this report is  learning hierarchical models from data, i.e. inducing the structure of  model prototypes from observed exemplars of an object. At each node  in the hierarchy, a probability distribution governing its parameters must  be learned. The connections between nodes reflects the structure of the  object. The formulation of substructures is encouraged such that their  parts become conditionally independent. The resulting model can be  interpreted as a Bayesian Belief Network and also is in many respects  similar to the stochastic visual grammar described by Mjolsness.  1 INTRODUCTION  Model-based object recognition solves the problem of invariant recognition by relying on  stored prototypes at unit scale positioned at the origin of an object-centered coordinate  system. Elastic matching techniques are used to find a correspondence between features of  the stored model and the data and can also compute the parameters of the transformation the  observed instance has undergone relative to the stored model. An example is the TRAFFIC  system (Zemel, Mozer and Hinton, 1990) or the Frameville system (Mjolsness, Gindi and  285  286 Utans  ......  Human  f------ I Arm  I  I  I  Lower Arm  Figure 1: Example of a compositional  hierarchy. The simple figure can be  represented as hierarchical composi-  tion of parts. The hierarchy can  be represented as a graph (a tree in  this case). Nodes represent parts and  edges represent the structural relation-  ship. Nodes at the bottom represent  individual parts of the object; nodes  at higher levels denote more complex  substructures. The single node at the  top of the tree represents the entire ob-  ject.  Anandan, 1989; Gindi, Mjolsness and Anandan, 1991; Utans, 1992). Frameville stores  models as compositional hierarchies and by matching at each level in the hierarchy reduces  the combinatorics of the match.  The attractive feature of feed-forward neural networks for object recognition is the relative  ease with which their parameters can be learned from training data. Multilayer feed-forward  networks are typically trained on input/output pairs (supervised learning) and thus are tuned  to recognize instances of objects as seen during training. Difficulties arise if the observed  object appears at a different position in the input image, is scaled or rotated, or has been  subject to distortions. Some of these problems can be overcome by suitable preprocessing or  judicious choice of features. Other possibilities are weight sharing (LeCun, Boser, Denker,  Henderson, Howard, Hubbard and Jackel, 1989) or invariant distance measures (Simard,  LeCun and Denker, 1993).  Few attempts have been reported in the neural network literature to learn the prototype  models for model based recognition from data. For example, the Frameville system uses  hand-designed models. However, models learned from data and reflecting the statistics of  the data should be superior to the hand-designed models used previously. Segen (1988a;  1988b) reports an approach to learning structural descriptions where features are clustered  to substructures using a Minimum Description Length (MDL) criterion to obtain a sparse  representation. Saund (1993) has proposed a algorithm for constructing tree presentation  with multiple "causes" where observed data is accounted for by multiple substructures at  higher levels in the hierarchy. Ueda and Suzuki (1993) have developed an algorithm for  learning models from shape contours using multiscale convex/concave structure matching  to find a prototype shape typical for exemplars from a given class.  2 LEARNING COMPOSITIONAL HIERARCHIES  The algorithm described here merges parts by means of grouping variables to form sub-  structures. The model architecture is a compositional hierarchy, i.e. a part-whole hierarchy  (an example is shown in Figure 1). The nodes in the graph represent parts and substruc-  tures, the arcs describe the structure of the object. At each node a probability density for  part parameters is stored. A prominent advocate of such models has been Marr (1982)  and models of this type are used in the Frameville system (Mjolsness et al., 1989; Gindi  et al., 1991; Utans, 1992). The nodes in the graph represent parts and substructures, the  Learning in Compositional Hierarchies: Inducing the Structure of Objects from Data 287  Figure 2: Examples of differ-  ent compositional hierarchies for  the same object (the digit 9 for  a seven-segment LED display).  One model emphasizes the paral-  lel lines making up the square in  the top part of the figure while for  another model angles are chosen  as intermediate substructures. The  example on the right shows a hier-  archy that "reuses" parts.  arcs describe the structure of the object. The arcs can be regarded as "part-of" or "ina"  relationships (similar to the notion used in semantic networks). At each node a probability  density for part parameters such as position, size and orientation is stored.  The model represents a typical prototypeobject at unit scale in an object-centered coordinate  system. Parameters of parts are specified relative to parameters of the parent node in the  hierarchy. Substructures thus provide a local context for their parts and decouple their parts  from other parts and substructures in the model. The advantages of this representation are  sparseness, invariance with respect to viewpoint transformations and the ability to model  local deformations. In addition, the model explicitly represents the structure of an object  and emphasizes the importance of structure for recognition (Cooper, 1989).  Learning requires estimating the parameters of the distributions at each node (the mean and  variance in the case of Gaussians) and finding the structure of model. The emphasis in this  report is on learning structure from exemplars. The parameterization of substructures may  be different than for the parts at the lowest level and become more complex and require more  parameters as the substructures themselves become more complex. The representation as  compositional hierarchy can avoid overfitting since at higher levels in the hierarchy more  exemplars are available for parameter estimation due to the grouping of parts (Omohundro,  1991).  2.1 Structure and Conditional Independence: Bayesian Networks  In what way should substructures be allocated? Figure 2 shows examples of different  compositional hierarchies for the same object (the digit 9 for a seven-segment LED display).  One model emphasizes the parallel lines making up the square in the top part of the figure  while for another model angles are chosen as intermediate substructures. It is not clear  which of these models to choose.  The important benefit of a hierarchical representation of structure is that parts belonging to  different substructures become decoupled, i.e. they are assigned to a different local context.  The problem of constructing structured descriptions of data that reflect this independence  relationship has been studied previously in the field of Machine Learning (see (Pearl, 1988)  for a comprehensive introduction). The resulting models are Bayesian Belief Networks.  Central to the idea of Bayesian Networks is the assumption that objects can be regarded  as being composed of components that only sparsely interact and the network captures  the probabilistic dependency of these components. The network can be represented as  an interaction graph augmented with conditional probabilities. The structure of the graph  represents the dependence of variables, i.e. connects them with and arc. The strength of the  288 Utans  Figure 3: Bayesian Networks and conditional  independence (see text).  ina p?,  Oata  Figure 4: The model architecture. Circles denote  the grouping variables ina (here a possible valid  model after learning is shown).  dependence is expressed as forward conditional probability. The conditional independence  is represented by the absence of an arc between two nodes and leads to the sparseness of  the model.  The notion of conditional independence in the context studied here manifest itself as follows.  By just observing two parts in the image, one must assume that they, i.e. their parameters  such as position, are dependent and must be modeled using their joint distribution. How-  ever, if one knows that these two parts are grouped to form a substructure then knowing  the parameters of the substructure, the parts become conditionally independent, namely  conditioned on the parameters of the substructure. Thus, the internal nodes representing the  substructures summarize the interaction of their child nodes. The correlation between the  child nodes is summarized in the parent node and what remains is, for example, independent  noise in observed instances of the child nodes.  The probability of observing an instance can be calculated from the model by starting at  the root node and multiplying with the conditional probabilities of nodes traversed until the  leaf nodes are reached. For example, given the graph in Figure 3, the joint distribution can  be factored as  P(z], V], V2, z], z2, z3, z4) =  P( z] )P(v] Iz] )P( z] Iv] )P( z] Iv] )P( z2lv] )P( z31v2 )P(z41v2) (1)  (note that the hidden nodes are treated just like the nodes corresponding to observable parts).  Note that the stochastic visual grammar described by Mjolsness (1991) is equivalent to this  model. The model used there is a stochastic forward (generative) model where each level  of the compositional hierarchy corresponds to a stochastic production rule that generates  nodes in the next lower level. The distribution of parameters at the next lower level  are conditioned on the parameters of the parent node. Thus, the model obtained from  constructing a Bayesian network is equivalent to the stochastic grammar if the network is  constrained to a directed acyclic graph (DAG).  If all the nodes of the network correspond to observable events, techniques exist for finding  the structure of the Bayesian Network and estimate its parameters (Pearl, 1988) (see also  (Cooper and Herskovits, 1992)). However, for the hierarchical models considered here,  only the nodes at the lowest layer (the leaves of the tree) correspond to observable instances  of parts of the object in the training data. The learning algorithm must induce hidden,  unobservable substructures. That is, it is assumed that the observables are "caused" by  internal nodes not directly accessible. These are represented as nodes in the network just  Learning in Compositional Hierarchies: Inducing the Structure of Objects from Data 289  like the observables and their parameters must be estimated as well. See (Pearl, 1988) for  an extensive discussion and examples of this idea.  Learning Bayesian networks is a hard problem when the network contains hidden nodes  but a construction algorithm exists if it is known that the data is in fact tree-decomposable  (Pearl, 1988). The methods is based on computing the correlations p between child nodes  and constraints on the correlation coefficients dictated by a particular structure. The entire  tree can be constructed recursively using this method. Here, the case of Normal-distributed  real-valued random variables is of interest:  p(z,...,z)- x/exp - (x--/)Ts-(x --tt) (2)  where x --- (z, z2,..., :r,) with mean tt = E{x} and covariance matrix  -- E{(x -  tt)(x - tO T } The method is based on a condition under which a set of random variables  is star-decomposable. The question one ask is whether a set of n random variables can  be represented as the marginal distribution of n + 1 variables x, ..., z,, w such that the  x, ..., x,, are conditionally independent given w, i.e.  = Hp(lo)p(w) (3)  i  p(c,...,c) = lp(c,...,c, w)dw (4)  In the graph representation of the Bayesian Network w is the central node relating the  x,..., x,, hence the name star-decomposable. In the general case of n variables this is  hard to verify but a result by Xu and Pearl (1987) is available for 3 variables: A necessary  and sufficient condition for 3 random variables with a joint normal distribution to be star-  decomposable is that the pairwise correlation coefficients satisfy the triangle inequality  (5)  Pjk >_ PjiPik with Pij -- tr  for all i, j, k G [1,2, 3] and i  j - k. Equality holds if node w coincides with node i. For  the lowest level of the hierarchy, nodes j and k represent parts and node i = w represents  the common substructure.  2.2 An Objective Function for Grouping Parts  The algorithm proposed here is based on "soft" grouping by means of grouping variables ina  where both the grouping variables and the parameter estimates are updated concurrently.  The learning algorithms described in (Pearl, 1988) incrementally construct a Bayesian  network and decisions made at early stages cannot be reversed. It is hoped that the method  proposed here is more robust with regard to inaccuracies of the estimates. However, if the  true distribution is not a star-decomposable normal distribution it can only be approximated.  Let inaij be a binary variable associated with the arc connecting node i and node j; inaij --  1 if the arc is present in the network (ina is the adjacency matrix of the graph describing the  structure of the model). The model architecture is restricted to a compositional hierarchy (a  departure from the more general structure of a Bayesian Network, i.e. nodes are preassigned  to levels of the hierarchy (see Figure 4)). Based on the condition in equation (5) a cost  290 Utans  function term for the grouping variables ina is  Ep : E inawjinaw (Pwjpw -- Pjk) 2  w,j,kj  (6)  The term penalizes the grouping of two part nodes to the same parent if the term in  parentheses is large (i and k index part nodes, w nodes at the next higher level in the  hierarchy) The inaoj can be regarded as assignment variables the assign child nodes j to  parent nodes w. The parameters at each node and the assignment variables ina are estimated  using an EM algorithm (Dempster, Laird and Rubin, 1977; Utans, 1993; Yuille, Stolorz and  Utans, 1994). For the details of the implementation of grouping with match networks see  (Mjolsness et al., 1989; Mjolsness, 1991; Gindi et al., 1991; Utans, 1992; Utans, 1994).  At each node for each parameter a probability distribution is stored. Nodes at the lowest  level of the hierarchy represent parts in the input data. For the Gaussian distributions used  here for all nodes, the parameters are the mean tt and the variance o- and can be estimated  from data. Each part node can potentially be grouped to any substructure at the next  higher level in the hierarchy. The parameters of the distributions at this level are estimated  from data as well but using the current value of the grouping variables inaij to weight the  contribution from each part node. Because each child node j can have only one parent node  i, an additional constraint for a unique assignment is Y'o inaoj = 1.  3 AN EXAMPLE  Initial simulations of the proposed algorithm were performed using a hierarchial model for  dot clusters. The training data was generated using the three-level model shown in Figure 5.  Each node is parameterized by its position (x, y). The node at the top level represents the  entire dot cluster. At the intermediate level nodes represent subcluster centers. The leaf  nodes at the lowest level represent individual dots that are output by the model and observed  in the image. The top level node represents the position of the entire cluster. At each level  1 + 1 stored offsets dl - are added to the parent coordinates x i to obtain the coordinates  of the child nodes. Then, independent, zero-mean Gaussian distributed noise e is added:  x/+l I /+1  j = x i + did + e The training data consists of a vector of positions at the lowest level  {x 5 } with x 5 = (zi, Yi), J = 1... 9 for each exemplar.  The identity of the parts in the training data is assumed known. In addition, the data consists  of parts from a single object. For the simulations, the model architecture is restricted to a  three-level hierarchy. Since at the top level a single node represents the entire object, only  the grouping variables from the lowest to the intermediate level are unknown (the nodes  at the intermediate level are implicitly grouped to the single node at the top level). In the  current implementation the parameters of a parent node are defined as the average over the  ^, , a/j 3+'  parameters of its child nodes: x i =  5- 5 i :  For this problem the algorithm has recovered the structure of the model that generated the  training data. Thus in this case it is possible to use the correlation coefficients to learn  the structure of an object from noisy training exemplars. However, the algorithm does  not recover the same parameter values x used in the generative model at the intermediate  layers. These cannot uniquely specified due to the ambiguity between the parameters xi  and offsets dij (a different choice for xi leads to different values for dij ).  Learning in Compositional Hierarchies: Inducing the Structure of Objects from Data 291  ition     Dots X Global Position  Cluster Center 0 Dot  Figure 5: The model used to generated training data. The structure of the model is a three-level  hierarchy. The model parameters are chosen such that the generated dot cluster spatially overlap. On  the left, an example of an instance of a dot cluster generated from the model is shown (these constitute  the training data).  4 EXTENSIONS  The results of the initial experiments are encouraging but more research needs to be done  before the algorithm can be applied to real data. For the example used here, the training  data was generated by a hierarchical model. Thus the distribution of the training exemplars  could, in principle, be learned exactly using the proposed model architecture. I plan to  study the effect of approximating the distribution of real-world data by applying the method  to the problem of learning models for handwritten digit recognition.  The model should be extended to include provisions to deal with missing data. Instead of  being binary variables, inaij could be the conditional probability that part j is present in a  typical instance of the object given that the parent node i itself is present (similar to the dot  deletion rule described in (Mjolsness, 1991)). These probabilities must also be estimated  from data. Under this interpretation the inaij are similar to the mixture coefficients in the  mixture of experts model (Jordan and Jacobs, 1993)  The robustness of the algorithm can be improved when the desired locality of the model is  explicitly favored via an additional constraint.  Elocal- A E inaijinaik Ixj - xk 12  ijk  In this sense, the toy problem shown here is unnecessarily difficult. Preliminary experiments  indicate that including this term reduces the sensitivity to spurious correlations between  parts that are far apart.  As described the algorithm performs unsupervised grouping; learning the hierarchical model  does not take in to account the recognition performance obtained when using the model.  While the problem of learning and representing models in a hierarchical form is interesting  in its own right, the final criteria for judging the model in the context of a recognition  problem should be recognition performance. The assumption is that the model should pick  up substructures that are specific to a particular class of objects and maximally discriminate  between objects belonging to other classes. For example, after a initial model is obtained  that roughly captures the structure of the training data, it can be refined on-line during the  recognition stage.  292 Utans  Acknowledgements  Initial work on this project was performed while the author was with the International  Computer Science Institute, Berkeley, CA. At OGI supported was provided in part under  grant ONR N00014-92-J-4062. Discussions with S. Knerr, E. Mjolsness and S. Omohundro  were helpful in preparing this work.  References  Cooper, G. F. and Herskovits, E. (1992), 'A bayesian method for induction of probabilistic networks  from data', Machine Learning 9, 309-347.  Cooper, P. R. (1989), Parallel Object Recognition from Structure (The Tinkertoy Project), PhD thesis,  University of Rochester, Computer Science. also Technical Report No. 301.  Dempster, A. P., Laird, N.M. and Rubin, D. B. (1977), 'Maximum likelihood from incomplete data  via the EM algorithm', J. Royal Statist. Soc. B 39, 1-39.  Gindi, G., Mjolsness, E. and Anandan, P. (1991), Neural networks for model based recognition, in  'Neural Networks: Concepts, Applications and Implementations', Prentice-Hall, pp. 144-173.  Jordan, M. I. and Jacobs, R. A. (1993), Hierarchical mixtures of experts and the EM algorithm,  Technical Report 9301, MIT Computational Cognitive Science.  LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W. and Jackel, L. D.  (1989), 'Backpropagation applied to handwritten zip code recognition', Neural Computation  1,541-551.  Marr, D. (1982), Vision, W. H. Freeman and Co., New York.  Mjolsness, E. (1991), Bayesian inference on visual grammars by neural nets that optimize, Technical  Report YALEU-DCS-TR-854, Yale University, Dept. of Computer Science.  Mjolsness, E., Gindi, G. R. and Anandan, P. (1989), 'Optimization in model matching and perceptual  organization', Neural Computation 1(2).  Omohundro, S. M. (1991), Bumptrees for efficient function, constraint, and classification learning, in  R. Lippmann, J. Moody and D. Touretzky, eds, 'Advances in Neural Information Processing 3',  Morgan Kaufmann Publishers, San Mateo, CA.  Pearl, J. (1988), Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference,  Morgan Kaufmann Publishers, Inc., San Mateo, CA.  Saund, E. (1993), A multiple cause mixture model for unsupervised leaming, Technical report, Xerox  PARC, Palo Alto, CA. preprint, submitted to Neural Computation.  Segen, J. (1988a), Learning graph models of shape, in 'Proceedings of the 5th International Conference  on Machine Learning'.  Segen, J. (1988b), 'Learning structural description of shape', Machine Vision pp. 257-269.  Simard, P., LeCun, Y. and Denker, J. (1993), Efficient pattern recognition using a new transformation  distance, in S. J. Hanson, J. Cowan and L. Giles, eds, 'Advances in Neural Information Processing  5', Morgan Kaufmann Publishers, San Mateo, CA.  Ueda, N. and Suzuki, S. (1993), 'Learning visual models from shape contours using multiscale  convex/concave structure matching', IEEE Transactions on Pattern Analysis and Machine In-  telligence 15(4), 337-352.  Utans, J. (1992), Neural Networks for Object Recognition within Compositional Hierarchies, PhD  thesis, Department of Electrical Engineering, Yale University, New Haven, CT 06520.  Utans, J. (1993), Mixture models and the EM algorithm for object recognition within compositional  hierarchies. part 1: Recognition, Technical Report TR-93-004, International Computer Science  Institute, 1947 Center St., Berkeley, CA 94708.  Utans, J. (1994), 'Mixture models for learning and recognition in compositional hierarchies', in  preparation.  Xu, L. and Pearl, J. (1987), Structuring causal tree models with continous variables, in 'Proceedings  of the 3rd Workshop on Uncertainty in AI', pp. 170-179.  Yuille, A., S tolorz, P. and Utans, J. (1994), 'Statistical physics, mixtures of distributions and the EM  algorithm', to appear in Neural Computation.  Zemel, R. S., Mozer, M. C. and Hinton, G. E. (1990), Traffic: Recognizing objects using hierarchical  reference frame transformations, in D. S. Touretzky, ed., 'Advances in Neural Information  Processing 2', Morgan Kaufman Pulishers, San Mateo, CA.  
A Computational Model  for Cursive Handwriting  Based on the Minimization Principle  Yasuhiro Wada * Yasuharu Koike  Eric Vatikiotis-Bateson  Mitsuo Kawato  ATR Human Information Processing Research Laboratories  2-2 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-02, Japan  ABSTRACT  We propose a trajectory planning and control theory for continuous  movements such as connected cursive handwriting and continuous  natural speech. Its hardware is based on our previously proposed  forward-inverse-relaxation neural network (Wada & Kawato, 1993).  Computationally, its optimization principle is the minimum torque-  change criterion. Regarding the representation level, hard constraints  satisfied by a trajectory are represented as a set of via-points extracted  from a handwritten character. Accordingly, we propose a via-point  estimation algorithm that estimates via-points by repeating the  trajectory formation of a character and the via-point extraction from the  character. In experiments, good quantitative agreement is found  between human handwriting data and the trajectories generated by the  theory. Finally, we propose a recognition schema based on the  movement generation. We show a result in which the recognition  schema is applied to the handwritten character recognition and can be  extended to the phoneme timing estimation of natural speech.  1 INTRODUCTION  In reaching movements, trajectory formation is an ill-posed problem because the hand  can move along an infinite number of possible trajectories from the starting to the target  point. However, humans move an arm between two targets along consistent one of an  *' Present Address: Systems Lab., Kawasaki Steel Corporation,  Makuhari Techno Garden, 1-3.Nakase, Mihama-ku, Chiba 261, Japan  727  728 Wada, Koike, Vatikiotis-Bateson, and Kawato  infinite number of trajectories. Therefore, the brain should be able to compute a unique  solution by imposing an appropriate criterion to the ill-posed problem. Especially, a  smoothness performance index was intensively studied in this context.  Flash & Hogan (1985) proposed a mathematical model, the minimum-jerk model. Their  model is based on the kinematics of movement, independent of the dynamics of the  musculoskeletal system. On the other hand, based on the idea that the objective function  must be related to dynamics, Uno, Kawato & Suzuki (1989) proposed the minimum  torque-change criterion which accounts for the desired trajectory determination. The  criterion is based on the theory that the trajectory of the human arm is determined so as to  minimize the time integral of the square of the rate of torque change. They proposed the  following quadratic measure of performance. Where :J is the torque generated by the j-  th actuator of M actuators, and tfis the movement time.  Handwriting production is an attractive subject in human motor control studies. In  cursive handwriting, a symbol must be transformed into a motor command stream. This  transformation process raises several questions. How can the central nervous system  (CNS) represent a character symbol for producing a handwritten letter? By what principle  can motor planning be made or a motor command be produced? In this paper we propose  a handwriting model whose computational theory and representation are the same as the  model in reaching movements. Our proposed computational model for cursive  handwriting is assumed to generate a trajectory that passes through many via-points. The  computational theory is based on the minimum torque-change criterion, and a  representation of a character is assumed to be expressed as a set of via-points extracted  from a handwritten character. In reaching movement, the boundary condition is given by  the visual information, such as the location of a cup, and the trajectory formation is based  on the minimum torque-change criterion, which is completely the same as the model of  handwriting (Fig. 1). However, it is quite difficult to determine the via-points in order to  reproduce a cursive handwritten character. We propose an algorithm that can determine  the via-points of the handwritten character, based only on the same minimization  principle and which does not use any other ad hoc information such as zero-crossing  velocity (Holierbach, 1981).  Representation  Reaching  (reach to   the object)  Handwriting   (write a character)  Location  of he object  ..i_su___al__ I_ n.f.o..rm ation  Via-Point  (representation  of character) -  Via-Point Estimation  Al.....jgi. th m  Computational Hardware  Theory  Figure 1: A handwriting model.  A Computational Model for Cursive Handwriting Based on the Minimization Principle 729  2 PREVIOUS WORK ON THE HANDWRITING MODEL  Several handwriting models (Holierbach, 1981; Morasso & Mussa-Ivaldi, 1982; Edleman  & Flash, 1987) have been proposed. Holierbach proposed a handwriting model based on  oscillation theory. The model basically used a vertical oscillator and a horizontal  oscillator. Morasso & Mussa-Ivaldi proposed a trajectory formation model using a spline  function, and realized a handwritten character using the formation model.  Edleman & Flash (1987) proposed a handwriting model based on snap (fourth derivative  of position) minimization. The representation of a character was four basic strokes and a  handwritten character was regenerated by a combination of several strokes. However,  their model was different from their theory for reaching movement. Flash & Hogan  (1985) have proposed the minimum jerk criterion in the reaching movement.  3 A HANDWRITING MODEL  3.1 Trajectory formation neural network:  Forward-Inverse Relaxation Model (FIRM)  First, we explain the trajectory formation neural network. Because the dynamics of the  human arm are nonlinear, finding a unique trajectory based on the minimum torque-  change criterion is a nonlinear optimization problem. Moreover, it is rather difficult.  There are several criticisms of previous proposed neural networks based on the minimum  torque-change criterion: (1) their spatial representation of time, (2) back propagation is  essential, and (3) much time is required. Therefore, we have proposed a new neural  network, FIRM(Forward-Inverse Relaxation Model) for trajectory formation (Wada &  Kawato, 1993). This network can be implemented as a biologically plausible neural  network and resolve the above criticisms.  3.2 Via-point estimation model  Edelman & Flash (1987) have pointed out the difficulty of finding the via-points in a  handwritten character. They have argued two points: (1) the number of via-points, (2) a  reason for the choice of every via-point locus. It is clear in approximation theory that a  character can be regenerated perfectly if the number of extracted via-points is large.  Appropriate via-points can not be assigned according to a regular sampling rule if the  sample duration is constant and long. Therefore, there is an infinite number of  combinations of numbers and via-point positions in the problem of extracting via-points  from a given trajectory, and a unique solution can not be found if a trajectory reformation  theory is not identified. That is, it is an ill-posed problem.  The algorithm for assigning the via-points finds the via-points by iteratively activating  both the trajectory formation module (FIRM) and the via-point extraction module (Fig.  2). The trajectory formation module generates a trajectory based on the minimum torque-  change criterion using the via-points which are extracted by the via-point extraction  module. The via-point extraction module assigns the via-points so as to minimize the  square error between the given trajectory and the trajectory generated by the trajectory  formation module. The via-point extraction algorithm will stop when the error between  the given trajectory and the trajectory generated from the extracted via-points reaches a  threshold.  730 Wada, Koike, Vatikiotis-Bateson, and Kawato  Via-Points Extraction Module  ft'r[(O(t )-O,.(t))dt  Min  Via-points assignment to  decrease the above trajectory  error  Minimum Torque-  Change Trajectory  Via-Point  Information  (Position  Time)  Trajectory Formation Module  (FIRM)   dt  Min  Trajectory generation  based on minimum torque-  change criterion  Figure 2: Via-point estimation model. Ot(t) is the given trajectory of the j-th joint angle  and o(t) represents the generated trajectory.  3.2.1 Algorithm of via-point extraction  There are a via-point extraction procedure and a trajectory production procedure in the  via-point extraction module, and they are iteratively computed. Trajectory production in  the module is based on the minimum-jerk model (Flash & Hogan 1985 ) on a joint angle  space, which is equivalent to the minimum torque-change model when arm dynamics are  approximated as in the following dynamic equation:  J = lJ j (j= 1,..., M) (2)  where I j and t j are the inertia of the link and the acceleration of the j-th joint angle,  respectively.  The algorithm for via-point extraction is illustrated in Fig. 3. The procedural sequence is  as follows:  (Step 1) A trajectory between a starting point and a final point is generated by using the  minimum torque-change principle of the linear dynamics model.  (Step 2) The point with the maximum square error value between the given trajectory and  the generated trajectory is selected as a via-point candidate.  (Step 3) If the maximum value of the square error is less than the preassigned threshold,  the procedure described above is finished. If the maximum value of the square error is  greater than the threshold, the via-point candidate is assigned as via-point i and a  trajectory is generated from the starting point through the via-point i to the final point.  This generated trajectory is added to the trajectory that has already been generated. The  time of the start point of the generated trajectory is a via-point located just before the  assigned via-point i, and the time of the final point of the generated trajectory is a via-  point located just after the assigned via-point i. The position error of the start point and  the final point equal 0, since the compensation for the error has already been made. Thus,  the boundary conditions of the generated trajectory at the start and final point become 0.  The velocity and acceleration constraints at the start and final point are set to 0.  (Step 4) By repeating Steps 2 and 3, a set of via-points is found.  The j-th actuator velocity constraint t,i,, and acceleration constraint TM  0,i,, at the via-point i  are set by minimizing the following equation.  ., .., +  J(O,i,,,Ovi,,) = ij 2 ,, ft} 2dt Min(3)  A Computational Model for Cursive Handwriting Based on the Minimization Principle 731  trajectory by Stql  Figure 3: An algorithm for extracting via-points.  Finally, the via-points are fed to the FIRM, and the minimum torque change trajectory is  produced. This trajectory and the given trajectory are then compared again. If the value  of the square error does not reach the threshold, the procedure above is repeated.  It can be mathematically shown that a given trajectory is perfectly approximated with this  method (completeness), and furthermore that the number of extracted via-points for a  threshold is the minimum (optimality). (Wada & Kawato, 1994)  4 PERFORMANCE OF THE VIA-POINT ESTIMATION MODEL  4.1 Performance of single via-point movement  First, we examine the performance of our proposed via-point estimation model. A result  of via-point estimation in a movement with a via-point is shown in Fig 4. Two  movements (T3-P1-T5 and T3-P2-T5) are examined. The white circle and the solid lines  show the target points and measured trajectories, respectively. P1 and P2 show target  via-points. The black circle shows the via-points estimated by the algorithm. The  estimated via-points were close to the target via-points. Thus, our proposed via-point  estimation algorithm can find a via-point on the given trajectory.  0.55  ' 0.50  ., 0.45  O.4O  0.35  0.30  Estimated Via-Point  P1  Target Point 0  T5  T3  P2  I I I I I I I  4}.3 4}.2 4}.1 0.0 0.1 0.2 0.3  X Ira]  Figure 4: A result of via-point estimation in a movement with a via-point.  4.2 Performance of the handwriting model  Fig. 5 shows the case of cursive connected handwritten characters. The handwriting  model can generate trajectories and velocity curves of cursive handwritten characters that  are almost identical to human data. The estimated via-points are classified into two  groups. The via-points in one group are extracted near the minimum points of the  732 Wada, Koike, Vatikiotis-Bateson, and Kawato  0.$2   Eatimate. d Via-Pot  ..... Tmct   I ' I ' I   -O. lO o.oo O. lO  X [in] Tu[c]  (a) CO)  ' 0.1  0.0  I I I I l  ] 2  4 5  Figure 5: Estimated via-points in cursive handwriting. (a) and (b) show the trajectory and  tangential velocity profile, respectively. The via-point estimation algorithm extracts a via-  point (segmentation point) between characters.  velocity profile. The via-points of the other group are assigned to positions that are  independent of the above points. Generally, the minimums of the velocity are considered  to be the feature points of the movement. However, we confirmed that a given trajectory  can not be reproduced by using only the first group of via-points. This finding shows that  the second group of via-points is important. Our proposed algorithm based on the  minimization principle can estimate points that can not be selected by any kinematic  criterion. Furthermore, it is important in handwritten character recognition that the via-  point estimation algorithm extracts via-points between characters, that is, their  segmentation points.  5 FROM FORMATIONTO RECOGNITION  5.1 A recognition model  Next, we propose a recognition system using the trajectory formation model and the via-  point estimation model. There are several reports in the literature of psychology which  suggest that the formation process is related to the recognition process. (tiberman &  Mattingly, 1985; Freyd, 1983)  Here, we present a pattern recognition model that strongly depends on the handwriting  model and the via-point estimation model (Fig.6). (1) The features of the handwritten  character are extracted by the via-point estimation algorithm. (2) Some of via-points are  segmented and normalized in space and time. Then, (3) a trajectory is regenerated by  using the normalized via-points. (4) A symbol is identified by comparing the regenerated  trajectory with the template trajectory.  m (Tem late0  I1-I '[  I [  leCf%gr[n     Z  & Comparon)  Figu 6: Movement pattern recoition using exacd via-poin obned through  movement pattern generator  A Computational Model for Cursive Handwriting Based on the Minimization Principle 733  1 :BAD: (0,17)  2 :BAD: (0.18)  3 :BAD: (0,17)  1 :DEAR: (0,8)  2 :DEAR: (0,8)  3 :DEAR : (0,8)  (18,35) (36,52)  (18,35) (36,52)  (18,35) (35,52)  (9,18) (19,31) (30,51)  (9,18) (19,31) (30,50)  (9.18) (19,30) (30,51)  Figure 7: Results of character recognition  5.2 Performance of the character recognition model  Fig. 7 shows a result of character recognition. The right-hand side shows the recognition  results for the left-hand side. The best three candidates for recognition are listed.  Numerals in parentheses show the number of starting via-points and the final via-point  for the recognized character.  5.3 Performance of the estimation of timing of phonemes in real speech  Fig. 8 shows the acoustic waveform, the spectrogram, and the articulation movement  when the sentence" Sam sat on top of the potato cooker..." is spoken. The phonemes are  identified, and the vertical lines denote phoneme midpoints. White circles show the via-  points estimated by our proposed algorithm. Rather good agreement is found between the  estimated via-points and the phonemes.  From this experiment, we can point out two important possibilities for the estimation  model of phoneme timing. The first possibility concerns speech recognition, and the  second concerns speech data compression. It seems possible to extend the via-point  estimation algorithm to speech recognition if a mapping from acoustic to articulator  motion is identified (Shirai & Kobayashi, 1991, Papcun et al., 1992). Furthermore, with  training of a forward mapping from articulator motion to acoustic data (Hirayama et al.,  1993), the via-point estimation model can be used for speech data compression.  6 SUMMARY  We have proposed a new handwriting model. In experiments, good qualitative and  quantitative agreement is found between human handwriting data and the trajectories  generated by the model. Our model is unique in that the same optimization principle and  hard constraints used for reaching are also used for cursive handwriting. Also, as  opposed to previous handwriting models, determination of via-points is based on the  optimization principle and does not use a priori knowledge.  We have demonstrated two areas of recognition, connected cursive handwritten character  recognition and the estimation of phoneme timing. We incorporated the formation model  into the recognition model and realized the recognition model suggested by Freyd (1983)  and Liberman and Mattingly(1985). The most important point shown by the models is  that the human recognition process can be realized by specifying the human formation  process.  REFERENCES  S. Edelman & T. Flash (1987) A Model of Handwriting. Biol. Cybern., 57, 25-36.  734 Wada, Koike, Vatikiotis-Bateson, and Kawato  Sam sat on top of the potato cooker...  =============================== =========================== :::-::x-: :.".:::::;:::.:: .... . :: .::'.::::::: .......  ....... :' ".,,-..-..-: ...:...';.'. ::.-.'.:.-:-: ......... .: ..... -x. .: ........... :.:::"';:., ..... -'..:.. .:   S AE M S2  T.aJ N I'2 H A PUt V'IlU!P'211t]I3H2 EI 'I'40 K I'B  TBY  6. - o.'4 . . . - .'4  Time[Sec]  Figure 8: Estimation result of phoneme time. Temporal acousucs and vertical  positions of the tongue blade (TBY),tongue tip (TRY), jaw (JY), and lower lip (LLY)  are shown with overlaid via-point trajectories. Vertical lines correspond to acoustic  segment centers; O denotes via-points.  T. Flash, & N. Hogan (1985) The coordination of arm movements; An experimentally  confirmed mathematical model. Journal of Neuroscience, 5, 1688-1703.  J. J. Freyd (1983) Representing the dynamics of a static form. Memory & Cognition, 11,  342-346.  M. Hirayama, E. Vatikiotis-Bateson, K. Honda, Y. Koike, & M. Kawato (1993)  Physiologically based speech synthesis. In (3iles, C. L., Hanson, S. J., and Cowan, J. D.  (eds) Advances in Neural Information Processing Systems 5, 658-665. San Mateo, CA:  Morgan Kaufmann Publishers.  J. M. Holierbach (1981) An oscillation theory of handwriting. Biol. Cybe rn., 39,139-156.  A.M. Liberman & I. (3. Mattingly (1985) The motor theory of speech perception  revised. Cognition, 21, 1-36.  P. Morasso, & F. A. Mussa-Ivaldi (1982) Trajectory formation and handwriting: A  computational model. Biol. Cybern., 45, 131-142.  J. Papcun, J. Hochberg, T. R. Thomas, T. Laroche, J. Zacks, & S. Levy (1992) Inferring  articulation and recognition gestures from acoustics with a neural network trained on x-  ray microbeam data. Journal of Acoustical Society of America, 92 (2) Pt. 1.  K. Shirai, & T. Kobayashi (1991) Estimation of articulatory motion using neural  networks. Journal of Phonetics, 19, 379-385.  Y. Uno, M. Kawato, & R. Suzuki (1989) Formation and control of optimal trajectory in  human arm movement - minimum torque-change model. Biol. Cybern. 61, 89-101.  Y. Wada, & M. Kawato (1993) A neural network model for arm trajectory formation  using forward and inverse dynamics models. Neural Networks, 6(7),919-932.  Y. Wada, & M. Kawato (1994) Long version of this paper, in preparation.  PART VI  APPLICATIONS  
Unsupervised Parallel Feature Extraction  from First Principles  Mats sterberg  Image Processing Laboratory  Dept. EE., LinkSping University  S-58183 LinkSping Sweden  Reiner Lenz  Image Processing Laboratory  Dept. EE., LinkSping University  S-58183 LinkSping Sweden  Abstract  We describe a number of learning rules that can be used to train  supervised parallel feature extraction systems. The learning rules  are derived using gradient ascent of a quality function. We con-  sider a number of quality functions that are rational functions of  higher order moments of the extracted feature values. We show  that one system learns the principle components of the correla-  tion matrix. Principal component analysis systems are usually not  optimal feature extractors for classification. Therefore we design  quality functions which produce feature vectors that support unsu-  pervised classification. The properties of the different systems are  compared with the help of different artificially designed datasets  and a database consisting of all Munsell color spectra.  I Introduction  There are a number of unsupervised Hebbian learning algorithms (see Oja, 1992  and references therein) that perform some version of the Karhunen-Love expan-  sion. Our approach to unsupervised feature extraction is to identify some desirable  properties of the extracted feature vectors and to construct a quality functions that  measures these properties. The filter functions are then learned from the input pat-  terns by optimizing this selected quality function. In comparison to conventional  unsupervised Hebbian learning this approach reduces the amount of communication  between the units needed to learn the weights in parallel since the complexity now  lies in the learning rule used.  136  Unsupervised Parallel Feature Extraction from First Principles 137  The optimal (orthogonal) solution to two of the proposed quality functions turn out  to be related to the Karhunen-Lo6ve expansion: the first learns an arbitrary rota-  tion of the eigenvectors whereas the later learns the pure eigenvectors. A common  problem with the Karhunen-Lo6ve expansion is the fact that the first eigenvector is  normally the mean vector of the input patterns. In this case one filter function will  have a more or less uniform response for a wide range of input patterns which makes  it rather useless for classification. We will show that one quality function leads to  a system that tend to learn filter functions which have a large magnitude response  for just one class of samples (different for each filter function) and low magnitude  response for samples from all other classes. Thus, it is possible to classify an in-  coming pattern by simply observing which filter function has the largest magnitude  response. Similar to Intrator's Projection Pursuit related network (see Intrator g;  Cooper, 1992 and references therein) some quality functions use higher order (> 2)  statistics of the input process but in contrast to Intrator's network there is no need  to specify the amount of lateral inhibition needed to learn different filter functions.  All systems considered in this paper are linear but at the end we will briefly discuss  possible non-linear extensions.  2 Quality functions  In the following we consider linear filter systems. These can be described by the  equation:  O(t) = W(t)P(t) (1)  where P(t)  R M:I is the input pattern at iteration t, W(t)  R N:M is the filter  coefficient matrix and O(t) - (o(t),...,ON(t)) t  R N: is the extracted feature  vector. Usually M > N, i.e. the feature extraction process defines a reduction of  the dimensionality. Furthermore, we assume that both the input patterns and the  filter functions are normed; IlP(t)l I - 1 and IIw(t)ll- 1, w vn. This implies that  Iog(t)l _< 1,Vt n.  Our first decision is to measure the scatter of the extracted feature vectors around  the origin by the determinant of the output correlation matrix:  QMS(t) = det Et{O(t)O'(l)} (2)  QMS(t) is the quality function used in the Maximum Scatter Filter System (MS-  system). The use of the determinant is motivated by the following two observations:  1. The determinant is equal to the product of the eigenvalues and hence the product  of the variances in the principal directions and thus a measure of the scattering  volume in the feature space. 2. The determinant vanishs if some filter functions are  linearly dependent.  In (Lenz &: 6sterberg, 1992) we have shown that the optimal filter functions to  QMS(I) are given by an arbitrary rotation of the N eigenvectors corresponding to  the N largest eigenvalues of the input correlation matrix:  Wop = RU.9 (3)  where U, 0 contains the largest eigenvectors (or principal components) of the in-  put correlation matrix Et{P(t)Pt(t)}. R is an arbitrary rotation matrix with  det(R) = 1. To differentiate between these solutions we need a second criterion.  138 0stepberg and Lenz  One attempt to define the best rotation is to require that the mean energy E,{o} (t)}  should be concentrated in as few components on(t) of the extracted feature vector  as possible. Thus, the mean energy Et{o} (t)} of each filter function should be either  very high (i.e. near 1) or very low (i.e. near 0). This leads to the following second  order concentration measure:  N  Q2(t) = E E'{a"(t)} (1 - E,{oa,(t)}) (4)  which has a low non-negative value if the energies are concentrated.  Another idea is to find a system that produces feature vectors that have unsuper-  vised discrimination power. In this case each learned filter function should respond  selectively, i.e. have a large response for some input samples and low response for  others. One formulation of this goal is that each extracted feature vector should be  (up to the sign) binary; oi(t) = :kl and o,(t) = O, n y i, t. This can be measured   E,{o}(0)- E,{o(0) (5)  by the following fourth order expression:  N N  Q() = ,{ o() ( - o())) =  n=l n=l  which has a low non-negative value if the features are binary. Note that it is not  sufficient to use on(t) instead of o}(t) since Qa(t) will have a low value also for  feature vectors with components equal in magnitude but with opposite sign. A  third criterion can be found as follows: if the filter functions have selective filter  response then the response to different input patterns differ in magnitude and thus  the variance of the mean energy E,{oan(t)} is large. The total variance is measured  by:  N N  Qv,() =  v, {4(0) =  E,{(o(0- E,{o(0)) 2)  n--1  N  =  E,{o4(t)) - (E,{o())) 2  ()  Q,() = Q()  Q(O (7)  Qro(t) =  Q4(t) (8)  Qa4v(t) = Qvar(t)Qa4s(t) (9)  Following (Darlington, 1970) it can be shown that the distribution of o} should  be bimodal (modes below and above Et{o}}) to maximize Qvar(t). The main  difference between QVar(t) and the quality function used by Intrator is the use  4  of a fourth order term Et{o,(t)} instead of a third order term Et{oa(t)}. With  Et{oan(t)} the quality function is a measure of the skewness of the distribution  o(t) and it is maximized when one mode is at zero and one (or several) is above  E,{o(O).  In this paper we will examine the following non-parametric combinations of the  quality functions above:  Unsupervised Parallel Feature Extraction from First Principles 139  We refer to the corresponding filter systems as: the Karhunen-Lodve Filter Sys-  tem (KL-system), the Fourth Order Filter System (FO-system) and the Maximum  Variance Filter System (MV-system).  Since each quality function is a combination of two different functions it is hard to  find the global optimal solution. Instead we use the following strategy to determine  a local optimal solution.  Definition 1 The optimal orthogonal solution to each quality function is of the  form:  Wop = 9 (10)  where Rort is the rotation of the largest eigenvectors which minimize Qe(t), Q4(t)  or maximize QVar(t).  In (Lenz g; 0sterberg, 1992 and 6sterberg, 1993) we have shown that the optimal  orthogonal solution to the KL-system are the N pure eigenvectors if the N largest  eigenvalues are all distinct (i.e. Ropt -- I). If some eigenvalues are equal then the  solution is only determined up to an arbitrary rotation of the eigenvectors with  equal eigenvalues. The fourth order term Et{o4(t)} in Q4(t) and Qvar(t) makes it  difficult to derive a closed form solution. The best we can achieve is a numerical  method (in the case of Q4(t) see 0sterberg, 1993) for the computation of the optimal  orthogonal filter functions.  3 Maximization of the quality function  The partial derivatives of QMs(t), Qa(t), Q4(t) and Qvar(t) with respect to Wnm(t)  (the rn ta weight in the n ta filter function at iteration t) are only functions of the  input pattern P(t), the output values O(t): (o(t),. ON(t)) and the previous  values of the weight coefficients (w} (t - 1),.. , w(t -iii within the alter function  (see 0sterberg, 1993). Especially, they are not functions of the internal weights  ((w(t- 1),...,w(t 1)),i y n) of the other filter functions in the system. This  implies that the filter coefficients can be learned in parallel using a system of the  structure shown in Figure 1.  In (0sterberg, 1993) we used on-line optimization techniques based on gradient  ascent. We tried two different methods to select the step length parameter. One  rather heuristical depending on the output o,(t) of the filter function and one  inverse proportional to the second partial derivative of the quality function with  respect to w(t). In each iteration the length of each filter function was explicitly  normalized to one. Currently, we investigate standard unconstrained optimization  methods (Dennis g; Schnabel, 1983) based on batch learning. Now the step length  parameter is selected by line search in the search direction $(t):  max Q(W(t) + ,XS(t)) (11)  Typical choices of S(t) include S(t) = I and S(t) = H-. With the identity matrix  we get Steepest Ascent and with the inverse Hessian the quasi-Newton algorithm.  Using sufficient synchronism the line search can be incorporated in the parallel  structure (Figure 1). To incorporate the quasi-Newton algorithm we have to assume  140 Osterberg and Lenz  )  . Correlatlo  Outplt  -" oJO =  Output  o.,(t) =   Output  o.v(t) - .v'(t)P(t)  Figure 1: The architecture of the filter system  that the Hessian matrix is block diagonal, i.e. the second partial derivatives with  respect to w(t)w?(t),k y l, m are assumed to be zero. In general this is not the  case and it is not clear if a block diagonal approximation is valid or not. The second  partial derivatives can be approximated by secant methods (normally the BFGS  method). Furthermore the condition of normalized filter functions can be achieved  by optin4izing in hyperspherical polar coordinates. Preliminary experiments (mustly  with Steepest Ascent) show that more advanced optimization techniques lead to a  more robust convergence of the filter functions.  4 Experiments  In (6sterberg, 1993) we describe a series of experiments in which we investigate  systematically the following properties of the MS-system, the KL-system and the  FO-system: convergence speed, dependence on initial solution W(0) , distance be-  tween learned solution and optimal (orthogonal) solution, supervised classification  of the extracted feature vectors using linear regression and the degree of selective  response of the learned filter functions. We use training sets with controlled scalar  products between the cluster centers of three classes of input patterns embedded  in a 32-D space. The results of the experiments can be summarized as follows. In  contrast to the MS-system, we noticed that the KL- and FO-system had problems  to converge to the optimal orthogonal solutions for some initial solutions. All sys-  tems learned orthogonal solutions regardless of W(0). The supervised classification  power was independent of the filter system used. Only the FO-system produced  Unsupervised Parallel Feature Extraction from First Principles 141  Table 1: Typical filter response to patterns from (a)-(c) Tsetx and (d) Tset2 using  the filter functions learned with (a) the KL-system, (b) the FO-system and (c)-(d)  the MV-system. (e)-(f) Output covariance matrix using the filter functions learned  with (e) the KL-system and (f) the MV-system.  0.92 , 0.83 , 0.66 0.59 , --0.08 , --0.04  --0.38 0.32 0.14 0.28 0.01 0.97  (b)  --0.91 , --0.39 , --0.23 --0.80 , --0.50 , -0.49  0.44 0.95 0.11 0.50 0.81 0.50  (c) (a)  0.0001 0.9300 0.0000 0.3463 0.3760 --0.3467  0.0005 0.0000 0.0353 --0.3473 -0.3467 0.3814  (e) (f)  filter functions which mainly react for patterns from just one class and only if the  similarity (measured by the scalar product) between the classes in the training set  was smaller than approximately 0.5. Thus, the FO-system extracts feature vectors  which have unsupervised discrimination power. Furthermore, we showed that the  FO-system can distinguish between data sets having identical correlation matrices  (second order statistics) but different fourth order statistics. Recent experiments  with more advanced optimization techniques (Steepest Ascent) show better conver-  gence properties for the KL- and FO-system. Especially the distance between the  learned filter functions and the optimal orthogonal ones becomes smaller.  We will describe some experiments which show that the MV-system is more suitable  for tasks requiring unsupervised classification. We use two training sets Tset and  Tset2. In the first set the mean scalar product between class one and two is 0.7,  between class one and three 0.5 and between class two and three 0.3. In the second  set the mean scalar products between all classes are 0.9, i.e. the angle between all  cluster centers is arccos(0.9) = 26 . In Table 4(a)-(c) we show the filter response of  the learned filter functions with the KL-, FO- and MV-system to typical examples  of the input patterns in the training set Tset. For the KL-system we see that the  second filter function gives the largest magnitude response for both, patterns from  class one and two. For the FO-system the feature vectors are more binary. Still the  first filter function has the largest magnitude response for patterns from class one  and two. For the MV-system we see that each filter function has largest magnitude  response for only one class of input patterns and thus the extracted feature vectors  support unsupervised discrimination. In Table 4(d) (computed from Tseta) we see  that this is the case even then the scalar products between the cluster centers are  as high as 0.9. The filter functions learned by the MV-system are approximately  orthogonal. The system learns thus the rotation of the largest eigenvectors which  maximizes QVar(/). Therefore it will not extract uncorrelated features (see Ta-  142 0stepberg and Lenz  O2  o 15  o. 1  oo  )o  028  02  o15  Ol  oo5  -o I  )o  (b)  Figure 2: (a) Examples of normalized reflectance spectra of typical reddish (solid  curve), greenish (dotted curve) and bluish (dashed curve) Munsell color chips. (b)  The three largest eigenvectors belonging to the correlation matrix of the 1253 dif-  ferent reflectance spectra. (c) The learned filter functions with the MV-system. (d)  The learned non-negative filter functions with the MV-system. In all figures the  x-axes show the wave length (nm)  ble 4(f)) but the variances (e.g. the diagonal elements of the covariance matrix) of  the features are more or less equal. In Table 4(e) we see that the KL-system ex-  tracts uncorrelate features with largely different variance. This demonstrates that  the KL-system tries to learn the pure eigenvectors.  Recently, we have applied the MV-system to real world data. The training set  consists of normalized reflectance spectra of the 1253 different color chips in the  Munsell color atlas. Figure 2(a) shows one typical example of a red, a green and  a blue color chip and Figure 2(b) the three largest eigenvectors belonging to the  correlation matrix of the training set. We see that the first eigenvector (the solid  curve) has a more or less uniform response for all different colors. On the other hand,  the MV-system (Figure 2 (c)) learns one bluish, one greenish and one reddish filter  function. Thus, the filter functions divide the color space according to the primary  colors red, green and blue. We notice that the learned filter functions are orthogonal  and tend to span the same space as the eigenvectors since [[W, ol- tortUeig[[ r -  0.0199 (the Frobenius norm) where Rort maximizes Qvar(t). Figure 2(d) show one  preliminary attempt to include the condition of non-negative filter functions in the  Unsupervised Parallel Feature Extraction from First Principles 143  optimization process (Steepest Ascent). We see that the learned filter functions  are non-negative and divide the color space according to the primary colors. One  possible real word application is optical color analysis where non-negative filter  functions are much easier to realize using optical components. Smoother filter  functions can be optained by incorporating additional constraints into the quality  function.  5 Non-linear extensions  The proposed strategy to extract feature vectors apply to nonlinear filter sys-  tems as well. In this case the input output relation O(t) = W(t)P(t) is replaced  by O(t) -- f(W(t)P(t)) where f describes the desired non-linearity. The corre-  sponding learning rule can be derived using gradient based techniques as long as  the non-linearity f(.) is differenttable. The exact form of f(-) will usually be appli-  cation oriented. Node nonlinearities of sigmoid type are one type of nonlinearities  which has received a lot of attention (see for example Oja  Karhunen, 1993).  Typical applications include: robust Principal Component Analysis PCA (outlier  protection, noise suppression and symmetry breaking), sinusoidal signal detection  in colored noise and robust curve fitting.  Acknowledgements  This work was done under TFR-contract TFR-93-00192. The visit of M. )sterberg  at the Dept. of Info. Tech., Lappeenranta University of Technology was supported  by a grant from the Nordic Research Network in Computer Vision. The Munsell  color experiments were performed during this visit.  References  R. B. Darlington. (1970) Is Kurtosis really peakedhess? American Statistics  24(2):19-20.  J. E. Dennis k Robert B. Schnabel. (1983) Numerical Methods for Unconstrained  Optimization and Nonlinear Equations. Prentice-Hall.  N. Intrator k L.N. Cooper. (1992) Objective Function Formulation of the BCM  Theory of Visual Cortical Plasticity: Statistical Connections, Stability Conditions.  Neural Networks 5:3-17.  R. Lenz k M. )sterberg. (1992) Computing the Karhunen-Loeve expansion with a  parallel, unsupervised filter system. Neural Computations 4(3):382-392.  E. Oja. (1992) Principal Components, Minor Components, and Linear Neural Net-  works. Neural Networks 5:927-935.  E. Oja k J. Karhunen. (1993) Nonlinear PCA: algorithms and Applications Tech-  nical Report A18, Helsinki University of Technology, Laboratory of Computer and  Information Sciences, SF-02150 Espoo, Finland.  M. )sterberg. (1993) Unsupervised Feature Extraction using Parallel Linear Filters.  LinkSping Studies in Science and Technology. Thesis No. 372.  
Structural and Behavioral Evolution  of Recurrent Networks  Gregory M. Saunders, Peter J. Angeline, and Jordan B. Pollack  Laboratory for Artificial Intelligence Research  Department of Computer and Information Science  The Ohio State University  Columbus, Ohio 43210  saunders @cis.ohio-state.edu  Abstract  This paper introduces GNARL, an evolutionary program which induces  recurrent neural networks that are structurally unconstrained. In contrast  to constructive and destructive algorithms, GNARL employs a popula-  tion of networks and uses a fitness function's unsupervised feedback to  guide search through network space. Annealing is used in generating  both gaussian weight changes and structural modifications. Applying  GNARL to a complex search and collection task demonstrates that the  system is capable of inducing networks with complex internal dynamics.  1 INTRODUCTION  A variety of methods to induce network architecture exist. Some start with a very simple  network and incrementally add nodes and links (Hanson 1990; Fahlman & Lebiere, 1990;  Fahlman 1991; Chen, et al., 1993); others start with a large network and then prune off  superfluous pieces (Mozer & Smolensky, 1989; Cun, Denker, and Solla, 1990; Hassibi &  Stork, 1993; Omlin & Giles, 1993). But these constructive and destructive algorithms are  monotonic extremes that ignore a more moderate solution: "dynamically add or remove  pieces of architecture as needed." Moreover, by exclusively exploring either feedforward  networks (e.g., Ash, 1989), fully-connected recurrent networks (e.g., Chen, et al. 1993), or  some restricted middle ground (e.g., Fahlman,. 1991), these algorithms allow only limited  structural change. Finally, constructive and destructive algorithms are supervised methods  88  Structural and Behavioral Evolution of Recurrent Networks 89  num-in input units  Random(max-hidden) hidden units  Random(max-links) links  num-out output units  Figure 1: Sample initial network. The number of input nodes and number of output nodes  is fixed for the particular task, but the number of hidden units and the connectivity  (although bounded), is random.  which rely on complex predicates to determine when to add or delete pieces of network  architecture (e.g., "when rate of improvement falls below threshold").  Genetic algorithms (Holland 1975), on the other hand, are unsupervised methods which can  induce networks by making stochastic modifications to a population of bitstrings, each of  which is interpreted as a network. Most studies, however, still assume a fixed structure for  the network (e.g., Belew et al., 1990; Jefferson, et al., 1991; see also Schaffer, et al. 1992),  and those that do not allow only limited structural change (e.g., Potter, 1992, and Karu-  nanithi et al., 1992).  Evolutionary programming (Fogel, 1992) is an alternate optimization technique which,  when applied to network induction, obviates the need for a bitstring-to-network mapping  by mutating networks directly. Furthermore, because EP does not employ crossover (an  operator of questionable efficacy on distributed representations), it is a better candidate for  inducing network structures (Angeline, Saunders, and Pollack, 1993; Fogel et al., 1990).  2 THE GNARL ALGORITHM  GNARL (GeNeralized Acquisition of Recurrent Links) is an evolutionary program that  non-monotonically constructs recurrent networks to solve a given task. It begins with an  initial population of n random individuals; a sample network N is shown in Figure 1. The  number of input nodes (num-in) and number of output nodes (num-out) are fixed for a given  task; the number of hidden nodes as well as the connections among them are free to vary.  Self-links as well as general loops are allowed. Thus GNARL's search space is N = {N:  network N has num-in input nodes and num-out output nodes }.  In each epoch of search, the networks are ranked by a user-supplied fitness function f.' N  ) R, where R represents the reals. Reproduction of the best n/2 individuals entails modi-  fying both the weights and structure of each parent network N. First, the temperature T(N)  is calculated:  f(N)  T(N) = 1 (1)  fmax  where fmax (provided by the user) is the maximum possible fitness for a given task. This  90 Saunders, Angeline, and Pollack  measure of N's performance is used to anneal the structural and parametric (Barto, 1990)  similarity between parent and offspring, so that networks with a high temperature are  mutated severely, and those with a low temperature are mutated only slightly. This allows  a coarse-grained search initially, and a finer-grained search as a network approaches a solu-  tion (cf. Kirkpatrick et al., 1983).  More concretely, parametric mutations are accomplished by perturbing each weight with  gaussian noise, whose variance is T(N)2:  w <-- w +Normal (0;T(N)), Vw N (2)  Structural mutations are accomplished by:   adding k 1 hidden nodes with probability Padd-node   deleting k 2 hidden nodes with probability Pdelete-node   adding k s links with probability Padd-link   deleting k 4 links with probability Pdelete-link  where each k i is selected uniformly from a user-defined range, again annealed by T(N).  When a node is added, it is initialized without connections; when a node is deleted, all its  incident links are removed. All new links are initialized to 0. (See also Angeline, Saunders,  and Pollack, 1993.)  3 RESULTS  GNARL was tested on a simple control task - the Tracker task of Jefferson, et al. (1991)  and Koza (1992). In this problem, a simulated ant is placed on a two-dimensional toroidal  grid and must maximize the number of pieces of food it collects in a given time period (Fig-  ure 2a). Each ant is controlled by a network with two input nodes and four output nodes  (Figure 2b). At each step, the action whose corresponding output node has maximum acti-  vation is performed. Fitness is the number of grid positions cleared within 200 time steps.  The experiments used a population of 100 networks. In the first run (2090 generations),  GNARL found a network (Figure 3b) that cleared 81 grid positions within the 200 time  steps. Figure 4 shows the state of the output units of the network over three different sets  of inputs. Each point is a triple of the form (move, right, left). (No-op is not shown because  it was never used in the final network.) Figure 4a shows the result of supplying to the net-  work 200 "food" inputs - a fixed point that executes "Move." Figure 4b shows the sequence  of states reached when 200 "no food" signals are supplied to the network - a collection of  points describing a limit cycle of length 5 that repeatedly executes the sequence "Right,  Right, Right, Right, Move." These two attractors determine the response of the network to  the task (Figure 4c,d); the additional points in Figure 4c are transients encountered as the  network alternates between these attractors.  However, not all evolved network behaviors are so simple as to approximate an FSA (Pol-  lack, 1991). In a second run (1595 generations) GNARL induced a network that cleared 82  grid points within the 200 time steps. Figure 5 demonstrates the behavior of this network.  Once again, the "food" attractor, shown in Figure 5a, is a single point in the space that  always executes "Move." The "no food" behavior, however, is not an FSA; instead, it is a  Structural and Behavioral Evolution of Recurrent Networks 91  Move  Turn left Turn right  +1 +1  No-op    Food N o food  (a) (b)  Figure 2: The ant problem. (a) The trail is connected initially, but becomes progressively  more difficult to follow. The underlying 2-d grid is toroidal, so that position "P" is the first  break in the trail. The ellipse indicates the 7 pieces of food that the network of the second  run failed to reach. (b) The semantics of the I/O units for the ant network. The first input  node denotes the presence of food in the square directly in front of the ant; the second  denotes the absence of food in this same square. No-op, from Jefferson, allows the  network to stay in one position while activation flows through recurrent links. This  particular network "eats" 42 pieces of food before spinning endlessly in place at position  P, illustrating a very deep local minimum in the search space.  quasiperiodic trajectory of points shaped like a "D" in output space (Figure 5b). The place-  ment of the "D" is in the "Move / Right" corner of the space and encodes a complex alter-  nation between these two operations (Figure 5d).  4 CONCLUSIONS  Artificial architectural constraints (such as "feedforwardness") close the door on entire  classes of behavior; forced liberties (such as assumed full recurrence) may unnecessarily  increase structural complexity or learning time. By relying on a simple stochastic process,  GNARL strikes a middle ground between these two, allowing the network's complexity  and behavior to emerge in response to the demands of the task.  Acknowledgments  The research reported in this paper has been partially supported by Office of Naval  Research grants N00014-93-1-0059 and N00014-92-J-1195. We are indebted to all those  who read and reviewed this work, especially John Kolen, Ed Large, and Barbara Becker.  92 Saunders, Angeline, and Pollack  I  . F ullv  Fod Noood connecied  (a)  (c)  !  t !  t  t  /  I  !  (b)  Figure 3: The Tracker Task, first run. (a) The best network in the initial population. Nodes  0 & 1 are input, nodes 5-8 are output, and nodes 2-4 are hidden nodes. (b) Network  induced by GNARL after 2090 generations. Forward links are dashed; bidirectional links  & loops are solid. The light gray connection between nodes 8 and 13 is the sole backlink.  This network clears the trail in 319 epochs. (c) Jefferson et al.'s fixed network structure for  the Tracker task.  References  Angeline, P., Saunders, G., Pollack, J. (1993). An evolutionary algorithm that constructs  recurrent neural networks. LAIR Technical Report 93-PA-GNARL, The Ohio State Uni-  versity, Columbus Ohio. To be published in IEEE Transactions on Neural Networks.  Structural and Behavioral Evolution of Recurrent Networks 93  1  (a)  3.  3.  (c)  1  (b)  3  1 2  40  20(  10 2  3O  Time Step  (d)  Figure 4: Limit behavior of the network that clears the trail in 319 steps. Graphs show the  state of the output units Move, Right, Left. (a) Fixed point attractor that results for  sequence of 200 "food" signals; (b) Limit cycle attractor that results when a sequence of  200 "no food" signals is given to network; (c) All states visited while traversing the trail;  (d) The x position of the ant over time when run on an empty grid.  Ash, T. (1989). "Dynamic node creation in backpropagation networks," Connection Sci-  ence, 1:365-375.  Barto, A. G. (1990). Connectionist learning for control. In Miller, W. T. III, Sutton, R. S.,  and Werbos, P. J., editors, Neural Networks for Control. Chapter 1, pages 5-58. MIT Press,  Cambridge.  Belew, R. K., Mclnerney, J., and Schraudolf, N. N. (1990). Evolving networks: Using the  genetic algorithm with connectionist learning. Technical Report CS90-174, University of  California, San Diego.  94 Saunders, Angeline, and Pollack  1  (a) i  i  1  (c)  (b)  0 i 2  0   300  200  3.00  2 /  3O  Time Step  (d)  Figure 5: Limit behavior of the network of the second run. Graphs show the state of the  output units Move, Right, Left. (a) Fixed point attractor that results for sequence of 500  "food" signals; (b) Limit cycle attractor that results when a sequence of 500 "no food"  signals is given to network; (c) All states visited while traversing the trail; (d) The x  position of the ant over time when run on an empty grid.  Chen, D., Giles, C., Sun, G., Chen, H., Less, Y., and Goudreau, M. (1993). Constructive  learning of recurrent neural networks. IEEE International Conference on Neural Networks,  3:1196-1201.  Cun, Y.L., Denker, J., and Solla, S. (1990). Optimal brain damage. In Touretzky, D., editor,  Advances in Neural Information Processing Systems 2. Morgan Kaufmann.  Fahlman, S. and Lebiere, C. (1990). The cascade-correlation architecture. In Touretzky, D.  S., editor, Advances in Neural Information Processing Structures 2, pages 524-532. Mor-  gan Kaufmann.  Fahlman, S. (1991). The recurrent cascade-correlation architecture. In Lippmann, R.,  Structural and Behavioral Evolution of Recurrent Networks 95  Moody, J., and Touretzky, D., editors, Advances in Neural Information Processing Systems  3, pages 190-196. Morgan Kaufmann, San Mateo.  Fogel, D. (1992). Evolving Artificial Intelligence. Ph.D. thesis, University of California,  San Diego.  Fogel, D., Fogel, L., and Porto, V. W. (1990). Evolving neural networks. Biological Cyber-  netics. 63:487-493.  Hanson, S. J. (1990). Meiosis networks. In Touretzky, D., editor, Advances in Neural Infor-  mation Processing Systems 2, pages 533-541. Morgan Kaufmann, San Mateo.  Hassibi, B. and Stork, D. G. (1993). Second order derivatives for network pruning: Optimal  brain surgeon. In Hanson, S. J., Cowan, J. D., and Giles, C. L., editors, Advances in Neural  Information Processing Systems 5, pages 164-171. Morgan Kaufmann.  Holland, J. (1975). Adaptation in Natural and Artificial Systems. The University of Mich-  igan Press, Ann Arbor, MI.  Jefferson, D., Collins, R., Cooper, C., Dyer, M., Flowers, M., Korf, R., Taylor, C., and  Wang, A. (1991). Evolution as a theme in artificial life: The genesys/tracker system. In  Langton, C. G., Taylor, C., Farmer, J. D., and Rasmussen, S., editors, Artificial Life II: Pro-  ceedings of the Workshop on Artificial Life. pages 549-577. Addison-Wesley.  Karunanithi, N., Das, R., and Whitley, D. (1992). Genetic cascade learning for neural net-  works. In Proceedings of COGANN-92 International Workshop on Combinations of  Genetic Algorithms and Neural Networks.  Kirkpatrick, S., Gelatt, C. D., and Vecchi, M.P. (1983). Optimization by simulated anneal-  ing. Science, 220:671-680.  Koza, J. (1992). Genetic evolution and co-evolution of computer programs. In Christopher  G. Langton, Charles Taylor, J. D. F. and Rasmussen, S., editors, Artificial Life II. Addison  Wesley Publishing Company, Reading Mass.  Mozer, M. and Smolensky, P. (1989). Skeletonization: A technique for trimming the fat  from a network via relevance assessment. In Touretzky, D., editor, Advances in Neural  Information Processing Systems 1, pages 107-115. Morgan Kaufmann, San Mateo.  Omlin, C. W. and Giles, C. L. (April 1993). Pruning recurrent neural networks for improved  generalization performance. Technical Report Tech Report No 93-6, Computer Science  Department, Rensselaer Polytechnic Institute.  Pollack, J. B. (1991). The induction of dynamical recognizer. Machine Learning. 7:227-  252.  Potter, M. A. (1992). A genetic cascade-correlation learning algorithm. In Proceedings of  COGANN-92 International Workshop on Combinations of Genetic Algorithms and Neural  Networks.  Schaffer, J. D., Whitley, D., and Eshelman, L. J. (1992). Combinations of genetic algo-  rithms and neural networks: A survey of the state of the art. In Proceedings of COGANN-  92 International Workshop on Combinations of Genetic Algorithms and Neural Networks.  
Connectionist Models for  Auditory Scene Analysis  Richard O. Duda  Department of Electrical Engineering  San Jose State University  San Jose, CA 95192  Abstract  Although the visual and auditory systems share the same basic  tasks of informing an organism about its environment, most con-  nectionist work on hearing to date has been devoted to the very  different problem of speech recognition. We believe that the most  fundamental task of the auditory system is the analysis of acoustic  signals into components corresponding to individual sound sources,  which Bregman has called auditory scene analysis. Computational  and connectionist work on auditory scene analysis is reviewed, and  the outline of a general model that includes these approaches is  described.  1 INTRODUCTION  The primary task of any perceptual system is to tell us about the external world.  The primary problem is that the sensory inputs provide too much data and too little  information. A perceptual system must glean from the flood of incomplete, noisy,  redundant and constantly changing streams of data those invariant properties that  reveal important objects and events in the environment. For humans, the perceptual  systems with the widest bandwidths are the visual system and the auditory system.  There are many obvious similarities and differences between these modalities, and  in addition to using them to perceive different aspects of the physical world, we also  use them in quite different xvays to communicate with one another.  1069  1070 Duda  The earliest neural-network models for vision and hearing addressed problems in  pattern recognition, with optical character recognition and isolated word recogni-  tion among the first engineering applications. However, about twenty years ago  the research goals in vision and hearing began to diverge. In particular, the need  for computers to perceive the external environment motivated vision researchers to  seek the principles and procedures for recovering information about the physical  world from visual data (Marr, 1982; Ballard and Brown, 1982). By contrast, the  vast majority of work on machine audition remained focused on the communica-  tion problem of speech recognition (Morgan and Scofield, 1991; Rabiner and Juang,  1993). While this focus has produced considerable progress, the resulting systems  are still not very robust, and perform poorly in uncontrolled environments. Fur-  thermore, as Richards (1988) has noted, "... Speech, like writing and reading, is  a specialized skill of advanced animals, and understanding speech need not be the  best route to understanding how we interpret the patterns of natural sounds that  comprise most of the acoustic spectrum about us."  In recent years, some researchers concerned with modeling audition have begun to  shift their attention from speech understanding to sound understanding. The inspi-  ration for much of this activity has come from the work of Bregman, whose book  on auditory scene analysis documents experimental evidence for important gestalt  principles that summarize the ways that people group elementary events in fre-  quency/time into sound objects or streams (Bregman, 1990). In this survey paper,  we briefly review this activity and consider its implications for the development of  connectionist models for auditory scene analysis.  2 AUDITORY SCENE ANALYSIS  In vision, Mart (1982) emphasized the importance of identifying the tasks of the  visual system and developing a computational theory that is distinct from partic-  ular algorithms or implementations. The computational theory had to specify the  problems to be solved, the sensory data that is available, and the additional knowl-  edge or assumptions required to solve the problems. Among the various tasks of  the visual system, Mart believed that the recovery of the three-dimensional shapes  of the surfaces of objects from the sensory image data was the most fundamental.  The auditory system also has basic tasks that are more primitive than the recog-  nition of speech. These include (1) the separation of different sound sources, (2)  the localization of the sources in space (3) the suppression of echoes and reverber-  ation, (4) the decoupling of sources from the environment, (5) the characterization  of the sources, and (6) the characterization of the environment. Unfortunately, the  relation between physical sound sources and perceived sound streams is not a sim-  ple one-to-one correspondence. Distributed sound sources, echoes, and synthetic  sounds can easily confuse auditory perception. Nevertheless, humans still do much  better at these six basic tasks than any machine hearing system that exists today.  From the standpoint of physics, the raw data available for performing these tasks  is the pair of acoustic signals arriving at the two ears. From the standpoint of  neurophysiology, the raw data is the activity on the auditory nerve. The nonlinear,  mechano-neural spectral analysis performed by the cochlea converts sound pres-  sure fluctuations into auditory nerve firings. For better or for worse, the cochlea  Connectionist Models for Auditory Scene Analysis 1071  decomposes the signal into many frequency components, transforming it into a fre-  quency/time (or, more accurately, a place/time) spectrogram-like representation.  The auditory system must find the underlying order in this dynamic flow of data.  For a specific case, consider a simple musical mixture of several periodic signals.  Within its limits of resolution, the cochlea decomposes each individual signal into  its discrete harmonic components. Yet, under ordinary circumstances, we do not  hear these components as separate sounds, but rather we fuse them into a single  sound having, as musicians say, its particular timbre or tone color. However, if  there is something distinctive about the different signals (such as different pitch or  different modulation), we do not fuse all of the sounds together, but rather hear the  separate sources, each with its own timbre.  What information is available to group the spectral components into sound streams?  Hartmann (1988) identifies the following factors that influence grouping: (1) com-  mon onset/offset, (2) common harmonic relations, (3) common modulation, (4)  common spatial origin, (5) continuity of spectral envelope, (6) duration, (7) sound  pressure level, and (8) context. These properties are easier to name than to pre-  cisely specify, and it is not surprising that no current model incorporates them all.  However, several auditory scene analysis systems have been built that exploit some  subset of these cues (Weintraub, 1985; Cooke, 1993; Mellinger, 1991; Brown, 1992;  Brown and Cooke, 1993; Ellis, 1993). Although these are computational rather  than connectionist models, most of them at least find inspiration in the structure  of the mammalian auditory system.  3 NEURAL AND CONNECTIONIST MODELS  The neural pathways from the cochlea through the brainstem nuclei to the auditory  cortex are complex, but have been extensively investigated. Although this system  is far from completely understood, neurons in the brainstem nuclei are known to be  sensitive to various acoustic features -- onsets, offsets and modulation in the dorsal  cochlear nucleus, interaural time differences (ITD's) in the medial superior olive  (MSO), interaural intensity differences (IID's) in the lateral superior olive (LSO),  and spatial location maps in the inferior colliculus (Pickles, 1988).  Both functional and connectionist models have been developed for all of these func-  tions. Because it is both important and relatively well understood, the cochlea  has received by far the most attention (Allen, 1985). As a result of this work, we  now have real-time implementations for some of these models as analog VLSI chips  (Lyon and Mead, 1988; Lazzaro et al., 1993). Connectionist models for sound local-  ization have also been extensively explored. Indeed, one of the earliest of all neural  network models was Jeffress's classic crosscorrelation model (Jefftess, 1948), which  was hypothesized forty years before neural crosscorrelation structures were actually  found in the barn owl (Cart and Konishi, 1988). Models have subsequently been  proposed for both the LSO (Reed and Blum, 1990) and the MSO (Hah and Col-  burn, 1991). Mathematically, both the ITD and IID cues for binaural localization  are exposed by crosscorrelation. Lyon showed that crosscorrelation can also be used  to separate as well as localize the signals (Lyon, 1983). VLSI crosscorrelation chips  can provide this information in real time (Lazzaro and Mead, 1989; Bhadkamkar  and Fowler, 1993).  1072 Duda  While interaural crosscorrelation can determine the azimuth to a sound source,  full three-dimensional localization also requires the determination of elevation and  range. Because of a lack of symmetry in the orientation of its ears, the barn owl can  actually determine azimuth from the ITD and elevation from the IID. This at least  in part explains why it has been such a popular choice for connectionist modeling  (Spence et al., 1990; Moiseff et al., 1991; Palmieri et al., 1991; Rosen, Rumelhart  and Knudsen, 1993). Unfortunately, the localization mechanisms used by humans  are more complicated.  It is well known that humans use monaural, spectral shape cues to estimate elevation  in the median sagittal plane (Blauert, 1983; Middlebrooks and Green, 1991), and  source localization models based on this approach have been developed (Neti, Young  and Schneider, 1992; Zakarauskas and Cynander, 1993). The author has shown that  there are strong binaural cues for elevation at short distances away from the median  plane, and has used statistical methods to estimate both azimuth and elevation  accurately from IID data alone (Duda, 1994). In addition, backprop models have  been developed that can estimate azimuth and elevation from IID and ITD inputs  jointly (Backman and Karjalainen 93; Anderson, Gilkey and Janko, 1994).  Finally, psychologists have long been aware of an important reverberation-  suppression phenomenon known as the precedence effect or the law of the first  wavefront (Zurek, 1987). It is usually summarized by saying that echoes of a sound  source have little effect on its localization, and are not even consciously heard if  they are not delayed more than the so-called echo threshold, which ranges from  5-10 ms for sharp clicks to more than 50 ms for music. It is generally believed  that the precedence effect can be accounted for by contralateral inhibition in the  crosscorrelation process, and Lindemann has accounted for many of the phenomena  by a conceptually simple connectionist model (Lindemann, 1986).  However, Clifton and her colleagues have found that the echoes are indeed heard  if the timing of the echoes suddenly changes, as might happen when one moves  from one acoustic environment into another one (Clifton 1987; Freyman, Clifton  and Litovsky, 1991). Clifton conjectures that the auditory system is continually  analyzing echo patterns to model the acoustic environment, and that the resulting  expectations modify the echo threshold. This suggests that simple crosscorrelation  models will not be adequate when the listener is moving, and thus that even the  localization problem is still unsolved.  4 ARCHITECTURE OF AN AUDITORY MODEL  If we look back at the six basic tasks for the auditory system, we see that only one  (source localization) has received much attention from connectionist researchers,  and its solution is incomplete. In particular, current localization models cannot  handle multiple sources and cannot cope with significant room echoes and reverber-  ation. The common problem for all of the basic tasks is that of source separation,  which only the auditory scene analysis systems have addressed.  Fig. 1 shows a fimctional block diagram for a hypothetical auditory model that  coinbines the computational and connectionist models and has the potential of  coping with a multisource environment. The inputs to the model are the left-ear  Connectionist Models for Auditory Scene Analysis 1073  and right-ear signals, and the main output is a dynamic set of streams. The system  is primarily data driven, although low-bandwidth efferent paths could be added for  tasks such as automatic gain control.  Data flow on the left half of the diagram is monaural, and dataflow of the right  half is binaural. The binaural processing is based on crosscorrelation analysis of  the cochlear outputs. The author has shown that interaural differences not only  effective in determining azimuth, but can also be used to determine elevation as well  (Duda, 1994). We have chosen to follow Slaney and Lyon (Slaney and Lyon, 1993)  in basing the monaural analysis on autocorrelation analysis. Originally proposed  by Licklider (1951) to explain pitch phenomena, autocorrelation is a biologically  plausible operation that supports the common onset, modulation and harmonicity  analysis needed for stream formation (Duda, Lyon and Slaney, 1990; Brown and  Cooke, 1993).  While the processes at lower levels of this diagram are relatively well understood,  the process of stream formation is problematic. Bregman (1990) has posed this  problem in terms of grouping the components of the "neural spectrogram" in both  frequency and time. He has identified two principles that seem to be employed  in stream formation: exclusive allocation (a component may not be used in more  than one description at a time) and accounting (all incoming components must be  assigned to some source). The various auditory scene analysis systems that we  mentioned earlier provide different mechanisms for exploiting these principles to  form auditory streams. Unfortunately, the principles admit of exceptions, and the  existing implementations seem rather ad hoc and arbitrary. The development of a  biologically plausible model for stream formation is the central unsolved problem  for connectionist research in audition.    Acoustic Environment  Short-Term Stream Formation  Model  Auditory Memory  Binaural Localization  Analysis Analysis  Harmonicity Intensity Time Binaural Maps  Menaural Maps Analysis Differences Difference  Auto-Correlation Cross-Correlation  Analysis Analysis  Spectral Analysis Spectral Analysis  (Cochlear Model) (Cochloar Model)  I  Left Input Right input  Figure 1: Block diagram for a basic auditory model  1074 Duda  Acknowledgements  This work was supported by the National Science Foundation under NSF Grant  No. IRI-9214233. This paper could not have been written without the many dis-  cussions on these topics with A1 Bregman, Dick Lyon, David Mellinger, Bernard  MontReynaud, John R. Pierce, Malcolm Slaney and J. Martin Tenenbaum, and  from the stimulating CCRMA Hearing Seminar at Stanford University that Bernard  initiated and that Malcolm has maintained and invigorated.  References  Allen, J. B. (1985). "Cochlear modeling," IEEE ASSP Magazine, vol. 2, pp. 3-29.  Anderson, T. R., R. H. Gilkey and J. A. Janko (1994). "Using neural networks to  model human sound localization," in T. Anderson and R. H. Gilkey (eds.), Binaural  and Spatial Hearing. Hillsdale, NJ: Lawrence Erlbaum Associates.  Backman, J. and M. Karjalainen (1993). "Modelling of human directional and  spatial hearing using neural networks," ICASSPg$, pp. 1-125-1-128. (Minneapolis,  MN).  Bhadkamkar, N. and B. Fowler (1993). "A sound localization system based on  biological analogy," 1993 IEEE International Conference on Neural Networks,  pp. 1902-1907. (San Francisco, CA).  Ballard, D. H. and C. M. Brown (1982). Computer Vision. Englewood Cliffs, NJ:  Prentice-Hall.  Blauert, J. P. (1983). Spatial Hearing. Cambridge, MA: MIT Press.  Bregman, A. S. (1990). Auditory Scene Analysis. Cambridge, MA: MIT Press,  1990.  Brown, G. J. (1992). "Computational auditory scene analysis: A representa-  tional approach," PhD dissertation, Department of Computer Science, University  of Sheffield, Sheffield, England, UK.  Brown, G. J. and M. Cooke (1993). "Physiologically-motivated signal representa-  tions for computational auditory scene analysis," in M. Cooke, S. Beet and M. Craw-  ford (eds.), Visual Representations of Speech Signals, pp. 181-188. Chichester, Eng-  land: John Wiley and Sons.  Carr, C. E. and M. Konishi (1988). "Axonal delay lines for time measurement in  the owl's brainstem," Proc. Nat. Acad. Sci. USA, vol. 85, pp. 8311-8315.  Clifton, R. K. (1987). "Breakdown of echo suppression in the precedence effect,"  J. Acoust. Soc. Am., vol. 82, pp. 1834-1835.  Cooke, M.P. (1993). Modelling Auditory Processing and Organisation. Cambridge,  UK: Cambridge University Press.  Duda, R. O., R. F. Lyon and M. Slaney (1990). "Correlograms and the separation  of sounds," Proc. 2.th Asilomar Conf. on Signals, Systems and Computers, pp. 457-  461 (Asilomar, CA).  Connectionist Models for Auditory Scene Analysis 1075  Duda, R. O. (1994). "Elevation dependence of the interaural transfer function," in  T. Anderson and R. H. Gilkey (eds.), Binaural and Spatial Hearitg. Hillsdale, NJ'  Lawrence Erlbaum Associates.  Ellis, D. P. W. (1993). "Hierarchic models of hearing for sound separation and re-  construction," 1995 IEEE Workshop 07, Applications of Signal Processing to Audio  and Acoustics.  Freyman, R. L., R. K. Clifton and R. Y. Litovsky (1991). "Dynamic processes in  the precedence effect," J. Acoust. Soc. Am., vol. 90, pp. 874-884.  Han, Y. and H. S. Colburn (1991). "A neural cell model of MSO," Proc. 1991 IEEE  Seventeenth Annual Northeast Bioenginering Conference, pp. 121-122 (Hartford,  CT).  Hartmann, W. A. (1988). "Pitch perception and the segregation and integration of  auditory entities," in G. M. Edelman, W. E. Gail and W. M. Cowan (eds.), Auditory  Function. New York, NY: John Wiley and Sons, Inc.  Jefftess, L. A. (1948). "A place theory of sound localization," J. Comp. Phys-  iol. Psychol., vol. 41, pp. 35-39.  Lazzaro, J. and C. A. Mead (1989). "A silicon model of auditory localization,"  Neural Computation, vol. 1, pp. 47-57.  Lazzaro, J., J. Wawrzynek, M. Mahowald, M. Sivilotti and D. Gillespie (1993). "Sil-  icon auditory processors as computer peripherals," IEEE Transactions on Neural  Networks, vol. 4, pp. 523-528.  Licklider, J. C. R. (1951). "A duplex theory of pitch perception," Ezperentia, vol. 7,  pp. 128-133.  Lindemann, W. (1986). "Extension of a binaural cross-correlation model  by contralateral inhibition. I. Simulation of lateralization for stationary sig-  nals,"J. Acoust. Soc. Am., vol. 80, pp. 1608-1622; II. The law of the first wave  front," J. Acoust. Soc. Am., vol. 80, pp. 1623-1630.  Lyon, R. F. (1983). "A computational model of binaural localization and separa-  tion," ICASSP83, pp. 1148-1151. (Boston, MA).  Lyon, R. F. and C. Mead (1988). "An analog electronic cochlea," IEEE  Trans. Acoustics, Speech and Signal Processing, vol. 36, pp. 1119-1134.  Mart, D. (1982). Vision. San Francisco, CA: W. H. Freeman and Company.  Mellinger, D. K. (1991). "Event formation and separation of musical sound,"  PhD dissertation, Department of Music, Stanford University, Stanford, CA; Report  No. STAN-M-77, Center for Computer Research in Music and Acoustics, Stanford  University, Stanford, CA.  Middlebrooks, J. C. and D. M. Green (1991). "Sound localization by human listen-  ers," Anna. Rev. Psychol., vol. 42, pp. 135-159.  Moiseff, A. et al. (1991). "An artificial neural network for studying binaural sound  localization," Proc. 1991 IEEE Seventeenth Annual Northeast Bioengineering Con-  ference, pp. 1-2 (Hartford, CT).  1076 Duda  Morgan, D. P. and C. L. Scofield (1991). Neural Networks and Speech Processing.  Boston, MA: Kluwer Academic Publishers.  Neti, C., E. D. Young and M. H. Schneider (1992). "Neural network models of  sound localization based on directional filtering by the pinna," J. Acoust. Soc. Am.,  vol. 92, pp. 3140-3156.  Palmieri, F., M. Datum, A. Shah and A. Moiseff (1991). "Sound localization  with a neural network trained with the multiple extended Kalman algorithm,"  Proc. Int. Joint Conf. on Neural Networks, pp. I125-I131 (Seattle, WA).  Pickles, James O. (1988). An Introduction to the Physiology of Hearing, 2nd edition.  London, Academic Press, 1988.  Rabiner, L. and B-H Juang (1993). Fundamentals of Speech Recognition. Engel-  wood Cliffs, N J: Prentice-Hall.  Reed, M. C. and J. J. Blum (1990). "A model for the computation and encoding of  azimuthal information by the lateral superior olive," J. Acoust. Soc. Am., vol. 88,  pp. 1442-1453.  Richards, W. (1988). "Sound interpretation," in W. Richards (ed.), Natural Com-  putation, pp. 301-308. Cambridge, MA: MIT Press.  Rosen, D. , D. Rumelhart and E. Knudsen (1993). "A connectionist model of the  owl's localization system," in J. D. Cowan, G. Tesauro and J. Alspector (eds.),  Advances in Neural Information Processing Systems t7. San Francisco, CA: Morgan  Kaufmann Publishers.  Slaney, M. and R. F. Lyon (1993). "On the importance of time -- A temporal  representation of sound," in M. Cooke, S. Beet and M. Craxvford (eds.), Visual  Representations of Speech Signals, pp. 95-116. Chichester, England: John Wiley  and Sons.  Spence, C. D. and J. C. Pearson (1990). "The computation of sound source eleva-  tion in the barn owl," in D. S. Touretzsky (ed.), Advances in Neural Information  Processing Systems 2, pp. 10-17. San Mateo, CA: Morgan Kaufmann.  Weintraub, M. (1985). "A theory and computational model of auditory monaural  sound separation," PhD dissertation, Department of Electrical Engineering, Stan-  ford University, Stanford, CA.  Zakarauskas, P. and M. S. Cynander (1993). "A computational theory of spectral  cue localization," J. Acoust. Soc. Am., vol. 94, pp. 1323-1331.  Zurek, P.M. (1987). "The precedence effect," in W. A. Yost and G. Gourevitch  (eds.) Directional Hearing, pp. 85-106. New York, NY: Springer Verlag.  
I II  Learning Mackey-Glass from 25  examples, Plus or Minus 2  Mark Plutowski* Garrison Cottrell* Halbert White**  Institute for Neural Computation  *Department of Computer Science and Engineering  **Department of Economics  University of California, San Diego  La Jolla, CA 92093  Abstract  We apply active exemplar selection (Plutowski & White, 1991;  1993) to predicting a chaotic time series. Given a fixed set of ex-  amples, the method chooses a concise subset for training. Fitting  these exemplars results in the entire set being fit as well as de-  sired. The algorithm incorporates a method for regulating network  complexity, automatically adding exemplars and hidden units as  needed. Fitting examples generated from the Mackey-Glass equa-  tion with fractal dimension 2.1 to an rmse of 0.01 required about 25  exemplars and 3 to 6 hidden units. The method requires an order  of magnitude fewer floating point operations than training on the  entire set of examples, is significantly cheaper than two contend-  ing exemplar selection techniques, and suggests a simpler active  selection technique that performs comparably.  I Introduction  Plutowski & White (1991; 1993), have developed a method of active selection of  training exemplars for network learning. Active selection uses information about  the state of the network when choosing new exemplars. The approach uses the sta-  tistical sampling criterion Integrated Squared Bias (ISB) to derive a greedy selection  method that picks the training example maximizing the decrement in this measure.  (ISB is a special case of the more familiar Integrated Mean Squared Error in the  case that noise variance is zero.) We refer to this method as AISB. The method  automatically regulates network complexity by growing the network as necessary  1135  1136 Plutowski, Cottrell, and White  to fit the selected exemplars, and terminates when the model fits the entire set of  available examples to the desired accuracy. Hence the method is a nonparametric  regression technique. In this paper we show that the method is practical by apply-  ing it to the Mackey-Glass time series prediction task. We compare AISB with  the method of training on all the examples. AISB consistently learns the time  series from a small subset of the available examples, finding solutions equivalent  to solutions obtained using all of the examples. The networks obtained by AISB  consistently perform better on test data for single step prediction, and do at least  as well at iterated prediction, but are trained at much lower cost.  Having demonstrated that this particular type of exemplar selection is worthwhile,  we compare AISB with three other exemplar selection methods which are easier  to code and cost leas to compute. We compare the total cost of training, as well  as the size of the exemplar sets selected. One of the three contending methods was  suggested by the AISB algorithm, and is also an active selection technique, as its  calculation involves the network state. Among the four exemplar selection methods,  we find that the two active selection methods provide the greatest computational  savings and select the most concise training sets.  2 The Method  We are provided with a set of N "candidate" examples of the form (zi,g(zi)).  Given g, we can denote this as z v. Let f(-, w) denote the network function pa-  rameterized by weights w. For a particular subset of the examples denoted z n, let  w, = wn (z ') minimize  - w))  Let w* be the "best" set of weights, which minimizes  (g(x) - f(x, w'))2p(dx),  where  is the distribution over the inputs. Our objective is to select a subset x"  of x v such that n : N, while minimizing f(f(z, w,)- f(x, w*))2p(dx). Thus,  we desire a subset representative of the whole set. We choose the z" C zv giving  weights wn that minimize the Integrated Squared Bias (ISB):  ISB(x ') =/(g(z)- f(z,  (1)  We generate z ' incrementally. Given a candidate example ,+, let "+ =  (x n, ,+). Selecting x I optimally with respect to (1) is straightforward. Then  given x" minimizing ISB(x"), we opt to select x,+  zv maximizing ISB(x") -  ISB(: "+). Note that using this property for ,+ will not necessarily deliver the  globally optimal solution. Nevertheless, this approach permits a computationally  feasible and attractive method for sequential selection of training examples.  Learning Mackey-Glass from 25 Examples, Plus or Minus 2 1137  Choosing z.+i to maximize this decrement directly is expensive. We use the follow-  ing simple approximation (see Plutowski & White, 1991) for justification): Given  z ', select z,+ ( arg max,+iAISB(5:n+i Ix"), where  and  N  ats(i.+, =/'k'tbn+l' Z Vwf(zi, wn)(g(,r,) -- f(a&, ton)),  i=1  A'ff.,n+' -- (g(5,+x)- f(.+i , w,))'V,f(,+x, wn)'[H(x',w.)] -1,  = y]. vwf(,,  In practice we approximate H appropriately for the task at hand. Although we  arrive at this criterion by making use of approximations valid for large n, this crite-  rion has an appealing interpretation as picking the single example having individual  error gradient most highly correlated with the average error gradient of the entire  set of examples. Learning with this example is therefore likely to be especially in-  formative. The AISB criterion thus possesses heuristic appeal in training sets of  any size.  3 The Algorithm  Before presenting the algorithm we first explain certain implementation details. We  integrated the AISB criterion with a straightforward method for regulating network  complexity. We begin with a small network nd an initial training set composed of  a single exemplar. When a new exemplar is added, if training stalls, we rndomize  the network weights and restart training. After 5 stalls, we grow the network by  adding another unit to each hidden layer.  Before we can select a new exemplar, we require that the network fit the current  training set "sufficiently well." Let en( m) measure the rmse (root mean squared  error) network fit over rn arbitrary examples m when trained on z n. Let F,  +  denote the rmse fit we require over the current set of n exemplars before selecting  a new one. Let FN  + denote the rmse fit desired over all N examples. (Our  goal is en(x N) _ F.) It typically suffices to set Fn = Fv, that is, to train to a fit  over the exemplars which is at least as stringent as the fit desired over the entire  set (normalized for the number of exemplars.) However,. active selection sometimes  chooses a new exemplar "too close" to previously selected exemplars even when this  is the case. This is easy to detect, and in this case we reject the new exemplar and  continue with training.  We use an "exemplar spacing"parameter d to detect when a new exemplar is too  close to a previous selection. Two examples zi and zj are "close"in this sense if  they are within Euclidean distance d, and if additionally [g(z)-(zj)[ _ Fv. The  additional condition allows the new exemplar to be accepted even when it is close to  a previous selection in input space, provided it is sufficiently far away in the output  space. In our experiments, the input and output space are of the same scale, so we  set d = Ft. When a new selection is too close to a current exemplar, we reject the  1138 Plutowski, Cottrell, and White  new selection, reduce F, by 20%, and continue training, resetting Fn = FN when  a subsequent selection is appended to the current training set. We now outline the  algorithm:  Initialize:   Specify user-set parameters: initial network size, the desired fit Fv, the  exemplar spacing parameter, and the maximum number of restarts.   Select the first training set, x = {x}. Set n = 1 and Fn = FN. Train the  network on x until en(x l) _< F,.  While(e.(x v) > F.) {  Select a new exemplar, zn+  z v, maximizing AISB.  If (z.+ is "too close" to any z 6 z") {  Reject  Reduce F. by 20%. }  Else {  Append Zn+ to z n.  Increment n.  Set F, = Fv. }  While(en (z ) > F) {  Train the network on the current training set  restarting and growing as necessary. }}  4 The Problem  We generated the data from the Mackey-Glass equation (Mackey & Glass, 1977),  with r = 17, a = 0.2, and b = 0.1. We integrated the equation using fourth order  Runge-Kutta with step size 0.1, and the history initialized to 0.5. We generated two  data sets. We iterated the equation for 100 time steps before beginning sampling;  this marks t = 0. The next 1000 time steps comprise Data Set 1. We generated  Data Set 2 from the 2000 examples following t = 5000.  We used the standard feed-forward network architecture with [0, 1] sigmoids and one  or two hidden layers. Denoting the time series as x(t), the inputs were x(t), z(t -  6), z(/- 12), z(t- 18), and the desired output is z(t +6) (Lavedes & Farbet, 1987).  We used conjugate gradient optimization for all of the training runs. The line search  routine typically required 5 to 7 passes through the data set for each downhill step,  and was restricted to use no more than 10.  Initgaily, the single hidden layer network has a single hidden unit, and the 2 hidden  layer network has 2 units per hidden layer. A unit is added to each hidden layer  when growing either architecture. All methods use the same growing procedure.  Thus, other exemplar selection techniques are implemented by modifying how the  next training set is obtained at the beginning of the outer while loop. The method  of using all the training examples uses only the inner while loop.  In preliminary experiments we evaluated sensitivity of AISB to the calculation of  H. We compared two ways of estimating H, in terms of the number of exemplars  Learning Mackey-Glass from 25 Examples, Plus or Minus 2 1139  selected and the total cost of training. The first approach uses the diagonal terms  of H (Plutowski & White, 1993). The second approach replaces H with the identity  matrix. Evaluated over 10 separate runs, fitting 500 examples to an rmse of 0.01,  AISB gave similar results for both approaches, in terms of total computation used  and the number of exemplars selected. Here, we used the second approach.  5 The Comparisons  We performed a number of experiments, each comparing the AISB algorithm with  competing training methods. The competing methods include the conventional  method of using all the examples, henceforth referred to as "the strawman," as well  as three other data selection techniques. In each comparison we denote the cost  as the total number of floating point multiplies (the number of adds and divides is  always proportional to this count).  For each comparison we ran two sets of experiments. The first compares the total  cost of the competing methods as the fit requirement is varied between 0.02, 0.015,  and 0.01, using the first 500 examples from Data Set 1. The second compares the  cost as the size of the "candidate" set (the set of available examples) is varied using  the first 500, 625, 750, 875, and 1000 examples of Data Set 1, and a tolerance of  0.01. To ensure that each method is achieving a comparable fit over novel data,  we evaluated each network over a test set. The generalization tests also looked at  the iterated prediction error (IPE) over the candidate set and test set (Lapedes &  Farbet, 1987). Here we start the network on the first example from the set, and  feed the output back into the network to obtain predictions in multiples of 6 time  steps. Finally, for each of these we compare the final network sizes. Each data point  reported is an average of five runs. For brevity, we only report results from the two  hidden layer networks.  6 Comparison With Using All the Examples  We first compare AISB with the conventional method of using all the available  examples, which we will refer to as "the strawman." For this test, we used the first  500 examples of Data Set 1. For the two hidden layer architecture, each method  required 2 units per hidden layer for a fit of 0.02 and 0.015 rmse, and from 3 to  4 (typically 3) units per hidden layer for a fit of 0.01 rmse. While both methods  did quite well on the generalization tests, AISB clearly did better. Whereas the  strawman networks do slightly worse on the test set than on the candidate set,  networks trained by AISB tended to give test set fits close to the desired (training)  fit. This is partially due to the control flow of the algorithm, which often fits the  candidate set better than necessary. However, we also observed AISB networks  exhibited a test set fit better than the candidate set fit 7 times over these 15 training  runs. This never occurred over any of the strawman runs.  Overall, AISB networks performed at least as well as the strawman with respect to  IPE. Figure la shows the second half of Data Set 1, which is novel to this network,  plotted along with the iterated prediction of a AISB network to a fit of 0.01, giving  an IPE of 0.081 rmse, the median IPE observed for this set of five runs. Figure lb  shows the iterated prediction over the first 500 time steps of Data Set 2, which is  1140 Plutowski, Cottrell, and White  4500 time steps later than the training set. The IPE is 0.086 rmse, only slightly  worse than over the "nearer" test set. This fit required 22 exemplars. Generalization  tests were excellent for both methods, although AISB was again better overall.  AISB networks performed better on Data Set 2 than they did on the candidate  set 9 times out of the 25 runs; this never occurred for the strawman. These effects  demand closer study before using them to infer that data selection can introduce a  beneficial bias. However, they do indicate that the AISB networks performed at  least as well as the strawman, ensuring the validity of our cost comparisons.  1  o.  500  5100 5200 5'00 5400 55'0 I ->  Figure 1: Iterated prediction for a 2 hidden layer network trained to 0.01 rmse over the  first 500 time steps of Data Set 1. The dotted line gives the network prediction; the solid  line is the target time series. Figure la, on the left, is over the next (consecutive) 500 time  steps of Data Set 1, with IPE = 0.081 rinse. Figure lb, on the right, is over the first 500  steps of Data Set 2, with IPE = 0.086 rmse. This network was typical, being the median  IPE of 5 runs.  Figure 2a shows the average total cost versus required fit Fv for each method.  The strawman required 109, 115, and 4740 million multiplies for the respective  tolerances, whereas AISB required 8, 28, and 219 million multiplies, respectively.  The strawman is severely penalized by a tighter fit because growing the network  to fit requires expensive restarts using all of the examples. Figure 2b shows the  average total cost versus the candidate set sizes. One reason for the difference is  that AISB tended to select smaller networks. For cndidate sets of size 500, 625,  750 and 875, each method typically required 3 units per hidden layer, occasionally  4. Given 1000 examples, the strawman selected networks larger thn 3 hidden units  per layer over twice as often as AISB. AISB also never required more than 4  hidden units per layer, while the strawman sometimes required 6. This suggests  that the growing technique is more likely to fit the data with a smaJler network  when exemplar selection is used.  CoC  7000  6000  5000  4000  3000  2000  1000  0.02  CoC  35000  30000  20000  S= 15000  10000  5000  0.015 0.01  500 625 750 875 1000  N  Figure 2: Cost (in millions of multiplies) of training AISB, compaxed to the Strawman.  Figure 2a on the left gives total cost versus the desired fit, and Figure 2b on the right  gives totM cost versus the number of candidate exaxnples. Ech point is the avera4e of 5  runs; the error bars are equal in width to twice the standard deviation.  Learning Mackey-Glass from 25 Examples, Plus or Minus 2 1141  7 Contending Data Selection Techniques  The results above clearly demonstrate that exemplar selection can cut the cost of  training dramatically. In what follows we compare AISB with three other exemplar  selection techniques. Each of these is easier to code and cheaper to compute, and  are considerably more challenging contenders than the strawman. In addition to  comparing the overall training cost we will also evaluate their data compression  ability by comparing the size of the exemplar sets each one selects. We proceed in  the same manner as with AISB, sequentially growing the training set as necessary,  until the candidate set fit is as desired.  Two of these contending techniques do not depend upon the state of the network,  and are therefore are not "Active Selection" methods. Random Selection selects an  example randomly from the candidate set, without replacement, and appends it to  the current exemplar set. Uniform Grid exploits the time series representation of  our data set to select training sets composed of exemplars evenly spaced at regular  intervals in time. Note that Uniform Grid does not append a single exemplar to  the training set, rather it selects an entirely new set of exemplars each time the  training set is grown. Note further that this technique relies heavily upon the time  series representation. The problem of selecting exemplars uniformly spaced in the  4 dimensional input space would be much more difficult to compute.  The third method, "Mazimum Error," was suggested by the AISB algorithm, and  is also an Active Selection technique, since it uses the network in selecting new  exemplars. Note that the error between the network and the desired value is a  component of the AISB criterion. AISB need not select an exemplar for which  network error is maximum, due to the presence of terms involving the gradient  of the network function. In comparison, the Maarimum Error method selects an  exemplar maximizing network error, ignoring gradient information entirely. It is  cheaper to compute, typically requiring an order of magnitude fewer multiplies in  overhead cost as compared to AISB. This comparison will test, for this particular  learning task, whether the gradient information is worth its additional overhead.  7.1 Comparison with Random Selection  Random Selection fared the worst among the four contenders. However, it still  performed better overall than the strawman method. This is probably because the  cost due to growing is cheaper, since early on restarts are performed over small  training sets. As the network fit improves, the likelihood of randomly selecting  an informative exemplar decreases, and Random Selection typically reaches a point  where it adds exemplars in rapid succession, often doubling the size of the exemplar  set in order to attain a slightly better fit. Random Selection also had a very high  variance in cost and number of exemplars selected.  7.2 Comparison with Uniform Grid and Maximum Error  Uniform Grid and Mazimum Error are comparable with AISB in cost as well as  in the size of the selected exemplar sets. Overall, AISB and Mazimum Error  performed about the same, with Uniform Grid finishing respectably in third place.  Maximum Error was comparable to AISB in generalization also, doing better on  the test set than on the candidate set 10 times out of 40, whereas AISB did so a  1142 Plutowski, Cottrell, and White  total of 16 times. This occurred only 3 times out of 40 for Uniform Grid.  Figure 3a shows that Uniform Grid requires more exemplars at all three tolerances,  whereas AISB and Maximum Errorselect about the same number. Figure 3b shows  that Uniform Grid typically requires about twice as many exemplars as the other  two. Maximum Error and AISB selected about the same number of exemplars,  typically selecting about 25 exemplars, plus or minus two.  n  5O  4O  3O  2O  10   Max Er  Delta ISB  Rrns  6O  5O  4O  3O  2O  10  1  Delt ISB  Mx  ro7--  0.02 0.015 0.02 500 625 750 875 10'00 N  Figure 3: Number of examples selected by three contending selection techniques: Uni-  form, AISB (diamonds) and Max Error (triangles.) Figure 3a on the left gives number of  examples selected versus the desired fit, and Figure 3b on the right is versus the number  of candidate examples. The two Active Selection techniques selected about 25 exemplars,  +2. Each point is the average of 5 runs; the error bars are equal in width to twice the  standard deviation. The datapoints for /XlSB and Max Error are shifted slightly in the  graph to make them easier to distinguish.  8 Conclusions  These results clearly demonstrate that exemplar selection can dramatically lower  the cost of training. This particular learning task also showed that Active Selection  methods are better overall than two contending exemplar selection techniques.  AISB and Maximum Error consistently selected concise sets of exemplars, reducing  the total cost of training despite the overhead associated with exemplar selection.  This particular learning task did not provide a clear distinction between the two  Active Selection techniques. Maximum Error is more attractive on problems of this  scope even though we have not justified it analytically, as it performs about as well  as AISB but is easier to code and cheaper to compute.  Acknowledgements  This work was supported by NSF grant IRI 92-03532.  References  Lapedes, Alan, and Robert Farbet. 1987. "Nonlinear signal processing using neural net-  works. Prediction and system modelling." Los Alamos technical report LA-UR-87-2662.  Mackey, M.C., and L. Glass. 1977. "Oscillation and chaos in physiological control sys-  tems." Science 197, 287.  Plutowsld, Mark E., and Halbert White. 1991. "Active selection of training examples for  network learning in noiseless environments." Technical Report No. CS91-180, CSE Dept.,  UCSD, La Jolla, California.  Plutowski, Mark E., and Halbert White. 1993. "Selecting concise training sets from clean  data." To appear, IEEE Transactions on Neural Networks. 3, 1.  
Learning Temporal Dependencies in  Connectionist Speech Recognition  Steve Renals Mike Hochberg Tony Robinson  Cambridge University Engineering Department  Cambridge CB2 1PZ, UK  {S5 r,mmh, a5 r}@en. cam. ac.uk  Abstract  Hybrid connectionist/HMM systems model time both using a Markov  chain and through properties of a connectionist network. In this paper,  we discuss the nature of the time dependence currently employed in our  systems using recurrent networks (RNs) and feed-forward multi-layer  perceptrons (MLPs). In particular, we introduce local recurrences into a  MLP to produce an enhanced input representation. This is in the form  of an adaptive gamma filter and incorporates an automatic approach for  learning temporal dependencies. We have experimented on a speaker-  independent phone recognition task using the TIMIT database. Results  using the gamma filtered input representation have shown improvement  over the baseline MLP system. Improvements have also been obtained  through merging the baseline and gamma filter models.  1 INTRODUCTION  The most common approach to large-vocabulary, talker-independent speech recognition  has been statistical modelling with hidden Markov models (HMMs). The HMM has an  explicit model for time specified by the Markov chain parameters. This temporal model  is governed by the grammar and phonology of the language being modelled. The acoustic  signal is modelled as a random process of the Markov chain and adjoining local temporal  information is assumed to be independent. This assumption is certainly not the case and a  great deal of research has addressed the problem of modelling acoustic context.  Standard HMM techniques for handling the context dependencies of the signal have ex-  1051  1052 Renals, Hochberg, and Robinson  plicitly modelled all the n-tuples of acoustic segments (e.g., context-dependent triphone  models). Typically, these systems employ a great number of parameters and, subsequently,  require massive amounts of training data and/or care in smoothing of the parameters. Where  the context of the model is greater than two segments, an additional problem is that it is  very likely that contexts found in testing data are never observed in the training data.  Recently, we have developed state-of-the-art continuous speech recognition systems using  hybrid connectionist/HMM methods (Robinson, 1994; Renals et al., 1994). These hybrid  connectionist/HMM systems model context at two levels (although these levels are not  necessarily at distinct scales). As in the traditional HMM, a Markov process is used to  specify the duration and lexical constraints on the model. The connectionist framework  provides a conditional likelihood estimate of the local (in time) acoustic waveform given  the Markov process. Acoustic context is handled by either expanding the network input to  include multiple, adjacent input frames, or using recurrent connections in the network to  provide some memory of the previous acoustic inputs.  2 DEPTH AND RESOLUTION  Following Principe et al. (1993), we may characterise the time dependence displayed by a  particular model in terms of depth and resolution. Loosely speaking, the depth tells us how  far back in time a model is able to look I , and the resolution tells us how accurately the past  to a given depth may be reconstructed. The baseline models that we currently use are very  different in terms of these characteristics.  Multi-layer Perceptron  The feed-forward multi-layer perceptron (MLP) does not naturally model time, but simply  maps an input to an output. Crude temporal dependence may be imparted into the system by  using a delay-lined input (figure la); an extension of this approach is the time-delay neural  network (TDNN). The MLP may be interpreted as acting as a FIR filter. A delay-lined  input representation may be characterised as having low depth (limited by the delay line  length) and high resolution (no smoothing).  Recurrent Network  The recurrent network (RN) models time dependencies of the acoustic signal via a fully-  connected, recurrent hidden layer (figure lb). The RN has a potentially infinite depth  (although in practice this is limited by available training algorithms) and low resolution,  and may be regarded as analogous to an IIR filter. A small amount of future context is  available to the RN, through a four frame target delay.  Experiments  Experiments on the DARPA Resource Management (RM) database have indicated that  the tradeoff between depth and resolution is important. In Robinson et al. (1993), we  compared different acoustic front ends using a MLP and a RN. Both networks used 68  In the language of section 3, the depth may be expressed as the mean duration, relative to the  target, of the last kernel in a filter that is convolved with the input.  Learning Temporal Dependencies in Connectionist Speech Recognition 1053  P(qk I X n+c' Vk = 1, K  -- ii_C,, ..,  Output Layer  Hidden Layer  512 - 1,024 hidden units  (a) Multi-layer Perceptron  (b) Recurrent Network  Figure 1: Connectionist architectures used for speech recognition.  outputs (corresponding to phones); the MLP used 1000 hidden units and the RN used  256 hidden units. Both architectures were trained using a training set containing 3990  sentences spoken by 109 speakers. Two different resolutions were used in the front-end  computation of mel-frequency cepstral coefficients (MFCCs): one with a 20ms Hamming  window and a 10ms frame step (referred to as 20/10), the other with a 32ms Hamming  window and a 16ms frame step (referred to as 32/16). A priori, we expected the higher  resolution frame rate (20/10) to produce a higher performance recogniser because rapid  speech events would be more accurately modelled. While this was the case for the MLP,  the RN showed better results using the lower resolution front end (32/16) (see table 1). For  the higher resolution front-end, both models require a greater depth (in frames) for the same  context (in milliseconds). In these experiments the network architectures were constant so  increasing the resolution of the front end results in a loss of depth.  Word Error Rate %  Net Front End feb89 oct89 feb91 sep92  RN 20/10 6.1 7.6 7.4 12.1  RN 32/16 5.9 6.3 6.1 11.5  MLP 20/10 5.7 7.1 7.6 12.0  MLP 32/16 6.6 7.8 8.5 15.0  Table 1: Comparison of acoustic front ends using a RN and a MLP for continuous speech  recognition on the RM task, using a wordpair grammar of perplexity 60. The four test sets  (feb89, oct89, feb91 and sep92, labelled according to their date of release by DARPA) each  contain 300 sentences spoken by 10 new speakers.  In the case of the MLP we were able to explicitly set the memory depth. Previous experi-  ments had determined that a memory depth of 6 frames (together with a target delayed by 3  frames) was adequate for problems relating to this database. In the case of the RN, memory  1054 Renals, Hochberg, and Robinson  P(qlx)  I Output Layer  P(qlx)  I Output Layer  Hidden Layer (1000 hidden units) 1 I Hidden Layer (1000 hidden units)  x(t) x(t+2)  (a) Gamma Filtered Input (b) Gamma Filter + Future Context  Figure 2: Gamma memory applied to the network input. The simple gamma memory in (a)  does not incorporate any information about the future, unless the target is delayed. In (b)  there is an explicit delay line to incorporate some future context.  depth is not determined directly, but results from the interaction between the network archi-  tecture (i.e., number of state units) and the training process (in this case, back-propagation  through time). We hypothesise that the RN failed to make use of the higher resolution front  end because it did not adapt to the required depth.  3 GAMMA MEMORY STRUCTURE  The tradeoff between depth and resolution has led us to investigate other network archi-  tectures. The gamma filter, introduced by de Vries and Principe (1992) and Principe et al.  (1993), is a memory structure designed to automatically determine the appropriate depth  and resolution (figure 2). This locally recurrent architecture enables lowpass and bandpass  filters to be learned from data (using back-propagation through time or real-time recurrent  learning) with only a few additional parameters.  We may regard the gamma memory as a generalisation of a delay line (Mozer, 1993) in  which the kth tap at time t is obtained by convolving the input time series with a kernel  function, g(t), and where fi parametrises the Kth order gamma filter,  g(t) = rS(t)  I -tk t k-le-t 1 < k < K  g(t) =  :- -  This family of kernels is attractive, since it may be computed incrementally by  dxk(t)  - -fix(t) + I. tx_ l (t) .  dt  This is in contrast to some other kernels that have been proposed (e.g., Gaussian kernels  proposed by Bodenhausen and Waibel (1991) in which the convolutions must be performed  Learning Temporal Dependencies in Connectionist Speech Recognition 1055  explicitly). In the discrete time case the filter becomes:  x(t)= (1-I)X(t- 1)+xk-l(t-- 1) .  This recursive filter is guaranteed to be stable when 0 < kt < 2.  In the experiments reported below we have replaced the input delay line of a MLP with a  gamma memory structure, using one gamma filter for each input feature. This structure is  referred to as a "focused gamma net" by de Vries and Principe (1992).  Owing to the effects of anticipatory coarticulation, information about the future is as  important as past context in speech recognition. A simple gamma filtered input (figure 2a)  does not include any future context. There are various ways in which this may be remedied;   Use the same architecture, but delay the target (similar to figure lb);   Explicitly specify future context by adding a delay line from the future (figure 2b);   Use two gamma filters per feature: one forward, one backward in time.  A drawback of the first approach is that the central frame corresponding to the delayed target  will have been smoothed by the action of the gamma filter. The third approach necessitates  two passes when either training or running the network.  4 SPEECH RECOGNITION EXPERIMENTS  We have performed experiments using the standard TIMIT speech database. This database  is divided into 462 training speakers and 168 test speakers. Each speaker utters eight  sentences that are used in these experiments, giving a training set of 3696 sentences and a  test set of 1344 sentences. We have used this database for a continuous phone recognition  task: labelling each sentence using a sequence of symbols, drawn from the standard 61  element phone set.  The acoustic data was preprocessed using a 12th order perceptual linear prediction (PLP)  analysis to produce an energy coefficient plus 12 PLP cepstral coefficients for each frame  of data. A 20ms Hamming window was used with a 10ms frame step. The temporal  derivatives of each of these features was also estimated (using a linear regression over + 3  adjacent frames) giving a total of 26 features per frame.  The networks we employed (table 2) were MLPs, with 1000 hidden units, 61 output  units (one per phone) and a variety of input representations. The Markov process used  single state phone models, a bigram phone grammar, and a Viterbi decoder was used for  recognition. The feed-forward weights in each network were initialised with identical sets  of small random values. The gamma filter coefficients were initialised to 1.0 (equivalent  to a delay line). The feed-forward weights were trained using back-propagation and the  gamma filter coefficients were trained in a forward in time back-propagation procedure  equivalent to real-time recurrent learning. An important detail is that the gradient step size  was substantially lower (by a factor of 10) for the gamma filter parameters compared with  the feed-forward weights. This was necessary to prevent the gamma filter parameters from  becoming unstable.  The baseline system using a delay line (Base) corresponds to figure 1 a, with + 3 frames of  context. The basic four-tap gamma filter G4 is illustrated in figure 2a (but using 1 fewer  1056 Renals, Hochberg, and Robinson  System ID Description  Base Baseline delay line, + 3 frames of context  G4 Gamma filter, 4 taps  G7 Gamma filter, 7 taps, delayed target  G7i G7 initialised using weights from Base  G4F3 Gamma filter, 4 taps, 3 frames future context  G4F3i G4F3 initialised using weights from Base  Table 2: Input representations used in the experiments. Note that G7i and G4F3i were  initialised using a partially trained weight matrix (after six epochs) from Base.  tap than the picture) and G7 is a 7 frame gamma filter with the target delayed for 3 frames,  thus providing some future context (but at the expense of smoothing the "centre" frame).  Future context is explicitly incorporated in 134F3, in which the three adjacent future frames  are included (similar to figure 2b). Systems 137i and 134F3i were both initialised using  a partially trained weight matrix for the delay line system, Base. This was equivalent to  fixing the value of the gamma filter coefficients to a constant (1.0) during the first six epochs  of training and only adapting the feed-forward weights, before allowing the gamma filter  coefficients to adapt.  The results of using these systems on the TIMIT phone recognition task are given in table  3. Table 4 contains the results of some model merging experiments, in which the output  probability estimates of 2 or more networks were averaged to produce a merged estimate.  System ID Depth Correct% Insert.% Subst.% Delet.% Error%  Base 4.0 67.6 4.1 24.7 7.7 36.5  134 8.5 65.8 4.1 25.9 8.3 38.2  137 11.7 65.5 4.1 26.0 8.5 38.6  137i 5.8 67.3 3.8 24.5 8.2 36.5  134F3 9.6 67.8 3.8 24.2 8.0 36.0  134F3i 4.9 68.0 3.9 24.2 7.8 35.9  Table 3: TIMIT phone recognition results for the systems defined in table 2. The Depth  value is estimated as the ratio of filter order to average filter parameter K/fl. Future context  is ignored in the estimate of depth, and the estimates for 137 and 137i are adjusted to account  for the delayed target.  System ID Correct% Insert.% Subst.% Delet.% Error%  G4F3 + Base 68.1 3.2 23.7 8.2 35.1  G4F3 + G4F3i 68.2 3.2 23.5 8.3 35.0  G7 + Base 67.0 3.2 24.4 8.6 36.2  G7 + G7i 67.4 3.6 24.4 8.2 36.2  Table 4: Model merging on the TIMIT phone recognition task.  Learning Temporal Dependencies in Connectionist Speech Recognition 1057  0.8  0.6  " PLP Coefficients  ,--,-,--- Derivatives  0.4  0.2  E C1 C2 C3 C4 C5 C6 C7 C8 C9 C10 Cll C12  Feature  Figure 3: Gamma filter coefficients for G4F3. The coefficients correspond to energy (E)  and 12 PLP cepstral coefficients (C1-C12) and their temporal derivatives.  5 DISCUSSION  Several comments may be made about the results in section 4. As can be seen in table 3,  replacing a delay line with an adaptive gamma filter can lead to an improvement in per-  formance. Knowledge of future context is important. This is shown by G4, which had no  future context or delayed target information, and had poorer performance than the baseline.  However, incorporating future context using a delay line (G4F3) gives better performance  than a pure gamma filter representation with a delayed target (G7). Training the locally  recurrent gamma filter coefficients is not trivial. Fixing the gamma filter coefficients to  1.0 (delay line) whilst adapting the feed-forward weights during the first part of training  is beneficial. This is demonstrated by comparing the performance of G7 with G7i and  G4F3 with G4F3i. Finally, table 4 shows that model merging generally leads to improved  recognition performance relative to the component models. This also indicates that the  delay line and gamma filter input representations are somewhat complementary.  Figure 3 displays the trained gamma filter coefficients for G4F3. There are several points  to make about the learned temporal dependencies.  The derivative parameters are smaller compared with the static PLP parameters.  This indicates the derivative filters have greater depth and lower resolution com-  pared with the static PLP filters.  If a gamma filter is regarded as a lowpass IIR filter, then lower filter coefficients  indicate a greater degree of smoothing. Better estimated coefficients (e.g., static  PLP coefficients C1 and C2) give rise to gamma filters with less smoothing.  The training schedule has a significant effect on filter coefficients. The depth  estimates of G4F3 and G4F3i in table 3 demonstrate that very different sets of  filters were arrived at for the same architecture with identical initial parameters,  but with different training schedules.  1058 Renals, Hochberg, and Robinson  We are investigating the possibility of using gamma filters to model speaker characteristics.  Preliminary experiments in which the gamma filters of speaker independent networks were  adapted to a new speaker have indicated that the gamma filter coefficients are speaker  dependent. This is an attractive approach to speaker adaptation, since very few parameters  (26 in our case) need be adapted to a new speaker.  Gamma filtering is a simple, well-motivated approach to modelling temporal dependencies  for speech recognition and other problems. It adds minimal complexity to the system  (in our case a parameter increase of 0.01%), and these initial experiments have shown an  improvement in phone recognition performance on the TIMIT database. A further increase  in performance resulted from a model merging process. We note that gamma filtering and  model merging may be regarded as two sides of the same coin: gamma filtering smooths  the input acoustic features, while model merging smooths the output probability estimates.  Acknowledgement  This work was supported by ESPRIT BRA 6487, WERNICKE. SR was supported by a SERC  postdoctoral fellowship and a travel grant from the NIPS foundation. TR was supported by  a SERC advanced fellowship.  References  Bodenhausen, U., & Waibel, A. (1991). The Tempo 2 algorithm: Adjusting time delays  by supervised learning. In Lippmann, R. P., Moody, J. E., & Touretzky, D. S. (Eds.),  Advances in Neural Information Processing Systems, Vol. 3, pp. 155-161. Morgan  Kaufmann, San Mateo CA.  de Vries, B., & Principe, J. C. (1992). The gamma model--a new neural model for temporal  processing. Neural Networks, 5, 565-576.  Mozer, M. C. (1993). Neural net architectures for temporal sequence processing. In  Weigend, A. S., & Gershenfeld, N. (Eds.), Predicting the future and understanding  the past. Addison-Wesley, Redwood City CA.  Principe, J. C., de Vries, B., & de Oliveira, P. G. (1993). The gamma filter--a new class of  adaptive IIR filters with restricted feedback. IEEE Transactions on SignalProcessing,  41,649-656.  Renals, S., Morgan, N., Boutlard, H., Cohen, M., & Franco, H. (1994). Connectionist  probability estimators in HMM speech recognition. IEEE Transactions on Speech  and Audio Processing. In press.  Robinson, A. J., Almeida, L., Boite, J.-M., Boutlard, H., Fallside, F., Hochberg, M., Ker-  shaw, D., Kohn, P., Konig, Y., Morgan, N., Nero, J.P., Renals, S., Saerens, M., &  Wooters, C. (1993). A neural network based, speaker independent, large vocabu-  lary, continuous speech recognition system: the WERNICKE project. In Proceedings  European Conference on Speech Communication and Technology, pp. 1941-1944  Berlin.  Robinson, T. (1994). The application of recurrent nets to phone probability estimation.  IEEE Transactions on Neural Networks. In press.  
Probabilistic Anomaly Detection in  Dynamic Systems  Padhralc Smyth  Jet Propulsion Laboratory 238-420  California Institute of Technology  4800 Oak Grove Drive  Pasadena, CA 91109  Abstract  This paper describes probabilistic methods for novelty detection  when using pattern recognition methods for fault monitoring of  dynamic systems. The problem of novelty detection is particular-  ly acute when prior knowledge and training data only allow one  to construct an incomplete classification model. Allowance must  be made in model design so that the classifier will be robust to  data generated by classes not included in the training phase. For  diagnosis applications one practical approach is to construct both  an input density model and a discriminative class model. Using  Bayes' rule and prior estimates of the relative likelihood of data  of known and unknown origin the resulting classification equations  are straightforward. The paper describes the application of this  method in the context of hidden Markov models for online fault  monitoring of large ground antennas for spacecraft tracking, with  particular application to the detection of transient behaviour of  unknown origin.  I PROBLEM BACKGROUND  Conventional control-theoretic models for fault detection typically rely on an accu-  rate model of the plant being monitored (Patton, Frank, and Clark, 1989). However,  in practice it common that no such model exists for complex non-linear systems.  The large ground antennas used by JPL's Deep Space Network (DSN) to track  825  826 Smyth  Deep Space Communlcaons Unk  Jet Propulsion Laboak)ry  GCF Ground Communcations  Facility, low-error rate link  Figure 1' Block diagram of typical Deep Space Network downlink  planetary spacecraft fall into this category. Quite detailed analytical models exist  for the electromechanical pointing systems. However, these models are primarily  used for determining gross system characteristics such as resonant frequencies; they  are known to be a poor fit for fault detection purposes.  We have previously described the application of adaptive pattern recognition meth-  ods to the problem of online health monitoring of DSN antennas (Smyth and Mell-  strom, 1992; Smyth, in press). Rapid detection and identification of failures in the  electromechanical antenna pointing systems is highly desirable in order to minimize  antenna downtime and thus minimise telemetry data loss when communicating with  remote spacecraft (see Figure 1). Fault detection based on manual monitoring of  the various antenna sensors is neither reliable or cost-effective.  The pattern-recognition monitoring system operates as follows. Sensor data such as  motor current, position encoder, tachometer voltages, and so forth are synchronous-  ly sampled at 50Hz by a data acquisition system. The data are blocked off into  disjoint windows (200 samples are used in practice) and various features (such as  estimated autoregressive coefficients) are extracted; let the feature vector be 0_.  The features are fed into a classification model (every 4 seconds) which in turn pro-  vides posterior probability estimates of the m possible states of the system given the  estimated features from that window, P(i ]_0). 0./1 corresponds to normal conditions,  the other aq's, i _< i _< m, correspond to known fault conditions.  Finally, since the system has "memory" in the sense that it is more likely to remain  in the current state than to change states, the posterior probabilities need to be  correlated over time. This is achieved by a standard first-order hidden Markov  Probabilistic Anomaly Detection in Dynamic Systems 827  model (HMM) which models the temporal state dependence. The hidden aspect  of the model reflects the fact that while the features are directly observable, the  underlying system states are not, i.e., they are in effect "hidden." Hence, the purpose  of the HMM is to provide a model from which the most likely sequence of system  states can be inferred given the observed sequence of feature data.  The classifier portion of the model is trained using simulated hardware faults. The  feed-forward neural network has been the model of choice for this application be-  cause of its discrimination ability, its posterior probability estimation properties  (Richard and Lippmann, 1992; Miller, Goodman and Smyth, 1993) and its rel-  atively simple implementation in software. It should be noted that unlike typical  speech recognition HMM applications, the transition probabilities are not estimated  from data but are designed into the system based on prior knowledge of the sys-  tem mean time between failure (MTBF) and other specific knowledge of the system  configuration (Smyth, in press).  2 LIMITATIONS OF THE DISCRIMINATIVE MODEL  The model described above assumes that there are m known mutually exclusive and  exhaustive states (or "classes") of the system, Wl,..., win. The mutually exclusive  assumption is reasonable in many applications where multiple simultaneous failures  are highly unlikely. However, the exhaustive assumption is somewhat impractical.  In particular, for fault detection in a complex system such as a large antenna, there  are thousands of possible fault conditions which might occur. The probability of  occurrence of any single condition is very small, but nonetheless there is a significant  probability that at least one of these conditions will occur over some finite time.  While the common faults can be directly modelled it is not practical to assign model  states to all the other minor faults which might occur.  As discussed in (Smyth and Mellstrom, 1992; Smyth 1994) a discriminative model  directly models p(wi10_), the posterior probabilities of the classes given the feature  data, and assumes that the classes Wl,..., wm are exhaustive. On the other hand, a  generarive model directly models the probability density function of the input data  conditioned on each class, p(0_laq), and then indirectly determines posterior class  probabilities by application of Bayes' rule. Examples of generative classifiers include  parametric models such as Gaussian classifiers and memory-based methods such as  kernel density estimators. Generative models are by nature well suited to novelty  detection whereas discriminative models have no built-in mechanism for detecting  data which are different to that on which the model was trained. However, there  is a trade-off; because generative models typically are doing more modelling than  just searching for a decision boundary, they can be less efficient (than discriminant  methods) in their use of the data. For example, generative models typically scale  poorly with input dimensionality for fixed training sample size.  3 HYBRID MODELS  A relatively simple and practical approach to the novelty detection problem is to  use both a generative and discriminative classifier (an idea originally suggested to  the author by R. P. Lippmann). An extra "ra + lth" state is added to the model to  828 Smyth  cover "all other possible states" not accounted for by the known m states. In this  framework, the posterior estimates of the discriminative classifier are conditioned  on the event that the data come from one of the m known classes.  Let the symbol w{1,...,m} denote the event that the true system state is one of the  known states, let 03rn+l be the unknown state, and let p(OZrn+l[0_) be the posterior  probability that the system is in an unknown state given the data. Hence, one can  estimate the posterior probability of individual known states as  IS(wil_O) - pa(wilO, w{1,...,m}) x (1 -  i <_ i _< m (1)  where p,(wi[O_, 0./{1,...,rn}) is the posterior probability estimate of state i as provided  by a discriminative model, i.e., given that the system is in one of the known states.  The calculation of p(Wrn+l]0_) can be obtained via the usual application of Bayes'  rule ifp(_010Zrn+l), p(OZm+l), and p(_010{1 .... ,m}) are known:  P(Olwm+ l )P(COm+ l )  p(wm+llO)- p(_Olwm+1)p(wm+1 ) q- p(_OIw{1,...,m}) -]  (2)  Specifying the prior density p(O_lw,+l), the distribution of the features conditioned  on the occurrence of the unknown state, can be problematic. In practice we have  used non-informative Bayesian priors for p(O_lw,n+l ) over a bounded space of feature  values (details are available in a technical report (Smyth and Mellstrom, 1993)),  although the choosing of a prior density for data of unknown origin is basically  ill-posed. The stronger the constraints which can be placed on the features the  narrower the resulting prior density and the better the ability of the overall model  to detect novelty. If we only have very weak prior information, this will translate  into a weaker criterion for accepting points which belong to the unknown category.  The term p(com+l) (in Equation (2)) must be chosen based on the designer's prior  belief of how often the system will be in an unknown state -- a practical choice is  that the system is at least as likely to be in an unknown failure state as any of the  known failure states.  The p(0lOZ{1,...,m }) term in Equation (2) is provided directly by the generative mod-  el. Typically this can be a mixture of Gaussian component densities or a kernel  density estimate over all of the training data (ignoring class labels). In practice,  for simplicity of implementation we use a simple Gaussian mixture model. Further-  more, because of the afore-mentioned scaling problem with input dimensions, only  a subset of relatively significant input features are used in the mixture model. A  less heuristic approach to this aspect of the problem (with which we have not yet  experimented) would be to use a method such as projection pursuit to project the  data into a lower dimensional subspace and perform the input density estimation in  this space. The main point is that the generative model need not necessarily work  in the full dimensional space of the input features.  Integration of Equations (1) and (2) into the hidden Markov model scheme is s-  traightforward and is not derived here -- the HMM now has an extra state, "un-  known." The choice of transition probabilities between the unknown and other states  is once again a matter of design choice. For the antenna application at least, many  of the unknown states are believed to be relatively brief transient phenomena which  Probabilistic Anomaly Detection in Dynamic Systems 829  last perhaps no longer than a few seconds: hence, the Markov matrix is designed  to reflect these beliefs since the expected duration of any state d[wi] (in units of  sampling intervals) must obey  d[coi] -- 1 - Pii  where pii is the self-transition probability of state mi.  4 EXPERIMENTAL RESULTS  For illustrative purposes the experimental results from 2 particular models are com-  pared. Each was applied to monitoring the servo pointing system of a DSN 34m  antenna at Goldstone, California. The models were implemented within LabView  data acquisition software running in real-time on a Macintosh II computer at the an-  tenna site. The models had previously been trained off-line on data collected some  months earlier. 12 input features were used consisting of estimated autoregressive  coefficients and variance terms from each window of 200 samples of multichannel  data. For both models a discriminative feedforward neural network model (with 8  hidden units, sigmoidal hidden and output activation functions) was trained (us-  ing conjugate-gradient optimization) to discriminate between a normal state and 3  known and commonly occurring fault states (failed tachometer, noisy tachometer,  and amplifier short circuit -- also known as "compensation loss"). The network  output activations were normalised to sum to i in order to provide posterior class  probability estimates.  Model (a) used no HMM and assumed that the 4 known states are exhaustive, i.e.,  it just used the feedforward network. Model (b) used a HMM with 5 states, where a  generative model (a Gaussian mixture model) and a fiat prior (with bounds on the  feature values) were used to determine the probability of the 5th state (as described  by Equations (1) and (2)). The same neural network as in model (a) was used as a  discriminator for the other 4 known states. The generative mixture model had 10  components and used only 2 of the 12 input features, the 2 which were judged to be  the most sensitive to system change. The parameters of the HMM were designed  according to the guidelines described earlier. Known fault states were assumed to  be equally likely with i hour MTBF's and with i hour mean duration. Unknown  faults were assumed to have a 20 minute MTBF and a 10 second mean duration.  Both HMMs used 5-step backwards smoothing, i.e., the probability estimates at  any time n are based on all past data up to time n and future data up to time  n + 5 (using a larger number of backward steps was found empirically to produce  no effect on the estimates).  Figures 2 (a) and (b) show each model's estimates (as a function of time) that  the system is in the normal state. The experiment consisted of introducing known  hardware faults into the system in a controlled manner after 15 minutes and 45  minutes, each of 15 minutes duration.  Model (a)'s estimates are quite noisy and contain a significant number of potential  false alarms (highly undesirable in an operational environment). Model (b) is much  more stable due to the smoothing effect of the HMM. Nonetheless, we note that  between the 8th and 10th minutes, there appear to be some possible false alarms:  830 Smyth  1  0.8  Probability  of normal 0.6  O.4  O.2  0  0 of  tachometer fult  .I  Discriminafive model, no HMM   30  Rcsun tm of  nomaal ctmditiom  loss fault  I I   50 60  ction of Time (minutes)  1  0.8  Probability  of normal 0.6  0.4  0.2  0  o   lnttod cti of  lachommr fault  I  Hybrid model, with HMM  I I I  tlo of wtion of Time (minutes)  nomaal conditions comt-tmation loss fault  Figure 2: Estimated posterior probability of normal state (a) using no HMM and  the exhaustive assumption (normal + 3 fault states), (b) using a HMM with  hybrid model (normal + 3 faults + other state).  these data were classified into the unknown state (not shown). On later inspection  it was found that large transients (of unknown origin) were in fact present in the  original sensor data and that this was what the model had detected, confirming  the classification provided by the model. It is worth pointing out that the model  without a generative component (whether with or without the HMM) also detected  a non-normal state at the same time, but incorrectly classified this state as one of  the known fault states (these results are not shown).  Also not shown are the results from using a generative model alone, with no dis-  criminative component. While its ability to detect unknown states was similar to  the hybrid model, its ability to discriminate between known states was significantly  worse than the hybrid model.  The hybrid model has been empirically tested on a variety of other conditions where  various "known" faults are omitted from the discriminative training step and then  Probabilistic Anomaly Detection in Dynamic Systems 831  presented to the model during testing: in all cases, the anomalous unknown state  was detected by the model, i.e., classified as a state which the model had not seen  before.  5 APPLICATION ISSUES  The model described here is currently being integrated into an interactive antenna  health monitoring software tool for use by operations personnel at all new DSN  antennas. The first such antenna is currently being built at the Goldstone (Califor-  nia) DSN site and is scheduled for delivery to DSN operations in late 1994. Similar  antennas, also equipped with fault detectors of the general nature described here,  will be constructed at the DSN ground station complexes in Spain and Australia in  the 1995-96 time-frame.  The ability to detect previously unseen transient behaviour has important practical  consequences: as well as being used to warn operators of servo problems in real-  time, the model will also be used as a filter to a data logger to record interesting  and anomalous servo data on a continuous basis. Hence, potentially novel system  characteristics can be recorded for correlation with other antenna-related events  (such as maser problems, receiver lock drop during RF feedback tracking, etc.) for  later analysis to uncover the true cause of the anomaly. A long-term goal is to  develop an algorithm which can automatically analyse the data which have been  classified into the unknown state and extract distinct sub-classes which can be  added as new explicit states to the HMM monitoring system in a dynamic fashion.  Stolcke and Omohundro (1993) have described an algorithm which dynamically  creates a state model for HMMs for the case of discrete-valued features. The case  of continuous-valued features is considerably more subtle and may not be solvable  unless one makes significant prior assumptions regarding the nature of the data-  generating mechanism.  6 CONCLUSION  A simple hybrid classifier was proposed for novelty detection within a probabilis-  tic framework. Although presented in the context of hidden Markov models for  fault detection, the proposed scheme is perfectly general for generic classification  applications. For example, it would seem highly desirable that fielded automated  medical diagnosis systems (such as various neural network models which have been  proposed in the literature) should always contain a "novelty-detection" component  in order that novel data are identified and appropriately classified by the system.  The primary weakness of the methodology proposed in this paper is the necessity  for prior knowledge in the form of densities for the feature values given the unknown  state. The alternative approach is not to explicitly model the the data from the  unknown state but to use some form of thresholding on the input densities from the  known states (Aitchison, Habbema, and Kay, 1977; Dubuisson and Masson, 1993).  However, direct specification of threshold levels is itself problematic. In this sense,  the specification of prior densities can be viewed as a method for automatically  determining the appropriate thresholds (via Equation (2)).  832 Smyth  As a final general comment, it is worth noting that online learning systems must  use some form of novelty detection. Hence, hybrid generative-discriminative models  (a simple form of which has been proposed here) may be a useful framework for  modelling online learning.  Acknowledgement s  The author would like to thank Jeff Mellstrom, Paul Scholtz, and Nancy Xiao for  assistance in data acquisition and analysis. The research described in this paper  was performed at the Jet Propulsion Laboratory, California Institute of Technology,  under a contract with the National Aeronautics and Space Administration and was  supported in part by ARPA under grant number N00014-92-J-1860  References  R. Patton, P. Frank, and R. Clark (eds.), Fault Diagnosis in Dynamic Systems:  Theory and Application, New York, NY: Prentice Hall, 1989.  P. Smyth and J. Mellstrom, 'Fault diagnosis of antenna pointing systems using  hybrid neural networks and signal processing techniques,' in Advances in Neural  Information Processing Systems J, J. E. Moody, S. J. Hanson, R. P. Lippmann  (eds.), San Mateo, CA: Morgan Kaufmann, pp.667-674, 1992.  P. Smyth, 'Hidden Markov models for fault detection in dynamic systems,' Pattern  Recognition, vol.27, no.1, in press.  M. D. Richard and R. P. Lippmann, 'Neural network classifiers estimate Bayesian  a posterJori probabilities,' Neural Computation, 3(4), pp.461-483, 1992.  J. Miller, R. Goodman, and P. Smyth, 'On loss functions which minimize to con-  ditional expected values and posterior probabilities,' IEEE Transactions on Infor-  mation Theory, vol.39, no.4, pp.1404-1408, July 1993.  P. Smyth, 'Probability density estimation and local basis function neural networks,'  in Computational Learning Theory and Natural Learning Systems, T. Petsche, M.  Kearns, S. Hanson, R. Rivest (eds.), Cambridge, MA: MIT Press, 1994.  P. Smyth and J. Mellstrom, 'Failure detection in dynamic systems: model con-  struction without fault training data,' Telecommuncations and Data Acquisition  Progress Report, vol. 112, pp.37-49, Jet Propulsion Laboratory, Pasadena, CA,  February 15th 1993.  A. Stolcke and S. Omohundro, 'Hidden Markov model induction by Bayesian merg-  ing,' in Advances in Neural Information Processing Systems 5, C. L. Giles, S. J.  Hanson and J. D. Cowan (eds.), San Mateo, CA: Morgan Kaufmann, pp.11-18,  1993.  J. Aitchison, J. D. F. Habbema, and J. W. Kay, 'A critical comparison of two  methods of statistical discrimination,' Applied Statistics, vol.26, pp.15-25, 1977.  B. Dubuisson and M. Masson, 'A statistical decision rule with incomplete knowledge  about the classes,' Pattern Recognition, vol.26, no. 1, pp.155-165, 1993.  
How to Choose an Activation Function  H. N. Mhaskar  Department of Mathematics  California State University  Los Angeles, CA 90032  hmhaska@ calstatela.edu  C. A. Micchelli  IBM Watson Research Center  P.O. Box 218  Yorktown Heights, NY 10598  cam@watson.ibm.corn  Abstract  We study the complexity problem in artificial feedforward neural networks  designed to approximate real valued functions of several real variables; i.e.,  we estimate the number of neurons in a network required to ensure a given  degree of approximation to every function in a given function class. We  indicate how to construct networks with the indicated number of neurons  evaluating standard activation functions. Our general theorem shows that  the smoother the activation function, the better the rate of approximation.  I INTRODUCTION  The approximation capabilities of feedforward neural networks with a single hidden  layer has been studied by many authors, e.g., [1, 2, 5]. In [10], we have shown that  such a network using practically any nonlinear activation function can approximate  any continuous function of any number of real variables on any compact set to any  desired degree of accuracy.  A central question in this theory is the following. If one needs to approximate  a function from a known class of functions to a prescribed accuracy, how many  neurons will be necessary to accomplish this approximation for all functions in the  class? For example, Barron shows in [1] that it is possible to approximate any  function satisfying certain conditions on its Fourier transform within an L 2 error  of O(1/n) using a feedforward neural network with one hidden layer comprising of  n 2 neurons, each with a sigmoidal activation function. On the contrary, if one is  interested in a class of functions of s variables with a bounded gradient on [-1, 1] s,  319  320 Mhaskar and Micchelli  then in order to accomplish this order of approximation, it is necessary to use at  least O(n s) number of neurons, regardless of the activation function (cf. [3]).  In this paper, our main interest is to consider the problem of approximating a  function which is known only to have a certain number of smooth derivatives. We  investigate the question of deciding which activation function will require how many  neurons to achieve a given order of approximation for all such functions. We will  describe a very general theorem and explain how to construct networks with various  activation functions, such as the Gaussian and other radial basis functions advocated  by Girosi and Poggio [13] as well as the classical squashing function and other  sigmoidal functions.  In the next section, we develop some notation and briefly review some known facts  about approximation order with a sigmoidal type activation function. In Section  3, we discuss our general theorem. This theorem is applied in Section 4 to yield  the approximation bounds for various special functions which are commonly in use.  In Section 5, we briefly describe certain dimension independent bounds similar to  those due to Barron [1], but applicable with a general activation function. Section  6 summarizes our results.  2 SIGMOIDAL-TYPE ACTIVATION FUNCTIONS  In this section, we develop some notation and review certain known facts. For the  sake of concreteness, we consider only uniform approximation, but our results are  valid also for other LP-norms with minor modifications, if any. Let s >_ I be the  nmnber of input variables. The class of all continuous functions on [-1, 1] s will be  denoted by C s. The class of all 2r- periodic continuous functions will be denoted  by C s*. The uniform norm in either case will be denoted by I I' I I. Let  denote the set of all possible outputs of feedforward neural networks consisting of  n neurons arranged in I hidden layers and each neuron evaluating an activation  function rr where the inputs to the network are from R s. It is customary to assume  more a priori knowledge about the target function than the fact that it belongs  to C s or C s* For example, one may assume that it has continuous derivatives of  order r >_ I and the sum of the norms of all the partial derivatives up to (and  including) order r is bounded. Since we are interested mainly in the relative error  in approximation, we may assume that the target function is normalized so that this  sum of the norms is bounded above by 1. The class of all the functions satisfying  this condition will be denoted by W (or W* if the functions are periodic). In this  paper, we are interested in the universal approcimation of the classes W (and their  periodic versions). Specifically, we are interested in estimating the quantity  (2.1) sup  where  (2.2)  E',,t,s,o(f) := inf Ilf-PII.  PIIn,l,s,,  The quantity E,,t,s,o(f) measures the theoretically possible best order of approxi-  mation of an individual function f by networks with n neurons. We are interested  How to Choose an Activation Function 321  in determining the order that such a network can possibly achieve for all functions  in the given class. An equivalent dual formulation is to estimate  (2.3) /,t,s,o(Wf) := min{m e Z  sup Em,t,s,o(f) _< l/n).  This quantity measures the minimum number of neurons required to obtain accuracy  of 1In for all functions in the class W. An analogous definition is assumed for W*  in place of W.  Let / denote the class of all s-variable trigonometric polynomials of order at most  n and for a continuous function f, 2r-periodic in each of its s variables,  (2.4) E,(f) := min Ilf- PII.  We observe that/H can be thought of as a subclass of all outputs of networks with  a single hidden layer comprising of at most (2n + 1) * neurons, each evaluating the  activation function sin x. It is then well known that  (2.5) sup E(f) < cn-".  lewd.  Here and in the sequel, c, c,... will denote positive constants independent of the  functions and the number of neurons involved, but generally dependent on the other  parameters of the problem such as r, s and rr. Moreover, several constructions for  the approximating trigonometric polynomials involved in (2.5) are also well known.  In the dual formulation, (2.5)states that if rr(x):= sinx then  (2.6) ln,l,s,sin (Wr s*) --  It can be proved [3] that any "reasonable" approzimation process that aims to ap-  proximate all functions in W* up to an order of accuracy 1In must necessarily  depend upon at least O(n s/") parameters. Thus, the activation function sin x pro-  vides optimal convergence rates for the class Wf*.  The problem of approximating an r times continuously differentiable function  f : R s  R on [-1,1] s can be reduced to that of approximating another  function from the corresponding periodic class as follows. We take an infinitely  many times differentiable function p which is equal to I on [-2, 2] s and 0 outside  of [-r, r] s. The function fP can then be extended as a 2r-periodic function. This  function is r times continuously differentiable and its derivatives can be bounded  by the derivatives of f using the Leibnitz formula. A function that approximates  this 2r-periodic function also approximates f on [-1, 1] s with the same order of  approximation. In contrast, it is not customary to choose the activation function  to be periodic.  In [10] we introduced the notion of a higher order sigmoidal function as follows. Let  k >_ 0. We say that a function rr  R -- R is sigmoidal of order k if  (2.7) lina  - 1, lira  - 0,  and  (2.8) I()1 - c(1 q-Ixl) , x GR.  322 Mhaskar and Micchelli  A sigrnoidal function of order 0 is thus the customary bounded sigmoidal function.  We proved in [10] that for any integer r ) 1 and a sigmoidal function tr of order  r-l, wehave  W? ( O(n /") if s - 1,  (2.9) /,,,,,o(r) = O(n,/,.+(,+2,.)/,., ) if s _ 2.  Subsequently, Mhaskar showed in [6] that if tr is a sigmoidal function of order k ) 2  and r > 1 then, with I - O(logr/logk)),  (2.10)  k.,,.,o(w;) =  Thus, an optimal network can be constructed using a sigmoidal function of higher  order. During the course of the proofs in [10] and [6], we actually constructed the  networks explicitly. The various features of these constructions from the connec-  tionist point of view are discussed in [7, 8, 9].  In this paper, we take a different viewpoint. We wish to determine which acti-  vation function leads to what approximation order. As remarked above, for the  approximation of periodic functions, the periodic activation function sin x provides  an optimal network. Therefore, we will investigate the degree of approximation by  neural networks first in terms of a general periodic activation function and then  apply these results to the case when the activation function is not periodic.  3 A GENERAL THEOREM  In this section, we discuss the degree of approximation of periodic functions using  periodic activation functions. It is our objective to include the case of radial basis  functions as well as the usual "first order" neural networks in our discussion. To  encompass both of these cases, we discuss the following general formuation. Let  s >_ d ) I be integers and b G C d*. We will consider the approximation of functions  in C s* by linear combinations of quantities of the form d(Ax + t) where A is a d x s  matrix and t G R d. (In general, both A and t are parameters of the network.) When  d: s, A is the identity matrix and b is a radial function, then a linear combination  of n such quantities represents the output of a radial basis function network with n  neurons. When d = I then we have the usual neural network with one hidden layer  and periodic activation function b.  We define the Fourier coefficients of d by the formula  1 /_ c(t)e_im.tdt, m  Z   (3.1) (ln) .- (2r)a ,,,]a '  Let  (3.2) S4, _C {m  Z a  (m) y O}  and assume that there is a set J co taining d x s matrices with integer entries such  that  (3.3) Z s ={ATm  nSz, ACJ}  How to Choose an Activation Function 323  where A r denotes the transpose of A. If d = 1 and (1) :/- 0 (the neural network  case) then we may choose S, = {1} and d to be Z  (considered as row vectors).  If d = s and 6 is a function with none of its Fourier coefficients equal to zero (the  radial basis case) then we may choose S 4,= Z  and d = {Ix}. For m G Z , we  let km be the multi-integer with minimum magnitude such that m = ATkm for  some A - Am  d. Our estimates will need the quantities  (3.4) mr := min{l(km)l ' -2n _< m < 2n}  and  (3.5) Nr := max{lkml ' -2n < m < 2n}  where Ikml is the maximum absolute value of the components of km. In the neural  network case, we have m = I(1)1 and N = 1. In the radial basis case, N, = 2n.  Our main theorem can be formulated as follows.  THEOREM 3.1. Let s >_ d >_ 1, n > 1 and N > Nr be integers, f  C *, c  C a*.  It is possible to construct a network  (3.6)  Gr,N,,(f;x) :=  djb(Ajx + tj)  such that  (3.7) IIf }  - Ilfll   In (3.6), the sum contains at most O(nN a) terms, Aj  J, tj  R a, and dj are  linear functionals of f, depending upon n, N,  The estimate (3.7) relates the degree of approximation of f by neural networks  explicitly in terms of the degree of approximation of f and  by trigonometric poly-  nomials. Well known estimates from approximation theory, such as (2.5), provide  close connections between the smoothness of the functions involved and their degree  of trigonometric polynomial approximation. In particular, (3.7) indicates that the  smoother the function  the better will be the degree of approximation.  In [11], we have given explicit constructions of the operator G,,N,. The formulas in  [11] show that the network can be trained in a very simple manner, given the Fourier  coefficients of the target function. The weights and thresholds (or the centers in the  case of the radial basis networks) are determined universally for all functions being  approximated. Only the coefficients at the output layer depend upon the function.  Even these are given explicitly as linear combinations of the Fourier coefficients of  the target function. The explicit formulas in [11] show that in the radial basis case,  the operator G,,N,O actually contains only O(n + N)  summands.  4 APPLICATIONS  In Section 3, we had assumed that the activation function & is periodic. If the  activation function rr is not periodic, but satisfies certain decay conditions near  324 Mhaskar and Micchelli  c, it is still possible to construct a periodic function for which Theorem 3.1 can  be applied. Suppose that there exists a function p in the linear span of.Ao,. :=  {rr(Ax + t) : A E J, t E Rd}, which is integrahie on R d and satisfies the condition  that  (4.1) I(x)l _< cllxll -, for some r > d.  Under this assumption, the function  (4.2) )(x) :=  )(x- 2rk)  keZ d  is a 2r-periodic function integrahie on [-r, r] . We can then apply Theorem 3.1  with o instead of . In G,,N,o, we next replace po by a function obtained by  judiciously truncating the infinite sum in (4.2). The error made in this replacement  can be estimated using (4.1). Knowing the number of evaluations of rr in the  expression for b as a finite linear combination of elements of.Ao,., we then have an  estimate on the degree of approximation of f in terms of the number of evaluations of  rr. This process was applied on a number of functions rr. The results are summarized  in Table 1.  5 DIMENSION INDEPENDENT BOUNDS  In this section, we describe certain estimates on the L 2 degree of approximation that  are independent of the dimension of the input space. In this section, II' II denotes  the L 2 norm on [-1, 1]  (respectively [-r, r] ) and we approximate functions in the  class SF defined by  (5.1) SF, :- {f e C  Ilfllsr, := If(m)[ _< 1}.  mZ'  Analogous to the degree of approximation from F/,, we define the n-th degree of  approximation of a function f  C * by the formula  (5.2) e,,(f) := inf Ilf-  f(m)enn'Xll  AcZ',IAIin nleA  where we require the norm involved to be the L 2 norm. In (5.2), there is no need  to assume that n is an integer.  Let b be a square integrahie 27r-periodic function of one variable. We define the L 2  degree of approximation by networks with a single hidden layer by the formula  (5.3) (2)  ,,,,(f) := inf III-PII  PH,,,,,  where m is the largest integer not exceeding n. Our main theorem in this connection  is the following  THEOREM 5.1. Lets>_ 1 be an integer, f G SF, c) G L21  integers n, N _ 1,  and ((1) - 0. Then, for  (5.4)  How to Choose an Activation Function 325  Table 1: Order of magnitude of r,t,,o(Wd) for different o"s  Function o' r,,,o Remarks  Sigmoidal, order r- 1 n / s = d = 1, 1 = 1  Sigmoidal, order r - 1 n*/"+( +2")/" s _> 2, d = 1, I = 1  x ,ifx>0,0, ifx<0. nlr+( 2r+)lr k>_2, s>2, d= 1,1= 1  (1 q- e-') -' n/"(logn) 2 s >_ 2, d - 1, l- 1  Sigmoidal, order k n / k ) 2, s ) 1, d = 1,  I: O(logr/logk))  exp(-lxl/2) n 2/ s = d> 2,1 = 1  [xl(loglxl) * n (s/r)(2+(3s+2)/) s = d >_ 2, k > O, k + s even,  5-0ifsodd, lifseven, l- 1  where {Sr} is a sequence of positive numbers, 0 _< 5, _< 2, depending upon f such  that 6, -- 0 as n -- co. Moreover, the coefficients in the network that yields (5.4)  are bounded, independent of n and N.  We may apply Theorem 5.1 in the same way as Theorem 3.1. For the squashing  activation function, this gives an order of approximation O(n -/2) with a network  consisting of n(log n)  neurons arranged in one hidden layer. With the truncated  power function x[ (cf. Table 1, entry 3) as the activation function, the same  order of approximation is obtained with a network with a single hidden layer and  O(n +/()) neurons.  6 CONCLUSIONS.  We have obtained estimates on the number of neurons necessary for a network with  a single hidden layer to provide a given accuracy of all functions under the only a  priori assumption that the derivatives of the function up to a certain order should  exist. We have proved a general theorem which enables us to estimate this number  326 Mhaskar and Micchelli  in terms of the growth and smoothness of the activation function. We have explicitly  constructed networks which provide the desired accuracy with the indicated number  of neurons.  Acknowledgement s  The research of H. N. Mhaskar was supported in part by AFOSR grant 2-26 113.  References  1. B^RRON, A. R., Universal approximation bounds for superposition of a  sigmoidal function, IEEE Trans. on Information Theory, 39.  2. CYBENKO, G., Approximation by superposition of sigmoidal functions,  Mathematics of Control, Signals and Systems, 2, # 4 (1989), 303-314.  3. DEVORE, R., HOWARD, R. aND MlCCHELLI, C.A., Optimal nonlinear  approximation, Manuscripta Mathematica, 63 (1989), 469-478.  4. HECHT-NILESEN, R., Thoery of the backpropogation neural network, IEEE  International Conference on Neural Networks, I (1988), 593-605.  5. HORNIK, K., STINCHCOMBE, M. AND WHITE, H., Multilayerfeedforward  networks are universal approximators, Neural Networks, 2 (1989), 359-366.  6. MHASK^R, H. N., Approximation properties of a multilayered feedfor-  ward artificial neural network, Advances in Computational Mathematics  I (1993), 61-80.  7. MH^SKAR, H. N., Neural networks for localized approximation of real  functions, in "Neural Networks for Signal Processing, III", (Kamm, Huhn,  Yoon, Chellappa and Kung Eds.), IEEE New York, 1993, pp. 190-196.  8. MHASKAR, H. N., Approximation of real functions using neural networks,  in Proc. of Int. Conf. on Advances in Cornput. Math., New Delhi, India,  1993, World Sci. Publ., H. P. Dikshit, C. A. Micchelli eds., 1994.  9. MHASKAR, H. N., Noniterative training algorilhms for neural networks,  Manuscript, 1993.  10. MHASKAR, H. N. AND MICCHELLI, C. A., Approximation by superposi-  tion of a sigmoidal function and radial basis functions, Advances in Ap-  plied Mathematics, 13 (1992), 350-373.  11. MHASKAR, H. N. AND MICCHELLI, C. A., Degree of approximation by  superpositions of a fixed function, in preparation.  12. MHASKAR, H. N. AND MICCHELLI, C. A., Dimension independent bounds  on the degree of approximation by neural networks, Manuscript, 1993.  13. POGGIO, T. AND GIROSI, F., Regularization algorithms for learning that  are equivalent to multilayer networks, Science, 247 (1990), 978-982.  
The "Softmax" Nonlinearity:  Derivation Using Statistical Mechanics  and Useful Properties  as a Multiterminal Analog Circuit  Element  I. M. Elfadel  Research Laboratory of Electronics  Massachusetts Institute of Technology  Cambridge, MA 02139  J. L. Wyatt, Jr.  Research Laboratory of Electronics  Massachusetts Institute of Technology  Cambridge, MA 02139  Abstract  We use mean-field theory methods from Statistical Mechanics to  derive the "softmax" nonlinearity from the discontinuous winner-  take-all (WTA) mapping. We give two simple ways of implementing  "softmax" as a multiterminal network element. One of these has a  number of important network-theoretic properties. It is a recipro-  cal, passive, incrementally passive, nonlinear, resistive multitermi-  nal element with a content function having the form of information-  theoretic entropy. These properties should enable one to use this  element in nonlinear ItC networks with such other reciprocal el-  ements as resistive fuses and constraint boxes to implement very  high speed analog optimization algorithms using a minimum of  hardware.  1 Introduction  In order to efficiently implement nonlinear optimization algorithms in analog VLSI  hardware, maximum use should be made of the natural properties of the silicon  medium. Reciprocal circuit elements facilitate such an implementation since they  882  The "Softmax" Nonlinearity 883  can be combined with other reciprocal elements to form an analog network having  Lyapunov-like functions: the network content or co-content. In this paper, we show  a reciprocal implementation of the "softmax" nonlinearity that is usually used to  enforce local competition between neurons [Peterson, 1989]. We show that the cir-  cuit is passive and incrementally passive, and we explicitly compute its content and  co-content functions. This circuit adds a new element to the library of the analog  circuit designer that can be combined with reciprocal constraint boxes [Harris, 1988]  and nonlinear resistive fuses [Harris, 1989] to form fast, analog VLSI optimization  networks.  2 Derivation of the Softmax Nonlinearity  To a vector y ( R" of distinct real numbers, the discrete winner-take-all (WTA)  mapping W assigns a vector of binary numbers by giving the value 1 to the com-  ponent of y corresponding to max<i<, yi and the value 0 to the remaining com-  ponents. Formally, W is defined as  W(y) = (W(y),..., Wn(y)) T  where for every 1 _ j _ n,  1 ifyj>yi, l<_i<_n  Wj (y) = 0 otherwise (1)  Following [Geiger, 1991], we assign to the vector y the "energy" function  Eye) = - zkuk = _Ty, e (2)  where ', is the set of vertices of the unit simplex $, = {x  ", zi _ 0, 1 < i <  n and y]: z} = 1}. Every vertex in the simplex encodes one possible winner. It  is then easy to show that W(y) is the solution to the linear programming problem  max  zy.  Moreover, we can assign to the energy Ey(Z) the Gibbs distribution  e-y(Z)/T  Py(z) = Py(zl,..., z,) = ZT  where T is the temperature of the heat bath and ZT is a normalizing constant.  Then one can show that the mean of z considered as a random variable is given  by [Geiger, 1991]  ej/T eJ/T  rs (yIT)  The mapping F: " -- " whose components are the Fj 's, 1 <_ j _< n, is the gener-  alized sigmoid mapping [Peterson, 1989] or "softmax". It plays, in WTA networks,  a role similar to that of the sigmoidal function in Hopfield and backpropagation  884 Elfadel and Wyatt  Figure 1: A circuit implementation of softmax with 5 inputs and 5 outputs. This  circuit is operated in subthreshold mode, takes the gates voltages as inputs and  gives the drain currents as outputs. This circuit is not a reciprocal multiterminal  element.  networks [Hopfield, 1984, Rumelhart, 1986] and is usually used for enforcing com-  petitive behavior among the neurons of a single cluster in networks of interacting  clusters [Peterson, 1989, Waugh, 1993].  For y 6 ", we denote by FT(y)  F(y/T). The softmax mapping satisfies the  following properties:  1. The mapping FT converges pointwise to W over ' as T -- 0 and to the  ie =  (1, 1 1) T ( "  center of mass of $,, ,  ,..., , as T -- +o.  2. The Jacobian DF of the softmax mapping is a symmetric n x n matrix that  satisfies  DF(y) = diatl(Fk(y))- F(y)F(y) T. (3)  It is always singular with the vector e being the only eigenvector correspond-  ing to the zero eigenvalue. Moreover, all its eigenvalues are upper-bounded  by max<}<n F}(y) < 1.  3. The softmax mapping is a gradient map, i.e, there exists a "potential"  function   n --  such that F = VP. Moreover  is convex.  The symbol P was chosen to indicate that it is a potential function. It should be  noted that if F is the gradient map of P then FT is the gradient map of TT  where PT(y) = P(y/T). In a related paper [Elfadel, 1993], we have found that  the convexity of P is essential in the study of the global dynamics of analog WTA  networks. Another instance where the convexity of P was found important is the  one reported in [Kosowsky, 1991] where a mean-field algorithm was proposed to  solve the linear assignment problem.  The "Softmax" Nonlinearity 885  V V Vs V4 Vs   12 I 14 I5  T5  1 T T T4  Figure 2: Modified circuit implementation of softmax. In this circuit all the tran-  sistors are diode-connected, and all the drain currents are well in saturation region.  Note that for every transistor, both the voltage input and the current output are  on the same wire - the drain. This circuit is a reciprocal multiterminal element.  3 Circuit Implementations and Properties  Now we propose two simple CMOS circuit implementations of the generalized sig-  moid mapping. See Figures I and 2. When the transistors are operated in the  subthreshold region the drain currents i,..., in are the outputs of a softmax map-  ping whose inputs are the gate voltages v,..., Vn. The explicit v - i characteristics  are given by  exp( vm /Vo )  i, : IcE: exp(vr/Vo), (4)  where  is a process-dependent parameter and V0 is the thermal voltage  ([Mead, 1989],p. 36). These circuits have the interesting properties of being un-  clocked and parallel. Moreover, the competition constraint is imposed naturally  through the KCL equation and the control current source. From a complexity  point of view, this circuit is most striking since it computes n exponentials, n ra-  tios, and n - I sums in one time constant! A derivation similar to the above was  independently carried out in [Waugh, 1993] for the circuit of Figure 1. Although  the first circuit implements softmax, it has two shortcomings. The first is practical:  the separation between inputs and outputs implies additional wiring. The second  is theoretical: this circuit is not a reciprocal multiterminal element, and therefore  it can't be combined with other reciprocal elements like resistive fuses or constraint  boxes to design analog, reciprocal optimization networks.  Therefore, we only consider the circuit of Figure 2 and let v and i be the n-  dimensional vectors representing the input voltages and the output currents, respec-  tively.  The softmax mapping i = F(v) represents a voltage-controlled, nonlinear,  XCompare with Lazarro et. al.'s WTA circuit [Lazzaro, 1989] whose inputs are currents  and outputs are voltages.  886 Elfadel and Wyatt  resistive multiterminal element. The main result of our paper is the following: 2  Theorem 1 The softmaz multiterminal element F is reciprocal, passive,  passive and has a co-content function given by   (v): 1IV0 In ]] expOcvm/Vo ) (5)  and a content function given by   *(i) = IVo y]  In (6)  locally  Thus, with this reciprocal, locally passive implementation of the softmax mapping,  we have added a new circuit element to the library of the circuit designer. Note  that this circuit element implements in an analog way the constraint = Yk = 1  defining the unit simplex $,. Therefore, it can be considered a nonlinear constraint  box [Harris, 1988] that can be used in reciprocal networks to implement analog  optimization algorithms.  The expression of * is a strong reminder of the information-theoretic definition of  entropy. We suggest the name "entropic resistor" for the circuit of Figure 2.  4 Conclusions  In this paper, we have discussed another instance of convergence between the sta-  tistical physics paradigm of Gibbs distributions and analog circuit implementation  in the context of the winner-take-all function. The problem of using the simple,  reciprocal circuit implementation of softmax to design analog networks for find-  ing near optimal solutions of the linear assignment problem [Kosowsky, 1991] or  the quadratic assignment problem [Simic, 1991] is still open and should prove a  challenging task for analog circuit designers.  Acknowledgements  I. M. Elfadd would like to thank Alan Yuille for many helpful discussions and Fred  Waugh for helpful discussions and for communicating the preprint of [Waugh, 1993].  This work was supported by the National Science Foundation under Grant No.  MIP-91-17724.  References  [Peterson, 1989] C. Peterson and B. SSderberg. A new method for mapping opti-  mization problems onto neural networks. International Journal of  Neural Systems, 1(1):3 - 22, 1989.  aThe concepts of reciprocity, passivity, content, and co-content are fundamental to  nonlinear circuit theory. They are carefully developed in [Wyatt, 1992].  The "Softmax" Nonlinearity 887  [Harris, 1988] J.G. Harris. Solving early vision problems with VLSI constraint  networks. In Neural Architectures .for Computer Vision Workshop,  AAAI-88, Minneapolis, MN, 1988.  [Harris, 1989] J.G. Harris, C. Koch, J. Luo, and J. Wyatt. Resistive fuses:  Analog hardware for detecting discontinuities in early vision. In  C. Mead and M. Ismail, editors, Analog VLSI Implemenation  Neural Ststems. Kluwer Academic Publishers, 1989.  [Geiger, 1991] D. Geiger and A. Yuille. A common framework for image segmen-  tation. Int. J. Computer Vision, 6:227- 253, 1991.  [Hopfield, 1984] J.J. Hopfield. Neurons with graded responses have collective com-  putational properties like those of two-state neurons. Proc. Nat'l  Acad. Sci., USA, 81:3088-3092, 1984.  [Rumelhart, 1986] D. E. Rumelhart et. ai. Parallel Distributed Processing, vol-  ume 1. MIT Press, 1986.  [Waugh, 1993] F.R. Waugh and R. M. Westervelt. Analog neural networks with  local competition. I. dynamics and stability. Phtsical Review E,  1993. in press.  [Elfadd, 1993] I.M. Elfadd. Global dynamics of winner-take-all networks. In  SPIE Proceedings, Stochastic and Neural Methods in Image Pro-  cessing, volume 2032, pages 127 - 137, San Diego, CA, 1993.  [Kosowsky, 1991] J. J. Kosowsky and A. L. Yuille. The invisible hand algorithm:  Solving the assignment problem with statistical physics. TR  91-1, Harvard Robotics Laboratory, 1991.  [Mead, 1989] Carver Mead. Analog VLSI and Neural Ststems. Addison-Wesley,  1989.  [Lazzaro, 1989] J. Lazarro, S. Ryckebush, M. Mahowald, and C. Mead. Winner-  take-all circuits of O(n) complexity. In D. S. Tourersky, editor,  Advances in Neural Information Processing Systems I, pages 703  - 711. Morgan Kaufman, 1989.  [Wyatt, 1992] J.L. Wyatt. Lectures on Nonlinear Circuit Theory. MIT VLSI  memo # 92-685, 1992.  [Simic, 1991] P.D. Simic. Constrained nets for graph matching and other  quadratic assignment problems. Neural Computation, 3:169 - 281,  1991.  
A Network Mechanism for the Determination of  Shape-From-Texture  Ko Sakai and Leif H. Finkel  Department of Bioengineering and  Institute of Neurological Sciences  University of Pennsylvania  220 South 33rd Street, Philadelphia, PA 19104-6392  ko @ ganymede.seas.upenn.edu, leif@ ganymede.seas.upenn.edu  Abstract  We propose a computational model for how the cortex discriminates  shape and depth from texture. The model consists of four stages: (1)  extraction of local spatial frequency, (2) frequency characterization, (3)  detection of texture compression by normalization, and (4) integration  of the normalized frequency over space. The model accounts for a  number of psychophysical observations including experiments based on  novel random textures. These textures are generated from white noise  and manipulated in Fourier domain in order to produce specific  frequency spectra. Simulations with a range of stimuli, including real  images, show qualitative and quantitative agreement with human  perception.  1 INTRODUCTION  There are several physical cues to shape and depth which arise from changes in projection  as a surface curves away from view, or recedes in perspective. One major cue is the  orderly change in the spatial frequency distribution of texture along the surface. In  machine vision approaches, various techniques such as Fourier transformation or wavelet  decomposition are used to determine spatial frequency spectra across a surface. The  determination of the transformation relating these spectra is a difficult problem, and  several techniques have been proposed such as an affine transformation (Super and Bovik  953  954 Sakai and Finkel  1992) or a momentum method (Krumm and Shafer 1992). We address the question of  how a biological system which has access only to limited spatial frequency information  and has constrained computational capabilities can nonetheless accurately determine  shape and depth from texture. For example, the visual system might avoid the direct  comparison of frequency spectra themselves and instead rely on a simpler  characterization of the spectra such as the mean frequency, peak frequency, or the  gradient of a frequency component (Sakai and Finkel 1993; Turner, Gerstein, Bajcsy  1991). In order to study what frequency information is actually utilized by humans, we  created novel random texture patterns and carried out psychophysical experiments with  these stimuli. These patterns are generated by manipulating the frequency components of  white noise stimuli in the Fourier domain so as to produce stimuli with exactly specified  frequency spectra. Based on these experiments, we propose a network mechanism for the  perception of shape-from-texture which takes into account physiological and anatomical  constraints as well as computational considerations.  2 MODEL FOR SHAPE FROM TEXTURE  The model consists of four major processes: extraction of the local spatial frequency at  each orientation, frequency characterization, determination of texture compression by  frequency normalization, and the integration of the normalized frequency over space. A  schematic illustration of the model is shown in figure 1. Our psychophysical experiments  suggest that the visual system may use spatially averaged peak frequency for  characterizing the frequency distribution. The change of surface orientation is  determined from the locally aligned compression of texture which is detected by  frequency normalization followed by lateral inhibition among different orientations.  Depth is then computed from the integration of the normalized frequency over space.  The model is implemented in feed-forward distributed networks and simulated using the  NEXUS neural network simulator (Sajda, Sakai, Yen and Finkel 1993).  3 MOTIVATION FOR EACH STAGE OF THE MODEL  The frequency extraction is carried out by units modeling complex cells in area V1.  These units have subunits with On and Off center difference of Gaussian(DOG) masks  tuned to specific frequencies and orientations. The units take local maximum of the  subunits. As in energy-based models (Bergen and Adelson 1989; Malik and Perona  1990), these units accomplish some major aspects of complex cell functions in the space  domain including invariance to the direction of contrast and spatial phase.  The second stage of the model extracts spatially averaged peak frequency. In order to  examine what frequency information is actually utilized by humans, we created random  texture patterns with specific frequency spectra generated by manipulating the frequency  components of a white noise pattern in Fourier domain. Figure 2 shows a vertical  cylinder and a tilted perspective plane constructed by this technique from white noise.  We are able to see the three dimensional shape of the cylinder in (1). The stimuli were  constructed by making each frequency component undergo a step change at some  A Network Mechanism for the Determination of Shape-from-Texture 955  o= i  Early Vision  Stage  max (x y)  Frequency  Characterzabon  F(x,y...) o  F low  Frequency  Normalization and  Lateral Inhibition  Integration  Figure 1. A schematic illustration of the shape-from-texture model consisting of four  major stages. The early vision stage models major spatial properties of complex cells in  order to decompose local spatial frequency. The second stage characterizes the  frequency by the spatially averaged peak frequency. The third stage detects locally  aligned texture compression by normalizing frequency and taking lateral inhibition  among orientation channels. The last stage determines 3D depth by integrating the  amount of texture compression - which corresponds to the local surface slant. Indices "f"  and "o" denote frequency and orientation channels, respectively. max, min, ave, and LI  stand for taking maximum, minimum, average, and lateral inhibition. The vertical bar  indicates that the function is processed independently within each of denoted channels.  956 Sakai and Finkel  position along the cylinder; higher frequencies undergo the change at positions closer to  the cylinder's edges. Since the gradient of each frequency component is always either  zero or infinity, this suggests that gradients of individual frequency components over  space do not serve as a dominant cue for three dimensional shape perception. Similar  experiments have been conducted using various stimuli with controlled frequency  spectra. The results of these experiments suggest that averaged peak frequency is a  strong cue for the human perception of three dimensional shape and depth.  The third stage of the model normalizes local frequencies by the global lowest frequency  on the surface. We assume that the region containing the global lowest frequency is the  frontal plane standing vertically with respect to the viewer. One of the justifications for  this assumption can be seen in simple artificial images shown in figure 3. In both (1) and  (2), the bottom region looks vertical to us, and the planes above this region looks slanted,  although the patterns of the center region of (1) and the lower region of (2) are identical.  From a computational point of view, the normalization of frequency corresponds to an  approximation of the relation between local slant and spatial frequency. Depth, Z, as a  function of X (see figure 4) is given by:  Z(x) = tan{ cos4(F ) }dx= ( ))2- I dx eq.(l)  F(x)  where Fo is the global lowest frequency. Considering a boundary condition, Z(x) = 0, if  F(x) = Fo, the integrand can be reasonably approximated by (F(x) - F0) ! F 0 . The second  stage of the model actually computes this value, and a later stage carries out the  integration.  Figure 2. Random texture patterns generated by manipulating the frequency components  of white noise in Fourier domain. A horizontal cylinder embedded in white noise (1),  and a tilted plane (2).  A Network Mechanism for the Determination of Shape-from-Texture 957  The second half of this stage detects the local alignment of texture compression. This  local alignment is detected by taking the lateral inhibition of normalized frequencies  among different orientations. Recent psychophysical experiments (Todd and Akerstrom  1987; Cumming, Johnson, and Parker 1993) show that the compression of texture in a  single orientation is a cue for the perception of shape-from-texture. We can confirm this  result from figure 5. Three images on the top of this figure have compression in a single  orientation, but those on the bottom do not. We clearly see smooth three dimensional  ellipsoids from the top images but not from the bottom images.  The last stage of the model computes the integral of the normalized frequency in order to  obtain depth. This integration begins from the region with lowest spatial frequency and  follows the path of the local steepest descent in spatial frequency.  Figure 3. Objects consist of three planes(left), and two planes(right). In both stimuli, the  bottom regions look vertical to us, and the planes above this region look slanted, although  the patterns of the center region of (1) and the lower region of (2) are identical.  Depth: Z(x)  Z(Xl)  x  Figure 4. The coordinate system for the equation (1). Depth, Z, is given as a function of  position, X.  958 Sakai and Finkel  4 SIMULATIONS  A quantitative test of the model was carried out by constructing ellipsoids with different  eccentricities and texture patterns shown in figure 5. Results are plotted in figure 6. For  the regular ellipsoids, there is a linear relation between real depth and that determined by  the model. This linear relation agrees with psychophysical experiments (Todd and  Akerstrom 1987; Btilthoff 1991) showing similar human performance for such stimuli.  All of the irregular texture patterns produced little perception of depth, in agreement with  human performance.  Many artificial and real images have been tested with the model and show good  agreement with human perception. For an example, a real image of a part of cantaloupe,  and its computed depth are shown in figure 7. Real images were obtained with a CCD  camera and were input to NEXUS via an Imaging Technology's S 151 image processor.  Figure 5. (Top) Regular ellipsoids with eccentricities of 1,2, and 4. (Bottom) Irregular  texture patterns: (left) no compression with regular density change, (middle) randomly  oriented regular compression, (right) pan-orientational regular compression.  A Network Mechanism for the Determination of Shape-from-Texture 959  4OO  3OO  2OO  lOO  0 I 2 3 4 5 6  Eccentricity  D regular ellipsoids   no compression   randomly oriented  compression   pan-orientational  compression  Figure 6. Depth perceived by the model as a function of actual eccentricity. The  simulated depth of regular ellipsoids shows a linear relation to the actual depth. Irregular  patterns produced little depth, in agreement with human perception.  Figure 7. An example of the model's response to a real image. A part of cantaloupe (left),  and its depth computed by the model(right).  5 CONCLUSIONS  (1) We propose a biologically-based network model of shape-from-texture based on the  determination of change in spatial frequency.  (2) Preliminary psychophysical evidence suggests that the spatially averaged peak  frequency is employed to characterize the spatial frequency distribution rather than using  a frequency spectrum or each component of frequency.  960 Sakai and Finkel  (3) This characterization is validated by psychophysical experiments using novel random  textures with specified frequency spectra. The patterns are generated from white noise  and manipulated in Fourier domain in order to realize specific frequency characteristics.  (4) The model has been tested with a number of artificial stimuli and real images taken  by video camera. Responses show qualitative and quantitative agreements with human  perception.  Acknowledgments  This work is supported by grants from The Office of Naval Research (N00014-90-J-1864,  N00014-93-1-0681), The Whitaker Foundation, and The McDonnell-Pew Program in  Cognitive Neuroscience.  References  Super, B.J. and Bovik, A.C. (1992), Shape-from-texture by wavelet-based measurement  of local spectral moments. Proc. IEEE CVPR 1992, p296-300  Krumm, J. and Shafer, S.A. (1992), Shape from periodic texture using the spectrogram.  Proc. IEEE CVPR 1992, p284-289  Sakai, K. and Finkel, L.H. (1994), A cortical mechanism underlying the perception of  shape-from-texture. In F.Eeckman, et al.(ed.), Computation and Neural Systems 1993,  Norwell, MA: Kluwer Academic Publisher [in press]  Sajda, P., Sakai, K., Yen, S-C., and Finkel, L.H. (1993), In Skrzypek, J. (ed.), Neural  Network Simulation Environments, Norwell, MA: Kluwer Academic Publisher[in press]  Bergen, J.R. and Adelson, E.H. (1988), Visual texture segmentation and early vision.  Nature, 333, p363-364  Malik, J. and Perona, P. (1990), Preattentive texture discrimination with early vision  mechanisms. J. Opt. Soc. Am., A Vol.7, No.5, p923-932  Cumming, B.G., Johnston, E.B., and Parker, A.J. (1993), Effects of different texture cues  on curved surfaces viewed stereoscopically. Vision Res. Vol.33, No5, p827-838  Todd, J.T. and Akerstrom, R.A. (1987), Perception of three-dimensional form from  patterns of optical texture. Journal of Experimental Psychology, vol. 13, No.2, p242-255,  Turner, M.R., Gerstein, G.L., and Bajcsy, R. (1991), Underestimation of visual texture  slant by human observers: a model. Biol. Cybern. 65, p215-226  Btilthoff, H.H. (1991), Shape from X: Psychophysics and computation. In Landy, M.S.,  et al.(ed.) Computational Models of Visual Processing, Cambridge, MA: MIT press,  p305-330  
Cross-Validation Estimates IMSE  Mark Plutowski t,  Shinichi Sakata   Halbert White  Department of Computer Science and Engineering   Department of Economics  , Institute for Neural Computation  University of Clifornia, San Diego  Abstract  Integrated Mean Squared Error (IMSE) is a version of the usual  mean squared error criterion, averaged over all possible training  sets of a given size. If it could be observed, it could be used  to determine optimal network complexity or optimal data sub-  sets for efficient training. We show that two common methods of  cross-validating average squared error deliver unbiased estimates  of IMSE, converging to IMSE with probability one. These esti-  mates thus make possible approximate IMSE-based choice of net-  work complexity. We also show that two variants of cross validation  measure provide unbiased IMSE-based estimates potentially useful  for selecting optimal data subsets.  I Summary  To begin, assume we are given a fixed network architecture. (We dispense with  this assumption later.) Let z N denote a given set of N training examples. Let  QN(z N) denote the expected squared error (the expectation taken over all possible  examples) of the network after being trained on z N. This measures the quality of  fit afforded by training on a given set of N examples.  Let IMSEN denote the Integrated Mean Squared Error for training sets of size  N. Given reasonable assumptions, it is straightforward to show that IMSEN ----  E[QN(ZN)] -- o '2, where the expectation is now over all training sets of size N, Z N  is a random training set of size N, and a2 is the noise variance.  Let Cs '- CN(z N) denote the "delete-one cross-validation" squared error measure  for a network trained on z N. Cs is obtained by training networks on each of the N  training sets of size N- 1 obtained by deleting a single example; the measure follows  391  392 Plutowski, Sakata, and White  by computing squared error for the corresponding deleted example and averaging the  results. Let GN, M = GN, M(Z N, M) denote the "generalization" measure obtained  by separating the available data of size N + M into a training set z N of size N, and  a validation ("test") set M of size M; the measure follows by training on z N and  computing averaged squared error over M.  We show that CN is an unbiased estimator of E[QN_x(zN)], and hence, estimates  IMSEN_x up to noise variance. Similarly, GN, M is an unbiased estimator of  E[QN(ZN,M]. Given reasonable conditions on the estimator and on the data  generating process we demonstrate convergence with probability I of GN, M and  aN to E[QN(ZN)] as N and M grow large.  A direct consequence of these results is that when choice is restricted to a set of  network architectures whose complexity is bounded above a priori, then choosing  the architecture for which either Cs (or CS, M) is minimized leads to choice of  the network for which IMSEN is nearly minimized for all N (respectively, N, M)  sufficiently large.  We also provide results for training sets sampled at particular inputs. Conditional  IMSE is an appealing criterion for evaluating a particular choice of training set in  the presence of noise. These results demonstrate that delete-one cross-validation es-  timates average MSE (the average taken over the given set of inputs,) and that hold-  out set cross-validation gives an unbiased estimate of E[QN(ZN)I Zv = (z N, yN)],  given a set of N input values z N for which corresponding (random) output values  yN are obtained. Either cross-validation measure can therefore be used to select  a representative subset of the entire dataset that can be used for data compaction,  or for more efficient training (as training can be faster on smaller datasets) [4].  2 Definitions  2.1 Learning Task  We consider the learning task of determining the relationship between a random  vector X and a random scalar Y, where X takes values in a subset X of r, and Y  takes values in a subset Y of g. (e.g. X = r and Y = g). We refer to X as the  input space. The learning task is thus one of training a neural network with r inputs  and one output. It is straightforward to extend the following analysis to networks  with multiple targets. We make the following assumption on the observations to be  used in the training of the networks.  Assumption I X is a Borel subset of r and Y is a Borel subset of . Let Z =  X x Y and f2 _-- Z O =- xxZ. Let (f2,',7 ) be a probability space with .T =  The observations on Z -- (X ,Y) to be used in the training of the network are a  realization of an i.i.d. stochastic process {Zi -= (X, Y/): f2 - X x Y}.  When w 6 f2 is fixed, we write zi = Zi(w) for each i = 1, 2, .... Also write Z v =  (Zx,..., Zv) and z  -- (zx,..., z).  Assumption 1 allows uncertainty caused by measurement errors of observations  as well as a probabilistic relationship between X and Y . It, however, does not  prevent a deterministic relation ship between X and Y such that Y = g(X) for  some measurable mapping g  Rr __ R.  Cross-Validation Estimates IMSE 393  We suppose interest attaches to the conditional expectation of Y given X, written  g(z) = E(YIX ). The next assumption guarantees the existence of E(YilXi) and  E(eilXi), ei -= Yi - E(YilXi). Next, for convenience, we assume homoscedasticity  of the conditional variance of Y/ given Xi.  Assumption 2 E(Y 2) < cx.  Assumption 3 E(qlXx) - where  is a strictly positive constant.  2.2 Network Model  Let fP(., .)  X x Wp -- y be a network function with the "weight space" Wp,  where p denotes the dimension of the "weight space" (the number of weights.) We  impose some mild conditions on the network architecture.  Assumption 4 For each p  {1,2,...,fi}, i6  N, Wp is a compact subset ofp,  and fP  X x Wp --  satisfies the following conditions:  1. if(., w)  X - Y is measurable for each w  Wp ;  . if(z, .)' W r -- Y is continuous for all z  X.  We further make a joint assumption on the underlying data generating process  and the network architecture to assure that the training dataset and the networks  behaves appropriately.  Assumption 5 There ezists a function D  X - R+ = [0, c) such that for each  z  X and w  We, Ire(z, w)l _< D(z), and E [(D(X)) ] < c.  Hence, fP is square integrable for each w p  W p. We will measure network per-  formance using mean squared error, which for weights w p is given by .(wP;p)   E [(Y - fP(X, wp)2]. The optimal weights are the weights that minimize .(wP;p).  The set of all optimal weights are given by W p* =- {w*  Wp  A(w*;p) _  A(w;p) for any w 6 Wp}. The index of the best network is p*, given by the smallest  p minimizing minopEwp .(wP;p), p  {1,2,...,}.  2.3 Least-Squares Estimator  When assumptions 1 and 4 hold, the nonlinear least-squares estimator exists.  mally, we have  For-  Lemma I Suppose that Assumptions I and d hold. Then 1. For each N 6 N,  there ezists a measurable function 1N(.;p) ' Z N -- W e such that 1N(ZN;p) solves   N-XEiN=x(yi_f(Xi,w))   the following problem with probability one: mmoEW p   . A(.;p)  Wp -- R is continuous on Wp, and W p* is not empty.  For convenience, we also define bv  9 -- We by bv(co ) = 1N(ZN (CO); p) for  each co  9. Next let i,ie,...,iv be distinct natural numbers and let ,v =   ., ir). Then lv(, v) given above solves N Ej=x( ij - f(Xij, wp))2 with  Z ' y.  probability one. In particular, we will consider the estimate using the dataset z_i  made by deleting the ith observation from z v. Let Z i be a random matrix made  394 Plutowski, Sakata, and White  by deleting the ith row from Z N. Thus, IN-1 (z_Ni;p) is a measurable least squares  estimator and we can consider its probabilistic behavior.  3 Integrated Mean Squared Error  Integrated Mean Squared Error (IMSE) has been used to regulate network complex-  ity [9]. Another (conditional) version of IMSE is used as a criterion for evaluating  training examples [5, 6, 7, 8]. The first version depends only on the sample size,  not the particular sample. The second (conditional) version depends additionally  upon the observed location of the examples in the input space.  3.1 Unconditional IMSE  The (unconditional) mean squared error (MSE) of the network output at a partic-  ular input value  is  MN(i:;p) = E [{g(i) - f(i:,lN(ZN;p))}2] . (1)  Integrating MSE over all possible inputs gives the unconditional IMSE:  = f  (2)  = E (3)  where p is the input distribution.  3.2 Conditional IMSE  To evaluate exemplars obtained at inputs :r N, we modify Equation (1) by condi-  tioning on z v, giving  Mv(lzV;p) - E [{g(5:)- f(i:,lv(Zv))}2lXV -  The conditional IMSE (given inputs z N) is then  IMSEN(xN;p) -- f mN(zlzN;p)p(dz) (4)  = E (5)  4 Cross-Validation  Cross-validatory measures have been used successfully to assess the performance of  a wide range of estimators [10, 11, 12, 13, 14, 15]. Cross-validatory measures have  been derived for various performance criteria, including the Kullback-Liebler Infor-  mation Criterion (KLIC) and the Integrated Squared Error (ISE, asymptotically  equivalent to IMSE) [16]. Although provably inappropriate in certain applications  [17, 18], optimality and consistency results for the cross-validatory measures have  been obtained for several estimators, including linear regression, orthogonal series,  splines, histograms, and kernel density estimators [16, 19, 20, 21, 22, 23, 24]. The  authors are not aware of similar results applicable to neural networks, although two  more general, but weaker results do apply [26]. A general result applicable to neural  networks shows asymptotic equivalence between cross-validation and Akaike's Cri-  terion for network selection [25, 29], as well as between cross-validation and Moody's  Criterion [30, 29].  Cross-Validation Estimates IMSE 395  4.1 Expected Network Error  Given our assumptions, we can relate cross-validation to IMSE. For clarity and  notational convenience, we first introduce a measure of network error closely related  to IMSE. For each weight wp  Wp, we have defined the mean squared error A(wp; p)  in Section 2.2. We define QN to map each dataset to the mean squared error of the  estimated network  QN(zN;p) ---- A(1N(zN;p);p).  When Assumption 3 holds, we have  A(wP;p) -- E[(g(X)- f(X, wP)) 2] +o '2  = E [(g(XN+x) -- f(XN+x, wr)) 2] + r 2  as is easily verified. We therefore have  Qv(zV ;p) = E [(g(Xv+x) - f(XN+x,IN(ZN;p)))2IZN = Z v] + O '2.  Thus, by using the law of iterated expectations, we have  E [QN(ZN;p)] -- IMSEN(p)+ er 2.  Likewise, given x N  X N,  p)lxV = = (6)  4.2 Cross-Validatory Estimation of Error  In practice we work with observable quantities only. In particular, we must esti-  mate the error of network p over novel data ("generalization") from a finite set of  examples. Such an estimate is given by the delete-one cross-validation measure:  N  1  CN(zN;P) --  Y (Yi -- f(xi,lN-x(z-Ni;P))) 2 (7)  i----1  where z N. denoS.es the training set obtained by deleting the ith example. Using  z_/-instet of z  avoids a downward bias due to testing upon exampies used in  training, as we show below (Theorem 3.) Another version of cross-validation is  commonly used for evaluating "generalization" when an abundant supply of novel  data is available for use as a "hold-out" set:  M  1  GN'M(zN';?M;P) = M  (Oi- f('i,lN(zN;P))) 2 (8)  ,  i--1  where M _ (ZN+X,...,ZN+M).  5 Expectation of the Cross-Validation Measures  We now consider the relation between cross-validation measure and IMSE. We ex-  amine delete-one cross-validation first.  Proposition 1 (Unbiasedness of Cv) Let Assumptions I through 5 hold. Then  for given N, Cs is an unbiased estimator of IMSEN-x (p) + tr 2'  E [Cv(ZV;p)] = IMSEv_x(p) +o '2. (9)  396 Plutowski, Sakata, and White  With hold-out set cross-validation, the validation set Z M gives i.i.d. information  regarding points outside of the training set Z N.  Proposition 2 (Unbiasedness of GN, M) Let Assumptions I through 5 hold. Let  ,M _-- (ZN+x,..., ZM) . Then for given N and M, GN, M is an unbiased estimator  of IMSEv (p) + a 2:  =  The latter result is appealing for large M, N. We expect delete-one cross-validation  to be more appealing when training data is not abundant.  6  Expectation of Cross-Validation when Sampling at  Selected Inputs  We obtain analogous results for training sets obtained by sampling at a given set  of inputs x N. We first consider the result for delete-one cross-validation.  Proposition 3 (Expectation of Cs given xN Let Assumptions I through 5  hold. Then, given a particular set of inputs, x , CN is an unbiased estimator  of average MSEN-x + rr 2, the average taken over xN :  N  1  E [Cv(ZV,P)lXV-x v] - y.MN_x(xi[x_/;p) + ,  i=1  where x_Ni is a matrix made by deleting the ith row of x N.  This essentially gives an estimate of MSEv_x limited to x  x v, losing a degree of  freedom while providing no estimate of the MSE off of the training points. For this  average to converge to IMSEv_, it will suffice for the empirical distribution of  xN, v, to converge to/N, i.e., fiV =Y /N. We obtain a stronger result for hold-out  set cross-validation. The hold-out set gives independent information on MSEv off  of the training points, resulting in an estimate of IMSEv for given x v.  Proposition 4 (Expectation of GV,M given v) Let Assumptions I through 5  hold. Let M = (Zv+x,..., ZV+M)  GN,M is an unbiased estimator of of lMSEN(xN;p) +  7 Strong Convergence of Hold-Out Set Cross-Validation  Our conditions deliver not only unbiasedhess, but also convergence of hold-out set  cross-validation to IMSEv, with probability 1.  Theorem 1 (Convergence of Hold-Out Set w.p. 1) Let Assump-  tions I through 5 hold. Also let ,M __-- (ZN+x,...,ZN+M) . Ill or some A > 0  a sequence {MN} of natural numbers satisfies MN > AN for any N = 1,2,...,  then  p) p)]  , , - ; , O, asN--oo.  Cross-Validation Estimates IMSE 397  8 Strong Convergence of Delete-One Cross-Validation  Given an additional condition (uniqueness of optimal weights) we can show strong  convergence for delete-one cross-validation. First we establish uniform convergence  of the estimators P(Zi) to optimal weights (uniformly over 1 < i < N.)  Theorem 2 Let Assumptions I through 5 hold. Let Z} be the dataset made by  deleting the kth observation from Z N. Then  max d (/N_(Z_i;p),W p') --0 a.s.- as N --, ()  where d(w, WP*) _--inf.EWp. IIw- w*ll.  This convergence result leads to the next result that the delete-one cross validation  measure does not under-estimate the optimized MSE, namely, infopEW,  Theorem 3 Let Assumptions I through 5 hold. Then  liminfCN(ZN;p) >_ min A(w;p) a.s.-7 .  N-*oo wW  When the optimum weight is unique, we have a stronger result about convergence  of the delete-one cross validation measure.  Assumption 6 W p* is a singleton, i.e., W p* has only one element.  Theorem 4 Let Assumptions I through 6 hold. Then  CN (ZN;p) -- E [QN(ZN;p)] -- 0 a.s. as N --  O Conclusion  Our results justify the intuition that cross-validation measures unbiasedly and con-  sistently estimate the expected squared error of networks trained on finite training  sets, therefore providing means of obtaining IMSE-approximate methods of select-  ing appropriate network architectures, or for evaluating particular choice of training  set.  Use of these cross-validation measures therefore permits us to avoid underfitting  the data, asymptotically. Note, however, that although we also thereby avoid over-  fitting asymptotically, this avoidance is not necessarily accomplished by choosing  a minimally complex architecture. The possibility exists that IMSEN-x(p) =  IMSEN-x(p + 1). Because our cross-validated estimates of these quantities are  random we may by chance observe CN(ZN;p) > CN(ZN;p + 1) and therefore se-  lect the more complex network, even though the less complex network is equally  good. Of course, because the IMSE's are the same, no performance degradation  (overfitting) will result in this solution.  Acknowledgement s  This work was supported by NSF grant IRI 92-03532. We thank David Wolpert,  Jan Larsen, Jeff Racine, Vjachislav Krushkal, and Patrick Fitzsimmons for valuable  discussions.  398 Plutowski, Sakata, and White  References  [41  [51  [7]  [8]  [9]  [lO]  [11]  [12]  [13]  [14]  [15]  [16]  [1] White, H. 1989. "Learning in Artificial Neu-  ral Networks: A Statistical Perspective." Neu-  ral Computation, I 4, pp.425-464. MIT Press,  Cambridge, MA.  [2] Plutowski, Mark E., Shinichi Sakata, and Hal-  bert White. 1993. "Cross-validation delivers  strongly consistent unbiased estimates of Inte-  grated Mean Squared Error." To appear.  [3] Plutowski, Mark E., and Halbert White. 1993.  "Selecting concise training sets from clean data."  IEEE Transactions on Neural Networks. 4, 3,  pp.305-318.  Plutowski, Mark E., Garrison Cottrell, and Hal-  bert White. 1992. "Learning Mackey-Glass from  25 examples, Plus or Minus 2." (Presented  at 1992 Neural Information Processing Systems  conference.) Jack D. Cowan, Gerald Tesauro,  Joshua Aspector (eds.), Advances in neural in-  formation processing systems 6, San Mateo, CA:  Morgan Kaufmann Publishers.  Fedorov, V.V. 1972. Theory of Optimal Ex-  periments. Academic Press, New York.  Box,G, and N.Draper. 1987. Empirical Model-  Building and P. esponse Surfaces. Wiley, New  York.  Khuri, A.I., and J.A.Cornell. 1987. P. esponse  Surfaces (Designs and Analyses). Marcel  Dekker, Inc., New York.  Faraway, Julian J. 1990. "Sequential design for  the nonparametric regression of curves and sur-  faces." Technical Report 177, Department of  Statistics, The University of Michigan.  Geman, Stuart, Elle Bienenstock, and lCten  Doursat. 1992. "Neural networks and the  bias/variance dilemma." Neural Computation. 4,  1, 1-58.  Stone, M. 1959. "Application of a measure of in-  formation to the design and comparison of re-  gression experiments." Annals Math. Star. 30  55-69  "Cross-validatory choice and assessment of sta-  tistical predictions." J.R. Statist. Soc. B. 86, 2,  111-47.  ]Bowman, Adrian W. 1984. "An alternative  method of cross-validation for the smoothing of  density estimates." Biometrika (1984), 71, 2, pp.  353-60.  Bowman, Adrian W., Peter Hall, D.M. Titter-  ington. 1984. "Cross-validation in nonparametric  estimation of probabilities and probability den-  sities." Biometrika (1984), 71, 2, pp. 341-51.  Matron, M. 1987. "A comparison of cross-  validation techniques in density estimation." The  Annals of Statistics. 15, 1, 152-162.  Wahba, Grace. 1990. Spline Models for Ob-  servational Data. v. 59 in the CBMS-NSF Re-  glonal Conference Series in Applied Mathemat-  ics, SIAM, Philadelphia, PA, March 1990. Soft-  cover, 169 pages, bibliography, author index.  ISBN 0-89871-244-0  Hall, Peter. 1983. "Large sample optimality of  least squares cross-vMidation in density estima-  tion." The Annals of Statistics. 11, 4, 1156-1174.  [17] Schuster, E.F., and G.G. Gregory. 1981. "On the  nonconsistency of maximum likelihood nonpara-  metric density estlmators". Comp. Sci.  Statis-  tics: Proc. 13th Symp. on the Interface. W.F.  Eddy ed. 295-298. Springer-Verlag.  [18] Chow, Y.-S. and S.Geman, L.-D. Wu. 1983.  "Consistent cross-validated density estimation."  The Annals of Statistics. 11, 1, 25-38.  [19] ]Bowman, Adrian W. 1980. "A note on consis-  tency of the kernel method for the analysis of  categorical data." Biometrika (1980), 67, 3, pp.  682-4.  [20] Stone, Charles J. 1984 "An asymptotically opti-  mal window selection rule for kernel density  timates." The Annals of Statistics. 12, 4, 1285-  1297.  [21] Li, Ker-Chau. 1986. "Asymptotic optimality of  C L and generalized cross-vMidation in ridge re-  gression with application to spline smoothing."  The Annals of Statistics. 14, 3, 1101-1112.  [22] Li, Ker-Chau. 1987. "Asymptotic optimality for  C'p, C'L, cross-validation, and generalized cross-  validation: discrete index set." The Annals of  Statistics. 15, 3, 958-975.  [23] Utreras, Florencio I. 1987. "On generalized cross-  validation for multivariate smoothing spline  functions." SIAM J. Sci. Star. Cornput. 8, 4, July  1987.  [24] Andrews, Donald W.K. 1991. "Asymptotic opti-  mality of generalized C' L, cross-vMidation, and  generalized cross-validation in regression with  heteroskedastic errors." Journal of Econometrics.  47 (1991) 359-377. North-Holland.  [25] Stone, M. 1977. "An asymptotic equivalence of  choice of model by cross-validation and Akaike's  criterion." J. Roy. Stat. Soc. Ser B, 39, 1, 44-47.  [26] Stone, M. 1977. "Asymptotics for and against  cross-validation." Biometrika. 64, 1, 29-35.  [27] BillingsIcy, Patrick. 1986. Probability and  Measure. Wiley, New York.  [28] Jennrich, R. 1969. "Asymptotic properties of  nonlinear least squares estimators." Ann. Math.  Stat. 40, 633-643.  [29] Liu, Yong. 1993. "Neural network model selec-  tion using asymptotic jackknife estimator and  cross-validation method." In Giles, C.L., Han-  son, S.J., and Cowan, J.D. (eds.), Advances in  neural information processing systems 5, San  Mateo, CA: Morgan Kaufmann Publishers.  [30] Moody, John E. 1992. "The effective number  of parameters, an analysis of generalization and  regularization in nonlinear learning system." In  Moody, J.E., Hanson, S.J., and Lippmann, R.P.,  (eds.), Advances in neural information process-  ing systems 4, San Mateo, CA: Morgan Kauf-  mann Publishers.  [31] Bailey, Timothy L. and Charles Elkan. 1993.  "Estimating the accuracy of learned concepts."  To appear in Proc. International Joint Confer-  ence on Artificial Intelligence.  [32] White, Halbert. 1993. Estimation, Inference,  and Specification Analysis. Manuscript.  [33] White, Halbert. 1984. Asymptotic Theory for  Econometricians. Academic Press.  [34] Klein, Erwin and Anthony C. Thompson. 1984  Theory of correspondences : including ap-  plications to mathematical economics. Wi-  ley.  
Backpropagation Convergence Via  Deterministic Nonmonotone Perturbed  Minimization  O. L. Mangasarian & M. V. Solodov  Computer Sciences Department  University of Wisconsin  Madison, WI 53706  Email: olvi@cs.wisc.edu, solodov@cs.wisc.edu  Abstract  The fundamental backpropagation (BP) algorithm for training ar-  tificial neural networks is cast as a deterministic nonmonotone per-  turbed gradient method. Under certain natural assumptions, such  as the series of learning rates diverging while the series of their  squares converging, it is established that every accumulation point  of the online BP iterates is a stationary point of the BP error func-  tion. The results presented cover serial and parallel online BP,  modified BP with a momentum term, and BP with weight decay.  1 INTRODUCTION  We regard  problem  training artificial neural networks as an unconstrained minimization  N  min f(x):= y. fj(x) (1)  xE '  j--1  where fj: " - , j = 1,..., N are continuously differentiable functions from the  n-dimensional real space " to the real numbers . Each function fj represents the  error associated with the j-th training example, and N is the number of examples  in the training set. The n-dimensional variable space here is that of the weights  associated with the arcs of the neural network and the thresholds of the hidden and  383  384 Mangasarian and Solodov  output units. For an explicit description of f(x) see (Mangasarian, 1993). We note  that our convergence results are equally applicable to any other form of the error  function, provided that it is smooth.  BP (Rumelhart,Hinton & Williams, 1986; Khanna, 1989) has long been successfully  used by the artificial intelligence community for training artificial neural networks.  Curiously, there seems to be no published deterministic convergence results for this  method. The primary reason for this is the nonmonotonic nature of the process.  Every iteration of online BP is a step in the direction of negative gradient of a partial  error function associated with a single training example, e.g. fj(x) in (1). It is clear  that there is no guarantee that such a step will decrease the full objective function  f(x), which is the sum of the errors for all the training examples. Therefore a single  iteration of BP may, in fact, increase rather than decrease the objective function  f(x) we are trying to minimize. This difficulty makes convergence analysis of BP  a challenging problem that has currently attracted interest of many researchers  (Mangasarian & Solodov, 1994; Gaivoronski, 1994; Grippo, 1994; Luo & Tseng,  1994; White, 1989).  By using stochastic approximation ideas (Kashyap,Blaydon & Fu, 1970; Ermoliev &  Wets, 1988), White (White, 989) has shown that, under certain stochastic assump-  tions, the sequence of weights generated by BP either diverges or converges almost  surely to a point that is a stationary point of the error function. More recently,  Gaivoronski obtained stronger stochastic results (Gaivoronski, 994). It is worth  noting that even if the data is assumed to be deterministic, the best that stochastic  analysis can do is to establish convergence of certain sequences with probability  one. This means that convergence is not guaranteed. Indeed, there may exist some  noise patterns for which the algorithm diverges, even though this event is claimed  to be unlikely.  By contrast, our approach is purely deterministic. In particular, we show that  online BP can be viewed as an ordinary perturbed nonmonotone gradient-type  algorithm for unconstrained optimization (Section 3). We note in the passing, that  the term gradient descent which is widely used in the backpropagation and neural  networks literature is incorrect. From an optimization point of view, online BP  is not a descent method, because there is no guaranteed decrease in the objective  function at each step. We thus prefer to refer to it as a nonmonotone perturbed  gradient algorithm.  We give a convergence result for a serial (Algorithm 2.1), a parallel (Algorithm 2.2)  BP, a modified BP with a momentum term, and BP with weight decay. To the best  of our knowledge, there is no published convergence analysis, either stochastic or  deterministic, for the latter three versions of BP. The proposed parallel algorithm is  an attempt to accelerate convergence of BP which is generally known to be relatively  slow.  2  CONVERGENCE OF THE BACKPROPAGATION  ALGORITHM AND ITS MODIFICATIONS  We now turn our attention to the classical BP algorithm for training feedforward  artificial neural networks with one layer of hidden units (Rumelhart,Hinton &  Backpropagation Convergence via Deterministic Nonmonotone Perturbed Minimization 385  Williams, 1986; Khanna, 1989). Throughout our analysis the number of hidden  units is assumed to be fixed. The choice of network topology is a separate issue  that is not addressed in this work. For some methods for choosing the number of  hidden units see (Courrien, 1993; Arai, 1993).  We now summarize our notation.  N: Nonnegative integer denoting number of examples in the training set.  i = 1, 2,...: Index number of major iterations (epochs) of BP. Each major itera-  tion consists of going through the entire set of error functions fx (x),..., rs(x).  j ---- 1,..., N: Index of minor iterations. Each minor iteration j consists of a step  in the direction of the negative gradient --7fm(j)(Z i'j) and a momentum step. Here  re(j) is an element of the permuted set {1,..., N}, and z i,j is defined immediately  below. Note that if the training set is randomly permuted after every epoch, the  map m(-) depends on the index i. For simplicity, we skip this dependence in our  notation.  a: i: Iterate in n of major iteration (epoch) i = 1, 2, ....  z id : Iterate in 3 ' of minor iteration j = 1,..., N, within major iteration i =  1, 2, .... Iterates z i,j can be thought of as elements of a matrix with N columns and  infinite number of rows, with row i corresponding to the i-th epoch of BP.  By r/i we shall denote the learning rate (the coefficient multiplying the gradient),  and by ci the momentum rate (the coefficient multiplying the momentum term).  For simplicity we shall assume that the learning and momentum rates remain fixed  within each major iteration. In a manner similar to that of conjugate gradients  (Polyak, 987) we reset the momentum term to zero periodically.  Algorithm 2.1. Serial Online BP with a Momentum Term.  Start with any x   Rn. Having x i, stop if 7 f(xi) : O, else compute x i+1  follows   as  zi,1 __ ;r i  zi,j+X __ zi,j _ ]i fm(j)(Z i'j) q- oiAz i'j ,  xi+i -- zi,N+l  where  j = 1,...,N  (2)  ($)  (4)  Azi,j = { 0 if j = 1  z i,j - z i,j-1 otherwise (5)  0<r/i< 1, 0<_ai<l  Remark 2.1. Note that the stopping criterion of this algorithm is typically that  used in first order optimization methods, and is not explicitly related to the abil-  ity of the neural network to generalize. However, since we are concerned with  convergence properties of BP as a numerical algorithm, this stopping criterion is  386 Mangasarian and Solodov  justified. Another point related to the issue of generalization versus convergence is  the following. Our analysis allows the use of a weight decay term in the objective  function (Hinton, 1986; Weigend,Huberman & Rumelhart, 1990) which often yields  a network with better generalization properties. In the latter case the minimization  problem becomes  N  min fix) :: Z f(x) + Allxll - (6)  xER'  j-1  where  is a small positive scaling factor.  Remark 2.2. The choice of ai = 0 reduces Algorithm 2.1 to the original BP  without a momentum term.  Remark 2.3. We can easily handle the "mini-batch" methods (Mller, 1992) by  merely redefining the meaning of the partial error function fj to represent the error  associated with a subset of training examples. Thus "mini-batch" methods also fall  within our framework.  We next present a parallel modification of BP. Suppose we have k parallel pro-  cessors, k > 1. We consider a partition of the set {1,...,N} into the subsets  Jr, I: 1,..., k, so that each example is assigned to at least one processor. Let  Nt be the cardinality of the corresponding set Jr. In the parallel BP each processor  performs one (or more) cycles of serial BP on its set of training examples. Then a  synchronization step is performed that consists of averaging the iterates computed  by all the k processors. From the mathematical point of view this is equivalent to  each processor I  {1,...,k} handling the partial error function ft(x) associated  with the corresponding set of training examples Jr. In this setting we have  k  jEJ I=1  We note that in training a neural network it might be advantageous to assign  some training examples to more than one parallel processor. We thus allow for the  possibility of overlapping sets Jr.  The notation for Algorithm 2.2 is similar to that for Algorithm 2.1, except for the  index l that is used to label the partial error function and minor iterates associated  with the /-th parallel processor. We now state the parallel BP with a momentum  term.  Algorithm 2.2. Parallel Online BP with a Momentum Term.  Start with any x   3 n. Having x i, stop if x i+1 = x i, else compute x i+1 as follows  (i) Parallelization. For each parallel processor I  {1,..., k} do  z[,J+ 1 .-- z'J  where  (7)  (8)  /Xz,J = { 0i.  Zl,$ -- z,J --1  /.fj = 1  otherwise (9)  Backpropagation Convergence via Deterministic Nonmonotone Perturbed Minimization 387  (ii) Synchronization  0 < r/ < 1, O_<oq<l  --  y Z[ 'N'+I (10)  I=1  We give below in Table I a flowchart of this algorithm.  Major iteration i  z i  ,(v)  fl(x)---- /j(j fj(x)  ( v, )  '" fl(x) '-/--.,jEJ, fj(x) ...  w( vk )  = zjEJk  Serial BP on  examples in dl  Serial BP on  examples in Jl  Serial BP on  examples in Je  Major iteration i + 1  x i+1 -  Table 1.  k EI=i Z; 'Nl+l  Flowchart of the Parallel BP  Remark 2.4. It is well known that ordinary backpropagation is a relatively slow  algorithm. One appealing remedy is parallelization (Zhang,Mckenna,Mesirov &  Waltz, 990). The proposed Algorithm 2.2 is a possible step in that direction.  Note that in Algorithm 2.2 all processors typically use the same program for their  computations. Thus load balancing is easily achieved.  Remark 2.5. We wish to point out that synchronization strategies other than  (10) are possible. For example, one may choose among the k sets of weights and  thresholds the one that best classifies the training data.  To the best of our knowledge there are no published deterministic convergence  388 Mangasarian and Solodov  proofs for either of Algorithms 2.1,2.2. Using new convergence analysis for a class of  nonmonotone optimization methods with perturbations (Mangasarian & Solodov,  1994), we are able to derive deterministic convergence properties for online BP  and its modifications. Once again we emphasize the equivalence of either of those  methods to a deterministic nonmonotone perturbed gradient-type algorithm.  We now state our main convergence theorem. An important result used in the proof  is given in the Mathematical Appendix. We refer interested readers to (Mangasarian  & Solodov, 1994) for more details.  Theorem 2.1. If the learning and momentum rates are chosen such that  i=0 i=0  E airli < oo, (11)  i=0  then for any sequence {x i} generated by any of the Algorithms 2.1 or 2.2, it follows  that {f(xi)}. converges, {Vf(xi)} - O, and for each accumulation point ' of the  sequence {z}, Vf() = 0.  Remark 2.6. We note that conditions (11) imply that both the learning and  momentum rates asymptotically tend to zero. These conditions are similar to those  used in (White, 1989; Luo & Tseng, 1994) and seem to be the inevitable price paid  for rigorous convergence. For practical purposes the learning rate can be fixed or  adjusted to some small but finite number to obtain an approximate solution to the  minimization problem. For state-of-the-art techniques of computing the learning  rate see (le Cun, Simard & Pearlmutter, 1993).  Remark 2.7. We wish to point out that Theorem 2.1 covers BP with momentum  and/or decay terms for which there is no published convergence analysis of any  kind.  Remark 2.8. We note that the approach of perturbed minimization provides  theoretical justification to the well known properties of robustness and recovery  from damage for neural networks (Sejnowski & Rosenberg, 1987). In particular, this  approach shows that the net should recover from any reasonably small perturbation.  Remark 2.9. Establishing convergence to a stationary point seems to be the  best one can do for a first-order minimization method without any additional re-  strictive assumptions on the objective function. There have been some attempts  to achieve global descent in training, see for example, (Cetin,Burdick & Barhen,  1993). However, convergence to global minima was not proven rigorously in the  multidimensional case.  3  MATHEMATICAL APPENDIX: CONVERGENCE OF  ALGORITHMS WITH PERTURBATIONS  In this section we state a new result that enables us to establish convergence prop-  erties of BP. The full proof is nontrivial and is given in (Mangasarian & Solodov,  1994).  Backpropagation Convergence via Deterministic Nonmonotone Perturbed Minimization 389  Theorem 3.1. General Nonmonotonic Perturbed Gradient Convergence  (subsumes BP convergence).  Suppose that f(x) is bounded below and that V f(x) is bounded and Lipschitz contin-  uous on the sequence {z i} defined below. Consider the following perturbed gradient  algorithm. Start with any x   n. Having x i, stop if 7 f(x i) -- O, else compute  xi+l _ X i - ?It di  (12)  where  for some e i  n,  = --7f(x i) + e i  and such that for some 3'  0  (13)  i=0 i--0  v, 11e'11 Ile'11 3' vi (14)  i--0  It follows that (f(xl)} converges, (Vf(xi)} - O, and for each accumulation point   ofthe sequence (z'), Vf(')= O. If, in addition, the number of stationary points  of fix) is finite, then the sequence (z i) converges to a stationary point of f(z).  Remark 3.1. The error function of BP is nonnegative, and thus the boundedness  condition on f(x) is satisfied automatically. There are a number of ways to ensure  that f(x) has Lipschitz continuous and bounded gradient on (xi). In (Luo &; Tseng,  1994) a simple projection onto a box is introduced which ensures that the iterates  remain in the box. In (Grippo, 1994) a regularization term as in (6) is added to the  error function so that the modified objective function has bounded level sets. We  note that the latter provides a mathematical justification for weight decay (Hinton,  1986; Weigend,Huberman &; Rumelhart, 1990). In either case the iterates remain  in some compact set, and since f(x) is an infinitely smooth function, its gradient is  bounded and Lipschitz continuous on this set as desired.  Acknowledgement s  This material is based on research supported by Air Force Office of Scientific  Research Grant F49620-94-1-0036 and National Science Foundation Grant CCR-  9101801.  References  M. Arai. (1993) Bounds on the number of hidden units in binary-valued three-layer  neural networks. Neural Networks, 6:855-860.  B.C. Cetin, J. W. Burdick, and J. Barhen. (1993) Global descent replaces gradient  descent to avoid local minima problem in learning with artificial neural networks.  In IEEE International Conference on Neural Networks, (San Francisco), volume 2,  836-842.  P. Courrien.(1993) Convergent generator of neural networks. Neural Networks,  6:835-844.  Yu. rrmoliev and R.J.-B. Wets (editors). (1988) Numerical Techniques for Stochas-  tic Optimization Problems. Springer-Verlag, Berlin.  390 Mangasarian and $olodov  A.A. Gaivoronski. (1994) Convergence properties of backpropagation for neural  networks via theory of stochastic gradient methods. Part 1. Optimization Methods  and Software, 1994, to appear.  L. Grippo. (1994) A class of unconstrained minimization methods for neural net-  work training. Optimization Methods and Software, 1994, to appear.  G. E. Hinton. (1986) Learning distributed representations of concepts. In Pro-  ceedings of the Eighth Annual Conference of the Cognitive Science Society, 1-12,  Hillsdale. Erlbaum.  R. L. Kashyap, C. C. Blaydon and K. S. Fu. (1970) Applications of stochastic  approximation methods. In J.M.Mendel and K.S. Fu, editors, Adaptive, Learning,  and Pattern Recognition Systems. Academic Press.  T. Khanna. (1989) Foundations of neural networks. Addison-Wesley, New Jersey.  Y. le Cun, P.Y. Simard, and B. Pearlmutter. (1993) Automatic learning rate  maximization by on-line estimation of the Hessian's eigenvectors. In C.L.Giles  S.J.Hanson, J.D.Cowan, editor, Advances in Neural Information Processing Sys-  tems 5, 156-163, San Mateo, California, Morgan Kaufmann.  Z.-Q. Luo and P. Tseng. (1994) Analysis of an approximate gradient projection  method with applications to the backpropagation algorithm. Optimization Methods  and Software, 1994, to appear.  O.L. Mangasarian. (1993) Mathematical programming in neural networks. ORSA  Journal on Computing, 5(4), 349-360.  O.L. Mangasarian and M.V. Solodov. (1994) Serial and parallel backpropagation  convergence via nonmonotone perturbed minimization. Optimization Methods and  Software, 1994, to appear. Proceedings of Symposium on Parallel Optimization 3,  Madison July 7-9, 1993.  M.F. Moller. (1992) Supervised learning on large redundant training sets. In Neural  Networks for Signal Processing . IEEE Press.  B.T. Polyak. (1987) Introduction to Optimization. Optimization Software, Inc.,  Publications Division, New York.  D.E. Rumelhart, G.E. Hinton, and R.J. Williams. (1986) Learning internal repre-  sentations by error propagation. In D.E. Rumelhart and J.L. McClelland, editors,  Parallel Distributed Processing, 318-362, Cambridge, Massachusetts. MIT Press.  T.J. Sejnowski and C.R. Rosenberg. (1987) Parallel networks that learn to pro-  nounce english text. Complex Systems, 1:145-168.  A.S. Weigend, B.A. Huberman, and D.E. Rumelhart. (1990) Predicting the future:a  connectionist approach. International Journal of Neural Systems, 1:193-209.  H. White. (1989) Some asymptotic results for learning in single hidden-layer  feedforward network models. Journal of the American Statistical Association,  84(408):1003-1013.  X. Zhang, M. Mckenna, J. P. Mesirov, and D. L. Waltz. (1990) The backpropagation  algorithm on grid and hypercube architectures. Parallel Computing, 14:317-327.  
The Parti-game Algorithm for Variable  Resolution Reinforcement Learning in  Multidimensional State-spaces  Andrew W. Moore  School of Computer Science  Carnegie-Mellon University  Pittsburgh, PA 15213  Abstract  Parti-game is a new algorithm for learning from delayed rewards  in high dimensional real-valued state-spaces. In high dimensions  it is essential that learning does not explore or plan over state  space uniformly. Parti-game maintains a decision-tree partitioning  of state-space and applies game-theory and computational geom-  etry techniques to efficiently and reactively concentrate high reso-  lution only on critical areas. Many simulated problems have been  tested, ranging from 2-dimensional to 9-dimensional state-spaces,  including mazes, path planning, non-linear dynamics, and uncurl-  ing snake robots in restricted spaces. In all cases, a good solution  is found in less than twenty trials and a few minutes.  1 REINFORCEMENT LEARNING  Reinforcement learning [Samuel, 1959, Sutton, 1984, Watkins, 1989, Barto et al.,  1991] is a promising method for control systems to program and improve themselves.  This paper addresses its biggest stumbling block: the curse of dimensionality [Bell-  man, 1957], in which costs increase exponentially with the number of state variables.  Some earlier work [Simons et al., 1982, Moore, 1991, Chapman and Kaelbling, 1991,  Dayan and Hinton, 1993] has considered recursively partitioning state-space while  learning from delayed rewards. The new ideas in the parti-game algorithm in-  711  712 Moore  clude (i) a game-theoretic splitting criterion to robustly choose spatial resolution  (ii) real-time incremental maintenance and planning with a database of all previ-  ous experiences, and (iii) using local greedy controllers for high-level "funneling"  actions.  2 ASSUMPTIONS  The patti-game algorithm applies to difficult learning control problems in which:  1. State and action spaces are continuous and multidimensional.  2. "Greedy" and hill-climbing techniques would become stuck, never attaining  the goal.  3. Random exploration would be hopelessly time-consuming.  4. The system dynamics and control laws can have discontinuities and are  unknown: they must be learned.  The experiments reported later all have properties 1-4. However, the initial algo-  rithm, described and tested here, has the following restrictions:  5. Dynamics are deterministic.  6. The task is specified by a goal, not an arbitrary reward function.  7. The goal state is known.  8. A "good" solution is required, not necessarily the optimal path. This no-  tion of goodness can be formalized as "the optimal path to within a given  resolution of state space".  9. A local greedy controller is available, which we can ask to move greedily  towards any desired state. There is no guarantee that a request to the  greedy controller will succeed. For example, in a maze a greedy path to the  goal would quickly hit a wall.  Future developments may include relatively straightforward additions to the algo-  rithm that would remove the need for restrictions 6-9. Restriction 5 is harder to  remove.  3 ESSENTIALS OF THE PARTI-GAME ALGORITHM  The state space is broken into partitions by a kd-tree [Friedman et al., 1977]. The  controller can always sense its current (continuous valued) state, and can cheaply  compute which partition it is in. The space of actions is also discretized so that  in a partition with N neighboring partitions, there are N high-level actions. Each  high level action corresponds to a local greedy controller, aiming for the center of  the corresponding neighboring partition.  Each partition keeps records of all the occasions on which the system state has  passed through it. Along with each record is a memory of which high level action  was used (i.e. which neighbor was aimed for) and what the outcome was. Figure 1  provides an illustration.  Given this database of (partition, high-level-action, outcome) triplets, and our  knowledge of the partition containing the goal state, we can try to compute the  The Parti-Game Algorithm for Variable Resolution Reinforcement Learning 713  Partition I Partition 2  BARRIER   Partition 3  Figure 1: Three trajectories starting  in partition 1, using high-level action  "Aim at partition 2". Partition 1 re-  members three outcomes.  (Part 1, Aim 2  Part 2)  (Part 1, Aim 2 -- Part 1)  (Part 1, Aim 2 --+ Part 3)  best route to the goal. The standard approach would be to model the system  as a Markov Decision Task in which we empirically estimate the partition tran-  sition probabilities. However, the probabilistic interpretation of coarse resolution  partitions can lead to policies which get stuck. Instead, we use a game-theoretic  approach, in which we imagine an adversary. This adversary sees our choice of  high-level action, and is allowed to select any of the observed previous outcomes  of the action in this partition. Partitions are scored by minimaxing: the adversary  plays to delay or prevent us getting to the goal and we play to get to the goal as  quickly as possible.  Whenever the system's continuous state passes between partitions, the database of  state transitions is updated and, if necessary, the minimax scores of all partitions  are updated. If real-time constraints do not permit full recomputation, the updates  take place incrementally in a manner similar to prioritized sweeping [Moore and  Atkeson, 1993].  As well as being robust to coarseness, the game-theoretic approach also tells us  where we should increase the resolution. Whenever we compute that we are in a  losing partition we perform resolution increase. We first compute the complete set  of connected partitions which are also losing partitions. We then find the subset of  these partitions which border some non-losing region. We increase the resolution of  all these border states by splitting them along their longest axes .  4 INITIAL EXPERIMENTS  Figure 2 shows a 2-d continuous maze. Figure 3 shows the performance of the robot  during the very first trial. It begins with intense exploration to find a route out of  the almost entirely enclosed start region. Having eventually reached a sufficiently  high resolution, it discovers the gap and proceeds greedily towards the goal, only  to be stopped by the goal's barrier region. The next barrier is traversed at a much  lower resolution, mainly because the gap is larger.  Figure 4 shows the second trial, started from a slightly different position. The  policy derived from the first trial gets us to the goal without further exploration.  The trajectory has unnecessary bends. This is because the controller is discretized  according to the current partitioning. If necessary, a local optimizer could be used  More intelligent splitting criteria are under investigation.  714 Moore  Start I'   Figure 2: A 2-d maze problem. The point  robot must find a path from start to goal  without crossing any of the barrier lines. Re-  member that initially it does not know where  any obstacles are, and must discover them by  finding impassable states.  Figure 3: The path taken during the entire  first trial. See text for explanation.  to refine this trajectory 2.  The system does not explore unnecessary areas. The barrier in the top left remains  at low resolution because the system has had no need to visit there. Figures 5 and 6  show what happens when we now start the system inside this barrier.  Figure 7 shows a 3-d state space problem. If a standard grid were used, this would  need an enormous number of states because the solution requires detailed three-  point-turns. Parti-game's total exploration took 18 times as much movement as  one run of the final path obtained.  Figure 8 shows a 4-d problem in which a ball rolls around a tray with steep edges.  The goal is on the other side of a ridge. The maximum permissible force is low,  and so greedy strategies, or globally linear control rules, get stuck in a limit cycle.  Parti-game's solution runs to the other end of the tray, to build up enough velocity  to make it over the ridge. The exploration-length versus final-path-length ratio is  24.  Figure 9 shows a 9-joint snake-like robot manipulator which must move to a specified  configuration on the other side of a barrier. Again, no initial model is given: the  controller must learn it as it explores. It takes seven trials before fixing on the  solution shown. The exploration-length versus final-path-length ratio is 60.  2Another method is to increase the resolution along the trajectory [Moore, 1991].  The Parti-Game Algorithm for Variable Resolution Reinforcement Learning 715  Figure 4: The second trial.  Figure 5: Starting inside the  top left barrier.  Figure 6: The trial after  that.  Figure 7: A problem with a planar rod being guided past obstacles. The state space  is three-dimensional: two values specify the position of the rod's center, and the third  specifies the rod's angle from the horizontal. The angle is constrained so that the pole's  dotted end must always be below the other end. The pole's center may be moved a short  distance (up to 1/40 of the diagram width) and its angle may be altered by up to 5 degrees,  provided it does not hit a barrier in the process. Patti-game converged to the path shown  below after two trials. The partitioning lines on the solution diagram only show a 2-d slice  of the full kd-tree.  I  i  Trials 1 2 3 4 IS 16 I 7 I 8 19 110  Steps 2975 189 187 no further  Partitions 149 149 149 change  716 Moore  Figure 8: A puck sliding over a hilly surface (hills shown by contours below: the surface  is bowl shaped, with the lowest points nearest the center, rising steeply at the edges).  The state space is four-dimensional: two position and two velocity variables. The controls  consist of a force which may be apphed in any direction, but with bounded magnitude.  Convergence time was two trials.  Go 1   Trials 1 2 3 14 Is 16 I* I 8 I 9 I 0  Steps 2609 115 no further  Partitions 13 13 change  Figure 9: A nine-degree-of-freedom planar robot must move from the shown start con-  figuration to the goal. The solution entails curling, rotating and then uncutling. It may  not intersect with any of the barriers, the edge of the workspace, or itself. Convergence  occurred after seven trials.  ,   (Fixed  base  I  Trials 1 -  4     I I 0  Steps 1090 430 353 330 739 200 52 no further  Partitions 41 66 67 69 78 85 85 change  The Parti-Game Algorithm for Variable Resolution Reinforcement Learning 717  5 DISCUSSION  Possible extensions include:   Splitting criteria that lay down splits between trajectories with spatially  distinct outcomes.   Allowing humans to provide hints by permitting user-specified controllers  ("behaviors") as extra high-level actions.   Coalescing neighboring partitions that mutually agree.  We finish by noting a promising sign involving a series of snake robot experiments  with different numbers of links (but fixed total length). Intuitively, the problem  should get easier with more links, but the curse of dimensionality would mean  that (in the absence of prior knowledge) it becomes exponentially harder. This is  borne out by the observation that random exploration with the three-link arm will  stumble on the goal eventually, whereas the nine link robot cannot be expected to  do so in tractable time. However, Figure 10 indicates that as the dimensionality  rises, the amount of exploration (and hence computation) used by parti-game does  not rise exponentially. Real-world tasks may often have the same property as the  snake example: the complexity of the ultimate task remains roughly constant as the  number of degrees of freedom increases. If so, we may have uncovered the Achilles'  heel of the curse of dimensionality.  180  160  140  120  100  4 5 6 7  Dimensionality  8 9  Figure 10: The number of par-  titions finally created against de-  grees of freedom for a set of snake-  like robots. The kd-trees built were  all highly non-uniform, typically  having maximum depth nodes of  twice the dimensionality. The rela-  tion between exploration time and  dimensionality (not shown) had a  similar shape.  References  [Barto t al., 1991] A. G. Barto, S. J. Bradtke, and S. P. Singh. Real-time Learning and  Control using Asynchronous Dynamic Programming. Technical Report 91-57, University  of Massachusetts at Amherst, August 1991.  [Bellman, 1957] R. E. Bellman. Dynamic Programming. Princeton University Press,  Princeton, N J, 1957.  [Chapman and Kaelbling, 1991] D. Chapman and L. P. Kaelbling. Learning from Delayed  Reinforcement In a Complex Domain. Technical Report, Teleos Research, 1991.  718 Moore  [Dayan and Hinton, 1993] P. Dayan and G. E. Hinton. Feudal Reinforcement Learning.  In S. J. Hanson, J. D Cowan, and C. L. Giles, editors, Advances in Neural Information  Processing Systems 5. Morgan Kaufmann, 1993.  [Friedman et al., 1977] J. H. Friedman, J. L. Bentley, and R. A. Finkel. An Algorithm for  Finding Best Matches in Logarithmic Expected Time. ACM Trans. on Mathematical  Software, 3(3):209-226, September 1977.  [Moore and Atkeson, 1993] A. W. Moore and C. G. Atkeson. Prioritized Sweeping: Rein-  forcement Learning with Less Data and Less Real Time. Machine Learning, 13, 1993.  [Moore, 1991] A. W. Moore. Variable Resolution Dynamic Programming: Efficiently  Learning Action Maps in Multivariate Real-valued State-spaces. In L. Birnbaum and  G. Collins, editors, Machine Learning: Proceedings of the Eighth International Work-  shop. Morgan Kaufman, June 1991.  [Samuel, 1959] A. L. Samuel. Some Studies in Machine Learning using the Game of Check-  ers. IBM Journal on Research and Development, 3, 1959. Reprinted in E. A. Feigenbaum  and J. Feldman, editors, Computers and Thought, McGraw-Hill, 1963.  [Simons et al., 1982] J. Simons, H. Van Brussel, J. De Schutter, and J. Verhaert. A Self-  Learning Automaton with Variable Resolution for High Precision Assembly by Industrial  Robots. IEEE Trans. on Automatic Control, 27(5):1109-1113, October 1982.  [Singh, 1993] S. Singh. Personal Communication. , 1993.  [Sutton, 1984] R. S. Sutton. Temporal Credit Assignment in Reinforcement Learning.  Phd. thesis, University of Massachusetts, Amherst, 1984.  [Watkins, 1989] C. J. C. H. Watkins. Learning from Delayed Rewards. PhD. Thesis,  King's College, University of Cambridge, May 1989.  
On the Non-Existence of a Universal Learning  Algorithm for Recurrent Neural Networks  Herbert Wiklicky  Centrum voor Wiskunde en Informatica  P.O.Box 4079, NL-1009 AB Ansterdam, The Netherlands'  e-mail: herbertcwi.nl  Abstract  We prove that the so called "loading problem" for (recurrent) neural net-  works is unsolvable. This extends several results which already demon-  strated that training and related design problems for neural networks are  (at least) NP-complete. Our result also implies that it is impossible to  find or to formulate a universal training algorithm, which for any neu-  ral network architecture could determine a correct set of weights. For  the simple proof of this, we will just show that the loading problem is  equivalent to "Hilbert's tenth problem" which is known to be unsolvable.  I THE NEURAL NETWORK MODEL  It seems that there are relatively few commonly accepted general formal definitions of the  notion of a "neural network". Although our results also hold if based on other formal  definitions we will try to stay here very close to the original setting in which Judd's NP  completeness result was given [Judd, 1990]. But in contrast to [Judd, 1990] we will deal  here with simple recurrent networks instead of feed forward architectures.  Our networks are constructed from three different types of units: ,U-units compute just  the sum of all incoming signals; for//-units the activation (node) function is given by the  product of the incoming signals; and with 60-units - depending if the input signal is smaller  or larger than a certain threshold parameter 0 - the output is zero or one. Our units ,are  connected or linked by real weighted connections and operate synchronously.  Note that we could base our construction also just on one general type of units, namely  what usually is called ,U//-units. Furthermore, one could replace the H-units in the below  431  432 Wiklicky  construction by (recurrenO modules of simple linear threshold units which had to perform  unary integer multiplication. Thus, no higher order elements are actually needed.  As we deal with recurrent networks, the behavior of a network now is not just given by a  simple mapping from input space to output space (as with feed forward architectures). In  general, an input pattern now is mapped to an (infinite) output sequence. But note, that if  we consider as the output of a recurrent network a certain final, stable output pauern, we  could return to a more static setting.  2 THE MAIN RESULT  The question we will look at is how difficult it is to construct or train a neural network of  the described type so that it actually exhibits a certain desired behavior, i.e. solves a given  leaming task. We will investigate this by the following decision problem:  Decision 1 Loading Problem  INSTANCE: A neural network architecture N and a learning task T.  QUESTION: Is there a configuration C for N such that T is realized by C?  By a network configuration we just think of a certain setting of the weights in a neural  network. Our main result concerning this problem now just states that it is undecidable or  unsolvable.  Theorem 1 There exists no algorithm which could decide for any learning task T and any  (recurrent) neural network (consisting of,E-, FI-, and O-units) if the given architecture can  perform T.  The decision problem (as usual) gives a "lower bound" on the hardness of the related con-  structive problem [Garey and Johnson, 1979]. If we could construct a correct configuration  for all instances, it would be trivial to decide instantly if a correct configuration exists at  all. Thus we have:  Corollary 2 There exists no universal learning algorithm for (recurrent) neural networks.  3 THE PROOF  The proof of the above theorem is by constructing a class of neural networks for which it  is impossible to decide (for all instance) if a certain leaming task can be satisfied. We will  refer for this to "Hilbert's tenth problem" and show that for each of its instances we can  construct a neural network, so that solutions to the loading problem would lead to solutions  to the original problem (and vice versa). But as we know that Hilbert's tenth problem is  unsolvable we also have to conclude that the loading problem we consider is unsolvable.  3.1 HILBERT'S TENTH PROBLEM  Our reference problem - of which we know it is unsolvable - is closely related to several  famous and classical mathematical problems including for example Fennat's last theorem.  On the Non-Existence of a Universal Learning Algorithm for Recurrent Neural Networks 433  Definition 1 A diophantine equation is a polynomial D in .7 variables with integer coeffi-  cients, that is  i  with each term cli of the form di(.rl, .r2 .... , .7:,): el..7:i,  .7:; 2 ..... .r i,,,, where the indices  {il, i2,..., ira} are taken from {1,2,..., ,} and the coefficient c;  Z.  The concrete problem, first formulated in [Hilbert, 1900] is to develop a universal algorithm  how to find the integer solutions for all D, i.e. a vector (.r, .7:2,..., .r,,) with .ri  ; (or  lq), such that D (.7:, .r2, .... .7:,) = 0. The corresponding decision problem therefore is the  following:  Decision 2 Hilbert's Tenth Problem  INSTANCE: Given a diophantine equation D.  QUESTION: Is there an integer solution for D ?  Although this problem might seem to be quite simple- it formulation is actually the shortest  among D. Hilbert's famous 23 problems- it was not until 1970 when Y. Matijasevich could  prove that it is unsolvable or undecidable [Matijasevich, 1970]. There is no recursive  computable predicate for diophantine equations which holds if a solution in ; (or I,,l) exists  and fails otherwise [Davis, 1973, Theorem 7.4].  3.2 THE NETWORK ARCHITECTURE  The construction of a neural network N for each diophantine D is now straight forward  (see Fig.l). It is just a three step construction.  First, each variable .ri of D is represented in N by a small sub-network. The structure  of these modules is quite simple (left side of Fig.l). Note that only the self-recurrent  connection for the unit at the bottom of these modules is "weighted" by 0.0 < w < 1.0.  All other connection transmit their signals unaltered (i.e. w = 1.0).  Second, the terms d in D are represent by H-units in N (as show in Fig.l). Therefore,  the connections to these units from the sub-modules representing the variables .ri of D  correspond to the occurrences of these variables in each term d.  Finally, the output signals of all these I-I-units is multiplied by the corresponding coefficients  ci and summed up by the E-unit at the top.  3.3 THE SUB-MODULES  The fundamental property of the networks constructed in the above way is given by the  simple fact that the behavior of such a neural network N corresponds uniquely to the  evaluation of the original diophantine D.  First, note that the behavior of N only depends on the weights wi in each of the variable  modules. Therefore, we will take a closer look at the behavior of these sub-modules.  Suppose, that at some initial moment a signal of value 1.0 is received by each variable  module. After that the signal is reset again to 0.0.  434 Wiklicky  Figure 1: The network for el.fl.7:2 + c23722373 q- C33;33:4  The "seed" signal starts circling via wi. With each update circle this signal becomes a little  bit smaller. On the other hand, the same signal is also sent to the central O-unit, which  sends a signal 1.0 to the top accumulator unit as long as the "circling" activation of the  bottom unit is larger then the (preset) threshold 0i. The top unit (which also keeps track of  its former activiations via a recurrent connection) therefore just counts how many updates  it takes before the activiation of the bouom unit drops below 0i.  The final, maximum, value which is emitted by the accumulator unit is some integer .: for  which we have:  I I  We thus have a correspondence between wi and the integer .:i = [h ,,,], where [.rJ the  largest integer which is smaller or equal to .7:. Given .r; we also can construct an appropriate  weight wl by choosing it from the interval exp  ? exp \ --r//.  3.4 THE EQUIVALENCE  To conclude the proof, we now have to demonstrate the equivalence of Hilbert's tenth  problem and the loading problem for the discussed class of recurrent networks and some  learning task.  The learning task we will consider is the following: Map an input pattern with all signals  equal to 1.0 (presented only once) to an output sequence which after afinite number of steps  On the Non-Existence of a Universal Learning Algorithm for Recurrent Neural Networks 435  is constant equal to 0.0. Note that - as discussed above - we could also consider a more  static learing task where a final state, which determines the (single) output of the network,  was determined by the condition that the outgoing signals of all O-units had to be zero.  Considering this learing task and with what we said about the behavior of the sub-modules it  is now trivial to see that the constructed network just evaluates the diophantine polynomial  for a set of variables .:i corresponding to the (final) output signals of the sub-modules  (which are determined uniquely by the weight values wi) if the input to the network is a  pattern of all 1.0s.  If we had a solution .:; of the original diophantine equation D, and if we take the corre-  sponding values 'u,,'i (according to the above relation) as weights in the sub-modules of N,  then this would also solve the loading problem for this architecture. On the other hand, if  we knew the correct weights w; for any such network N, then the corresponding integers   :i would also solve the corresponding diophantine equation D.  In particular, if it would be possible to decide if a correct set of weights w; for N exists  (for the above learning task), we could also decide if the corresponding diophantine D had  a solution .:i  lq (and vice versa). As the whole construction was trivial, we have shown  that both problems are equivalent.  4 CONCLUSIONS  We demonstrated that the loading problem not only is NP-complete - as shown for simple  feed forward architectures in [Judd, 1990], [Lin and Vitter, 1991], [Blum and Rivest, 1992],  etc. - but actually unsolvable, i.e. that the training of (recurrent) neural networks is among  those problems which "indeed are intractable in an especially strong sense" [Garey and  Johnson, 1979, p 12]. A related non-existence result conceming the training of higher order  neural networks with integer weights was shown in [Wiklicky, 1992, Wildicky, 1994].  One should stress once again that the fact that no general algorithm exists for higher order  or recurrent networks, which could solve the loading problem (for all its instances), does  not imply that all instances of this problem are unsolvable or that no solutions exist. One  could hope, that in most relevant cases - whatever that could mean - or, when we restrict  the problem, a sub-class of problems things might become tractable. But the difference  between solvable and unsolvable problems often can be very small.  In particular, it is known that the problem of solving linear diophantine equations (instead of  general ones) is polynomially computable, while if we go to quadratic diophantine equations  the problem already becomes NP complete [Johnson, 1990]. And for general diophantine  the problem is even unsolvable. Moreover, it is also known that this problem is unsolvable if  we consider only diophantine equations of maximum degree 4, and there exists a universal  diophantine with only 13 variables which is unsolvable [Davis et al., 1976].  But we think, that one should interpret the "negative" results on NP-complexity as well  as on undecidability of the loading problem not as restrictions for neural networks, but as  related to their computational power. As it was shown that concrete neural networks can  be constructed, so that they simulate a universal Turing machine [Siegelmann and Sontag,  1992, Cosnard et al., 1993]. It is mere the complexity of the problem one attempts to solve  which simply cannot disappear and not some intrinsic intractability of the neural network  approach.  436 Wiklicky  Acknowledgement  This work was started during the author's affiliation with the "Austrian Research Institute  for Artificial Intelligence", Schottengasse 3, A-1010 Wien, Austria. Further work was  supported by a grant from the Austrian "Fonds zur FOrderung der wissenschaftlichen  Forschung" as Projekt J0828-PHY.  References  [Blum and Rivest, 1992] Avrim L. Blum and Ronald L. Rivest. Training a 3-node neural  network is NP-complete. Neural Networks, 5:117-127, 1992.  [Cosnard et al., 1993] Michael Cosnard, Max Garzon, and Pascal Koimn. Computability  properties of low-dimensional dynamical systems. In Symposium on Theoretical  Aspects of Computer Science (STACS '93), pages 365-373, Springer-Verlag, Berlin -  New York, 1993.  [Davis, 1973] Martin Davis. Hilbert's tenth problem is unsolvable. Amer. Math. Monthly,  80:233-269, March 1973.  [Davis et al., 1976] Martin Davis, Yuri Matijasevich, and Julia Robinson. Hilbert's tenth  problem - diophantine equations: Positive aspects of a negative solution. In Felix E.  Browder, editor, Mathematical developments arising from Hilbert, pages 323-378,  ganerican Mathematical Society, 1976.  [Garey and Johnson, 1979] Michael R. Garey and David S. Johnson. Computers and In-  tractability - A Guide to the Theory of NP-Completeness. W. H. Freeman, New York,  1979.  [Hilbert, 1900] David Hilbert. Mathematische Probleme. Nachr. Ges. Wiss. G6ttingen,  math.-phys.Kl., :253-297, 1900.  [Johnson, 1990] David S. Johnson. A catalog of complexity classes. In Handbook of  Theoretical Computer Science (Volume A: Algorithms and Complexity), chapter 2,  pages 67-161, Elsevier - MIT Press, Amsterdam - Cambridge, Massachusetts, 1990.  [Judd, 1990] J. Stephen Judd. Neural Network Design and the Complexity of Learning.  MIT Press, Cambridge, Massachusetts- London, England, 1990.  [Lin and Vitter, 1991] Jyh-Han Lin and Jeffrey Scott Vitter. Complexity results on learning  by neural networks. Machine Learning, 6:211-230, 1991.  [Matijasevich, 1970] Yuri Matijasevich. Enumerable sets are diophantine. Dokl. Acad.  Nauk., 191:279-282, 1970.  [Siegelmann and Sontag, 1992] Hava T. Siegelmann and Eduardo D. Sontag. On the com-  putational power of neural nets. In Fifth Workshop on Computational Learning Theory  (COLT' 92), pages 440-449, 1992.  [Wiklicky, 1992] Herbert Wiklicky. Synthesis and Analysis of Neural Networks -- On  a Framework for Artificial Neural Networks. PhD thesis, University of Vienna -  Technical University of Vienna, September 1992.  [W'fidicky, 1994] Herbert Wiklicky. The neural network loading problem is undecidable.  In Euro-COLT '93 - Conference on Computational Learning Theory, page (to appear),  Oxford University Press, Oxford, 1994.  PART III  THEORETICAL  A.NALYSIS: DYNAMICS  AND STATISTICS  
A Massively-Parallel $IMD Processor for  Neural Network and Machine Vision  Applications  Michael A. Glover  Current Technology, Inc.  99 Madbury Road  Durham, NH 03824  W. Thomas Miller, III  Department of Electrical and Computer Engineering  The University of New Hampshire  Durham, NH 03824  Abstract  This paper describes the MM32k, a massively-parallel SIMD com-  puter which is easy to program, high in performance, low in cost  and effective for implementing highly parallel neural network ar-  chitectures. The MM32k has 32768 bit serial processing elements,  each of which has 512 bits of memory, and all of which are inter-  connected by a switching network. The entire system resides on  a single PC-AT compatible card. It is programmed from the host  computer using a C++ language class library which abstracts the  parallel processor in terms of fast arithmetic operators for vectors  of variable precision integers.  I INTRODUCTION  Many well known neural network techniques for adaptive pattern classification and  function approximation are inherently highly parallel, and thus have proven dif-  ficult to implement for real-time applications at a reasonable cost. This includes  843  844 Glover and Miller  a variety of learning systems such as radial basis function networks [Moody 1989],  Kohonen self-organizing networks [Kohonen 1982], ART family networks [Carpenter  1988], and nearest-neighbor interpolators [Duda 1973], among others. This paper  describes the MM32k, a massively-parallel SIMD computer which is easy to pro-  gram, high in performance, low in cost and effective for implementing highly parallel  neural network architectures. The MM32k acts as a coprocessor to accelerate vector  arithmetic operations on PC-AT class computers, and can achieve giga-operation  per second performance on suitable problems. It is programmed from the host  computer using a C++ language class library, which overloads typical arithmetic  operators, and supports variable precision arithmetic. The MM32k has 32768 bit  serial PEs, or processing elements, each of which has 512 bits of memory, and all  of which are interconnected by a switching network. The PEs are combined with  their memory on an single DRAM memory chip giving 2048 processors per chip.  The entire 32768 processor system resides on a single ISA bus compatible card. It  is much more cost effective than other SIMD processors [Hammerstrom 1990; Hillis  1985; Nickolls 1990; Potter 1985] and more flexible than fixed purpose chips [Holler  1991].  2 SIMD ARCHITECTURE  The SIMD PE array contains 32768 one bit processors, each with 512 bits of memory  and a connection to the interconnection network. The PE array design is unique  in that 2048 PEs, including their PE memory, are realized on a single chip. The  total PE array memory is 2 megabytes and has a peak memory bandwidth is 25  gigabytes per second. The PE array can add 8 bit integers at 2.5 gigaoperations  per second. It also dissipates less than 10 watts of power and is shown in Figure 1.  Each PE has three one bit registers, a 512 bit memory, and a one bit ALU. It  performs bit serial arithmetic and can therefore vary the number of bits of precision  to fit the problem at hand, saving SIMD instruction cycles and SIMD memory.  There are 17 instructions in the PE instruction set, all of which execute at a 6.25  MIPS rate. The PE instruction set is functionally complete in that it can perform  boolean NOT and OR functions and can therefore perform any operation, including  arithmetic and conditional operations. A single PE is shown in Figure 2.  The interconnection network allows data to be sent from one PE to another. It is  implemented by a 64*64 full crossbar switch with 512 PEs connected to each port  of the switch. It allows data to be sent from one PE to another PE, an arbitrary  distance away, in constant time. The peak switch bandwidth is 280 megabytes per  second. The switch also allows the PE array to perform data reduction operations,  such as taking the sum or maximum over data elements distributed across all PEs.  3 C-I-+ PROGRAMMING ENVIRONMENT  The purpose of the C++ programming environment is to allow a programmer to  declare and manipulate vectors on the MM32k as if they were variables in a pro-  gram running on the host computer. Programming is performed entirely on the  host, using standard MS-DOS or Windows compatible C++ compilers. The C++  programming environment for the MM32k is built around a C++ class, named  A Massively-Parallel SIMD Processor for Neural Network and Machine Vision Applications 845  I Host Computer  (PC-AT)  I Vector Instructions and Data  Controller  PE Instructions and Data  Switch  Figure 1: A block diagram of the MM$2k.  9 Bit Address  from Controller  Address Bus  Bit 511  Bit 510  Bit 509  Bit 5  Bit 4  Bit 3  Bit 2  Bit 1  Bit 0  PE ALU Opcode  from Controller  Data Bus '  Data to  Switch  A Register  1 Bit  512 Bit Memory  ALU  I M Register  1 Bit  _Register  Bit   Data from  itch  Figure 2: A block diagram of a single processing elemen[ (PE).  846 Glover and Miller  Table 1:8 Bit Operations With 32768 and 262144 Elements  8 bit Actual MOPS Actual MOPS  operation with length with length  of 32768 of 262144  copy 1796 9429  vector+vector 1455 2074  vector+s calar 1864 3457  vector*vector 206 215  vector*scalar 426 450  vector>scalar 1903 6223  align (vector ,scalar) 186 213  sum(vector) 52 306  maximum(vector) 114 754  MM_VECTOR, which represents a vector of integers. Most of the standard C  arithmetic operators, such as +, -, *, /, =, and > have been overloaded to work  with this class. Some basic functions, such as absolute value, square root, mini-  mum, maximum, align, and sum, have also been overloaded or defined to work with  the class.  The significance of the class MM_VECTOR is that instances of it look and act  like ordinary variables in a C++ program. So a programmer may add, subtract,  assign, and manipulate these vector variables from a program running on the host  computer, but the storage associated with them is in the SIMD memory and the  vector operations are performed in parallel by the SIMD PEs. MM_VECTORs can  be longer than 32768. This is managed (transparent to the host program) by placing  two or more vector elements in the SIMD memory of each PE. The class library  keeps track of the number of words per PE. MM_VECTORs can be represented by  different numbers of bits. The class library automatically keeps track of the number  of bits needed to represent each MM_VECTOR without overflow. For example, if  two 12 bit integers were added together, then 13 bits would be needed to represent  the sum without overflow. The resulting MM_VECTOR would have 13 bits. This  saves SIMD memory space and SIMD PE instruction cycles. The performance of  the MM32k on simple operators running under the class library is listed in Table 1.  4 NEURAL NETWORK EXAMPLES  A common operation found in neural network classifiers (Kohonen, ART, etc.) is  the multi-dimensional nearest-neighbor match. If the network has a large number  of nodes, this operation is particularly inefficient on single processor systems, which  must compute the distance metric for each node sequentially. Using the MM32k, the  distance metrics for all nodes (up to 32768 nodes) can be computed simultaneously,  and the identification of the minimum distance can be made using an efficient tree  compare included in the system microcode.  A Massively-Parallel SIMD Processor for Neural Network and Machine Vision Applications 847  Processor  Table 2: Speedup on Nearest Neighbor Search  Time for Time for MM32k  32768 nodes 65536 nodes speedup for  32768 nodes  MM32k  speedup for  65536 nodes  MM32k 2.2 msec 3.1 msec 1:1 1:1  i486 350 msec 700 msec 159:1 226:1  MIPS 970 msec 1860 msec 441:1 600:1  Alpha 81 msec 177 msec 37:1 57:1  SPARC 410 msec 820 msec 186:1 265:1  Figure 3 shows a C++ code example for performing a 16-dimensional nearest neigh-  bor search over 32768 nodes. The global MM_VECTOR variable state[16] defines  the 16-dimensional location of each node. Each logical element of state[] (state[O],  state[1], etc.) is actually a vector with 32768 elements distributed across all pro-  cessors. The routine find_best_match() computes the euclidean distance between  each node's state and the current test vector test_input[ ], which resides on the host  processor. Note that the equations appear to be scalar in nature, but in fact direct  vector operations to be performed by all processors simultaneously.  The performance of the nearest neighbor search shown in Figure 3 is listed in Table  2. Performance on the same task is also listed for four comparison processors: a  Gateway2000 model 4DX2-66V with 66 MHz 80486 processor (i486), a DECstation  5000 Model 200 with 25 MHz MIPS R3000A processor (MIPS), a DECstation 3000  Model 500AXP with 150 MHz Alpha AXP processor (Alpha), and a Sun SPARC-  station 10 Model 30 with 33 MHz SuperSPARC processor (SPARC). There are 16  subtractions, 16 additions, 16 absolute values, one global minimum, and one global  first operation performed. The MM32k is tested on problems with 32768 and 65536  exemplars and compared against four popular serial machines performing equivalent  searches. The MM32k requires 3.1 milliseconds to search 65536 exemplars which is  265 times faster than a SPARC 10.  The flexibility of the MM32k for neural network applications was demonstrated  by implementing complete fixed-point neural network paradigms on the MM32k  and on the four comparison processors (Table 3). Three different neural network  examples were evaluated. The first was a radial basis function network with 32,768  basis functions (rational function approximations to gaussian functions). Each basis  function had 9 8-bit inputs, 3 16-bit outputs (a vector basis function magnitude),  and independent width parameters for each of the nine inputs. The performances  listed in the table (RBF) are for feedforward response only. The second example  was a Kohonen self-organizing network with a two-dimensional sheet of Kohonen  nodes of dimension 200x150 (30,000 nodes). The problem was to map a nonlinear  robotics forward kinematics transformation with eight degrees of freedom (8-bit  parameters) onto the two-dimensional Kohonen layer. The performances listed in  the table (Kohonen) are for self-organizing training. The third example problem  was a neocognitron for target localization in a 256x256 8-bit input image. The first  hidden layer of the neocognitron had 8 256x256 sheets of linear convolution units  848 Glover and Miller  /* declare 16-D MMB2k exemplars */  MM_VECTOR state[16] = {  MM_VECTOR(32768), MM_VECTOR(32768),  MM_VECTOR(327O8), MM_VECTOR(327O8),  MM_VECTOR(32788), MM_VECTOR(327O8),  MM_VECTOR(32768), MM_VECTOR(327O8),  MM_VECTOR(327O8), MM_VECTOR(327O8),  MM_VECTOR(327O8), MM_VECTOR(32768),  MM_VECTOR(32788), MM_VECTOR(327O8),  MM_VECTOR(327O8), MM_VECTOR(327O8)  };  /* return PE number of processor with closest match */  long find_best_match(long test_input[16])  {  int i;  MM_VECTOR difference(32768); /* differences */  MM_VECTOR distance(32768); /* distances */  /* compute the 16-D distance scores */  distance = O;  for (i=O; i<16; ++i) {  difference = state[i] - test_input[i];  distance = distance + (difference * difference);  }  /* return the PE number for minimum distance */  return first(distance == minimum(distance));  Figure 3: A C++ code example implementing a nearest neighbor search.  A Massively-Parallel SIMD Processor for Neural Network and Machine Vision Applications 849  Table 3: MM32k Speedup for Select Neural Network Paradigms  Processor RBF Kohonen NCGTRN  MM32k 1:1 1:1 1:1  i486 161:1 76:1 336:1  MIPS 180:1 69:1 207:1  Alpha 31:1 11:1 35:1  SPARC 94:1 49:1 378:1  with 16x16 receptive fields in the input image. The second hidden layer of the  neocognitron had 8 256x256 sheets of sigmoidal units (fixed-point rational function  approximations to sigmoid functions) with 3x3x8 receptive fields in the first hidden  layer. The output layer of the neocognitron had 256x256 sigmoidal units with  3x3x8 receptive fields in the second hidden layer. The performances listed in the  table (NCGTRN) correspond to feedforward response followed by backpropagation  training. The absolute computation times for the MM32k were 5.1 msec, 10 msec,  and 1.3 sec, for the RBF, Kohonen, and NCGTRN neural networks, respectively.  Acknowledgement s  This work was supported in part by a grant from the Advanced Research Projects  Agency (ARPA/ONR Grant N00014-92-J-1858).  References  J. L. Potter. (1985) The Massively Parallel Processor, Cambridge, MA: MIT Press.  G. A. Carpenter and S. Grossberg. (1988) The ART of adaptive pattern recognition  by a self-organizing neural network. Computer vol. 21, pp. 77-88.  R. O. Duda and P. E. Hart. (1973) Pattern Classification and Scene Analysis. New  York: Wiley.  D. Hammerstrom. (1990) A VLSI architecture for high-performance, low cost, on-  chip learning, in Proc. IJCNN, San Diego, CA, June 17-21, vol. II, pp. 537-544.  W. D. Hillis. (1985) The Connection Machine. Cambridge, MA: MIT Press.  M. Holler. (1991) VLSI implementations of learning and memory systems: A review.  In Advances in Neural Information Processing Systems 3, ed. by R. P. Lippman, J.  E. Moody, and D. S. Touretzky, San Francisco, CA: Morgan Kaufmann.  T. Kohonen. (1982) Self-organized formation of topologically correct feature maps.  Biological Cybernetics, vol. 43, pp. 56-69.  J. Moody and C. Darken. (1989) Fast learning in networks of locally- tuned pro-  cessing units. Neural Computation, vol. 1, pp. 281-294.  J. R. Nickoils. (1990) The design of the MasPar MP-i: A cost-effective massively  parallel computer. In Proc. COMPCON Spring '90, San Francisco, CA, pp. 25-28..  
Globally Trained Handwritten Word  Recognizer using Spatial Representation  Convolutional Neural Networks and  Hidden Markov Models  Yoshua Bengio *  Dept. Informatique et Recherche Opdrationnelle  Universit6 de Montr6al  Montreal, Qc H3C-3J7  Yann Le Cun  AT&T Bell Labs  Holmdel NJ 07733  Donnie Henderson  AT&T Bell Labs  Holmdel NJ 07733  Abstract  We introduce a new approach for on-line recognition of handwrit-  ten words written in unconstrained mixed style. The preprocessor  performs a word-level normalization by fitting a model of the word  structure using the EM algorithm. Words are then coded into low  resolution "annotated images" where each pixel contains informa-  tion about trajectory direction and curvature. The recognizer is a  convolution network which can be spatially replicated. From the  network output, a hidden Markov model produces word scores. The  entire system is globally trained to minimize word-level errors.  I Introduction  Natural handwriting is often a mixture of different "styles", lower case printed,  upper case, and cursive. A reliable recognizer for such handwriting would greatly  improve interaction with pen-based devices, but its implementation presents new  *also, AT&T Bell Labs, Holmdel NJ 07733  937  938 Bengio, Le Cun, and Henderson  technical challenges. Characters taken in isolation can be very ambiguous, but con-  siderable information is available from the context of the whole word. We propose  a word recognition system for pen-based devices based on four main modules: a  preprocessor that normalizes a word, or word group, by fitting a geometrical model  to the word structure using the EM algorithm; a module that produces an "anno-  tated image" from the normalized pen trajectory; a replicated convolutional neural  network that spots and recognizes characters; and a Hidden Markov Model (I-IMM)  that interprets the networks output by taking word-level constraints into account.  The network and the HMM are jointl!l trained to minimize an error measure defined  at the word level.  Many on-line handwriting recognizers exploit the sequential nature of pen trajec-  tories by representing the input in the time domain. While these representations  are compact and computationally advantageous, they tend to be sensitive to stroke  order, writing speed, and other irrelevant parameters. In addition, global geometric  features, such as whether a stroke crosses another stroke drawn at a different time,  are not readily available in temporal representations. To avoid this problem we  designed a representation, called AMAP, that preserves the pictorial nature of the  handwriting.  In addition to recognizing characters, the system must also correctly segment the  characters within the words. One approach, that we call INSEG, is to recognize  a large number of heuristically segmented candidate characters and combine them  optimally with a postprocessor (Burges et al 92, Schenkel et al 93). Another ap-  proach, that we call OUTSEG, is to delay all segmentation decisions until after the  recognition, as is often done in speech recognition. An OUTSEG recognizer must  accept entire words as input and produce a sequence of scores for each character at  each location on the input. Since the word normalization cannot be done perfectly,  the recognizer must be robust with respect to relatively large distortions, size van-  ations, and translations. An elastic word model -e.g., an HMM- can extract word  candidates from the network output. The HMM models the long-range sequential  structure while the neural network spots and classifies characters, using local spatial  structure.  2 Word Normalization  Input normalization reduces intra-character variability, simplifying character recog-  nition. This is particularly important when recognizing entire words. We propose a  new word normalization scheme, based on fitting a geometrical model of the word  structure. Our model has four "flexible" lines representing respectively the ascen-  ders line, the core line, the base line and the descenders line (see Figure 1). Points  on the lines are parameterized as follows:  y: fk(x) = k(x- zo) 2 + s(x - zo)+ yok  (1)  where k controls curvature, s is the skew, and (z0,l/0) is a translation vector. The  parameters k, s, and z0 are shared among all four curves, whereas each curve has  its own vertical translation parameter i/ok. First the set of local maxima U and  minima L of the vertical displacement are found. z0 is determined by taking the  average abscissa of extrema points. The lines of the model are then fitted to the  extrema: the upper two lines to the maxima, and the lower two to the minima.  The fit is performed using a probabilistic model for the extrema points given the  lines. The idea is to find the line parameters 0* that maximize the probability of  Globally Trained Handwritten Word Recognizer 939  Figure 1' Word Normalization Model: Ascenders and core curves fit y-maxima  whereas descenders and baseline curves fit !/-minima. There are 6 parameters: a  (ascenders curve height relative to baseline), b (baseline absolute vertical position),  c (core line position), d (descenders curve position), k (curvature), s (angle).  generating the observed points.  /9* = argmax log P(X I D) q- log  (2)  The above conditional distribution is chosen to be a mixture of Gaussians (one  per curve) whose means are the y-positions obtained from the actual x-positions  through equation 1:  3  r(xi, Yi I 0) -- log  Wk N(yi; fk(xi), try) (3)  k=0  where N(x; it, rr) is a univariate Normal distribution of mean tt and standard devi-  ation rr. The wk arc the mixture parameters, some of which are set to 0 in order to  constrain the upper (lower) points to be fitted to the upper (lower) curves. They are  computed a-priori using measured frequencies of associations of extrema to curves  on a large set of words. The priors P(O) on the parameters are required to prevent  the collapse of the curves. They can be used to incorporate a-priori information  about the word geometry, such as the expected position of the baseline, or the  height of the word. These priors for each parameter are chosen to be independent  normal distributions whose standard deviations control the strength of the prior.  The variables that associate each point with one of the curves arc taken as hidden  variables of the EM algorithm. One can thus derive an auxiliary function which can  be analytically (and cheaply) solved for the 6 free parameters 0. Convergence of  the EM algorithm was typically obtained within 2 to 4 iterations (of maximization  of the auxiliary function).  3 AMAP  The recognition of handwritten characters from a pen trajectory on a digitizing  surface is often done in the time domain. Trajectories are normalized, and local  940 Bengio, Le Cun, and Henderson  geometrical or dynamical features are sometimes extracted. The recognition is  performed using curve matching (Tappert 90), or other classification techniques such  as Neural Networks (Guyon et al 91). While, as stated earlier, these representations  have several advantages, their dependence on stroke ordering and individual writing  styles makes them difficult to use in high accuracy, writer independent systems that  integrate the segmentation with the recognition.  Since the intent of the writer is to produce a legible ima#e, it seems natural to  preserve as much of the pictorial nature of the signal as possible, while at the same  time exploit the sequential information in the trajectory. We propose a representa-  tion scheme, called AMAP, where pen trajectories are represented by low-resolution  images in which each picture element contains information about the local proper-  ties of the trajectory. More generally, an AMAP can be viewed as a function in a  multidimensional space where each dimension is associated with a local property of  the trajectory, say the direction of motion 0, the X position, and the Y position of  the pen. The value of the function at a particular location (0, X, Y) in the space  represents a smooth version of the "density" of features in the trajectory that have  values (0, X, Y) (in the spirit of the generalized Hough transform). An AMAP is a  multidimensional array (say 4x10x10) obtained by discretizing the feature density  space into "boxes". Each array element is assigned a value equal to the integral of  the feature density function over the corresponding box. In practice, an AMAP is  computed as follows. At each sample on the trajectory, one computes the position  of the pen (X, Y) and orientation of the motion 0 (and possibly other features, such  as the local curvature c). Each element in the AMAP is then incremented by the  amount of the integral over the corresponding box of a predetermined point-spread  function centered on the coordinates of the feature vector. The use of a smooth  point-spread function (say a Gaussian) ensures that smooth deformations of the  trajectory will correspond to smooth transformations of the AMAP. An AMAP can  be viewed as an "annotated image" in which each pixel is a feature vector.  A particularly useful feature of the AMAP representation is that it makes very few  assumptions about the nature of the input trajectory. It does not depend on stroke  ordering or writing speed, and it can be used with all types of handwriting (capital,  lower case, cursive, punctuation, symbols). Unlike many other representations (such  as global features), AMAPs can be computed for complete words without requiring  segmentation.  4 Convolutional Neural Networks  Image-like representations such as AMAPs are particularly well suited for use in  combination with Multi-Layer Convolutional Neural Networks (MLCNN) (Le Cun  89, Le Cun et al 90). MLCNNs are feed-forward neural networks whose architectures  are tailored for minimizing the sensitivity to translations, rotations, or distortions  of the input image. They are trained with a variation of the Back-Propagation  algorithm (Rumelhart et al 86, Le Cun 86).  The units in MCLNNs are only connected to a local neighborhood in the previous  layer. Each unit can be seen as a local feature detector whose function is determined  by the learning procedure. Insensitivity to local transformations is built into the  network architecture by constraining sets of units located at different places to use  identical weight vectors, thereby forcing them to detect the same feature on different  parts of the input. The outputs of the units at identical locations in different feature  maps can be collectively thought of as a local featur vector. Features of increasing  Globally Trained Handwritten Word Recognizer 941  complexity and globality are extracted by the neurons in the successive layers.  This weight-sharing technique has two interesting side effects. First, the number  of free parameters in the system is greatly reduced since a large number of units  share the same weights. Classically, MLCNNs are shown a single character at the  input, and have a single set of outputs. However, an essential feature of MLCNNs  is that they can be scanned (replicated) over large input fields containing multiple  unse9mentcd characters (whole words) very economically by simply performing the  convolutions on larger inputs. Instead of producing a single output vector, SDNNs  produce a series of output vectors. The outputs detects and recognize characters at  different (and overlapping) locations on the input. These multiple-input, multiple-  output MLCNN are called Space Displacement Neural Networks (SDNN) (Matan  et al 92).  One of the best networks we found for character recognition has 5 layers arranged  as follows: layer 1: convolution with 8 kernels of size 3x3, layer 2:2x2 subsampling,  layer 3: convolution with 25 kernels of size 5x5, layer 4 convolution with 84 kernels  of size 4x4, layer 5:2x2 subsampling. The subsampling layers are essential to the  network's robustness to distortions. The output layer is one (single MLCNN) or  a series of (SDNN) 84-dimensional vectors. The target output configuration for  each character class was chosen to be a bitmap of the corresponding character in a  standard 7x12 (:84) pixel font. Such a code facilitates the correction of confusable  characters by the postprocessor.  5 Post-Processing  The convolutional neural network can be used to give scores associated to characters  when the network (or a piece of it corresponding to a single character output) has  an input field, called a segment, that covers a connected subset of the whole word  input. A segmentation is a sequence of such segments that covers the whole word  input. Because there are in general many possible segmentations, sophisticated  tools such as hidden Markov models and dynamic programming are used to search  for the best segmentation.  In this paper, we consider two approaches to the segmentation problem called IN-  SEG (for input segmentation) and OUTSEG (for output segmentation). The post-  processor can be generally decomposed into two levels: 1) character level scores and  constraints obtained from the observations, 2) word level constraints (grammar,  dictionary). The INSEG and OUTSEG systems share the second level.  In an INSEG system, the network is applied to a large number of heuristically  segmented candidate characters. A cutter generates candidate cuts, which can po-  tentially represent the boundary between two character segments. It also generates  definite cuts, which we assume that no segment can cross. Using these, a number  of candidate segments are constructed and the network is applied to each of them  separately. Finally, for each high enough character score in each of the segment, a  character hypothesis is generated, corresponding to a node in an observation graph.  The connectivity and transition probabilities on the arcs of the observation graph  represent segmentation and geometrical constraints (e.g., segments must not over-  lap and must cover the whole word, some transitions between characters are more  or less likely given the geometrical relations between their images).  In an OUTSEG system, all segmentation decisions are delayed until after the recog-  942 Bengio, Le Cun, and Henderson  nition, as is often done in speech recognition. The AMAP of the entire word is  shown to an SDNN, which produces a sequence of output vectors equivalent to (but  obtained much more cheaply than) scanning the single-character network over all  possible pixel locations on the input. The Euclidean distances between each output  vector and the targets are interpreted as log-likelihoods of the output given a class.  To construct an observation graph, we use a set of character models (HMMs). Each  character HMM ]nodels the sequence of network outputs observed for that charac-  ter. We used three-state HMMs for each character, with a left and right state to  model transitions and a center state for the character itself. The observation graph  is obtained by connecting these character models, allowing any character to follow  any character.  On top of the constraints given in the observation graph, additional constraints that  are independent of the observations are given by what we call a grammar graph,  which can embody lexical constraints. These constraints can be given in the form  of a dictionary or of a character-level grammar (with transition probabilities), such  as a trigram (in which we use the probability of observing a character in the context  of the two previous ones). The recognition finds the best path in the observation  graph that is compatible with the grammar graph. The INSEG and OUTSEG  architecture are depicted in Figure 2.  OUTSEG ARCHITECTURE  FOR WORD RECOGNITION  raw word  word  normalization  normalized  word  AMAP  AMAP  SDNN  graph  candi.  dates  Character  HMMs  Lexical  constraints  S....c ..... r ...... i....p....t  s....e ..... n ..... e.j...o.T  : 5 ...... a...i...u ...... p ..... f  word "Script"  INSEG ARCHITECTURE  FOR WORD RECOGNITION  raw w,.o<.r d  I wo_d _   normalization  n o r m ,a 12z.ed.,, ' ,,  word  .....   Cut hypotheses   generation  qrauh W  - - t AMAP  graph  of ch?acter  canal  I constraints  w E .., .......  Figure 2' INSEG and OUTSEG architectures for word recognition.  A crucial contribution of our system is the joint training of the neural network and  the post-processor with respect to a single criterion that approximates word-level  errors. We used the following discriminant criterion: minimize the total cost (sum  of negative log-likelihoods) along the "correct" paths (the ones that yield the correct  interpretations), while minimizing the costs of all the paths (correct or not). The  discriminant nature of this criterion can be shown with the following example. If  Globally Trained Handwritten Word Recognizer 943  the cost of a path associated to the correct interpretation is much smaller than all  other paths, then the criterion is very close to 0 and no gradient is back-propagated.  On the other hand, if the lowest cost path yields an incorrect interpretation but dif-  fers from a path of correct interpretation on a sub-path, then very strong gradients  will be propagated along that sub-path, whereas the other parts of the sequence  will generate almost no gradient. Within a probabilistic framework, this criterion  corresponds to the maximizing the mutual information (MMI) between the obser-  vations and the correct interpretation. During global training, it is optimized using  (enhanced) stochastic gradient descent with respect to all the parameters in the sys-  tem, most notably the network weights. Experiments described in the next section  have shown important reductions in error rates when training with this word-level  criterion instead of just training the network separately for each character. Similar  combinations of neural networks with HMMs or dynamic programming have been  proposed in the past, for speech recognition problems (Bengio et al 92).  6 Experimental Results  In a first set of experiments, we evaluated the generalization ability of the neural  network classifier coupled with the word normalization preprocessing and AMAP  input representation. All results are in writer independent mode (different writers  in training and testing). Tests on a database of isolated characters were performed  separately on four types of characters: upper case (2.99% error on 9122 patterns),  lower case (4.15% error on 8201 patterns), digits (1.4% error on 2938 patterns), and  punctuation (4.3% error on 881 patterns). Experiments were performed with the  network architecture described above.  The second and third set of experiments concerned the recognition of lower case  words (writer independent). The tests were performed on a database of 881 words.  First we evaluated the improvements brought by the word normalization to the  INSEG system. For the OUTSEG system we have to use a word normalization  since the network sees a whole word at a time. With the INSEG system, and  before doing any word-level training, we obtained without word normalization 7.3%  and 3.5% word and character errors (adding insertions, deletions and substitutions)  when the search was constrained within a 25461-word dictionary. When using the  word normalization preprocessing instead of a character level normalization, error  rates dropped to 4.6% and 2.0% for word and character errors respectively, i.e., a  relative drop of 37% and 43% in word and character error respectively.  In the third set of experiments, we measured the improvements obtained with the  joint training of the neural network and the post-processor with the word-level  criterion, in comparison to training based only on the errors performed at the char-  acter level. Training was performed with a database of 3500 lower case words. For  the OUTSEG system, without any dictionary constraints, the error rates dropped  from 38% and 12.4% word and character error to 26% and 8.2% respectively after  word-level training, i.e., a relative drop of 32% and 34%. For the INSEG system  and a slightly improved architecture, without any dictionary constraints, the error  rates dropped from 22.5% and 8.5% word and character error to 17% and 6.3%  respectively, i.e., a relative drop of 24.4% and 25.6%. With a 25461-word dictio-  nary, errors dropped from 4.6% and 2.0% word and character errors to 3.2% and  1.4% respectively after word-level training, i.e., a relative drop of 30.4% and 30.0%.  Finally, some further improvements can be obtained by drastically reducing the size  of the dictionary to 350 words, yielding 1.6% and 0.94% word and character errors.  944 Bengio, Le Cun, and Henderson  7 Conclusion  We have demonstrated a new approach to on-line handwritten word recognition  that uses word or sentence-level preprocessing and normalization, image-like repre-  sentations, Convolutional neural networks, word models, and global training using  a highly discriminant word-level criterion. Excellent accuracy on various writer  independent tasks were obtained with this combination.  References  Bengio, Y., R. De Mori and G. Flammia and R. Kompe. 1992. Global Optimization  of a Neural Network-Hidden Markov Model Hybrid. IEEE Transactions on Neural  Networks v.3, nb.2, pp.252-259.  Burges, C., O. Matan, Y. Le Cun, J. Denker, L. Jackel, C. Stenard, C. Nohl and J.  Ben. 1992. Shortest Path Segmentation: A Method for Training a Neural Network  to Recognize character Strings. Proc. IJCNN'92 (Baltimore), pp. 165-172, v.3.  Guyon, I., Albrecht, P., Le Cun, Y., Denker, J. S., and Weissman, H. 1991 design  of a neural network character recognizer for a touch terminal. Pattern Recognition,  24(2):105-119.  Le Cun, Y. 1986. Learning Processes in an Asymmetric Threshold Network. In  Bienenstock, E., Fogelman-Soulid, F., and Weisbuch, G., editors, Disordered systems  and biological organization, pages 233-240, Les Houches, France. Springer-Verlag.  Le Cun, Y. 1989. Generalization and Network Design Strategies. In Pfeifer, R.,  Schreter, Z., Fogelman, F., and Steels, L., editors, Connectionism in Perspective,  Zurich, Switzerland. Elsevier. an extended version was published as a technical  report of the University of Toronto.  Le Cun, Y., Matan, O., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hub-  bard, W., Jackel, L. D., and Baird, H. S. 1990. Handwritten Zip Code Recognition  with Multilayer Networks. In IAPR, editor, Proc. of the International Conference  on Pattern Recognition, Atlantic City. IEEE.  Matan, O., Burges, C. J. C., LeCun, Y., and Denker, J. S. 1992. Multi-Digit  Recognition Using a Space Displacement Neural Network. In Moody, J. M., Han-  son, S. J., and Lippman, R. P., editors, Neural Information Processing Systems,  volume 4. Morgan Kaufmann Publishers, San Mateo, CA.  Rumelhart, D. E., Hinton, G. E., and Williams, R. J. 1986. Learning internal rep-  resentations by error propagation. In Parallel distributed processing: Explorations  in the microstructure of cognition, volume I, pages 318-362. Bradford Books, Cam-  bridge, MA.  Schenkel, M., Guyon, I., Weissman, H., and Nohl, C. 1993. TDNN Solutions for  Recognizing On-Line Natural Handwriting. In Advances in Neural Information  Processing Systems 5. Morgan Kaufman.  Tappert, C., Suen, C., and Wakahara, T. 1990. The state of the art in on-line  handwriting recognition. IEEE Trans. PAMI, 12(8).  
Agnostic PAC-Learning of Functions  Analog Neural Nets  (Extended Abstract)  on  Wolfgang Maass  Institute for Theoretical Computer Science  Technische Universitaet Graz  Klosterwiesgasse 32/2  A-8010 Graz, Austria  e-mail: maass@igi.tu-graz.ac.at  Abstract:  There exist a number of negative results ([J], [BR], [KV]) about  learning on neural nets in Valiant's model IV] for probably approx-  imately correct learning ("PAC-learning"). These negative results  are based on an asymptotic analysis where one lets the number of  nodes in the neural net go to infinity. Hence this analysis is less ad-  equate for the investigation of learning on a small fixed neural net  with relatively few analog inputs (e.g. the principal components of  some sensory data). The latter type of learning problem gives rise  to a different kind of asymptotic question: Can the true error of the  neural net be brought arbitrarily close to that of a neural net with  "optimal" weights through sufficiently long training? In this paper  we employ some new arguments in order to give a positive answer  to this question in Haussler's rather realistic refinement of Valiant's  model for PAC-learning ([H], [KSS]). In this more realistic model  no a-priori assumptions are required about the "learning target",  noise is permitted in the training data, and the inputs and outputs  are not restricted to boolean values. As a special case our result  implies one of the first positive results about learning on multi-layer  neural nets in Valiant's original PAC-learning model. At the end  of this paper we will describe an efficient parallel implementation  of this new learning algorithm.  311  312 Maass  We consider multi-layer high order feedforward neural nets A/' with arbitrary piece-  wise polynomial activation functions. Each node g of fan-in m > 0 in iV' is  called a computation node. It is labelled by some polynomial Q9(yl,..., y,)  and some piecewise polynomial activation function 79 : R - R. We assume  that 7 g consists of finitely many polynomial pieces and that its definition in-  volves only rational parameters. The computation node g computes the function  {Y,...,Y,/ - 79(Q9(Y,  ..,Y,)) from R ' into R. The nodes of fan-in 0 in  ("input nodes") are labelled by variables xl,..., xk. The nodes g of fan-out 0 in  iV' ("output nodes") are labelled by 1,..., l. We assume that the range B of their  activation functions 79 is bounded. Any parameters that occur in the definitions of  the 79 are referred to as architectural parameters of iV'.  The coefficients of all the polynomials Q9 are called the programmable parameters  (or weights) of iV'. Let w be the number of programmable parameters of iV'. For any  assignment c E R w to the programmable parameters of iV' the network computes  a function from R k into R l which we will denote by A/'-.  We write Qn for the set of rational numbers that can be written as quotients of  l  integers with bit-length _< n. For _z = Iz,...,zll E R l we write IIzl] for  Izj].  j=l  Let F  R  - R l be some arbitrary functi6n, which we will view as a "prediction  rule". For any given instance (_x,y) G R  x R l we measure the error of F by  I[F(_x) -yll. For any distribution A over some subset of R  x R t we measure the  true error off with regard to A by E(,y__)e.4[llF(_x ) -Yll], i.e. the expected value  of the error of F with respect to distribution A.  Theorem 1: Let A/' be some arbitrary high order feedforward neural net with piece-  wise polynomial activation functions. Let w be the number of programmable para-  meters of A/' (we assume that w = O(1)). Then one can construct from A/' some  first order feedforward neural net J(/' with piecewise linear activation functions and  2  the quadratic activation function 7(x) = x , which has the following property:  There exists a polynomial m(, ) and a learning algorithm LEARN such that for  any given s, 5, (0,1) and s,n  N and any distribution A over Q x (Q, (3B)   the following holds:  For any sample  '- ((xi, Yi})i=X,...,m of m > m(1 1  - 7, 7) points that are independently  drawn according to A the algorithm LEARN computes in polynomially in m, s, n  computation steps an assignment  of rational numbers to the programmable para-  meters of J(/' such that with probability >_ 1 - 5:  - yll] _< . + inf - Yllx],  _ EQ  or in other words:  The true error of J(f5 with regard to A is within e of the least possible true error  that can be achieved by any A/'- with c E Q.  Remarks  a) One can easily see (see [M 93b] for details) that Theorem 1 provides a  positive learning result in Haussler's extension of Valiant's model for PAC-  learning ([H], [KSS]). The "touchstone class" (see [KSS]) is defined as the  Agnostic PAC-Learning of Functions on Analog Neural Nets 313  b)  class of function f  R k - R l that are computable on Af with program-  mable parameters from Q.  This fact is of some general interest, since so far only very few positive  results are known for any learning problem in this rather realistic (but  quite demanding) learning model.  Consider the special case where the distribution A over Qn x (Qn n B) l is  of the form  { , if_y =  AD,_T(_x,y) = 0 , otherwise  for some arbitrary distribution D over the domain Qn and some arbitrary  --T E QsW. Then the term  inf ylll]  EQ  is equal to 0. Hence the preceding theorem states that with learning algo-  rithm LEARN the "learning network" J(/' can "learn" with arbitrarily small  true error any target fimction Afar that is computable on Af with rational  "weights" --T' Thus by choosing Af sufficiently large, one can guarantee  that the associated "learning network" j(/' can learn any target-function  that might arise in the context of a specific learning problem.  In addition the theorem also applies to the more realistic situation where  the learner receives examples (x, y) of the form (_x, Af-T (_x)+ noise), or even  if there exists no "target functih" Af-" that would "explain" the actual  distribution A of examples (x, y) ("agnostic learning").  The proof of Theorem 1 is mathematically quite involved,  only an outline. It consists of three steps:  and we can give here  (1) Construction of the auxiliary neural net ..  (2) Reducing the optimization of weights in jrfor a given distribution A to a  finite nonlinear optimization problem.  (3) Reducing the resulting finite nonliuear optimization problem to a family of  finite linear optimization problems.  Details to step (1): If the activation fianctions  in Af are piecewise linear and  all computation. nodes in Af have fan-out _< 1 (this occurs for example if Af has just  one hidden layer and only one output) then one can set J(/' :- Af. If the  are  piecewise linear but not all computation nodes in Af have fan-out _< 1 one defines  J(f as the tree of the same depth as Af, where subcircuits of computation nodes with  fan-out m > 1 are duplicated m times. The activation functions remain unchanged  in this case.  If the activation fimctions 7 g are piecewise polynomial but not piecewise linear,  one has to apply a rather complex construction which is described in detail in the  Journal version of [M 93a]. In any case j(f has the property that all functions that  314 Maass  are computable on A/' can also be computed on Jf, the depth of  is bounded by a  constant, and the size of JQ' is bounded by a polynomial in the size of A/' (provided  that the depth and order of iV', as well as the number and degrees of the polynomial  pieces of the 7  are bounded by a constant).  Details to step (2): Since the VC-dimension of a neural net is only defined  for neural nets with boolean output, one has to consider here instead the pseudo-  dimension of the function class c that is defined by  Definition: (see Itaussler [It]).  Let X be some arbitrary domain, and let jr be an arbitrary class of functions from  X into It. Then the pseudo-dimension of Y z is defined by  dimf,(.) := max{IS1 ' $ C- x and 3h  $-- It such that  Vb  {0, 3f  Vx  ,S (re) >_ be) = 1)).  Note that in the special case where . is a concept class (i.e. all f 6 . are 0- 1  valued) the pseudo-dimension dimf,(.) coincides with the VC-dimension of.. The  pseudo-dimension of the function class associated with network architectures J(/' with  piecewise polynomial activation finctions can be bounded with the help of Milnor's  Theorem [Mi] in the same way as the VC-dimension for the case of boolean network  output (see [GJ]):  Theorem 2: Consider arbitrary network architectures J(/' of order v with k input  nodes, 1 output nodes, and w programmable parameters. Assume that each gate in  Jff employs as activation fimction some piecewise polynomial (or piecewise rational)  function of degree _< d with at most q pieces. For some arbitrary p  {1,2,...}  we define jr := { f  R +t - R  __a  R w _x 6 R  y G Rt(f(__x,y) =  I1-(_) - yll)}. The,, one has dimp(.) = O(w 2 log q) if v, d, l = O(1).  With the hell) of the pseudo-dinension one can carry out the desired reduction of  the optimization of weights in JQ' (with regard to an arbitrary given distribution A  of examples {_x, y)) to a finite optimization problen. Fix some interval [b, b2] C- R  such that B C_ [bx, b2], bx < ha, and such that the ranges of the activation functions  of the output gates of J(f are contained in [bl,b2]. We define b := I. (ba-bx) , and  ' :---- {f  R  x [b,b2]  --+ [0, b]' __a G R w V_.x G R k Vy G [b,b2]  (f(_x,y) -  II-(_)-yll)). Assume now that parameters s, 5 6 (0, 1) with s < b and s,n  N  have been fixed. For convenience we assume that s is sufficiently large so that  all architectural parameters in A/' are from Q, (we assume that all architectural  parameters in iV' are rational). We define  m .- 7 2-dime(). In 33eb 8  , '- + In  By Corollary 2 of Theorem 7 in Haussler [HI one h for m km(}, ), K :=  e  (2,3), nd any distribution A over Q x (Q, [bl,b2])'  (1) P,',4-*[{Hf e 5.1(  f(,y)) E(_,y_)ea[f(ac, y)][ > ) < 5,  (,u_) e  Agnostic PAC-Learning of Functions on Analog Neural Nets 315  where E(,y_)e.4 [f(__x, y)] is the expectation of f(_x,y) with regard to distribution A.  We design an algorithm LEARN that computes for any rn E N, any sample   ' ((3i,IJi))i{1,...,m}  (Qkn X (Qn o [bl,b2])l) m,  and any given s G N in polynomially in m, s, n computation steps an assignment  _5 of rational numbers to the parameters in J(f such that the function  that is  computed by J(/'5 satisfies  (2) lll(xi )_ yilll  i=1  2  <_ (1 - -)e + inf  i=1  This suffices for the proof of Theorem 1, since (1) and (2) together imply that, for  any distribution A over Q x (Q,, cq lb1, b2])' and any ra > m( 17, ), with probability  >_ 1 - 5 (with respect to the random drawing of (  /"*) the algorithm LEARN  outputs for inputs ( and s an assignment  of rational numbers to the parameters  in :Q' such that  - yl111 <_  + inf - yllx].  Details to step (3): The computation of weights __& that satisfy (2) is nontrivial,  since this amounts to solvinga nonlinear optimization problem. This holds even if  each activation function in Af is piecewise linear, because weights from successive  layers are multiplied with each other.  We employ a method from [M 93a] that allows us to replace the nonlinear conditions  on the programmable parameters __a of.f by linear conditions for a transformed set  c,  of parameters. We simulate f- by another network architecture JQ'[.q] (which  one may view as a "normal form" for /'-) that uses the same graph (V, E) as  J(f, but different activation functions and different values/ for its programmable  parameters. The activation functions of :(f[_c] depend on IVI new architectural  parameters _c E R lul, which we call scaling parameters in the following. Whereas  the architectural parameters of a network architecture are usually kept fixed, we  will be forced to change the scaling parameters of JQ' along with its programmable  parameters/. Although this new network architecture has the disadvantage that  it requires ]VI additional parameters _c, it has the advantage that we can choose in  J(fLc] all weights on edges between computation nodes to be from {-1,0, 1}. Hence  we can treat them as constants with at most 3 possible values in the system of  inequalities that describes computations of JQ']. Thereby we can achieve that all  variables that appear in the inqualities that describe computations of J(f[_q] for fixed  network inputs (the variables for weights of gates on level 1, the variables for the  biases of gates on all levels, and the new variables for the scaling parameters _c)  appear only linearly in those inqualities.  We briefly indicate the construction of JQ' in the case where each activation function  7 in Af is piecewise linear. For any c > 0 we consider the associated piecewise linear  activation function 7  with  W: C  =  316 Maass  Assume that  is some arbitrary given assignment to the programmable parameters  in J(f. We transform j(fa_ through a recursive process into a "normal form" J(/'-  in which all weights on edges between computation nodes are from {-1,0, 1}, such  u  q  Assume that an output gate gout of J(/'- receives as input y] oiy i --oo, where  i--1  a,..., aq, a0 are the weights and the bias of gout (under the assignment _) and  yx,..., yq are the (real valued) outputs of the immediate predecessors g,..., gq of  g. For each i G {1,..., q} with ai  0 such that gi is not all input node we replace  the activation function 7i of gi by 7? '1, and we multiply the weights and the bias  of gate gg with lail. Finally we replace the weight ai of gate gout by sgn(ai), where  sgn(ai) := 1 if ai > 0 and sgn(ai) := -1 if a < 0. This operation has the effect  that the multiplication with I-l is carried out before the gate gi (rather than after  gi, as done in J(f-), but that the considered output gate gout still receives the same  input as before. If "i -- 0 we want to "freeze" that weight at 0. This can be done  by deleting gi and all gates below gi from jr.  The analogous operations are recursively carried out for the predecessors gi of gout  (note however that the weights of gi are no longer the original ones from j/-a_, since  they have been changed in the preceding step). We exploit here the assumption  that each gate in J(/' has fan-out _< 1.  Let /3 consist of the new weights on edges adjacent to input nodes and of the  resulting biases of all gates in Jf. Let c consist of the resulting scaling parameters  at the gates of f. Then we have a_,' E R k (f-(_x) = [C]-(_x)). Furthermore c > 0  for all scaling parameters c in c.  At the end of this proof we will also need the fact that the previously described para-  meter transformation can be inverted, i.e. one can compute from c,__fi an equivalent  weight assignment _ for J(f (with the original activation functions 7).  We now describe how the algorithm LEARN computes for any given sample  ( -- ((xi,Yi))i=l ..... ,  (Q x (Q,, cl [bl,b2])l) m and any given s G N with the  help of linear programming a new assignment _,  to the parameters in J(/' such that  the function h that is computed by Jr[]  satisfies (2). For that purpose we describe  the computations of J(/' for the fized inputs x/from the sample ( - ((xi, Y__i))i=,...,m  by polynomially in m many systems L1,  ., Lp(m) that each consist of O(m) linear  inequalities with the transformed parameters c,/3 as variables. Each system Lj re-  flects one possibility for employing specific linear pieces of the activation functions in  Jf for specific network inputs xx,..., x,n, and for employing different combinations  of weights from {-1, 0, 1} for edges between computation nodes.  One can show that it suffices to consider only polynomially in m many systems  of inequalities Lj by exploiting that all inequalities are linear, and that the input  space for Jf has bounded dimension k.  Agnostic PAC-Learning of Functions on Analog Neural Nets 317  We now expand each of the systems Lj (which has only O(1) variables) into a  linear programming problem LPj with O(m) variables. We add to Lj for each of  the 1 output nodes v of J(f 2m new variables u, v[ for i = 1,..., m, and the 4m  inequalities  ty(xi) < (Yi)v + u - v[ ty (xi) > (y)v + u - v   _ , _ o, o,  where ({x,})i=l ..... , is the fixed sample  and () is that coordinate of V which  corresponds o the output node u of. In these inequalities the symbol ty(xi) de-  notes the term (which is by construction linear in the variables , ) that represents  the output of gate u for network input xi in this system Lj. One should note that  these terms ty(zi) will in general be different for different j, since different linear  pieces of the actuation functions at preceding gates may be used in the computation  of  for the same network input x. We expand the system Lj of linear inequalities  o a linear programming problem LP in canonical form by adding the optimization  requirement  minimize  ]  (u + v).  i= v output node  The algorithm LEARN employs an efficient algorithm for linear programming (e.g.  the ellipsoid algorithm, see [PSI) in order to compute in altogether polynomially  in m, s and n many steps an optimal solution for each of the linear programming  problems LP,..., LP,(,, O. We write hj for the function from R k into R t that is  computed by fr[_c] for the optimal solution _c,/3 of LPj. The algorithm LEARN  m  1  computes -- ' [Ih(xi)- yil for j: 1,...,p(m). Let  be that index for which  i=l  this expression has a minimal value. Let __5,/} be the associated optimal solution of  LPj (i.e. fr[ computes hj). LEARN employs the previously mentioned back-  wards transformation from _,  into values _5_5 for the programmable parameters of  2Q' such that Vx  R k (2Q'-a(_x) = jQ'[_5.](__x)). These values _& are given as output of  the algorithm LEARN.  We refer to [M 93b] for the verification that this weight assignment _& has the  property that is claimed in Theorem 1. We also refer to [M 93b] for the proof in the  more general case where the activation fimctions of Af are piecewise polynomial. !  Remark: The algorithm LEARN can be speeded up substantially on a parallel ma-  chine. Furthermore if the individual processors of the parallel machine are allowed  to use random bits, hardly any global control is required for this parallel computa-  tion. We use polynomially in m many processors. Each processor picks at random  one of the systems Lj of linear inequalities and solves the corresponding linear pro-  gramming problem LP. Then the parallel machine compares in a "competitive  phase" the costs  y_lll of the solutions hj that have been computed by  i:1  the individual processors. It outputs the weights _& for JQ' that correspond to the  318 Maass  best ones of these solutions hi. If one views the number w of weights in A/' no longer  as a constant, one sees that the number of processores that are needed is simply  exponential in w, but that the parallel computation time is polynomial in m and  W.  Acknowledgements  I would like to thank Peter Auer, Phil Long and Hal White for their helpful com-  ments.  References  [GJ]  [HI  [J]  [KV]  [KSS]  [M 93a]  [M 9361  [Mi]  [PSI  IV]  A. Blum, R. L. Rivest, "Training a 3-node neural network is NP-  complete", Proc. of the 1988 Workshop on Computational Learning  Theory, Morgan Kaufmann (San Mateo, 1988), 9 - 18  P. Goldberg, M. Jerrum, "Bounding the Vapnik-Chervonenkis dimen-  sion of concept classes parameterized by real numbers", Proc. of the  6th Annual ACM Conference on Computational Learning Theory, 361  - 369.  D. Haussler, "Decision theoretic generalizations of the PAC model  for neural nets and other learning applications", Information and  Computation, vol. 100, 1992, 78 - 150  J. S. Judd, "Neural Network Design and the Complexity of Learning",  MIT-Press (Cambridge, 1990)  M. Kearns, L. Valiant, "Cryptographic limitations on learning  boolean formulae and finite automata", Proc. of the 21st ACM Sym-  posium on Theory of Computing, 1989, 433 - 444  M. J. Kearns, R. E. Schapire, L. M. Sellie, "Toward efficient agnostic  learning", Proc. of the 5th ACM Workshop on Uomputational Learn-  ing Theory, 1992, 341 - 352  W. Maass, "Bounds for the computational power and learning com-  plexity of analog neural nets" (extended abstract), Proc. of the Sth  ACM Symposium on Theory of Computing, 1993, 335- 344. Journal  version submitted for publication  W. Maass, "Agnostic PAC-learning of functions on analog neural  nets" (journal version), to appear in Neural Computation.  J. Milnor, "On the Betti numbers of real varieties", Proc. of the Amer-  ican Math. ,.qoc., vol. 15, 1964, 275 - 280  C. H. Papadimitriou, K. Steiglitz, "Combinatorial Optimization: Al-  gorithms and Complexity", Prentice Hall (Englewood Cliffs, 1982)  L. G. Valiant, "A theory of the learnable", Comm. of the ACM, vol.  27, 1984, 1134- 1142  
Emergence of Global Structure from  Local Associations  Thea B. Ghiseili-Crippa  Department of Information Science  University of Pittsburgh  Pittsburgh PA 15260  Paul W. Munro  Department of Information Science  University of Pittsburgh  Pittsburgh PA 15260  ABSTRACT  A variant of the encoder architecture, where units at the input and out-  put layers represent nodes on a graph, is applied to the task of mapping  locations to sets of neighboring locations. The degree to which the re-  suiting internal (i.e. hidden unit) representations reflect global proper-  ties of the environment depends upon several parameters of the learning  procedure. Architectural bottlenecks, noise, and incremental learning of  landmarks are shown to be important factors in maintaining topograph-  ic relationships at a global scale.  I INTRODUCTION  The acquisition of spatial knowledge by exploration of an environment has been the sub-  ject of several recent experimental studies, investigating such phenomena as the relation-  ship between distance estimation and priming (e.g. McNamara et al., 1989) and the influ-  ence of route information (McNamara et al., 1984). Clayton and Habibi (1991) have gath-  ered data suggesting that temporal contiguity during exploration is an important factor in  determining associations between spatially distinct sites. This data supports the notion  that spatial associations are built by a temporal process that is active during exploration  and by extension supports Hebb's (1949) neurophysiological postulate that temporal as-  sociations underlie mechanisms of synaptic learning. Local spatial information acquired  during the exploration process is continuously integrated into a global representation of  the environment (cognitive map), which is typically arrived at by also considering global  consxaints, such as low dimensionality, not explicitly represented in the local relation-  ships.  11Ol  1102 Ghiselli-Crippa and Munro  2 NETWORK ARCHITECTURE AND TRAINING  The goal of this network design is to reveal structure among the internal representations  that emerges solely from integration of local spatial associations; in other words, to show  how a network Ixained to learn only local spatial associations characteristic of an environ-  ment can develop internal representations which capture global spatial properties. A vari-  ant of the encoder architecture (Ackley et al., 1985) is used to associate each node on a 2-  D graph with the set of its neighboring nodes, as defined by the arcs in the graph. This 2-  D neighborhood mapping task is similar to the 1-D task explored by Wiles (1993) using  an N-2-N architecture, which can be characterized in terms of a graph environment as a  circular chain with broad neighborhoods.  In the neighborhood mapping experiments described in the following, the graph nodes are  visited at random: at each iteration, a Ixaining pair (node-neighborhood) is selected at ran-  dom from the training set. As in the standard encoder task, the input patterns are all or-'  thogonal, so that there is no structure in the input domain that the network could exploit  in constructing the internal representations; the only information about the structure of  the environment comes from the local associations that the network is shown during  training.  2.1 N-H-N NETWORKS  The neighborhood mapping task was first studied using a strictly layered feed-forward N-  H-N architecture, where N is the number of input and output units, corresponding to the  number of nodes in the environment, and H is the number of units in the single hidden  layer. Experiments were done using square grid environments with wrap-around (toroidal)  and without wrap-around (bounded) at the edges. The resulting hidden unit representations  reflect global properties of the environment to the extent that distances between them cor-  relate with distances between corresponding points on the grid. These two distance mea-  sures are plotted against one another in Figure 1 for toroidal and bounded environments.  5x5 Grid 4 Hidden Units 5x5 Grid 4 Hidden Units  1.4 R^2 = 0. R^2 = 0.499  1.2 1.s'! , =I ,1  .  0.  e  0.4  0 1 2 3 0 1 2 3 4 ,5  Grid Distance Grid Distance  With wrap-around No wrap-around  Figure 1: Scatterplots of Distances between Hidden Unit Representations vs. Distances  between Corresponding Locations in the Grid Environment.  Emergence of Global Structure from Local Associations 1103  2.2 N-2-H-N Networks  A hidden layer with just two units forces representations into a 2-D space, which matches  the dimensionality of the environment. Under this constraint, the image of the environ-  ment in the 2-D space may reflect the topological structure of the environment. This con-  jecture leads to a further conjecture that the 2-D representations will also reveal global re-  lationships of the environment. Since the neighborhoods in a 2-D representation are not  linearly separable regions, another layer (H-layer) is introduced between the two-unit layer  and the output (see Figure 2). Thus, the network has a strictly layered feed-forward  N-2-H-N architecture, where the N units at the input and output layers correspond to the  N nodes in the environment, two units make up the topographic layer, and H is the num-  ber of units chosen for the new layer (H is estimated according to the complexity of the  graph). Responses for the hidden units (in both the T- and H-layers) are computed using  the hyperbolic tangent (which ranges from -1 to +1), while the standard sigmoid (0 to +1)  is used for the output units, to promote orthogonality between representations (Munro,  1989). Instead of the squared error, the cross enU:opy function (Hinton, 1987) is used to  avoid problems with low derivatives observed in early versions of the network.  o 1 2  3 5  6 7 8  ooo ....... o  o o  (oo  ooooo  Figure 2: A 3x3 Environment and the Corresponding Network. When input unit 3 is  activated, the network responds by activating the same unit and all its neighbors.  3 RESULTS  3.1 T-UNIT RESPONSES  Neighborhood mapping experiments were done using bounded square grid environments  and N-2-H-N networks. After training, the topographic unit activities corresponding to  each of the N possible inputs are plotted, with connecting lines representing the arcs from  1104 Ghiselli-Crippa and Munro  the environment. Each axis in Figure 3 represents the activity of one of the T-units.  These maps can be readily examined to study the relationship between their global struc-  ture and the structure of the environment. The receptive fields of the T-units give an alter-  native representation of the same data: the response of each T-unit to all N inputs is repre-  sented by N circles arranged in the same configuration as the nodes in the grid environ-  ment. Circle size is proportional to the absolute value of the unit activity; foled circles  indicate negative values, open circles indicate positive values. The receptive field repre-  sents the T-unit's sensitivity with respect to the environment.    0  00'  o0   o 0  e'  Figure 3: Representations at the Topographic Layer. Activity plots and receptive fields  for two 3x3 grids (left and middle) and a 4x4 grid(righ0.  The two 3x3 cases shown in Figure 3 illuslxate alternative solutions that are each locally  consistent, but have different global structure. In the first case, it is evident how the first  unit is sensitive to changes in the vertical location of the grid nodes, while the second  unit is sensitive to their horizontal location. The axes are essentially rotated 45 degrees in  the second case. Except for this rotation of the reference axes, both representations cap-  tured the global structure of the 3x3 environment.  3.2 NOISE IN THE HIDDEN UNITS  While networks tended to form maps in the T-layer that reflect the global structure of the  environment, in some cases the maps showed correspondences that were less obvious:  i.e., the grid lines crossed, even though the network converged. A few techniques have  proven valuable for promoting global correspondence between the topographic representa-  tions and the environment, including Judd and Munro's (1993) inlxoduction of noise as  pressure to separate representations. The noise is implemented as a small probability for  Emergence of Global Structure from Local Associations 1105  reversing the sign of individual H-unit outputs. As reported in a previous study  (Ghiselli-Crippa and Munro, 1994), the presence of noise causes the network to develop  topographic representations which are more separated, and therefore more robust, so that  the correct output units can be activated even if one or more of the H-units provides an  incorrect output. From another point of view, the noise can be seen as causing the net-  work to behave as if it had an effective number of hidden units which is smaller than the  given number H. The introduction of noise as a means to promote robust topographic  representations can be appreciated by examining Figure 4, which illustrates the represen-  tations of a 5x5 grid developed by a 25-2-20-25 network trained without noise (left) and  with noise (middle) (the network was initialized with the same set of small random  weights in all cases). Note that the representations developed by the network subject to  noise are more separated and exhibit the same global structure as the environment. To  avoid convergence problems observed with the use of noise throughout the whole training  process, the noise can be introduced at the beginning of training and then gradually re-  duced over time.  A similar technique involves the use of low-level noise injected in the T-layer to directly  promote the formation of well-separated representations. Either Gaussian or uniform  noise directly added to the T-unit outputs gives comparable results. The use of noise in  either hidden layer has a beneficial influence on the formation of globally consistent rep-  resentations. However. since the noise in the H-units exerts only an indirect influence on  the T-unit representations, the choice of its actual value seems to be less crucial than in  the case where the noise is directly applied at the T-layer.  The drawback for the use of noise is an increase in the number of iterations required by  the network to converge, that scales up with the magnitude and duration of the noise.  Figure 4: Representations at the Topographic Layer. Training with no noise (left) and  with noise in the hidden units (middle); training using landmarks (righ0.  3.3 LANDMARK LEARNING  Another effective method involves the organization of training in 2 separate phases, to  model the acquisition of landmark information followed by the development of route  and/or survey knowledge (Hart and Moore, 1973; Siegel and White, 1975). This method  is implemented by manipulating the training set during learning, using coarse spatial res-  olution at the outset and introducing interstitial features as learning progresses to the sec-  ond phase. The first phase involves training the network only on a subset of the possible  1106 Ghiselli-Crippa and Munro  N patterns (landmarks). Once the landmarks have been learned, the remaining patterns are  added to the training set. In the second phase, xaining proceeds as usual with the full set  of training patterns; the only restriction is applied to the landmark points, whose topo-  graphical representations are not allowed to change (the corresponding weights between  input units and T-units are frozen), thus modeling the use of landmarks as stable reference  points when learning the details of a new environment. The right pane of Figure 4 illus-  trams the representations developed for a 5x5 grid using landmark training; the same 25-2-  20-25 network mentioned above was trained in 2 phases, first on a subset of 9 patterns  (landmarks) and then on the full set of 25 panems (the landmarks are indicated as white  circles in the activity plot).  3.4 NOISE IN LANDMARK LEARNING  The techniques described above (noise and landmark learning) can be combined together to  better promote the emergence of well-structured representation spaces. In particular, noise  can be used during the first phase of landmark learning to encourage a robust representa-  tion of the landmarks: Figure 5 illustrates the representations obtained for a 5x5 grid  using landmark training with two different levels of noise in the H-units during the first  phase. The effect of noise is evident when comparing the 4 comer landmarks in the right  pane of Figure 4 (landmark learning with no noise) with those in Figure 5. With increas-  ing levels of noise, the T-unit activities corresponding to the 4 comer landmarks approach  the asymptotic values of +1 and -1; the activity plots illustrate this effect by showing  how the comer landmark representations move toward the comers of T-space, reaching a  configuration which provides more resistance to noise. During the second phase of train-  ing, the landmarks function as reference points for the additional features of the environ-  ment and their positioning in the representational space therefore becomes very impor-  tant. A well-formed, robust representation of the landmarks at the end of the first phase  is crucial for the formation of a map in T-space that reflects global structure, and the use  of noise can help promote this.  Figure 5: Representations at the Topographic Layer. Landmark training using noise in  phase 1: low noise level (left), high noise level (right).  4 DISCUSSION  Large scale constraints intrinsic to natural environments, such as low dimensionality, are  not necessarily reflected in local neighborhood relations, but they constitute information  which is essential to the successful development of useful representations of the environ-  Emergence of Global Structure from Local Associations 1107  ment. In our model, some of the constraints imposed on the network architecture effec-  tively reduce the dimensionality of the representational space. Constraints have been in-  troduced several ways: bottlenecks, noise, and landmark learning; in all cases, these con-  sU:aints have had constructive influences on the emergence of globally consistent repre-  sentation spaces. The approach described presents an alternative to Kohonen's (1982)  scheme for capturing topography; here, topographic relations emerge in the representa-  tional space, rather than in the weights between directly connected units.  The experiments described thus far have focused on how global spatial structure can  emerge from the integration of local associations and how it is affected by the introduc-  tion of global constraints. As mentioned in the inoduction, one additional factor influ-  encing the process of acquisition of spatial knowledge needs to be considered: temporal  contiguity during exploration, that is, how temporal associations of spatially adjacent lo-  cations can influence the representation of the environment. For example, a random type  of exploration ("wandering") can be considered, where the next node to be visited is select-  ed at random from the neighbors of the current node. Preliminary studies indicate that  such temporal contiguity during training results in the formation of hidden unit represen-  tations with global properties qualitatively similar to those reported here. Alternatively,  more directed exploration methods can be studied, with a systematic pattern guiding the  choice of the next node to be visited. The main purpose of these studies will be to show  how different exploration strategies can affect the formation and the characteristics of cog-  nitive maps of the environment.  Higher order effects of temporal and spatial contiguity can also be considered. However,  in order to capture regularities in the training process that span several exploration steps,  simple feed-forward networks may no longer be sufficient; partially recurrent networks  (Elman, 1990) are a likely candidate for the study of such processes.  Acknowledgements  We wish to thank Stephen Hirtle, whose expertise in the area of spatial cognition greatly  benefited our research. We are also grateful for the insightful comments of Janet Wiles.  References  D. H. Ackley, G. E. Hinton, and T. J. Sejnowski (1985) "A learning algorithm for  Boltzmann machines," Cognitive Science, vol. 9, pp. 147-169.  K. Clayton and A. Habibi (1991) "The contribution of temporal contiguity to the spatial  priming effect," Journal of Experimental Psychology: Learning, Memory, and  Cognition, vol. 17, pp. 263-271.  J. L. Elman (1990) "Finding structure in time," Cognitive Science, vol. 14, pp. 179-  211.  T. B. Ghiselli-Crippa and P. W. Munro (1994) "Learning global spatial structures from  local associations," in M. C Mozer, P. Smolensky, D. S. Touretzky, J. L. Elman, and A.  S. Weigend (Eds.), Proceedings of the 1993 Connectionist Models Summer School,  Hillsdale, NJ: Efibaum.  1108 Ghiselli-Crippa and Munro  R. A. Hart and G. T. Moore (1973) "The development of spatial cognition: A review," in  R. M. Downs and Stea (Eds.), Image and Environment, Chicago, IL: Aldine.  D. O. Hebb (1949) The Organization of Behavior, New York, NY: Wiley.  G. E. Hinton (1987) "Connectionist learning procedures," Technical Report CMU-CS-  87-115, version 2, Pittsburgh, PA: Carnegie-Mellon University, Computer Science  Department.  S. Judd and P. W. Munro (1993) "Nets with unreliable hidden nodes learn error-correcting  codes," in C. L. Giles, S. J. Hanson, and J. D. Cowan, Advances in Neural Information  Processing Systems 5, San Mateo, CA: Morgan Kaufmann.  T. Kohonen (1982) "Self-organized formation of topological correct feature maps,"  Biological Cybernetics, vol. 43, pp. 59-69.  T. P. McNamara, J. K. Hardy, and S.C. Hirtle (1989) "Subjective hierarchies in spatial  memory," Journal of Experimental Psychology: Learning, Memory, and Cognition, vol.  15, pp. 211-227.  T. P. McNamara, R. Ratcliff, and G. McKoon (1984) "The mental representation of  knowledge acquired from maps," Journal of Experimental Psychology: Learning,  Memory, and Cognition, vol. 10, pp. 723-732.  P. W. Munro (1989) "Conjectures on representations in backpropagation networks,"  Technical Report TR-89-035, Berkeley, CA: International Computer Science Institute.  A. W. Siegel and S. H. White (1975) "The development of spatial representations of  large-scale environments," in H. W. Reese (Ed.), Advances in Child Development and  Behavior, New York, NY: Academic Press.  J. Wiles (1993) "Representation of variables and their values in neural networks," in  Proceedings of the Fifteenth Annual Conference of the Cognitive Science Society,  Hillsdale, NJ: Erlbaum.  
A Hodgkin-Huxley Type Neuron Model  That Learns Slow Non-Spike Oscillation  Kenji Doya*  Allen I. Selverston  Department of Biology  University of California, San Diego  La Jolla, CA 92093-0357, USA  Peter F. Rowat  Abstract  A gradient descent algorithm for parameter estimation which is  similar to those used for continuous-time recurrent neural networks  was derived for Hodgkin-Huxley type neuron models. Using mem-  brane potential trajectories as targets, the parameters (maximal  conductances, thresholds and slopes of activation curves, time con-  stants) were successfully estimated. The algorithm was applied to  modeling slow non-spike oscillation of an identified neuron in the  lobster stomatogastric ganglion. A model with three ionic currents  was trained with experimental data. It revealed a novel role of  A-current for slow oscillation below -50 mV.  I INTRODUCTION  Conductance-based neuron models, first formulated by Hodgkin and Huxley [10],  are commonly used for describing biophysical mechanisms underlying neuronal be-  havior. Since the days of Hodgkin and Huxley, tens of new ionic channels have  been identified [9]. Accordingly, recent H-H type models have tens of variables and  hundreds of parameters [1, 2]. Ideally, parameters of H-H type models are deter-  mined by voltage-clamp experiments on individual ionic currents. However, these  experiments are often very difficult or impossible to carry out. Consequently, many  parameters must be hand-tuned in computer simulations so that the model behavior  resembles that of the real neuron. However, a manual search in a high dimensional  *current address: The Salk Institute, CNL, P.O. Box 85800, San Diego, CA 92186-5800.  566  A Hodgkin-Huxley Type Neuron Model That Learns Slow Non-Spike Oscillation 567  Figure 1: A connectionist's view of the H-H neuron model.  parameter space is very unreliable. Moreover, even if a good match is found be-  tween the model and the real neuron, the validity of the parameters is questionable  because there are, in general, many possible settings that lead to apparently the  same behavior.  We propose an automatic parameter tuning algorithm for H-H type neuron models  [5]. Since a H-H type model is a network of sigmoid functions, multipliers, and  leaky integrators (Figure 1), we can tune its parameters in a manner similar to the  tuning of connection weights in continuous-time neural network models [0, 12]. By  training a model from many initial parameter points to match the experimental  data, we can systematically estimate a region in the parameter space, instead of a  single point.  We first test if the parameters of a spiking neuron model can be identified from the  membrane potential trajectories. Then we apply the learning algorithm to a model  of slow non-spike oscillation of an identified neuron in the lobster stomatogastric  ganglion [7]. The resulting model suggests a new role of A-current [3] for slow  oscillation in the membrane potential range below -50 mV.  2 STANDARD FORM OF IONIC CURRENTS  Historically, different forms of voltage dependency curves have been used to repre-  sent the kinetics of different ionic channels. However, in order to derive a simple,  efficient learning algorithm, we chose a unified form of voltage dependency curves  which is based on statistical physics of ionic channels [11] for all the ionic currents  in the model.  The dynamics of the membrane potential v is given by  Ci '- I- EIj, I.i = g.ia?bjq. J(v - vrj), (1)  J  where C is the membrane capacitance and I is externally injected current. The j-th  ionic current Ij is the product of the maximum conductance gj, activation variable  568 Doya, Selverston, and Rowat  aj, inactivation variable bj, and the difference of the membrane potential v from  the reversal potential vrj. The exponents pj and qj represent multiplicity of gating  elements in the ionic channels and are usually an integer between 0 and 4. Variables  aj and bj are assumed to obey the first order differential equation   = k(v). (-x + x(v)), (: aj, bj). (2)  Their steady states aj and bj are sigmoid functions of the membrane potential  1   () = ( =  ), (3)  1 + e-(-) ' '  where v and s represent the threshold and slope of the steady state curve, re-  spectively. The rate coefficients k(v) and ka(v) have the voltage dependence  []  1 cosh s(v - v) (x = a, b), (4)  ():  2  where t is the time constant.  3 ERROR GRADIENT CALCULUS  Our goal is to minimize the average error over a cycle with period T:  E: 7 (v(t)- *(t))"t,  where v*(t) is the target membrane potential trajectory.  (5)  i> = OF OF  OX Y + OOi'  .' = F(X;...,Oi,...),  is evaluated by the variation equation  i: kx(*(t)). (- + (*(t)))(: a,b).  We use (6) in place of (2) during training.  The effect of a small change in a parameter Oi of a dynamical system  (x  a ') (7)  (  a'), (8)  which is an n-dimensional linear system with time-varying coefficients [6, 12]. In  general, this variation calculus requires O(n 2) arithmetics for each parameter. How-  ever, in the case of H-H model with teacher forcing, (8) reduces to a first or second  order linear system. For example, the effect of a small change in the maximum  conductance gj on the membrane potential v is estimated by  Ct = -G(t)y - aj(7)PJ bj(7 )qJ (v(7) -- Vrj ), (9)  (6)  We first derive the gradient of E with respect to the model parameters (..., Oi, ...) =  (..., gj, va,, sa, ta, ...). In studies of recurrent neural networks, it has been shown  that teacher forcing is very important in training autonomous oscillation patterns [4,  6, 12, 13]. In H-H type models, teacher forcing drives the activation and inactivation  variables by the target membrane potential v* (t) instead of v(t) as follows.  A Hodgkin-Huxley Type Neuron Model That Learns Slow Non-Spike Oscillation 569  where G(t) :  ga(t)Pkb(t) qk is the total membrane conductance.  the effect of the activation threshold va is estimated by the equations  C = -G(t)y - gipjaj(t)PJ-bj(t)qJ(v(t) - vrj) z,  }: --kay(t)[Z + S{aj(t)+aj(t)- 2aj(t)aj(t)}] .  The solution y(t) represents the perturbation in v at time t, namely  error gradient is then given by  OE 1 for  ooi = 7  Similarly,  (10)  The  (11)  4 PARAMETER UPDATE  Basically, we can use arbitrary gradient-based optimization algorithms, for example,  simple gradient descent or conjugate gradient descent. The particular algorithm we  used was a continuous-time version of gradient descent on normalized parameters.  Because the parameters of a H-H type model have different physical dimensions and  magnitudes, it is not appropriate to perform simple gradient descent on them. We  represent each parameter by the default value Oi and the deviation i as below.  gj=jeJ,  Then we perform gradient descent on the normalized parameters i.  Instead of updating the parameters in batches, i.e. after running the model for T  and integrating the error gradient by (11), we updated the parameters on-line using  the running average of the gradient as follows.  Ov(t) 00  +  Oi -- --/X,, (13)  where Ta is the averaging time and e is the learning rate. This on-line scheme was  less susceptible to 2T-periodic parameter oscillation than batch update scheme and  therefore we could use larger learning rates.  5 PARAMETER ESTIMATION OF A SPIKING MODEL  First, we tested if a model with random initial parameters can estimate the pa-  rameters of another model by training with its membrane potential trajectories.  The default parameters Oi of the model was set to match the original H-H model  [10] (Table 1). Its membrane potential trajectories at five different levels of current  injection (I = 0, 15,30, 45, and 60yA/cm ) were used alternately as the target v*(t).  We ran 100 trials after initializing i randomly in [-0.5,+0.5]. In 83 cases, the error  became less than 1.3 mV rms after 100 cycles of training. Figure 2a is an exam-  ple of the oscillation patterns of the trained model. The mean of the normalized  570 Doya, Selverston, and Rowat  Table 1: Parameters of the spiking neuron model. Subscripts L, Na and K speci-  fies leak, sodium and potassium currents, respectively. Constants: C=lluF/cm 2,  VN,=55mV, vK=-72mV, v=-50mV, pN,=3, qN,=l, pK=4, qK=p=q=O,  Av=20mV, e=0.1, T, = 5T.  gL  gNa  VaNa  $aNa  7aN a  VbNa  $bNa  7bNa  gK  YaK  8aK  7 a K  default value  0.3 mS/cm  120.0 mS/cm  -36.0 mV  0.1 1/mY  0.5 l'nsec  -62.0 mV  -0.09 1/mV  12.0 msec  40.0 mS/cm  -50.0 mV  0.06 1/mV  5.0 msec  ti after learning  mean s.d.  -0.017 0.252  -0.002 0.248  0.006 0.033  -0.052 0.073  -0.103 0.154  0.012 0.202  -0.010 0.140  0.093 0.330  0.050 0.264  -0.021 0.136  -0.061 0.114  -0.073 0.168  a_Na tbNa  sbNa  b_Na taNa  saNa  gNa  0 10 20 30  time (ms)  gL gNavaNasaNataNavbNasbNatbNa gK yak saK taK  (a) (b)  Figure 2: (a) The trajectory of the spiking neuron nodel at I = 30t. tA/cm . v:  membrane potential (-80 to +40 mV). a and b: activation and inactivation variables  (0 to 1). The dotted line in v shows the target trajectory v*(t). (b) Covariance  matrix of the normalized parameters i after learning. The black and white squares  represent negative and positive covariances, respectively.  A Hodgkin-Huxley Type Neuron Model That Learns Slow Non-Spike Oscillation 571  Table 2: Parameters of the DG cell model. Constants: C=lF/cm 2, VA=-80mV,  VH= -10mV, v=-50mV, pA=3, qA=l, pH=l, qH=p=q=O, Av=20mV, e=0.1,  T, = 2T.  tuned 0 i  0.01 0.025 mS/cm 2  50 41.0 mS/cm 2  -12 -11.1 mV  0.04 0.022 1/mV  7.0 7.0 msec  -62 -76 mV  -0.16 -0.19 1/mV  300 292 msec  0.1 0.039 mS/cm 2  -70 -75.1 mV  -0.14 -0.11 1/mV  3000 4400 msec  gL  VaA  $aA  taA  VbA  8bA  7bA  gH  Wall  Sail  7 aH  0 10000 20000 30000 40000 50000  time(ms)  Figure 3: Oscillation pattern of the DG cell model. v: membrane potential (-70 to  -50 mV). a and b: activation and inactivation variables (0 to 1). I: ionic currents  (-1 to +1 tA/cm2).  parameters i were nearly zero (Table 1), which implies that the original parame-  ter values were successfully estimated by learning. The standard deviation of each  parameter indicates how critical its setting is to replicate the given oscillation pat-  terns. From the covariance matrix of the parameters (Figure 2b), we can estimate  the distribution of the solution points in the parameter space.  6 MODELING SLOW NON-SPIKE OSCILLATION  Next we applied the algorithm to experimental data from the "DG cell" of the lob-  ster stomatogastric ganglion [7]. An isolated DG cell oscillates endogenously with  the acetylcholine agonist pilocarpine and the sodium channel blocker TTX. The  oscillation period is 5 to 20 seconds and the membrane potential is approximately  between -70 and -50 mV. From voltage-clamp data from other stomatogastric neu-  rons [8], we assumed that A-current (potassium current with inactivation) [3] and  H-current (hyperpolarization-activated slow inward current) are the principal active  currents in this voltage range. The default parameters for these currents were taken  from [2] (Table 2).  572 Doya, Selverston, and Rowat  2  -2  ionic currents  !  !  !  -60 -40 -20 0 20 40  v (mY)  --I_L  .... I_A  -- I_H  Figure 4: Current-voltage curves of the DG cell model. Outward current is positive.  Figure 3 is an example of the model behavior after learning for 700 cycles. The  actual output v of the model, which is shown in the solid curve, was very close  to the target output v*(t), which is shown in the dotted curve. The bottom three  traces show the ionic currents underlying this slow oscillation. Figure 4 shows the  steady state I-V curves of three currents. A-current has negative conductance in  the range from -70 to -40 mV. The resulting positive feedback on the membrane  potential destabilizes a quiescent state. If we rotate the I-V diagram 180 degrees, it  looks similar to the I-V diagram for the H-H model; the faster outward A-current  in our model takes the role of the fast inward sodium current in the H-H model and  the slower inward H-current takes the role of the outward potassium current.  7 DISCUSSION  The results indicate that the gradient descent algorithm is effective for estimating  the parameters of H-H type neuron models from membrane potential trajectories.  Recently, an automatic parameter search algorithm was proposed by Bhalla and  Bower [1]. They chose only the maximal conductances as free parameters and used  conjugate gradient descent. The error gradient was estimated by slightly changing  each of the parameters. In our approach, the error gradient was more efficiently de-  rived by utilizing the variation equations. The use of teacher forcing and parameter  normalization was essential for the gradient descent to work.  In order for a neuron to be an endogenous oscillator, it is required that a fast pos-  itive feedback mechanism is balanced with a slower negative feedback mechanism.  The most popular example is the positive feedback by the sodium current and the  negative feedback by the potassium current in the H-H model. Another common  example is the inward calcium current counteracted by the calcium dependent out-  ward potassium current. We found another possible combination of positive and  negative feedback with the help of the algorithm: the inactivation of the outward  A-current and the activation of the slow inward H-current.  A Hodgkin-Huxley Type Neuron Model That Learns Slow Non-Spike Oscillation 573  Acknowledgement s  The authors thank Rob Elson and Thom Cidand for providing physiological data  from stomatogastric cells. This study was supported in part by ONR grant N00014-  91-J-1720.  References  [1] U. S. Bhalla and J. M. Bower. Exploring parameter space in detailed single  neuron models: Simulations of the mitral and granule cells of the olfactory  bulb. Journal of Neurophysiology, 69:1948-1965, 1993.  [2] F. Buchholtz, J. Golowasch, I. R. Epstein, and E. Marder. Mathematical model  of an identified stomatogastric ganglion neuron. Journal of Neurophysiology,  67:332-340, 1992.  [3] J. A. Connor, D. Walter, and R. McKown. Neural repetitive firing, modifi-  cations of the Hodgkin-Huxley axon suggested by experimental results from  crustacean axons. Biophysical Journal, 18:81-102, 1977.  [4] K. Doya. Bifurcations in the learning of recurrent neural networks. In Proceed-  ings of 1992 IEEE International Symposium on Circuits and Systems, pages  6:2777-2780, San Diego, 1992.  [5] K. Doya and A. I. Selverston. A learning algorithm for Hodgkin-Huxley type  neuron models. In Proceedings of IJCNN'93, pages 1108-1111, Nagoya, Japan,  1993.  [6] K. Doya and S. Yoshizawa. Adaptive neural oscillator using continuous-time  back-propagation learning. Neural Networks, 2:375-386, 1989.  [7] R. C. Elson and A. I. Selverston. Mechanisms of gastric rhythm generation  in the isolated stomatogastric ganglion of spiny lobsters: Bursting pacemaker  potential, synaptic interactions, and muscarinic modulation. Journal of Neu-  rophysiology, 68:890-907, 1992.  [8] J. Golowasch and E. Marder. Ionic currents of the lateral pyloric neuron of  stomatogastric ganglion of the crab. Journal of Neurophysiology, 67:318-331,  1992.  [9]  [10]  [11]  [12]  [13]  B. Hille. Ionic Channels of Excitable Membranes. Sinauer, 1992.  A. L. Hodgkin and A. F. Huxley. A quantitative description of membrane  currents and its application to conduction and excitation in nerve. Journal of  Physiology, 117:500-544, 1952.  H. Lecar, G. Ehrenstein, and R. Latorre. Mechanism for channel gating in  excitable bilayers. Annals of the New York Academy of Sciences, 264:304-313,  1975.  P. F. Rowat and A. I. Selverston. Learning algorithms for oscillatory networks  with gap junctions and membrane currents. Network, 2:17-41, 1991.  R. J. Williams and D. Zipser. Gradient based learning algorithms for recurrent  connectionist networks. Technical Report NU-CCS-90-9, College of Computer  Science, Northeastern University, 1990.  
A Learning Analog Neural Network Chip  with Continuous-Time Recurrent  Dynamics  Gert Cauwenberghs*  California Institute of Technology  Department of Electrical Engineering  128-95 Caltech, Pasadena, CA 91125  E-mail: gertcco. caltech. edu  Abstract  We present experimental results on supervised learning of dynam-  ical features in an analog VLSI neural network chip. The recur-  rent network, containing six continuous-time analog neurons and 42  free parameters (connection strengths and thresholds), is trained to  generate time-varying outputs approximating given periodic signals  presented to the network. The chip implements a stochastic pertur-  bative algorithm, which observes the error gradient along random  directions in the parameter space for error-descent learning. In ad-  dition to the integrated learning functions and the generation of  pseudo-random perturbations, the chip provides for teacher forc-  ing and long-term storage of the volatile parameters. The network  learns a I kHz circular trajectory in 100 sec. The chip occupies  2mm x 2mm in a 2pm CMOS process, and dissipates 1.2 mW.  I Introduction  Exact gradient-descent algorithms for supervised learning in dynamic recurrent net-  works [1-3] are fairly complex and do not provide for a scalable implementation in  a standard 2-D VLSI process. We have implemented a fairly simple and scalable  *Present address: Johns Hopkins University, ECE Dept., Baltimore MD 21218-2686.  858  A Learning Analog Neural Network Chip with Continuous-Time Recurrent Dynamics 859  learning architecture in an analog VLSI recurrent network, based on a stochastic  perturbative algorithm which avoids calculation of the gradient based on an explicit  model of the network, but instead probes the dependence of the network error on  the parameters directly [4]. As a demonstration of principle, we have trained a  small network, integrated with the learning circuitry on a CMOS chip, to gener-  ate outputs following a prescribed periodic trajectory. The chip can be extended,  with minor modifications to the internal structure of the cells, to accommodate  applications with larger size recurrent networks.  2 System Architecture  The network contains six fully interconnected recurrent neurons with continuous-  time dynamics,  d 6  r = + + ,  j=l  with xi(t) the neuron states representing the outputs of the network, y(t) the  external inputs to the network, and a(.) a sigmoidal activation function. The 36  connection strengths Wi and 6 thresholds Oj constitute the free parameters to be  learned, and the time constant r is kept fixed and identical for all neurons. Below,  the parameters W and 0 are denoted as components of a single vector p.  The network is trained with target output signals x(t) and x(t) for the first two  neuron outputs. Learning consists of minimizing the time-averaged error  12  (p) = li__}rn  T E [x(t) -- xk(t)[Vdt , (2)  using a distance metric with norm y. The learning algorithm [4] iteratively specifies  incremental updates in the parameter vector p as  p(+) = p() - t () *r  (3)  with the perturbed error  (k) = _1 ((p() + r()) _ (p() _ r()) ) (4)  2  obtained from a two-sided parallel activation of fixed-amplitude random perturba-  tions h(k) onto the parameters pi(t); .i(t) _ -t-a with equal probabilities for both  polarities. The algorithm basically performs random-direction descent of the error  as a multi-dimensional extension to the Kiefer-Wolfowitz stochastic approximation  method [5], and several related variants have recently been proposed for optimiza-  tion [6,7] and hardware learning [8-10].  To facilitate learning, a teacher forcing signal is initially applied to the external  input y according to  yi(t) = X ff(xiT(t)-xi(t)) , i= 1,2 (5)  providing a feedback mechanism that forces the network outputs towards the tar-  gets [3]. A symmetrical and monotonically increasing "squashing" function for '7(-)  serves this purpose. The teacher forcing amplitude A needs to be attenuated along  the learning process, as to suppress the bias in the network outputs at convergence  that might result from residual errors.  860 Cauwenberghs  3 Analog VLSI Implementation  The network and learning circuitry are implemented on a single analog CMOS chip,  which uses a transconductance current-mode approach for continuous-time opera-  tion. Through dedicated transconductance circuitry, a wide linear dynamic range  for the voltages is achieved at relatively low levels of power dissipation (experimen-  tally 1.2 mW while either learning or refreshing). While most learning functions,  including generation of the pseudo-random perturbations, are integrated on-chip in  conjunction with the network, some global and higher-level learning functions of  low dimensionality, such as the evaluation of the error (2) and construction of the  perturbed error (4), are performed outside the chip for greater flexibility in tailoring  the learning process. The structure and functionality of the implemented circuitry  are illustrated in Figures i to 3, and a more detailed description follows below.  3.1 Network Circuitry  Figure 1 shows the schematics of the synapse and neuron circuitry. A synapse cell of  single polarity is shown in Figure I (a). A high output impedance triode multiplier,  using an adjustable regulated cascode [11], provides a constant current Ii. linear in the  voltage Wij over a wide range. The synaptic current Iff feeds into a differential pair,  injecting a differential current Ii.i cr(xj -0.) into the diode-connected Io+ut and I-ut output  lines. The double-stack transistor configuration of the differential pair offers an expanded  linear sigmoid range. The summed output currents Io+, and I-, of a row of synapses are  collected in the output cell, Figure I (b), which also subtracts the reference currents  and /f obtained from a reference row of "dummy" synapses defining the "zero-point"  synaptic strength Woa for bipolar operation. The thus established current corresponds to  the summed synaptic contributions in (1). Wherever appropriate (i = 1, 2), a differential  transconductance element with inputs xi and x is added to supply an external input  current for forced teacher action in accordance with (5).  xof  (a) co)  Figure 1 Schematics of synapse and neuron circuitry. (a) Synapse of single polarity.  (b) Output cell with current-to-voltage converter.  The output current is converted to the neuron output voltage xi, through an active resistive  element using the same regulated high output impedance triode circuitry as used in the  synaptic current source. The feedback delay parameter r in (1) corresponds to the RC  A Learning Analog Neural Network Chip with Continuous-Time Recurrent Dynamics 861  product of the regulated triode active resistance value and the capacitance Oot. With  Cout = 5 pF, the delay ranges between 20 and 200/sec, adjustable by the control voltage  of the regulated cascode. Figure 2 shows the measured static characteristics of the synapse  and neuron functions for different values of Wij and 0j ( i = j = 1), obtained by disabling  the neuron feedback and driving the synapse inputs externally.  II i I I I I  'V  , -0.4-  -0.8 - -- - 1.6 V  I I I I I  4.o -o.s o.o o.s .o  Input Voltage xj (v)  (a)  0.0  .0.4  4).6  4).8  ! I I I I  I I I I  4.o -o.s o.o o.s .o  Input Voltage x j (v)  Figure 2 Measured static synapse and neuron characteristics, for various values of  (a) the connection strength Wil, and (b) the threshold 0.  3.2 Learning Circuitry  Figure 3 (a) shows the simplified schematics of the learning and storage circuitry, replicated  locally for every parameter (connection strength or threshold) in the network. Most of  the variables relating to the operation of the cells are local, with exception of a few global  signals communicating to all cells. Global signals include the sign and the amplitude  of the perturbed error  and predefined control signals. The stored parameter and its  binary perturbation are strictly local to the cell, in that they do not need to communicate  explicitly to outside circuitry (except trivially through the neural network it drives), which  simplifies the structural organization and interconnection of the learning cells.  The parameter voltage Pi is stored on the capacitor Cstore, which furthermore couples  to capacitor Cpert for activation of the perturbation. The perturbation bit *ri selects  either of two complementary signals V+ and V_ with corresponding polarity. With  the specific shape of the waveforms V+ and V- depicted in Figure 3 (b), the proper  sequence of perturbation activations is established for observation of the complementary  error terms in (4). The obtained global value for  is then used, in conjunction with  the local perturbation bit *ri, to update the parameter value pi according to (3). A fine-  resolution charge-pump, shown in the dashed-line inset of Figure 3 (a), is used for this  purpose. The charge pump dumps either of a positive or negative update current, of equal  amplitude, onto the storage capacitor whenever it is activated by means of an EN_UPD  high pulse, effecting either of a given increment or decrement on the parameter value pi  respectively. The update currents are supplied by two complementary transistors, and are  switched by driving the source voltages of the transistors rather than their gate voltages  in order to avoid typical clock feed-through effects. The amplitude of the incremental  update, set proportionally to [[, is controlled by the VUPD n and Vum> p gate voltage  levels, operated in the sub-threshold region. The polarity of the increment or decrement  action is determined by the control signal DECR/INCR, obtained from the polarities of  862 Cauwenberghs  the perturbed error  and the perturbation bit *ri through an exclusive-or operation. The  learning cycle is completed by activating the update by a high pulse on EN_UPD. The  next learning cycle then starts with a new random bit value for the perturbation  >0=  o.1 pF   Cstor  1 pF  I I I I  El'q UPD I I I I H  - I I I I  i i i I  (p) (p+J (p )  Figure 3 Learning cell circuitry. (a) Simplified schematics.  (b) Waveform and timing diagram.  The random bit stream 7ri  is generated on-chip by means of a set of linear feedback  shift registers [12]. For optimal performance, the perturbations need to satisfy certain  statistical orthogonality conditions, and a rigorous but elaborate method to generate a  set of uncorrelated bit streams in VLSI has been derived [13]. To preserve the scalability  of the learning architecture and the local nature of the perturbations, we have chosen a  simplified scheme which does not affect the learning performance to first order, as verified  experimentally. The array of perturbation bits, configured in a two-dimensional arrange-  ment as prompted by the location of the parameters in the network, is constructed by an  outer-product exclusive-or operation from two generating linear sets of uncorrelated row  and column bits on lines running horizontally and vertically across the network array.  In the present implementation the evaluation of the error functional (2) is performed  externally with discrete analog components, leaving some flexibility to experiment with  different formulations of error functionals that otherwise would have been hardwired. A  mean absolute difference ( = 1) norm is used for the metric distance, and the time-  averaging of the error is achieved by a fourth-order Butterworth low-pass filter. The  cut-off frequency is tuned to accommodate an AC ripple smaller than 0.1%, giving rise to  a filter settling time extending 20 periods of the training signal.  3.3 Long-Term Volatile Storage  After learning, it is desirable to retain ("freeze") the learned information, in principle  for an infinite period of time. The volatile storage of the parameter values on capacitors  undergoes a spontaneous decay due to junction leakage and other drift phenomena, and  needs to be refreshed periodically. For eight effective bits of resolution, a refresh rate of  10 Hz is sufficient. Incidentally, the charge pump used for the learning updates provides  for refresh of the parameter values as well. To that purpose, probing and multiplexing  circuitry (not shown) are added to the learning cell of Figure 3 (a) for sequential refresh.  In the experiment conducted here, the parameters are stored externally and refreshed  sequentially by activating the corresponding charge pump with a DECR/INCR bit defined  by the polarity of the observed deviation between internally probed and externally stored  A Learning Analog Neural Network Chip with Continuous-Time Recurrent Dynamics 863  values. The parameter refresh is performed in the background with a 100 msec cycle,  and does not interfere with the continuous-time network operation. A simple internal  analog storage method obliterating the need of external storage is described in [14], and  is supported by the chip architecture.  4 Learning Experiment  As a proof of principle, the network is trained with a circular target trajectory  defined by the quadrature-phase oscillator  cos (2,,ft) (6)  sin  with A - 0.8V and f = lkHz. In principle a recurrent network of two neurons  suffices to generate quadrature-phase oscillations, and the extra neurons in the  network serve to accommodate the particular amplitude and frequency requirements  and assist in reducing the nonlinear harmonic distortion.  Clearly the initial conditions for the parameter values distinguish a trivial learning  problem from a hard one, and training an arbitrarily initialized network may lead  to unpredictable results of poor generality. Incidentally, we found that the majority  of randomly initialized learning sessions fail to generate oscillatory behavior at con-  vergence, the network being trapped in a local minimum defined by a strong point  attractor. Even with strong teacher forcing these local minima persist. In contrast,  we obtained consistent and satisfactory results with the following initialization of  network parameters: strong positive diagonal connection strengths Wil = 1, zero  off-diagonal terms Wij = 0; i  j and zero thresholds 0i - 0. The positive di-  agonal connections Wii repel the neuron outputs from the point attractor at the  origin, counteracting the spontaneous decay term -xi in (1). Applying non-zero  initial values for the cross connections Wij; i  j would introduce a bias in the  dynamics due to coupling between neurons. With zero initial cross coupling, and  under strong initial teacher forcing, fairly fast and robust learning is achieved.  Figure 4 shows recorded error sequences under training of the network with the tar-  get oscillator (6), for five different sessions of 1,500 learning iterations each starting  from the above initial conditions. The learning iterations span 60 msec each, for a  total of 100 sec per session. The teacher forcing amplitude A is set initially to 3 V,  and thereafter decays logarithmically over one order of magnitude towards the end  of the sessions. Fixed values of the learning rate and the perturbation amplitude  are used throughout the sessions, with/ - 25.6 V - and a = 12.5 mV. All five ses-  sions show a rapid initial decrease in the error under stimulus of the strong teacher  forcing, and thereafter undergo a region of persistent fiat error slowly tapering off  towards convergence as the teacher forcing is gradually released. Notice that this  fiat region does not imply slow learning; instead the learning constantly removes  error as additional error is adiabatically injected by the relaxation of the teacher  forcing.  864 Cauwenberghs  I ! I I I  2.5 ( 12.5 mV  2.0  1.5  0.5  0.0  0 20 40 60 80 100  Time (set)  Figure 4 Recorded evolution of the error during learning,  for five different sessions on the network.  Near convergence, the bias in the network error due to the residual teacher forcing  becomes negligible. Figure 5 shows the network outputs and target signals at con-  vergence, with the learning halted and the parameter refresh activated, illustrating  the minor effect of the residual teacher forcing signal on the network dynamics.  The oscillogram of Figure 5 (a) is obtained under a weak teacher forcing signal,  and that of Figure 5 (b) is obtained with the same network parameters but with  the teacher forcing signal disabled. In both cases the oscilloscope is triggered on  the network output signals. Obviously, in absence of teacher forcing the network  does no longer run synchronously with the target signal. However, the discrepancy  in frequency, amplitude and shape between either of the free-running and forced  oscillatory output waveforms and the target signal waveforms is evidently small.  (a) (b)  Figure 15 Oscillograms of the network outputs and target signals after learning,  (a) under weak teacher forcing, and (b) with teacher forcing disabled.  Top traces: z(t) and z'(t). Bottom traces: z2(t) and z2'(t).  A Learning Analog Neural Network Chip with Continuous-Time Recurrent Dynamics 865  5 Conclusion  We implemented a small-size learning recurrent neural network in an analog VLSI  chip, and verified its learning performance in a continuous-time setting with a simple  dynamic test (learning of a quadrature-phase oscillator). By virtue of its scalable  architecture, with constant requirements on interconnectivity and limited global  communication, the network structure with embedded learning functions can be  freely expanded in a two-dimensional arrangement to accommodate applications of  recurrent dynamical networks requiring larger dimensionality. A present limitation  of the implemented learning model is the requirement of periodicity on the input  and target signals during the learning process, which is needed to allow a repetitive  and consistent evaluation of the network error for the parameter updates.  Acknowledgments  Fabrication of the CMOS chip was provided through the DARPA/NSF MOSIS service.  Financial support by the NIPS Foundation largely covered the expenses of attending the  conference.  References  [1] B.A. Pearlmutter, "Learning State Space Trajectories in Recurrent Neural Networks,"  Neural Computation, vol. I (2), pp 263-269, 1989.  [2] R.J. Williams and D. Zipset, "A Learning Algorithm for Continually Running Fully  Recurrent Neural Networks," Neural Computation, vol. I (2), pp 270-280, 1989.  [3] N.B. Toomarian, and J. Barhen, "Learning a Trajectory using Adjoint Functions and  Teacher Forcing," Neural Networks, vol. 5 (3), pp 473-484, 1992.  [4] G. Cauwenberghs, "A Fast Stochastic Error-Descent Algorithm for Supervised Learning  and Optimization," in Advances in Neural Information Processing Systems, San Mateo,  CA: Morgan Kaufman, vol. 5, pp 244-251, 1993.  [5] H.J. Kushner, and D.S. Clark, "Stochastic Approximation Methods for Constrained  and Unconstrained Systems," New York, NY: Springer-Verlag, 1978.  [6] M.A. Styblinski, and T.-S. Tang, "Experiments in Nonconvex Optimization: Stochastic  Approximation with Function Smoothing and Simulated Annealing," Neural Networks,  vol. 3 (4), pp 467-483, 1990.  [7] J.C. Spall, "Multivariate Stochastic Approximation Using a Simultaneous Perturbation  Gradient Approximation," IEEE Trans. Automatic Control, vol. 37 (3), pp 332-341, 1992.  [8] J. Alspector, R. Meir, B. Yuhas, and A. Jayakumar, "A Parallel Gradient Descent  Method for Learning in Analog VLSI Neural Networks," in Advances in Neural Information  Processing Systems, San Mateo, CA: Morgan Kaufman, vol. 5, pp 836-844, 1993.  [9] B. Flower and M. Jabri, "Summed Weight Neuron Perturbation: An O(n) Improve-  ment over Weight Perturbation," in Advances in Neural Information Processing Systems,  San Mateo, CA: Morgan Kaufman, vol. 5, pp 212-219, 1993.  [10] D. Kirk, D. Kerns, K. Fleischer, and A. Bart, "Analog VLSI Implementation of  Gradient Descent," in Advances in Neural Information Processing Systems, San Mateo,  CA: Morgan Kaufman, vol. 5, pp 789-796, 1993.  [11] J.W. Fattaruso, S. Kiriaki, G. Warwar, and M. de Wit, "Self-Calibration Techniques  for a Second-Order Multibit Sigma-Delta Modulator," in ISSCC Technical Digest, IEEE  Press, vol. 36, pp 228-229, 1993.  [12] S.W. Golomb, "Shift Register Sequences," San Francisco, CA: Holden-Day, 1967.  [13] J. Alspector, J.W. Gannett, S. Haber, M.B. Parker, and R. Chu, "A VLSI-Efficient  Technique for Generating Multiple Uncorrelated Noise Sources and Its Application to  Stochastic Neural Networks," IEEE T. Circuits and Systems, 38 (1), pp 109-123, 1991.  [14] G. Cauwenberghs, and A. Yariv, "Method and Apparatus for Long-Term Multi-Valued  Storage in Dynamic Analog Memory," U.S. Patent pending, filed 1993.  
Illumination-Invariant Face Recognition with a  Contrast Sensitive Silicon Retina  Joachim M. Buhmann  Rheinische Friedrich-Wilhelms-Universitfit  Institut ftir Informatik II, R6merstrage 164  D-53117 Bonn, Germany  Martin Lades  Ruhr-Universitfit Bochum  Institut ftir Neuroinformatik  D-44780 Bochum, Germany  Frank Eeckman  Lawrence Livermore National Laboratory  ISCR, P.O.Box 808, L-426  Livermore, CA 94551  Abstract  Changes in lighting conditions strongly effect the performance and reli-  ability of computer vision systems. We report face recognition results  under drastically changing lighting conditions for a computer vision sys-  tem which concurrently uses a contrast sensitive silicon retina and a con-  ventional, gain controlled CCD camera. For both input devices the face  recognition system employs an elastic matching algorithm with wavelet  based features to classify unknown faces. To assess the effect of analog  on-chip preprocessing by the silicon retina the CCD images have been  "digitally preprocessed" with a bandpass filter to adjust the power spec-  trum. The silicon retina with its ability to adjust sensitivity increases  the recognition rate up to 50 percent. These comparative experiments  demonstrate that preprocessing with an analog VLSI silicon retina gen-  erates image data enriched with object-constant features.  1 Introduction  Neural computation as an information processing paradigm promises to enhance artificial  pattern recognition systems with the learning capabilities of the cerebral cortex and with the  769  770 Buhmann, Lades, and Eeckman  adaptivity of biological sensors. Rebuilding sensory organs in silicon seems to be particu-  larly promising since their neurophysiology and neuroanatomy, including the connections  to cortex, are known in great detail. This knowledge might serve as a blueprint for the design  of artificial sensors which mimic biological perception. Analog VLSI retinas and cochleas,  as designed by Carver Mead (Mead, 1989; Mahowald, Mead, 1991) and his collaborators  in a seminal research program, will ultimately be integrated in vision and communication  systems for autonomous robots and other intelligent information processing systems.  The study reported here explores the influence of analog retinal preprocessing on the  recognition performance of a face recognition system. Face recognition is a challenging  classification task where object inherent distortions, like facial expressions and perspective  changes, have to be separated from other image variations like changing lighting conditions.  Preprocessing with a silicon retina is expected to yield an increased recognition rate since the  first layers of the retina adjust their local contrast sensitivity and thereby achieve invariance  to variations in lighting conditions.  Our face recognizer is equipped with a silicon retina as an adaptive camera. For comparison  purposes all images are registered simultaneously by a conventional CCD camera with  automatic gain control. Galleries with images of 109 different test persons each are taken  under three different lighting conditions and two different viewing directions (see Fig. 1).  These different galleries provide separate statistics to measure the sensitivity of the system  to variations in light levels or contrast and image changes due to perspective distortions.  Naturally, the performance of an object recognition system depends critically on the classifi-  cation strategy pursued to identify unknown objects in an image with the models stored in a  database. The matching algorithm selected to measure the performance enhancing effect of  retinal preprocessing deforms prototype faces in an elastic fashion (Buhmann et al., 1989;  Buhmann et al., 1990; Lades et al., 1993). Elastic matching has been shown to perform  well on the face classification task recognizing up to 80 different faces reliably (Lades et al.,  1993) and in a translation, size and rotation invariant fashion (Buhmann et al., 1990). The  face recognition algorithm was initially suggested as a simplified version of the Dynamic  Link Architecture (von der Malsburg, 1981), an innovative neural classification strategy with  fast changes in the neural connectivity during recognition stage. Our recognition results  and conclusions are expected to be qualitatively typical for a whole range of face/object  recognition systems (Turk, Pentland, 1991; Yuille, 1991; Brunelli, Poggio, 1993), since any  image preprocessing with emphasis on object constant features facilitates the search for the  correct prototype.  2 The Silicon Retina  The silicon retina used in the recognition experiments models the interactions between  receptors and horizontal cells taking place in the outer plexiform layer of the vertebrate  retina. All cells and their interconnections are explicitly represented in the chip so that  the following description simultaneously refers to both biological wetware and silicon  hardware. Receptors and horizontal cells are electrically coupled to their neighbors. The  weak electrical coupling between the receptors smoothes the image and reduces the in-  fluence of voltage offsets between adjacent receptors. The horizontal cells have a strong  lateral electrical coupling and compute a local background average. There are reciprocal  excitatory-inhibitory synapses between the receptors and the horizontal cells. The horizon-  tal cells use shunting inhibition to adjust the membrane conductance of the receptors and  Illumination-Invariant Face Recognition with a Contrast Sensitive Silicon Retina 771  thereby adjust their sensitivity locally. This feedback interaction produces an antagonistic  center/surround organization of receptive fields at the output. The center is represented  by the weakly coupled excitatory receptors and the surround by the more strongly coupled  inhibitory horizontal cells. The center/surround organization removes the average intensity  and expands the dynamic range without response compression. Furthermore, it enhances  edges.  In contrast to this architecture, a conventional CCD camera can be viewed as a very primitive  retina with only one layer of non-interacting detectors. There is no DC background removal,  causing potential over- and underexposure in parts of the image which reduces the useful  dynamic range. A mechanical iris has to be provided to adjust the mean luminance level  to the appropriate setting. Since cameras are designed for faithful image registration rather  than vision, on-chip pixel processing, if provided at all, is used to improve the camera  resolution and signal-to-noise ratio.  Three adjustable parameters allow us to fine tune the retina chip for an object recognition  experiment: (i) the diffusivity of the cones (ii) the diffusivity of the horizontal cells (iii) the  leak in the horizontal cell membrane. Changes in the diffusivities affect the shape of the  receptive fields, e.g., a large diffusivity between cones smoothes out edges and produces a  blurred image. The other extreme of large diffusivity between horizontal cells pronounces  edges and enhances the contrast gain. The retina chip has a resolution of 90 x 92 pixels,  it was designed by (Boahen, Andreou, 1992) and fabricated in 2/tm n-well technology by  MOSIS.  3 Elastic Matching Algorithm for Face Recognition  Elastic matching is a pattern classification strategy which explicitly accounts for local  distortions. A prototype template is elastically deformed to measure local deviations from a  new, unknown pattern. The amount of deformation and the similarity of local image features  provide us with a decision criterion for pattern classification. The rubbersheet-like behavior  of the prototype transformation makes elastic matching a particularly attractive method for  face recognition where ubiquitous local distortions are caused for example by perspective  changes and different facial expressions. Originally, the technique was developed for  handwritten character recognition (Burr, 1981). The version of elastic matching employed  for our face recognition experiments is based on attributed graph matching. A detailed  description with a plausible interpretation in neural networks terms is published in (Lades  et al., 1993). Each prototype face is encoded as a planar graph with feature vectors attached  to the vertices of the graph and metric information attached to the edges. The feature vectors  extract local image information at pixel :i in a multiscale fashion, i.e., they are functions  of wavelet coefficients. Each feature vector establishes a correspondence between a vertex  i of a prototype graph and a pixel gi in the image. The components of a feature vector are  defined as the magnitudes of the convolution of an image with a set of two-dimensional,  DC free Gaussian kernels centered at pixel gi. The kernels with the form  (  p; ( =  exp  2rr 2  [exp (ig)- exp (-cr2/2)]  (1)  are parameterized by the wave vector/ defining their orientations and their sizes. To  construct a self-similar set of filter functions we select eight different orientations and five  772 Burmann, Lades, and Eeckman  different scales according to  7r 2_/2 (cos(-/t), ' 7r  (v,/t) =  7r s,n(/t)) (2)  with v E {0,...,4};/t E {0,..., 7}. The multi-resolution data format represents local  distortions in a robust way, i.e., only feature vectors in the vicinity : of an image distortion  are altered by the changes. The edge labels encode metric information, in particular we  choose the difference vectors Agij ---- gi - gj as edge labels.  To generate a new prototype graph for the database, the center of a new face is determined  by matching a generic face template to it. A 7 x 10 rectangular grid with 10 pixel spacing  between vertices and edges between adjacent vertices is then centered at that point. The  saliency of image points is taken into account by deforming that generic grid so that each  vertex is moved to the nearest pixel with a local maximum in feature vector length.  The classification of an unknown face as one of the models in the database or its rejection  as an unclassified object is achieved by computing matching costs and distortion costs.  The matching costs are designed to maximize the similarity between feature vector M of  vertex i in the model graph (M) and feature vector Jr(7i) associated with pixel :i in the  new image (I). The cosine of the angle between both feature vectors  =  IIP(dll IIMII (3)  is suited as a similarity function for elastic matching since global contrast changes in images  only scale feature vectors but do not rotate them. Besides maximizing the similarity between  feature vectors the elastic matching algorithm penalizes large distortions. The distortion  cost term is weighted by a factor  which can be interpreted as a prior for expected  distortions. The combined matching cost function which is used in the face recognition  system compromises between feature similarity and distortion, i.e, it minimizes the cost  function  )2  5 5; -  (i,) '  (4)  for the model M in the face database with respect to the correspondence points {: ). (i, j)  in Eq. (4) denotes that index j runs over the neighborhood of vertex i and index i runs  over all vertices. By minimizing Eq. (4) the algorithm assigns pixel : in the new image  I to vertex i in the prototype graph M. Numerous classification experiments revealed that  a steepest descent algorithm is sufficient to minimize cost function (4) although it is non-  convex and local minima may cause non-optimal correspondences with reduced recognition  rates.  During a recognition experiment all prototype graphs in the database are matched to the  new image. A new face is classified as prototype A if H 4 is minimal and if the significance  criterion  7g = - > o. (5)  is fulfilled. The average costs {7-/) and their standard deviation EH are calculated excluding  match A. This heuristic is based on the assumption that a new face image strongly  Illumination-Invariant Face Recognition with a Contrast Sensitive Silicon Retina 773  Datacube  Figure 1: Laboratory setup of the face recognition experiments.  correlates with the correct prototype but the matching costs to all the other prototype faces  is approximately Gaussian distributed with mean (7-t) and standard deviation Zu. The  threshold parameter E} is used to limit the rate of false positive matches, i.e., to exclude  significant matches to wrong prototypes.  4 Face Recognition Results  To measure the recognition rate of the face recognition system using a silicon retina or a  CCD camera as input devices, pictures of 109 different persons are taken under 3 different  lighting conditions and 2 different viewing directions. This setup allows us to quantify the  influence of changes in lighting conditions on the recognition performance separate from  the influence of perspective distortions. Figure 2 shows face images of one person taken  under two different lighting setups. The images in Figs. 2a,c with both lights on are used  as the prototype images for the respective input devices. To test the influence of changing  lighting conditions the left light is switched off. The faces are now strongly illuminated  from the right side. The CCD camera images (Figs. 2a,b) document the drastic changes of  the light settings. The corresponding responses of the silicon retina shown in Figs. 2c,d  clearly demonstrate that the local adaptivity of the silicon retina enables the recognition  system to extract object structure from the bright and the dark side of the face. For control  purposes all recognition experiments have been repeated with filtered CCD camera images.  The filter was adjusted such that the power spectra of the retina chip images and the filtered  CCD images are identical. The images (e,f) are filtered versions of the images (a,b). It  is evident that information in the dark part of image (b) has been erased due to saturation  effects of the CCD camera and cannot be recovered by any local filtering procedure.  We first measure the performance of the silicon retina under uniform lighting conditions,  b  Figure 2: (a) Conventional CCD camera images (a,b) and silicon retina image (c,d) under  different lighting conditions. The images (e,f) are filtered CCD camera images with a  power spectrum adjusted to the images in (c,d). The images (a,c) are used to generate the  Illumination-Invariant Face Recognition with a Contrast Sensitive Silicon Retina 775  Table 1: (a) Face recognition results in a well illuminated environment and (b) in an  environment with drastic changes in lighting conditions.  f. p. rate silicon retina conv. CCD flit. CCD  100% 83.5 86.2 85.3  10% 81.7 83.5 84.4  5% 76.2 82.6 80.7  1% 71.6 79.8 75.2  100% 96.3 80.7 78.0  10% 96.3 76.2 75.2  5% 96.3 72.5 72.5  1% 93.6 64.2 62.4  a  i.e., both lamps are on and the person looks 20-30 degrees to the right. The recognition  system has to deal with perspective distortions only. A gallery of 109 faces is matched  to a face database of the same 109 persons. Table la shows that the recognition rate  reaches values between 80 and 90 percent if we accept the best match without checking  its significance. Such a decision criterion is unpractical for many applications since it  corresponds to a false positive rate (f.p. ram) of 100 percent. If we increase the threshold O  to limit false positive matches to less than 1 percent the face recognizer is able to identify  three out of four unknown faces. Filtering the CCD imagery does not hurt the recognition  performance as the third column in Table l a demonstrates. All necessary information for  recognition is preserved in the filtered CCD images.  The situation changes dramatically when we switch off the lamp on the left side of the  test person. We compare a test gallery of persons looking straight ahead, but illuminated  only from the right side, to our model gallery. Table 1 b summarizes the recognition results  for different false positive rates. The advantage of using a silicon retina are 20 to 45  percent higher recognition rates than for a system with a CCD camera. For a false positive  rate below one percent a silicon retina based recognition system identifies two third more  persons than a conventional system. Filtering does not improve the recognition rate of a  system that uses a CCD camera as can be seen in the third column.  Our comparative face recognition experiment clearly demonstrates that a face recognizer  with a retina chip is performing substantially better than conventional CCD camera based  systems in environments with uncontrolled, substantially changing lighting conditions.  Retina-like preprocessing yields increased recognition rates and increased significance  levels. We expect even larger discrepancies in recognition rams if object without a bilateral  symmetry have to be classified. In this sense the face recognition task does not optimally  explore the potential of adaptive preprocessing by a silicon retina. Imagine an object  recognition task where the most significant features for discrimination are hardly visible  or highly ambiguous due to poor illumination. High error rams and very low significance  levels are an inevitable consequence of such lighting conditions.  The limited resolution and poor signal-to-noise ratio of silicon retina chips are expected to  be improved by a new generation of chips fabricated in 0.7/tm CMOS technology with a  776 Buhmann, Lades, and Eeckman  potential resolution of 256 x 256 pixels. Lighting conditions as simulated in our recognition  experiment are ubiquitous in natural environments. Autonomous robots and vehicles or  surveillance systems are expected to benefit from the silicon retina technology by gaining  robustness and reliability. Silicon retinas and more elaborate analog VLSI chips for low  level vision are expected to be an important component of an Adaptive Vision System.  Acknowledgement: It is a pleasure to thank K. A. Boahen for providing us with the  retina chips. We acknowledge stimulating discussions with C. von der Malsburg and C.  Mead. This work was supported by the German Ministry of Science and Technology  (ITR-8800-H 1) and by the Lawrence Livermore National Laboratory (W-7405-Eng-48).  References  Boahen, K., Andreou, A. 1992. A Contrast Sensitive Silicon Retina with Reciprocal  Synapses. Pages 764-772 of.' NIPS91 Proceedings. IEEE.  Brunelli, R., Poggio, T. (1993). Face Recognition: Features versus Templates. IEEE Trans.  on Pattern Analysis Machine Intelligence, 15, 1042-1052.  Buhmann, J., Lange, J., von der Malsburg, C. 1989. Distortion Invariant Object Recognition  by Matching Hierarchically Labeled Graphs. Pages I 155-159 of.' Proc. IJCNN,  Washington. IEEE.  Buhmann, J., Lades, M., von der Malsburg, C. 1990. Size and Distortion Invariant Object  Recognition by Hierarchical Graph Matching. Pages H 411-416 of.' Proc. IJCNN,  SanDiego. IEEE.  Burr, D. J. (1981). Elastic Matching of Line Drawings. IEEE Trans. on Pat. An. Mach.  Intel., 3, 708-713.  Lades, M., Vorbriiggen, J.C., Buhmann, J., Lange, J., von der Malsburg, C., Wfirtz, R.P.,  Konen, W. (1993). Distortion Invariant Object Recognition in the Dynamic Link  Architecture. IEEE Transactions on Computers, 42, 300-311.  Mahowald, M., Mead, C. (1991). The Silicon Retina. Scientific American, 264(5), 76.  Mead, C. (1989). Analog VLSI and Neural Systems. New York: Addison Wesley.  Turk, M., Pentland, A. (1991). Eigenfaces for Recognition. J. Cog. Sci., 3, 71-86.  von der Malsburg, Christoph. 1981. The Correlation Theory of Brain Function. Internal  Report. Max-Planck-Institut, Biophys. Chem., G6ttingen, Germany.  Yuille, A. (1991). Deformable Templates for Face Recognition. J. Cog. Sci., 3, 60-70.  
Dynamic Modulation of Neurons and Networks  Eve Marder  Center for Complex Systems  Brandeis University  Waltham, MA 02254 USA  Abstract  Biological neurons have a variety of intrinsic properties because of the  large number of voltage dependent currents that control their activity.  Neuromodulatory substances modify both the balance of conductances  that determine intrinsic properties and the strength of synapses. These  mechanisms alter circuit dynamics, and suggest that functional circuits  exist only in the modulatory environment in which they operate.  1 INTRODUCTION  Many studies of artificial neural networks employ model neurons and synapses that are  considerably simpler than their biological counterparts. A variety of motivations underly  the use of simple models for neurons and synapses in artificial neural networks. Here,  I discuss some of the properties of biological neurons and networks that are lost in overly  simplified models of neurons and synapses. A fundamental principle in biological nervous  systems is that neurons and networks operate over a wide range of time scales, and that  these are modified by neuromodulatory substances. The flexible, multiple time scales in  the nervous system allow smooth transitions between different modes of circuit operation.  2 NEURONS HAVE DIFFERENT INTRINSIC PROPERTIES  Each neuron has complex dynamical properties that depend on the number and kind of  ion channels in its membrane. Ion channels have characteristic kinetics and voltage  511  512 Marder  dependencies that depend on the sequence of amino acids of the protein. Ion channels  may open and close in several milliseconds; others may stay open for hundreds of  milliseconds or several seconds.  Some neurons are silent unless they receive synaptic inputs. Silent neurons can be  activated by depolarizing synaptic inputs, and many will fire on rebound from a  hyperpolarizing input (postinhibitory rebound). Some neurons are tonically active in the  absence of synaptic inputs, and synaptic inputs will increase or decrease their firing rate.  Some neurons display rhythmic bursts of action potentials. These bursting neurons can  display stable patterns of oscillatory activity, that respond to perturbing stimuli with  behavior characteristic of oscillators, in that their period can be stably reset and  entrained. Bursting neurons display a number of different voltage and time dependent  conductances that interact to produce slow membrane potential oscillations with rapid  action potentials riding on the depolarized phase. In a neuron such as R15 of Aplysia  (Adams and Levitan 1985) or the AB neuron of the stomatogastric ganglion (STG)  (Harris-Warrick and Flarnm 1987), the time scale of the burst is in the second range, but  the individual action potentials are produced in the 5-10mc time scale.  Neurons can generate bursts by combining a variety of different conductances. The  particular balance of these conductantes can have significant impact on the oscillator's  behavior (Epstein and Marder 1990; Kepler et al 1990; Skinner et al 1993), and therefore  the choice of oscillator model to use must be made with care (Somers and Kopell 1993).  Some neurons have a balance of conductances that give them bistable membrane  potentials, allowing to produce plateau potentials. Typically, such neurons have two  relatively stable states, a hyperpolarized silent state, and a sustained depolarized state in  which they fire action potentials. The transition between these two modes of activity can  be made with a short depolarizing or hyperpolarizing pulse (Fig. 1). Plateau potentials,  like fiip-fiops" in electronics, are a "short-term memory" mechanism for neural circuits.  The intrinsic properties of neurons can be modified by sustained changes in membrane  potential. Because the intrinsic properties of neurons depend on the balance of  conductantes that activate and inactivate in different membrane potential ranges and over  a variety of time scales, hyperpolarization or depolarization can switch a neuron between  modes of intrinsic activity (Llin,'is 1988; McCormick 1991; Leresche et al 1991).  An interesting "memory-like"effect is produced by the slow inactivation properties of  some K + currents (McCormick 1991; Storm 1987). In cells with such currents a  sustained depolarization can "mplify"a synaptic input from subthreshold to  suprathreshold, as the sustained depolarization causes the K + current to inactivate  (Marom and Abbott 1994; Turrigiano, Marder and Abbott in preparation). This is another  "hort-term memory"mechanism that does not depend on changes in synaptic efficacy.  Dynamic Modulation of Neurons and Networks 513  A. CONTROL B 10'7M TNRNFLRFNH:,  lse .Sse  Figure 1: Intracellular recording from the DG neuron of the crab STG. A: control saline,  a depolarizing current pulse elicits action potentials for its duration. B: In  SDRNFLRFamide, a short depoIarization elicits a plateau potential that lasts until a short  hyperpolarizing current pulse terminates it. Modified from Weimann et al 1993.  2 INTRINSIC MEMBRANE PROPERTIES ARE MODULATED  Biological nervous systems use many substances as neurotransmitters and  neuromodulators. The effects of these substances include opening of rapid, relatively non-  voltage dependent ion channels, such as those mediating conventional rapid synaptic  potentials. Alternatively, modulatory substances can change the number or type of  voltage-dependent conductances displayed by a neuron, and in so doing dramatically  modify the intrinsic properties of a neuron. In Fig. 1, a peptide, SDRNFLRFamide  transforms the DG neuron of the crab STG from a state in which it fires only during a  alepolarizing pulse to one in which it displays long-lasting plateau properties (Weimann  et al 1993). The salient feature here is that modulatory substances can elicit slow  membrane properties not otherwise expressed.  3 SYNAPTIC STRENGTH IS MODULATED  In most neural network models synaptic weights are modified by learning rules, but are  not dependent on the temporal pattern of presynaptic activity. In contrast, in many  biological synapses the amount of transmitter released depends on the frequency of firing  of the presynaptic neuron. Facilitation, the increase in the amplitude of the postsynaptic  current when' the presynaptic neuron is activated several times in quick succession is quite  common. Other synapses show depression. The same neuron may show facilitation at  some of its terminals while showing depression at others (Katz et al 1993). The  facilitation and depression properties of any given synapse can not be deduced on first  principles, but must be determined empirically.  Synaptic efficacy is often modified by modulatory substances. A dramatic example is seen  in the Aplysia gill withdrawal reflex, where serotonin significantly enhances the amplitude  of the monosynaptic connection from the sensory to motor neurons (Clark and Kandel  1993; Emptage and Carew 1993). The effects of modulatory substances can occur on  different branches on a neuron independently (Clark and Kandel 1993), and the same  modulatory substance may have different actions at different sites of the same neuron.  514 Marder  Electrical synapses are also subject to neuromodulation (Dowling, 1989). For example,  in the retina dopamine reversibly uncouples horizonal cells.  Modulation of synaptic strength can be quite extreme; in some cases synaptic contacts  may be virtually invisible in some modulatory environments, while strong in others. The  implications of this for circuit operation will be discussed below.  Hormones Neuromodulators  [] DA [] ACh [] AST  : 5-HT [] DA [] Buc  [] Oct ;J GABA [] cCCK  [] CC [] Oct  IornTK  [] cCCK [] Momod  [] IomTK  Pr  [] RPCH [] RPCH  [] SDRN   RN  aln  Sensory Transmitters  I ACh  AST []  Figure 2: Modulatory substances found in inputs to the STG. See Harris-Warrick et aL,  1992 for details. Figure courtesy of P. Skiebe.  4 TRANSMITTERS ARE COLOCALIZED IN NEURONS  The time course of a synaptic potential evoked by a neurotransmitter or modulator is a  characteristic property of the ion channels gated by the transmitter and/or the second  messenger system activated by the signalling molecule. Synaptic currents can be relatively  fast, such as the rapid action of ACh at the vertebrate skeletal neuromuscular junction  where the synaptic currents decay in several milliseconds. Alternatively, second  messenger activated synaptic events may have durations lasting hundreds of milliseconds,  seconds, or even minutes. Many neurons contain several different neurotransmitters.  It is common to find a small molecule such as glutamate or GABA colocalized with an  amine such as serotonin or histamine and one or more neuropeptides. To describe the  synaptic actions of such neurons, it is necessary to determine for each' signalling molecule  how its release depends on the frequency and pattern of activity in the presynaptic  Dynamic Modulation of Neurons and Networks 515  terminal, and characterize its postsynaptic actions. This is important, because different  mixtures of cotransmitters, and consequently of postsynaptic action may occur with  different presynaptic patterns of activity.  5 NEURAL NETWORKS ARE MULTIPLY MODULATED  Neural networks are controlled by many modulatory inputs and substances. Figure 2  illustrates the patterns of modulatory control to the crustacean stomatogastric nervous  system, where the motor patterns produced by the only 30 neurons of the stomatogastric  ganglion are controlled by about 60 input fibers (Coleman et al 1992) that contain at least  15 different substances, including a variety of amines, amino acids, and neuropeptides  (Marder and Weimann 1992; Harris-Warrick et al 1992). Each of these  modulatory substances produces characteristic and different effects on the motor patterns  of the STG (Figs. 3,4). This can be understood if one remembers that the intrinsic  membrane properties as well as the strengths of the synaptic connections within this  group of neurons are all subject to modulation. Because each cell has many conductantes,  many of which are subject to modulation, and because of the large number of synaptic  connections, the modes of circuit operation are theoretically large.  6 CIRCUIT RECONFIGURATION BY MODULATORY CONTROL  Figure 3 illustrates that modulatory substances can tune the operation of a single  functional circuit. However, neuromodulatory substances can also produce far more  extensive changes in the functional organization of neuronal networks. Recent work on  the STG demonstrates that sensory and modulatory neurons and substances can cause  neurons to switch between different functional circuits, so that the same neuron is part  of several different pattern generating circuits at different times (Hooper and Moulins  1989; Dickinson et al 1990; Weimann et al 1991; Meyrand et al 1991; Heinzel et al  1993).  In the example shown in Fig. 4, in control saline the LG neuron is firing in time with the  fast pyloric rhythm (the LP neuron is also firing in pyloric time), but there is no ongoing  gastric rhythm. When the gastric rhythm was activated by application of the peptide  SDRNFLRFNm, the LG neuron fired in time with the gastric rhythm (Weimann et al  1993). These and other data lead us to conclude that it is the modulatory environment  that constructs the functional circuit that produces a given behavior (Meyrand et al  1991). Thus, by tuning intrinsic membrane properties and synaptic strengths,  neuromodulatory agents can recombine the same neurons into a variety of circuits,  capable of generating remarkably distinct outputs.  Acknowledgements  I thank Dr. Petra Skiebe for Fig 3 art work. Research was supported by NS17813.  516 Marder  CONTROL  PD  PILOCARPINE SEROTONIN  '.g I,1 i,h l. hL, lu [,,I, .,., ![i 1!I;,111, ,.,,,,,.,, !. I,  LP  PD  Ivn  SDRNFLRFNH 2  PROCTOLIN  CCAP  Figure 3: Different forms of the pyloric rhythm different modulators. Each panel, the top  two traces: simulataneous intracellular recordings from LP and PD neurons of crab STG;  bottom trace: extracellular recording, lvn nerve. Control, rhythmic pyloric activity  absent. Substances were bath applied, the pyloric patterns produced were different.  Modified from Marder and Weimann 1992.  DG - -  dgn  LG  VD  Figure 4: Neurons switch between different pattern-genreating circuits. Left panel, the  gastric rhythm not active (monitored by DG neuron), LG neuron in time with the pyloric  rhythm (seen as activity in LP neuron). Right panel, gastric rhythm activated by  SDRNFLRFamide, monitored by the DG neuron bursts recorded on the dgn. LG now  fired in alternation with DG neuron. Pyloric time is seen as the interruptions in the  activity of the VD neuron. Modified from Marder and Weimann 1992.  Dynamic Modulation of Neurons and Networks 517  References  Adams WB and Levitan IB 1985 Voltage and ion dependencies of the slow currents  which mediate bursting in Aplysia neurone R5. J Physiol 360 69-93  Clark GA, Kandel ER 1993 Induction of long-term facilitation inAplysia sensory neurons  by local application of serotonin to remote synapses. Proc Natl Acad Sci USA  90:1141-11415  Coleman MJ, Nusbaum MP, Coumil I, Claiborne BJ 1992 Distribution of mopdulatory  inputs to the stomatogastric ganglion of the crab, Cancer borealis. J Comp Neur  325:581-594  Dickinson PS, Mecsas C, Marder E 1990 Neuropeptide fusion of two motor-pattern  generator circuits. Nature 344:155-158  Dowling JE 1989 Neuromodulation in the retina: the role of dopamine. Sem Neur 1:35-  43  Eruptage NJ and Carew TJ 1993 Long-term synaptic facilitation in the absence of short-  term facilitation in Aplysia neurons. Science 262 253-256  Epstein IR, Marder E 1990 Multiple modes of a conditional neural oscillator. Biol  Cybern 63:25-34  Harris-Warrick RM, Flamm RE 1987 Multiple mechanisms of bursting in a conditional  bursting neuron. J Neurosci 7:2113-2128  Harris-Warrick RM, Marder E, Selverston AI, Moulins Meds 1992 Dynamic Biological  Networks: The Stomatogastric Nervous System. MIT Press Cambridge  Heinzel H-G, Weimann JM, Marder E 1993 The behavioral repertoire of the gastric mill  in the crab, Cancer pagurus: An in vivo endoscopic and electrophysiological  examination. J Neurosci 13:1793-1803  Hooper SL, Moulins M 1989 Switching of a neuron from one network to another by  sensory-induced changes in membrane properties. Science 244:1587-1589  Katz PS, Kirk MD, and Govind CK 1993 Facilitation and depression at different  branches of the same motor axon: evidence for presynaptic differences in  release. J Neurosci 13:3075-3089  Kepler TB, Marder E, Abbott LF 1990 The effect of electrical coupling on the frequency  of model neuronal oscillators. Science 248:83-85  Leresche N, Lightowler S, Soltesz I, Jassik-Gerschenfeld D, and Crunelli V 1991 Low-  frequency oscillatory activities intrinsic to rat and car thalamocortical cells. J  Physiol 441 155-174  Llin RR 1988 The intrinsic electrophysiological properties of mammalian neurons:  insights into central nervous system function. Science 242 1654-1664  McCormick DA 1991 Functional properties of slowly inactivating potassium current in  guinea pig dorsal lateral geniculate relay neurons. J Physiol 66 1176-1189  Marom S and Abbott LF 1994 Modeling state-dependent inactivation of membrane  currents. Biophysical J in press  Marder E, Weimann JM 1992 Modulatory control of multiple task processing in the  stomatogastric nervous system. IN: Neurobioiogy of Motor Programme  Selection: new approaches to mechanisms of behavioral choice, Kien J,  518 Marder  McCrohan C, Winlow Weds Pergamon Press Oxford  Meyrand P, Simmers J, Moulins, M 1991 Construction of a pattern-generating circuit  with neurons of different networks. Nature 351:60-63  Skinner FK, Turdgiano GG, Marder E 1993 Frequency and burst duration in oscillating  neurons and two cell networks. Biol Cybern 69:375-383  Somers D, Kopell N 1993 Rapid synchronization through fast threshold modulation. Biol  Cybern 68:393407  Storm JF 1987 Temporal integration by a slowly inactivating K + current in hippocampal  neurons. Nature 336:379-381  Weimann JM, Meyrand P, Marder E 1991 Neurons that form multiple pattern  generators: Identification and multiple activity patterns of gastric/pyloric neurons  in the crab stomatogastric system. J Neurophysiol 65:111-122  Weimann JM, Marder E, Evans B, Calabrese RL 1993 The effects of SDRNFLRFsm  and TNRNFLRFsm on the motor patterns of the stomatogastric ganglion of the  crab, Cancer borealis. J Exp Biol 181:1-26  
Classifying Hand Gestures with a View-based  Distributed Representation  Trevor J. Darrell  Perceptual Computing Group  MIT Media Lab  Alex P. Pentland  Perceptual Computing Group  MIT Media Lab  Abstract  We present a method for learning, tracking, and recognizing human hand  gestures recorded by a conventional CCD camera without any special  gloves or other sensors. A view-based representation is used to model  aspects of the hand relevant to the trained gestures, and is found using an  unsupervised clustering technique. We use normalized correlation net-  works, with dynamic time warping in the temporal domain, as a distance  function for unsupervised clustering. Views are computed separably for  space and time dimensions; the distributed response of the combination  of these units characterizes the input data with a low dimensional repre-  sentation. A supervised classification stage uses labeled outputs of the  spatio-temporal units as training data. Our system can correctly classify  gestures in real time with a low-cost image processing accelerator.  1 INTRODUCTION  Gesture recognition is an important aspect of human interaction, either interpersonally or  in the context of man-machine interfaces. In general, there are many facets to the "gesture  recognition" problem. Gestures can be made by hands, faces, or one's entire body; they  can be static or dynamic, person-specific or cross-cultural. Here we focus on a subset of  the general task, and develop a method for interpreting dynamic hand gestures generated  by a specific user. We pose the problem as one of spotting instances of a set of known  (previously trained) gestures. In this context, a gesture can be thought of as a set of hand  views observed over time, or simply as a sequence of images of hands over time. These  images may occur at different temporal rates, and the hand may have different spatial  945  946 Darrell and Pentland  offset or gross illumination condition. We would like to achieve real- or near real-time  performance with our system, so that it can be used interactively by users.  To achieve this level of performance, we take advantage of the principle of using only  as much "representation" as needed to perform the task. Hands are complex, 3D articu-  lated structures, whose kinematics and dynamics are difficult to fully model. Instead of  performing explicit model-based reconstruction, and attempting to extract these 3D model  parameters (for example see [4, 5, 6]), we use a simpler approach which uses a set of 2D  views to represent the object. Using this approach we can perform recognition on objects  which are either too difficult to model or for which a model recovery method is not feasible.  As we shall see below, the view-based approach affords several advantages, such as the  ability to form a sparse representation that only models the poses of the hands that are  relevant to the desired recognition tasks, and the ability to learn the relevant model directly  from the data using unsupervised clustering.  2 VIEW-BASED REPRESENTATION  Our task is to recognize spatio-temporal sequences of hand images. To reduce the dimen-  sionality of the matching involved, we find a set of view images and a matching function  such that the set of match scores of a new image with the view images is adequate for recog-  nition. The matching function we use is the normalized correlation between the image and  the set of learned spatial views.  Each view represents a different pose of the object being tracked or recognized. We  construct a set of views that "spans" the set of images seen in the training sequences, in  the sense that at least one view matches every frame in the sequence (given a distance  metric and threshold value). We can then use the view with the maximum score (minimum  distance) to localize the position of the object during gesture performance, and use the  ensemble response of the view units (at the location of maximal response) to characterize  the actual pose of the object. Each model is based on one or more example images of a  view of an object, from which mean and variance statistics about each pixel in the view are  computed.  The general idea of view-based representation has been advocated by Ullman [12] and  Poggio [9] for representing 3-D objects by interpolating between a small set of 2-D views.  Recognition using views was analyzed by Breuel, who established bounds on the number  of views needed for a given error rate [3]. However the view-based models used in these  approaches rely on a feature-based representation of an image, in which a "view" is the  list of vertex locations of semantically relevant features. The automatic extraction of these  features is not a fully solved problem. (See [2] for a nearly automated system of finding  corresponding points and extracting views.)  Most similar to our work is that of Murase and Nayar[8] and Turk[11] which use low-  order eigenvectors to reduce the dimensionality of the signal and perform recognition. Our  work differs from theirs in that we use normalized-correlation model images instead of  eigenfunctions and can thus localize the hand position more directly, and we extend into  the temporal domain, recognizing image sequences of gestures rather than static poses.  A particular view model will have a range of parameter values of a given transformation  (e.g., rotation, scale, articulation) for which the correlation score shows a roughly convex  "tuning curve". If we have a set of view models which sample the transformation parameter  Classifying Hand Gestures with a View-Based Distributed Representation 947  (a)  (b)  (c)  (d)  Figure 1: (a) Three views of an eyeball: +30, 0, and -30 of gaze angle. (a) Normalized  correlation scores of the +30 degree view model when tracking a eyeball rotating from  approximately -30 to +30 degrees of gaze angle. (b) Score for 0 degree view model. (c)  Score for -30 degree model.  finely enough, it is possible to infer the actual transform parameters for new views by  examining the set of model correlation scores. For example, Figure I a shows three views  of an eyeball that could be used for gaze tracking; one looking 30 degrees left, one looking  center-on, and one looking 30 degrees to the right. The three views span a -t-30 degree  subspace of the gaze direction parameter. Figure 1 (b,c,d) shows the normalized correlation  score for each view model when tracking a rotating eyeball. Since the tuning curves  produced by these models are fairly broad with respect to gaze angle, one could interpolate  from their responses to obtain a good estimate of the true angle.  When objects are non-rigid, either constructed out of flexible materials or an articulated  collection of rigid parts (like a hand), then the dimensionality of the space of possible  views becomes much larger. Full coverage of the view space in these cases is usually  not possible since enumerating it even with very coarse sampling would be prohibitively  expensive in terms of storage and search computation required. However, many parts of a  high dimensional view space may never be encountered when processing real sequences,  due to unforeseen additional constraints. These may be physical (some joints may not  be completely independent), or behavioral (some views may never be used in the actual  communication between user and machine). A major advantage of our adaptive scheme is  that it has no difficulty with sparse view spaces, and derives from the data which regions of  the space are full.  948 Darrell and Pentland  Figure 2: (a) Models automatically acquired from a sequence of images of a rotating box.  (b) Normalized correlation scores for each model as a function of image sequence frame  number.  3 UNSUPERVISED LEARNING OF VIEW UNITS  To derive a set of new view models, we use a simple form of unsupervised clustering  in which the first example forms a new view, and subsequent examples that are below a  distance threshold are merged into the nearest existing view. A new view is created when  an example is below the threshold distance for all views in the current set, but is above a  base threshold which establishes that the object is still (roughly) being tracked. Over time,  this "follow-the-leader" algorithm results in a family of view models that sample the space  of object poses in the training data. This method is similar to those commonly used in  vector quantization [7]. Variance statistics are updated for each model pixel, and can be  used to exclude unreliable points from the correlation computation.  For simple objects and transformations, this adaptive scheme can build a model which  adequately covers the entire space of possible views. For example, for a convex rigid body  undergoing a 1D rotation with fixed relative illumination, a relatively small number of view  models can suffice to track and interpolate the position of the object at any rotation. Figures  2 illustrates this with a simple example of a rotating box. The adaptive tracking scheme  was run with a camera viewing a box rotating about a fixed axis. Figure 2a shows the view  models in use when the algorithm converged, and all possible rotations were matched with  score greater than 01. To demonstrate the tuning properties of each model under rotation,  Figure 2b shows the correlation scores for each model plotted as a function of input frame  Classifying Hand Gestures with a View-Based Distributed Representation 949  ::: :' ",!'.. -::::.<::.:  :-': ... :4':'.  -.: '?  Figure 3: Four spatial views found by unsupervised clustering method on sequence con-  taining two hand-waving gestures: side-to-side and up-down.  I  I  I  spatial  views  temporal  views  Figure 4: Overview of unsupervised clustering stage to learn spatial and temporal views. An  input image sequence is reduced to sequence of feature vectors which record the maximum  value in a normalized correlation network corresponding to each spatial view. A similar  process using temporal views reduces the spatial feature vectors to a single spatio-temporal  feature vector.  number of a demonstration sequence. In this sequence the box was held fixed at its initial  position for the first 5 frames, and then rotated continuously from 0 to 340 degrees. The  responses of each model are broadly tuned as a function of object angle, with a small  number of models sufficing to represent/interpolate the object at all rotations (at least about  a single axis).  We ran our spatial clustering method on images of hands performing two different "waving"  gestures. One gesture was a side-to-side wave, with the fingers rigid, and the other was  an up-down wave, with the wrist held fixed and the fingers bending towards the camera  in synchrony. Running instances of both through our view learning method, with a base  threshold of 00=0.6 and a "new model" threshold of 01 - 0.7, the clustering method found  4 four spatial templates to span all of the images in the both sequences Figure 3 shows the  pixel values for these four models.  950 Darrell and Pentland  Figure 5: Surface plot of temporal templates found by unsupervised clustering method on  sequences of two hand-waving gestures. Vertical axis is score, horizontal axis is time, and  depth axis is spatial view index.  3.1 TEMPORAL VIEWS  The previous sections provide a method for finding spatial views to reduce the dimen-  sionality in a tracking task. The same method can be applied in the temporal domain as  well, using a set of "temporal views". Figure 4 shows an overview of these two stages.  We construct temporal views using a similar method to that used for spatial views, but  with temporal segmentation cues provided by the user. Sequences of spatial-feature vector  outputs (the normalized correlation scores of the spatial views) are passed as input to the  unsupervised clustering method, yielding a set of temporal views. To find the distance  between two sequences, we again use a normalized correlation metric, with Dynamic Time  Warping (DTW) method [1, 10]. This allows the time course of a gesture to vary, as long  as the same series of spatial poses is present.  In this way a set of temporal views acting on spatial views which in turn act on image  intensities, is created. The responses of these composite views yield a single spatio-temporal  stimulus vector which describes spatial and temporal properties of the input signal. As an  example, for the "hand-waving" example shown above, two temporal views were found by  the clustering method. These are shown as surface plots in Figure 5. Empirically we have  found that the spatio-temporal units capture the salient aspects of the spatial and temporal  variation of the hand gestures in a low-dimensional representation, so efficient classification  is possible. The response of these temporal view units on an input sequence containing  three instances of each gesture is shown in Figure 6.  4 CLASSIFICATION OF GESTURES  The spatio-temporal units obtained by the unsupervised procedure described above are used  as inputs to a supervised learning/classification stage (Figure 7(a)). We have implemented  two different classification strategies, a traditional Diagonal Gaussian Classifier, and a  multi-layer perceptron.  Classifying Hand Gestures with a View-Based Distributed Representation 951  Figure 6: (a) surface plot of spatial view responses on input sequence containing three  instances of each hand-waving gesture. (b) final spatio-temporal view unit response: the  time-warped, normalized correlation score of temporal views on spatial view feature vectors.  As an experiment, we collected 42 examples of a "hello" gesture, 26 examples of "good-  bye" and 10 examples of other gestures intended to generate false alarms in the classifier.  All gestures were performed by a single user under similar imaging conditions. For each  trial we randomly selected half of the target gestures to train the classifier, and tested on the  remaining half. (All of the conflictor gestures were used in both training and testing sets  since they were few in number.)  Figure 7(b) summarizes the results for the different classification strategies. The Gaussian  classifier (DG) achieved an hit rate of 67%, with zero false alarms. The multi-layer  perceptron (MLP) was more powerful but less conservative, with a hit rate of 86% and a  false alarm rate of 5%. We found the results of the MLP classifier to be quite variable;  on many of the trials the classifier was stuck in a local minima and failed to converge on  the test set. Additionally there was considerable dependence on the number of units in  the hidden layer; empirically we found 12 gave best performance. Nonetheless, the MLP  classifier provided good performance. When we excluded the trials on which the classifier  failed to converge on the training set, the performance increased to 91% hit rate, 2% false  alarm rate.  5 CONCLUSION  We have demonstrated a system for tracking and recognition of simple hand gestures. Our  entire recognition system, including time-warping and classification, runs in real time (over  10Hz). This is made possible through the use of a special purpose normalized correlation  search co-processor. Since the dimensionality of the feature space is low, the dynamic time  warping and classifications steps can be implemented on conventional workstations and  still achieve real-time performance. Because of this real-time performance, our system is  952 Darrell and Pentland  ST unit  outputs  I training  data  ICLASSlFIER  "hello"  "bye"  ::::::::::::::::::::::::::::::::::::::: i!$i!.9j::[::::::i:i:i:ii!!!::i: ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::  ::: :::: ::?-  ,-c.,- - ..-.!:.:!8!::?.: ::: S:.  .,----- ,...., .....  ...........  ...:'.'..' .......... :,;..?,  :?g ***:*::::'*: :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::  Figure 7: Overview of supervised classification stage and results obtained for different  types of classifiers.  directly applicable to interactive "glove-free" gestural user interfaces.  References  [lO]  [11]  [12]  [1] Bellman, R. E., (1957) Dynamic Programming. Princeton, NJ: Princeton Univ. Press.  [2] Beymer, D., Shashua, A., and Poggio, T, (1993) "Example Based Image Analysis  and Synthesis", MIT AI Lab Memo No. 1431  [3] Breuel, T, (1992) "View-based Recognition", IAPR Workshop on Machine Vision  Applications.  [4] Cipolla, R., Okamotot, Y., and Kuno, Y., (1992) "Qualitative visual interpretation  of 3D hand gestures using motion parallax", IAPR Workshop on Machine Vision  Applications.  [5] Fukumoto, M., Mase, K., and Suenaga, Y., (1992) "Real-Time Detection of Pointing  Actions for a Glove-Free Interface", IAPR Workshop on Machine Vision Applications.  [6] Ishibuchi, K., Takemura, H., and Kishino, F., "Real-Time Hand Shape Recognition  using Pipe-line Image Processor", (1992) IEEE Workshop on Robot and Human  Communication, pp. 111-116.  [7] Makhoul, J., Roucos, S., and Gish, H., (1985)"Vector Quantization in Speech Coding"  Proc. IEEE, Vol. 73, No. 11, pp. 1551-1587.  [8] Murase, H.,and Nayar, S. K., (1993) "Learning and Recognition of 3D Objects from  Appearance", Proc. IEEE Qualitative Vision Workshop, New York City, pp. 39-49.  [9] Poggio, T., and Edelman, S., (1990) "A Network that Learns to Recognize Three  Dimensional Objects," Nature, Vol. 343, No. 6255, pp. 263-266.  Sakoe, H., and Chiba, S., (1980) "Dynamic Programming optimization for spoken  word recognition", IEEE Trans. ASSP, Vol. 26, pp. 623-625.  Turk, M., and Pentland, A. P., (1991) "Eigenfaces for Recognition", Journal of  Cognitive Neuroscience, vol. 3, pp. 71-89.  Ullman, S., and Basri, R., (1991)"Recognition by Linear Combinations of Models,"  IEEE PAMI, Vol. 13, No. 10, pp. 992-1007.  
Hoeffding Races: Accelerating Model  Selection Search for Classification and  Function Approximation  Oded Maron  Artificial Intelligence Laboratory  Massachusetts Institute of Technology  Cambridge, MA 02139  Andrew W. Moore  lobotics Institute  School of Computer Science  Carnegie Mellon University  Pittsburgh, PA 15213  Abstract  Selecting a good model of a set of input points by cross validation  is a computationally intensive process, especially if the number of  possible models or the number of training points is high. Tech-  niques such as gradient descent are helpful in searching through  the space of models, but problems such as local minima, and more  importantly, lack of a distance metric between various models re-  duce the applicability of these search methods. Hoeffding Races is  a technique for finding a good model for the data by quickly dis-  carding bad models, and concentrating the computational effort at  differentiating between the better ones. This paper focuses on the  special case of leave-one-out cross validation applied to memory-  based learning algorithms, but we also argue that it is applicable  to any class of model selection problems.  1 Introduction  Model selection addresses "high level" decisions about how best to tune learning  algorithm architectures for particular tasks. Such decisions include which function  approximator to use, how to trade smoothness for goodness of fit and which fea-  tures are relevant. The problem of automatically selecting a good model has been  variously described as fitting a curve, learning a function, or trying to predict future  60 Maron and Moore  0.22  0.2  0.18  0.16  0.14  o. x2  1 3 5 7 9  k Nearest Neighbors Used  Figure 1: A space of models consisting of local-weighted-regression models with  different numbers of nearest neighbors used. The global minimum is at one-nearest-  neighbor, but a gradient descent algorithm would get stuck in local minima unless  it happened to start in in a model where k < 4.  instances of the problem. One can think of this as a search through the space of  possible models with some criterion of "goodness" such as prediction accuracy, com-  plexity of the model, or smoothness. In this paper, this criterion will be prediction  accuracy. Let us examine two common ways of measuring accuracy: using a test  set and leave-one-out cross validation (Wahba and Wold, 1975).   The test set method arbitrarily divides the data into a training set and a  test set. The learner is trained on the training set, and is then queried with  just the input vectors of the test set. The error for a particular point is the  difference between the learner's prediction and the actual output vector.   Leave-one-out cross validation trains the learner N times (where N is  the number of points), each time omitting a different point. We attempt to  predict each omitted point. The error for a particular point is the difference  between the learner's prediction and the actual output vector.  The total error of either method is computed by averaging all the error instances.  The obvious method of searching through a space of models, the brute force ap-  proach, finds the accuracy of every model and picks the best one. The time to find  the accuracy (error rate) of a particular model is proportional to the size of the test  set ITESTI, or the size of the training set in the case of cross validation. Suppose  that the model space is discretized into a finite number of models IMODELSI --  then the amount of work required is O(IMODELS I x [TESTI) , which is expensive.  A popular way of dealing with this problem is gradient descent. This method can  be applied to find the parameters (or weights) of a model. However, it cannot be  used to find the structure (or architecture) of the model. There are two reasons for  Hoeffding Races: Accelerating Model Selection 61  this. First, we have empirically noted many occasions on which the search space is  peppered with local minima (Figure 1). Second, at the highest level we are selecting  from a set of entirely distinct models, with no numeric parameters over which to  hill-climb. For example, is a neural net with 100 hidden units closer to a neural net  with 50 hiden units or to a memory-based model which uses 3 nearest neighbors?  There is no viable answer to this question since we cannot impose a viable metric  on this model space.  The algorithm we describe in this paper, Hoeffding Races, combines the robustness  of brute force and the computational feasibility of hill climbing. We instantiated the  algorithm by specifying the set of models to be memory-based algorithms (Stan-  fill and Waltz, 1986) (Atkeson and Reinkensmeyer, 1989) (Moore, 1992) and the  method of finding the error to be leave-one-out cross validation. We will discuss  how to extend the algorithm to any set of models and to the test set method in the  full paper. We chose memory-based algorithms since they go hand in hand with  cross validation. Training is very cheap - simply keep all the points in memory, and  all the algorithms of the various models can use the same memory. Finding the  leave-one-out cross validation error at a point is cheap as making a prediction: sim-  ply "cover up" that point in memory, then predict its value using the current model.  For a discussion of how to generate various memory-based models, see (Moore et  al., 1992).  2 Hoeffding Races  The algorithm was inspired by ideas from (Haussler, 1992) and (Kaelbling, 1990)  and a similar idea appears in (Greiner and Jurisica, 1992). It derives its name from  Hoeffding's formula (Hoeffding, 1963), which concerns our confidence in the sample  mean of n independently drawn points Xl, ..., x. The probability of the estimated  1  mean Ees =  Y']i<i< xi being more than epsilon far away from the true mean  Et,.,e after n independently drawn points is bounded by:  where B bounds the possible spread of point values.  We would like to say that with confidence 1 - 5, our estimate of the mean is within  e of the true mean; or in other words, Pr(lEtre - Eest[ > e) < 5. Combining the  two equations and solving for e gives us a bound on how close the estimated mean  is to the true mean after n points with confidence 1 - 5:  os(2/, )  (' V 2n  The algorithm starts with a collection of learning boxes. We call each model a  learning box since we are treating the models as if they were black boxes. We  are not looking at how complex or time-consuming each prediction is, just at the  input and output of the box. Associated with each learning box are two pieces of  information: a current estimate of its error rate and the number of points it has  been tested upon so far. The algorithm also starts with a test set of size N. For  leave-one-out cross validation, the test set is simply the training set.  62 Maron and Moore  F_.RROR  learning le.,ning learning learning learning learning le..a.,nin g  box #0 box #1 box  box #3 box 1 box t; box #6  Figure 2: An example where the best upper bound of learning box 2 eliminates  learning boxes l and 5. The size of e varies since each learning box has its own  upper bound on its error range, B.  At each point in the algorithm, we randomly select a point from the test set. We  compute the error at that point for all learning boxes, and update each learning  box's estimate of its own total error rate. In addition, we use Hoeffding's bound  to calculate how close the current estimate is to the true error for each learning  box. We then eliminate those learning boxes whose best possible error (their lower  bound) is still greater than the worst error of the best learning box (its upper  bound); see Figure 2. The intervals get smaller as more points are tested, thereby  "racing" the good learning boxes, and eliminating the bad ones.  We repeat the algorithm until we are left with just one learning box, or until we  run out of points. The algorithm can also be stopped once e has reached a certain  threshhold. The algorithm returns a set of learning boxes whose error rates are  insignificantly (to within e) different after N test points.  3 Proof of Correctness  The careful reader would have noticed that the confidence 5 given in the previous  section is incorrect. In order to prove that the algorithm indeed returns a set of  learning boxes which includes the best one, we'll need a more rigorous approach.  We denote by A the probability that the algorithm eliminates what would have  been the best learning box. The difference between A and 5 which was glossed over  in the previous section is that I -A is the confidence for the success of the entire  algrithm, while 1 - 5 is the confidence in Hoeffding's bound for one learning box  Hoeffding Races: Accelerating Model Selection 63  during one iteration of the algorithm.  We would like to make a formal connection between A and 5. In order to do that, let  us make the requirement of a correct algorithm more stringent. We'll say that the  algorithm is correct if every learning box is within e of its true error at every iteration  of the algorithm. This requirement encompasses the weaker requirement that we  don't eliminate the best learning box. An algorithm is correct with confidence A if  Pt{ all learning boxes are within e on all iterations} _ 1 - A.  We'll now derive the relationship between 5 and A by using the disjunctive proba-  bility inequality which states that Pr{A V B} _ Pr{A} + Pr{B}.  Let's assume that we have n iterations (we have n points in our test set), and that  we have m learning boxes (LB1 ... LB,). By Hoeffding's inequality, we know that  Pt{ a particular LB is within e on a particular iteration} _ I - 5  Flipping that around we get:  Pr{a particular LB is wrong on a particular iteration} < 5  Using the disjunctive inequality we can say  Pt{ a particular LB is wrong on iteration 1  a particular LB is wrong on iteration 2  a particular LB is  Let's rewrite this as:  Pt{ a particular LB is wrong on any iteration} _ 5  n  Now we do the same thing for all learning boxes:  Pr( LB1 is wrong on  LBs is wrong on  LB. is wrong on  or in other words:  V  V  wrong on iteration n) _< 5  n  Pr{ some LB is wrong in some iteration) _ 5  n  m  We flip this to get:  Pt{all LBs are within e on all iterations} _ 1 - 5  n  m  Which is exactly what we meant by a correct algorithm with some confidence.  Therefore, 5 - a . When we plug this into our expression for e from the previous  section, we find that we have only increased it by a constant factor. In other words,  by pumping up e, we have managed to ensure the correctness of this algorithm with  confidence A. The new e is expressed as:  e -- /B'(lg(2nnm)--lg(A))  any iteration V  any iteration V  any iteration} _ 5  n  m  64 Maron and Moore  Table 1: Test problems  Problem  Description  ROBOT  PROTEIN  ENERGY  POWER  POOL  DISCONT  10 input attributes, 5 outputs. Given an initial and a final description  of a robot arm, learn the control needed in order to make the robot  perform devil-sticking (Schaal and Atkeson, 1993).  3 inputs, output is a classification into one of three classes. This is the  famous protein secondary structure database, with some preprocessing  (Zhang et al., 1992).  Given solar radiation sensing, predict the cooling load for a building.  This is taken from the Building Energy Predictor Shootout.  Market data for electricity generation pricing period class for the new  United Kingdom Power Market.  The visually perceived mapping from pool table configurations to shot  outcome for two-ball collisions (Moore, 1992).  An artificially constructed set of points with many discontinuities. Lo-  cal models should outperform global ones.  Clearly this is an extremely pessimistic bound and tighter proofs are possible (Omo-  hundro, 1993).  4 Results  We ran Hoeffding Races on a wide variety of learning and prediction problems.  Table I describes the problems, and Table 2 summarizes the results and compares  them to brute force search.  For Table 2, all of the experiments were run using A - .01. The initial set of possible  models was constructed from various memory based algorithms: combinations of  different numbers of nearest neighbors, different smoothing kernels, and locally  constant vs. locally weighted regression. We compare the algorithms relative to  the number of queries made, where a query is one learning box finding its error at  one point. The brute force method makes [TEST] x ]LEARNING BOXES[ queries.  Hoeffding Races eliminates bad learning boxes quickly, so it should make fewer  queries.  5 Discussion  Hoeffding Races never does worse than brute force. It is least effective when all  models perform equally well. For example, in the POOL problem, where there  were 75 learning boxes left at the end of the race, the number of queries is only  slightly smaller for Hoeffding Races than for brute force. In the ROBOT problem,  where there were only 6 learning boxes left, a significant reduction in the number of  queries can be seen. Therefore, Hoeffding Races is most effective when there exists  a subset of clear winners within the initial set of models. We can then search over  a very broad set of models without much concern about the computational expense  Hoeffding Races: Accelerating Model Selection 65  Table 2: Results of Brute Force vs. Hoeffding Races.  Problem  points  Initial  queries queries  with with learning  learning boxes  boxes Brute Hoeffding  Force Races left  ROBOT  PROTEIN  ENERGY  POWER  POOL  DISCONT  972 95 92340 15637 6  4965 95 471675 349405 60  2444 189 461916 121400 40  210 95 19950 13119 48  259 95 24605 22095 75  500 95 47500 25144 29  0000  50000  0000  0000  Figure 3: The x-axis is the size of a set of initial learning boxes (chosen randomly)  and the y-axis is the number of queries to find a good model for the ROBOT  problem. The bottom line shows performance by the Hoeffding Race algorithm,  and the top line by brute force.  66 Maron and Moore  of a large initial set. Figure 3 demonstrates this. In all the cases we have tested,  the learning box chosen by brute force is also contained by the set returned from  Hoeffding Races. Therefore, there is no loss of performance accuracy.  The results described here show the performance improvement with relatively small  problems. Preliminary results indicate that performance improvements will increase  as the problems scale up. In other words, as the number of test points and the  number of learning boxes increase, the ratio of the number of queries made by  brute force to the number of queries made by Hoeffding Races becomes larger.  However, the cost of each query then becomes the main computational expense.  Acknowledgement s  Thanks go to Chris Atkeson, Marina Meila, Greg Galperin,  Stephen Omohundro for helpful and stimulating discussions.  Holly Yanco, and  References  [Atkeson and Reinkensmeyer, 1989] C. G. Atkeson and D. J. Reinkensmeyer. Using asso-  ciative content-addressable memories to control robots. In W. T. Miller, R. S. Sutton,  and P. J. Werbos, editors, Neural Networks for Control. MIT Press, 1989.  [Greiner and Jurisica, 1992] R. Greiner and I. Jurisica. A statisticaJ approach to solv-  ing the EBL utility problem. In Proceedings of the Tenth International conference on  Artificial Intelligence (AAAI-9). MIT Press, 1992.  [Haussler, 1992] D. Haussler. Decision theoretic generaJizations of the pac model for neural  net and other learning applications. Information and Computation, 100:78-150, 1992.  [Hoeffding, 1963] WassHy Hoeffding. Probability inequalities for sums of bounded random  variables. Journal of the American Statistical Association, 58:13-30, 1963.  [Kaelbling, 1990] L. P. Kaelbling. Learning in Embedded Systems. PhD. Thesis; TechnicaJ  Report No. TR-90-04, Stanford University, Department of Computer Science, June 1990.  [Moore et al., 1992] A. W. Moore, D. J. Hill, and M.P. Johnson. An empiricaJ inves-  tigation of brute force to choose features, smoothers and function approximators. In  S. Hanson, S. Judd, and T. Petsche, editors, Computational Learning Theory and Nat-  ural Learning Systems, Volume 3. MIT Press, 1992.  [Moore, 1992] A. W. Moore. Fast, robust adaptive control by learning only forward mod-  els. In J. E. Moody, S. J. Hanson, and R. P. Lippman, editors, Advances in Neural  Information Processing Systems . Morgan Kaufmann, April 1992.  [Omohundro, 1993] Stephen Omohundro. Private communication, 1993.  [Pollard, 1984] David Pollard. Convergence of Stochastic Processes. Springer-Verlag, 1984.  [SchaaJ and Atkeson, 1993] S. SchaaJ and C. G. Atkeson. Open loop stable control strate-  gies for robot juggling. In Proceedings of IEEE conference on Robotics and Automation,  May 1993.  [Stanfill and WaJtz, 1986] C. Stanfill and D. WaJtz. Towards memory-based reasoning.  Communications of the A CM, 29(12):1213-1228, December 1986.  [Wahba and Wold, 1975] G. Wahba and S. Wold. A completely automatic french curve:  Fitting spline functions by cross-validation. Communications in Statistics, 4(1), 1975.  [Zhang et al., 1992] X. Zhang, J.P. Mesirov, and D.L. WaJtz. Hybrid system for protein  secondary structure prediction. Journal of Molecular Biology, 225:1049-1063, 1992.  
Optimality Criteria for LMS and  Backpropagation  Babak Hassibi  Information Systems Laboratory  Stanford University  Stanford, CA 94305  All H. Sayed  Dept. of Elec. and Comp. Engr.  University of California Santa Barbara  Santa Barbara, CA 93106  Thomas Kailath  Information Systems Laboratory  Stanford University  Stanford, CA 94305  Abstract  We have recently shown that the widely known LMS algorithm is  an H a optimal estimator. The H a criterion has been introduced,  initially in the control theory literature, as a means to ensure ro-  bust performance in the face of model uncertainties and lack of  statistical information on the exogenous signals. We extend here  our analysis to the nonlinear setting often encountered in neural  networks, and show that the backpropagation algorithm is locally  H a optimal. This fact provides a theoretical justification of the  widely observed excellent robustness properties of the LMS and  backpropagation algorithms. We further discuss some implications  of these results.  I Introduction  The LMS algorithm was originally conceived as an approximate recursive procedure  that solves the following problem (Widrow and Hoff, 1960): given a sequence of n x 1  input column vectors {hi), and a corresponding sequence of desired scalar responses  {di), find an estimate of an n x 1 column vector of weights w such that the sum  of squared errors, -./N=0[di- hiw[ 2, is minimized. The LMS solution recursively  351  352 HassiN, Sayed, and Kailath  updates estimates of the weight vector along the direction of the instantaneous gra-  dient of the squared error. It has long been known that LMS is an approximate  minimizing solution to the above least-squares (or H 2) minimization problem. Like-  wise, the celebrated backpropagation algorithm (Rumelhart and McClelland, 1986)  is an extension of the gradient-type approach to nonlinear cost functions of the form  /N=0 ]di- hi(w)l 2, where hi(.) are known nonlinear functions (e.g., sigmoids). It  also updates the weight vector estimates along the direction of the instantaneous  gradients.  We have recently shown (HassiN, Sayed and Kailath, 1993a) that the LMS algo-  rithm is an H -optimal filter, where the H  norm has recently been introduced  as a robust criterion for problems in estimation and control (Zames, 1981). In gen-  eral terms, this means that the LMS algorithm, which has long been regarded as  an approximate least-mean squares solution, is in fact a minimizer of the H  error  norm and not of the H 2 norm. This statement will be made more precise in the  next few sections. In this paper, we extend our results to a nonlinear setting that  often arises in the study of neural networks, and show that the backpropagation  algorithm is a locally H-optimal filter. These facts readily provide a theoretical  justification for the widely observed excellent robustness and tracking properties of  the LMS and backpropagation algorithms, as compared to, for example, exact least  squares methods such as RLS (Haykin, 1991).  In this paper we attempt to introduce the main concepts, motivate the results, and  discuss the various implications. We shall, however, omit the proofs for reasons of  space. The reader is refered to (Hassibi et al. 1993a), and the expanded version of  this paper for the necessary details.  2 Linear H  Adaptive Filtering  We shall begin with the definition of the H  norm of a transfer operator. As  will presently become apparent, the motivation for introducing the H  norm is to  capture the worst case behaviour of a system.  Let h2 denote the vector space of square-summable complex-valued causal sequences  {fk, 0 _< k < co}, viz.,  h2 = {set of sequences {fk} such that ff < co}  k=0  with inner product < {f),{g} > = y=ofgk , where  denotes complex  conjugation. Let T be a transfer operator that maps an input sequence {ui} to an  output sequence {Yi}. Then the H  norm of T is equal to  IIrlloo- sup Ilyl12  11'112  the notation I111= anote the n-norm or sequence viz.,  k=Ok k  The H  norm may thus be regarded as the maximum ener9 gain from the input  u to the output F.  OptimaLity Criteria for LMS and Backpropagation 353  Suppose we observe an output sequence {di} that obeys the following model:  & = + vi  where hi T = [ hi hi2 ... hi,, ] is aknown input vector, w is an unknown weight  vector, and {vi} is an unknown disturbance, which may also include modeling errors.  We shall not make any assumptions on the noise sequence {vi} (such as whiteness,  normally distributed, etc.).  Let wi = .T'(do, di,..., di) denote the estimate of the weight vector w given the  observations {d j} from time 0 up to and including time i. The objective is to  determine the functional .T', and consequently the estimate wi, so as to minimize a  certain norm defined in terms of the prediction error  =hTw-Tw,-  which is the difference between the true (uncorrupted) output hTw and the pre-  dicted output hiZwi_. Let T denote the transfer operator that maps the unknowns  {w- w_, {vi}} (where w-1 denotes an initial guess of w) to the prediction errors  {ei}. The H  estimation problem can now be formulated as follows.  Problem 1 (Optimal H  Adaptive Problem) Find an H-optimal estima-  tion strategy wi = .T'(do, dl,..., di) that minimizes Ilrlloo, and obtain the resulting  7o 2 = inf []Tll2 = inf sup Ilel122  w-l' + Ilvlll (2)  where Iw- w_112 = (w- w_)'(w - w_), and I u is a positive constant that reflects  apriori knowledge as to how close w is to the initial guess w_.  Note that the infimum in (2) is taken over all causal estimators .T'. The above  problem formulation shows that H  optimal estimators guarantee the smallest  prediction error energy over all possible disturbances of fixed energy. H  estimators  are thus over conservative, which reflects in a more robust behaviour to disturbance  variation.  Before stating our first result we shall define the input vectors {hi} exciting if, and  only if,  N  lim  N-+oo  i=0  Theorem 1 (LMS Algorithm) Consider the model (I), and suppose we wish to  minimize the H  norm of the transfer operator from the unknowns w - w_ and  vi to the prediction errors el. If the input vectors hi are exciting and  1  0 <  < inf (3)  i hiThi  then the minimum H   given by the LMS algorithm witIs learning rate I u, viz.  Wi --' Wi--1 '- hi(di -- hiTwi-1)  norm is '/opt -- 1. In this case an optimal H  estimator is  , w_ (4)  354 Hassibi, Sayed, and Kailath  In other words, the result states that the LMS algorithm is an Ha-optimal filter.  Moreover, Theorem 1 also gives an upper bound on the learning rate/ that ensures  the H  optimality of LMS. This is in accordance with the well-known fact that  LMS behaves poorly if the learning rate is too large.  Intuitively it is not hard to convince oneself that 7opt cannot be less than one. To  this end suppose that the estimator has chosen some initial guess w_l. Then one  may conceive of a disturbance that yields an observation that coincides with the  output expected from w_l, i.e.  hiT w-1 = hiT w q- vi -- di  In this case one expects that the estimator will not change its estimate of w, so that  W i -- W_ 1 for all i. Thus the prediction error is  i -- hi T w -- hi T wi_ 1 = hi T w - hi T w_ 1 -- -v i  and the ratio in (2) can be made arbitrarily close to one.  The surprising fact though is that '/opt is one and that the LMS algorithm achieves  it. What this means is that LMS guarantees that the energy of the prediction  error will never exceed the energy of the disturbances. This is not true for other  estimators. For example, in the case of the recursive least-squares (RLS) algorithm,  one can come up with a disturbance of arbitrarily small energy that will yield a  prediction error of large energy.  To demonstrate this, we consider a special case of model (1) where hi is now a  scalar that randomly takes on the values +1 or -1. For this model/ must be less  than 1 and we chose the value tt -- .9. We compute the H * norm of the transfer  operator from the disturbances to the prediction errors for both RLS and LMS. We  also compute the worst case RLS disturbance, and show the resulting prediction  errors. The results are illustrated in Fig. 1. As can be seen, the H * norm in  the RLS case increases with the number of observations, whereas in the LMS case  it remains constant at one. Using the worst case RLS disturbance, the prediction  error due to the LMS algorithm goes to zero, whereas the prediction error due to  the RLS algorithm does not. The form of the worst case RLS disturbance is also  interesting; it competes with the true output early on, and then goes to zero.  We should mention that the LMS algorithm is only one of a family of H * optimal  estimators. However, LMS corresponds to what is called the central solution, and  has the additional properties of being the maximum entropy solution and the risk-  sensitive optimal solution (Whittle 1990, Glover and Mustafa 1989, Hassibi et al.  199ab).  If there is no disturbance in (1) we have the following  Corollary 1 If in addition to the assumptions of Theorem I there is no disturbance  in (I), then LMS guarantees 11 e 112_< y-llw- w_ll 2, meaning that the prediction  error converges to zero.  Note that the above Corollary suggests that the larger y is (provided (3) is satisfied)  the faster the convergence will be.  Before closing this section we should mention that if instead of the prediction error  one were to consider the filtered error eli -- hiw - hiwi, then the H  optimal  estimator is the so-called normalized LMS algorithm (Hassibi et al. 1993a).  Optimality Criteria for LMS and Backpropagation 355  2.5  2  1.5  1  0.5  0  1  0.98  0.96  0.94  0.92  0.9  o  5O 5O  0.5  0  -0.5  (c)  I  0.5  (d)  o  o.I It , ,., , .r- c-:r, ;  -1 -1  0 50 0 50  Figure 1: H  norm of transfer operator as a function of the number of observations  for (a) RLS, and (b) LMS. The true output and the worst case disturbance signal  (dotted curve) for RLS are given in (c). The predicted errors for the RLS (dashed)  and LMS (dotted) algorithms corresponding to this disturbance are given in (d).  The LMS predicted error goes to zero while the RLS predicted error does not.  3 Nonlinear H  Adaptive Filtering  In this section we suppose that the observed sequence {di} obeys the following  nonlinear model  di = hi(w) + (5)  where hi(.) is a known nonlinear function (with bounded first and second order  derivatives), w is an unknown weight vector, and {vi} is an unknown disturbance  sequence that includes noise and/or modelling errors. In a neural network context  the index i in hi(.) will correspond to the nonlinear function that maps the weight  vector to the output when the ith input pattern is presented, i.e., hi(w) = h(x(O, w)  where x  is the ith input pattern. As before we shall denote by wi = .T(do,..., di)  the estimate of the weight vector using measurements up to and including time i,  and the prediction error by  i ---- hi(w)--hi(wi-1)  Let T denote the transfer operator that maps the unknowns/disurbances  {w-w_i, {vi}} to the prediction errors {e}.  Problem 2 (Optimal Nonlinear H  Adaptive Problem) Find  an H-optimal estimation strategy wi - .T(do, dl,..., di) that minimizes IlTIl,  356 Hassibi, Sayed, and Kailath  and obtain the resulting  V0  --inf IlTll 2  = inf sup    w,vEh2  Currently there is no general solution to the above problem, and the class of non-  linear functions hi (.) for which the above problem has a solution is not known (Ball  and Helton, 1992).  To make some headway, though, note that by using the mean value theorem (5)  may be rewritten as  (:?hi T  d i -- hi(Wi_l) q- (-- (W_l).(w - wi_l) q- v i (7)  where w_ is a point on the line connecting w and wi-. Theorem 1 applied to (7)  shows that the recursion  W i -- Wi-- 1 q- lww(Wi_l)(d i -- hi(Wi_l) ) (S)  will yield 7 = 1. The problem with the above algorithm is that the w i are  not known. But it suggests that the 7opt in Problem 2 (if it exists) cannot be  less than one. Moreover, it can be seen that the backpropagation algorithm is an  approximation to (8) where w is replaced by wi. To pursue this point further we  use again the mean value theorem to write (5) in the alternative form  Ohi T 1 )T. 02hi  di -- hi(Wi_l)q-w w (wi-1).(w-wi-1)q-(w-wi-1 Ow 2 (i-1)'(w-wi-1) q-vi  (9)  where once more tbi_ lies on the line connecting wi-1 and w. Using (9) and  Theorem I we have the following result.  Theorem 2 (Backpropagation Algorithm) Consider the model (5)  backpropagation algorithm  Ohi .  Wi -- Wi_l q- lww (Wi-1)(di -- hi(Wi_l) )  and the  then if the ow [wi-) are exciting, and  , W-1 (10)  1  0 < y < inf (11)  i Oh T  Ow (Wi--1) Oh(Wi 1)   (9w -  then for all nonzero w, v  he  <1  0h T 2  II o (Wi_l)(W- wi_x) Ils  I(W-- Wi_l) T Oa'-xh (i_l).(W -- Wi_l) 112 2 --  -IIw-- W-Xl 2q- I[ vi q-  o  where  Ohi _   (W-- Wi--1) T OW9 (i--1) (W-- Wi--1) ---- hi(w ) -- hi(wi-1) ohiT  (wi-1).(w-- wi-1)  Hoo Optimality Criteria for LMS and Backpropagation 357  The above result means that if one considers a new disturbance v i - vi q-   wi_) T -(i_) (w- wi-), whose second term indicates how far hi(w) is from a  first order approximation at point wi-, then backpropagation guarantees that the  energy of the linearized prediction error 0 (wi_)(w- wi_) does not exceed the  energy of the new disturbances w - W_l and v i.  It seems plausible that if w_  is close enough to w then the second term in v i should  be small and the true and linearized prediction errors should be close, so that we  should be able to bound the ratio in (6). Thus the following result is expected,  where we have defined the vectors {hi} persistently exciting if, and only if, for all  a 6   N  i=0  Theorem 3 (Local H  Optimality) Consider the model (5) and the backprop-  agation algorithm (10). Suppose that the h'fw  Ow  i-   are persistently exciting, and  that (11) is satisfied. Then for each  > O, there exist 51, 52 > 0 such that for all  ]W- W_11 < (1 and all v 6 h2 with ]vii < 2, we have  II <  II v -  The above Theorem indicates that the backpropagation algorithm is locally H *  optimal. In other words for w_ sufficiently close to w, and for sufficiently small  disturbance, the ratio in (6) can be made arbitrarily close to one. Note that the  conditions on w and vi are reasonable, since if for example w is too far from W_l,  or if some vi is too large, then it is well known that backpropagation may get stuck  in a local minimum, in which case the ratio in (6) may get arbitrarily large.  As before (11) gives an upper bound on the learning rate /, and indicates why  backpropagation behaves poorly if the learning rate is too large.  If there is no disturbance in (5) we have the following  Corollary 2 If in addition to the assumptions in Theorem 3 there is no disturbance  in (5), then for every  > 0 there exists a t5 > 0 such that for all I w - W_l[ < ,  the backpropagation algorithm will yield l[ e' [12_< y-1(1 + ), meaning that the  prediction error converges to zero. Moreover wi will converge to w.  Again provided (11) is satisfied, the larger/ is the faster the convergence will be.  4 Discussion and Conclusion  The results presented in this paper give some new insights into the behaviour of  instantaneous gradient-based adaptive algorithms. We showed that if the underlying  observation model is linear then LMS is an H * optimal estimator, whereas if the  underlying observation model is nonlinear then the backpropagation algorithm is  locally H * optimal. The H * optimality of these algorithms explains their inherent  robustness to unknown disturbances and modelling errors, as opposed to other  estimation algorithms for which such bounds are not guaranteed.  358 Hassibi, Sayed, and Kailath  Note that if one considers the transfer operator from the disturbances to the pre-  diction errors, then LMS (backpropagation) is H  optimal (locally), over all causal  estimators. This indicates that our result is most applicable in situations where  one is confronted with real-time data and there is no possiblity of storing the train-  ing patterns. Such cases arise when one uses adaptive filters or adaptive neural  networks for adaptive noise cancellation, channel equalization, real-time control,  and undoubtedly many other situations. This is as opposed to pattern recognition,  where one has a set of training patterns and repeatedly retrains the network until  a desired performance is reached.  Moreover, we also showed that the H  optimality result leads to convergence proofs  for the LMS and backpropagation algorithms in the absence of disturbances. We  can pursue this line of thought further and argue why choosing large learning rates  increases the resistance of backpropagation to local minima, but we shall not do so  due to lack of space.  In conclusion these results give a new interpretation of the LMS and backpropaga-  tion algorithms, which we believe should be worthy of further scrutiny.  Acknowledgement s  This work was supported in part by the Air Force Office of Scientific Research, Air  Force Systems Command under Contract AFOSR91-0060 and in part by a grant  from Rockwell International Inc.  References  J. A. Ball and J. W. Helton. (1992) Nonlinear H m control theory for stable plants.  Math. Control Signals Systems, 5:233-261.  K. Glover and D. Mustafa. (1989) Derivation of the maximum entropy H m con-  troller and a state space formula for its entropy. Int. J. Control, 50:899-916.  B. Hassibi, A. H. Sayed, and T. Kailath. (1993a) LMS is H * Optimal. IEEE Conf.  on Decision and Control, 74-80, San Antonio, Texas.  B. Hassibi, A. H. Sayed, and T. Kailath. (1993b) Recursive linear estimation in  Krein spaces - part II: Applications. IEEE Conf. on Decision and Control, 3495-  3501, San Antonio, Texas.  S. Haykin. (1991) Adaptive Filter Theory. Prentice Hall, Englewood Cliffs, NJ.  D. E. Rumelhart, J. L. McClelland and the PDP Research Group. (1986) Parallel  distributed processing: explorations in the microstructure of cognition. Cambridge,  Mass. : MIT Press.  P. Whittle. (1990) Risk Sensitive Optimal Control. John Wiley and Sons, New  York.  B. Widrow and M. E. Hoff, Jr. (1960) Adaptive switching circuits. IRE WESCON  Cony. Rec., Pt.4:96-104.  G. Zames. (1981) Feedback optimal sensitivity: model preference transformation,  multiplicative seminorms and approximate inverses. IEEE Trans. on Automatic  Control, AC-26:301-320.  
SPEAKER RECOGNITION USING  NEURAL TREE NETWORKS  Kevin R. Farrell and Richard J. Mammone  CAIP Center, Rutgers University  Core Building, Frelinghuysen Road  Piscataway, New Jersey 08855  Abstract  A new classifier is presented for text-independent speaker recognition. The  new classifier is called the modified neural tree network (MNTN). The  NTN is a hierarchical classifier that combines the properties of decision  trees and feed-forward neural networks. The MNTN differs from the stan-  dard NTN in that a new learning rule based on discriminant learning  is used, which minimizes the classification error as opposed to a norm  of the approximation error. The MNTN also uses leaf probability mea-  sures in addition to the class labels. The MNTN is evaluated for several  speaker identification experiments and is compared to multilayer percep-  ttons (MLPs) decision trees, and vector quantization (VQ) classifiers. The  VQ classifier and MNTN demonstrate comparable performance and per-  form significantly better than the other classifiers for this task. Addition-  ally the MNTN provides a logarithmic saving in retrieval time over that  of the VQ classifier. The MNTN and VQ classifiers are also compared  for several speaker verification experiments where the MNTN is found to  outperform the VQ classifier.  1 INTRODUCTION  Automatic speaker recognition consists of having a machine recognize a person based  on his or her voice. Automatic speaker recognition is comprised of two categories:  speaker identification and speaker verification. The objective of speaker identifi-  cation is to identify a person within a fixed population based on a test utterance  from that person. This is contrasted to speaker verification where the objective is  to verify a person's claimed identity based on the test utterance.  1035  1036 Farrell and Mammone  Speaker recognition systems can be either text dependent or text independent.  Text-dependent speaker recognition systems require that the speaker utter a specific  phrase or a given password. Text-independent speaker identification systems iden-  tify the speaker regardless of the utterance. This paper focuses on text-independent  speaker identification and speaker verification tasks.  A new classifier is introduced and evaluated for speaker recognition. The new clas-  sifier is the modified neural tree network (MNTN). The MNTN incorporates mod-  ifications to the learning rule of the original NTN [1] and also uses leaf probability  measures in addition to the class labels. Also, vector quantization (VQ) classifiers,  multilayer perceptrons (MLPs), and decision trees are evaluated for comparison  This paper is organized as follows. Section 2 reviews the neural tree network and  discusses the modifications. Section 3 discusses the feature extraction and classifica-  tion phases used here for text-independent speaker recognition Section 4 describes  the database used and provides the experimental results. The summary and con-  clusions of the paper are given in Section 5.  2 THE MODIFIED NEURAL TREE NETWORK  The NTN [1] is a hierarchical classifier that uses a tree architecture to implement  a sequential linear decision strategy. Each node at every level of the NTN divides  the input training vectors into a number of exclusive subsets of this data. The  leaf nodes of the NTN partition the feature space into homogeneous subsets i.e.,  a single class at each leaf node. The NTN is recursively trained as follows. Given  a set of training data at a particular node, if all data within that node belongs to  the same class, the node becomes a leaf. Otherwise, the data is split into several  subsets, which become the children of this node. This procedure is repeated until  all the data is completely uniform at the leaf nodes.  For each node the NTN computes the inner product of a weight vector w and an  input feature vector :, which should be approximately equal to the the output  label y  {0, 1}. Traditional learning algorithms minimize a norm of the error  e = (y- < w,: >), such as the 2 or  norm. The splitting algorithm of the  modified NTN is based on discriminant learning [2]. Discriminant learning uses a  cost function that minimizes the classification error  For an M class NTN, the discriminant learning approach first defines a misclassifi-  cation measure d(:) as [2]:  1  1  - < > + :u---1 < ,  j#i  where n is a predetermined smoothing constant. If : belongs to class i, then d(:)  will be negative, and if : does not belong to class i, d(:) will be positive. The  misclassification measure d(:) is then applied to a sigmoid to yield:  1  g[d(:)] = 1 + -a(x)' (2)  The cost finction in equation (2) is approximately zero for correct classifications  and one for misclassifications. Hence, minimizing this cost function will tend to  Speaker Recognition Using Neural Tree Networks 1037  "'0 0   0 0  \4 01100..'  000X  0,  _ 00 '  \1  '". 10  I I : '"\ l 1  01 I :\..  LABEL = 0 LABEL = I LABEL = 0 LABEL = 1  CONFIDENCE = I_.0 CONFIDENCE = 0.6 CONFIDENCE = 0.8 CONFIDENCE = 0.7  Figure 1: Forward Pruning and Confidence Measures  minimize the classificat;ion error. It is noted that for binary NTNs, the weight  updat;es obtained by t;he discriminant learning approach and t;he L norm of t;he  error are equivalent; [3].  The NTN training algorit;hm described above const;ructs a t;ree having 100% per-  formance on the t;raining set;. However, an NTN t;rained t;o t;his level may not; have  optimal generalizat;ion due t;o overt;raining. The generalizat;ion can be improved  by reducing t;he number of nodes in a t;ree, which is known as pruning. A t;ech-  nique known as backward pruning was recently proposed [1] for t;he NTN. Given a  fully grown NTN, i.e., 100% performance on t;he t;raining set;, t;he backward pruning  met;hod uses a Lagrangian cost funct;ion t;o minimize t;he classificat;ion error and  t;he number of leaves in the tree. The met;hod used here prunes t;he t;ree during it;s  growl;h, hence it; is called .forward pruning.  The forward pruning algorit;hm consist;s of simply t;runcat;ing t;he growt;h of t;he t;ree  beyond a cert;ain level. For t;he leaves at; t;he t;runcat;ed level, a vot;e is t;aken and  the leaf is assigned the label of t;he majorit;yo In addit;ion t;o a label, t;he leaf is  also assigned a confidence. The confidence is comput;ed as t;he rat;io of the number  of element;s for the vot;e winner t;o t;he t;ot;al number of element;s. The confidence  provides a measure of confusion for t;he different regions of feat;ure space. The  concept; of forward pruning is illust;rat;ed in Figure 1.  3 FEATURE EXTRACTION AND CLASSIFICATION  The process of feature extraction consists of obtaining characteristic parameters of  a signal t;o be used t;o classify the signal. The extraction of salient features is a  key step in solving any pattern recognition problem. For speaker recognition, the  features extracted from a speech signal should be invariant with regard t;o the desired  1038 Farrell and Mammone  Feature X i  Vector  I Speakerl [yl_i  (NTN, VQ   ' Codebook)  Speaker 2 I   (NTN VQ y2 I  Codebook)  Speaker N I '  (NTN, VQ yN :  Codebook)  Decision  Speaker  Identity  or  Authenticity  Figure 2: Classifier Structure for Speaker Recognition  speaker while exhibiting a large distance to any imposter. Cepstral coefficients are  commonly used for speaker recognition [4] and shall be considered here to evaluate  the classifiers.  The classification stage of text-independent speaker recognition is typically imple-  mented by modeling each speaker with an individual classifier. The classifier struc-  ture for speaker recognition is illustrated in Figure 2. Given a specific feature vectors  each speaker model associates a number corresponding to the degree of match with  that speaker. The stream of numbers obtained for a set of feature vectors can be  used to obtain a likelihood score for each speaker model. For speaker identification,  the feature vectors for the test utterance are applied to all speaker models and the  corresponding likelihood scores are computed. The speaker is selected as having  the largest score. For speaker verification, the feature vectors are applied only to  the speaker model for the speaker to be verified. If the likelihood score exceeds a  threshold, the speaker is verified or else is rejected.  The classifiers for the individual speaker models are trained using either supervised  or unsupervised training methodso For supervised training methods the classifier for  each speaker model is presented with the data for all speakers. Here, the extracted  feature vectors for that speaker are labeled as %he" and the extracted feature vec-  tors for everyone else are labeled as "zero" The supervised classifiers considered  here are the multilayer perceptron (MLP), decision trees and modified neural tree  network (MNTN). For unsupervised training methods each speaker model is pre-  sented with only the extracted feature vectors for that speaker. This data can  then be clustered to determine a set of centfolds that are representative of that  speaker. The unsupervised classifiers evaluated here are the full-search and tree-  structure vector quantization classifiers, henceforth denoted as FSVQ and TSVQo  Speaker models based on supervised training capture the differences of that speaker  to other speakers, whereas models based on unsupervised training use a similarity  measllre.  Specifically, a trained NTN can be applied to speaker recognition as follows. Given  a sequence of feature vectors as from the test utterance and a trained NTN for  Speaker Recognition Using Neural Tree Networks 1039  speaker $i, the corresponding speaker score is found as the "hit" ratio:  M  PvT;v(x]Si) : N + M  (3)  Here, M is the number of vectors classified as %ne" and N is the number of vec-  tors classified as 'zero'" The modified NTN computes a hit ratio weighed by the  confidence scores:  1  E= cj (4)  o 1   where c  and c  are he confidence scores for he speaker and anispeaker, respec-  tively. These scores can be used for decisions regarding identification or verification.  4 EXPERIMENTAL RESULTS  4.1 Database  The database considered for the speaker identification and verification experiments  is a subset of the DARPA TIMIT database. This set represents 38 speakers of he  same (New England) dialect. The preprocessing of the TIMIT speech data consists  of several steps. First the speech is downsampled from 16KHz to 8 KHz sampling  frequency. The downsampling is performed to obtain a toll quality signal. The  speech data is processed by a silence removing algorithm followed by the application  of a pre-ernphasis filter H(z) = 1-0.95z -x A 30 ms Hamming window is applied to  the speech every 10 ms. A twelfth order linear predictive (LP) analysis is performed  for each speech frame. The features consist of the twelve cepstral coefficients derived  from this LP polynomial.  There are 10 utterances for each speaker in the selected set. Five of the utterances  are concatenated and used for training. The remaining five sentences are used  individually for testing. The duration of the training data ranges from 7 to 13  seconds per speaker and the duration of each test utterance ranges from 0.7 to 3.2  seconds.  4.2 Speaker Identification  The first experiment is for closed set speaker identification using 10 and 20 speakers  from the TIMIT New England dialect. The identification is closed set in that the  speaker is assumed to be one of the 10 or 20 speakers, i.e., no 'none of the above"  option. The NTN, MLP [5], and VQ [4] classifiers are each evaluated on this data in  addition to the ID3 [6], C4 [7], CART [8], and Bayesian [9] decision trees. The VQ  classifier is trained using a K-means algorithm and tested for codebook sizes varying  from 16 to 128 centroids. The MNTN used here is pruned at levels ranging from the  fourth through seventh. The MLP is trained using the backpropagation algorithm  [10] for architectures having 16, 32, and 64 hidden nodes (within one hidden layer).  The results are summarized in Table 1. The * denotes that the CART tree could  not be grown for the 20 speaker experiment due to memory limitations.  1040 Farrell and Mammone  Classifier 10 speakers 20 speakers  ID3 88% 79%  CART 76% *  C4 84% 73%  Bayes 92% 83%  MLP (16/32/64) 90/90/94% 90/82/85%  NTN (4/5/6/7) 66/84/92/92% 67/76/82/91%  MNTN (4/5/6/7) 88/90/94/98% 75/87/93/96%  TSVQ(16/32/64/128) 92/92/96/94% 83/90/90/88%  FSVQ (16/32/64/128) 98/98/98/98% 90/92/95/96%  Table 1: Speaker Identification Experiments  4.3 Speaker Verification  The FSVQ classifier and MNTN are evaluated next for speaker verification. The  first speaker verification experiment consists of 10 speakers and 10 imposters (i.e.,  people not used in the training set). The second speaker verification experiment  uses 20 enrolled speakers and 18 iraposters. The MNTN is pruned at the seventh  level (128 leaves) and the FSVQ classifier has a codebook size of 128 entries.  Speaker verification performance can be enhanced by using a technique known as  cohort normalization [11]. Traditional verification systems accept a speaker if:  where p(X]I) is the likelihood that the sequence of feature vectors X was generated  by speaker I and T(I) is the corresponding likelihood threshold. Instead of using  the fixed threshold criteria in equation (5), an adaptive threshold can be used via  the likelihood measure:  > 2(). (6)  P(XlZ)  Here, the speaker score is first normalized by the probability that the feature vectors  X were generated by a speaker other than I. The likelihood p(X]f) can be estimated  with the scores of the speaker models that are closest to I, denoted as I' cohorts  [11]. This estimate can consist of a maximum minimum, average, etc., depending  on the classifier used.  The threshold for the VQ and MNTN likelihood scores are varied from the point  of 0% false acceptance to 0% false rejection to yield the operating curves shown  in Figures 3 and 4 for the 10 and 20 speaker populations, respectively. Note that  all operating curves presented in this section for speaker verification represent the  posterior performance of the classifiers, given the speaker and imposter scores. Here  it can be seen that the MNTN and VQ classifiers are both improved by the cohort  normalized scores. The equal error rates for the MNTN and VQ classifier are  summarized in Table 2.  For both experiments (10 and 20 speakers), the MNTN provides better performance  than the VQ classifier,both with and without cohort normalization, for most of the  operating curve.  Speaker Recognition Using Neural Tree Networks 1041  VQ VQ MNTN MNTN  (w/ocohort) (w/cohort) (w/ocohort) (w/cohort)  10 Speakers 3.6% 2.2% 2.4070 1.8%  20 Speakers 4.6% 2.0% 2.8% 1.9%  Table 2: Equal error rates for speaker verification  0.35  0.25  '* 0,2  -.,  o.5  0.1  0.05  Speaker Verification (10 speakers)  : -+ VQ  : -. VQ with cohort  : MNTN  -- MNTNiwith cohort  0.04  0.01 0.02 0.03 0.05 0.06  P(False Accept)  Figure 3: Speaker Verification (10 Speakers)  Speaker Verification (20 speakers)  0.35 r' .............  0.3 "' ...........  X i ! ' -. MNTN ! :: I  -0 25 i-..\. .........  0.15 ':'"X'"'" ............... ! ............... :: ................  ............... ! ................  ',\,.   ! :: ::  0.1 .... - - X.-:- i'h,<,': ....... ! ............... i ................ :. ................ '. ................  0.05 ,,,, -,- -.---... -.., .,. - --.,... ................ ! ............... ! ................  :  o  o  0.02 0.04 0.06 0,08 0.1 0.12  P(False Accept)  Figure 4: Speaker Verification (20 Speakers)  1042 Farrell and Mammone  5 CONCLUSION  A new classifier called the modified NTN is examined for text-independent speaker  recognition. The performance of the MNTN is evaluated for several speaker recog-  nition experiments using various sized speaker sets from a 38 speaker corpus. The  features used to evaluate the classifiers are the LP-derived cepstrum. The MNTN is  compared to hill-search and tree-structured VQ classifiers, multi-layer perceptrons,  and decision trees. The FSVQ and MNTN classifiers both demonstrate equivalent  performance for the speaker identification experiments and outperform the other  classifiers. For speaker verification, the MNTN consistently outperforms the FSVQ  classifier. In addition to performance advantages for speaker verification, the MNTN  also demonstrates a logarithmic saving in retrieval time over that of the FSVQ clas-  sifier. This computational advantage can be obtained by using TSVQ, although  TSVQ will reduce the performance with respect to FSVQ.  6 ACKNOWLEDGEMENTS  The authors gratefully acknowledge the support of Rome Laboratories, Contract  No. F30602-91-C-0120. The decision tree simulations utilized the IND package  developed by W. Bunfine of NASA.  References  [1]  [2]  [3]  [4]  [s]  [6]  [7]  [s]  [9]  [10]  [11]  A. Sankar and R.J. Mammone. Growing and pruning neural tree networks.  IEEE Transactions on Computers, C-42:221-229, March 1993.  S. Katagiri, B.H Juang, and A. Biemo Discriminative feature extraction. In  Artificial Neural Networks for Speech and Vision Processing edited by R.J.  Mammone. Chapman and Hall, 1993.  K.R. Farrell. Speaker Recognition Using the Modified Neural Tree Network.  PhD thesis Rutgers University, Oct. 1993.  F.K. Soong, A.E. Rosenberg, L.R. Rabiner, and B.H. Juang. A vector quanti-  zation approach to speaker recognition. In Proceedings ICASSP 1985.  J. Oglesby and J.S. Mason. Optimization of neural models for speaker identi-  fication. In Proceedings ICASSP, pages 261-264, 1990.  J. Quinlan. Induction of decision trees. Machine Learning, 1:81-106, 1986.  J. Quinlan. Simplifying decision trees in Knowledge Acquisition for Knowledge-  Based Systems, by G. Gaines and J. Boose. Academic Press London, 1988.  L. Breiman, J.H. Friedman, RoA. Olshen, and C.J. Stone. Classification and  Regression Trees. Wadsworth international group, Belmont, CA, 1984.  W. Bunfine. Learning classification trees. Statistics and Computing, 2:63-73,  1992.  D.E. Rumelhart and J.L. McClellan& Parallel Distributed Processing. MIT  Cambridge Press, Cambridge, Ma, 1986.  A.E. Rosenberg, J. Delong,C.H. Lee,B.H. Juang and F.K. Soong. The use of  cohort normalized scores for speaker recognition. In Proc. ICSLP, Oct. 1992.  
A Hybrid Radial Basis Function Neurocomputer  and Its Applications  Steven S. Watkins  ECE Department  UCSD  La Jolla, CA. 92093  Paul M. Chau  ECE Department  UCSD  La Jolla, CA. 92093  Raoul Tawel  JPL  Caltech  Pasadena, CA. 91109  Bjorn Lambrigtsen  JPL  Caltech  Pasadena, CA. 91109  Mark Plutowski  CSE Department  UCSD  La Jolla, CA. 92093  Abstract  A neurocomputer was implemented using radial basis functions and a  combination of analog and digital VLSI cimuits. The hybrid system  uses custom analog circuits for the input layer and a digital signal  processiag board for the hidden and output layers. The system combines  the advantages of both analog and digital cimuits, featuring low power  consumption while minimizing overall system error. The analog circuits  have been fabricated and tested, the system has been built, and several  applications have been executed on the system. One application  provides significantly better results for a remote sensing problem than  have been previously obtained using onvelltional methods.  1.0 Introduction  This paper describes a neurocomputer development system that uses a radial basis  function as the transfer function of a neuron rather than the traditional sigmoid function.  This neurocomputer is a hybrid system which has been implemented with a combination  of analog and digital VLSI technologies. It offers the low-power advantage of analog  circuits operating in the subthreshold region and the high-precision advantage of digital  circuits. The system is targeted for applications that require low-power operation and use  input data in analog form. particularly remote sensing and portable computing  applications. It has already provided significantly better results for a remote sensing  850  A Hybrid Radial Basis Function Neurocomputer and Its Applications 851  NEURON  (el k ' k )'  I 0 I 1  EXPONENTIAL  I k  INPUT  OUTPUTS  YO Yl Y  MULTIPLY AND ACCUMULATE  Figure 1: Radial Basis Famction Network  NEURON  OILBERT MULTIPLIERS  I 0 11 12 13  I 0 1 t 12 13  Yo Y,i Y2 Y3  BOA FID  Yo Yt Y2 Y3  Figure 2: Mapping of RBF Network to Hardware  Analog Board  /  DIA Board  Figure 3: The RBF Neurocomputer Development System  852 Watkins, Chau, Tawel, Lambrigsten, and Plutowski  climate problem than have been previously obts_ined using conventional methods.  Figure 1 illustrates a radial basis functien (RBF) network. Radial basis functions have  been used to solve mapping and function estimation problems with positive results  (Moody and Darken, 1989; Lipproart, 1991). When coupled with a dynamic neuron  allocation algorithm such as Platt's RANN (Platt, 1991), RBF networks can usually be  trained much more quickly than a traditional sigmoidal, back-propagation network.  RBF networks have been implemented with completely-analog (Platt, Anderson and Kirk,  1993), completely-digital (Watkins, Chau and Tawel, Nov., 1992), and with hybrid analog/  digital approaches (Watkins, Chau and Tawel, Oct., 1992). The hybrid approach is optimal  for applications which require low power consumption and use input data that is naturally  in the analog domain while also requiring the high precision of the digital domain.  2.0 System Architecture and Benefits  Figure 2 shows the mapping of the RBF network to hardware. Figure 3 shows the  neurocomputex development system. The system consists of a PC controller, a DSP board  with a Motorola 56000 DSP chip and a board with analog multipliers. The benefits of the  hybrid approach are lower-cost parallelism than is possible with a completely-digital  system, and more precise computation than is possible with a completely-analog system.  The parallelism is available for low cost in terms of area and power, when the inputs are in  the analog domain. When comparing a single analog multiplier to a 10-bit fixed point  digital multiplier, the analog cell uses less than one-quarter the area and approximately  five orders of magnitude less power. When comparing an array of analog multipliers to a  Motorola 56000 DSP chip, 1000 Gilbert multipliers can fit in an area about haft the size of  the DSP chip, while consuming .003% of the power.  The restriction of requiring analog inputs is placed on the system, because if the inputs  were digital, the high cost of D to A conversion would remove the low cost benefit of the  system. This restriction causes the neurocomputer to be targeted for applications using  inputs that are in the analog domain, such as remote sensing applications that use  microwave or infrared sensors and speech recognition applications that use analog filters.  The hybrid system reduces the overall system error when compared with a completely-  analog solution. The digital circuits compute the hidden and output layers with 24 bits of  precision while analog circuits are limited to about 8 bits of precision. Also the RANN  algorithm requires a large range of width variation for the Gaussian function and this is  more easily achieved with digital computation. Completely analog solutions to this  problem are severely limited by the voltage rails of the chip.  3.0 Circuits  Several different analog circuit approaches were explored as possible implementations of  the network. After the dust settled, we chose to implement only the input layer with analog  circuits because it offers the greatest oppommity for parallelism, providing parallel  performance benefits at a low cost in terms of area and power. The input layer requires  more than O (N 2) computations (where N is the number of neurons), while the hidden and  output layers require only O(N)computafions (because there is one hidden layer  computation per neuron and the number of outputs is either one or very small).  A Hybrid Radial Basis Function Neurocomputer and Its Applications 853  The analog circuits used in the input layer are Gilbert multipliers (Mead, 1989).'The  circuits were fabricated with 2.0 micron, double-poly, P-well, CMOS technology. The  Gilbert cell performs the operation of multiplying two voltage differences: (V1-V2)x(VY  V4). In this system, V1 =V3 and V2=V4, which causes the circuit to compute the square of  the difference between a stored weight and the input. The current outputs of the Gilbert  cells in a row are wired together to sum their currents, giving a sum of squared errors. This  current is converted to a voltage, fed to an A to D converter and then passed to the DSP  board where the hidden and output layers are computed. The radial basis function  (Gaussian) of the hidden layer is computed by using a lookup table. The system uses the  fast multiply/accumulate operation of the DSP chip to compute the output layer.  4.0 Applications  The low-power feature of the hybrid system makes it attractive for applications where  power consumption is a prime consideration, such as satellite-based applications and  portable computing (using battery power). The neurocomputer has been applied to three  problems: a remote sensing climate problem, the Mackey-Glass chaotic time series  estimation and speech phoneme mco.tmitict. The remote sensing application falls into the  satellite category. The Mackey-Glass and speech recognition applications are potentially  portable. Systems for these applications are likely to have inputs in the analog domain  (eliminating the need for D to A conversion, as already discussed) making it feasible to  execute them on the hybrid neurocomputer.  4.1 The Remote Sensing Application  The remote sensing problem is an inverse mapping problem that uses microwave energy  in different bands as input to predict the water vapor content of the atmosphere at different  altitudes. Water vapor content is a key parameter for predicting weather in the tropics and  mid-latitudes (Kakar and Lambrigtsen, 1984). The application uses 12 inputs and 1 output.  The system input is naturally in analog form, the result of amplified microwave signals, so  no D to A conversion of input data is required. Others have used neural networks with  success to perform a similar inverse mapping to predict the temperature gradient of the  atmosphere (Motteler et al., 1993). Section 5 details the improved restfits of the RBF  network over conventional methods. Since water vapor content is a very important  comptment of climate models, improved results in predicted water vapor content means  improved climate models.  Remote sensing problems require satellite hardware where power consumptioxt is always a  major constraint. The low-power nature of the hybrid network would allow the network to  be placed on board a satellite. With future EOS missions requiring several thousand  sensors, the on-board network would reduce the bandwidth requirements of the data being  sent back to earth, allowing the reduced water vapor content data to be transmitted rather  than the raw sensor data. This data bandwidth reduction could be used either to send back  more meaningful data to further improve climate models, or to reduce the mount of data  transmitted, saving energy.  4.2 The Mackey-Glass Application  The Mackey-Glass chaotic time series application uses several previous time sample  values to predict the current value of a time series which was generated by the Mackey-  Glass delay-difference equation. It was used because it has proved to be difficult for  854 Watkins, Chau, Tawel, Lambrigsten, and Plutowski  sigmoidal neural networks (Platt, 1991). The application uses 4 inputs and 1 output. The  Mackey-Glass time series is representative of time series found in medical apphcations  such as detecting arrh_hmias in heartbeats. It could be advantageous to implement this  application with portable hardware.  4.3 The Speech Phoneme Recognition Application  The speech phoneme recognition problem used the same data as Waibel (Waibel et al.,  1989) to learn to recognize the acoustically similar phoneroes of b, d and g. The  application uses 240 inputs and 3 outputs. The speech phoneme recognition problem  represents a sub problem of the more difficult continuous speech recogtion problem.  Speech recognition applications also represent opportjnities for portable computing.  5.0 Results  5.1 The Remote Sensing Application  Using the RBF neural network on the remote sensing climate problem produced  significantly better results than had been previously obtained using conventional statistical  methods (Kakar and Lambrigtsen, 1984). The input layer of the RBF network was  implemented in two different ways: 1) it was simulated with 32-bit floating point precision  to represent a digital input layer, and 2) it was implemented with the analog Gilbert  multipliers as the input layer. Both implementations produced similar results.  At an altitude corresponding to 570 mb pressure, the RBF neural network with a digital  input layer produced results with .33 absolute rms error vs..42 rms error for the best  results using conventional methods. This is an improvement of 21%. Figure 4 shows the  plot of retrieved rs. actual water vapor content for both the RBF network and the  conventional method. Using the hybrid neurocompnter with the analog input layer for the  data at 570 mb pressure produced results with .338 rms error. This is an improvement of  19.5% over the conventional method. Using the analog input layer produced nearly as  much improvement as a completely-digital system, demonstrating the feasibility of  placing the network on board a satellite. Similar results were obtained for other altitudes.  The RBF network also was compared to a sigmoidal network using back propagation  learning enhanced with line-search capability (to automatically set step-size). Both  networks used eight neurons in the hidden layer. As Figure 5 shows, the RBF network  learned much faster than the sigmoidal network.  Key'  + w stariatieal mthod + o  J o +  0 I 2 ' ---  Actual Specific Humtdd  Figure 4: Comparison of Retrieved vs. Acm_a_l Water Vapor Content for 570 mb Pressure  for RBF Network and Conventional Statistical Method  A Hybrid Radial Basis Function Neurocomputer and Its Applications 855  solid - rbf software  08 dash .ed = rbf analog hardwine  dottea - sigmoid bckprop  06  o ..........  *--,  .........  1 2 4 $ 7 8 g  number of passes through training pattern. x 104  Figram 5: Comparison of T_aming Curves for R.BF and SimoidaJ Networks for Water  Vapor Application  0 25  02  015  01  Key'  solid = rbf software  dashed = rbf analog hardware  dotted - sigmoid backpop  number of passes through training patterns x 104  Figure 6: Comparison of Learning Curves for RBF and Sigmoidal Networks for Mackey-  Glass Application  5.2 The Mackey-Glass Application  The RBF network was not compared to any non-neural network method for the Mackey-  Glass time series estimation. It was only compared to a traditional sis, moidal network  using back propagation learning enhanced with line search. Both networks used four neu-  rons. As Figure 6 shows, applying the RBF neural network to the Mackey-Glass chaotic  time series estimation produced much faster learning than the sigmoidal network. The  RBF network with a digital input layer and the RBF hybrid network with an analog input  layer both produced similar results in droppiag to an rms error of about .025 after only 5  minutes of training on a PC using a 486 CPU.  Using the digital input layer, the RBF network reached a minimum absolute rats error of  .017, while the sigmoidal network reached a minimum absolute rrns error of .025. This is  an improvement of 32% over the sigmoidal network. Using the hybrid neuroctmaputer  with the analog input layer produced a minim_m absolute rms error of .022. This is an  improvement of 12% over the sis, moidal network  856 Watkins, Chau, Tawel, Lambrigsten, and Plutowski  5.3 The Speech Phoneme Recognition Application  The RBF network was not compared to any non-neural network method for the speech  phoneme recognition problem. It was only compared to Waibel's Time Delay Neural  Network (TDNN) (Waibel et al., 1989). The TDNN uses a topology matched to the time-  varying nature of speech with two hidden layers of eight and three neurons respectively.  The RBF network used a single hidden layer with the number of neurons varying between  eight and one hundred.  The TDNN achieved a 98% accuracy on the test set discriminating between the phoneroes  b, d and g. The RBF network achieved over 99% accuracy in training, but was only able to  achieve an 86% accuracy on the test set. To obtain better results, it is clear that the  topology of the RBF network needs to be altered to more closely match Waibel's TDNN.  However, this topology will complicate the VLSI implementation.  5.4 The Feasibility of Using the Analog Input Layer  One potential problem with using an analog input layer is that every individual hybrid  RBF neurocomputer might need to be trained on a problem, rather than being able to use a  common set of weights obtained from another RBF neurocomputer (which had been  previously trained). This potential problem exists because every analog circuit is unique  due to variation in the fabrication process. A set of experiments was designed to test this  possibility.  The remote sensing application and the Mackey-Glass application were trained and tested  two different ways: 1) hardware-trained/hardware-tested, that is, the analog input layer  was used for both training and testing; 2) software-trained/hardware-tested, that is the  analog input layer was simulated with 32-bit floating point precision for training and then  the analog hardware was used for testing..The hardware/hardware restfits provided a  benchmark. The software/hardware results demonstrated the feasibility of having a  standard set of weights that are not particular to a given set of analog hardware. For both  the remote sensing and the Mackey-Glass applications, the rms error performance was  only slightly degraded by using weights learned during software simulation. The remote  sensing results degraded by only .011 in terms of absolute rms error, and the Mackey-  Glass results degraded by only .002 in terms of absolute rms error. The results of the  experiment indicate that each individual hybrid RBF neurocomputer only needs to be  calibrated, not trained.  6.0 Conclusions  A low-power, hybrid analog/digital neurocomputer development system was constructed  using custom hardware. The system implements a radial basis function (RBF) network  and is targeted for applications that require low power consumption and use analog data as  their input, particularly remote sensing and portable applications. Several applications  were executed and results were obtained for a remote sensing application that are superior  to any previous results. Comparison of the restfits of a completely-digital simulation of the  RBF network and the hybrid analog/digital RBF network demonstrated the feasibility of  the hybrid approach.  A Hybrid Radial Basis Function Neurocomputer and Its Applications 857  Acknowledgments  The research described in this paper was performed at the Center for Space  Microelectronics Technology, Jet Propulsion Laboratory, California Institute of  Technology, and was sponsored by the National Aeronautics and Space Administration.  One of the authors, Steven S. Watkins, acknowledges the receipt of a Graduate Student  Researcher's Center Fellowship from the National Aeronautics and Space Administration.  Useful discussions with Silvio Ebedt, Ron Fellman, Eric Fossnm, Doug Kerns,  Fernando Pined& John Platt, and Anil Thakoor are also gratefully acknowledged.  References  Rsmesh Kakar and Bjom Lambrigtsen, "A Statistical Correlation Method for the Retrieval  of Atmospheric Moisture Profiles by Microwave Radiomerry," Journal of Climate and  Applied Meteorology, vol. 23, no. 7, July 1984, pp. 1110-1114.  R. P. Lippman, "A Critical Overview of Neural Network Pattern Classifiers," Proceedings  of the IEEE Neural Networks for Signal Processing Workshop, 1991, Princeton, N3., pp.  266-275.  Carver Mead, Analog VLSI and Neural Systems, Addison-Wesley, 1989, pp. 90-94.  J. Moody and C. Darken, "Fast Learning in Networks of Locally-Tuned Precessing  Units," Neural Computation, vol. 1, no. 2, Summer 1989, pp. 281-294.  Howard Motteler, J.A. Gualtieri, L.L. Strow and Larry McMillin, "Neural Networks for  Atmospheric Retrievals," NASA Goddard Conference on Space Applications of Artificial  Intelligence, 1993, pp. 155-167.  John Hart, "A Resource-Allocating Neural Network for Function Interpolation," Neural  Computation, vol. 3, no. 2, Summer 1991, pp. 213-225.  John Platt, Janeen Anderson and David B. Kirk, "An Analog VLSI Chip for Radial Basis  Functions," NIPS 5, 1993, pp. 765-772.  Alexander Waibel, T. Hanazawa, G. Binton, K. Shikano and K. Lang, "Phoneme  Recognition Using Tune-Delay Neural Networks," IEEE International Conference on  Acoustics, Speech and Signal Processing, May 1989, pp. 393-404.  Steve Watkins, Paul Chau and Raoul Tawel, "A Radial Basis Function Neurocomputer  with an Analog Input Layer," Proceedings of the IJCNN, Beijing, China, November 1992,  pp. n1225-230.  Steve Watkins, Paul Chau and Raoul Tawel, "Different Approaches to Implementing A  Radial Basis Function Neurocomputer," RNNS/IEEE Symposium on Neuroinformatics  and Neurocomputing, Rostov-on-Don, Russia, October 1992, pp. 1149-1155.  
Tonal Music as a Componential Code:  Learning Temporal Relationships Between and  Within Pitch and Timing Components  Catherine Stevens  Department of Psychology  University of Queensland  QLD 4072 Australia  kates @psych.psy.uq.oz.au  Janet Wiles  Depts of Psychology & Computer Science  University of Queensland  QLD 4072 Australia  janetw@ cs.uq.oz.au  Abstract  This study explores the extent to which a network that learns the  temporal relationships within and between the component features of  Western tonal music can account for music theoretic and psychological  phenomena such as the tonal hierarchy and rhythmic expectancies.  Predicted and generated sequences were recorded as the representation of  a 153-note waltz melody was learnt by a predictive, recurrent network.  The network learned transitions and relations between and within pitch  and timing components: accent and duration values interacted in the  development of rhythmic and metric structures and, with training, the  network developed chordal expectancies in response to the activation of  individual tones. Analysis of the hidden unit representation revealed  that musical sequences are represented as transitions between states in  hidden unit space.  1 INTRODUCTION  The fundamental features of music, derivable from frequency, time and amplitude  dimensions of the physical signal, can be described in terms of two systems - pitch and  timing. The two systems are frequently disjoined and modeled independently of one  another (e.g. Bharucha & Todd, 1989; Rosenthal, 1992). However, psychological  evidence suggests that pitch and timing factors interact (Jones, 1992; Monaban, Kendall  1085  1086 Stevens and Wiles  & Carterette, 1987). The pitch and timing components can be further divided into tone,  octave, duration and accent which can be regarded as a quasi-componential code. The  important features of a componential code are that each component feature can be viewed  as systematic in its own right (Fodor & Pylyshyn, 1988). The significance of  componential codes for learning devices lies in the productivity of the system - a small  (polynomial) number of training examples can generalise to an exponential test set  (Brousse & Smolensky, 1989; Phillips & Wiles, 1993). We call music a quasi-  componential. code as there are significant interactions between, as well as within, the  component features (we adopt this term from its use in describing other cognitive  phenomena, such as reading, Plaut & McClelland, 1993).  Connectionist models have been developed to investigate various aspects of musical  behaviour including composition (Hild, Feulner & Menzel, 1992), performance (Sayegh,  1989), and perception (Bharucha & Todd, 1989). The models have had success in  generating novel sequences (Mozer, 1991), or developing properties characteristic of a  listener, such as tonal expectancies (Todd, 1988), or reflecting properties characteristic of  musical structure, such as hierarchical organisation of notes, chords and keys (Bharucha,  1992). Clearly, the models have been designed with a specific application in mind and,  although some attention has been given to the representation of musical information  (e.g. Mozer, 1991; Bharucha, 1992), the models rarely explain the way in which musical  representations are constructed and learned. These models typically process notes which  vary in pitch but are of constant duration and, as music is inherently temporal, the  temporal properties of music must be reflected in both representation and processing.  There is an assumption implicit in cognitive modeling in the information processing  framework that the representations used in any cognitive process are specified a priori  (often assumed to be the output of a perceptual process). In the neural network  framework, this view of representation has been challenged by the specification of a dual  mechanism which is capable of learning representations and the processes which act upon  them. Since the systematic properties of music are inherent in Western tonal music as  an environment, they must also be reflected in its representation in, and processes of, a  cognitive system. Neural networks provide a mechanism for learning such  representations and processes, particularly with respect to temporal effects.  In this paper we study how representations can be learned in the domain of Western tonal  music. Specifically, we use a recurrent network trained on a musical prediction task to  construct representations of context in a musical sequence (a well-known waltz), and then  test the extent to which the learned representation can account for the classic phenomena  of music cognition. We see the representation construction as one aspect of learning and  memory in music cognition, and anticipate that additional mechanisms of music  cognition would involve memory processes that utilise these representations (e.g.  Stevens & Latimer, 1992), although they are beyond the scope of the present paper. For  example, Mozer (1992) discusses the development of higher-order, global representations  at the level of relations between phrases in musical patterns. By contrast, the present  study focusses on the development of representations at the level of relations between  individual musical events. We expect that the additional mechanisms would, in part,  develop from the behavioural aspects of music cognition that are made explicit at this  early, representation construction stage.  Tonal Music as a Componential Code 1087  2 METHOD & RESULTS  A simple recurrent network (Elman, 1989), consisting of 25 input and output units, and  20 hidden and context units, was trained according to Elman's prediction paradigm and  used Backprop Through Time (BPTT) for one time step. The training data comprised the  fin'st 2 sections of The Blue Danube by Johann Strauss wherein each training pattern  represented one event, or note, in the piece, coded as components of tone (12 values), rest  (1 value), octave (3 values), duration (6 values) and accent (3 values).  In the early stages of training, the network learned to predict the prior probability of  events (see Figure 1). This type of information could be encoded in the bias of the  output units alone, as it is independent of temporal changes m the input patterns. After  further training, the network learned to modify its predictions based on the input event  (purely feedforward information) and, later still, on the context in which the event  occurred (see Figure 2). Note that an important aspect of this type of recurrent network  is that the representation of context is created by the network itself (Elman, 1989) and is  not specified a priori by the network designer or the environment (Jordan, 1986). In this  way, the context can encode the information m the environment which is relevant to the  prediction task. Consequently, the network could, in principle, be adapted to other styles  of music without modification of the design, or input and output representations.  a. Output vector  b, Target histogram  AA#B CC#DD#E F F#GG#4 5 6 1 2 3 4 6  tones octaves durations  8 SWVR  accent rest  Figure 1: Comparison of output vector for the first event at Epoch 4 (a) and a histogram  of the target vector averaged over all events (b). The upper graph is the predicted output  of Event 1 at Epoch 4. The lower graph is a histogram of all the events in the piece,  created by averaging all the target vectors. The comparison shows that the net learned  initially to predict the mean target before learning the variations specific to each event.  1088 Stevens and Wiles  Elxx:h 64  EDoch_4 _  Epoch 2  fl  n rl rl 8   fl .... J'l , _ 7  N fl N rl 6  fl__ rl ,rl .... N , _ 5  fl R I't n 4  fl__  _fl rl _ 3  ,,:l fl N , _ 2  A A#B C C#DIE F F#GG4 5 6 I 2 3 4 6 8 S WV R event  toems octaves durations accent rest  Figure 2: Evolution of the first eight events (predicted). The first block (targets) shows  the correct sequence of events for the four components. In the second block (Epoch 2),  the net is beginning to predict activation of strong and weak accents. In the third block  (Epoch 4), the transition from one octave to another is evident. By the fourth block  (Epoch 64), all four components are substantially correct. The pattern of activation  across the output vector can be interpreted as a statistical description of each musical  event. In psychological terms, the pattern of activation reflects the harmonic or chordal  expectancies induced by each note (Bharucha, 1987) and characteristic of the tonal  hierarchy (Kmmhansl & Shepard, 1979).  3.1 NETWORK PERFORMANCE  The performance of the network as it learned to master The Blue Danube was recorded at  log steps up to 4096 epochs. One recording comprised the output predicted by the  network as the correct event was recycled as input to the network (similar to a teacher-  forcing paradigm). The second recording was generated by feeding the best guess of the  Tonal Music as a Componential Code 1089  output back as input. The accent - the lilt of the waltz - was incorporated very early in  the training. Despite numerous errors in the individual events, the sequences were clearly  identifiable as phrases from The Blue Danube and, for the most part, errors in the tone  component were consistent with the tonality of the piece. The errors are of  psychological importance and the overall performance indicated that the network learnt  the typical features of The Blue Danube and the waltz genre.  3.2 INTERACTIONS BETWEEN ACCENT & DURATION  Western tonal music is characterised by regularities in pitch and timing components. For  example, the occurrence of particular tones and durations in a single composition is  structured and regular given that only a limited number of the possible combinations  occur. Therefore, one way to gauge performance of the network is to compare the  regularities extracted and represented in the model with the statistical properties of  components in the training composition. The expected frequencies of accent-duration  pairs, such as a quarter-note coupled with a strong accent, were compared with the actual  frequency of occurrence in the composition: the accent and duration couplings with the  highest expected frequencies were strong quarter-note (35.7), strong half-note (10.2), weak  quarter-note (59.0), and weak half-note (16.9). Scrutiny of the predicted outputs of the  network over the time course of learning showed that during the initial training epochs  there was a strong bias toward the event with the highest expected frequency - weak  quarter-note. The output of this accent-duration combination by the network decreased  gradually. Prediction of a strong quarter-note by the network reached a value close to the  expected frequency of 35.7 by Epoch 2 (33) and then decreased gradually and approximated  the actual composition frequency of 19 at Epoch 64. Similarly, by Epoch 64, output of  the most common accent and duration pairs was very close to the actual frequency of  occurrence of those pairs in the composition.  3.3 ANALYSIS OF HIDDEN UNIT REPRESENTATIONS  An analysis of hidden unit space most often reveals structures such as regions, hierarchies  and intersecting regions (Wiles & Bloesch, 1992). In the present network, four sub-  spaces would be expected (tone, octave, duration, accent), with events lying at the  intersection of these sub-spaces. A two-dimensional projection of hidden unit space  produced from a canonical discriminant analysis (CDA) of duration-accent pairs reveals  these divisions (see Figure 3). In essence, there is considerable structure in the way  events are represented into clusters of regions with events located at the intersection of  these regions. In Figure 3 the groups used in the CDA relate to the output values which  are observable groups. An additional CDA using groups based on position in bar showed  that the hidden unit space is structured around inferred variables as well as observable  ones.  1090 Stevens and Wiles  i !  I  Figure 3: Two-dimensional projection of hidden unit space generated by canonical  discriminant analysis of duration-accent pairs. Each note in the composition is depicted  as a labelled point, and the first and third canonical components are represented along the  abscissa and ordinate, respectively. The first canonical component divides strong from  weak accents (denoted s and w). In the strong (s) accent region of the third canonical  component, quarter- and half-notes are separated (denoted by 2 and 4, respectively), and  the remaining right area separates rests, weak quarter- and weak half-notes. The  superimposed line shows the first five notes of the opening two bars of the composition  as a trajectory through hidden unit space: there is movement along the first canonical  component depending on accent (s or w) and the second bar starts in the half-note region.  4 DISCUSSION & CONCLUSIONS  The focus of this study has been the extraction of information from the environment -  the temporal stream of events representing/he Blue Danube - and its incorporation into  the static parameters of the weights and biases in the network. Evidence for the stages at  which information from the environment is incorporated into the network representation  is seen in the predicted output vectors (described above and illustrated in Figure 2).  Different musical styles contain different kinds of information in the components. For  example, the accent and duration components of a waltz take complementary roles in  regulating the rhythm. From the durations of events alone, the position of a note in a  Tonal Music as a Componential Code 1091  bar could be predicted without error. However, if the performer or listener made a single  error of duration, a rhythm system based on durations alone could not recover. By  contrast, accent is not a completely reliable predictor of the bar structure, but it is  effective for recovery from rhythmic errors. The interaction between these two timing  components provides an efficient error correction representation for the rhythmic aspect of  the system. Other musical styles are likely to have similar regulatory functions  performed by different components. For example, consider the use of ornaments, such as  trills and mordents, in Baroque harpsichord music which, in the absence of variations in  dynamics, help to signify the beat and metric structure. Alternatively, consider the  interaction between pitch and timing components with the placement of harmonically-  important tones at accented positions in a bar (Jones, 1992).  The network described here has learned transitions and relations between and within pitch  and timing musical components and not simply the components per se. The interaction  between accent and duration components, for example, demonstrates the manifestation of  a componential code in Western tonal music. Patterns of activation across the output  vector represented statistical regularities or probabilities characteristic of the composition.  Notably, the representation created by the network is reminiscent of the tonal hierarchy  which reflects the regularities of tonal music and has been shown to be responsible for a  number of performance and memory effects observed in both musically trained and  untrained listeners (Krumhansl, 1990). The distribution of activity across the tone  output units can also be interpreted as chordal or harmonic expectancies akin to those  observed in human behaviour by Bharucha & Stoeckig (1986). The hidden unit  activations represent the rules or grammar of the musical environment; an interesting  property of the simple recurrent network is that a familiar sequence can be generated by  the trained network from the hidden unit activations alone. Moreover, the intersecting  regions in hidden unit space represent composite states and the musical sequence is  represented by transitions between states. Finally, the course of learning in the network  shows an increasing specificity of predicted events to the changing context: during the  early stages of training, the default output or bias of the network is towards the average  pattern of activation across the entire composition but, over time, predictions are refined  and become attuned to the pattern of events in particular contexts.  Acknowledgements  This research was supported by an Australian Research Council Postdoctoral Fellowship  granted to the first author and equipment funds to both authors from the Departments of  Psychology and Computer Science, University of Queensland. The authors wish to thank  Michael Mozer for providing the musical database which was adapted and used in the present  simulation. The modification to McClelland & Rumelhart's (1989) bp program was developed  by Paul Bakker, Department of Computer Science, University of Queensland. The comments  and suggestions made by members of the Computer Science/Psychology Connectionist  Research Group at the University of Queensland are acknowledged.  References  Bharucha, J. J. (1987). Music cognition and perceptual facilitation: A connectionist  framework. Music Perception, 5, 1-30.  Bharucha, J. J. (1992). Tonality and learnability. In M. R. Jones & S. Holleran (Eds.),  Cognitive bases of musical communication, pp. 213-223. Washington: American  Psychological Association.  1092 Stevens and Wiles  Bharucha, J. J., & Stoeckig, K. (1986). Reaction time and musical expectancy: Priming of  chords. Journal of Experimental Psychology: Human Perception & Performance, 12, 403-  410.  Bharucha, J., & Todd, P.M. (1989). Modeling the perception of tonal structure with neural  nets. Computer Music Journal, 13, 44-53.  Brousse, O., & Smolensky, P. (1989). Virtual memories and massive generalization in  connectionist combinatorial learning. In Proceedings of the 11th Annual Conference of  the Cognitive Science Society, pp. 380-387. Hillsdale, N J: Lawrence Erlbaum.  Elman, J. L. (1989). Structured representations and connectionist models. (CRL Tech. Rep.  No. 8901). San Diego: University of California, Center for Research in Language.  Fodor, J. A., & Pylyshyn, Z. W. (1988). Connectionism and cognitive architecture: A critical  analysis. Cognition, 28, 3-71.  Hild, H., Feulner, J., & Menzel, W. (1992). HARMONET: A neural net for harmonizing  chorales in the style of J. S. Bach. In J. E. Moody, S. J. Hanson & R. Lippmann (Eds.),  Advances in Neural Information Processing Systems 4, pp. 267-274. San Mateo, CA.:  Morgan Kaufmann.  Jones, M. R. (1992). Attending to musical events. In M. R. Jones & S. Holleran (Eds.),  Cognitive bases of musical communication, pp. 91-110. Washington: American  Psychological Association.  Jordan, M. I. (1986). Serial order: A parallel distributed processing approach (Tech. Rep. No.  8604). San Diego: University of California, Institute for Cognitive Science.  Krumhansl, C., & Shepard, R. N. (1979). Quantification of the hierarchy of tonal functions  within a diatonic context. Journal of Experimental Psychology: Human Perception &  Performance, 5, 579-594.  McClelland, J. L., & Rumelhart, D. E. (1989). Explorations in parallel distributed processing:  A handbook of models, programs and exercises. Cambridge, Mass.: MIT Press.  Monahan, C. B., Kendall, R. A., & Carterette, E. C. (1987). The effect of melodic and  temporal contour on recognition memory for pitch change. Perception & Psychophysics,  41, 576-600.  Mozer, M. C. (1991). Connectionist music composition based on melodic, stylistic, and  psychophysical constraints. In P.M. Todd & D. G. Loy (Eds.), Music and connectionism,  pp. 195-211. Cambridge, Mass.: MIT Press.  Mozer, M. C. (1992). Induction of multiscale temporal structure. In J. E. Moody, S. J.  Hanson, & R. P. Lippmann (Eds.), Advances in Neural Information Processing Systems 4,  pp. 275-282. San Mateo, CA.: Morgan Kaufmann.  Phillips, S., & Wiles, J. (1993). Exponential generalizations from a polynomial number of  examples in a combinatorial domain. Submitted to IJCNN, Japan, 1993.  Plaut, D.C., & McClelland, J. L. (1993). Generalization with componential attractors: Word  and nonword reading in an attractor network. To appear in Proceedings of the 15th Annual  Conference of the Cognitive Science Society. Hillsdale, N J: Erlbaum.  Rosenthal, D. (1992). Emulation of human rhythm perception. Computer Music Journal, 16,  64-76.  Sayegh, S. (1989). Fingering for string instruments with the optimum path paradigm.  Computer Music Journal, 13, 76-84.  Stevens, C., & Latimer, C. (1991). Judgments of complexity and pleasingness in music: The  effect of structure, repetition, and training. Australian Journal of Psychology, 43, 17-22.  Stevens, C., & Latimer, C. (1992). A comparison of connectionist models of music  recognition and human performance. Minds and Machines, 2, 379-400.  Todd, P.M. (1988). A sequential network design for musical applications. In D. Touretzky,  G. Hinton & T. Sejnowski (Eds.), Proceedings of the 1988 Connectionist Models Summer  School, pp. 76-84. Menlo Park, CA: Morgan Kaufmann.  Wiles, J., & Bloesch, A. (1992). Operators and curried functions: Training and analysis of  simple recurrent networks. In J. E. Moody, S. J. Hanson, & R. P. Lippmann (Eds.),  Advances in Neural Information Processing Systems 4. San Mateo, CA: Morgan Kaufmann.  
High Performance Neural Net Simulation  on a Multiprocessor System with  "Intelligent" Communication  Urs A. Miiller, Michael Kocheisen, and Anton Gunzinger  Electronics Laboratory, Swiss Federal Institute of Technology  CH-8092 Zurich, Switzerland  Abstract  The performance requirements in experimental research on arti-  ficial neural nets often exceed the capability of workstations and  PCs by a great amount. But speed is not the only requirement.  Flexibility and implementation time for new algorithms are usually  of equal importance. This paper describes the simulation of neural  nets on the MUSIC parallel supercomputer, a system that shows a  good balance between the three issues and therefore made many  research projects possible that were unthinkable before. (MUSIC  stands for Multiprocessor _ystem with Intelligent Communication)  I Overview of the MUSIC System  The goal of the MUSIC project was to build a fast parallel system and to use it in  real-world applications like neural net simulations, image processing or simulations  in chemistry and physics [1, 2]. The system should be flexible, simple to program  and the realization time should be short enough to not have an obsolete system by  the time it is finished. Therefore, the fastest available standard components were  used. The key idea of the architecture is to support the collection and redistribution  of complete data blocks by a simple, efficient and autonomously working commu-  nication network realized in hardware. Instead of considering where to send data  and where from to receive data, each processing element determines which part of  a (virtual) data block it has produced and which other part of the same data block  it wants to receive for the continuation of the algorithm.  888  Parallel Neural Net Simulation 889  MUSIC board  Board  manager  PE PE  Host computer  (Sun, PC, Macintosh)  - user terminal  - mass storage  I  I SCSI  ............................  Board ?    manager i l  !  !  Transpoter links  ,,  !  .,  PE  [ I  PE PE I/0  Comm. Comm. Comm.  into'face interface i rite rface  Comm. Comm. Comm. Comm.  interface interface interface interface  32+8 bit, 5 MHz  Outside world  Figure 1: Overview of the MUSIC hardware  Figure 1 shows an overview of the MUSIC architecture. For the realization of the  communication paradigm a ring architecture has been chosen. Each processing  element has a communication interface realized with a XILINX 3090 programmable  gate array. During communication the data is shifted through a 40-bit wide bus (32  bit data and 8 bit token) operated at a 5-MHz clock rate. On each clock cycle, the  processing elements shift a data value to their right neighbors and receive a new  value from their left neighbors. By counting the clock cycles each communication  interface knows when to copy data from the stream passing by into the local memory  of its processing element and, likewise, when to insert data from the local memory  into the ring. The tokens are used to label invalid data and to determine when a  data value has circulated through the complete ring.  Three processing elements are placed on a 9 x 8.5-inch board, each of them consist-  ing of a Motorola 96002 floating-point processor, 2 Mbyte video (dynamic) RAM,  I Mbyte static RAM and the above mentioned communication controller. The  video RAM has a parallel port which is connected to the processor and a serial port  which is connected to the communication interface. Therefore, data processing is  almost not affected by the communication network's activity and communication  and processing can overlap in time. This allows to use the available communication  bandwidth more efficiently. The processors run at 40 MHz with a peak performance  of 60 MFlops. Each board further contains an Inmos T425 transputer as a board  890 Mfiller, Kocheisen, and Gunzinger  Number of processing elments:  Peak performance:  Floating-point format:  Memory:  Programming language:  Cabinet:  Cooling:  Total power consumption:  Host computer:  6O  3.6 Gflops  44 bit IEEE single extended precision  180 Mbyte  C, Assembler  19-inch rack  forced ir cooling  less than 800 Watt  Sun workstation, PC or Macintosh  Table 1: MUSIC system technical data  manager, responsible for performance measurements and data communication with  the host (a Sun workstation, PC or Macintosh).  In order to provide the fast data throughput required by many applications, special  I/O modules (for instance for real-time video processing applications) can be added  which have direct access to the fast ring bus. An SCSI interface module for four  parallel SCSI-2 disks, which is currently being developed, will allow the storage  of huge amount of training data for neural nets. Up to 20 boards (60 processing  elements) fit into a standard 19-inch rack resulting in a 3.6-Gfiops system. MUSIC's  technical data is summarized in Table 1.  For programming the communication network just three library functions are nec-  essary: In:_corm() to specify the data block dimensions and data partitioning,  Da:a_ready() to label a certain amount of data as ready for communication and  Wa:_da:a() to wait for the arrival of the expected data (synchronization). Other  functions allow the exchange and automatic distribution of data blocks between the  host computer and MUSIC and the calling of individual user functions. The activity  of the transputers is embedded in these functions and remains invisible for the user.  Each processing element has its own local program memory which makes MUSIC  a MIMD machine (multiple instructions multiple data). However, there is usually  only one program running on all processing elements (SPMD = single program mul-  tiple data) which makes programming as simple or even simpler as programming a  SIMD computer (single instruction multiple data). The difference to SIMD machines  is that each processor can take different program pathes on conditional branches  without the performance degradation that occurs on SIMD computers in such a  case. This is especially important for the simulation of neural nets with nonregular  local structures.  2 Parallelization of Neural Net Algorithms  The first implemented learning algorithm on MUSIC was the well-known back-  propagation applied to fully connected multilayer perceptrons [3]. The motivation  was to gain experience in programming the system and to demonstrate its perfor-  mance on a real-world application. All processing elements work on the same layer  a time, each of them producing an individual part of the output vector (or error  vector in the backward path) [1]. The weights are distributed to the processing  elements accordingly. Since a processing element needs different weight subsets in  Parallel Neural Net Simulation 891  20O  150  100  Linear spe  / 9oo-6oo  -* 300-200-10  10 20 30 40 50 60  Number of processing elements  Figure 2: Estimated (lines) and measured (points) back-propagation performance  for different neural net sizes.  the forward and in the backward path, two subsets are stored and updated on each  processing element. Each weight is therefore stored and updated twice on different  locations on the MUSIC system [1]. This is done to avoid the communication of  the weights during learning what would cause a saturation of the communication  network. The estimated and experimentally measured speedup for different sizes of  neural nets is illustrated in Figure 2.  Another frequently reported parallelization scheme is to replicate the complete net-  work on all processing elments and to let each of them work on an individual subset  of the training patterns [4, 5, 6]. The implementation is simpler and the commu-  nication is reduced. However, it does not allow continuous weight update, which is  known to converge significantly faster than batch learning in many cases. A com-  parison of MUSIC with other back-propagation implementations reported in the  literature is shown in Table 2.  Another category of neural nets that have been implemented on MUSIC are cellular  neural nets (CNNs) [10]. A CNN is a two-dimensional array of nonlinear dynamic  cells, where each cell is only connected to a local neighborhood [11, 12]. In the  MUSIC implementation every processing elment computes a different part of the  array. Between iteration steps only the overlapping parts of the neighborhoods  need to be communicated. Thus, the computation to communication ratio is very  high resulting in an almost linear speedup up to the maximum system size. CNNs  are used in image processing and for the modeling of biological structures.  3 A Neural Net Simulation Environment  After programming all necessary functions for a certain algorithm (e.g. forward  propagate, backward propagate, weight update, etc.) they need to be combined  892 Mailer, Kocheisen, and Gunzinger  Performance Cont.  System No. of Forward Learning Peak weight  PEs [MCPS] [MCUPS] (%) update  PC (80486, 50 MHz)* 1 1.1 0.47 38.0 Yes  Sun (Sparcstation 10)* 1 3.0 1.1 43.0 Yes  Alpha Station (150 MHz)* 1 8.3 3.2 8.6 Yes  Hypercluster [7] 64 27.0 9.9 -- --  Warp [4] 10 -- 17.0 -- No  CM-2** [6] 64K 180.0 40.0 -- No  Cray Y-MP C90'** 1 220.3 65.6 -- Yes  RAP [8] 40 574.0 106.0 50.0 Yes  NEC SX-3*** 1 -- 130.0 9.6 Yes  MUSIC* 60 504.0 247.0 28.0 Yes  Sandy/8** [9] 256 -- 583.0 31.0 Yes  GFll [5] 356 -- 901.0 54.0 No  *Own measurements  **Estimated numbers  ***No published reference available.  Table 2: Comparison of floating-point back-propagation implementations. "PEs"  means processing elements, "MCPS" stands for millions of connections per second  in the forward path and "MCUPS" is the number of connection updates per second  in the learning mode, including both forward and backward path. Note that not all  implementations allow continuous weight update.  in order to construct and train a specific neural net or to carry out a series of  experiments. This can be done using the same programming language that was  used to program the neural functions (in case of MUSIC this would be C). In this  case the programmer has maximum flexibility but he also needs a good knowledge  of the system and programming language and ater each change in the experimental  setup a recompilation of the program is necessary.  Because a set of neural functions is usually used by many different researchers who,  in many cases, don't want to be involved in a low-level (parallel) programming of  the system, it is desirable to have a simpler front-end for the simulator. Such a  front-end can be a shell program which allows to specify various parameters of the  algorithm (e.g. number of layers, number of neurons per layer, etc.). The usage of  such a shell can be very easy and changes in the experimental setup don't require  recompilation of the code. However, the flexibility for experimental research is  usually too much limited with a simple shell program. We have chosen a way in  between: a command language to combine the neural functions which is interactive  and much simpler to learn and to use than an ordinary programming language like  C or Fortran. The command language should have the following properties:  - interactive  - easy to learn and to use  - flexible  - loops and conditional branches  - variables  - transparent interface to neural functions.  Parallel Neural Net Simulation 893  Instead of defining a new special purpose command language we decided to consider  an existing one. The choice was Basic which seems to meet the above requirements  best. It is easy to learn and to use, it is widely spread, flexible and interactive. For  this purpose a Basic interpreter, named Neuro-Basic, was written that allows the  calling of neural (or other) functions running parallel on MUSIC. From the Basic  level itself the parallelism is completely invisible. To allocate a new layer with 300  neurons, for instance, one can type  a = new_layer(BOO)  The variable a afterwards holds a pointer to the created layer which later can be  used in other functions to reference that layer. The following command propagates  layer a to layer b using the weight set w  propagate(a, b, w)  Other functions allow the randomization of weights, the loading of patterns and  weight sets, the computation of mean squared errors and so on. Each instruction  can be assigned to a program line and can then be run as a program. The sequence  10 a = new_layer(BOO)  20 b = new_layer(10)  BO w = new_weights(a, b)  for instance defines a two-layer perceptron with 300 input and 10 output neurons be-  ing connected with the weights w. Larger programs, loops and conditional branches  can be used to construct and train complete neural nets or to automatically run  complete series of experiments where experimental setups depend on the result of  previous experiments. The Basic environment thus allows all kinds of gradations in  experimental research, from the interactive programming of small experiments till  large off-line learning jobs. Extending the simulator with new learning algorithms  means that the programmer just has to write the parallel code of the actual algo-  rithm. It can then be controlled by a Basic program and it can be combined with  already existing algorithms.  The Basic interpreter runs on the host computer allowing easy access to the in-  put/output devices of the host. However, the time needed for interpreting the  commands on the host can easily be in the same order of magnitude as the runtime  of the actual functions on the attached parallel processor array. The interpretation  of a Basic program furthermore is a sequential part of the system (it doesn't run  faster if the system size is increased) which is known to be a fundamental limit in  speedup (Amdahls law [13]). Therefore the Basic code is not directly interpreted on  the host but first is compiled to a simpler stack oriented meta-code, named b-code,  which is afterwards copied and run on all processing elements at optimum speed.  The compilation phase is not really noticeable to the user since compiling 1000  source lines takes less than a second on a workstation.  Note that Basic is not the programming language for the MUSIC system, it is a  high level command language for the easy control of parallel algorithms. The actual  programming language for MUSIC is C or Assembler.  894 Mfiller, Kocheisen, and Gunzinger  Of course, Neuro-Basic is not restricted to the MUSIC system. The same principle  can be used for neural net simulation on conventional workstations, vector comput-  ers or other parallel systems. Furthermore, the parallel algorithms of MUSIC also  run on sequential computers. Simulations in Neuro-Basic can therefore be executed  locally on a workstation or PC as well.  4 Conclusions  Neuro-Basic running on MUSIC proved to be an important tool to support exper-  imental research on neural nets. It made possible to run many experiments which  could not have been carried out otherwise. An important question, however, is,  how much more programming effort is needed to implement a new algorithm in  the Neuro-Basic environment compared to an implementation on a conventional  workstation and how much faster does it run.  Algorithm additional speedup  programming  Back-propagation (C) x 2 60  Back-propagation (Assembler) x 8 240  Cellular neural nets (CNN) x 3 60  Table 3: Implementation time and performance ratio of a 60-processor MUSIC  system compared to a Sun Sparcstation-10  Table 3 contains these numbers for back-propagation and cellular neural nets. It  shows that if an additional programming effort of a factor two to three is invested  to program the MUSIC system in C, the return of investment is a speedup of ap-  proximately 60 compared to a Sun Sparcstation-10. This means one year of CPU  time on a workstation corresponds to less than a week on the MUSIC system.  Acknowledgements  We would like to express our gratitude to the many persons who made valuable  contributions to the project, especially to Peter Kohler and Bernhard Biumle for  their support of the MUSIC system, Jos6 Osuna for the CNN implementation and  the students Ivo Hasler, BjSrn Tiemann, Ren6 Hauck, Rolf Kr/ihenb/ihl who worked  for the project during their graduate work.  This work was funded by the Swiss Federal Institute of Technology, the Swiss Na-  tional Science Foundation and the Swiss Commission for Support of Scientific Re-  search (KWF).  References  [1] Urs A. M/iller, Bernhard Biumle, Peter Kohler, Anton Gunzinger, and Walter  Guggenbiihl. Achieving supercomputer performance for neural net simulation  with an array of digital signal processors. IEEE Micro Magazine, 12(5):55-65,  October 1992.  Parallel Neural Net Simulation 895  [10]  [11]  [12]  [13]  [2] Anton Gunzinger, Urs A. Miiller, Walter Scott, Bernhard B/iumle, Peter  Kohler, Hansruedi Vonder Miihll, Flortan Miiller-Plathe, Wilfried F. van Gun-  steren, and Walter Guggenbiihl. Achieving super computer performance with  a DSP array processor. In Robert Werner, editor, Supercomputing '9, pages  543-550. IEEE/ACM, IEEE Computer Society Press, November 16-20, 1992,  Minneapolis, Minnesota 1992.  [3] D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning internal represen-  tation by error propagation. In David E. Rumelhart and James L. McClelland,  editors, Parallel Distributet Processing: Explorations in the Microstructure of  Cognition, volume 1, pages 318-362. Bradford Books, Cambridge MA, 1986.  [4] Dean A. Pomerleau, George L. Guscfora, David S. Touretzky, and H. T. Kung.  Neural network simulation at Warp speed: How we got 17 million connections  per second. In IEEE International Conference on Neural Networks, pages  II.143-150, July 24-27, San Diego, California 1988.  [5] Michael Witbrock and Marco Zagha. An implementation of backpropaga-  tion learning on GFll, a large SIMD parallel computer. Parallel Computing,  14(3):329-346, 1990.  [6] Xiru Zhang, Michael Mckenna, Jill P. Mesirov, and David L. Waltz. An ef-  ficient implementation of the back-propagation algorithm on the Connection  Machine CM-2. In David S. Touretzky, editor, Advances in Neural Information  Processing Systems (NIPS-89), pages 801-809, 2929 Campus Drive, Suite 260,  San Mateo, CA 94403, 1990. Morgan Kaufmann Publishers.  [7] Heinz Miihlbein and Klaus Wolf. Neural network simulation on parallel com-  puters. In David J. Evans, Gerhard R. Joubert, and Frans J. Peters, editors,  Parallel Computing-89, pages 365-374, Amsterdam, 1990. North Holland.  [8] Phil Kohn, Jeff Bilmes, Nelson Morgan, and James Beck. Software for ANN  training on a Ring Array Processor. In John E. Moody, Steven J. Hanson,  and Richard P. Lippmann, editors, Advances in Neural Information Processing  Systems d (NIPS-91), 2929 Campus Drive, Suite 260, San Mateo, California  94403, 1992. Morgan kaufmann.  [9] Hideki Yoshizawa, Hideki Kato Hiroki Ichiki, and Kazuo Asakawa. A  highly parallel architecture for back-propagation using a ring-register data  path. In nd International Conference on Microelectrnics for Neural Networks  (ICMNN-91), pages 325-332, October 16-18, Munich 1991.  J. A. Osuna, G. S. Moschytz, and T. Roska. A framework for the classifica-  tion of auditory signals with cellular neural networks. In H. Dedieux, editor,  Procedings of 11. European Conference on Circuit Theory and Design, pages  51-56 (part 1). Elsevier, August 20 - Sept. 3 Davos 1993.  Leon O. Chua and Lin Yang. Cellular neural networks: Theory. IEEE Trans-  actions on Circuits and Systems, 35(10):1257-1272, October 1988.  Leon O. Chua and Lin Yang. Cellular neural networks: Applications. IEEE  Transactions on Circuits and Systems, 35(10):1273-1290, October 1988.  Gene M. Amdahl. Validity of the single processor approach to achieving large  scale computing capabilities. In AFIPS Spring Computer Conference Atlantic  City, N J, pages 483-485, April 1967.  
Two-Dimensional Object Localization by  Coarse-to-Fine Correlation Matching  Chien-Ping Lu and Eric Mjolsness  Department of Computer Science  Yale University  New Haven, CT 06520-8285  Abstract  We present a Mean Field Theory method for locating two-  dimensional objects that have undergone rigid transformations.  The resulting algorithm is a form of coarse-to-fine correlation  matching. We first consider problems of matching synthetic point  data, and derive a point matching objective function. A tractable  line segment matching objective function is derived by considering  each line segment as a dense collection of points, and approximat-  ing it by a sum of Gaussians. The algorithm is tested on real images  from which line segments are extracted and matched.  I Introduction  Assume that an object in a scene can be viewed as an instance of the model placed  in space by some spatial transformation, and object recognition is achieved by dis-  covering an instance of the model in the scene. Two tightly coupled subproblems  need to be solved for locating and recognizing the model: the correspondence prob-  lem (how are scene features put into correspondence with model features?), and the  localization problem (what is the transformation that acceptably relates the model  features to the scene features?). If the correspondence is known, the transformation  can be determined easily by least squares procedures. Similarly, for known trans-  formation, the correspondence can be found by aligning the model with the scene,  or the problem becomes an assignment problem if the scene feature locations are  jittered by noise.  985  986 Lu and Mjolsness  Several approaches have been proposed to solve this problem. Some tree-pruning  methods [1, 3] make hypotheses concerning the correspondence by searching over  a tree in which each node represents a partial match. Each partial match is then  evaluated through the pose that best fits it. In the generalized Hough transform or  equivalently template matching approach [7, 3], optimal transformation parameters  are computed for each possible pairing of a model feature and a scene feature, and  these "optimal" parameters then "vote" for the closest candidate in the discretized  transformation space.  By contrast with the tree-pruning methods and the generalized Hough transform, we  propose to formulate the problem as an objective function and optimize it directly  by using Mean Field Theory (MFT) techniques from statistical physics, adapted as  necessary to produce effective algorithms in the form of analog neural networks.  2 Point Matching  Consider the problem of locating a two-dimensional "model" object that is believed  to appear in the "scene". Assume first that both the model and the scene are  represented by a set of "points" respectively, {xi} and {Ya}- The problem is to  recover the actual transformation (translation and rotation) that relates the two  sets of points. It can be solved by minimizing the following objective function  Ematch(Mia,O,t) ----- E Mia[[xi- ILoYa - t[[ 2  ia  (1)  where {Mi,} = M is a 0/1-valued "match matrix" representing the unknown cor-  respondence, R0 is a rotation matrix with rotation angle 0, and t is a translation  vector.  2.1 Constraints on match variables  We need to enforce some constraints on correspondence (match) variables Mia;  otherwise all Mia = 0 in (1). Here, we use the following constraint  = _> 0; (2)  ia  implying that there are exactly N matches among all possible matches, where N  is the number of the model features. Summing over permutation matrices obeying  this constraint, the effective objective function is approximately [5]:  1  F(O, t,/) - - E e-/511x'-ReY-tll' (3)  a  which has the same fixed points as  1  Spena,ty(U,/, t) -- Eraarch(M,/, t) +  E Mia(log Mia - 1), (4)  where Mi is treated as a continuous variable and is subject to the penalty function  z(log z - 1).  Two-Dimensional Object Localization by Coarse-to-Fine Correlation Matching 987  Figure 1: Assume that there is only translation between the model and the scene,  each containing 20 points. The objective functions at at different temperatures  (/-1): 0.0512 (top left), 0.0128 (top right), 0.0032 (bottom left) and 0.0008 (bot-  tom right), are plotted as energy surfaces of x and y components of translation.  Now, let $ = 1/2rr 2 and write  Epont(O, t) =  e- '"x'-R'Y -t" (5)  ia  The problem then becomes that of maximizing Epoint, which in turn can be interpre-  tated as minimizing the Euclidean distance between two Gaussian-blurred images  containing the scene points xi and a transformed version of the model points ya.  Tracking the local maximum of the objective function from large a to small a, as in  deterministic annealing and other continuation methods, corresponds to a coarse-  to-fine correlation matching. See Figure 1 for a demonstration of a simpler case in  which only translation is applied to the model.  2.2 The descent dynamics  A gradient descent dynamics for finding the saddle point of the effective objective  function F is   -- -mia(Xi - RoYa - t)  ia   = -  -a(x - l0y - t/(l0+y),  ia  (6)  988 Lu and Mjolsness  where rnia -- {Mia}/ = e -llx'-Ry-tll2 is the "soft correspondence" associated  with Mia. Instead of updating t by descent dynamics, we can also solve for t  directly.  3 The Vernier Network  Though the effective objective is non-convex over translation at low temperatures,  its dependence on rotation is non-convex even at relatively high temperatures.  3.1 Hierachical representation of variables  We propose overcoming this problem by applying Mean Field Theory (MFT) to a  hierachical representation of rotation resulting from the change of variables [4]  B-1  0 = Xb(Ob +0b), 0  [--0,01, (7)  b=0  where 0 = r/2B,  = (b + ) are the constant centers of the intervals, and 0  are fine-scale "vernier" variables. The X's are binary variables (so Xb G {0, 1}) that  satisfy the winner-take-all (WTA) constraint  X - 1.  The essential reason that this hierarchical representation of 0 has fewer spurious  local minima than the conventional analog representation is that the change of  variables also changes the connectivity of the network's state space: big jumps in 0  can be achieved by local variations of X-  3.2 Vernier optimization dynamics  Epoint can be transformed as (see [6, 4])   /point ( 0, t)  Avbl  b b  b  1Notation: Coordinate descent with 2-phase clock q,(t):  (8)   for clocked sum   for a clamped variable  x A for a set of variables to be optimized analytically  (v, u) s for Hopfield/Grossberg dynamics  E(x, Y)o) for coordinate descent/scent on x, then y, iterated if necessary. Nested  angle brackets correspond to nested loops.  Two-Dimensional Object Localization by Coarse-to-Fine Correlation Matching 989  06. 0%0.  oQ O   O oO  Figure 2: Shown here is an example of matching a 20-point model to a scene with  66.7% spurious outliers. The model is represented by circles. The set of square dots  is an instance of the model in the scene. All other dots are outliers. From left to  right are configurations at the annealing steps 1, 10, and 51, respectively.  1 sinh(eub)  MI/_,T  XbE( + v, tb) + 7 (uvb - log )  b b  +WqA(x,  (9)  Each bin-specific rotation angle v can be found by the following fixed point equa-  tions  tb = rn, a(x,-- RvbYa)  a  ub : --fyrnia(xi -- Rvbya -- tb)(R.+ya)  ia  1 o  v = (O + ) - u tanh(e0u) = g(u). (10)  The algorithm is illustrated in Figure 2.  4 Line Segment Matching  In many vision problems, representation of images by line segments has the ad-  vantage of compactness and subpixel accuracy along the direction transverse to the  line. However, such a representation of an object may vary substantially from image  to image due to occlusions and different illumination conditions.  4.1 Indexing points on line segements  The problem of matching line segments can be thought of as a point matching  problem in which each line segment is treated as a dense collection of points. Assume  now that both the scene and the model are represented by a set of line segments  respectively, {si} and {ma}. Both the model and the scene line segments are  990 Lu and Mjolsness  Figure 3' Approximating (t) by a sum of 3 Gaussians.  represented by their endpoints as si: (pi,Pti) and ma -- (qa, qt,), where pi,Pti,  and qa, q[ are the endpoints of the ith scene segment and the ath model segment,  respectively. The locations of the points on each scene segment and model segments  can be parameterized as  x i = si(u) =  Ya = ma(v) =  Pi + u(p'i - pi), u 6 [0, 1] and (11)  qa q- v(q'. --qa), V C [0, 1]. (12)  Now the model points and the scene points can be though of as indexed by i =  (i, u) and a = (a, v). Using this indexing, we have Ei  El lifO 1 dt and Ea c<  , I, fo 1 dv, where li = IIPi-p'll and l = Ilq,-q'll. The point matching objective  function (5) can be specialized to line segment matching as [5]  /01f01  Eseg(0, t): lila e 2211s'()-Rm*(v)-tl12du dv. (13)  ia  As a special case of point matching objective function, (13) can readily be trans-  formed to the vernier network previously developed for point matching problem.  4.2 Gaussian sum approximation  Note that, as in Figure 3 and [5],  1 if tC[0,1]  (t) _= 0 otherwise  a i (c - t)   m Aexp 2 rr (14)  k--1  where by numerical minimization of the Euclidean distance between these two func-  tions of t, the parameters may be chosen as A1 - Aa: 0.800673, A2: 1.09862,  ax = 3 = 0.0929032,  = 0.237033, cx = 1 - ca = 0.116807, and c = 0.5.  Using this approximation, each finite double integral in (13) can be replaced by   A/A, e 2 e --(-? 11sd')+Rm()-tl12dudv.(15)  C 2 TM  k,l=l  Each of these nine Gaussian integrals can be done exactly. Defining  vi,: si(c)- Roma(c,)- t (16)  i = p' - p,  = Ro(q'. - q), (17)  Two-Dimensional Object Localization by Coarse-to-Fine Correlation Matching 991  Figure 4: The model line segments, which are transformed with the optimal pa-  rameter found by the matching algorithm, are overlayed on the scene image. The  algorithm has successfully located the model object in the scene.  (15) becomes  a AArq  27rcr21ila  ^2 2  k,i=l V/( O'2 q- PiO'k)(O' q- la20') 2 2 ^  ^ 2 2  ' + x + x (18)  Pffk k i   was calculated by Garrett [2, 5]. From the Gaussian sum approximation, we get  a closed form objective function which can be readily optimized to give a solution  to the line segment matching problem.  5 Results and Discussion  The line segment matching algorithm described in this paper was tested on scenes  captured by a CCD camera producing 640 x 480 images, which were then processed  by an edge detector. Line segments were extracted using a polygonal approximation  to the edge images. The model line segments were extracted from a scene containing  a canonically positioned model object (Figure 4 left). They were then matched to  that extracted from a scene containing differently positioned and partially occluded  model object (Figure 4 r,lht). The result of matching is shown in Figure 5.  Our approach is based on a scale-space continuation scheme derived from an appli-  cation of Mean Field Theory to the match variables. It provides a means to avoid  trapping by local extrema and is more efficient than stochastic searches such as  simulated annealing. The estimation of location parameters based on continuously  improved "soft correspondences" and scale-space is often more robust than that  based on crisp (but usually inaccurate) correspondences.  The vernier optimization dynamics arises from an application of Mean Field The-  ory to a hierarchical representation of the rotation, which turns the original uncon-  strained optimization problem over rotation 0 into several constrained optimization  problems over smaller 0 intervals. Such a transformation results in a Hopfield-style  992 Lu and Mjolsness  Figure 5: Shows how the model line segments (gray) and the scene segments (black)  are matched. The model line segments, which are transformed with the optimal pa-  rameter found by the matching algorithm, are overlayed on the scene line segments  with which they are matched. Most of the the endpoints and the lengths of the line  segments are different. Furthermore, one long segment frequently corresponds to  several short ones. However, the matching algorithm is robust enough to uncover  the underlying rigid transformation from the incomplete and ambiguous data.  dynamics on rotation O, which effectively coordinates the dynamics of rotation and  translation during the optimization. The algorithm tends to find a roughly correct  translation first, and then tunes up the rotation.  6 Acknowledgements  This work was supported under grant N00014-92-J-4048 from ONR/DARPA.  References  [1] H. So Baird. Model-Based Image Matching Using Locationo The MIT Press,  Cambridge, Massachusetts, first edition, 84.  [2] C. Garrett, 1990. Private communication to Eric Mjolsness.  [3] W. E. L. Grimson and T. Lozano-Perez. Localizing overlapping parts by search-  ing the interpretation tree. IEEE Transaction on Pattern Analysis and Machine  Intelligence, 9:469-482, 1987.  [4] C.-P. Lu and E. Mjolsness. Mean field point matching by vernier network and  by generalized Hough transform. In World Congress on Neural Networks, pages  674-684, 1993.  [5] E. Mjolsness. Bayesian inference on visual grammars by neural nets that opti-  mize. In SPIE Science of Artificial Neural Networks, pages 63-85, April 1992.  [6] E. Mjolsness and W. L. Miranker. Greedy Lagrangians for neural net-  works: Three levels of optimization in relaxation dynamics. Technical Report  YALEU/DCS/TR-945, Yale Computer Science Department, January 1993.  [7] G. Stockman. Object recognition and localization via pose clustering. Computer  Vision, Graphics, and Image Processing, (40), 1987.  
Credit Assignment through Time:  Alternatives to Backpropagation  Yoshua Bengio *  Dept. Informatique et  Recherche Opdrationnelle  Universitd de Montrdal  Montreal, Qc H3C-3J7  Paolo Frasconi  Dip. di Sistemi e Informatica  Universit& di Firenze  50139 Firenze (Italy)  Abstract  Learning to recognize or predict sequences using long-term con-  text has many applications. However, practical and theoretical  problems are found in training recurrent neural networks to per-  form tasks in which input/output dependencies span long intervals.  Starting from a mathematical analysis of the problem, we consider  and compare alternative algorithms and architectures on tasks for  which the span of the input/output dependencies can be controlled.  Results on the new algorithms show performance qualitatively su-  perior to that obtained with backpropagation.  1 Introduction  Recurrent neural networks have been considered to learn to map input sequences to  output sequences. Machines that could efficiently learn such tasks would be useful  for many applications involving sequence prediction, recognition or production.  However, practical difficulties have been reported in training recurrent neural net-  works to perform tasks in which the temporal contingencies present in the in-  put/output sequences span long intervals. In fact, we can prove that dynamical  systems such as recurrent neural networks will be increasingly difficult to train with  gradient descent as the duration of the dependencies to be captured increases. A  mathematical analysis of the problem shows that either one of two conditions arises  in such systems. In the first case, the dynamics of the network allow it to reliably  store bits of information (with bounded input noise), but gradients (with respect  to an error at a given time step) vanish exponentially fast as one propagates them  *also, AT&T Bell Labs, Holmdel, NJ 07733  76 Bengio and Frasconi  backward in time. In the second case, the gradients can flow backward but the sys-  tem is locally unstable and cannot reliably store bits of information in the presence  of input noise.  In consideration of the above problem and the understanding brought by the theo-  retical analysis, we have explored and compared several alternative algorithms and  architectures. Comparative experiments were performed on artificial tasks on which  the span of the input/output dependencies can be controlled. In all cases, a dura-  tion parameter was varied, from T/2 to T, to avoid short sequences on which the  algorithm could much more easily learn. These tasks require learning to latch, i.e.  store bits of information for arbitrary durations (which may vary from example to  example). Such tasks cannot be performed by Time Delay Neural Networks or by  recurrent networks whose memories are gradually lost with time constants that are  fixed by the parameters of the network.  Of all the alternatives to gradient descent that we have explored, an approach based  on a probabilistic interpretation of a discrete state space, similar to hidden Markov  models (HMMs), yielded the most interesting results.  2 A Difficult Problem of Error Propagation  Consider a non-autonomous discrete-time system with additive inputs, such as a  recurrent neural network a with a continuous activation function:  at = M(at_)+ ut (1)  and the corresponding autonomous dynamics  at: M(at-) (2)  where M is a nonlinear map (which may have tunable parameters such as network  weights), and at G /n and ut G R r are vectors representing respectively the system  state and the external input at time t.  In order to latch a bit of state information one wants to restrict the values of the  system activity at to a subset S of its domain. In this way, it will be possible to  later interpret at in at least two ways: inside S and outside S. To make sure that at  remains in such a region, the system dynamics can be chosen such that this region  is the basin of attraction of an attractor X (or of an attractor in a sub-manifold or  subspace of at's domain). To "erase" that bit of information, the inputs may push  the system activity at out of this basin of attraction and possibly into another one.  In (Bengio, Simard, & Frasconi, 1994) we show that only two conditions can arise  when using hyperbolic attractors to latch bits of information in such a system.  Either the system is very sensitive to noise, or the derivatives of the cost at time t  with respect to the system activations a0 converge exponentially to 0 as t increases.  This situation is the essential reason for the difficulty in using gradient descent to  train a dynamical system to capture long-term dependencies in the input/output  sequences.  A first theorem can be used to show that when the state at is in a region where  IM'I > 1, then small perturbations grow exponentially, which can yield to a loss of  the information stored in the dynamics of the system:  Theorem 1 Assume x is a point of R'* such that there exists an open sphere U(x)  centered on x for which [M'(z) > i for all z 6 U(x). Then there exist y 6 U(x)  such that IIM(x)- M(y) l> llx- YlI-  Credit Assignment through Time: Alternatives to Backpropagation 77  A second theorem shows that when the state at is in a region where ]M[ < 1, the  gradients propagated backwards in time vanish exponentially fast:  Theorem 2 If the input ut is such that a system remains robustly latched  (]M(at)[ < 1) on attractor X after time O, then  -+ 0 as t  c.  Oao  See proofs in (Bengio, Simard, &; Frasconi, 1994). A consequence of these results  is that it is generally very difficult to train a parametric dynamical system (such  as a recurrent neural network) to learn long-term dependencies using gradient de-  scent. Based on the understanding brought by this analysis, we have explored and  compared several alternative algorithms and architectures.  3 Global Search Methods  Global search methods such as simulated annealing can be applied to this prob-  lem, but they are generally very slow. We implemented the simulated annealing  algorithm presented in (Corana, Marchesi, Martini, &; Ridella, 1987) for optimizing  functions of continuous variables. This is a "batch learning" algorithm (updating  parameters after all examples of the training set have been seen). It performs a cy-  cle of random moves, each along one coordinate (parameter) direction. Each point  is accepted or rejected according to the Metropolis criterion (Kirkpatrick, Gelatt,  & Vecchi, 1983). The simulated annealing algorithm is very robust with respect  to local minima and long plateaus. Another global search method evaluated in  our experiments is a multi-grid random search. The algorithm tries random points  around the current solution (within a hyperrectangle of decreasing size) and accepts  only those that reduce the error. Thus it is resistant to problems of plateaus but  not as much resistant to problems of local minima. Indeed, we found the multi-grid  random search to be much faster than simulated annealing but to fail on the parity  problem, probably because of local minima.  4 Time Weighted Pseudo-Newton  The time-weighted pseudo-Newton algorithm uses second order derivatives of the  cost with respect to each of the instantiations of a weight at different time steps to  try correcting for the vanishing gradient problem. The weight update for a weight  wi is computed as follows:  Awi(p) -- -- i x (3)  where Wit is the instantiation for time t of parameter wi, 1 is a global learning  rate and C(p) is he cos for paern p. In his way, each (temporal) contribution  Lo wi(p) is weighted by the inverse curvature wih respect to Wit. Like for the  pseudo-Newton algorithm of Becker and Le Cun (1988) we prefer using a diagonal  approximation of he Hessian which is cheap o compute and guaranteed o be  positive definite.  The constant p is introduced to preven w from becoming very large (when I a  is very small). We found [he performance of [his algori[hm [o be belief [han the  regular pseudo-Newton algori[hm, which is betlet than the simple s[ochasfic back-  propagation algorithm, bu all of these algorithms perform worse and worse as the  leng[h of the sequences is increased.  78 Bengio and Frasconi  Discrete Error Propagation  The discrete error propagation algorithm replaces sigmoids in the network by dis-  crete threshold units and attempts to propagate discrete error information back-  wards in time. The basic idea behind the algorithm is that for a simple discrete  element such as a threshold unit or a latch, one can write down an error propagation  rule that prescribes desired changes in the values of the inputs in order to obtain  certain changes in the values of the outputs. In the case of a threshold unit, such  a rule assumes that the desired change for the output of the unit is discrete (+2,  0 or -2). However, error information propagated backwards to such as unit might  have a continuous value. A stochastic process is used to convert this continuous  value into an appropriate discrete desired change. In the case of a self-loop, a clear  advantage of this algorithm over gradient back-propagation through sigmoid units  is that the error information does not vanish as it is repeatedly propagated back-  wards in time around the loop, even though the unit can robustly store a bit of  information. Details of the algorithm will appear in (Bengio, Simard, & Frasconi,  1994). This algorithm performed better than the time-weighted pseudo-Newton,  pseudo-Newton and back-propagation algorithms but the learning curve appeared  very irregular, suggesting that the algorithm is doing a local random search.  6 An EM Approach to Target Propagation  The most promising of the algorithms we studied was derived from the idea of  propagating targets instead of gradients. For this paper we restrict ourselves to  sequence classification. We assume a finite-state learning system with the state qt  at time t taking on one of n values. Different final states for each class are used  as targets. The system is given a probabilistic interpretation and we assume a  MarkovJan conditional independence model. As in HMMs, the system propagates  forward a discrete distribution over the n states. Transitions may be constrained  so that each state j has a defined set of successors $j.  P(qt- = jlutx-x; O)  P(qt = ilqt_ = j, ut;O)  tit  Figure 1' The proposed architecture  Learning is formulated as a maximum likelihood problem with missing data. Missing  variables, over which an expectation is taken, are the paths in state-space. The  Credit Assignment through Time: Alternatives to Backpropagation 79  EM (Expectation/Maximization) or GEM (Generalized EM) algorithms (Dempster,  Laird, 5; Rubin, 1977) can be used to help decoupling the influence of different  hypothetical paths in state-space. The estimation step of EM requires propagating  backward a discrete distribution of targets. In contrast to HMMs, where parameters  are adjusted in an unsupervised learning framework, we use EM in a supervised  fashion. This new perspective has been successful in training static models (Jordan  5; Jacobs, 1994).  Transition probabilities, conditional on the current input, can be computed by a  parametric function such as a layer of a neural network with softmax units. We pro-  pose a modular architecture with one subnetwork Afj for each state (see Figure 1).  Each subnetwork is feedforward, takes as input a continuous vector of features ut  and has one output for each successor state, interpreted as P(qt - i I qt-1 -- j, ut; 0),  (j = 1,..., n, i 6 Sj). 0 is a set of tunable parameters. Using a Markovian assump-  tion, the distribution over states at time t is thus obtained as a linear combination  of the outputs of the subnetworks, gated by the previously computed distribution:  t-1  P(qt=ilu;O)=P(qt_=jlu ;O)P(qt=ilqt_x=j, ut;O) (4)  J  where u is a subsequence of inputs from time i to t inclusively. The training  algorithm looks for parameters 0 of the system that maximize the likelihood L of  falling in the "correct" state at the end of each sequence:  * n'0)  L(O) - H P(qT. =qT. [ u ,  p  where p ranges over training sequences, Tp the length of the pt training sequence,  and q the desired state at time Tp.  An auxiliary function Q(O, 0 ) is constructed by introducing as hidden variables the  whole state sequence, hence the complete likelihood function is defined as follows:  L(O) = HP(q" l u ;o) (6)  p  and  Q(o, 0 k) = E[log Lc(0) 1O k] (7)  where at the k+l th EM (or GEM) iteration, 0 k+ is chosen to maximize (or increase)  the auxiliary function Q with respect to 0.  If the inputs are quantized and the subnetworks perform a simple look-up in a table  of probabilities, then the EM algorithm can be used, i.e., OQ(S,") _ 0 can be solved  00 --  analytically. If the networks have non-linearities, (e.g., with hidden units and a  softmax at their output to constrain the outputs to sum to 1), then one can use  the GEM algorithm (which simply increases Q, for example with gradient ascent)  or directly perform (preferably stochastic) gradient ascent on the likelihood.  An extra term was introduced in the optimization criterion when we found that in  many cases the target information would not propagate backwards (or would be  diffused over all the states). These experiments confirmed previous results indicat-  ing age. neral difficulty of training fully connected HMMs, with the EM algorithm  converging very often to poor local maxima of the likelihood. In an attempt to  understand better the phenomenon, we looked at the quantities propagated for-  ward and the quantities propagated backward (representing credit or blame) in the  80 Bengio and Frasconi  training algorithm. We found a diffusion of credit or blame occurring when the  forward maps (i.e. the matrix of transition probabilities) at each time step are such  that many inputs map to a few outputs, i.e., when the ratio of a small volume in  the image of the map with respect to the corresponding volume in the domain is  small. This ratio is the absolute value of the determinant of the Jacobian of the  map. Hence, using an optimization criterion that incorporates the maximization of  the average magnitude of the determinant of the transition matrices, this algorithm  performs much better than the other algorithms. Two other tricks were found to  be important to help convergence and reduce the problem of diffusion of credit.  The first idea is to use whenever possible a structured model with a sparse con-  nectivity matrix, thus introducing some prior knowledge about the state-space. For  example, applications of HMMs to speech recognition always rely on such structured  topologies. We could reduce connectivity in the transition matrix for the 2-sequence  problem (see next section for its definition) by splitting some of the nodes into two  subsets, each specializing on one of the sequence classes. However, sometimes it is  not possible to introduce such constraints, such as in the parity problem. Another  trick that drastically improved performance was to use stochastic gradient ascent in  a way that helps the training algorithm get out of local optima. The learning rate  is decreased when the likelihood improves but it is increased when the likelihood  remains fiat (the system is stuck in a plateau or local optimum).  As the results in the next section show, the performances obtained with this algo-  rithm are much better than those obtained with the other algorithms on the two  simple test problems that were considered.  7 Experimental Results  We present here results on two problems for which one can control the span of  input/output dependencies. The 2-sequence problem is the following: classify an  input sequence, at the end of the sequence, in one of two types, when only the first  N elements (N -- 3 in our experiments) of this sequence carry information about  the sequence class. Uniform noise is added to the sequence. For the first 6 methods  (see Tables i to 4), we used a fully connected recurrent network with 5 units (with  25 free parameters). For the EM algorithm, we used a 7-state system with a sparse  connectivity matrix (an initial state, and two separate left-to-right submodels of  three states each to model the two types of sequences).  The parity problem consists in producing the parity of an input sequence of 1's and  -1's (i.e., a i should be produced at the final output if and only if the number of  1's in the input is odd). The target is only given at the end of the sequence. For  the first 6 methods we used a minimal size network (1 input, i hidden, i output,  7 free parameters). For the EM algorithm, we used a 2-state system with a full  connectivity matrix.  Initial parameters were chosen randomly for each trial. Noise added to the sequence  was also uniformly distributed and chosen independently for each training sequence.  We considered two criteria: (1) the average classification error at the end of training,  i.e., after a stopping criterion has been met (when either some allowed number of  function evaluations has been performed or the task has been learned), (2) the  average number of function evaluations needed to reach the stopping criterion.  In the tables, "p-n" stands for pseudo-Newton. Each column corresponds to a value  of the maximum sequence length T for a given set of trials. The sequence length for  a particular training sequence was picked randomly within T/2 and T. Numbers  Credit Assignment through Time: Alternatives to Backpropagation 81  reported are averages over 20 or more trials.  8 Conclusion  Recurrent networks and other parametric dynamical systems are very powerful in  their ability to represent and use context. However, theoretical and experimental  evidence shows the difficulty of assigning credit through many time steps, which  is required in order to learn to use and represent context. This paper studies this  fundamental problem and proposes alternatives to the backpropagation algorithm  to perform such learning tasks. Experiments show these alternative approaches  to perform significantly better than gradient descent. The behavior of these algo-  rithms yields a better understanding of the central issue of learning to use context,  or assigning credit through many transformations. Although all of the alterna-  tive algorithms presented here showed some improvement with respect to standard  stochastic gradient descent, a clear winner in our comparison was an algorithm  based on the EM algorithm and a probabilistic interpretation of the system dynam-  ics. However, experiments on more challenging tasks will have to be conducted to  confirm those results. Furthermore, several extensions of this model are possible,  for example allowing both inputs and outputs, with supervision on outputs rather  than on states. Finally, similarly to the work we performed for recurrent networks  trained with gradient descent, it would be very important to analyze theoretically  the problems of propagation of credit encountered in training such Markov models.  Acknowledgements  We wish to emphatically thank Patrice Simard, who collaborated with us on the  analysis of the theoretical difficulties in learning long-term dependencies, and on  the discrete error propagation algorithm.  References  S. Becker and Y. Le Cun. (1988) Improving the convergence of back-propagation  learning with second order methods, Proc. of the 1988 Connectionist Models Sum-  mer School, (eds. Touretzky, Hinton and Sejnowski), Morgan Kaufman, pp. 29-37.  Y. Bengio, P. Simard, and P. Frasconi. (1994) Learning long-term dependencies  with gradient descent is difficult, IEEE Trans. Neural Networks, (in press).  A. Corana, M. Marchesi, C. Martini, and S. Ridella. (1987) Minimizing multimodal  functions of continuous variables with the simulated annealing algorithm, ACM  Transactions on Mathematical Software, vol. 13, no. 13, pp. 262-280.  A.P. Dempster, N.M. Laird, and D.B. Rubin. (1977) Maximum-likelihood from  incomplete data via the EM algorithm, J. of Royal Stat. Soc., vol. B39, pp. 1-38.  M.I. Jordan and R.A. Jacobs. (1994) Hierarchical mixtures of experts and the EM  algorithm, Neural Computation, (in press).  S. Kirkpatrick, C.D. Gelatt, and M.P. Vecchi. (1983) Optimization by simulated  annealing, Science 220, 4598, pp.671-680.  82 Bengio and Frasconi  Table 1: Final classification error for the 2-sequence problem wrt sequence length  5 10 20 50 100  back-prop 58 56 43 53 50  p-n 2 3 10 25 29  time-weighted p-n 0 0 9 34 14  multigrid 2 6 1 3 6  discrete err. prop. 6 16 29 23 22  simulated anneal. 6 0 7 4 11  EM 0 0 0 0 0  Table 2: p sequence presentations for the 2-sequence problem wrt sequence length  5 10 20 50 100  back-prop 2.9e3 3.0e3 2.9e3 3.0e3 2.8e3  p-n 5.1e2 1.1e3 1.9e3 2.6e3 2.5e3  time-weighted p-n 5.4e2 4.3e2 2.4e3 2.9e3 2.7e3  multigrid 4.1e3 5.8e3 2.5e3 3.9e3 6.4e3  discrete err. prop. 6.6e2 1.3e3 2.1e3 2.1e3 2.1e3  simulated anneal. 2.0e5 3.9e4 8.2e4 7.7e4 4.3e4  EM 3.2e3 4.0e3 2.9e3 3.2e3 2.9e3  Table 3: Final classification error for the parity problem wrt sequence length  3 5 10 20 50 100 500  back-prop 2 20 41 38 43  p-n 3 25 41 44 40 47  time-weighted p-n 26 39 43 44  multigrid 15 44 45  discrete err. prop. 0 0 0 5  simulated anneal. 3 10 0  EM 0 6 0 14 0 12  Table 4:  sequence presentations for the parity problem wrt sequence length  3 5 9 20 50 100 500  back-prop 3.6e3 5.5e3 8.7e3 1.6e4 1.1e4  p-n 2.5e2 8.9e3 8.9e3 7.7e4 1.1e4 1.1e5  time-weighted p-n 4.5e4 7.0e4 3.4e4 8.1e4  multigrid 4.2e3 1.5e4 3.1e4  discrete err. prop. 5.0e3 7.9e3 1.5e4 5.4e4  simulated anneal. 5.1e5 1.2e6 8.1e5  EM 2.3e3 1.5e3 1.3e3 3.2e3 2.6e3 3.4e3  
The  Statistical Mechanics of  k-Satisfaction  Scott Kirkpatrick*  Racah Institute for Physics and  Center for Neural Computation  Hebrew University  Jerusalem, 91904 Israel  kirk@fiz.huji.ac.il  G6za GySrgyl  Institute for Theoretical Physics  EStvSs University  1-1088 Puskin u. 5-7  Budapest, Hungary  gyorgyi@ludens.elte.hu,  Naftali Tishby and Lidror Troyansky  Institute of Computer Science and Center for Neural Computation  The Hebrew University of Jerusalem  91904 Jerusalem, Israel  {fishby, lidrort) @cs.huji.ac.il  Abstract  The satisfiability of random CNF formulae with precisely k vari-  ables per clause ("k-SAT") is a popular testbed for the performance  of search algorithms. Formulae have M clauses from N variables,  randomly negated, keeping the ratio a - M/N fixed. For k - 2,  this model has been proven to have a sharp threshold at a - 1  between formulae which are almost aways satisfiable and formulae  which are almost never satisfiable as N -- oc. Computer experi-  ments for k - 2, 3, 4, 5 and 6, (carried out in collaboration with  B. Selman of ATT Bell Labs.). show similar threshold behavior for  each value of k. Finite-size scaling, a theory of the critical point  phenomena used in statistical physics, is shown to characterize the  size dependence near the threshold. Annealed and replica-based  mean field theories give a good account of the results.  *Permanent address: IBM TJ Watson Research Center, Yorktown Heights, NY 10598  USA. (kirk@watson.ibm.com) Portions of this work were done while visiting the Salk  Institute, with support from the McDonnell-Pew Foundation.  439  440 Kirkpatrick, GyOrgyi, Tishby, and Troyansky  1 Large-scale computation without a length scale  It is increasingly possible to model the natural world on a computer. Condensed  matter physics has strategies to manage the complexities of such calculations, usu-  ally depending on a characteristic length. For example, molecules or atoms with  finite ranged in[eracfions can be broken down into weakly interac[ing smaller parts.  We may also use symmetry to identify natural modes of the system as a whole.  Even in the most difficult case, continuous phase transitions correlated over a wide  range of scales, the renormalization group provides a way of collapsing the problem  down to its "relevant" parts by providing a generator of behavior on all scales in  terms of the critical point itself.  But length scales are not much help in organizing another sort of large calculation.  Examples include large rule-based "expert systems" that model the particulars of  complex industrial processes. Digital Equipment, for example, has used a network  of three or more expert systems (originally called "R1/XCON") to check computer  orders for completeness and internal consistency, to schedule production and ship-  ping, and to aid a salesman to anticipate customers' needs. This very detailed set  of tasks in 1979 required 2 programmers and 250 rules to deal with 100 parts. In  the ten years described by Barker (1989), it grew 100X, employing 60 programmers  and nearly 20,000 rules to deal with 30,000 part numbers. 100X in ten years is only  moderate growth, and it would be valuable to understand how technical, social, and  business factors have constrained it.  Many important commercial and scientific problems without length scales are ready  for attack by computer modelling or automatic classification, and lie within a few  decades of XCON's size. Retail industries routinely track 105 - 106 distinct items  kept in stock. Banks, credit card companies, and specialized information providers  are building models of what 108 Americans have bought and might want to buy  next. In biology, human metabolism is currently described in terms of > 1000  substances coupled through > 10,000 reactions, and the data is doubling yearly.  Similarly, amino acid sequences are known for > 60,000 proteins.  A deeper understanding of the computational cost of these problems of order 1062  is needed to see which are practical and how they can be simplified. We study  an idealization of XCON-style resolution search, and find obvious collective effects  which may be at the heart of its computational complexity.  2 Threshold Phenomena and Random k-SAT  Properties of randomly generated combinatorial structures often exhibit sharp  threshold phenomena analogous to the phase transitions studied in condensed mat-  ter physics. Recently, thresholds have been observed in randomly generated Boolean  formulae. Mitchell et al. (1992) consider the k-satisfiability problem (k-SAT). An  instance of k-SAT is a Boolean formula in conjunctive normal form (CNF), i.e.,  a conjunction (logical AND) of disjunctions or clauses (logical ORs), where each  disjunction contains exactly k literals. A literal is a Boolean variable or, with equal  probability, its negation. The task is to determine whether there is an assignment  to the variables such that all clauses evaluate to true. Here, we will use N to denote  the number of variables and M for the number of clauses in a formula.  The Statistical Mechanics of k-Satisfaction 441  For randomly generated 2-SAT instances, it has been shown analytically that for  large N, when the ratio rt = M/N is less than 1 the instances are almost all  satisfiable, whereas for ratios larger than 1, almost all instances are unsatisfiable  (Chvtal and Reed 1992; Goerdt 1992). For k _> 3, a rigorous analysis has proven  to be elusive. Experimental evidence, however, strongly suggests a threshold with  a m 4.3 for 3SAT (Mitchell et al. 1992; Crawford and Auton 1993; Larrabee 1993).  One of the main reasons for studying randomly generated 3CNF formulae is for their  use in the empirical evaluation of combinatorial search algorithms. 3CNF formulae  are good candidates for the evaluation of such algorithms because determining their  satisfiability is an NP-complete problem. This also holds for larger values of k. For  k = 1 or 2, the satisfiability problem can be solved efficiently (Aspvall et al. 1979).  Despite the worst-case complexity, simple heuristic methods can usually determine  the satisfiability of random formulae. However, computationally challenging test  instances are found by generating formulae at or near the threshold (Mitchell et al.  1992). Cheeseman (1991) has made a similar observation of increased computational  cost for heuristic search at a boundary between two distinct phases or behaviors of  a combinatorial model.  We will provide a precise characterization of the N-dependence of the threshold  phenomena for k-SAT with k ranging from 2 to 6. We will employ finite size scaling,  a method from statistical physics in which direct observation of the width of the  threshold, or "critical region" of a transition is used to characterize the "universal"  behavior of quantities across the entire critical region, extending the analysis to  combinatorial problems in which N characterizes the size of the model observed.  For discussion of the applicability of finite-size scaling to systems without a metric,  see Kirkpatrick and Selman (1993).  Thresholds for 2SAT, 3SAT. 4SAT, 5SAT, and 6SAT  o. 6 .."'  0.4 i /  0.2  o ' ' ';! "";  o J'-' ..'?;' ,,1, ..:... ,  10 20 30  M/N  6O  Fig. 1: Fraction of unsatisfiable formulae for 2-, 3- 4-, 5- and 6-SAT.  442 Kirkpatrick, Gy6rgyi, Tishby, and Troyansky  3 Experimental data  We have generated extensive data on the satisfiability of randomly generated k-  CNF formulae with k ranging from 2 to 6. Fig. I shows the fraction of random  k-SAT formulae that is unsatisfiable as a function of the ratio, (. For example,  the left-most curve in Fig. I shows the fraction of formulae that is unsatisfiable for  random 2CNF formulae with 50 variables over a range of values of (.  Each data point was generated using 10000 randomly generated formulae, giving  1% accuracy. We used a highly optimized implementation of the Davis-Putnam  procedure (Crawford and Auton 1993). The procedure works best on formulae with  smaller k. Data was obtained for k = 2 on samples with N _ 500, for k: 3 with  N _ 100, and for k: 5 with N _ 40, all at comparable computing cost.  Fig. 1 (for N ranging from 10 to 50) shows a threshold for each value of k. Except  for the case k = 2, the curves cross at a single point and sharpen up with increasing  N. For k = 2, the intersections between the curves for the largest values of N seem  to be converging to a single point as well, although the curves for smaller N deviate.  The point where 50% of the formulae are unsatisfiable is thought to be where the  computationally hardest problems are found (Mitchell e! al. 1992; Cheeseman e! al.  1991). The 50% point lies consistently to the right of the scale-invariant point (the  point where the curves cross each other), and shifts with N.  There is a simple explanation for the rapid shift of the thresholds to the right  with increasing k. The probability that a given clause is satisfied by a random  input configuration is (2 } - 1)/2 } - (1 -2 -}) _ 7}. If we treat the clauses as  independent, the probability that all clauses are satisfied is 7 M = 7 v. We define  the entropy, $, per input as 1/N times the log2 of the expected number of satisfying  configurations, 2N3/ ' . S -- 1 + alog2(7}) --- 1 - Ot/Otann, and the vanishing of the  entropy gives an estimate of the threshold, identical to the upper bound derived  by several workers (see Franco (1983) and citations in Chvhtal (1992))' aa,m =  -(log2(1 - 2-})) -  (/n2)2 }. This is called an annealed estimate for ac, because  it ignores the interactions between clauses, just as annealed theories of materials  (see Mdzard 1986) average over many details of the disorder. We have marked aa,,,,  with an arrow for each k in the figures, and tabulate it in Table 1.  4 Results of Finite-Size Scaling Analysis  From Fig. 1, it is clear that the threshold "sharpens up" for larger values of N.  Both the threshold shift and the increasing slope in the curves of Fig. 1 can be  accounted for by finite size scaling. (See Stauffer and Aharony (1992) or Kirkpatrick  and Swendsen (1985).) We plot the fraction of samples unsatisfied against the  dimensionless rescaled variable,  y = N1/'(o -  Values for rc and y must be derived from the experimental data. First a is  determined as the crossing point of the curves for large N in Fig. 1. Then y is  determined to make the slopes match up through the critical region. In Fig. 2 (for  k = 3) we find that these two parameters capture both the threshold shift and the  steepening of the curves, using a = 4.17 and y = 1.5. We see that F, the fraction  The Statistical Mechanics of k-Satisfaction 443  O4  - -1 O  1 2 3 4 5  scaled crossover funcbon, all SAT models  ' ./' k=3, N = 100 .--  o.e ......, ... : k, N = SOO --  annealed limit - - -  o .7 x,. , "" ,-  0.6  0.2  Y  Fig. 2' Rescaled 3-SAT data using tc = 4.17, u = 1.5.  Fig. 3: Rescaled data for 2-, 3-, 4-, 5-, and 6-SAT approach annealed limit.  of unsatisfiable formulae, is given by F(N, )= f(y), where the invariant function,  f, is that graphed in Fig. 2.  A description of the 50% threshold shift follows immediately. If we define y/ by  f(y/) = 0.5, then ets0 = etc(1 -t- yN-1/). From Fig. 2 we find that cs0  4.17  3.1N -2/3. Crawford and Auton (1993) fit their data on the 50% point as a function  of N by arbitrarily assuming that the leading correction will be O(1/N). They  obtain eta0 = 4.24 + 6IN. However, the two expressions differ by only a few percent  as N ranges from 10 to c.  We also obtained good results in rescaling the data for the other values of k. In  Table i we give the critical parameters obtained from this analysis. The error  bars are subjective, and show the range of each parameter over which the best  fits were obtained. Note that u appears to be tending to 1, and aann becomes  an increasingly good approximation to etc as k increases. The success of finite-size  scaling with different powers, u, is strong evidence for criticality, i.e., diverging  correlations, even in the absence of any length.  Finally, we found that all the crossovers were similar in shape. In fact, combining  the various rescaled curves in figure 3 shows that the curves for k _ 3 all coincide  in the vicinity of the 50% point, and tend to a limiting form, which can be obtained  by extending the annealed arguments of the previous section. If we define  then the probability that a formula remains unsatisfied for all 2 N configurations is  f(y) = (1 - 2-(Y''"'"+'')) 2'  -:-"  The curve for k = 2 is similar in form, but shifted to the right from the other ones.  444 Kirkpatrick, Gy6rgyi, Tishby, and Troyansky  O a n n Ol 2 Oz c O?  2.41 1.38 1.0 2.25 2.6q-.2  5.19 4.25 4.17-4-.03 0.74 1.5-4-.1  10.74 9.58 9.75-4-.05 0.67 1.25-4-.05  21.83 20.6 20.9+.1 0.71  44.01 42.8 43.2+.2 0.69  Table 1' Critical parameters for random k-SAT.  5 Outline of Statistical Mechanics Analysis  Space permits only a sketch of our analysis of this model. Since the N inputs are  binary, we may represent them as a vector, X, of Ising spins:  X-- {xi: +l} i: 1,...N.  Each random formula, ., can be written as a sum of its M clauses, Cj,  M  where  j=l  k  Cj = II(1- JJtX)/2.  I=1  where the vector, jj,l, has only one non-zero element, q-I, at the input which  it selects. . evaluates to the number of clauses left unsatisfied by a particular  configuration. It is natural to take the value of ' to be the energy. The partition  function,  Z = tr{:)e : = tr{:) H ecj'  where /3 is the inverse of a fictitious temperature, factors into contributions from  each clause. The "annealed" approximation mentioned above consists simply of  taking the trace over each subproduct individually, neglecting their interactions. In  this construction, we expect both energy and entropy, $, to be extensive quantities,  that is, proportional to N. Fig. 4 shows that this is indeed the case for S(rt). The  lines in Fig. 4 are the annealed predictions S(rt, k) = 1 - alefarm. Expressions for  the energy can also be obtained from the annealed theory, and used to compare  the specific heat observed in numerical experiments with the simple limit in which  the clauses do not interact. This gives evidence supporting the identification of  the unsatisfied phase as a spin glass. Finally, a plausible phase diagram for the  spin glass-like "unsatisfied" phase is obtained by solving for S(T) = 0 at finite  temperatures.  To perform the averaging over the random clauses correctly requires introducing  replicas (see Mdzard 1986), which are identical copies of the random formula, and  defining q, the overlap between the expectation values of the spins in any two  replicas, as the new order parameter. The results appear to be capable of accounting  The Statistical Mechanics of k-Satisfaction 445  for the difference between experiment and the annealed predictions at finite k. For  example, an uncontrolled approximation in which we consider just two replicas  gives the values of as in Table 1, and accounts rather closely for the average overlap  found experimentally between pairs of lowest energy states, as shown in Fig. 5. The  2-replica theory gives q as the solution of  ,(k, q) = 2k(1 + q)k-l(4} - 2 }+ + (1 - q)})/ln((1 + q)(1 - q))  for q as a function of a. This gives the lines in Fig 5. We defined a2 (in Table 1)  as the point of inflection, or the maximum in the slope of  0 6  0 4  0 2  0  08  07  0.  05  03  02  01  0  0  Fig. 4: Entropy as function of a for k = g, 3, 4, and 5.  Fig. 5: q calculated from g-replica theory vs experimental ground state overlaps.  Arrows pointing up are Oann, arrows pointing down are ag.  6 Conclusions  We have shown how finite size scaling methods from statistical physics can be used  to model the threshold in randomly generated k-SAT problems. Given the good fit  of our scaling analysis, we conjecture that this method can also give useful models  of phase transitions in other combinatorial problems with a control parameter.  Several authors have attempted to relate NP-hardness or NP-completeness to the  characteristics of phase transitions in models of disordered systems. Fu and Ander-  son (see Fu 1989) have proposed spin glasses (magnets with 2-spin interactions of  random sign) as having inherent exponential complexity. Huberman and colleagues  (see Clearwater 1991) were first to focus on the diverging correlation length seen  at continuous phase transitions as the root of computational complexity. In fact,  both effects can play important roles, but are not sufficient and may not even be  necessary.  There are NP-complete problems (e.g. travelling salesman, or max-clique) which  lack a phase boundary at which "hard problems" cluster. Percolation thresholds  are phase transitions, yet the cost of exploring the largest cluster never exceeds  N steps. Exponential search cost in k-SAT comes from the random signs of the  inputs, which require that the space be searched repeatedly. Note that a satisfying  446 Kirkpatrick, Gy6rgyi, Tishby, and Troyansky  input configuration in 2-SAT can be determined, or its non-existence proven, in  polynomial time, because it can be reduced to a percolation problem on a random  directed graph (Aspvall 1979). The spin glass Hamiltonians studied by Fu and  Anderson have a form close to our 2-SAT formulae, but the questions studied are  different. Finding an input configuration which falsities the minimum number of  clauses is like finding the ground state in a spin glass phase, and is NP-hard when   > c, even for k = 2. Therefore, if both diverging correlations (diverging in size  if no lengths are defined) and random sign or "spin-glass" effects are present, we  expect a local search like Davis-Putnam to be exponentially difficult on average.  But these characteristics do not imply NP-completeness.  7 References  Aspvail, B., Plass, M.F., and Tarjan, R.E. (1979) A linear-time algorithm for testing the  truth of certain quantified Boolean formulae. Inform. Process. Let., Vol. 8., 1979,  289-314.  Barker, V. E., and O'Connor, D. (1989). Commun. Assoc. for Computing Machinery,  32(3), 1989, 298-318.  Cheeseman, P., Kanefsky, B., and Taylor, W.M. (1991). Where the really hard problems  are. Proceedings IJCAI-91, 1991, 163-169.  Clearwater, S.H., Huberman B.A., Hogg, T. (1991) Cooperative Solution of Constraint  Satisfaction Problems. Science, Vol. 254, 1991, 1181-1183  Crawford, J.M. and Auton L.D. (1993). Experimental Results on the Crossover Point in  Satisfiability Problems. Proc. of AAAI-93, 1993.  Chvgtal, V. and Reed, B. (1992) Mick Gets Some: The Odds are on his Side. Proc. of  STOC, 1992, 620-627.  u, Y. (1989). The Uses and Abuses of Statistical Mechanics in Computational Com-  plexity. in Lectures in the Sciences of Complexity, ed. D. Stein, pp. 815-826,  Addison-Wesley, 1989.  Franco, J. and Paull, M. (1988). Probabilistic Analysis of the Davis-Putnam Procedure  for solving the Satisfiability Problem. Discrete Applied Math., Vol. 5, 77-87, 1983.  Goerdt, A. (1992). A threshold for unsatisfiability. Proc. 17th Int. Syrup. on the Math.  Foundations of Comp. Sc., Prague, Czechoslovakia, 1992.  Kirkpatrick, S. and Swendsen, R.H. (1985). Statistical Mechanics and Disordered Sys-  tems. CACM, Vol. 28, 1985, 363-373.  Kirkpatrick, S., and Selman, B. (1993), submitted for publication.  Larrabee, T. and Tsuji, Y. (1993) Evidence for a Satisfiability Threshold for Random  3CNF Formulas, Proc. of the AAAI Spring Symposium on AI and NP-hard prob-  lems, Palto Alto, CA, 1993.  Mdzard, M., Parisi, G., Virasoro, M.A. (1986). Spin Glass Theory and Beyond, Singapore:  World Scientific, 1986.  Mitchell, D., Selman, B., and Levesque, H.J. (1992) Hard and Easy Distributions of SAT  problems. Proc. of AAAI-9, 1992, 456-465.  Stauffer, D. and Aharony, A. (1992) Introduction to Percolation Theory. London: Taylor  and Francis, 1992. See especially Ch. 4.  
Bayesian Backpropagation Over I-O Functions  Rather Than Weights  David H. Wolpert  The Santa Fe Institute  1660 Old Pecos Trail  Santa Fe, NM 87501  Abstract  The conventional Bayesian justification of backprop is that it finds the  MAP weight vector. As this paper shows, to find the MAP i-o function  instead one must add a correction term to backprop. That term biases one  towards i-o functions with small description lengths, and in particular fa-  vors (some kinds of) feature-selection, pruning, and weight-sharing.  INTRODUCTION  In the conventional Bayesian view of backpropagation (BP) (Buntine and Weigend, 1991;  Nowlan and Hinton, 1994; MacKay, 1992; Wolpert, 1993), one starts with the "likelihood"  conditional distribution P(training set = t I weight vector w) and the "prior" distribution  P(w). As an example, in regression one might have a "Gaussian likelihood", P(t I w) o,  exp[-z2(w, 0] -- IIi exp [-{net(w, tx(i)) - ty(i)} 2 / 202] for some constant c. (tx(i) and ty(i)  are the successive input and output values in the training set respectively, and net(w, .) is  the function, induced by w, taking input neuron values to output neuron values.) As another  example, the "weight decay" (Gaussian) prior is P(w) o, exp(.a(w2)) for some constant a.  Bayes' theorem tells us that P(w I 0 o P(t I w) P(w). Accordingly, the most probable weight  given the data - the "maximum a postefiofi" (MAP) w - is the mode over w of P(t I w) P(w),  which equals the mode over w of the "cost function" L(w, 0 -- ln[P(t I w)] + ln[P(w)]. So  for example with the Gaussian likelihood and weight decay prior, the most probable w giv-  en the data is the w minimizing Z2(w, 0 + ct w2. Accordingly BP with weight decay can be  viewed as a scheme for trying to find the function from input neuron values to output neu-  ron values (i-o function) induced by the MAP w.  200  Bayesian Backpropagation over I-O Functions Rather Than Weights 201  One peculiar aspect of this justification of weight-decay BP is the fact that rather than the  i-o function induced by the most probable weight vecwr, in practice one would usually pre-  fer to know the most probable i-o function. (In few situations would one care more about a  weight vector than about what that weight vector parameterizes.) Unfortunately, the differ-  ence between these two i-o functions can be large; in general it is not true that "the most  probable output corresponds to the most probable parameter" (Denker and LeCun, 1991).  This paper shows that to find the MAP i-o function rather than the MAP w one adds a "cor-  rection term" to conventional BP. That term biases one towards i-o functions with small  description lengths, and in particular favors feature-selection, pruning and weight-sharing.  In this that term constitutes a theoretical justification for those techniques.  Although cast in terms of neural nets, this paper's analysis applies to any case where con-  vention is to use the MAP value of a parameter encoding Z to estimate the value of Z.  2  BACKPROP OVER I-O FUNCTIONS  Assume the net's architecture is fixed, and that weight vectors w live in a Euclidean vector  space W of dimension IWl. Let X be the set of vectors x which can be loaded on the input  neurons, and O the set of vectors o which can be read off the output neurons. Assume that  the number of elements in X (IXl) is finite. This is always the case in the real world, where  measuring devices have finite accuracy, and where the computers used to emulate neural  nets are finite state machines. For similar reasons O is also finite in practice. However for  now assume that O is very large and "fine-grained", and approximate it as a Euclidean vec-  tor space of dimension IOI. (This assumption usually holds with neural nets, where output  values are treated as real-valued vectors.) This assumption will be relaxed later.  Indicate the set of functions taking X to O by . (net(w, .) is an element of .) Any     is an (IXl x IOI)-dimensional Euclidean vector. Accordingly, densities over W are related  to densities over  by the usual rules for transforming densities between IWl-dimensional  and (IXl x IOI)-dimensional Euclidean vector spaces. There are three cases to consider:  1) IWl < IXl IOI. In general, as one varies over all w's the corresponding i-o func-  tions net(w, .) map out a sub-manifold of  having lower dimension than .  2) IWl > IXl IOI. There are an infinite number of w's corresponding to each .  3) IWl - IXl IOI. This is the easiest case to analyze in detail. Accordingly I will deal  with it first, deferring discussion of cases one and two until later.  With some abuse of notation, let capital letters indicate random variables and lower case  letters indicate values of random variables. So for example w is a value of the weight vector  random variable W. Use 'p' to indicate probability densities. So for example Pn, IT( I t) is  the density of the i-o function random variable , conditioned on the training set random  variable T, and evaluated at the values  =  and T = t.  In general, any i-o function not expressible as net(w, .) for some w has zero probability. For  the other i-o functions, with 8(.) being the multivariable Dirac delta function,  pn,(net(w, .)) = ldw' Pw(W') 8(net(w', .) - net(w, .)).  When the mapping  = net(W, .) is one-to-one, we can evaluate equation (1) to get  pT(net(w, .)l t) = PwiT(W I t) / Jl,,w(W),  where Jn,,w(W) is the Jacobian of the W --)  mapping:  (1)  (2)  202 Wolpert  Jtl,,w(W) -- I det[/I i //}Wj ](w) I= I det[/} net(w, -)i //}wj ] I. 0)  "net(w, .)i"means the i'th component of the i-o function net(w, .). "net(w, x)" means the  vector o mapped by net(w, .) from the input x, and "net(w, X)k"is the k'th component of o.  So the "i" in "net(w, .)i" refers to a pair of values {x, k}. Each matrix value  //)wj is the  partial derivative of net(w, X)k with respect to some weight, for some x and k. Jq,,w(w) can  be rewduen as der it2 [gij(w)], where gij(w) -- Ek [( / wi) OR / wj)] is the metric of  the W -->  mapping. This form of Jtl,.w(w) is usually more difficult to evaluate though.  Unfortunately, { = net(w, .) is not one-to-one; where Jn,,w(W) ,e 0 the mapping is locally  one-to-one, but there are global symmetries which ensure that more than one w corre-  sponds to each {. (Such symmetries arise from things like permuting the hidden neurons  or changing the sign of all weights leading into a hidden neuron - see (Fefferman, 1993)  and references therein.) To circumvent this difficulty we must make a pair of assumptions.  To begin, restrict attention to Winj, those values w of the variable W for which the Jaco-  bian is non-zero. This ensures local injectivity of the map between W and . Given a par-  ticular w  Winj, let k be the number of w'  Winj such that net(w, .) = net(w', .). (Since  net(w, .) = net(w, .), k > 1.) Such a set of k vectors form an equivalence class, {w}.  The first assumption is that for all w  Winj the size of {w} (i.e., k) is the same. This will  be the case if we exclude degenerate w (e.g., w's with all first layer weights set to 0). The  second assumption is that for all w' and w in the same equivalence class, PwlD (w I d) =  PwI) (w' I d). This is usually the case. (For example, start with w' and telabel hidden neu-  rons to get a new w  {w'}. If we assume the Gaussian likelihood and prior, then since  neither differs for the two w's the weight-posterior is also the same for the two w's.) Given  these assumptions, plT(net(w, .) I t) = k PWlT(W I t) / J.w(W). So rather than minimize the  usual cost function, L(w, t), to find the MAP  BP should minimize L'(w, 0 -= L(w, t) +  In[ Jn,,w(W) ]. The In[ Jn,.w(W) ] term constitutes a correction term to conventional BP.  One should not confuse the correction term with the other quantities in the neural net liter-  ature which involve partial derivative matrices. As an example, one way to characterize  the "quality" of a local peak w' of a cost function involves the Hessian of that cost function  (Buntine and Weigend, 1991). The correction term doesn't direcfiy concern the validity of  such a Hessian-based quality measure. However it does concern the validity of some  implementations of such a measure. In particular, the correction term changes the location  of the peak w'. It also suggests that a peak's quality be measured by the Hessian of  L'(w', 0 with respect to , rather than by the Hessian of L(w', 0 with respect to w. (As an  aside on the subject of Hessians, note that some workers incorrectly use Hessians when  they attempt to evaluate quantities like output-variances. See (Wolpert, 1994).)  If we stipulate that the PqT({ I t) one encounters in the real world is independent of how  one chooses to parameterize , then the probability density of our parameter must depend  on how it gets mapped to . This is the basis of the correction term. As this suggests, the  correction term won't arise if we use non-pqT(  I O-based estimators, like maximum-like-  lihood estimators. (This is a basic difference between such estimators and MAP estimators  with a uniform prior.) The correction term is also irrelevant if it we use an MAP estimate  but J,t,,w(W) is independent of w (as when net (w, .) depends linearly on w). And for non-  linear net(w, .), the correction term has no effect for some non-MAP-based ways to apply  Bayesianism to neural nets, like guessing the posterior average  (Neal, 1993):  Bayesian Backpropagation over I-O Functions Rather Than Weights 203  E(, 10 -- $d paT( I 0  = ldw PWlT(W I 0 net(w, .),  (4)  so one can calculate E(d> I t) by working in W, without any concern for a correction term.  (Loosely speaking, the Jacobian associated with changing integration variables cancels the  Jacobian associated with changing the argument of the probability density. A formal deri-  vation - applicable even when IWl ,e IXI x IOI - is in the appendix of (Wolpert, 1994).)  One might think that since it's independent of t, the correction term can be absorbed into  Pw(W). Ironically, it is precisely because quantities like E(d> I t) aren't affected by the cor-  rection term that this is impossible: Absorb the correction term into the prior, giving a new  prior p*w(W)  d x pw(w) x Jn,,w(w) (asterisks refers to new densities, and d is a normal-  ization constan 0. Then p*n, iT(net(w, .) I t) = PWlT(W I 0. So by redefining what we call the  prior we can justify use of conventional uncorrected BP; the (new) MAP { corresponds to  the w minimizing L(w, 0. However such a redefinition changes E(d> I t) (amongst other  things): ld{ P*n, IT({ It) q = /dw p*WT(W I t) net(w, .) , ldw Pwrr(w I t) net(w, .) =  ld{ PqT({ I 0 {. So one can either modify BP (by adding in the correction term) and leave  E(d> I t) alone, or leave BP alone but change E(d> I 0; one can not leave both unchanged.  Moreover, some procedures involve both prior-based modes and prior-based integrals, and  therefore are affected by the correction term no mauer how pw(w) is redefined. For exam-  ple, in the evidence procedure (Wolpert, 1993; MacKay, 1992) one fixes the value of a  hyperparameter F (e.g., ct from the introduction) to the value y maximizing PrlT(Y I 0.  Next one find the value s' maximizing PSlT, r' (s' I t, y) for some variable S. Finally, one  guesses the { associated with s'. Now it's hard to see why one should use this procedure  with S = W (as is conventional) rather than with S = d>. But with S = d> rather than W, one  must factor in the correction term when calculating PSIT, F (S I t, ), and therefore the  guessed { is different from when S = W. If one tries to avoid this change in the guessed {  by absorbing the correction term into the prior Pwlr(w I ht), then Prl T(h t I 0 - which is given  by an integral involving that prior - changes. This in turn changes y, and therefore the  guessed { again is different. So presuming one is more directly interested in d> rather than  W, one can't avoid having the correction term affect the evidence procedure.  It should be noted that calculating the correction term can be laborious in large nets. One  should bear in mind the determinant-evaluation tricks mentioned in (Bunfine and  Weigend, 1991), as well as others like the identity In[ Jn,,w(w) ] = Tr(ln[  //}wj ]) -_-  Tr(ln*[ /wj ]), where In*(.) is In(.) evaluated to several orders.  3  EFFECTS OF THE CORRECTION TERM  To illustrate the effects of the correction term, consider a perceptton with a single output  neuron, N input neurons and a unary input space: o = tanh(w  x), and x always consist of  a single one and N - 1 zeroes. For this scenario {i //}vj is an N x N diagonal matrix, and  In[ Jn,.w(w) ] TM -2 EkN=l In[ cosh(wk) ]. Assume the Gaussian prior and likelihood of the  introduction, and for simplicity take 2o 2 = 1. Both L(w, t) and L'(w, 0 are sums of terms  each of which only concerns one weight and the corresponding input neuron. Accord-  ingly, it suffices to consider just the i'th weight and the corresponding input neuron.  Let x(i) be the input vector which has its 1 in neuron i. Let oj(i) be the output of the j'th  of the pairs in the training set with input x(i), and m i the number of such pairs. With a = 0  204 Wolpert  (no weight decay), L(w, t) = X2(t, w), which is minimized by w' i = tanh '1 [ ji 1 oj(i) / mi].  If we insWad try to minimize Z2(t, w) + Jkw(w) though, then for low enough m i (e.g., m i  = 1), we find that there is no minimum. The correction term pushes w away from 0, and  for low enough m i the likelihood isn't strong enough to counteract this push.  o o  Figures 1 through 3: Train using unmodified BP on training set t, and feed input x into  the resultant net. The horizontal axis gives the output you get. If t and x were still used  but training had been with modified BP, the output would have been the value on the  vertical axis. In succession, the three figures have c = .6, .4, .4, and m = 1, 4, 1.  i I  , , I . , . . I  0 1  Figure 3.  zo  0 S 10  Figure 4: The horizontal axis is Iwi I. The top  curve depicts the weight decay regularizer,  cwi 2, and the bottom curve shows that regu-  larizer modified by the correction term.  = .2.  When weight-decay is used though, modified BP finds a solution, just like unmodified BP  does. Since the correction term "pushes out" w, and since tanh(.) grows with its argument,  a  found by modified BP has larger (in magnitude) values of o than does the correspond-  ing  found by unmodified BP. In addition, unlike unmodified BP, modified BP has multi-  ple extrema over certain regimes. All of this is illustrated in figures (1) through (3), which  graph the value of o resulting from using modified BP with a particular training set t and  input value x vs. the value of o resulting from using unmodified BP with t and x. Figure  (4) depicts the wi-dependences of the weight decay term and of the weight-decay term  plus the correction term. (When there's no data, BP searches for minima of those curves.)  Now consider multi-layer nets, possibly with non-unary X. Denote a vector of the compo-  Bayesian Backpropagation over I-O Functions Rather Than Weights 205  nents of w which lead from the input layer into hidden neuron K by wtg]. Let x' be the  input vector consisting of all O's. Then  tanh(wtg ]  x') / wj = 0 for any j, w, and K, and  for any w, there is a row of  / vj which is all zeroes. This in turn means that Jn,,w(W) =  0 for any w, which means that Win j is empty, and PIT( I t) is independent of the data t.  (Intuitively, this problem arises since the o corresponding to x' can't vary with w, and  therefore the dimension of  is less than IWl) So we must forbid such an all-zeroes x'. The  easiest way to do this is to require that one input neuron always be on, i.e., introduce a bias  unit. An alternative is to redefine  to be the functions from the set {X - (0, 0, ..., 0)} to O  rather than from the set X to O. Another alternative, appropriate when the original X is the  set of all input neuron vectors consisting of 0's and 1 's, is to instead have input neuron val-  ues  {z  0, 1}. (In general z  -1 though; due to the symmetries of the tanh, for many  architectures z = -1 means that two rows of  / wj are identical up to an overall sign,  which means that Jn,,w(W) = 0.) This is the solution implicitly assumed from now on.  J,w(w) will be small - and therefore p(net(w, .)) will be large - whenever one can make  large changes to w without affecting  = net(w, .) much. In other words, p(net(w, .)) will  be large whenever we don't need to specify w very accurately. So the correction factor  favors those w which can be expressed with few bits. In other words, the correction factor  enforces a sort of automatic MDL (Rissanen, 1986; Nowlan and Hinton, 1994).  More generally, for any multi-layer architecture there are many "singular weights" wsi n   Win j such that Jn,.w(Wsin) is not just small but equals zero exactly. pw(w) must compen-  sate for these singularities, or the peaks of PT({ I t) won't depend on t. So we need to  have Pw(W) --> 0 as w --> wsi n. Sometimes this happens automatically. For example often  Wsin includes infinite-valued w's, since tanh'(oo) = 0. Because pw(oo) = 0 for the weight-  decay prior, that prior compensates for the infmite-w singularities in the correction tenn.  For other wsin there is no such automatic compensation, and we have to explicitly modify  pw(w) to avoid singularities. In doing so though it seems reasonable to maintain a "bias"  towards the wsin, that Pw(W) goes to zero slowly enough so that the values pn,(net(w, .))  are "enhanced" for w near wsi n. Although a full characterization of such enhanced w is not  in hand, it's easy to see that they include certain kinds of pruned nets (Hassibi and Stork,  1992), weight-shared nets (Nowlan and Hinton, 1994), and feature-selected nets.  To see that (some kinds of) pruned nets have singular weights, let w* be a weight vector  with a zero-valued weight coming out of hidden neuron K. By (1) pn,(net(w*, .)) =  ldw' pw(w) 5(net(w', .) - net(w*, .)). Since we can vary the value of each weight w* i lead-  ing into neuron K without affecting net(w*, .), the integral diverges. So w* is singular;  removing a hidden neuron results in an enhanced probability. This constitutes an a priori  argument in favor of trying to remove hidden neurons during training.  This argument does not apply to weights leading into a hidden neuron; Jq,,w(w) treats  weights in different layers differently. This fact suggests that however Pw(W) compen-  sates for the singularities in Jn,,w(W), weights in different layers should be treated differ-  ently by pw(w). This is in accord with the advice given in (MacKay, 1992).  To see that some kinds of weight-shared nets have singular weights, let w' be a weight vec-  tor such that for any two hidden neurons K and K' the weight from input neuron i to K  equals the weight from i to K', for all input neurons i. In other words, w is such that all hid-  206 Wolpert  den neurons compute identical functions of x. (For some architectures we'll actually only  need a single pair of hidden neurons to be identical.) Usually for such a situation there is a  pair of columns of the matrix b0i / wj which are exactly proportional to one another. (For  example, in a 3-2-1 architecture, with X = {z, 1} 3, IWl -- IXl x IOI -- 8, and there are four  such pairs of columns.) This means that Jl,.w(W') = 0; w' has an enhanced probability, and  we have an a priori argument in favor of trying to equate hidden neurons during training.  The argument that feature-selected nets have singular weights is architecture-dependent,  and there might be reasonable architectures for which it fails. To illustrate the argument,  consider the 3-2-1 architecture. Let Xl(k) and x2(k) with k = { 1, 2, 3} designate three dis-  tinct pairs of input vectors. For each k have Xl(k ) and x2(k) be identical for all input neu-  rons except neuron A, for which they differ. (Note there are four pairs of input vectors  with this property, one for each of the four possible patterns over input neurons B and C.)  Let w' be a weight vector such that both weights leaving A equal zero. For this situation  net(w', Xl(k)) = net(w', x2(k)) for all k. In addition  net(w, Xl(k)) / wj =   net(w, x2(k)) / wj for all weights wj except the two which lead out of A. So k = 1 gives  us a pair of rows of the matrix i / wj which are identical in all but two entries (one row  for Xl(k ) and one for x2(k)). We get another such pair of rows, differing from each other in  the exact same two entries, for k = 2, and yet another pair for k = 3. So there is a linear  combination of these six rows which is all zeroes. This means that Jl,.w(W') = 0. This con-  stitutes an a priori argument in favor of trying to remove input neurons during training.  Since it doesn't favor any pw(w), the analysis of this paper doesn't favor any p({). How-  ever when combined with empirical knowledge it suggests certain Pn,({). For example,  there are functions g(w) which empirically are known to be good choices for p(net(w, .))  (e.g., g(w) oexp[ctw2]). There are usually problems with such choices of p({) though.  For example, these g(w) usually make more sense as a prior over W than as a prior over  tb, which would imply p(net(w, .)) = g(w) / J,w(W). Moreover it's empirically true that  enhanced w should be favored over other w, as advised by the correction tenn. So it makes  sense to choose a compromise between g(w) and g(w) / J,w(w). An example is p() o  g(w) / [ 1 + tanh02 X Jo, w(W)) ] for two hyperparameters kl > 0 and k, 2 > 0.  4 BEYOND THE CASE OF BACKPROP WITH IWl-- IXl IOl  When O does not approximate a Euclidean vector space, elements of  have probabilities  rather than probability densities, and P(O I t) = $dw PwiT(W I t) J(net(w, .), ), ((., .) being  a Kronecker delta function). Moreover, if O is a Euclidean vector space but Wl > IXl IOI,  then again one must evaluate a difficult integral;  = net(W, .) is not one-to-one so one  must use equation (1) rather than (2). Fortunately these two situations are relatively rare.  The final case to consider is IWl < IXl IOI (see section two). Let S(W) be the surface in   which is the image (under net(W, .)) of W. For all  po(O) is either zero (when   S(W))  or infinite (when  e S(W)). So as conventionally defined, "MAP " is not meaningful.  One way to deal with this case is to embed the net in a larger net, where that larger net's  output is relatively insensitive to the values of the newly added weights. An alternative  that is applicable when IWl ! IOI is an integer is to reduce X by removing "uninteresting"  x's. A third alternative is to consider surface densities over S(W), Ps(w)(O), instead of vol-  Bayesian Backpropagation over I-O Functions Rather Than Weights 207  ume densities over , Pit,({). Such surface densities are given by equation (2), if one uses  the metric form of Jn,,w(W). (Buntine has emphasized that the Jacobian form is not even  defined for IWl < IXI IOI, since }{i / Wj is not square then (personal communication).)  As an aside, note that restricting Pit,({) to S(W) is an example of the common theoretical  assumption that "target functions" come from a pre-chosen "concept class". In practice  such an assumption is usually ludicrous - whenever it is made there is an implicit hope that  it constitutes a valid approximation to a more reasonable Pn,({).  When decision theory is incorporated into Bayesian analysis, only rarely does it advise us  to evaluate an MAP quantity (i.e., use BP). Instead Bayesian decision theory usually  advises us to evaluate quantities like E( I 0 (Wolpert, 1994). Just as it does for the use of  MAP estimators, the analysis of this paper has implications for the use of such E( I 0  esfimators. In particular, one way to evaluate E(l 0 = ldw PWlT(W I 0 net(w, .) is to  expand net(w, .) to low order and then approximate PWlT(W I 0 as a sum of Gaussians  (Buntine and Weigend, 1991). Equation (4) suggests that instead we write E( I t) as  ld{ PqT({ I t) { and approximate Pn, IT({ I 0 as a sum of Gaussians. Since fewer approxi-  mations are used (no low order expansion of net(w, .)), this might be more accurate.  Acknowledgements  Thanks to David Rosen and Wray Buntine for stimulating discussion, and to TXN and the  SFI for funding. This paper is a condensed version of (Wolpert 1994).  References  B untine, W., Weigend, A. (1991). Bayesian back-propagation. Complex Systems, 5, p. 603.  Denker, J., LeCun, Y. (1991). Transforming neural-net output levels to probability distri-  butions. In Neural Information Processing Systems 3, R. Lippman et al. (Eds).  Fefferman, C. (1993). Reconstructing a neural net from its output. Samoff Research Cen-  ter TR 93-01.  Hassibi, B., and Stork, D. (1992). Second order derivatives for network pruning: optimal  brain surgeon. Ricoh Tech Report CRC-TR-9214.  MacKay, D. (1992). Bayesian Interpolation, and A Practical Framework for Backpropa-  gation Networks. Neural Computation, 4, pp. 415 and 448.  Neal, R. (1993). Bayesian learning via stochastic dynamics. In Neural Information Pro-  cessing Systems 5, S. Hanson et al. (Eds). Morgan Kaufmann.  Nowlan, S., and Hinton, G. (1994). Simplifying Neural Networks by Soft Weight-Sharing.  In Theories of Induction: Proceedings of the SFI/CNLS Workshop on Formal Approaches  to Supervised Learning, D. Wolpert (F_A.). Addison-Wesley, to appear.  Rissanen, J. (1986). Stochastic complexity and modeling. Ann. Stat., 14, p. 1080.  Wolpert, D. (1993). On the use of evidence in neural networks. In Neural Information Pro-  cessing Systems 5, S. Hanson et al. (Eds). Morgan-Kauffman.  Wolpert, D. (1994). Bayesian back-propagation over i-o functions rather than weights. SFI  tech. report. ftp'able from archive.cis.ohio-state.edu, as pub/neuroprose/wolpert.nips.93.Z.  
Implementing Intelligence on Silicon  Using Neuron-Like Functional MOS Transistors  Tadashi Shibata, Koji Kotani, Takeo Yamashita, Hiroshi Ishii  Hideo Kosa -ka, and Tadahiro Ohmi  Department of Electronic Engineering  Tohoku University  Aza-Aoba, Aramaki, Aobaku, Sendal 980 JAPAN  Abstract  We will present the implementation of intelligent electronic circuits  realized for the first time using a new functional device called Neuron  MOS Transistor (neuMOS or vMOS in short) simulating the behavior  of biological neurons at a single transistor level. Search for the most  resembling data in the memory cell array, for instance, can be  automatically carried out on hardware without any software  manipulation. Soft Hardware, which we named, can arbitrarily change  its logic function in real time by external control signals without any  hardware modification. Implementation of a neural network equipped  with an on-chip self-learning capability is also described. Through the  studies of vMOS intelligent circuit implementation, we noticed an  interesting similarity in the architectures of vMOS logic circuitry and  biological systems.  1 INTRODUCTION  The motivation of this work has stemmed from the invention of a new functional  transistor which simulates the behavior of biological neurons (Shibata and Ohmi, 1991;  1992a). The transistor can perform weighted sumination of multiple input signals and  squashing on the sum all at a single transistor level. Due to its functional similarity, the  transistor was named Neuron MOSFET (abbreviated as neuMOS or vMOS). What is of  significance with this new device is that a number of new architecture electronic circuits  can be build using vMOS' which are different fi'om conventional ones both in operational  principles and functional capabilities. They are characterized by a high degree of  parallelism in hardware computation, a large flexibility in hardware configuration and a  dramatic reduction in the circuit complexity as coinpared to conventional integrated  919  920 Shibata, Kotani, Yamashita, Ishii, Kosaka, and Ohmi  circuits. During the course of studies in exploring vMOS circuit applications an interesting  similarity has been noticed between the basic vMOS logic circuit architecture and the  common structure found in biological neuronal systems, i.e., the competitive processes  of excitatory and inhibitory connections. The pu]pose of this article is to demonstrate how  powerful the neuron-like functionality in an elemental device is in implementing  intelligent functions in silicon integrated circuits.  2 NEURON MOSFET AND SOFT-HARDWARE LOGIC CIRCUITS  The symbolic representation of a vMOS is given in Fig. 1. A vMOS is a regular MOS  transistor except that its gate electrode is made electrically floating and multiple input  terminals are capacitively coupled to the floating gate. The potential of the floating gate  % is determined as a linear weighted sum of multiple input voltages where each  weighting factor is given by the magnitude of a coupling capacitance. When 0)F , the  weighted sum, exceeds the threshold voltage of the transistor, it tums on. Thus the  function of a neuron model (McCulloch and Pitts, 1943) has been directly imple]nented  in a simple transistor structure. vMOS transistors were fabricated using the double-  polysilicon gate technology and a CMOS process was employed for vMOS integrated  circuits fabrication. It should be noted here that no floating-gate charging effect was  employed in the operation of vMOS logic circuits.  V V V  F N, [ % FLOATING GATE  O . O  SOURCE DRAIN  .,,_c.,K+Gv,+ ..... +c,,v,, >  CTOT  Transistor "Turns ON"  Figure 1: Schematic of a neuron MOS transistor.  Since the weighting factors in a vMOS are determined by the overlapping areas of the  first poly (floating gate) and second poly (input gate) patterns, they are not alterable. For  this reason, in vMOS applications to self-learning neural network synthesis, a synapse  cell circuit was provided to each input terminal of a vMOS to represent an alterable  connection strength. Here the plasticity of a synaptic weight was created by  charging/discharging of the floating-gate in a vMOS synapse circuitry as described in 4.  The I- V characteristics of a two-input-gate vMOS having identical coupling capacitances  are shown in Fig. 2, where one of the input gates is used as a gate terminal and the other  as a threshold-control terminal. The apparent threshold voltage as seen from the gate  terminal is changed from a depletion-mode to an enhancement-mode threshold by the  voltage given to the control terminal. This variable threshold nature of a vMOS, we  believe, is most essential in creating flexibility in electronic hardware systems.  Figure 3(a) shows a two-input-variable Soft Hardware Logic (SHL) circuit which can  represent all possible sixteen Boolean functions for two binary inputs X and X 2 by  adjusting the control signals V^, V, and V o The inputs, X and X2, are directly coupled  to the floating gate of a complementary vMOS inverter in the output stage with a 1:2  coupling ratio. The vMOS inverter, which we call the main inverter, detemfines the logic.  Implementing Intelligence on Silicon Using Neuron-Like Functional MOS Transistors 921  CONTROL  ._-- C  2.0  1.0  00  -5.0  J acteristics of a variable  /////)/ threshold transistor. Vol-  tage at the threshold-con-  trol terminal was varied  from +5V to -5V (from  -2.s 0.0 2.5 50 left to right).  GATE VOLTAGE (V)  Xl 0 ,  Vco  I VDD  X 2  VOUT  i XOR .!.XNOR.i.N,4ND  Orns  I I I  2ms/div  NOR INHIBIT  .  20ms  5V  (a) Co)  Figure 3: Two-input-variable soft hardware logic circuit(a) and measured  characteristicsCo). The slow operation is due to the loading effect. (The test circuit has no  output buffers.)  The inputs are also coupled to the main inverter via three inter-stage vMOS inverters  (pre-inverters). When the analog variable represented by the binary inputs X and X 2  increases, the inputs tend to turn on the main inverter via direct connection, while the  indirect connection via pre-inverters tend to turn off the main inverter because pre-  inverter outputs change from Vrr to 0 when they turn on. This competitive process creates  logics. The turn-on thresholds of pre-inverters are made alterable by control signals  utilizing the variable threshold characteristics of vMOS'. Thus the real-time alteration of  logic functions has been achieved and are demonstrated by experiments in Fig. 3(b). With  the basic circuit architecture of the two-staged vMOS inverter configuration shown in Fig.  3(a), any Boolean function can be generated. We found the inverting connections via pre-  inverters are most essential in logic synthesis. The structure indicates an interesting  similarity to neuronal functional modules in which intramodular inhibitory connections  play essential roles.  Fixed function logics can be generated much more simply using the basic two-staged  structure, resulting in a dramatic reduction in transistor counts and interconnections. It has  been demonstrated that a full adder, 3-b and 4-b A/D converters can be constructed only  with 8, 16 and 28 transistors, respectively, which should be compared to 30, 174 and 398  transistors by conventional CMOS design, respectively. The details on vMOS circuit  design are described in Refs. (Shibata and Ohmi, 1993a; 1993b) and experimental  verification in Ref. (Kotani et al., 1992).  922 Shibata, Kotani, Yamashita, Ishii, Kosaka, and Ohmi  VDD  Xo .  YO-- o Y  Analog  ,5o  VOUT  INPUT  - OUTPUT   ?msec/dtv   5msec/dlv  (a) (b) (c)  Figure 4: Real-time rule-variable data matching circuit (a) and measured wave forms  Co & c). In (c),/5 is changed as 0.5, 1, 1.5, and 2 [V] from top to bottom.  A unique vMOS circuit based on the basic structure of Fig. 3(a) is the real-time rule-  variable data matching circuitry shown in Fig. 4(a). The circuit output becomes high when  I X - Y I </5. X is the input data, Y the template data and 5 the window width for data  matching where X, Y and/5 are all time variables. Measured data are shown in Figs. 4(b)  and 4(c), where it is seen the triple peaks are merged into a single peak as/5 increases  (Shibata et al., 1993c). The circuit is cronposed of only 10 vMOS' and can be easily  integrated with each pixel on a image sensor chip. If vMOS circuitry is combined with  a bipolar image sensor cell having an amplification function (Tanaka et al., 1989), for  instance, in situ image processing such as edge detection and variable-template matching  would become possible, leading to an intelligent image sensor chip.  3 BINARY-MULTIVALUED-ANALOG MERGED HARDWARE  COMPUTATION  A winner-take-all circuit (WTA) implemented by vMOS circuitry is given in Fig. 5.  Each cell is composed of a vMOS variable threshold inverter in which the apparent  threshold is modified by an analog input signals V^ - V o When the common input signal  V R is ramped up, the lowest threshold cell (a cell receiving the largest analog input) turus  on firsfly, at which instance a feedback loop is formed in each cell and the state of the  cell is self-latched. As a result, only the winner cell yields an output of 1. The circuit has  been applied to building an associative memory as demonstrated in Fig. 6. The binary data  stored in a SRAM cell array are all simultaneously matched to the sample data by taking  XNOR, and the number of matched bits are transferred to the floating gate of each WTA  cell by capacitance coupling. The WTA action finds the location of data having the largest  number of matched bits. This principle has been also applied to an sorting circuitry  (Yamashita et al., 1993). In these circuits all computations are conducted by an algorithm  directly imbedded in the hardware. Such an analog-digital merged hardware computation  algorithm is a key to implement intelligent data processing architecture on silicon. A  multivalued DRAM cell equipped with the association function and a multivalued SRAM  cell having self-quantizing and self-classification functions have been also developed  based on the binary-multivalued-analog merged hardware algorithm (Rita et al., 1994).  Implemennng Intelligence on Silicon Using Neuron-Like Functional MOS Transistors 923  INITIAL  WINNER LATCH  VR  - TIME  VAST  o  o v  Figure 5: Operational principle of vMOS Winner-Take-All  CONTROL  SIGNAL  circuit.  o  SAMPLE DATA  1 I WIIVIVER_TAKE_ALL  NETWORK  (a) (b'}  Figure 6: vMOS associative memory: (a) circuit diagram; (b) photomicrograph of a test  chip.  4 HARDWARE SELF-LEARNING NEURAL NETWORKS  Since vMOS itself has the basic function of a neuron, a neuron cell is very easily  implemented by a complementary vMOS inverter. The learning capability of a neural  network is due to the plasticity of synaptic connections. Therefore its circuit  implementation is a key issue. A stand-by power dissipation fi'ee synapse circuit which  has been developed using vMOS circuitry is shown in Fig. 7(a). The circuit is a  differential pair of N-channel and P-channel vMOS source followers sharing the same  floating gate, which are both merged into CMOS inverters to cut off dc cun-ent paths.  When the pre-synaptic neuron fires, both source followers are activated. Then the analog  weight value stored as charges in the common floating gate is read out and transferred to  the floating gate (dendrite) of the post-synaptic neuron by capacitance coupling as shown  in Figs. 7 (b) and (c). The outputs of N-vMOS (V*) and P-vMOS (V-) source followers  are averaged at the dendrite level, yielding an effective synapse output equal to (V* +  V-)/2. The synapse can represent both positive (excitatory) and negative (inhibitory)  weights depending on whether the effective output is larger or smaller than Vl>2,  respectively. The operation of the synapse cell is demonstrated in Fig. 8(a).  924 Shibata, Kotani, Yamashita, Ishii, Kosaka, and Ohmi  IJ MOS DIFFERENTIAL  SOURCE FOLLOWER  Vx W   lectrode   : " 'r   v  FLOATING GATE {DENDRE) OF NEURON 2  (a) )  Figure 7: Synapse cell circuit implemented  Previous Neuron at Rest Previous Neuron Fired  .s= Qr  ]1  I II II LJI I I I II I  Veff =  (c)  by vMOS circuitry.  DD  V{  Previous-Layer Neuron  [ Fired  EXCITA TORY  INHIBITORY  V+ 2  40)n;ec  3  21  -2  -3  -4  4  3  -2  CONVENTIONAL CELL  5 10 15 20 25  Number o! Programming Pulses  (a) (b)  Figure 8: (a) Measured synapse cell output characteristics;  characteristics as represented by N-vMOS threshold  without (conventional EEPROM cell: top) feed back.  (b) weight updating  with (our new cell:bottom) or  The weight updating is conducted by giving high programming pulses to both V x and V,  terminals. (Their coupling capacitances are made much larger than others). Then the  common floating gate is pulled up to the programming voltage, allowing electrons to flow  into the floating gate via Fowler-Nordhein tunneling. When either V x or V is low,  tunneling injection does not occur because the tunneling current is very sensitive to the  electric field intensity, being exponentially dependent upon the tunnel oxide field (Hieda  Implementing Intelligence on Silicon Using Neuron-Like Functional MOS Transistors 925  et al., 1985). The data updating occum only at the crossing point of V x and V v lines,  allowing Hebb-rule-like learning directly implemented on the hardware (Shibata and  Ohmi, 1992b). Hardware-Backpropagation (HBP) learning algorithm, which is a  simplified version of the original BP, has been also developed in order to facilitate its  hardware implementation (Ishii et al., 1992) and has been applied to build self-learning  vMOS neural networks (Ishii et al., 1993).  One of the drawbacks of programming by tunneling is the non-linearity in the data  updating characteristics under constant pulses as shown in Fig. 8(b) (top). This difficulty  has been beautifully resolved in our cell. With V s high, the output of the N-vMOS source  follower is fed back to the tunneling electrode and the floating-gate potential is set to the  tunneling electrode. In this manner, the voltage across the tunneling oxide is always preset  to a constant voltage (equal to the N-vMOS threshold) before a programming pulse is  applied, thus allowing constant charge to be injected or extracted at each pulse (Kosaka  et al, 1993) as demonstrated in Fig. 8(b) (bottom). A test self-learning circuit that learned  XOR is shown in Fig. 9.  tNPUT1  INPUT2  VHIDDEN  VOUT  INPUT1  INPUT2'  5V/dtv '  VHIOOEN  VOUT  "XOR"  400nsec/div  Figure 9: Test circuit of vMOS neural network and its response when XOR is learnt.  5 SUMMARY  Development of intelligent electronic circuit systems using a new functional device called  Neuron MOS Transistor has been described. vMOS circuitry is characterized by its high  parallelism in computation schene and the large flexibility in altering hardware functions  and also by its great simplicity in the circuit organization. The ideas of Soft tlardware and  the vMOS associative memory were not directly inspired fi'om biological systems.  However, an interesting similarity is found in their basic structures. It is also demonstrated  that the vMOS circuitry is very powerful in building neural networks in which learning  algorithms are imbedded in the hardware. We conclude that the neuron-like functionality  at an elementary device level is essentially important in implementing sophisticated  information processing algorithms directly in the hardware.  926 Shibata, Kotani, Yamashita, Ishii, Kosaka, and Ohmi  ACKNOWLEDGMENT  This work was partially supported by the Grant-in-Aid for Scientific Research  (04402029) and Grant-in-Aid for Developmental Scientific Research (05505003) from  the Ministry of Education, Science and Culture, Japan. A pal of this work was carried  out in the Super Clean Room of Laboratory for Microelectronics, Research Institute of  Electrical Communication, Tohoku University.  REFERENCES  [1] T. Shibata and T. Ohmi, "An intelligent MOS transistor featuring gate-level weighted  sum and threshold operations," in IEDM Tech. Dig., 1991, pp. 919-922.  [2] T. Shibata and T. Ohmi, "A functional MOS transistor featuring gate-level weighted  sum and threshold operations," IEEE Trans. Electron Devices, Vol. 39, No. 6, pp.1444-  1455 (1992a).  [3] W. S. McCulloch and W. Pitts, "A logical calculus of the ideas immanent in nervous  activity," Bull. Math. Biophys., Vol. 5, pp. 115-133, 1943.  [4] T. Shibata and T. Onmi, "Neuron MOS binary-logic integrated circuits: Part I, Design  fundamentals and soft-hardware-logic circuit implementation," IEEE Trans. Electron  Devices, Vol. 40, No. 3, pp. 570-576 (1993a).  [5] T. Shibata and T. Ohmi, "Neuron MOS binary-logic integrated circuits: Part II,  Simplifying techniques of circuit configuration and their practical applications," IEEE  Trans. Electron Devices, Vol. 40, No. 5, 974-979 (1993b).  [6] K. Kotani, T. Shibata, and T. Ohmi, "Neuron-MOS binary-logic circuits featuring  dramatic reduction in transistor count and interconnections," in IEDM Tech. Dig., 1992,  pp. 431-434.  [7] T. Shibata, K. Kotani, and T. Ohmi, "Real-time reconfigurable logic circuits using  neuron MOS transistors," in ISSCC Dig. Technical papers, 1993c, FA 15.3, pp. 238-239.  [8] N. Tanaka, T. Ohmi, and Y. Nakamura, "A novel bipolar imaging device with self-  noise reduction capability," IEEE Trans. Electron Devices, Vol. 36, No. 1, pp. 31-38  (1989).  [9] T. Yamashita, T. Shibata, and T. Ohmi, "Neuron MOS winner-take-all circuit and its  application to associative memory," in ISSCC Dig. Technical papers, 1993, FA 15.2, pp.  236-237.  [10] R. Au, T. Yamashita, T. Shibata, and T. Ohmi, "Neuron-MOS multiple-valued  memory technology for intelligent data processing," in ISSCC Dig. Technical papers,  1994, FA 16.3.  [11] K. Hieda, M. Wada, T. Shibata, and H. Iizuka, "Optimum design of dual-control  gate cell for high-density EEPROM's," IEEE Trans. Electron Devices, vol. ED-32, no.  9, pp. 1776-1780, 1985.  [12] T. Shibata and T. Ohmi, "A self-learning neural-network LSI using neuron  MOSFET's," in Dig. Tech. Papers, 1992 Symposium on VLSI Technology, Seattle, June,  1992, pp. 84-85.  [13] H. Ishii, T. Shibata, H. Kosaka, and T. Ohmi, "Hardware-Backpropagation learning  of neuron MOS neural networks," in IEDM Tech. Dig., 1992, pp. 435-438.  [14] H. Ishii, T. Shibata, H. Kosaka, and T. Ohmi, "Hardware-learning neural network  LSI using a highly functional transistor simulating neuron actions," in Proc. International  Joint Conference on Neural Networks '93, Nagoya, Oct. 0,25-29, 1993, pp. 907-910.  [15] H. Kosaka, T. Shibata, H. Ishii, and T. Ohmi, "An excellent weight-updating-  linearity synapse memory cell for self-learning neuron MOS neural networks," in IEDM  Tech. Dig., 1993, pp. 626-626.  
Odor Processing in the Bee: a Preliminary  Study of the Role of Central Input to the  Antennal Lobe.  Christiane Linster  David Marsan  ESPCI, Laboratoire dlectmnique  10, Rue Vauquelin, 75005 Paris  linste neurones.espci.fr  Claudine Masson  Laboratoire de Neurobiologie Compar6e  des Invert6r6es  INRA/CNRS (URA 1190)  91140 Bures sur Yvette, France  masson@jouy.inra. fr  Michel Kerszberg  Institut Pasteur  CNRS (URA 1284)  Neurobiologie Mo16:ulaire  25, Rue du Dr. Roux  75015 Paris, France  Abstract  Based on precise anatomical data of the bee's olfactory system, we  propose an investigation of the possible mechanisms of modulation and  control between the two levels of olfactory information processing: the  antennal lobe glomeruli and the mushroom bodies. We use simplified  neurons, but realistic architecture. As a first conclusion, we postulate  that the feature extraction performed by the antennal lobe (glomeruli and  interneurons) necessitates central input from the mushroom bodies for  fine tuning. The central input thus facilitates the evolution from fuzzy  olfactory images in the glomerular layer towards more focussed images  upon odor presentation.  1. Introduction  Honeybee foraging behavior is based on discrimination among complex odors which is the  result of a memory process involving extraction and recall of "key-features" representative  of the plant aroma (for a review see Masson et al. 1993). The study of the neural correlates  of such mechanisms requires a determination of how the olfactory system successively  analyses odors at each stage (namely: receptor cells, antennal lobe interneurons and  glomeruli, mushroom bodies). Thus far, all experimental studies suggest the implication  of both antennal lobe and mushroom bodies in these processes. The signal transmitted by  the receptor cells is essentially unstable and fluctuating. The antennal lobe appears as the  location of noise reduction and feature extraction. The specific associative components  operating on the olfactory memory trace would be essentially located in the mushroom  bodies. The results of neuroethological experiments indicate furthermore that both the  527  528 Linster, Marsan, Masson, and Kerszberg  feed-forward connections from the antennal lobe projection neurons to the mushroom  bodies and the feedback connections from the mushroom bodies to the antennal lobe  neurons are crucial for the storage and the recall of odor signals (Masson 1977; Erber et al.  1980; Erber 1981).  Interestingly, the antennal lobe compares to the mammalian olfactory bulb. Computational  models of the insect antennal lobe (Kerszberg and Masson 1993; Linster et al. 1993) and  the mammalian olfactory bulb (Anton et al. 1991; Li and Hopfield 1989; Schild 1988)  have demonstrated that feature extraction can be performed in the glomerular layer, but the  possible role of central input to the glomerular layer has not been investigated (although it  has been included, as a uniform signal, in the Li and Hopfield model). On the other hand,  several models of the mammalian olfactory cortex (Hasselmo 1993; Wilson and Bower  1989; Liljenstr6m 1991) have investigated its associative memory function, but have  ignored the nature of the input from the olfactory bulb to this system.  Based on anatomical and electrophysiological data obtained for the bee's olfactory system  (Fonta et al. 1993; Sun et al. 1993), we propose in this paper to investigate of the  possible mechanisms of modulation and control between the two levels of olfactory  information processing in a formal neural model. In the model, the presentation of an  "odor" (a mixture of several molecules) differentially activates several populations of  glomeruli. Due to coupling by local interneurons, competition is triggered between the  activated glomeruli, in agreement with a recent proposal (Kerszberg and Masson 1993). We  investigate the role of the different types of neurons implicated in the circuitry, and study  the modulation of the glomerular states by reentrant input from the upper centers in the  brain (i.e. mushroom bodies).  2. Olfactory circuitry in the bee's antennal lobe and mushroom  bodies  95% of sensory cells located on the bee's antenna are olfactory (Esslen and Kaissling  1976), and convey signals to the antennal lobes. In the honeybee, due to some overlap of  receptor cell responses, the peripheral representation of an odor stimulus is represented in  an across fiber code (Fonta et al. 1993). Sensory axons project on two categories of  antennal lobe neurons, namely local interneurons (LIN) and output neurons (ON). The  synaptic contacts between sensory neurons and antennal lobe neurons, as well as the  synaptic contacts between antennal lobe neurons are localized in areas of high synaptic  density, the antennal lobe glomeruli; each glomerulus represents an identifiable  morphological neuropilar sub-unit (of which there are 165 for the worker honeybee)  (Arnold et al. 1985).  Local interneurons constitute the majority of antennal lobe neurons, and there is evidence  that a majority of the LINs are inhibitory. As receptor cells are supposed to synapse  mainly with LINs, the high level of excitation observed in the responses of ONs suggests  that local excitation also exists (Malun 1991), in the form of spiking or non-spiking LINs,  or as a modulation of local excitatbility.  All LINs are pluriglomerular, but the majority of them, heterogeneous local intemeurons  (or HeteroLINs), have a high density of dendfite branches in one particular glomerulus, and  sparser branches distributed across other glomemli. A second category, homogeneous local  interneurons (or Homo LINs), distribute their branches more homogeneously over the  whole antennal lobe. Similarly, some of the ONs have dendrites invading only one  glomerulus (Uniglomerular, or Uni ON), whereas the others (Pluri ON) are  pluriglomerular. The axons of both types of ON project to different areas of the  protocerebmm, including the mushroom bodies (Fonta et al. 1993).  Odor Processing in the Bee 529  3. Olfactory processing in the bee's antennal lobe glomeruli  Responses of antennal lobe neurons to various odor stimuli are characterized by complex  temporal patterns of activation and inactivation (Sun et al. 1993). Intracellularly recorded  responses to odor mixtures are in general very complex and difficult to interpret from the  responses to single odor components. A tendency to select particular odor related  information is expressed by the category of "localized" antennal lobe neurons, both Hetero  LINs and Uni ONs. In contrast, "global" neurons, both Homo LINs and Pluri ONs are  often more responsive to mixtures than to single components. This might indicate that the  related localized glomeruli represent functional sub units which are particularly involved in  the discrimination of some key features.  An adaptation of the 2DG method to the honeybee antennal lobe has permitted to study the  spatial distribution of odor related activity in the antennal lobe glomeruli (Nicolas et al.  1993; Masson et al. 1993). Results obtained with several individuals indicate that a  correspondence can be established between two different odors and the activity maps they  induce. This suggests that in the antennal lobe, different odor qualities with different  biological meaning might be decoded according to separate spatial maps sharing a number  of common processing areas.  4. Model of olfactory circuitry  In the model, we introduce the different categories of neurons described above (Figure 1).  Glomeruli are grouped into several regions and each receptor cell projects onto all local  interneurons with arborizations in one region. Interneurons corresponding to heterogeneous  LINs can be (i) excitatory, these have a dendritic arborization (input and output synapses)  restricted to one glomerulus; they provide "local" excitation, or, (ii) inhibitory, these have  a dense arborization (mainly input synapses) in one glomerulus and sparse arborizations  (mainly output synapses) in all others; they provide "local inhibition" and "lateral  inhibition" between glomeruli. Interneurons corresponding to homogeneous LINs are  inhibitory and have sparse arborizations (input and output synapses) in all glomeruli; they  provide "uniform inhibition" over the glomerular layer.  Output neurons are postsynaptic only to interneurons, they do not receive direct input from  receptor cells. Each output neuron collects information from all interneurons in one  glomerulus: thus modeling uniglomerular ONs.  Implementation: The different neuron populations associated with one glomerulus are  represented in the program as one unit (each unit is governed by one differential equation);  the output of one unit represents the average firing probability of all neurons in this  population (assuming that on the average, all neurons in one population receive the same  input and have the same intrinsic properties). All units have membrane constants and a  non-linear output function. Connection delays and connection strengths between units are  chosen randomly around an average value: this assures a "realistic spatial averaging" over  populations. The differential equations associated with the units are translated into  difference equations and simulated by synchronous updating (sampling step 5ms).  530 Linster, Marsan, Masson, and Kerszberg  M  Molecule spectra  Receptor cell types  Local excitation  Receptor input   Global inhibition  Glomerulus  Glomerular region   Local inhibition and lateral inhibition  Local modulation  Modulation of global inhibition   Global inhibitory intemeuron  O Localized output neuron   Localized excitatory interneuron  0 Localized inhibitory interneuron  Figure 1: Organization of the model olfactory circuitry.  In the model, we introduce receptor cells with overlapping molecule spectra; each  receptor cell has its maximal spiking probability P for the presence of a particular  molecule i. The axons of the receptor cells project into distinct regions of the  glomerular layer. All allowed connections exist with the same probability, but with  different connection strengths. The activity of each glomerulus is represented by its  associated output neurons. Central input projects onto the global inhibitory  interneurons (modulation of global inhibition) or on all interneurons in one  glomerulus (local modulation).  5. Olfactory processing by the model circuitry  In the model, odors are represented as one-dimensional arrays of molecules; each molecule  can be present in varying amounts. Due to the gaussian distributions of receptor cell  sensitivities, an active molecule activates more than one receptor cell (with varying degrees  of activation). As each receptor cell projects into all glomeruli belonging to its target  region, thus, a molecular bouquet differentially activates a number of glomeruli in different  glomerular regions. This triggers several phenomena: (i) due to the excitatory elements  local to each glomerulus, and activated glomerulus tends to enhance the activation it  receives from the receptor cells, (ii) the local inhibitory elements are activated (with a  certain delay) by the receptor cell activity and by the self-activation of the local excitatory  elements, and, (iii) trend to inhibit neighboring glomeruli. These phenomena result in a  competition between active glomeruli: during a number of sampling steps, the output  activity of each glomerulus (represented by the firing probability of the associated output  neuron), oscillates from high activity to low activity. Due to the competition provided by  Odor Processing in the Bee 531  the lateral inhibition, the spatial oscillatory activity pattern changes over time, and a stable  activity map is reached eventually. A number of glomemli "win" and stay active, whereas  others "loose" and are inhibited (Figure 2).  The activities of individual output neurons follow the general pattern described above:  oscillation of the activity during a number of sampling steps until the activity "settles"  down to a stable value. A stable activity can either be a constant firing probability, or a  "stable" oscillation of the firing probability. An output neuron associated to a particular  glomerulus may be active for a particular odor input, and silent for others. Complex  temporal patterns of excitation and inhibition may occur after stimulus presentation,  Thus, the model predicts that odor representation is performed through spatial maps of  activity spanning the whole glomerular layer. Individual output neurons, representing the  activity of their associated glomeruli may be either excited or inhibited by a particular odor  pattern.  Glomeruli After stabilization  1- 15  Figure 2: Behavior of the model after stimulation of the receptor cells with the  molecule array indicated in the figure. For several sampling steps (of 5 ms), the  activity (firing probability) of the ON associated to each glomerulus is shown. At  step 1, all glomeruli are differentially activated by the receptor cell input. Lateral  inhibition silences all glomeruli during the next sampling step. At step 3, some  glomeruli are highly activated (due to their local excitation), whereas others are  almost silenced. Then, t spatial activation pattern oscillates for a number of  sampling steps (which depends on the strength of the lateral inhibitory connections  and on the number of active molecules in the odor array), and finally stabilizes in a  spatial activity map.  6. Comparison of odor processing in the Bee's antennal lobe  and in the model  Antennal lobe neurons in the bee show various response patterns to stimulation with pure  components and mixtures. Most LINs and ONs respond with simple excitation or  inhibition to stimulation, often followed by a hyperpolarized (resp. depolarized) phase.  Interestingly, most LINs respond with various degrees of excitation to stimulation with  binary odors and mixtures, whereas ONs respond equally often by excitation than by  inhibition (Sun et al. 1993). In the model, LINs receive direct afferent input from receptor  cells, and are therefore differentially activated by odor stimulation; they respond with  varying degrees of excitation to stimulation with pure components and their mixtures.  Output neurons in the model receive indirect input from receptor cells via local  interneurons. Output neurons in the model are either activated (if their associated  532 Linster, Marsan, Masson, and Kerszberg  glomerulus wins the competfion) or inhibited (if their associated glomerulus looses the  competition) by odor stimulation.  In the simulations, output neurons which are excited for a particular odor stimulation  belong to an active glomerulus in the spatial activity map associated to that odor. For each  odor, a particular activity map is established. An output neuron is either excited or  inhibited by a particular odor stimulation, indicating that it takes part in the representation  of an activity map across glomeruli, which might be compared to the antennal lobe 2DG  maps.  7. Modulation of the model dynamics  Odor detection by modulation of spontaneous activity  At high spontaneous activity, all glomeruli in the model oscillate spontaneously (Figure  3). Odor stimulation tends to synchronize these oscillations, but no feature detection is  performed. In the model, the underlying activity map which corresponds to the odor signal  can only emerge if the spontaneous activity is decreased (Figure 3). Decreasing of the  spontaneous activity can be achieved by 5i) activation of the global inhibitory interneurons  by central input, or, (ii) decreasing of the spiking threshold of all antennal lobe neurons.  These data fit well with experimental data (see Sun et al. 1993, Figures 7 and 8).  I I I I I I I I I  Stimulus armlication  500 ms  Reduction of spontaneous  activity  Figure 3  Figure 3: Modulation of the spontaneous activity. We show the spiking probabilities of  output neurons associated to different glomeruli. Arrows indicate stimulus onset. Stimulus  presentation synchronizes the oscillations. A decreasing of the spontaneous activity results  in the emergence of the underlying activity map: several output neurons exhibit high  activities, whereas the others are silent.  Contrast enhancement by modulation of lateral inhibition  Presentation of an odor in the model differentially activates many or all glomeruli, which,  due to the local excitation, try to enhance the activation due to the odor stimulus. Due to  the competition between glomeruli, feature detection is performed in the glomerular layer,  which enhances some elements of the stimulus and suppresses others.  In the model, for a given odor stimulation, the number of winning glomeruli depends on  the strength of the lateral inhibition between glomeruli (Figure 5). At low lateral  inhibition, most glomeruli stay active for any odor; no feature extraction is performed.  Odor Processing in the Bee 533  Increasing of the lateral inhibition focuses the odor maps, which can now  different odor inputs.  differentiate  oe$oocoo$  ooooooo$  Figure 5: Stabilized activity maps for different odor stimuli with increasing lateral  inhibition strength. At low competition, all glomeruli tend to be active due to their  local excitation. Increasing of lateral inhibition permits to enhance the important  features of each odor, and leads to uncon'elated activity maps for the different  stimulations.  Increasing of the lateral inhibition permits to focus a fuzzy olfactory image in the  glomerular layer, or to "smell closer". A fuzzy sampling of an odor may be useful at first  approach, whereas a more precise analysis of its important components is facilitated by  increasing the competition between glomeruli increases contrast enhancement.  8. Discussion  We have presented the computational abilities of the neural circuitry in the antennal lobe  model, based on what is known of the bee's circuitry. Single cell responses and global  activity patterns are comparable to the odor processing mechanisms proposed in the insect  (Linster et al. 1993; Masson et al. 1993; Kerszberg and Masson 1993) and in the vertebrate  (Kauer et al. 1991; Li and Hopfield 1989; Freeman 1991) literature. As suggested by  Kerszberg and Masson (1993), we show that odor preprocessing is based on spontaneous  dynamics of the antennal lobe glomeruli, and that, in addition, feature detection needs  competition between activated glomeruli due to global and lateral inhibition. The model is  able to predict the role of the four types of neurons morphologically identified in the bee  antennal lobe. It also predicts how intracellular recordings and 2DG data can be explained  by the odor processing mechanism. Furthermore, modulation of the models dynamics  opens up a number of new ideas about the respective role of the two main categories  ("localized" and "global") of antennal lobe neurons, and the possible role of central input to  these neurons.  Acknowledgements  The authors are grateful to G. Dreyfus and L. Personnaz for fruitful discussions.  534 Linster, Marsan, Masson, and Kerszberg  References  Arnold, G., Masson, C., Budhargusa, S. 1985. Comparative study of the antennal pathway  of the workerbee and the drone (Apis mellifera). Cell Tissue Res. 242: 593-605.  Ether, J. 1981 Neural correlates of learning in the honeybee. TINS 4:270-273.  Ether, J., Masuhr, T., Menzel, R. 1980. Localisation of short-term memory in the brain  of the bee, Apis mellifera. Physiolo. Entomol. 5: 343-358.  Esslen, J., Kaissling, K.E. 1976. Zahl und Verteilung antennaler Sensillen bei der  Honigbiene. Zoomorphologie 83: 227-251.  Fonta, C., Sun, X., Masson, C. 1193. Morphology and spatial distribution of bee  antennal lobe interneurons responsive to odours. Chemical Senses, 18 (2): pp. 101-  119.  Hasselmo, M.E. 1993. Acetycholine and Learning in a Cortical Associative Memory.  Neural Computation, 5: 32-44.  Kauer, J.S., Neff, S.R., Hamilton, K.A., Cinelli, A.R. 1991. The Salamander Olfactory  Pathway: Visualizing and Modeling Circuit Activity. in Olfaction: A Model  System for Computational Neuroscience. Davis, J. and Eichenbaum, H. (eds): 44-  68. MIT Press.  Kerszberg, M., Masson, C., 1993. Signal Induced Selection among Spontaneous Activity  Patterns of Bee's Olfactory Glomeruli, submitted.  Li, Z., Hopfield, J.J., 1989. Modeling the Olfactory Bulb and its Neural Oscillatory  Processings. Biological Cybernetics 61:379-392.  Liljenstr6m, H. 1991. Modeling the dynamics of olfactory cortex using simplified network  units and realistic architecture. International Journal of Neural Systems, (l&2): 1-  15.  Linster, C., Masson, C., Kerszberg, M., Personnaz, L., Dreyfus, G. 1993 Computational  Diversity in a Formal Model of the Insect Macroglomerulus., Neural Computation,  5:239-252.  Malun, D. 1991. Inventory and distribution of synapses of identified uniglomerular  projection neurons in the antennal lobe of periplaneta americana. J. Comp. Neurol.  305: 348-360.  Masson, C. 1977. Central olfactory pathways and plasticity of responses to odor stimuli  in insects. in Olfaction and Taste VI. Le Magnen, J., Mac Leod, P. (eds) IRL,  London: 305-314.  Masson, C., Mustaparta, H. 1990. Chemical Information Processing in the Olfactory  System of Insects. Physiol. Reviews 70(1): 199-245.  Masson, C., Pham-Delgue, MH., Fonta, C., Gascuel, J., Arnold, G., Nicolas, G.,  Kerszberg, M. 1993. Recent advances in the concept of adaptation to natural odour  signals in the honeybee Apis mellifera L. Apidologie 24: 169-194.  Menzel, R. 1983. Neurobiology of learning and memory: the honeybee as a model system.  Naturwissenschaften 70:504-511.  Nicolas, G., Arnold, G., Patte, F., Masson, C. 1993. Distribution r6gionale de  l'incorporation du 3H2-Desoxyglucose dans le lobe antennaire de l'ouvfire d'abeille.  CJt. Acad. Sc. Paris (Sciences de la Vie), 316: 1245-1249.  Schild, D. 1988 Principles of odor coding and a neural network for odor discrimination,  Biophys. J. 54:1001-1011.  Sun, X., Fonta, C., Masson, C. 1993. Odour quality processing by bee antennal lobe  neurons. Chemical Senses 18 (4): 355-377.  
Use of Bad Training Data For Better  Predictions  Tal Grossman  Complex Systems Group (T13) and CNLS  LANL, MS B213 Los Alamos N.M. 87545  Alan Lapedes  Complex Systems Group (T13)  LANL, MS B213 Los Alamos N.M. 87545  and The Santa Fe Institute, Santa Fe, New Mexico  Abstract  We show how randomly scrambling the output classes of various  fractions of the training data may be used to improve predictive  accuracy of a classification algorithm. We present a method for  calculating the "noise sensitivity signature" of a learning algorithm  which is based on scrambling the output classes. This signature can  be used to indicate a good match between the complexity of the  classifier and the complexity of the data. Use of noise sensitivity  signatures is distinctly different from other schemes to avoid over-  training, such as cross-validation, which uses only part of the train-  ing data, or various penalty functions, which are not data-adaptive.  Noise sensitivity signature methods use all of the training data and  are manifestly data-adaptive and non-parametric. They are well  suited for situations with limited training data.  I INTRODUCTION  A major problem of pattern recognition and classification algorithms that learn from  a training set of examples is to select the complexity of the model to be trained.  How is it possible to avoid an overparameterized algorithm from "memorizing"  the training data? The dangers inherent in over-parameterization are typically  343  344 Grossman and Lapedes  illustrated by analogy to the simple numerical problem of fitting a curve to data  points drawn from a simple function. If the fit is with a high degree polynomial then  prediction on new points, i.e. generalization, can be quite bad, although the training  set accuracy is quite good. The wild oscillations in the fitted function, needed to  acheire high training set accuracy, cause poor predictions for new data. When  using neural networks, this problem has two basic aspects. One is how to choose  the optimal architecture (e.g. the number of layers and units in a feed forward net),  the other is to know when to stop training. Of course, these two aspects are related:  Training a large net to the highest training set accuracy usually causes overfitting.  However, when training is stopped at the "correct" point (where train-set accuracy  is lower), large nets are generalizing as good as, or even better than, small networks  (as observed e.g. in Weigend 1994). This prompts serious consideration of methods  to avoid overparameterization. Various methods to select network architecture or  to decide when to stop training have been suggested. These include: (1) use of  a penalty function (c.f. Weigend et al. 1991). (2) use of cross validation (Stone  1974). (3) minimum description length methods (Rissanen 1989), or (4) "pruning"  methods (e.g. Le Cun et al. 1990).  Although all these methods are effective to various degrees, they all also suffer some  forrn of non-optimality:  (l) various forms of penalty function have been proposed and results differ between  them. Typically, using a penalty function is generally preferable to not using one.  However, it is not at all clear that there exists one "correct" penalty function and  hence any given penalty function is usually not optimal. (2) Cross validation holds  back part of the training data as a separate valdiation set. It therefore works best in  the situation where use of smaller training sets, and use of relatively small validation  sets, still allows close approximation to the optimal classifier. This is not likely to  be the case in a significantly data-limited regime. (3) MDL methods may be viewed  as a form of penalty function and are subject to the issues in point (1) above. (4)  pruning methods require training a large net, which can be time consuming, and  then "de-tuning" the large network using penalty functions. The issues expressed  in point(I) above apply.  We present a new method to avoid overfitting that uses "noisy" training data where  some of the output classes for a fraction of the data are scrambled. We describe  how to obtain the "noise sensitivity signature" of a classifier (with its learning  algorithm), which is based on the scrambled data. This new methodology is not  computationally cheap, but neither is it prohibitively expensive. It can provide an  alternative to methods (1)-(4) above that (i) can test any complexity parameter of  any classifying algorithm (i.e. the architecture, the stopping criterion etc.) (ii) uses  all the training data, and (iii) is data adaptive, in contrast to fixed penalty/pruning  functions.  2 A DETAILED DESCRIPTION OF THE METHOD  Define a "Learning Algorithm" L(S, P), as any procedure which produces a classifier  f(a:), which is a (discrete) function over a given input space X (a: E X). The input  of the learning algorithm L is a Training Set S and a set of parameters P. The  training set S is a set of M examples, each example is a pair of an input instance a:i  Use of Bad Training Data for Better Predictions 345  and the desired output yi associated with it (i = 1..M). We assume that the desired  output represents an unknown "target function" f* which we try to approximate,  i.e. yi = ?(zi). The set of parameters P includes all the relevant parameters of the  specific learning algorithm and architecture used. When using a feed-forward neural  network classifier this set usually includes the size of the network, its connectivity  pattern, the distribution of the initial weights and the learning parameters (e.g.  the learning rate and momentum term size in usual back-propagation). Some of  these parameters determine the "complexity" of the classifiers produced by the  learning algorithm, or the set of functions f that are realizable by L. The number  of hidden units in a two layer perceptron, for example, determines the number  of free parameters of the model (the weights) that the learning algorithm will fit  to the data (the training set). In general, the output of L can be any classifier:  a neural network, a decision tree, boolean formula etc. The classifier f can also  depend on some random choices, like the initial choice of weights in many network  learning algortihm. It can also depend, like in pruning algorithms on any "stopping  criterion" which may also influence its complexity.  2.1 PRODUCING f  The classification task is given as the training set S. The first step of our method  is to prepare a set of noisy, or partially scrambled realizations of S. We define S   as one particular such realization, in which for fraction p of the M examples te  desired output values (classes) are changed. In this work we consider only binary  classification tasks, which means that we choose pM examples at random for which  yi  = 1 - yi. For each noise level p and set of n such realizations S (tz = 1..n) is  prepared, each with a different random choice of scrambled examples. Practically,  8-10 noise levels in the range p = 0.0 - 0.4, with n .- 4 - 10 realizations of S for  each level are enough. The second step is to apply the learning algorithm to each  of the different S to produce the corresponding classifiers, which are the boolean  functions f = L(S, P).  2.2 NOISE SENSITIVITY MEASURES  Using the set of f, three quantities are measured for each noise level p:   The average performance on the original (noise free) training set S. We  define the average noise-free error as  n M  1  t i  And the noise-free pereformance, or score as Q.(p)= 1 - E.(p).   In a similar way, we define the average error on the noisy training-sets S:  n M  I   i  Note that the error of each classifier f is measured on the training set  by which it was created. The noisy-set performance is then defined as  (p) = _  346 Grossman and Lapedes  The average functional distance between classifiers. The functional distance  between two classifiers, or boolean functions, d(f, g) is the probability of  f(z)  g(a). For a uniform input distribution, it is simply the fraction of  the input space X for which f(z)  g(z). In order to approximate this  quantity, we can use another set of examples. In contrast with validation  set methods, these examples need not be classified, i.e. we only need a set of  inputs a, without the target outputs y, so we can usually use an "artificial"  set of rn random inputs. Although, in principle at least, these z instances  should be taken from the same distribution as the original task examples.  The approximated distance between two classifiers is therefore  d(f,g) 1  if(ai ) g(zi) I (3)  i  We then calculate the average distance, D(p), between the n classifiers f  obtained for each noise level p:  2  D(p) = n(n- 1)  d(f;, i;) (4)  3 NOISE SENSITIVITY BEHAVIOR  Observing the three quantities Qf(p), Q,(p) and D(p), can we distinguish between  an overparametrized classifier and a "well tuned" one ? Can we use this data in order  to choose the best generalizer out of several candidates ? Or to find the right point  to stop the learning algorithm L in order to achieve better generalization ? Lets  estimate how the plots of Q f, Q, and D rs. p, which we call the "Noise Sensitivity  Signature" (NSS) of the algorithm L, look like in several different scenarios.  3.1 D(p)  The average functional distance between realizations, D(p), measures the sensitiv-  ity of the classifier (or the model) to noise. An over-parametrized architecture is  expected to be very sensitive to noise since it is capable of changing its classifica-  tion boundary to learn the scrambled examples. Different realizations of the noisy  training set will therefore result in different classifiers.  On the other hand, an under-parametrized classifier should be stable against at  least a small amount of noise. Its classification boundary will not change when  a few examples change their class. Note, however, that if the training set is not  very "dense", an under-parametrized architecture can still yield different classifiers,  even when trained on a noise free training set (e.g. when using BP with differ-  ent initial weights). Therefore, it may be possible to observe some "background  variance", i.e. non-zero average distance for small (down to zero) noise levels for  under-parametrized classifiers.  3.2 Q.t(p) AND Qn(p)  Similar considerations apply for the two quantities Q.f(p) and Q,(p).  training set is large enough, an under-parametrized classifier cannot  When the  "follow" all  Use of Bad Training Data for Better Predictions 347  the changed examples. Therefore most of them just add to the training error.  Nevertheless its performance on the noise free training set, Qf(p), will not change  much. As a result, when increasing the noise level p from zero (where Q.f(p) =  q,(p)), we should find Q.f(p) > Q,(p) up to a high noise level- where the decision  boundary has changed enough so the error on the original training set becomes  larg,:r than the error on the actual noisy set. The more parameters our model has,  the sooner (i.e. smaller p) it will switch to the QI(p) < Q,(p) state. If a network  starts with q.f(p) = qn(p) and then exhibits a behavior with q.f(p) < q,(p), this  is a signature of overparameterization.  3.3 THE TRAINING SET  In addition to the set of parameters P and the learning algorithm itself, there  is another important factor in the learning process. This is the training set S.  The dependence on M, the number of examples is evident. When M is not large  enough, the training set does not provide enough data in order to capture the full  complexity of the original task. In other words, there are not enough constraints  to approximate well the target function f*. Therefore overfitting will occur for  smaller classifier complexity and the optimal network will be smaller.  4 EXPERIMENTAL RESULTS  To demonstrate the possible outcomes of the method described above in several  cases, we have performed the following experiment. A random neural network  "teacher" was created as the target function fx. This is a two layer perceptron  with 20 inputs, 5 hidden units and one output. A set of M random binary input  examples was created and the teacher network was used to classify the training  examples. Namely, a desired output yi was obtained by recording the output of  the teacher net when input i was presented to the network, and the output was  calculated by applying the usual feed forward dynamincs:  (5)  This binary threshold update rule is applied to each of the network's units j, i.e  the hidden and the output units. The weights of the teacher were chosen from a  uniform distribution [-1,1]. No threshold (bias weights) were used.  The set of scrambled training sets S t' was produced as explained above and different  network architectures were trained on t to produce the set of classifiers f. The  learning networks are standard two layer networks of sigmoid units, trained by con-  jugate gradient back-propagation, using a quadratic error function with tolerance,  i.e. if the difference between an output of the net and the desired 0 or 1 target is  smaller than the tolerance (taken as 0.2 in our experiment) it does not contribute  to the error. The tolerance is, of course, another parameter which may influences  the complexity of the resulting network, however, in this experiment it is fixed.  The quantities Q.f(p), Q,(p) and D(p) were calculated for networks with 1,2,3,..7  hidden units (1 hidden unit means just a perceptron, trained with the same error  function). In our terminology, the architecture specification is part of the set of  348 Grossman and Lapedes  Training Set Size  hidden units 400 700 1024  1 0.Sl (0.04) 0.Sl (0.001) 0.82 (0.001)  2 0.81 (0.04) 0.84 (o.os) 0.80 (0.04)  3 0.78 (0.02 0.82 (0.00) 0.9 (0.03)  4 0.77 (0.03) 0.Sl (0.05) 0.9 (0.03)  5 0.74 (0.03) 0.79(0.03) 0.87 (0.04)  6 0.74 (0.01) 0.S0 (0.05) 0.89(0.03)  7 0.71 (0.01) 0.70 (0.02) 0.8s (o.os)  Table 1: The prediction rate for 1..7 hidden units, averaged on 4 nets that were  trained on the noisefree training set of size M = 400,700, 1024 (the standard devi-  ation is given in parenthesis).  parameters P that is input to the learning algorithm . The goal is to identify the  "correct" architecture according to the behavior of Q f, Q, and D with p.  The experiment was done with three training set sizes M = 400, 700 and 1024.  Another set of m = 1000 random examples was used to calculate D. As an "ex-  ternal control" this set was also classified by the teacher network and was used to  measure the generalization (or prediciton rate) of the different learning networks.  The prediction rate, for the networks trained on the noise free training set (aver-  aged over 4 networks, trained with different random initial weights) is given for  the 1 to 7 hidden unit architectures, for the 3 sizes of M, in Table 1. The noise  sensitivity signatures of three architectures trained with M = 400 (1,2,3 hidden  units) and with M = 1024 examples (2,4,0 units) are shown in Figure 1. Compare  these (representative) results with the expected behaviour of the NSS as described  qualitatively in the previous section.  5 CONCLUSIONS and DISCUSSION  We have introduced a method of testing a learning model (with its learning algo-  rithm) against a learning task given as a finite set of examples, by producing and  characterizing its "noise sensitivity signature". Relying on the experimental results  presented here, and similar results obtained with other (less artificial) learning tasks  and algorithms, we suggest some guidelines for using the NSS for model tuning:  1. If D(p) approaches zero with p -4 0, or if Q.t(P) is significantly better than  Q,(p) for noise levels up to 0.3 or more - the network/model complexity can be  safely inreased.  2. If Qy(p) < Qn(P) already for small levels of noise (say 0.2 or less)- reduce the  network complexity.  3. In more delicate situations: a "good" model will have at least a trace of concavity  in D(p). A clearly convex D(p) probably indicates an over-parametrized model. In  a "good" model choice, Q.(p) will follow Qy(p) closely, from below, up to a high  noise level.  Use of Bad Training Data for Better Predictions 349  1  O6  04  02  0  0  08  06  04  02  08  06  04  005 01 015 02 025 03 035 04 04,5 45  005 01 015 02 025 03 035 04 045 0 005 01 015 02 025 03 035 04 045  .... ~..,. 400e:ampls 3 hdden umt  005 015 02 025 03 035 04 045 0 005 01 015 02 025 03 035 04 045  Figure 1: The signatures (Q and D vs. p) of networks with 1,2,3 hidden units (top  to bottom) trained on M=400 examples (left), and networks with 2,4,6 hidden units  trained on M=1024 examples. The (noisy) training set score Q,(p) is plotted with  full line, the noise free score Qf(p) with dotted line, and the average functional  distance D(p) with error bars (representing the standard deviation of the distance).  350 Grossman and Laperies  5.1 Advanatages of the Method  1. The method uses all the data for training. Therefore we can extract all the  available information. Unlike validation set methods - there is no need to spare  part of the examples for testing (note that classified examples are not needed for  the functional distance estimation). This may be an important advantage when  the data is limited. As the experiment presented here shows: taking 300 examples  out of the 1024 given, may result in choosing a smaller network that will give  inferior prediction (see table 1). Using "delete-1 cross-validation" will minimize  this problem but will need at least as much computation as the NSS calculation in  order to achieve reliable prediction estimation.  2. It is an "external" method, i.e. independent of the classifier and the training  algorithm. It can be used with neural nets, decision trees, boolean circuits etc. It  can evaluate different classifiers, algorithms or stopping/prunning criteria.  5.2 Disadvantages  1. Computationally expensive (but not prohibitively so). In principle one can use  just a few noise levels to reduce computational cost.  2. Presently requires a subjective decision in order to identify the signature, unlike  cross-validation methods which produce one number. In some situations, the noise  sensitivity signature gives no clear distinction between similar architectures. In  these cases, however, there is almost no difference in their generalization rate.  Acknowledgements  We thank David Wolpert, Michael Perrone and Jerom Friedman for many iluminat-  ing discussions and usefull comments. We also thank Rob Farher for his invaluable  help with software and for his assistance with the Connection Machine.  References Le Cun Y., Denker J.S. and Solla S. (1990), in Adv. in NIPS 2, Touretzky D.S. ed.  (Morgan Kaufmann 1990) 598.  Rissanen J. (1989), Stochastic Complezity in Statistical Inquiry (World Scientific  1989).  Stone M. (1974), J.Roy. Statist. Soc. Ser. B 36 (1974) 111.  Wiegend A.S. (1994), in the Proc. of the 1993 Connectionist Models Summer School,  edited by M.C. Mozer, P. Smolensky, D.S. Touretzky, J.L. Elman and A.S. Weigend,  pp. 335-342 (Erlbaum Associates, Hillsdale NJ, 1994).  Wiegend A.S., Rummelhart D. and Huberman B.A. (1991), in Adv. in NIPS 3,  Lippmann et al. eds. (Morgen Kaufmann 1991) 875.  
Optimal Stochastic Search and  Adaptive Momentum  Todd K. Leen and Genevieve B. Orr  Oregon Graduate Institute of Science and Technology  Department of Computer Science and Engineering  P.O.Box 91000, Portland, Oregon 97291-1000  Abstract  Stochastic optimization algorithms typically use learning rate  schedules that behave asymptotically as (t) = o/t. The ensem-  ble dynamics (Leen and Moody, 1993) for such algorithms provides  an easy path to results on mean squared weight error and asymp-  totic normality. We apply this approach to stochastic gradient  algorithms with momentum. We show that at late times, learning  is governed by an effective learning rate e# - o/(1 -/) where  / is the momentum parameter. We describe the behavior of the  asymptotic weight error and give conditions on e# that insure  optimal convergence speed. Finally, we use the results to develop  an adaptive form of momentum that achieves optimal convergence  speed independent of o.  I Introduction  The rate of convergence for gradient descent algorithms, both batch and stochastic,  can be improved by including in the weight update a "momentum" term propor-  tional to the previous weight update. Several authors (Tugay and Tanik, 1989;  Shynk and Roy, 1988) give conditions for convergence of the mean and covariance  of the weight vector for momentum LMS with constant learning rate. However  stochastic algorithms require that the learning rate decay over time in order to  achieve true convergence of the weight (in probability, in mean square, or with  probability one).  477  478 Leen and O  This paper uses our previous work on weight space probabilities (Leen and Moody,  1993; Orr and Leen, 1993) to study the convergence of stochastic gradient algo-  rithms with annealed learning rates of the form/ = Io/t, both with and without  momentum. The approach provides simple derivations of previously known results  and their extension to stochastic descent with momentum. Specifically, we show  that the mean squared weight misadjustment drops off at the maximal rate o 1/t  only if the effective learning rate Ieff -/o/(1 - 1) is greater than a critical value  which is determined by the Hessian.  These results suggest a new algorithm that automatically adjusts the momentum  coefficient to achieve the optimal convergence rate. This algorithm is simpler than  previous approaches that either estimate the curvature directly during the descent  (Venter, 1967) or measure an auxilliary statistic not directly involved in the opti-  mization (Darken and Moody, 1992).  2 Density Evolution and Asymptotics  We consider stochastic optimization algorithms with weight w E R N. We confine  attention to a neighborhood of a local optimum w, and express the dynamics in  terms of the weight error v -- w - w,. For simplicity we treat the continuous time  algorithm   v(t)  = (t) H[v(t),x(t)] (1)  dt  where (t) is the learning rate at time t, H is the weight update function and  x(t) is the data fed to the algorithm at time t. For stochastic gradient algorithms  H = -Vv (v, x(t) ), minus the gradient of the instantaneous cost function.  Convergence (in mean square) to w. is characterized by the average squared norm  of the weight error E [ Iv 12 ] - Trace C where  c _= f (2)  is the weight error correlation matrix and P(v, t) is the probability density at v and  time t. In (Leen and Moody, 1993) we show that the probability density evolves  according to the Kramers-Moyal expansion  8P(v,t)   (_1) i v  i----1 Jl,...ji=l  Ovj Ovj2 ... Ovj  1Although algorithms are executed in discrete time, continuous time formulations are  often advantagous for analysis. The passage from discrete to continuous time is treated  in various ways depending on the needs of the theoretical exposition. Kushner and Clark  (1978) define continous time functions that interpolate the discrete time process in order  to establish an equivalence between the asymptotic behavior of the discrete time stochastic  process, and solutions of an associated deterministic differential equation. Heskes et al.  (1992) draws on the results of Bedeaux et al. (1971) that link (discrete time) random  walk trajectories to the solution of a (continuous time) master equation. Heskes' master  equation is equivalent to our Kramers-Moyal expansion (3).  Optimal Stochastic Search and Adaptive Momentum 479  where Hj denotes the j}n component of the N-component vector H, and {..  denotes averaging over the density of inputs. Differentiating (2) with respect to  time, using (3) and integrating by parts, we obtain the equation of motion for the  weight error correlation  at = (t) any e(v,t)[ <H(,x):O>x + <H,x)>  /(t) 2 / dNv P(v,t) (H(v,x) H(v,x) ' ) (4)  2.1 Asymptotics of the Weight Error Correlation  Convergence of v can be understood by studying the late time behavior of (4).  Since the update function H(v, x) is in general non-linear in v, the time evolution  of the correlation matrix Cij is coupled to higher moments E[vi vj vk...] of the  weight error. However, the learning rate is assumed to follow a schedule/(t) that  satisfies the requirements for convergence in mean square to a local optimum. Thus  at late times the density becomes sharply peaked about v - 0 2. This suggests  that we expand H(v, x) in a power series about v - 0 and retain the lowest order  non-trivial terms in (4) leaving:  dC  = -(t) [ (R c) + (CRT)] + (t). D , (5)  dt  where R is the Hessian of the average cost function {  ), and  D = (H(O,x) H(O,x)  ) (6)  is the diffusion matrix, both evaluated at the local optimum w,. (Note that R " =  R.) We use (5) with the understanding that it is valid for large t. The solution to  (5) is  C(t) = U(t, to) C(to) Ur(t, to) +  where the evolution operator U(t2, tl) is  U(tp.,tl)  fia u()' rr(t, ) o v r (t, )  (7)  = exp [-i' a-u0-) ] (s)  We assume, without loss of generality, that the coordinates are chosen so that R is  diagonal (D won't be) with eigenvalues Ai, i = 1... N. Then with/(t) = lao/t we  obtain  E[[v[ 2] = Trace[C(t)] = i.. 1 Cii(to)  + (2uo Ai - 1) t to T . (9)  2In general the density will have nonzero components outside the basin of w.. We are  neglecting these, for the purpose of calculating the second moment of the the local density  in the vicinity of w..  480 Leen and On'  We define  1  - (lO)  Icrit -- 2 Amin  and identify two regimes for which the behavior of (9) is fundamentally different:  1. Io > Icrit: E[lvl ] drops off asymptotically as 1/t.  1 o A., )  2. Po < Icrit: E[lvl 2 ] drops off asymptotically as ( ? )<2  i.e. more slowly than 1/t.  Figure 1 shows results from simulations of an ensemble of 2000 networks trained by  LMS, and the prediction from (9). For the simulations, input data were drawn from  a gaussian with zero mean and variance R = 1.0. The targets were generated by  a noisy teacher neuron (i.e. targets =w.x + , where <) = 0 and <') = as). The  upper two curves in each plot (dotted) depict the behavior for o < Icrit -- 0.5.  The remaining curves (solid) show the behavior for o > Icrit   '7,  00 ]000 5000 50000 ]00 000 5000 50000  t t  Fig. l' LEFT - Simulation results from an ensemble of 2000 one-dimensional  LMS algorithms with R = 1.0, 62 _ 1.0 and /t = tto/t. RIGHT - Theo-  retical predictions from equation (9). Curves correspond to (top to bottom)  o = 0.2, 0.4, 0.6, 0.8, 1.0, 1.5.  By minimizing the coefficient of 1/t in (9), the optimal learning rate is found to  be Iopt = 1/X,in. This formalism also yields asymptotic normality rather simply  (Orr and Leen, 1994). These conditions for "optimal" (i.e. l/t) convergence of  the weight error correlation and the related results on asymptotic normality have  been previously discussed in the stochastic approximation literature (Darken and  Moody, 1992; Goldstein, 1987; White, 1989; and references therein) . The present  formal structure provides the results with relative ease and facilitates the extension  to stochastic gradient descent with momentum.  3 Stochastic Search with Constant Momentum  The discrete time algorithm for stochastic optimization with momentum is:  v(t + 1) = v(t) + (t) H[v(t),x(t)] +/3 f(t)  (ii)  Optimal Stochastic Search and Adaptive Momentum 481  (t + 1)  or in continuous time,  v(t)  dt  = v(t + 1)-(t)  = f(t) + !(t) H[v(t),x(t)] + (/3- 1) f(t),  (12)  - I(t) H[v(t),x(t)] +/3 f(t) (13)  aa(t)  dt  = i(t) H[v(t),x(t)] + (/3- 1) f(t).  (14)  As before, we are interested in the late time behavior of E[lvl 2 ]. To this end, we  define the 2N-dimensional variable Z _= (v, f)T and, following the arguments of  the previous sections, expand H[v(t), x(t)] in a power series about v - 0 retaining  the lowest order non-trivial terms. In this approximation the correlation matrix   -- E[ZZ T] evolves according to  with  = K + K T +/(t) 2  (15)  dt  ) - )  K = -u(t)n (/3- 1) , D = D D '  I is the N x N identity matrix, and R and D are defined as before. The evolution  operator is now  - ]  U(t2,tl) = exp dr K(r) (17)  1  and the solution to (15) is  U = (t, to) U(to) uT(t, to) + dr t2(r) U(t,r)  uT(t,r) (18)  The squared norm of the weight error is the sum of first N diagonal elements of C.  In coordinates for which R is diagonal and with/(t) = Io/t, we find that for t >> to  E[lvl2]  1 (to) +  /0 2 Dii  (1 -/3) (2/oAi - 1 +/3)  to  (19)  This reduces to (9) when/3 = 0. Equation (19) defines two regimes of interest:  1. /z0/(1 --/3) > lZcrit: E[lvl '] drops off asymptotically as 1/t.  2. to/(1 -/3) < tcrit: E[Ivl 2] drops off asymptotically as  2taOmin  i.e. more slowly than 1It.  482 Leen and Orr  The form of (19) and the conditions following it show that the asymptotics of  gradient descent with momentum are governed by the effective learning rate  Figure 2 compares simulations with the predictions of (19) for fixed/z0 and various  /3. The simulations were performed on an ensemble of 2000 networks trained by  LMS as described previously but with an additional momentum term of the form  given in (11). The upper three curves (dotted) show the behavior of E[Ivl 9'] for  lteff < ltcrit. The solid curves show the behavior for l%# > lZcrit.  The derivation of asymptotic normality proceeds similarly to the case without mo-  mentum. Again the reader is referred to (Orr and Leen, 1994) for details.  100 1000 5000 50000  t  Fig.2:  gorithms with momentum with R -  Theoretical predictions from equation  /3 = 0.0, 0.4, 0.5, 0.6, 0.7, 0.8 .  100 1000 5000 50000  t  LEFT - Simulation results from an ensemble of 2000 one-dimensional LMS al-  1.0, cr 2 = 1.0, and /z0 = 0.2. RIGHT-  (19). Curves correspond to (top to bottom)  4 Adaptive Momentum Insures Optimal Convergence  The optimal constant momentum parameter is obtained by minimizing the coeffi-  cient of 1It in (19). Imposing the restriction that this parameter is positive s gives  /3opt = max(O,  (20)  As with lZopt, this result is not of practical use because, in general, ,X,i, is unknown.  For 1-dimensional linear networks, an alternative is to use the instantaneous esti-  mate of A, X(t) = x2(t) where x(t) is the network input at time t. We thus define  the adaptive momentum parameter to be  max(O, 1 -/zox ') (1-dimension).  (21)  An algorithm based on (21) insures that the late time convergence is optimally fast.  An alternative route to achieving the same goal is to dispense with the momentum  term and adaptively adjust the learning rate. Vetnet (1967) proposed an algorithm  3E[ivl 2] diverges for 1/31 > 1. For -1 < /3 < O, E[lvl 2] appears to converge but oscil-  lations are observed. Additional study is required to determine whether/3 in this range  might be useful for improving learning.  Optimal Stochastic Search and Adaptive Momentum 483  that iteratively estimates  for 1-D algorithms and uses the estimate to adjust  Darken and Moody (1992) propose measuring an auxiliary statistic they call "drift"  that is used to determine whether or not /Zo > lZcrit. The adaptive momentum  scheme generalizes to multiple dimensions more easily than Vetner's algorithm,  and, unlike Darken and Moody's scheme, does not involve calculating an auxiliary  statistic not directly involved with the minimization.  A natural extension to N dimensions is to define a matrix of momentum coefficients,  ? - ! - lzo x x T, where ! is the N x N identity matrix. By zeroing out the negative  eigenvalues of % we obtain the adaptive momentum matrix  /3actap - I- CXX T, where c = min(/o, 1/(xTx)). (22)  Log( E[ Ivl 2] ) Log( E[ Ivl 2] )  6  4  3 4  2 2  1 o  0 -2  -1  I 2 3  Log(t) -4  1 2 3 4 5  Fig.3: Simulations of 2-D LMS with 1000 networks initialized at vo - (.2,.3) and with  ff2 = 1, hi = .4, 2 --' 4, and itctit = 1.25. LEFT- / = 0, RIGHT - / =/aaapt. Dashed  curves correspond to adaptive momentum.  Log(t)  Figure 3 shows that our adaptive momentum not only achieves the optimal con-  vergence rate independent of the learning rate parameter/o but that the value of  log(E[Iv]9']) at late times is nearly independent of/o and smaller than when mo-  mentum is not used. The left graph displays simulation results without momentum.  Here, convergence rates clearly depend on/o and are optimal for/o >/crit - 1.25.  When/o is large there is initially significant spreading in v so that the increased  convergence rate does not result in lower log(E[]v]9']) until very late times (t > 105).  The graph on the right shows simulations with adaptive momentum. Initially, the  spreading is even greater than with no momentum, but log(E[Iv[9']) quickly decreases  to reach a much smaller value. In addition, for t > 300, the optimal convergence  rate (slope--I) is achieved for all three values of/o and the curves themselves lie  almost on top of one another. In other words, at late times (t > 300), the value of  log(E[lv[9']) is independent of/o when adaptive momentum is used.  5 Summary  We have used the dynamics of the weight space probabilities to derive the asymp-  totic behavior of the weight error correlation for annealed stochastic gradient algo-  rithms with momentum. The late time behavior is governed by the effective learning  rate/eff-- /o/(1 -/3). For learning rate schedules/o/t, if/eff > 1/(2 Am/n), then  the squared norm of the weight error v -- co - co. falls off as 1/t. From these results  we have developed a form of momentum that adapts to obtain optimal convergence  rates independent of the learning rate parameter.  484 Leen and Orr  Acknowledgments  This work was supported by grants from the Air Force Office of Scientific Research  (F49620-93-1-0253) and the Electric Power Research Institute (RP8015-2).  References  D. Bedeaux, K. Laktos-Lindenberg, and K. Shuler. (1971) On the Relation Between  Master Equations and Random Walks and their Solutions. Journal of Mathematical  Physics, 12:2116-2123.  Christian Darken and John Moody. (1992) Towards Faster Stochastic Gradient  Search. In J.E. Moody, S.J. Hanson, and R.P. Lipmann (eds.) Advances in Neural  Information Processing Systems, vol. J. Morgan Kaufmann Publishers, San Mateo,  CA, 1009-1016.  Larry Goldstein. (1987) Mean Square Optimality in the Continuous Time Robbins  Monro Procedure. Technical Report DRB-306, Dept. of Mathematics, University  of Southern California, LA.  H.J. Kushner and D.S. Clark. (1978) Stochastic Approximation Methods for Con-  strained and Unconstrained Systems. Springer-Verlag, New York.  Tom M. Heskes, Eddy T.P. Slijpen, and Bert Kappen. (1992) Learning in Neural  Networks with Local Minima. Physical Review A, 46(8):5221-5231.  Todd K. Leen and John E. Moody. (1993) Weight Space Probability Densities in  Stochastic Learning: I. Dynamics and Equilibria. In Giles, Hanson, and Cowan  (eds.), Advances in Neural Information Processing Systems, vol. 5, Morgan Kauf-  mann Publishers, San Mateo, CA, 451-458.  G. B. Orr and T. K. Leen. (1993) Weight Space Probability Densities in Stochastic  Learning: II. Transients and Basin Hopping Times. In Giles, Hanson, and Cowan  (eds.), Advances in Neural Information Processing Systems, vol. 5, Morgan Kauf-  mann Publishers, San Mateo, CA, 507-514.  G. B. Orr and T. K. Leen. (1994) Momentum and Optimal Stochastic Search. In  M. C. Mozer, P. Smolensky, D. S. Touretzky, J. L. Elman, and A. S. Weigend (eds.),  Proceedings of the 1993 Connectionist Models Summer School, 351-357.  John J. Shynk and Sumit Roy. (1988) The LMS Algorithm with Momentum Up-  dating. Proceedings of the IEEE International Symposium on Circuits and Systems,  2651-2654.  Mehmet Ali Tugay and Yalin Tanik. (1989) Properties of the Momentum LMS  Algorithm. Signal Processing, 18:117-127.  J. H. Venter. (1967) An Extension of the Robbins-Monro Procedure. Annals of  Mathematical Statistics, 38:181-190.  Halbert White. (1989) Learning in Artificial Neural Networks: A Statistical Per-  spective. Neural Computation, 1:425-464.  
Autoencoders, Minimum Description Length  and Helmholtz Free Energy  Geoffrey E. Hinton  Department of Computer Science  University of Toronto  6 King's College Road  Toronto M5S 1A4, Canada  Richard S. Zemel  Computational Neuroscience Laboratory  The Salk Institute  10010 North Torrey Pines Road  La Jolla, CA 92037  Abstract  An autoencoder network uses a set of recognition weights to convert an  input vector into a code vector. It then uses a set of generative weights to  convert the code vector into an approximate reconstruction of the input  vector. We derive an objective function for training autoencoders based  on the Minimum Description Length (MDL) principle. The aim is to  minimize the information required to describe both the code vector and  the reconstruction error. We show that this information is minimized  by choosing code vectors stochastically according to a Boltzmann distri-  bution, where the generafive weights define the energy of each possible  code vector given the input vector. Unfortunately, if the code vectors  use distributed representations, it is exponentially expensive to compute  this Boltzmann distribution because it involves all possible code vectors.  We show that the recognition weights of an autoencoder can be used to  compute an approximation to the Boltzmann distribution and that this ap-  proximation gives an upper bound on the description length. Even when  this bound is poor, it can be used as a Lyapunov function for learning  both the generafive and the recognition weights. We demonstrate that  this approach can be used to learn factorial codes.  1 INTRODUCTION  Many of the unsupervised learning algorithms that have been suggested for neural networks  can be seen as variations on two basic methods: Principal Components Analysis (PCA)  3  4 Hinton and Zemel  and Vector Quantization (VQ) which is also called clustering or competitive learning.  Both of these algorithms can be implemented simply within the autoencoder framework  (Baldi and Hornik, 1989; Hinton, 1989) which suggests that this framework may also  include other algorithms that combine aspects of both. VQ is powerful because it uses  a very non-linear mapping from the input vector to the code but weak because the code  is a purely local representation. Conversely, PCA is weak because the mapping is linear  but powerful because the code is a distributed, factoriel representation. We describe a  new objective function for training autoencoders that allows them to discover non-linear,  factoriel representations.  2 THE MINIMUM DESCRIPTION LENGTH APPROACH  One method of deriving a cost function for the activities of the hidden units in an autoencoder  is to apply the Minimum Description Length (MDL) principle (Rissanen 1989). We imagine  a communication game in which a sender observes an ensemble of training vectors and must  then communicate these vectors to a receiver. For our purposes, the sender can walt until  all of the input vectors have been observed before communicating any of them - an online  method is not required. Assuming that the components of the vectors are finely quantized  we can ask how many bits must be communicated to allow the receiver to reconstruct the  input vectors perfectly. Perhaps the simplest method of communicating the vectors would  be to send each component of each vector separately. Even this simple method requires  some further specification before we can count the number of bits required. To send the  value, zi,c, of component i of input vector c we must encode this value as a bit string. If  the sender and the receiver have already agreed on a probability distribution that assigns  a probability p(z) to each possible quantized value, z, Shannon's coding theorem implies  that z can be communicated at a cost that is bounded below by - 1ogp(z) bits. Moreover,  by using block coding techniques we can get arbitrarily close to this bound so we shall  treat it as the true cost. For coding real values to within a quantization width of t it is  often convenient to assume a Gaussian probability distribution with mean zero and standard  deviation r. Provided that r is large compared with t, the cost of coding the value z is then  -log/+ 0.5 log 2ro '2 + z2/2o '.  This simple method of communicating the training vectors is generally very wasteful. If  the components of a vector are correlated it is generally more efficient to convert the input  vector into some other representation before communicating it. The essence of the MDL  principle is that the best model of the data is the one that minimizes the total number of  bits required to communicate it, including the bits required to describe the coding scheme.  For an autoencoder it is convenient to divide the total description length into three terms.  An input vector is communicated to the receiver by sending the activities of the hidden  units and the residual differences between the true input vector and the one that can be  reconstructed from the hidden activities. There is a code cost for the hidden activities and a  reconstruction cost for the residual differences. In addition there is a one-time model cost  for communicating the weights that are required to convert the hidden activities into the  output of the net. This model cost is generally very important within the MDL framework,  but in this paper we will ignore it. In effect, we are considering the limit in which there is  so much data that this limited model cost is negligible.  PCA can be viewed as a special case of MDL in which we ignore the model cost and we limit  the code cost by only using m hidden units. The question of how many bits are required  Autoencoders, Minimum Description Length, and Helmhotz Free Energy 5  to code each hidden unit activity is also ignored. Thus the only remaining term is the  reconstruction cost. Assuming that the residual differences are encoded using a zero-mean  Gaussian with the same predetermined variance for each component, the reconstruction  cost is minimized by minimizing the squared differences.  Similarly, VQ is a version of MDL in which we limit the code cost to at most log m bits by  using only m winner-take-all hidden units, we ignore the model cost, and we minimize the  reconstruction cost.  In standard VQ we assume that each input vector is converted into a specific code. Sur-  prisingly, it is more efficient to choose the codes stochastically so that the very same input  vector is sometimes communicated using one code and sometimes using another. This type  of"stochastic VQ" is exactly equivalent to maximizing the log probability of the data under  a mixture of Gaussians model. Each code of the VQ then corresponds to the mean of a  Gaussian and the probability of picking the code is the posterior probability of the input  vector under that Gaussian. Since this derivation of the mixture of Gaussians model is  crucial to the new techniques described later, we shall describe it in some detail.  2.1 The "bits-back" argument  The description length of an input vector using a particular code is the sum of the code cost  and reconstruction cost. We define this to be the energy of the code, for reasons that will  become clear later. Given an input vector, we define the energy of a code to be the sum  of the code cost and the reconstruction cost. If the prior probability of code i is ri and its  squared reconstruction error is d the energy of the code is  k d 2  Ei = - log ri - k log t +  log 2rr  + 2r-- 5 (1)  where k is the dimensionality of the input vector, r 2 is the variance of the fixed Gaussian  used for encoding the reconstruction errors and t is the quantization width.  Now consider the following situation: We have fitted a VQ to some training data and, for a  particular input vector, two of the codes are equally good in the sense that they have equal  energies. In a standard VQ we would gain no advantage from the fact that there are two  equally good codes. However, the fact that we have a choice of two codes should be worth  something. It does not matter which code we use so if we are vague about the choice of  code we should be able to save one bit when communicating the code.  To make this argument precise consider the following communication game: The sender  is already communicating a large number of random bits to the receiver, and we want to  compute the additional cost of communicating some input vectors. For each input vector  we have a number of alternative codes hi ...hi...h, and each code has an energy, Ei. In a  standard VQ we would pick the code, j, with the lowest energy. But suppose we pick code  i with a probability pi that depends on Ei. Our expected cost then appears to be higher  since we sometimes pick codes that do not have the minimum value of E.  < Cost  (2)  where < ... > is used to denote an expected value. However, the sender can use her  freedom of choice in stochastically picking codes to communicate some of the random  6 Hinton and Zemel  bits that need to be communicated anyway. It is easy to see how random bits can be used  to stochastically choose a code, but it is less obvious how these bits can be recovered by  the receiver, because he is only sent the chosen code and does not know the probability  distribution from which it was picked. This distribution depends on the particular input  vector that is being communicated. To recover the random bits, the receiver waits until all  of the training vectors have been communicated losslessly and then runs exactly the same  learning algorithm as the sender used. This allows the receiver to recover the recognition  weights that are used to convert input vectors into codes, even though the only weights that  are explicitly communicated from the sender to the receiver are the generative weights that  convert codes into approximate reconstructions of the input. After learning the recognition  weights, the receiver can reconstruct the probability distribution from which each code was  stochastically picked because the input vector has already been communicated. Since he  also knows which code was chosen, he can figure out the random bits that were used to do  the picking. The expected number of random bits required to pick a code stochastically is  simply the entropy of the probability distribution over codes  H = - E pi log pi  (3)  So, allowing for the fact that these random bits have been successfully communicated, the  F = EpiEi - H  true expected combined cost is  i  Note that F has exactly the form of Helmholtz free energy.  probability distribution which minimizes F is  -Ei  Pi -- -j e_E  o)  It can be shown that the  (5)  This is exactly the posterior probability distribution obtained when fitting a mixture of  Gaussians to an input vector.  The idea that a stochastic choice of codes is more efficient than just choosing the code with  the smallest value of E is an example of the concept of stochastic complexity (Rissanen,  1989) and can also be derived in other ways. The concept of stochastic complexity is  unnecessarily complicated if we are only interested in fitting a mixture of Gaussians.  Instead of thinking in terms of a stochastically chosen code plus a reconstruction error,  we can simply use Shannon's coding theorem directly by assuming that we code the input  vectors using the mixture of Gaussians probability distribution. However, when we start  using more complicated coding schemes in which the input is reconstructed from the  activities of several different hidden units, the formulation in terms of F is much easier  to work with because it liberates us from the constraint that the probability distribution  over codes must be the optimal one. There is generally no efficient way of computing the  optimal distribution, but it is nevertheless possible to use F with a suboptimal distribution  as a Lyapunov function for learning (Neal and Hinton, 1993). In MDL terms we are simply  using a suboptimal coding scheme in order to make the computation tractable.  One particular class of suboptimal distributions is very attractive for computational reasons.  In a factorial distribution the probability distribution over m a alternatives factors into d  independent distributions over m alternatives. Because they can be represented compactly,  Autoencoders, Minimum Description Length, and Helmhotz Free Energy 7  factorial distributions can be computed conveniently by a non-stochastic feed-forward  recognition network.  3 FACTORIAL STOCHASTIC VECTOR QUANTIZATION  Instead of coding the input vector by a single, stochastically chosen hidden unit, we could  use several different pools of hidden units and stochastically pick one unit in each pool.  All of the selected units within this distributed representation are then used to reconstruct  the input. This amounts to using several different VQs which cooperate to reconstruct the  input. Each VQ can be viewed as a dimension and the chosen unit within the VQ is the  value on that dimension. The number of possible distributed codes is m a where d is the  number of VQs and m is the number of units within a VQ. The weights from the hidden  units to the output units determine what output is produced by each possible distributed  code. Once these weights are fixed, they determine the reconstruction error that would be  caused by using a particular distributed code. If the prior probabilities of each code are also  fixed, Eq. 5 defines the optimal probability distribution over distributed codes, where the  index i now ranges over the m a possible codes.  Computing the correct distribution requires an amount of work that is exponential in d, so  we restrict ourselves to the suboptimal distributions that can be factored into d independent  distributions, one for each VQ. The fact that the correct distribution is not really factorial  will not lead to major problems as it does in mean field approximations of Boltzmann  machines (Galland, 1993). It will simply lead to an overestimate of the description length  but this overestimate can still be used as a bound when learning the weights. Also the excess  bits caused by the non-independence will force the generafive weights towards values that  cause the correct distribution to be approximately factorial.  3.1 Computing the Expected Reconstruction Error  To perform gradient descent in the description length given in F-xl. 4, it is necessary to  compute, for each training example, the derivative of the expected reconstruction cost with  respect to the activation probability of each hidden unit. An obvious way to approximate  this derivative is to use Monte Carlo simulations in which we stochastically pick one hidden  unit in each pool. This way of computing derivatives is faithful to the underlying stochastic  model, but it is inevitably either slow or inaccurate. Fortunately, it can be replaced by a  fast exact method when the output units are linear and there is a squared error measure for  the reconstruction. Given the probability, by, of picking hidden unit i in VQ v, we can  compute the expected reconstructed output y for output unit j on a given training case  V = b +  w,h (6)  where b is the bias of unit j and w is the generafive weight from i to j in VQ v. We  can also compute the variance in the reconstructed output caused by the stochastic choices  within the VQs. Under the assumption that the stochastic choices within different VQs are  independent, the variances contributed by the different VQs can simply be added.  (  (7)  8 Hinton and Zemel  The expected squared reconstruction error for each output unit is V. + (tj - dj)2 where dj is  the desired output. So if the reconstruction error is coded assuming a zero-mean Gaussian  distribution the expected reconstruction cost can be computed exactly . It is therefore  straightforward to compute the derivatives, with respect to any weight in the network, of all  the terms in Eq. 4.  4 AN EXAMPLE OF FACTORIAL VECTOR QUANTIZATION  Zemel (1993) presents several different data sets for which factorial vector quantization  (FVQ) produces efficient encodings. We briefly describe one of those examples. The data  set consists of 200 images of simple curves as shown in figure 1. A network containing 4  VQs, each containing 6 hidden units, is trained on this data set. After training, the final  outgoing weights for the hidden units are as shown in figure 2. Each VQ has learned  to represent the height of the spline segment that connects a pair of control points. By  chaining these four segments together the image can be reconstructed fairly accurately. For  new images generated in the same way, the description length is approximately 18 bits for  the reconstruction cost and 7 bits for the code. By contrast, a stochastic vector quantizer  with 24 hidden units in a single competing group has a reconstruction cost of 36 bits and  a code cost of 4 bits. A set of 4 separate stochastic VQs each of which is trained on a  different 8z3 vertical slice of the image also does slightly worse than the factorial VQ (by  5 bits) because it cannot smoothly blend the separate segments of the curve together. A  purely linear network with 24 hidden units that performs a version of principal components  analysis has a slightly lower reconstruction cost but a much higher code cost.  Random  Y  Positiom  Fixed x Positions  Figure 1: Each image in the spline dataset is generated by fitting a spline to 5 control  points with randomly chosen t-positions. An image is formed by blurting the spline with  a Gaussian. The intensity of each pixel is indicated by the area of white in the display. The  resulting images are 8x12 pixels, but have only 5 underlying degrees of freedom.  Each VQ contributes non-Gaussian noise and the combined noise is also non-Gaussian. But  since its variance is known, the expected cost of coding the reconslxuction error using a Gaussian  prior can be computed exactly. The fact that this prior is not ideal simply means that the computed  reconslxuction cost is an upper bound on the cost using a better prior.  Autoencoders, Minimum Description Length, and Helmhotz Free Energy 9  Figure 2: The outgoing weights of the hidden units for a network containing 4 VQs with 6  units in each, trained on the spline dataset. Each 8x12 weight block corresponds to a single  unit, and each row of these blocks corresponds to one VQ.  5 DISCUSSION  A natural approach to unsupervised learning is to use a generative model that defines a  probability distribution over observable vectors. The obvious maximum likelihoodlearning  procedure is then to adjust the parameters of the model so as to maximize the sum of the  log probabilities of a set of observed vectors. This approach works very well for generative  models, such as a mixture of Gaussians, in which it is tractable to compute the expectations  that are required for the application of the EM algorithm. It can also be applied to the wider  class of models in which it is tractable to compute the derivatives of the log probability of  the data with respect to each model parameter. However, for non-linear models that use  distributed codes it is usually intractable to compute these derivatives since they require that  we integrate over all of the exponentially many codes that could have been used to generate  each particular observed vector.  The MDL principle suggest a way of making learning tractable in these more complicated  generative models. The optimal way to code an observed vector is to use the correct  posterior probability distribution over codes given the current model parameters. However,  we are free to use a suboptimal probability distribution that is easier to compute. The  description length using this suboptimal method can still be used as a Lyapunov function  for learning the model parameters because it is an upper bound on the optimal description  length. The excess description length caused by using the wrong distribution has the form  of a Kullback-Liebler distance and acts as a penalty term that encourages the recognition  weights to approximate the correct distribution as well as possible.  There is an interesting relationship to statistical physics. Given an input vector, each  possible code acts like an alternative configuration of a physical system. The function  10 Hinton and Zemel  E defined in Eq. 1 is the energy of this configuration. The function F in Eq. 4 is  the Helmholtz free energy which is minimized by the thermal equilibrium or Boltzmann  distribution. The probability assigned to each code at this minimum is exactly its posterior  probability given the parameters of the generative model. The difficulty of performing  maximum likelihood learning corresponds to the difficulty of computing properties of the  equilibrium distribution. Learning is much more tractable if we use the non-equilibrium  Helmholtz free energy as a Lyapunov function 0geal and Hinton, 1993). We can then use  the recognition weights of an autoencoder to compute some non-equilibrium distribution.  The derivatives of F encourage the recognition weights to approximate the equilibrium  distribution as well as they can, but we do not need to reach the equilibrium distribution  before adjusting the generafive weights that define the energy function of the analogous  physical system.  In this paper we have shown that an autoencoder network can learn factorial codes by using  non-equilibrium Helmholtz free energy as an objective function. In related work (Zemel  and Hinton 1994) we apply the same approach to learning population codes. We anticipate  that the general approach described here will be useful for a wide variety of complicated  generafive models. It may even be relevant for gradient descent learning in situations where  the model is so complicated that it is seldom feasible to consider more than one or two of  the innumerable ways in which the model could generate each observation.  Acknowledgements  This research was supported by grants from the Ontario Information Technology Research  Center, the Institute for Robotics and Intelligent Systems, and NSERC. Geoffrey Hinton  is the Noranda Fellow of the Canadian Institute for Advanced Research. We thank Peter  Dayan, Yann Le Cun, Radford Neal and Chris Williams for helpful discussions.  References  Baldi, P. and Hornik, K. (1989) Neural networks and principal components analysis: Learn-  ing from examples without local minima. Neural Networks, 2, 53-58.  Galland, C. C. (1993) The limitations of deterministic Boltzmann machine learning. Net-  work, 4, 355-379.  Hinton, G. E. (1989) Connectionist learning procedures. Artificial Intelligence, 40, 185-  234.  Neal, R., and Hinton, G. E. (1993) A new view of the EM algorithm that justifies incremental  and other variants. Manuscript available from the authors.  Rissanen, J. (1989) Stochastic Complexity in Statistical Inquiry. World Scientific Publish-  ing Co., Singapore.  Zemel, R. S. (1993) A Minimum Description Length Framework for Unsupervised Learning.  PhD. Thesis, Department of Computer Science, University of Toronto.  Zemel, R. S. and Hinton, G. E. (1994) Developing Population Codes by Minimizing  Description Length. In J. Cowan, G. Tesauro, and J. Alspector (Eds.), Advances in Neural  Information Processing Systems 6, San Mateo, CA: Morgan Kaufmann.  
Central and Pairwise Data Clustering by  Competitive Neural Networks  Joachim Buhmann & Thomas Hofmann  Rheinische Friedrich-Wilhelms-Universitfit  Institut f/Jr Informatik II, R6merstrage 164  D-53117 Bonn, Fed. Rep. Germany  Abstract  Data clustering amounts to a combinatorial optimization problem to re-  duce the complexity of a data representation and to increase its precision.  Central and pairwise data clustering are studied in the maximum en-  tropy framework. For central clustering we derive a set of reestimation  equations and a minimization procedure which yields an optimal num-  ber of clusters, their centers and their cluster probabilities. A meanfield  approximation for pairwise clustering is used to estimate assignment  probabilities. A selfconsistent solution to multidimensional scaling and  pairwise clustering is derived which yields an optimal embedding and  clustering of data points in a d-dimensional Euclidian space.  1 Introduction  A central problem in information processing is the reduction of the data complexity with  minimal loss in precision to discard noise and to reveal basic structure of data sets. Data  clustering addresses this tradeoff by optimizing a cost function which preserves the original  data as complete as possible and which simultaneously favors prototypes with minimal  complexity (Linde et al., 1980; Gray, 1984; Chou et al., 1989; Rose et al., 1990). We dis-  cuss an objective function for the joint optimization of distortion errors and the complexity  of a reduced data representation. A maximum entropy estimation of the cluster assign-  ments yields a unifying framework for clustering algorithms with a number of different  distortion and complexity measures. The close analogy of complexity optimized clustering  with winner-take-all neural networks suggests a neural-like implementation resembling  topological feature maps (see Fig. 1).  104  Central and Pairwise Data Clustering by Competitive Neural Networks 105  xi  Figure 1: Architecture of a three  layer competitive neural network  for central data clustering with  d neurons in the input layer, K  neurons in the clustering layer  with activity (Mi) and G neu-  rons in the classification layer.  The output neurons estimate the  conditional probability P, li of  data point i being in class 7-  Given is a set of data points which are characterized either by coordinates {xi [xi C d; i -  1,..., N} or by pairwise distances {ikli, k - 1,..., N}. The goal of data clustering  is to determine a partitioning of a data set which either minimizes the average distance  of data points to their cluster centers or the average distance between data points of the  same cluster. The two cases are refered to as central or pairwise clustering. Solutions  to central clustering are represented by a set of data prototypes {YIY c =  1 .... , K }, and the size K of that set. The assignments { Mi] o = 1,..., K;i = 1 .... , N},  Mi  {0, 1} denote that data point i is uniquely assigned to cluster  (, Mi, = 1).  Rate distortion theo specifies the optimal choice of y being the cluster centroids, i.e.,   o  Mia oia(xi, ya) = 0. Given only a set of distances or dissimilities the solution  to pairwise clustering is chacterized by the expected assignment viables (Mi). The  complexity {C [ = 1 .... , K} of a clustering solution depends on the specific infomation  processing application at hand, in pticul, we assume that C is only a function of the  cluster probability p = i= l Mi IN. We propose the central clustering cost function  N K  i=1 =1  and the pairwise clustering cost function  N K N  The distortion and complexity costs are adjusted in size by the weighting parameter X. e  cost hnctions (1,2) have to be optimized in an imrafive hshion: (i) vary the assignment  r cc.pc t { Mia }) decrease;  variables Mi for a fixed number K of clusters such that the costs x  (ii) increment the number of clusters K  K + 1 and optimize Mi again.  Complexity costs which penalize small, sparsely populated clusters, i.e., C = 1/p,  1, 2 .... , favor equal clusmr probabilities, thereby emphasizing the hardware aspect of a  clustering solution. The special case s = 1 with constant costs per cluster coesponds  to K-means clustering. An alternative complexity measure which estimates encoding  costs for data compression and data transmission is the Shannon entropy of a cluster set  (C)  E,p,C, = - E,p, logp,.  106 Buhmann and Hofmann  The most common choice for the distortion measure are distances Dis =  which preserve the permutation symmetry of (1) with respect to the cluster index v. A data  partitioning scheme without permutation invariance of cluster indices is described by the  cost function  The generalized distortion error ((Dis)) = -'r T,Di,(x,, y,) between data point xi and  cluster center y quantifies the intrinsic quantization errors Di-r (xi, y,) and the additional  errors due to transitions T, from index 7 to a. Such transitions might be caused by noise  in communication channels. These index transitions impose a topological order on the  set of indices { a[ a = 1, .... K} which establishes a connection to self-organizing feature  maps (Kohonen, 1984; Ritter et al., 1992) in the case of nearest neighbor transitions in a  d-dimensional index space. We refer to such a partitioning of the data space as topology  preserving clustering.  2 Maximum Entropy Estimation of Central Clustering  Different combinations of complexity terms, distortion measures and topology constraints  define a variety of central clustering algorithms which are relevant in very different infor-  mation processing contexts. To derive robust, preferably parallel algorithms for these data  clustering cases, we study the clustering optimization problem in the probabilistic frame-  work of maximum entropy estimation. The resulting Gibbs distribution proved to be the  most stable distribution with respect to changes in expected clustering costs (Tikochinsky  et al., 1984) and, therefore, has to be considered optimal in the sense of robust statistics.  Statistical physics (see e.g. (Amit, 1989; Rose et al., 1990)) states that maximizing the  entropy at a fixed temperature T - 1/]3 is equivalent to minimizing the free energy   Tic = -TlnZ=-Tln( E exp(-fix))  - -N E p,20p, fi log exp C:)] (4)  with respect to the vmiables p, y. The effective complexity costs are C  0 C)/Opt.  For a derivation of (4) see (Buhmann, Kfihnel, 1993b).  The resulting re-estimation equations for the expected cluster probabilities and the expected  centroid positions are necess conditions of Wx being minimal, i.e.  1  i  1  0 '- EETT(MiT)_O T)i(xi'Y)' (6)  Oy  exp [--fi(((Di)) + XC;)]  (Mi) = x (7)  Central and Pairwise Data Clustering by Competitive Neural Networks 107  The expectation value (Mi) of the assignment variable Mi can be interpreted as a fuzzy  membership of data point xi in cluster a. The case of supervised clustering can be treated  in an analogous fashion (Buhmann, Kiihnel, 1993a) which gives rise to the third layer in the  neural network implementation (see Fig. 1). The global minimum of the free energy (4)  with respect to p,, y determines the maximum entropy solution of the cost function (1).  Note that the optimization problem (1) of a K v state space has been reduced to a K(d + 1 )  dimensional minimization of the free energy f'x (4). To find the optimal parameters p, y  and the number of clusters K which minimize the free energy, we start with one cluster  located at the centroid of the data distribution, split that cluster and reestimate p, y using  equation (5,6). The new configuration is accepted as an improved solution if the free energy  (4) has been decreased. This splitting and reestimation loop is continued until we fail to  find a new configuration with lower free energy. The temperature determines the fuzziness  of a clustering solution, whereas the complexity term penalizes excessively many clusters.  3 Meanfield Approximation for Pairwise Clustering  The maximum entropy estimation for pairwise clustering constitutes a much harder problem  than the calculation of the free energy for central clustering. Analytical expression for  the Gibbs distributions are not known except for the quadratic distance measure Dik =  (xi -- xk) 2. Therefore, we approximate the free energy by a variational principle commonly  refered to as meanfield approximation. Given the costfunction (2) we derive a lower bound  to the free energy by a system of noninteracting assignment variables. The approximative  costfunction with the variational parameters w is  K N  Z E (8)  v=l i=1  The original costfunction for pairwise clustering can be written as / -  K + V with a  (small) perturbation term V --  -  x due to cluster interactions. The partition function  = Zo(exp(-/SV))o > Zoexp(-/3(V)o)  (9)  is bound from below if terms of the order O(((V-(V)0) 3 )o) and higher are negligible com-  pared to the quadratic term. The angular brackets denote averages over all configurations  of the costfunction without interactions. The averaged perturbation term (V)o amounts to  (10)  being the averaged assignment variables  exp(-3m)  (Mm): E exp(-/w)'  (11)  108 Buhmann and Hofmann  The meanfield approximation with the cost function (8) yields a lower bound to the partition  function Z of the original pairwise clustering problem. Therefore, we vary the parameters  i, to maximize the quantity In Zo -/5(V)o which produces the best lower bound of Z  based on an interaction free costfunction. Variation of & leads to the conditions  (12)   being defined as  ( ' )  . _ i v,j (Mk)Vk + XC;. (13)  w - p---    For a given distance matrix Di the transcendental equations (11,12) have to be solved  simultaneously.  So far the i have been treated as independent variation parameters. An important  problem, which is usually discussed in the context of Multidimensional Scaling, is to  find an embedding for the data set in an Euclidian space and to cluster the embedded data.  The variational framework can be applied to this problem, if we consider the parameters  i as functions of data coordinates and prototype coordinates, i = Dic(Xi, Ya), e.g.  with a quadratic distortion measure Di(xi, y) = Ilxi - yc[[ 2. The variables xi, y C a  are the variational parameters which have to be determined by maximizing In Z0 -/5(V)o.  Without imposing the restriction for the prototypes to be the cluster centroids, this leads to  the following conditions for the data coordinates  (Mi) (w- i) (Y- E(Mo)Yu) = 0,  Vi C { 1, .... N}. (14)  After further algebraic manipulations we receive the explicit expression for the data points  ' E<Mw)(lly,[12- ,)(y,- E<Mi,)yu) (15)  /Cxi = 5 '  with the covariance matrix/Ci = ((yy')i- (y)i(y)/), (y)i = 5-,(Mw)y,. Let us  assume that the matrix/C is non-singular which imposes the condition K > d and the  cluster centers {yla = 1,..., K} being in general position. For K < d the equations  i, = i + i are exactly solvable and embedding in dimensions larger than K produces  non-unique solutions without improving the lower bound in (9).  Varying In Zo -/5(V)o with respect to y yields a second set of stationarity conditions  E(M5> (1-(Mss>)(j-*)(xs-yo ) = 0,  Vc G { 1 .... ,K). (16)  The weighting factors in (16), however, decay exponentially fast with the inverse temper-  ature, i.e., (Mss)(1 - (Mss))  0(/5 exp[-/sc]), c > 0. This implies that the optimal  solution for the data coordinates displays only a very weak dependence on the special  choice of the prototypes in the low temperature regime. Fixing the parameters y and  solving the transcendental equations (14,15) for xi, the solution will be very close to the  optimal approximation. It is thus possible to choose the prototypes as the cluster centroids  y = 1/(pN) 'i(Mic)xi and, thereby, to solve Eq. (15) in a self-consistent fashion.  Central and Pairwise Data Clustering by Competitive Neural Networks 109  a  b  Figure 2: A data distribution (4000 data points) (a), generated by four normally distributed  sources is clustered with the complexity measure C = - log p and A = 0.4 (b). The plus  signs (+) denote the centers of the Gaussians and stars (*) denote cluster centers. Figure (c)  shows a topology preserving clustering solution with complexity  = 1/p and external  noise 07 = 0.05).  If the prototype variables depend on the data coordinates, the derivatives Oy/Oxi will  not vanish in general and the condition (14) becomes more complicated. Regardless  of this complication the resulting algorithm to estimate data coordinates xi interleaves the  clustering process and the optimization of the embedding in a Euclidian space. The artificial  separation of multidimensional scaling from data clustering has been avoided. Data points  are embedded and clustered simultaneously. Furthermore, we have derived a maximum  entropy approximation which is most robust with respect to changes in the average costs  4 Clustering Results  Non-topological (Tav = 5.v) clustering results at zero temperature for the logarithmic  complexity measure (C = 1ogp) are shown in Fig. 2b. In the limit of very small com-  plexity costs the best clustering solution densely covers the data distribution. The specific  choice of logarithmic complexity costs causes an almost homogeneous density of cluster  centers, a phenomenon which is known from studies of asymptotic codebook densities and  which is explained by the vanishing average complexity costs () = -p log p of very  sparsely occupied clusters (for references see (Buhmann, Kiihnel, 1993b)).  Figure 2c shows a clustering configuration assuming a one-dimensional topology in index  space with nearest neighbor transitions. The short links between neighboring nodes of the  neural chain indicate that the distortions due to cluster index transitions have also been  optimized. Note, that complexity optimized clustering determines the length of the chain  or, for a more general noise distribution, an optimal size of the cluster set. This stopping  criterion for adding new cluster nodes generalizes self-organizing feature maps (Kohonen,  1984) and removes arbitrariness in the design of topological mappings. Furthermore, our  algorithm is derived from an energy minimization principle in contrast to self-organizing  feature maps which "cannot be derived as a stochastic gradient on any energy function"  (Erwin et al., 1992).  The complexity optimized clustering scheme has been tested on the real world task of  110 Buhmann and Hofmann  a  b  d  Figure 3: Quantization of a 128x 128, 8bit, gray-level image. (a) Original picture. (b)  Image reconstruction from wavelet coefficients quantized with entropic complexity. (c)  Reconstruction from wavelet coefficients quantized by K-means clustering. (d,e) Absolute  values of reconstruction errors in the images (b,c). Black is normalized in (d,e) to a deviation  of 92 gray values.  image compression (Bohmann, Kiihnel, 1993b). Entropy optimized clustering of wavelet  decomposed images has reduced the reconstruction error of the compressed images up to  30 percent. Images of a compression and reconstruction experiment are shown in Fig. 3.  The compression ratio is 24.5 for a 128 x 128 image. According to our efficiency criterion  entropy optimized compression is 36.8% more efficient than K-means clustering for that  compression factor. The peak SNR values for (b,c) are 30.1 and 27.1, respectively. The  considerable higher error near edges in the reconstruction based on K-means clustering (e)  demonstrates that entropy optimized clustering of wavelet coefficients not only results in  higher compression ratios but, even more important it preserves psychophysically important  image features like edges more faithfully than conventional compression schemes.  5 Conclusion  Complexity optimized clustering is a maximum entropy approach to central and pairwise  data clustering which determines the optimal number of clusters as a compromise between  distortion errors and the complexity of a cluster set. The complexity term tums out to be as  important for the design of a cluster set as the distortion measure. Complexity optimized  clustering maps onto a winner-take-all network which suggests hardware implementations  in analog VLSI (Andreou et al., 1991). Topology preserving clustering provides us with a  Central and Pairwise Data Clustering by Competitive Neural Networks 111  cost function based approach to limit the size of self-organizing maps.  The maximum entropy estimation for pairwise clustering cannot be solved analytically  but has to be approximated by a meanfield approach. This meanfield approximation of  the pairwise clustering costs with quadratic Euclidian distances establishes a connection  between multidimensional scaling and clustering. Contrary to the usual strategy which  embeds data according to their dissimilarities in a Euclidian space and, in a separate second  step, clusters the embedded data, our approach finds the Euclidian embedding and the data  clusters simultaneously and in a selfconsistent fashion.  The proposed framework for data clustering unifies traditional clustering techniques like  K-means clustering, entropy constraint clustering or fuzzy clustering with neural network  approaches such as topological vector quantizers. The network size and the cluster parame-  ters are determined by a problem adapted complexity function which removes considerable  arbitrariness present in other non-parametric clustering methods.  Acknowledgement: JB thanks H. Ktihnel for insightful discussions. This work was  supported by the Ministry of Science and Research of the state Nordrhein-Westfalen.  References  Amit, D. (1989). Modelling Brain Function. Cambridge: Cambridge University Press.  Andreou, A. G., Boahen, K. A., Pouliquen, P. O., Pavasovi6, A., Jenkins, R. E., Stro-  hbehn, K. (1991 ). Current Mode Subthreshold MOS Circuits for Analog VLSI Neural  Systems. IEEE Transactions on Neural Networks, 2, 205-213.  Buhmann, J., Ktihnel, H. (1993a). Complexity Optimized Data Clustering by Competitive  Neural Networks. Neural Computation, 5, 75-88.  Buhmann, J., Ktihnel, H. (1993b). Vector Quantization with Complexity Costs. IEEE  Transactions on Information Theory, 39(4), 1133-1145.  Chou, P. A., Lookabaugh, T., Gray, R. M. (1989). Entropy-Constrained Vector Quantization.  IEEE Transactions on Acoustics, Speech and Signal Processing, 37, 31-42.  Erwin, W., Obermayer, K., Schulten, K. (1992). Self-organizing Maps: Ordering, Conver-  gence Properties, and Energy Functions. Biological Cybernetics, 67, 47-55.  Gray, R. M. (1984). Vector Quantization. IEEE Acoustics, Speech and Signal Processing  Magazine, April, 4-29.  Kohonen, T. (1984). Self-organization and Associative Memory. Berlin: Springer.  Linde, Y., Buzo, A., Gray, R. M. (1980). An algorithm for vector quantizer design. IEEE  Transactions on Communications COM, 28, 84-95.  Ritter, H., Martinetz, T., Schulten, K. (1992). Neural Computation and Self-organizing  Maps. New York: Addison Wesley.  Rose, K., Gurewitz, E., Fox, G. (1990). Statistical Mechanics and Phase Transitions in  Clustering. Physical Review Letters, 65(8), 945-948.  Tikochinsky, Y., Tishby, N.Z., Levine, R. D. (1984). Alternative Approach to Maximum-  Entropy Inference. Physical Review A, 30, 2638-2644.  
Decoding Cursive Scripts  Yoram Singer and Naftali Tishby  Institute of Computer Science and  Center for Neural Computation  Hebrew University, Jerusalem 91904, Israel  Abstract  Online cursive handwriting recognition is currently one of the most  intriguing challenges in pattern recognition. This study presents a  novel approach to this problem which is composed of two comple-  mentary phases. The first is dynamic encoding of the writing tra-  jectory into a compact sequence of discrete motor control symbols.  In this compact representation we largely remove the redundancy of  the script, while preserving most of its intelligible components. In  the second phase these control sequences are used to train adaptive  probabilistic acyclic automata (PAA) for the important ingredients  of the writing trajectories, e.g. letters. We present a new and effi-  cient learning algorithm for such stochastic automata, and demon-  strate its utility for spotting and segmentation of cursive scripts.  Our experiments show that over 90% of the letters are correctly  spotted and identified, prior to any higher level language model.  Moreover, both the training and recognition algorithms are very  efficient compared to other modeling methods, and the models are  'on-line' adaptable to other writers and styles.  1 Introduction  While the emerging technology of pen-computing is already available on the world's  markets, there is an on growing gap between the state of the hardware and the  quality of the available online handwriting recognition algorithms. Clearly, the  critical requirement for the success of this technology is the availability of reliable  and robust cursive handwriting recognition methods.  833  834 Singer and Tishby  We have previously proposed a dynamic encoding scheme for cursive handwriting  based on an oscillatory model of handwriting [8, 9] and demonstrated its power  mainly through analysis by synthesis. Here we continue with this paradigm and use  the dynamic encoding scheme as the front-end for a complete stochastic model of  cursive script.  The accumulated experience in temporal pattern recognition in the past 30 years  has yielded some important lessons relevant to handwriting. The first is that one  can not predefine the basic 'units' of such temporal patterns due to the strong inter-  action, or 'coarticulation', between such units. Any reasonable model must allow for  the large variability of the basic handwriting components in different contexts and  by different writers. Thus true adaptability is a key ingredient of a good stochas-  tic model of handwriting. Most, if not all, currently used models of handwriting  and speech are hard to adapt and require vast amounts of training data for some  robustness in performance. In this paper we propose a simpler stochastic modeling  scheme, which we call Probabilistic Acyclic Automata (PAA), with the important  feature of being adaptive. The training algorithm modifies the architecture and  dimensionality of the model while optimizing its predictive power. This is achieved  through the minimization of the "description length" of the model and training  sequences, following the minimum description length (MDL) principle. Another  interesting feature of our algorithm is that precisely the same procedure is used in  both training and recognition phases, which enables continuous adaptation.  The structure of the paper is as follows. In section 2 we review our dynamic en-  coding method, used as the front-end to the stochastic modeling phase. We briefly  describe the estimation and quantization process, and show how the discrete motor  control sequences are estimated and used, in section 3. Section 4 deals with our  stochastic modeling approach and the PAA learning algorithm. The algorithm is  demonstrated by the modeling of handwritten letters. Sections 5 and 6 deal with  preliminary applications of our approach to segmentation and recognition of cursive  handwriting.  2 Dynamic encoding of cursive handwriting  Motivated by the oscillatory motion model of handwriting, as described e.g. by  Holierbach in 1981 [2], we developed a parameter estimation and regularization  method which serves for the analysis, synthesis and coding of cursive handwriting.  This regularization technique results in a compact and efficient discrete representa-  tion of handwriting.  Handwriting is generated by the human muscular motor system, which can be sim-  plified as spring muscles near a mechanical equilibrium state. When the movements  are small it is justified to assume that the spring muscles operate in the linear  regime, so the basic movements are simple harmonic oscillations, superimposed by  a simple linear drift. Movements are excited by selecting a pair of agonist-antagonist  muscles that are modeled by the spring pair. In a restricted form this simple motion  is described by the following two equations,  V(t) : d:(t): a cos(o:.t + c)) + c Vy(t) : (t) -- bcos(o:yt) , (1)  where Vx(t) and Vy(t) are the horizontal and vertical pen velocities respectively, wx  and wy are the angular velocities, a, b are the velocity amplitudes,  is the relative  Decoding Cursive Scripts 835  phase lag, and c is the horizontal drift velocity. Assuming that these describe  the true trajectory, the horizontal drift, c, is estimated as the average horizontal  1 N  velocity, b = y -]i=l V:(i). For fixed values of the parameters a, b, a; and d these  equations describe a cycloidal trajectory.  Our main assumption is that the cycloidal trajectory is the natural (free) pen mo-  tion, which is modified only at the velocity zero crossings. Thus changes in the  dynamical parameters occur only at the zero crossings and preserve the continuity  of the velocity field. This assumption implies that the angular velocities w,wy  and amplitudes a, b can be considered constant between consecutive zero crossings.  Denoting by t' and t, the i'th zero crossing locations of the horizontal and vertical  velocities, and by L and L, the horizontal and vertical progression during the i'th  interval, then the estimated amplitudes are, a - 7    Those  2(t+_t,) , b = 2(t,y+l_t,y).  amplitudes define the vertical and horizontal scales of the written letters.  Examination of the vertical velocity dynamics reveals the following: (a) There is  a virtual center of the vertical movement and velocity trajectory is approximately  symmetric around this center. (b) The vertical velocity zero crossings occur while  the pen is at almost fixed vertical levels which correspond to high, normal and low  modulation values, yielding altogether 5 quantized levels. The actual pen levels  achieved at the vertical velocity zero crossings vary around the quantized values,  with approximately normal distribution. Let the indicator, It (It  {1,..., 5}),  be the most probable quantized level when the pen is at the position obtained at  the t'th zero crossing. We need to estimate concurrently the 5 quantized levels  H,..., Hs, their variance cr (assumed the same for all levels), and the indicators  It. In this model the observed data is the sequence of actual pen levels L(t), while  the complete data is the sequence of levels and indicators {It,L(t)}. The task of  estimating the parameters {Hi, or} is performed via maximum likelihood estimation  from incomplete data, commonly done by the M algorithm[i] and described in [9].  The horizontal amplitude is similarly quantized to 3 levels.  After performing slant equalization of the handwriting, namely, orthogonalizing the  x and y motions, the velocities l;(t), (t) become approximately uncorrelated.  When w  wy, the two velocities are uncorrelated if there is a -t-900 phase-lag  between V: and l/s. There are also locations of total halt in both velocities (no pen  movement) which we take as a zero phase lag. Considering the vertical oscillations  as a 'master clock', the horizontal oscillations can be viewed as a 'slave clock' whose  phase and amplitude vary around the 'master clock'. For English cursive writing,  1 1 2} thus V  the frequency ratio between the two clocks is limited to the set {5, , ,  induces a grid for the possible V zero crossings. The phase-lag of the horizontal  oscillation is therefore restricted to the values 0 , 4-900 at the zero crossings of  V. The most likely phase-lag trajectory is determined by dynamic programming  over the entire grid. At the end of this process the horizontal oscillations are fully  determined by the vertical oscillations and the pen trajectory's description greatly  simplified.  The variations in the vertical angular velocity for a given writer are small, except  in short intervals where the writer hesitates or stops. The only information that  should be preserved is the typical vertical angular velocity, denoted by w. The  836 Singer and Tishby  normalized discretized equations of motion now become,  {  : aisin(wt q-j)q- 1 ai G {A,A,A} j  {-900,00,90 }   = bsin(wt) b  {Ht2 - Ht 11 < 1,12 _< 5} (2)  We used analysis by synthesis technique in order to verify our assumptions and  estimation scheme. The final result of the whole process is depicted in Fig. 1,  where the original handwriting is plotted together with its reconstruction from the  discrete representation.  Figure 1: The original and the fully quantized cursive scripts.  3 Discrete control sequences  The process described in the previous section results in a many to one mapping  from the continuous velocity field, V:(t), Vy(t), to a discrete set of symbols. This  set is composed of the cartesian product of the quantized vertical and horizontal  amplitudes and the phase-lags between these velocities. We treat this discrete con-  trol sequence as a cartesian product time series. Using the value '0' to indicate  that the corresponding oscillation continues with the same dynamics, a change in  the phase lag can be encoded by setting the code to zero for one dimension, while  switching to a new value in the other dimension. A zero in both dimensions in-  dicates no activity. In this way we can model 'pen ups'intervals and incorporate  auxiliary symbols like 'dashes','dots', and 'crosses', that play an important role in  resolving disambiguations between letters. These auxiliary are modeled as a sep-  arate channel and are ordered according to their X coordinate. We encode the  control levels by numbers from I to 5, for the 5 levels of vertical positions. The  quantized horizontal amplitudes are coded by 5 values as well: 2 for positive am-  plitudes (small and large), 2 for negative amplitudes, and one for zero amplitude.  Below is an example of our discrete representation for the handwriting depicted in  Fig. 1. The upper and lower lines encode the vertical and horizontal oscillations  respectively, and the auxiliary channel is omitted. In this example there is only one  location where both symbols are '0', indicating a pen-up at the end of the word.  24020420400 :t 005002040202204020402424204020500204020402400440240220  :t 040340304 :t 042032040 :t 0$00:t 05024253050 :t 050204:t 032403050033 :t 0500 :tOO0  4 Stochastic modeling of the motor control sequences  Existing stochastic modeling methods, such as Hidden Markov Models (HMM) [3],  suffer from several serious drawbacks. They suffer from the need to 'fix' a-priory the  Decoding Cursive Scripts 837  architecture of the model; they require large amounts of segmented training data;  and they are very hard to adapt to new data. The stochastic model presented here  is an on-line learning algorithm whose important property is its simple adaptability  to new examples. We begin with a brief introduction to probabilistic automata,  leaving the theoretical issues and some of the more technical details to another  place.  A probabilistic automaton is a 6-tuple (Q, E, r, 7, %, q), where Q is a finite set  of n states, E is an alphabet of size k, r : Q x E - Q is the state transition  function, 7: E x Q - [0, 1] is the transition (output) probability where for every  q  Q' oep. 7(crlq) = 1. q  Q is a start state, and q  Q is an end state. A  probabilistic automaton is called acyclic if it contains no cycles. We denote such  automata by PAA. This type of automaton is also known as a Markov process with  a single source and a single absorbing state. The rest of the states are all transient  states. Such automata induce non-zero probabilities on a finite set of strings. Given  an input string & - (rl,..., r) if at the of end its 'run' the automaton entered the  final state %, the probability of a string a is defined to be, P(a) - HV__l (dri]qi_l)  where qo = %, qi - -(qi-1, cri)- On the other hand, if q;v  q then P(a) - 0.  The inference of the PAA structure from data can be viewed as a communication  problem. Suppose that one wants to transmit an ensemble of strings, all created  by the same PAA. If both sides know the structure and probabilities of the PAA  then the transmitter can optimally encode the strings by using the PAA transition  probabilities. If only the transmitter knows the structure and the receiver has  to discover it while receiving new strings, each time a new transition occurs, the  transmitter has to send the next state index as well. Since the automaton is acyclic,  the possible next states are limited to those which do not form a cycle when the  .t be the number of legal next states  new edge is added to the automaton. Let kq  from a state q known to the receiver ar time t. Then the encoding of the next  state index requires at least log2(ktq + 1) bits. The receiver also needs to estimate  the state transition probability from the previously received strings. Let n(crlq ) be  the number of times the symbol r has been observed by the receiver while being in  state q. Then the transition probability is estimated by Laplace's rule of succession,  t the number of  n(elq)+l In sum if q is the current state and kq  (crJq)- -]o.,en(o.jq)+j j ,  possible next states known to the receiver, the number of bits required to encode the  next symbol r (assuming optimal coding scheme) is given by: (a) if the transition  r(q, r)has already been observed: -log2(P(r[q)); (b) if the transition r(q, r) has  never occurred before: -logu(iO(crlq))-4-logu(ktq + 1).  In training such a model from empirical observations it is necessary to infer the  structure of the PAA as well its parameters. We can thus use the above coding  scheme to find a minimal description length (MDL) of the data, provided that our  model assumption is correct. Since the true PAA is not known to us, we need to  imitate the role of the receiver in order to find the optimal coding of a message. This  can be done efficiently via dynamic programming for each individual string. After  the optimal coding for a single string has been found, the new states are added, the  transition probabilities/5(crJq) are updated and the number of legal next states kq  is recalculated. An example of the learning algorithm is given in Fig. 2, with the  estimated probabilities P, written on the graph edges.  838 Singer and Tishby  (a) '1. (b)   l.  (c)  0.5  .6  0.6  ',0,.33 .  Figure 2: Demonstration of the PAA learning algorithm. Figure (a) shows the  original automaton from which the examples were created. Figures (b)-(d) are the  intermediate automata built by the algorithm. Edges drawn with bold, dashed, and  grey lines correspond to transitions with the symbols '0', '1', and the terminating  symbol, respectively.  15 Automatic segmentation of cursive scripts  Since the learning algorithm of a PAA is an on-line scheme, only a small number  of segmented examples is needed in order to built an initial model. For cursive  handwriting we manually collected and segmented about 10 examples, for each  lower case cursive letter, and built 26 initial models. At this stage the models are  small and do not capture the full variability of the control sequences. Yet this set  of initial automata was sufficient to gradually segment cursive scripts into letters  and update the models from these segments. Segmented words with high likelihood  are fed back into the learning algorithm and the models are further refined. The  process is iterated until all the training data is segmented with high likelihood.  The likelihood of new data might not be defined due the incompleteness of the  automata, hence the learning algorithm is again applied in order to induce prob-  abilities. Let PiS, j be the probability that a model S (which represents a cursive  letter) generates the control symbols si,..., sj-1 (j > i). The log-likelihood of a  proposed segmentation (i:, i2,...,iN+:) of a word S1, S2,..., SN is,  N N  L((i1,.. ,iN+i)I(S'i .. ,S'N),(Sl .. ,SL)):log(l- IPS3.   , . , .  j=l j=l  The segmentation is calculated efficiently by maintaining a layers graph and using  dynamic programming to compute recursively the most likely segmentation. For-  mally, let ML(n, k) be the highest likelihood segmentation of the word up to the  Decoding Cursive Scripts 839  n'th control symbol and the k'th letter in the word. Then,  log }  ik_i<i<n  The best segmentation is obtained by tracking the most likely path from M(N, L)  back to M(1, 1). The result of such a segmentation is depicted in Fig. 3.  Figure 3: Temporal segmentation of the word impossible. The segmentation is  performed by applying the automata of the letters contained in the word, and  finding the Maximum-Likelihood sequence of models via dynamic programming.  6 Inducing probabilities for unlabeled words  Using this scheme we automatically segmented a database which contained about  1200 frequent english words, by three different writers. After adding the segmented  letters to the training set the resulting automata were general enough, yet very  compact. Thus inducing probabilities and recognition of unlabeled data could be  performed efficiently. The probability of locating letters in certain locations in new  unlabeled words (i.e. words whose transcription is not given) can be evaluated by  the automata. These probabilities are calculated by applying the various models  on each sub-string of the control sequence, in parallel. Since the automata can  accommodate different lengths of observations, the log-likelihood should be divided  by the length of the sequence. This normalized log-likelihood is an approximation  of the entropy induced by the models, and measures the uncertainty in determining  the transcription of a word. The score which measures the uncertainty of the occur-  1  rence of a letter S in place n in the a word is, Scove(nlS) = maxl T lg(P+-l)'  The result of applying several automata to a new word is shown in Fig. 4. High  probability of a given automaton indicates a beginning of a letter with the cor-  responding model. The probabilities for the letters k, a, e, b are plotted top to  bottom. The correspondence between high likelihood points and the relevant lo-  cations in the words are shown with dashed lines. These locations occur near the  'true' occurrence of the letter and indicate that these probabilities can be used for  recognition and spotting of cursire handwriting. There are other locations where  the automata obtain high scores. These correspond to words with high similarity to  the model letter and can be resolved by higher level models, similar to techniques  used in speech.  7 Conclusions and future research  In this paper we present a novel stochastic modeling approach for the analysis,  spotting, and recognition of online cursive handwriting. Our scheme is based on a  840 Singer and Tishby  Figure 4: The normalized log-likelihood scores induced by the automata for the  letters k, a, e, and b (top to bottom). Locations with high score are marked with  dashed lines and indicate the relative positions of the letters in the word.  discrete dynamic representation of the handwriting trajectory, followed by training  adaptive probabilistic automata for frequent writing sequences. These automata  are easy to train and provide simple adaptation mechanism with sufficient power  to capture the high variability of cursively written words. Preliminary experiments  show that over 90% of the single letters are correctly identified and located, without  any additional higher level language model. Methods for higher level statistical  language models are also being investigated [6], and will be incorporated into a  complete recognition system.  Acknowledgments  We would like to thank Dana Ron for useful discussions and Lee Giles for providing us  with the software for plotting finite state machines. Y.S. would like to thank the Clore  foundation for its support.  References  [1] A. Dempster, N. Laird, and D. Rubin. Maximum likelihood estimation from  incomplete data via the EM algorithm. J. Roy. Statist. Soc., 39(B):1-38, 1977.  [2] J.M. Holierbach. An oscillation theory of handwriting. Bio. Cyb., 39, 1981.  [3] L.R. Rabiner. A tutorial on hidden markov models and selected applications in  speech recognition. Proc. IEEE, pages 257-286, Feb. 1989.  [4] J. Rissanen. Modeling by shortest data description. Automatica, 14, 1978.  [5] J. Rissanen. Stochastic complexity and modeling. Annals of Star., 14(3), 1986.  [6] D. Ron, Y. Singer, and N. Tishby. The power of amnesia. In this volume.  [7] D.E. Rumelhart. Theory to practice: a  case study - recognizing cursire hand-  writing. In Proc. of 1992 NEC Conf. on Computation and Cognition.  [8] Y. Singer and N. Tishby. Dynamical encoding of cursire handwriting. In IEEE  Conference on Computer Vision and Pattern Recognition, 1993.  [9] Y. Singer and N. Tishby. Dynamical encoding of cursire handwriting. Technical  Report CS93-4, The Hebrew University of Jerusalem, 1993.  PART VII  IMPLEMENTATIONS  
Learning Classification with Unlabeled Data  Virginia R. de Sa  desa@cs. rochester. edu  Department of Computer Science  University of Rochester  Rochester, NY 14627  Abstract  One of the advantages of supervised learning is that the final error met-  ric is available during training. For classifiers, the algorithm can directly  reduce the number of misclassifications on the training set. Unfortu-  nately, when modeling human learning or constructing classifiers for au-  tonomous robots, supervisory labels are often not available or too ex-  pensive. In this paper we show that we can substitute for the labels by  making use of structure between the pattern distributions to different sen-  sory modalities. We show that minimizing the disagreement between the  outputs of networks processing patterns from these different modalities is  a sensible approximation to minimizing the number of misclassifications  in each modality, and leads to similar results. Using the Peterson-Barney  vowel dataset we show that the algorithm performs well in finding ap-  propriate placement for the codebook vectors particularly when the con-  fuseable classes are different for the two modalities.  1 INTRODUCTION  This paper addresses the question of how a human or autonomous robot can learn to classify  new objects without experience with previous labeled examples. We represent objects  with n-dimensional pattern vectors and consider piecewise-linear classifiers consisting of  a collection of (labeled) codebook vectors in the space of the input patterns (See Figure 1).  The classification boundaries are given by the voronoi tessellation of the codebook vectors.  Patterns are said to belong to the class (given by the label) of the codebook vector to which  they are closest.  112  Learning Classification with Unlabeled Data 113   XA  o  o  o  Figure 1: A piecewise-linear classifier in a 2-Dimensional input space. The circles represent data  samples from two classes (filled (A) and not filled (B)). The X's represent codebook vectors (They  are labeled according to their class A and B). Future patterns are classified according to the label of  the closest codebook vector.  In [de Sa and Ballard, 1993] we showed that the supervised algorithm LVQ2.1[Kohonen,  1990] moves the codebook vectors to minimize the number of misclassified patterns. The  power of this algorithm lies in the fact that it directly minimizes its final error measure (on  the training set). The positions of the codebook vectors are placed not to approximate the  probability distributions but to decrease the number of misclassifications.  Unfortunately in many situations labeled training patterns are either unavailable or ex-  pensive. The classifier can not measure its classification performance while learning (and  hence not directly maximize it). One such unsupervised algorithm, Competitive Learn-  ing[Grossberg, 1976; Kohonen, 1982; Rumelhart and Zipser, 1986], has unlabeled code-  book vectors that move to minimize a measure of the reconstruction cost. Even with sub-  sequent labeling of the codebook vectors, they are not well suited for classification because  they have not been positioned to induce optimal borders.  Supervised Unsupervised Self-Supervised  - implausible label - limited power - derives label from a  co-occuring input to  nCOWn another modality  Tget 000 O 0  000 000                   Input Input Input I Int 2  Figure 2:  The idea behind the algorithm  This paper presents a new measure for piecewise-linear classifiers receiving unlabeled pat-  terns from two or more sensory modalities. Minimizing the new measure is an approxi-  mation to minimizing the number of misclassifications directly. It takes advantage of the  structure available in natural environments which results in sensations to different sensory  modalities (and sub-modalities) that are correlated. For example, hearing "mooing" and  114 de Sa  P  o.5  o.  0.3  0.2  -2 -1 o  P  0.5  0.4  -'2 ' - 1  '(CB)p(x2CB)  0 1 2 3 4  Figure 3: This figure shows an example world as sensed by two different modalities. If modality A  receives a pattern from its Class A distribution, modality 2 receives a pattern from its own class A  distribution (and the same for Class B). Without receiving information about which class the patterns  came from, they must try to determine appropriate placement of the boundaries b and b2. P(Ci) is  the prior probability of Class i and p(xyICi) is the conditional density of Class i for modality j  seeing cows tend to occur together. So, although the sight of a cow does not come with an  internal homuncular "cow" label it does co-occur with an instance of a "moo". The key  is to process the "moo" sound to obtain a self-supervised label for the network processing  the visual image of the cow and vice-versa. See Figure 2.  2 USING MULTI-MODALITY INFORMATION  One way to make use of the cross-modality structure is to derive labels for the codebook  vectors (after they have been positioned either by random initialization or an unsupervised  algorithm). The labels can be learnt with a competitive learning algorithm using a network  such as that shown in Figure 4. In this network the hidden layer competitive neurons repre-  sent the codebook vectors. Their weights from the input neurons represent their positions  in the respective input spaces. Presentation of the paired patterns results in activation of  the closest codebook vectors in each modality (and O's elsewhere). Co-occurring code-  book vectors will then increase their weights to the same competitive output neuron. After  several iterations the codebook vectors are given the (arbitrary) label of the output neuron  to which they have the strongest weight. We will refer to this as the "labeling algorithm".  2.1 MINIMIZING DISAGREEMENT  A more powerful use of the extra information is for better placement of the codebook  vectors themselves.  In [de Sa, 1994] we derive an algorithm that minimizes 1 the disagreement between the  outputs of two modalities. The algorithm is originally derived not as a piecewise-linear  classifier but as a method of moving boundaries for the case of two classes and an agent  with two 1-Dimensional sensing modalities as shown in Figure 3.  Each class has a particular probability distribution for the sensation received by each modal-  ity. If modality 1 experiences a sensation from its pattern A distribution, modality 2 expe-  riences a sensation from its own pattern A distribution. That is, the world presents patterns  the goal is actually to find a non-trivial local minimum (for details see [de Sa, 1994])  Learning Classification with Unlabeled Data 115  Output (Class)   Implicit Labeling  Hidden Layer  Codebook  Ve21ri ((X7)  Modality/Network 1  Modality/Network 2  Figure 4: This figure shows a network for learning the labels of the codebook vectors. The weight  vectors of the hidden layer neurons represent the codebook vectors while the weight vectors of the  connections from the hidden layer neurons to the output neurons represent the output class that each  codebook vector currently represents. In this example there are 3 output classes and two modalities  each of which has 2-D input patterns and 5 codebook vectors.  from the 2-D joint distribution shown in Figure 5a) but each modality can only sample its  1-D marginal distribution (shown in Figure 3 and Figure 5a)).  We show [de Sa, 1994] that minimizing the disagreement error -- the proportion of pairs  of patterns for which the two modalities output different labels d  E(bl, b2)=Pr{xl <bl & x2>bl}+Pr{xl >bl & x2<b2} (1)  E(bl, b2) = f(xl, x2)dxldX2 + f(xl, x2)dxl dx2 (2)  2 1  (where f(x1, x2) = P(CA)p(xi [CA)p(x21CA) + P(CB)p(xl ICB)p(x2ICa) is the joint probability  density for the two modalities) in the above problem results in an algorithm that corresponds  to the optimal supervised algorithm except that the "label" for each modality's pattern is  the hypothesized output of the other modality.  Consider the example illustrated in Figure 5. In the supervised case (Figure 5a)) the labels  are given allowing sampling of the actual marginal distributions. For each modality, the  number of misclassifications can be minimized by setting the boundaries for each modality  at the crossing points of their marginal distributions.  However in the self-supervised system, the labels are not available. Instead we are given  the output of the other modality. Consider the system from the point of view of modality  2. Its patterns are labeled according to the outputs of modality 1. This labels the patterns  in Class A as shown in Figure 5b). Thus from the actual Class A patterns, the second  modality sees the "labeled" distributions shown. Letting a be the fraction of misclassified  patterns from Class A, the resulting distributions are given by (1 - a)P(CA)p(x2ICA) and  (a)P( CA)p(x2ICA).  Similarly Figure 5c) shows the effect on the patterns in class B. Letting b be the frac-  tion of Class B patterns misclassified, the distributions are given by (1 -b)P(CB)p(x21CB)  116 de Sa  and (b)P(CB)p(x21CB). Combining the effects on both classes results in the "labeled"  distributions shown in Figure 5d). The "apparent Class A" distribution is given by  (1 - a)P(C,)p(x21C,) + (b)P(CB)p(x2[C and the "apparent Class B" distribution by  (a)P(C,)p(x2[C,) + (1 -b)P(C)p(x21C). Notice that even though the approximated dis-  tributions may be discrepant, if a -- b, the crossing point will be close.  Simultaneously the second modality is labeling the patterns to the first modality. At each  iteration of the algorithm both borders move according to the samples from the "apparent"  marginal distributions.  - P(CA)p(x21CA) - P(CA)p(xllCA) - (a)P(CA}p(x2.1CA}  - p - P(CB)p(xllCB) - (1-a)P(CA)p(x21CA)  b)  - (1-b)P(CB)p(x2.1CB) - (a)P(CA)p(x2.1CA)+(1-b)P(CB)p(x21CB)  - (b)P(CB)p(x21C. B)  Figure 5: This figure shows an example of the joint and marginal distributions (For better visual-  ization the scale of the joint distribution is twice that of the marginal distributions) for the example  problem introduced in Figure 3. The darker gray represents patterns labeled "A", while the lighter  gray are labeled "B". The dark and light curves are the corresponding marginal distributions with  bold and regular labels respectively. a) shows the labeling for the supervised case. b),c) and d) reflect  the labels given by modality 1 and the corresponding marginal distributions seen by modality 2. See  text for more details  2.2 Self-Supervised Piecewise-Linear Classifier  The above ideas have been extended[de Sa, 1994] to rules for moving the codebook vectors  in a piecewise-linear classifier. Codebook vectors are initially chosen randomly from the  data patterns. In order to complete the algorithm idea, the codebook vectors need to be  given initial labels (The derivation assumes that the current labels are correct). In LVQ2.1  Learning Classification with Unlabeled Data 117  the initial codebook vectors are chosen from among the data patterns that are consistent  with their neighbours (according to a k-nearest neighbour algorithm); their labels are then  taken as the labels of the data patterns. In order to keep our algorithm unsupervised the  "labeling algorithm" mentioned earlier is used to derive labels for the initial codebook  vectors.  Also due to the fact that the codebook vectors may cross borders or may not be accurately  labeled in the initialization stage, they are updated throughout the algorithm by increas-  ing the weight to the output class hypothesized by the other modality, from the neuron  representing the closest codebook vector. The final algorithm is given in Figure 6  Randomly choose initial codebook vectors from data vectors  Initialize labels of codebook vectors using the labeling algorithm  described in text  Repeat for each presentation of input patterns Xl(n) and X2(n) to their  respective modalities   find the two nearest codebook vectors in modality 1 -- wl,i[, -* and  Wl,l 2 ,  modality 2 -- w2,k, w2,k to the respective input patterns   Find the hypothesized output class (CA, CB) in each modality (as  given by the label of the closest codebook vector)   For each modality update the weights according to the following  rules (Only the rules for modality 1 are given)  If neither or both Wl,i , Wl,i[ have the same label as w2, or Xl(n) does  not lie within c(n) of the border between them no updates are done,  otherwise  , . , (Xl (n) - wl i* (n - 1))  Wl,i* (n) = Wl,i(n - 1) + or[n)  [[Xl(rt)-Wl,i (n 1)ll  (Xl(n) - wl.?(n - 1))  wl.? (n) = w.j(n - 1)- or(n)  [p(, (n) - wi(n - 1)11  where wl,i* is the codebook vector with the same label, and wl.j* is  the codebook vector with another label.   update the labeling weights  Figure 6: The Self-Supervised piecewise-linear classifier algorithm  3 EXPERIMENTS  The following experiments were all performed using the Peterson and Barney vowel for-  mant data 2. The dataset consists of the first and second formants for ten vowels in a/hVd/  context from 75 speakers (32 males, 28 females, 15 children) who repeated each vowel  twice 3  To enable performance comparisons, each modality received patterns from the same  dataset. This is because the final classification performance within a modality depends  2obtained from Steven Nowlan  33 speakers were missing one vowel and the raw data was linearly transformed to have zero mean  and fall within the range [-3, 3] in both components  118 de Sa  Table 1' Tabulation of performance figures (mean percent correct and sample standard deviation  over 60 trials and 2 modalities). The heading i-j refers to performance measured after thej th step  during the i th iteration. (Note Step 1 is not repeated during the multi-iteration runs).  1-2 11-3  random pairing  2-2  75+4  77_+3  2-3  3,-3 14-3  5-3  76_+4  79_+2  same-paired vowels 60 _+ 5 73 _+ 4 76 _+ 4 76 _+ 4 76 _+ 4  60_+4 77+3 79_+2 79_+2 79_+2  not only on the difficulty of the measured modality but also on that of the other "labeling"  modality. Accuracy was measured individually (on the training set) for both modalities  and averaged. These results were then averaged over 60 runs. The results described below  are also tabulated in Table 1  In the first experiment, the classes were paired so that the modalities received patterns  from the same vowel class. If modality 1 received an [a] vowel, so did modality 2 and  likewise for all the vowel classes (i.e. p(xilCj) = p(x2lCj) for all j). After the labeling  algorithm stage, the accuracy was 60_+5% as the initial random placement of the codebook  vectors does not induce a good classifier. After application of the third step in Figure 6 (the  minimizing-disagreement part of the algorithm) the accuracy was 75 +4%. At this point the  codebook vectors are much better suited to defining appropriate classification boundaries.  It was discovered that all stages of the algorithm tended to produce better results on the  runs that started with better random initial configurations. Thus, for each run, steps 2 and  3 were repeated with the final codebook vectors. Average performance improved ( 73 _+ 4%  after step 2 and 76_+4% after step 3). Steps 2 and 3 were repeated several more times with  no further significant increase in performance.  The power of using the cross-modality information to move the codebook vectors can be  seen by comparing these results to those obtained with unsupervised competitive learn-  ing within modalities followed by an optimal supervised labeling algorithm which gave a  performance of 72%.  One of the features of multi-modality information is that classes that are easily confuseable  in one modality may be well separated in another. This should improve the performance of  the algorithm as the "labeling" signal for separating the overlapping classes will be more  reliable. In order to demonstrate this, more tests were conducted with random pairing of  the vowels for each run. For example presentation of [a] vowels to one modality would be  paired with presentation of [i] vowels to the other. That isp(x IC.) = p(x2lCtj) for a random  permutation ai, a2..aio. For the labeling stage the performance was as before (60 _+ 4%)  as the difficulty within each modality has not changed. However after the minimizing-  disagreement algorithm the results were better as expected. After 1 and 2 iterations of the  algorithm, 77 _+ 3% and 79 _+ 2% were classified correctly. These results are close to those  obtained with the related supervised algorithm LVQ2.1 of 80%.  4 DISCUSSION  In summary, appropriate classification borders can be learnt without an explicit external  labeling or supervisory signal. For the particular vowel recognition problem, the perfor-  mance of this "self-supervised" algorithm is almost as good as that achieved with super-  Learning Classification with Unlabeled Data 119  vised algorithms. This algorithm would be ideal for tasks in which signals for two or more  modalities are available, but labels are either not available or expensive to obtain.  One specific task is learning to classify speech sounds from images of the lips and the  acoustic signal. Stork et. al. [1992] performed this task with a supervised algorithm  but one of the main limitations for data collection was the manual labeling of the patterns  [David Stork, personal communication, 1993]. This task also has the feature that the speech  sounds that are confuseable are not confuseable visually and vice-versa [Stork et al., 1992].  This complementarity helps the performance of this classifier as the other modality provides  more reliable labeling where it is needed most.  The algorithm could also be used for learning to classify signals to a single modality where  the signal to the other "modality" is a temporally close sample. As the world changes  slowly over time, signals close in time are likely from the same class. This approach  should be more powerful than that of [F/51difik, 1991] as signals close in time need not be  mapped to the same codebook vector but the closest codebook vector of the same class.  Acknowledgements  I would like to thank Steve Nowlan for making the vowel formant data available to me.  Many thanks also to Dana Ballard, Geoff Hinton and Jeff Schneider for their helpful con-  versations and suggestions. A preliminary version of parts of this work appears in greater  depth in [de Sa, 1994].  References  [de Sa, 1994] Virginia R. de Sa, "Minimizing disagreement for self-supervised classification," In  M.C. Mozer, P. Smolensky, D.S. Touretzky, J.L. Elman, and A.S. Weigend, editors, Proceedings  of the 1993 Connectionist Models Summer School, pages 300--307. Erlbaum Associates, 1994.  [de Sa and Ballard, 1993] Virginia R. de Sa and Dana H. Ballard, "a note on learning vector quan-  tization," In C.L. Giles, S.J.Hanson, and J.D. Cowan, editors, Advances in Neural Information  Processing Systems 5, pages 220--227. Morgan Kaufmann, 1993.  [FOldifik, 1991] Peter FOldifik, "Learning Invariance from Transformation Sequences," Neural Com-  putation, 3(2): 194-200, 1991.  [Grossberg, 1976] Stephen Grossberg, "Adaptive Pattern Classification and Universal Recoding: I.  Parallel Development and Coding of Neural Feature Detectors," Biological Cybernetics, 23:121-  134, 1976.  [Kohonen, 1982] Teuvo Kohonen, "Self-Organized Formation of Topologically Correct Feature  Maps," Biological Cybernetics, 43:59-69, 1982.  [Kohonen, 1990] Teuvo Kohonen, "Improved Versions of Learning Vector Quantization," In IJCNN  International Joint Conference on Neural Networks, volume 1, pages 1-545-I-550, 1990.  [Rumelhart and Zipser, 1986] D. E. Rumelhart and D. Zipser, "Feature Discovery by Competitive  Learning," In David E. Rumelhart, James L. McClelland, and the PDP Research Group, editors,  Parallel Distributed Processing: Explorations in the Microstructure of Cognition, volume 2,  pages 151-193. MIT Press, 1986.  [Stork et al., 1992] David G. Stork, Greg Wolff, and Earl Levine, "Neural network lipreading system  for improved speech recognition," In IJCNN International Joint Conference on Neural Networks,  volume 2, pages II-286--II-295, 1992.  
Resolving motion ambiguities  K. I. Diamantaras  Siemens Corporate Research  755 College Rd East  Princeton, NJ 08540  D. Geiger*  Courant Institute, NYU  Mercer Street  New York, NY 10012  Abstract  We address the problem of optical flow reconstruction and in par-  ticular the problem of resolving ambiguities near edges. They oc-  cur due to (i) the aperture problem and (ii) the occlusion problem,  where pixels on both sides of an intensity edge are assigned the same  velocity estimates (and confidence). However, these measurements  are correct for just one side of the edge (the non occluded one).  Our approach is to introduce an uncertmnty field with respect to  the estimates and confidence measures. We note that the confi-  dence measures are large at intensity edges and larger at the con-  vex sides of the edges, i.e. inside corners, than at the concave side.  We resolve the ambiguities through local interactions via coupled  Markov random fields (MRF) The result is the detection of motion  for regions of images with large global convexity  I Introduction  In this paper we discuss the problem of figure ground separation, via optical flow, for  homogeneous images (textured images just provide more information for the disam-  biguation of figure-ground). We address the problem of optical flow reconstruction  and in particular the problem of resolving ambiguities near intensity edges. We  concentrate on a two frames problem, where all the motion ambiguities we discuss  can be disambiguiated by the human visual system.  *work done when the author was at the Isaac Newton Institute and at Siemens Corpo-  rate Research  977  978 Diamantaras and Geiger  Optical flow is a 2D (two dimensional) field defined as to capture the projection  of the 3D (three dimensional) motion field into the view plane (retina). The Horn  and Schunk[8] formulation of the problem is to impose (i) the brightness constraint  a:(,y,t) _ O, where E is the intensity image, and (ii) the smoothness of the velocity  dt --  field. The smoothness can be thought of coming from a rigidity or quasi-rigidity  assumption (see Ullman [12]).  We utilize two improvements which are important for the optical flow computation,  (i) the introduction of the confidence measure (Nagel and Enkelman [10], Anandan  [1]) and (ii) the application of smoothness while preserving discontinuities (Geman  and Geman [6], Blake and Zisserman [2], Mumford and Shah [9]). It is clear that as  an object moves with respect to a background not only optical flow discontinuities  occur, but also occlusions occur (and revelations). In stereo, occlusions are related  to discontinuities (e.g. Geiger et. al 1992 [5]), and for motion a similar relation  must exist. We study ambiguities ocuring at motion discontinuities and occlusions  in images.  The paper is organized as follows: Section 2 describes the problem with examples  and a brief discussion on possible approaches, section 3 presents our approach, with  the formulation of the model and a method to solve it, section 4 gives the results.  2 Motion ambiguities  Figure 1 shows two synthetic problems involving a translation and a rotation of  simple objects in front of stationary backgrounds.  Consider the case of the square translation (see figure la.). Humans perceive the  square translating, although block-matching (and any other matching technique)  gives translation on both sides of the square edges. Moreover, there are other inter-  pretations of the scene, such as the square belonging to the stationary background  and the outside being a translating foreground with a square hole. The examples  are synthetic, but emphasize the ambiguities. Real images may have more texture,  thus many times helping resolve these ambiguities, but not everywhere.  (a) (b)  Figure 1' Two image sequences of 128 x 128. (a) Square translation of 3 pixels; (b)  "Eight" rotation of 10o Note that the "eight" has concave and convex regions.  Resolving Motion Ambiguities 979  3 A Markov random field model  We describe a model capable of solving these ambiguities. It is based on coupled  Markov random fields and thus, based on local processes. Our main contribution is  to introduce the idea of uncertainty on the estimates and confidence measures. We  propose a Markov field that allows the estimates of each pixel to be chosen among  a large neighborhood, thus each pixel estimate can be neglected. We show that  convex regions of the image do bias the confidence measures such that the final  motion solutions are expected to be the ones with global larger convexity Note  that locally, one can have concave regions of a shape that give "wrong" bias (see  figure i b).  3.1 Block Matching  Block matching is the process of correlating a block region of one image, say of size  (2WM + 1) x (2WM + 1), with a block region of the other image. Block-matching yields  a set of matching errors di ' , where (i, j) is a pixel in the image and v: [ca, n] is  a displacement vector in a search window of size (2w$ + 1) x (2w$ + 1) around the  pixel. We define the velocity measurements gij and the covariance matrix Cij as the  mean and variance of the-vector v - [ca, n] averaged according to the distribution  m,n e-kd"v m,n e-kd (V -- gij)(V -- gij) T  gij -- _kd,, Vii  _kd,,  Figure 2 shows the block matching data gij for the two problems discussed above  and figure 3 shows the correspondent confidence measurse (inverse of the covariance  matrix as defined below).  3.2 The aperture problem and confidence  The aperture problem [7] occurs where there is a low confidence on the measure-  ments (data) in the direction along an edge; In particular we follow the approach  by [1].  The eigenvalues A, A2, of Cij correspond to the variance of distribution of v along  the directions of the corresponding eigenvectors Vl, v2o The confidence of the esti-  mate should be inversely proportional to the variance of the distribution, i.e. the  confidence along direction v (v2) is oc 1/A (cr 1/,X2). All this confidence informa-  tion can be packaged inside the confidence matrix defined as follows:  Rij: (cij +  where e is a very small constant that guarantees invertibility. Thus the eigenvalues of  lij are values between 0 and 1 corresponding to the confidence along the directions  v and v2, whereas v and v2 are still eigenvectors of lij.  The confidence measures at straight edges is high perpendincular to the edges and  low (zero) along the edges. However, at corners, the confidence is high on both  980 Diamantaras and Geiger  directions thus through smoothness this result can be propagated through the other  parts of the image, then resolving the aperture problem.  3.3 The localization problem and a binary decision field  The localization problem arises due to the local symmetry at intensity edges, where  both sides of an edge give the same correspondences. These cases occur when  occluded regions are homogeneous and so, block matching, pixel matching or any  matching technique can not distinguish which side of the edge is being occluded or  is occluding. Even if one considers edge based methods, the same problem arises in  the reconstruction stage, where the edge velocities have to be propagated to the rest  of the image. In this cases a localization uncertainty is introduced. More precisely,  pixels whose matching block contains a strong feature (e.g. a corner) will obtain a  high-confidence motion estimate along the direction in which this feature moved.  Pixels on both sides of this feature, and at distances less than half the matching  window size, M, will receive roughly the same motion estimates associated with  high confidences. However, it could have been just one of the two sides that have  moved in this direction. In that case this estimate should not be taken into account  on the other side. We note however a bias towards inside of corner regions from the  confidence measures.  Note that in a corner, despite both sides getting roughly the same velocity estimate  and high confidence measures, the inside pixel always get a larger confidence. This  bias is due to having more pixels outside the edge of a closed contour than outside,  and occurs at the convex regions (e.g. a corner). Thus, in general, the convex  regions will have a stronger confidence measure than outside them. Note that at  concavities in the "eight" rotation image, the confidence will be higher outside the  "eight" and correct at convex regions. Thus, a global optimization will be required  to decide which confidences to "pick up".  Our approach to resolve this ambiguity is to allow for the motion estimate at pixel  (i, j) to select data from a neighborhood Nij, and its goal is to maximize the total  estimates (taking into account the confidence measures). More precisely, let fij be  the vector motion field at pixel (i, j). We introduce a binary field cti  that indicates  which data gi+m,j+n in a neighborhood Nij of (i, j) should correspond to a motion  estimate fij. The size of Nij is given by M -{- 1 to overcome the localization  uncertainty. For a given lattice point (i, j) the boolean parameters cti3 should be  mutually exclusive, i.e. only one of them, ctj **, should be equal to 1 indicating  that fij should correspond to gi+,*,j+,*, while the rest cti , rn - m*, n - n*,  should be zero (or Y-rn*n*eN,3 ctir ** ---- 1). The conditional probability reflects  both an uncertainty due to noise and an uncertainty due to spatial localization of  the data  1  P(R, glf, a) = 22 exp{- '.  (2)  where Ilhl12 = hl + for h = [h,hy].  Resolving Motion Ambiguities 981  3.4 The piecewise smooth prior  The prior probability of the motion field fij is a piecewise smoothness condition,  in [6].  as  I  where hij = 0 (vij = 0) if there is no motion discontinuity separating pixels  (i- 1,j) ((i,j),(i,j- 1))9 otherwise hij - 1 (vii - 1). The parameter/ has to  be estimated. We have considered that the cost to create motion discontinuities  should be lowered at intensity edges (see Poggio et al. [11])9 i.e 7ij = 7(1 -  where eij is the intensity edge and 0 _< 5 <_ 1 and 7 have to be estimated.  3.5 The posterior distribution  The posterior distribution is given by Bayes' law  1 1 -v(j,,,;g)  P(f9 a9 h, v[g, R) - p(g, R) P(g' Rlf, a)P(f9 a, h9 v) = e  where  V(f9a,h,v)  (4)  ---- E( E Ozi? n[lli+m'j+n(fij --  ij mnN s  + (1 - hij)llfij -- fi-x,jl[ 2 + (1 - vij)llfij - fi,j-ll[ 2 d-  ij(hij -- vii)} (5)  is the energy of the system. Ideally, we would like to minimize V under all  possible configurations of the fields f, h, v and c9 while obeying the constraint  meN. "i? = 1.  3.6 Mean field techniques  Introducing the inverse temperature parameter fi(- l/T) we can obtain the trans-  formed probability distribution  P(f9alg9R ) - le-v(i') (6)  zz  where  ZZ  (7)  982 Diamantaras and Geiger  where i5 = (1 -Oii) and . -- (1 - 'd).  We have to obey the constraint meN,, ci  = 1. For the sake of simplicity we  have assumed that the neighborhood Nij around site (i,j) is Nij = {(i + rn, j +  n)  -1 _< rn _< 1,-1 _< n < 1}. The second factor in (7) can be explicitly  computed. Employing the mean field techniques proposed in [3] and extended in  [4] we can average out the variables h, v and c (including the constraint) and yield  (8)  which yields the following effective energy  since Z = Elf} e-V*1Y (') Using the saddle point approximation, i.e. considering  Z  e-Vejy (f) with f minimizing Vjy(f;g ), the mean field equations become  __ -m/, V  o - + +  v h -  with laij - /x(1- Oij) and I. tij -- /x(1- ij), Anfiy = (fij- fi-,j), Afij --  (j -- fi,j-), and  O ij :  1  , and ij = 1 + e'r,,-llAf,sll = (9)  The normalization constant Z called the partition function,  property that  1  lim- lnZ= min {V(f,a,h,v)}  has the important  (10)  Then using an annealing method we let  - oo and the minimum of V: -  In Z  approaches asymptotically the desired minimum.  Resolving Motion Ambiguities 983  4 Results  We have applied an iterative method along with an annealing schedule to solve the  above mean field equations for/ -- cw. The method was run on the two examples  already described Figure 4 depicts the results of the experiments. The system  chooses a natural interpretation (in agreement with human perception), namely  it interprets the object (e.g. the square in the first example or the eight-shaped  region in the second example) moving and the background being stationary. In the  beginning of the annealing process the localization field c may produce "erroneous"  results, however the neighbor information eventually forces the pixels outside the  moving object to coincide with the rest of the background which has zero motion  For the pixels inside the object, on the contrary, the neighbor information eventually  reinforces the adoption of the motion of the edges.  References  [1] P. Anandan, "Measuring Visual Motion from Image Sequences", PhD thesis.  COINS Dept., Univ. Massachusetts, Amherst, 1987.  [2] A. Blake and A. Zisserman, "Visual Reconstruction", Cambridge, Mass, MIT  press, 1987.  [3] D. Geiger and F. Girosi, "Parallel and Deterministic Algorithms for MRFs:  Surface Reconstruction and Integration", IEEE PAMI: 13(5), May 1991.  [4] D. Geiger and A. Yuille, "A Common Framework for Image Segmentation", Int.  J. Cornput. Vision, 6(3), ppo 227-243, 1991.  [5] D. Geiger and B. Ladendorf and A. Yuille "Binocular stereo with occlusion",  Computer Vision- ECCV92, ed. G Sandini, Springer-Verlag, 588, pp 423-433  May 1992o  [6] So Geman and D. Geman, "Stochastic Relaxation, Gibbs Distributions and the  Bayesian Restoration of Images", IEEE PAMI 6, pp 721-741, 1984  [7] E. C. Hildreth, "The measurement of visual motion", MIT press, 1983.  [8] B.K.P. Horn and B.G. Schunk, "Determining optical flow", Artificial Intelli-  gence, vol 17, pp 185-203, August 1981  [9] D. Mumford and J. Shah, "Boundary detection by minimizing functionals, I",  Proco IEEE Conf. on Computer Vision & Pattern Recognition San Francisco,  CA, 1985.  [10] H.-H. Nagel and W. Enkelmann "An Investigation of Smoothness Constraints  for the Estimation of Displacement Vector Fields from Image Sequences", IEEE  PAMI: 8, 1986.  [11] T. Poggio and E. B. Gamble and J. J. Little "Parallel Integration of Vision  Module", Science, vol 242, pp. 436-440, 1988.  [12] S. Ullman, "The Interpretation of Visual Motion", Cambridge, Mass, MIT  press, 1979.  984 Diamantaras and Geiger  (c)  Figure 2: Block matching data gi3. Both sides of the edges have the same data  (and same confidence). White represents motion to the right (x-direction) or up  (y-direction). Black is the complement. (a) The x-component of the data for the  square translation. (b) The x-component of the data for the rotation and (c) the  y-component of the data.  (a) (b)  Figure 3: The confidence R extracted from the block matching data gij. The display  is the sum of both eigenvalues, i.e. the trace of R. Both sides of the edges have the  same confidence. White represents high confidence. (a) For the square translation.  (b) For the rotation.  (a)  (b)  (c)  Figure 4: The final motion estimation, after 20000 iterations, resolved the ambigu-  ities with a natural interpretation of the scene /t: 10, 5 = 1, 3' = 100. (a) square  translation (b) x component of the motion rotation (c) y component of the motion  rotation  
Bayesian Modeling and Classification of  Neural Signals  Michael S. Lewicki  Computation and Neural Systems Program  California Institute of Technology 216-76  Pasadena, CA 91125  leickicns.caltech.edu  Abstract  Signal processing and classification algorithms often have limited  applicability resulting from an inaccurate model of the signal's un-  derlying structure. We present here an efficient, Bayesian algo-  rithm for modeling a signal composed of the superposition of brief,  Poisson-distributed functions. This methodology is applied to the  specific problem of modeling and classifying extracellular neural  waveforms which are composed of a superposition of an unknown  number of action potentials (APs). Previous approaches have had  limited success due largely to the problems of determining the spike  shapes, deciding how many are shapes distinct, and decomposing  overlapping APs. A Bayesian solution to each of these problems is  obtained by inferring a probabilistic model of the waveform. This  approach quantifies the uncertainty of the form and number of the  inferred AP shapes and is used to obtain an efficient method for  decomposing complex overlaps. This algorithm can extract many  times more information than previous methods and facilitates the  extracellular investigation of neuronal classes and of interactions  within neuronal circuits.  590  Bayesian Modeling and Classification of Neural Signals 591  1 INTRODUCTION  Extracellular electrodes typically record the activity of several neurons in the vicin-  ity of the electrode tip (figure 1). Most electrophysiological data is collected by  isolating action potentials (APs) from a single neuron by using a level detector or  window discriminator. Methods for extracting APs from multiple neurons can, in  addition to the obvious advantage of providing more data, provide the means to  investigate local hentonal interactions and response properties of neuronal popula-  tions. Determining from the voltage waveform what cell fired when is a difficult,  ill-posed problem which is compounded by the fact that cells frequently fire simul-  taneously resulting in large variations in the observed shapes.  There are three major difficulties in identifying and classifying action potentials  (APs) in a neuron waveform. The first is determining the AP shapes, the second is  deciding the number of distinct shapes, and the third is decomposing overlapping  spikes into their component parts. In general, these problems cannot be solved  independently, since the solution of one will affect the solution of the others.  Figure 1: Each neuron generates a stereotyped action potential (AP) which is observed  through the electrode as a voltage fluctuation. This shape is primarily a function of  the position of a neuron relative to the tip. The extracellular waveform shows several  different APs generated by an unknown number of neurons. Note the frequent presence of  overlapping APs which can completely obscure individual spikes.  The approach summarized here is to model the waveform directly to obtain a prob-  abilistic description of each action potential and, in turn, of the whole waveform.  This method allows us to compute the class conditional probabilities of each AP.  In addition, it is possible to quantify the certainty of both the form and number of  spike shapes. Finally, we can use this description to decompose overlapping APs  efficiently and assign probabilities to alternative spike sequences.  2 MODELING SINGLE ACTION POTENTIALS  The data from the event observed (at time zero) is modeled as resulting from  fixed underlying spike function, s(t), plus noise:  di -s(ti ;v)+ r/i, (1)  592 Lewicki  where v is the parameter vector that defines the spike function. The noise, r/, is  modeled as Gaussian with zero mean and standard deviation  From the Bayesian perspective, the task is to infer the posterior distribution of the  spike function parameters (assuming, for the moment, that rr. and rrw are known):  (2)  The two terms specifying the posterior distribution of v are 1) the probability of  the data given the model:  P(DIv' a" M) - ZD(a.) exp  2a2 y.(di - s(ti)) 2 , (3)  i=1  and 2) the prior assumptions of the structure of s(t) which are assumed to be of  the form:  P(vlrrw, M) orexp[-/dus(m)(u)'/r2]. (4)  The superscript (m) denotes differentiation which for these demonstrations we as-  sumed to be m = i corresponding to linear splines. The smoothness of s(t) is  controlled through rrw with small values of rrw penalizing large fluctuations.  The final step in determining the posterior distribution is to eliminate the depen-  dence of P(vID , rr. aw, M) on rr.and aw. Here, we use the approximation:  (s)  The most probable values of rr.and aw were obtained using the methods of MacKay  (1992) in which reestimation formulas are obtained from a Gaussian approximation  of the posterior distribution for r.and aw, P(rr., rrwlD, M). Correct inference of aw  prevents the spike function from overfitting the data.  3 MODELING MULTIPLE ACTION POTENTIALS  When a waveform contains multiple types of APs, determining the component spike  shapes is more difficult because the classes are not known a priori. The uncertainty  of which class an event belongs to can be incorporated with a mixture distribution.  The probability of a particular event, Dn, given all spike models, M:K, is  K  P(D.Iv:K, = P(D. Ivy, (6)  k--1  where r} is the a priori probability that a spike will be an instance of M}, and  As before, the objective is to determine the posterior distribution for the parameters  defining a set of spike models, P(v:,{, .rlD:N , rr., O'w, M:) which is obtained again  using Bayes' rule.  Bayesian Modeling and Classification of Neural Signals 593  Finding the conditions satisfied at a posterior maximum leads to the equation:  N  E P(M ID,, v, r, %)? . [d, -  (t, - v, 'v)] 0 (t,, v)  n=l ' ' OqVk  =0, (7)  where rn is the inferred occurrence time (typically to sub-sample period accuracy) of  the event D. This equation is solved iteratively to obtain the most probable values  of v:K. Note that the error for each event, Dn, is weighted by P(M [Dn, v, ,r, %)  which is the probability that the event is an instance of the kth spike model. This is  a soft clustering procedure, since the events are not explicitly assigned to particular  classes. Maximizing the posterior yields accurate estimates of the spike functions  even when the clusters are highly overlapping.  The techniques described in the previous section are used to determine the most  probable values for % and trw and, in turn, the most probable values of v:K and  4 DETERMINING THE NUMBER OF SPIKE SHAPES  Choosing a set of spike models that best fit the data, would result eventually in a  model for each event in the waveform. Heuristics might indicate whether two spike  models are identical or distinct, but ad hoc criteria are notoriously dependent on  particular circumstances, and it is difficult to state precisely what information the  rules take into account.  To determine the most probable number of spike models, we apply probability theory.  Let Sj = {M? )} denote a set of spike models and H denote information known  a priori. The probability of Sj, conditioned only on H and the data, is obtained  using Bayes' rule:  :N, )  (8)  The only data-dependent term is P(D:,ISj,H ) which is the evidence for Sj  (MacKay, 1992). With the assumption that all hypotheses S:j are equally probable  a priori, P(D:,[Sj, H) ranks alternative spike sets in terms of their probability.  The evidence term P(D:,ISi, H) is convenient because it is the normalizing con-  stant for the posterior distribution of the parameters defining the spike set. Al-  though calculation of P(D:NISj,H) is analytically intractable, it is often well-  approximated with a Gaussian integral which was the approximation used for these  demonstrations.  A convenient way of collapsing the spike set is to compare spike models pairwise.  Two models in the spike set are selected along with a sampled set of events fit by  each model. We then evaluate P(DISx ) and P(DIS. ). S is the hypothesis that  the data is modeled by a single spike shape, S. says there are two spike shapes. If  P(DI$) > P(D]$2), we replace both models in S. by the one in Sx. The procedure  terminates when no more pairs can be combined to increase the evidence.  594 Lewicki  5 DECOMPOSING OVERLAPPING SPIKES  Overlaps must be decomposed into their colnponent spikes for accurate inference  of the spike functions and accurate classification of the events. Determining the  best-fitting decomposition is difficult because of the enormous number of possible  spike sequences, not only all possible model combinations for each event but also  all possible event times.  A brute-force approach to this problem is to perform an exhaustive search of the  space of overlapping spike functions and event times to find the sequence with  maximum probability. This approach was used by Atiya (1992) in the case of two  overlapping spikes with the times optimized to one sample period. Unfortunately,  this is often computationally too demanding even for off-line analysis.  We make this search efficient utilizing dynamic programming and k-dimensional  trees (Friedman et al., 1977). Once the best-fitting decomposition can be obtained,  however, it may not be optimal, since adding more spike shapes can overfit the  data. This problem is minimized by evaluating the probability for alternative de-  compositions to determine the most probable spike sequence (figure 2).  Figure 2: Many spike function sequences can account for the same region of data. The  thick lines show the data, thin lines show individual spike functions. In this case, the best-  fitting overlap solution is not the most probable: the sequence with 4 spike functions is  more than 8 times more probable than the other solutions, even though these have smaller  mean squared error. Using the best-fitting overlap solution may increase the classification  error. Classification error is minimized by using t he overlap solution that is most probable.  6 PERFORMANCE  The algorithm was tested on 40 seconds of neurophysiological data. The task is  to determine the form and number of spike shapes in a waveform and to infer the  occurrence times of each spike shape. The output of the algorithm is shown in  figure 3. The uniformity of the residual error indicates that the six inferred spike  shapes account for the entire 40 seconds of data. The spike functions M. and M3  appear similar by eye, but the probabilities calculated with the methods in section 4  indicate that the two functions are significantly different. When plotted against each  Bayesian Modeling and Classification of Neural Signals 595  M1  lime (rm)  Tne (rm)  Txn (rm)  T,f"r  (ms) llme (ms)  Figure 3: The solid lines are the inferred spike models. The dta overlying each model  is  sample of at most 40 events with overlapping spikes sub[racted out. The residual  errors are plotted below each model. This spike set was obtained alter three iterations of  the algorithm, decomposing overlaps and determining the most probable number of spike  functions after each iteration. The whole inference procedure used 3 minutes of CPU  time on a Sparc IPX. Once the spike set is infe]red, classification of the same 40 second  waveform tkes about 10 seconds.  596 Lewicki  other, the two populations of APs are distinctly separated in the region around the  peak with M3 being wider than M..  The accuracy of the algorithm was tested by generating an artificial data set com-  posed of the six inferred shapes shown in figure 3. The event times were Poisson  distributed with frequency equal the inferred firing rate of the real data set. Gaus-  sian noise was then added with standard deviation equal to %. The classification  results are summarized in the tables below.  Table 1: Results of the spike model inference algorithm on the synthesized data set.  [ Model [[ i I 2 [ 3 [ 4 [ 5 [ 6 l[  [ ZXmax/rS/ [[ 0.44 [ 0.36 [ 1.07 [ 0.78 ] 0.84 [ 0.40 [[  The number of spike models was correctly determined by the algorithm with the six-model  spike set was preferred over the most probable five-model spike set by exp(34): 1 and over  the most probable seven-model spike set by exp(19): 1. The inferred shapes were accurate  to within a maximum error of 1.07%. The row elements show the maximum absolute  difference normalized by r., between each true spike function and the corresponding  inferred function.  Table 2: Classification results for the synthesized data set (non-overlapping events).  True [ Inferred Models [[  Models [ i I 2[ 3[ 4 I 51 611  1 17 0 0 0 '0 ' 0  2 0 25 1 o' 0 0  3 0 0 15 0 0 0  4 0 0 0 116 0 0  5 0 0 0 0 56 0  6 0 0 0 0 0 393  Missed Total  Events Events  0 17  0 26  15  1 117  17 73  254 647  Table 3: Classification results for the synthesized data set (overlapping events).  True InrredModels Missed Total  Models 1[ 2[ 3[ 4 I 5[ 6 Events Events  i 22 0 0 0 0 0 0 22  2 o 36  o o o o 37  3 0 0 20 0 0 " '0 0 20  4 0 i 0 116 0 1 3 121  5 0 0 0 1 61 1 19 82  6 0 0 0 3 2 243 160 408  Tables 2 and 3: Each matrix component indicates the number of times true model i was  classified as inferred model j. Events were missed if the true spikes were not detected  in an overlap sequence or if all sample values for the spike fell below the event detection  threshold (4%). There was 1 false positive for M and 7 for Mo.  Bayesian Modeling and Classification of Neural Signals 597  7 DISCUSSION  Formulating the task as having to infer a probabilistic model made clear what was  necessary to obtain accurate spike models. The soft clustering procedure accurately  determines the spike shapes even when the true underlying shapes are similar. Un-  less the spike shapes are well-separated, commonly used hard clustering procedures  will lead to inaccurate estimates.  Probability theory also allowed for an objective means of determining the number of  spike models which is an essential reason for the success of this algorithm. With the  wrong number of spike models overlap decomposition becomes especially difficult.  The evidence has proved to be a sensitive indicator of when two classes are distinct.  Probability theory is also essential to accurate overlap decomposition. Simply fit-  ting data with compositions of spike models leads to the same overfitting problem  encountered in determining the number of spike models and in determining the  spike shapes. Previous approaches have been able to handle only a limited class of  overlaps, mainly due to the difficultly in making the fit efficient. The algorithm used  here can fit an overlap sequence of virtually arbitrary complexity in milliseconds.  In practice, the algorithm extracts many times more information from a neural  waveform than previous methods. Moreover, this information is qualitatively dif-  ferent from a simple list of spike times. Having reliable estimates of the action  potential shapes makes it possible to study the properties of these classes, since  distinct neuronal types can have distinct hentonal spikes. Finally, accurate over-  lap decomposition makes it possible to investigate interactions among local neurons  which were previously very difficult to observe.  Acknowledgement s  I thank David MacKay for helpful discussions and Jamie Mazer for many conver-  sations and extensive help with the development of the software. This work was  supported by Caltech fellowships and an NIH Research Training Grant.  References  A.F. Atiya. (1992) Recognition of multiunit neural signals. IEEE Transactions on  Biomedical Engineering 39(7):723-729.  J.H. Friedman, J.L. Bently, and R.A. Finkel. (1977) An algorithm for finding best  matches in logarithmic expected time. A CM Trans. Math. Software 3(3):209-226.  D. J. C. MacKay. (1992) Bayesian interpolation. Neural Computation 4(3):415-445.  
Transition Point Dynamic Programming  Kenneth M. Buckland*  Dept. of Electrical Engineering  University of British Columbia  Vancouver, B.C, Canada V6T 1Z4  buckland@pmc-sierr a.bc.ca  Peter D. Lawrence  Dept. of Electrical Engineering  University of British Columbia  Vancouver, B.C, Canada V6T 1Z4  peterl@ee.ubc.ca  Abstract  Transition point dynamic programming (TPDP) is a memory-  based, reinforcement learning, direct dynamic programming ap-  proach to adaptive optimal control that can reduce the learning  time and memory usage required for the control of continuous  stochastic dynamic systems. TPDP does so by determining an  ideal set of transition points (TPs) which specify only the control  action changes necessary for optimal control. TPDP converges to  an ideal TP set by using a variation of Q-learning to assess the mer-  its of adding, swapping and removing TPs from states throughout  the state space. When applied to a race track problem, TPDP  learned the optimal control policy much sooner than conventional  Q-learning, and was able to do so using less memory.  1 INTRODUCTION  Dynamic programming (DP) approaches can be utilized to determine optimal con-  trol policies for continuous stochastic dynamic s. ystems when the state spaces of  those systems have been quantized with a resolution suitable for control (Barto et  al., 1991). DP controllers, in their simplest forill, are memory-based controllers  that operate by repeatedly updating cost values associated with every state in the  discretized state space (Barto et al., 1991). In a state space of any size the required  quantization can lead to an excessive memory requirement, and a related increase  in learning time (Moore, 1991). This is the "curse of dimensionality".  *Now at: PMC-Sierra Inc., 8501 Commerce Court, Burnaby, B.C., Canada VSA 4N3.  639  640 Buckland and Lawrence  Q-learning (Watkins, 1989, Watkins et al., 1992) is a direct form of DP that avoids  explicit system modeling - thereby reducing the memory required for DP control.  Further reductions are possible if Q-learning is modified so that its DP cost values  (Q-values) are associated only with states where control action changes need to be  specified. Transition point dynamic programming (TPDP), the control approach  described in this paper, is designed to take advantage of this DP memory reduction  possibility by determining the states where control action changes must be specified  for optimal control, and what those optimal changes are.  2 GENERAL DESCRIPTION OF TPDP  2.1 TAKING ADVANTAGE OF INERTIA  TPDP is suited to the control of continuous stochastic dynamic systems that have  inertia. In such systems "uniform regions" are likely to exist in the state space  where all of the (discretized) states have the same optimal control action (or the  same set of optimal actionsl). Considering one such uniform region, if the optimal  action for that region is specified at the "boundary states" of the region and then  maintained throughout the region until it is left and another uniform region is  entered (where another set of boundary states specify the next action), none of the  "dormant states" in the middle of the region need to specify any actions themselves.  Thus dormant states do not have to be represented in memory. This is the basic  premise of TPDP.  The association of optimal actions with boundary states is done by "transition  points" (TPs) at those states. Boundary states include all of the states that can  be reached from outside a uniform region when that region is entered as a result of  stochastic state transitions. The boundary states of any one uniform region form a  hyper-surface of variable thickness which may or may not be closed. The TPs at  boundary states must be represented in memory, but if they are small in number  compared to the dormant states the memory savings can be significant.  2.2 ILLUSTRATING THE TPDP CONCEPT  Figure 1 illustrates the TPDP concept when movement control of a "car" on a  one dimensional track is desired. The car, with some initial positive velocity to the  right, must pass Position A and return to the left. The TPs in Figure 1 (represented  by boxes) are located at boundary states. The shaded regions indicate all of the  states that the system can possibly move through given the actions specified at the  boundary states and the stochastic response of the car. Shaded states without TPs  are therefore dormant states. Uniform regions consist of adjacent boundary states  where the same action is specified, as well as the shaded region through which that  action is maintained before another boundary is encountered. Boundary states that  do not seem to be on the main state transition routes (the one identified in Figure 1  for example) ensure that any stochastic deviations fi'om those routes are realigned.  Unshaded states are "external states" the system does not reach.  The simplifying assumption that there is only one optional action in each uniform  region will be made throughout this paper. TPDP operates the same regardless.  Transition Point Dynamic Programming 641  q-  Each r is a  transition point (TP).  Dormant  State   . . Uniform  ..... Region  i:i:i:i:i13i:i:::: * *   *  A  Position  State  Figure 1' Application of TPDP to a One Dimension Movement Control Task  2.3 MINIMAL TP OPTIMAL CONTROL  The main benefit of the TPDP approach is that, where uniform regions exist, they  can be represented by a relatively sinall number of DP elements (TPs) - depending  on the shape of the boundaries and the size of the uniform regions they encom-  pass. This reduction in memory usage results in an accompanying reduction in the  learning time required to learn optimal control policies (Chapman et al., 1991).  TPDP operates by learning optimal points of transition in the control action specifi-  cation, where those points can be accurately located in highly resolved state spaces.  To do this TPDP must determine which states are boundary states that should  have TPs, and what actions those TPs should specify. In other words, TPDP must  find the right TPs for the right states. When it has done so, "minimal TP optimal  control" has been achieved. That is, optimal control with a minimal set of TPs.  3 ACHIEVING MINIMAL TP OPTIMAL CONTROL  3.1 MODIFYING A SET OF TPs  Given an arbitrary initial set of TPs, TPDP must modify that set so that it is  transformed into a minimal TP optimal control set. Modifications can include the  "addition" and "removal" of TPs throughout the state space, and the "swapping"  of one TP for another (each specifying a different action) at the same state. These  642 Buckland and Lawrence  modifications are performed one at a time in arbitrary order, and can continue  indefinitely. TPDP operates so that each TP modification results in an incremental  movement towards minimal TP optimal control (Buckland, 1994).  3.2 Q-LEARNING  TPDP makes use of Q-learning (Watkins, 1989, Watkins et al., 1992) to modify the  TP set. Normally Q-learning is used to determine the optimal control policy  for  a stochastic dynamic system subjected to immediate costs c(i, u) when action u is  applied in each state i (Barto et al., 1991). Q-learning makes use of "Q-values"  Q(i, u), which indicate the expected total infinite-horizon discounted cost if action  u is applied in state i, and actions defined by the existing policy  are applied in  all future states. Q-values are learned by using the following updating equation:  Qt+l(St, ut) = (1 - ctt)Qt($t, lit) q- olt [c($t, ttt) q- 7Vt($t+l)] (1)  Where at is the update rate, 7 is the discount factor, and st and ut are respectively  the state at time step t and the action taken at that time step (all other Q-values  remain the same at time step i). The evaluation function value Vt(i) is set to the  lowest Q-value action of all those possible U(i) in each state i:  Vt(i) = rain Qt(i,u) (2)  If Equations 1 and 2 are employed during exploratory movement of the system, it has  been proven that convergence to optimal Q-values Q*(i, u) and optimal evaluation  function values V,. (i) will result (given that the proper constraints are followed,  Watkins, 1989, Watkins et al., 1992, Jaakkola et al., 1994). From these values the  optimal action in each state can be determined (the action that fulfills Equation 2).  3.3 ASSESSING TPs WITH Q-LEARNING  TPDP uses Q-learning to determine how an existing set of TPs should be modified  to achieve minimal TP optimal control. Q-values can be associated with TPs, and  the Q-values of two TPs at the same "TP state", each specifying different actions,  can be compared to determine which should be maintained at that state - that is,  which has the lower Q-value. This is how TPs are swapped (Buckland, 1994).  States which do not have TPs, "non-TP states", have no Q-values from which  evaluation function values (i) can be determined (using Equation 2). As a result,  to learn TP Q-values, Equation I must be modified to facilitate Q-value updating  when the system makes d state transitions from one TP state through a number of  non-TP states to another TP state:  u,) = - + + (a)  When d = 1, Equation 3 takes the form of Equation 1. When d > 1, the intervening  non-TP states are effectively ignored and treated as inherent parts of the stochtic  dynamic behavior of the system (Buclda.nd, 1994).  If Equation 3 is used to determine the costs incurred when no action is specified  at a state (when the action specified at some previous state is maintained), an "K-  value" R(i) is the result. R-values can be used to expediently add and remove TPs  Transition Point Dynamic Programming 643  from each state. If the Q-value of a TP is less than the R-value of the state it is  associated with, then it is worthwhile having that TP at that state; otherwise it is  not (Buckland, 1994).  $.4 CONVERGENCE TO MINIMAL TP OPTIMAL CONTROL  It has been proven that a random sequence of TP additions, swaps and removals  attempted at states throughout the state space will result in convergence to min-  imal TP optimal control (Buckland, 1994). This proof depends mainly on all TP  modifications "locking-in" any potential cost reductions which are discovered as the  result of learning exploration.  The problem with this proof of convergence, and the theoretical form of TPDP  described up to this point, is that each modification to the existing set of TPs (each  addition, swap and removal) requires the determination of Q-values and R-values  which are negligibly close to being exact. This means that a complete session of  Q-learning must occur for every TP modification? The result is excessive learning  times - a problem circumvented by the practical form of TPDP described next.  4 PRACTICAL TPDP  4.1 CONCURRENT TP ASSESSMENT  To solve the problem of the protracted learning time required by the theoretical  form of TPDP, many TP modifications can be assessed concurrently. That is,  Q-learning can be employed not just to determine the Q-values and R-values for a  single TP modification, but instead to learn these values for a number of concurrent  modifications. Further, the modification attempts, and the learning of the values  required for them, need not be initiated simultaneously. The determination of each  value can be made part of the Q-learning process whenever new modifications are  randomly attempted. This approach is called "Practical TPDP". Practical TPDP  consists of a continually running Q-learning process (based on Equations 2 and 3),  where the Q-values and R-values of a constantly changing set of TPs are learned.  4.2 USING WEIGHTS FOR CONCURRENT TP ASSESSMENT  The main difficulty that arises when TPs are assessed concurrently is that of deter-  mining when an assessment is complete. That is, when the Q-values and R-values  associated with each TP have been learned well enough for a TP modification to  be made based on them. The technique employed to address this problem is to  associate a "weight" w(i, u) with each TP that indicates the general merit of that  TP. The basic idea of weights is to facilitate the random addition of trial TPs to  a TP "assessment group" with a low initial weight Winitial. The Q-values and R-  values of the TPs in the assessment group are learned in an ongoing Q-learning  process, and the weights of the TPs are adjusted heuristically using those values.  Of those TPs at any state i whose weights w(i, u) have been increased above Wthr  2The TPDP proof allows for more than one TP swap to be assessed simultaneously,  but this does little to reduce the overall problem being described (Buckland, 1994).  644 Buckland and Lawrence  lOO  50  o  Practical TPDP  0 2500  Epoch Number  Figure 2: Performance of Practical TPDP on a Race Track Problem  (Winitia I < Wth r < Wmax) , the one with the lowest Q-value Q(i, u) is swapped into  the "policy TP" role for that state. The heuristic weight adjustment rules are:  1. New, trial TPs are given an initial weight of Winitial (0 < Winitial < Wthr).  2. Each time the Q-value of a TP is updated, the weight w(i, u) of that TP is  incremented if Q(i, u) < R(i) and decremented otherwise.  3. Each TP weight w(i,u) is limited to a maximum value of Wmx. This  prevents any one weight from becoming so large that it cannot readily be  reduced again.  4. If a TP weight w(i, u) is decremented to 0 the TP is removed.  An algorithm for Practical TPDP implementation is described in Buckland (1994).  4.3 PERFORMANCE OF PRACTICAL TPDP  Practical TPDP was applied to a continuous version of a control task described by  Barto et al. (1991) - that of controlling the acceleration of a car down a race track  (specifically the track shown in Figures 3 and 4) when that car randomly experiences  control action non-responsiveness. As shown in Figure 2 (each epoch in this Figure  consisted of 20 training trials and 500 testing trials), Practical TPDP learned the  optimal control policy much sooner than conventional Q-learning, and it was able  to do so when limited to only 15% of the possible number of TPs (Buckland, 1994).  The possible number of TPs is the full set of Q-values required by conventional  Q-learning (one for each possible state and action combination).  The main advantage of Practical TPDP is that it facilitates rapid learning of pre-  liminary control policies. Figure 3 shows typical routes followed by the car early  Transition Point Dynamic Programming 645  Starting  Positions  Finishing  Positions  ...r  i ; { ' : i ; ;  : , , : ! :   .......... g....*....4...,g ......... i-..-i.....:........,:-...--..'...... .<--!:';'"  -!........:.--.-....{.....-...-............$....:..... .............. i .......... .... .........  .....? ........... i'"?" .".  ....  ......... .'"'W"'":  ': ' *  :  I :1i,.,...i.......,...,,,.,,,:,,,.!,,,........... ! ......... ! .......... i,.,.!,....!...,.  i r I  Figure 3: Typical Race Track Routes After 300 Epochs  Starting  Positions  Finishing  Positions  Figure 4: Typical Race rlh'ack Routes After 1300 Epochs  in the learning process. With the addition of relatively few TPs, the policy of ac-  celerating wildly down the track, smashing into the wall and continuing on to the  finishing positions was learned. Further learning centered around this preliminary  policy led to the optimal policy of sweeping around the left turn. Figure 4 shows  typical routes followed by the car during this shift in the learned policy - a shift  indicated by a slight drop in the learning curve shown in Figure 2 (around 1300  epochs). After this shift, learning progressed rapidly until roughly optimal policies  were consistently followed.  A problem which occurs in Practical TPDP is that of the addition of superfluous  TPs after the optimal policy has basically been learned. The reasons this occurs  are described in Buckland (1994), as well as a number of solutions to the problem.  CONCLUSION  The practical form of TPDP performs very well when compared to conventional  Q-learning. When applied to a race track problem it was able to learn optimal  policies more quickly while using less memory. Like Q-learning, TPDP has all the  646 Buckland and Lawrence  advantages and disadvantages that result from it being a direct control approach  that develops no explicit system model (Watkins, 1989, Buckland, 1994).  In order to take advantage of the sparse memory usage that occurs in TPDP, TPs  are best represented by ACAMs (associative content addressable memories, Atke-  son, 1989). A localized neural network design which operates as an ACAM and  which facilitates Practical TPDP control is described in Buckland et al. (1993) and  Buckland (1994).  The main idea of TPDP is to, "try this for a while and see what happens". This  is a potentially powerful approach, and the use of TPs associated with abstracted  control actions could be found to have substantial utility in hierarchical control  systems.  A cknowle dgement s  Thanks to John Ip for his help on this work. This work was supported by an NSERC  Postgraduate Scholarship, and NSERC Operating Grant A4922.  References  Atkeson, C. G. (1989), "Learning arm kinematics and dynamics", Annual Review  of Neuroscience, vol. 12, 1989, pp. 157-183.  Barto, A. G., S. J. Bradtke and S. P. Singh (1991), "Real-time learning and con-  trol using asynchronous dynamic programming", COINS' Technical Report 91-57,  University of Massachusetts, Aug. 1991.  Buckland, K. M. and P. D. Lawrence (1993), "A connectionist approach to direct  dynamic programming control", Proc. of the IEEE Pacific Rim Conf. on Commu-  nications, Computers and Signal Processing, Victoria, 1993, vol. 1, pp. 284-287.  Buckland, K. M. (1994), Optimal Control of Dynamic Systems Through the Rein-  forcement Learning of Transition Points, Ph.D. Thesis, Dept. of Electrical Engi-  neering, University of British Columbia, 1994.  Chapman, D. and L. P. Kaelbling (1991), "Input generalization in delayed  reinforcement-learning: an algorithm and performance comparisons", Proc. of the  12th Int. Joint Conf. on Artificial Intelligence, Sydney, Aug. 1991, pp. 726-731.  Jaakkola, T., M. I. Jordan and S. P. Singh (1994), "Stochastic convergence of iter-  ative DP algorithms", Advances in Neural Information Processing Systems 6, eds.:  J. D. Cowen, G. Tesauro and J. Alspcctor, San Francisco, CA: Morgan Kaufmann  Publishers, 1994.  Moore, A. W. (1991), "Variable resolution dynamic programming: efficiently learn-  ing action maps in multivariate real-valued state-spaces", Machine Learning: Proc.  of the 8th Int. Workshop, San Mateo, CA: Morgan Kaufmann Publishers, 1991.  Watkins, C. J. C. H. (1989), Learning from Delayed Rewards, Ph.D. Thesis, Cam-  bridge University, Cambridge, England, 1989.  Watkins, C. J. C. H. and P. Dayan (1992), "Q-learning", Machine Learning, vol. 8,  1992, pp. 279-292.  
Comparison Training for a Rescheduling  Problem in Neural Networks  Didier Keymeulen  Artificial Intelligence Laboratory  Vrije Universiteit Brussel  Pleinlaan 2, 1050 Brussels  Belgium  Martine de Gerlache  Prog Laboratory  Vrije Universiteit Brussel  Pleinlaan 2, 1050 Brnssels  Belgium  Abstract  Airline companies usually schedule their flights and crews well in  advance to optimize their crew pools activities. Many events such  as flight delays or the absence of a member require the crew pool  rescheduling team to change the initial schedule (rescheduling). In  this paper, we show that the neural network comparison paradigm  applied to the backgammon game by Tesauro (Tesatlro and Se-  jnowski, 1989) can also be applied to the rescheduling problem of  an aircrew pool. Indeed both problems correspond to choosing  the best solution from a set of possible ones without ranking them  (called here best choice problem). The paper explains from a math-  ematical point of view the architecture and the learning strategy of  the backpropagation neural network used for the best choice prob-  lem. We also show how the learning phase of the network can be  accelerated. Finally we apply the neural network model to some  real rescheduling problems for the Belgian Airline (Sabena).  I Introduction  Due to merges, reorganizations and the need for cost reduction, airline companies  need to improve the efficiency of their manpower by optimizing the activities of  their crew pools as much as possible. A standard scheduling of flights and crews is  usually made well in advance but many events, such as flight delays or the absence  of a crew member make many schedule changes (rescheduling) necessary.  801  802 Keymeulen and de Gerlache  Each day, the CPR 1 team of an airline company has to deal with these pertur-  bations. The probleln is to provide the best answer to these regularly occurring  perturbations and to limit their impact on the general schedule. Its solution is hard  to find and usually the CPR team calls on hall reserve crews. An efficient reschedul-  ing tool taking into account the experiences of the CPR team could substantially  reduce the costs involved in rescheduling notably by limiting the use of a reserve  crew.  The paper is organized as follow. In the second section we describe the rescheduling  task. In the third section we argue for the use of a neural network for the reschedul-  ing task and we apply an adequate architecture for such a network. Finally in  the last section, we present results of experiments with schedules based on actual  schedtales used by Sabena.  2 Rescheduling for an Airline Crew Pool  When a pilot is unavailable for a flight it becomes necessary to replace him, e.g.  to reschedule the crew. The rescheduling starts froin a list of potential substitute  pilots (PSP) given by a schedtaling prograan based generally on operation research or  expert system technology (Steels, 1990). The PSP list obtained respects legislation  and security rules fixing for example the mareher of flying hours per month, the  maximum number of consecutive working hour and the number of training hours  per year and their schedule. From the PSP list, the CPR team selects the best  candidates taking into account the schedule stability and equity. The schedule  stability requires that possible perturbations of the schedule can be dealt with with  only a minimal rescheduling effort. This criterion ensures work stability to the crew  members and has an important influence on their social behavior. The schedule  equity ensures the equal distribution of the work and payment among the crew  members during the schedule period.  One may think to solve this rescheduling problem in the same way as the scheduling  problem itself using software tools based on operational research or expert system  approach. But this is inefficient for two reasons, first, the scheduling issued from a  scheduling system and its adaptation to obtain an acceptable schedule takes days.  Second this system does not take into accotint the previous schedule. It follows  that the lapdated one anay (lifter significantly fi'om the previous one after each  perturbation. This is unacceptable fa'oan a pilot's point of view. ttence a specific  procedure for rescheduling is necessary.  3 Neural Network Approach  The problem of reassigning a new crew member to replace a missing member can  be seen as the problem of finding the best pilot in a pool of potential substittate  pilots (PSP), called the bcst choicc problem.  To solve the best choice problem, we choose the neural network approach for two  reasons. First the rules used by the expert are not well defined: to find the best PSP,  Crew Pool Rescheduler  Comparison Training for a Rescheduling Problem in Neural Networks 803  the expert associates implicitly a score vahle to each profile. The learning approach  is precisely well suited to integrate, ill a short period of time, the expert knowledge  given in an implicit form. Second, the neural network approach was applied with  success to board-games e.g. the Backgammon game described by Tesauro (Tesanro  and Sejnowski, 1989) and the Nine Men's Morris game described by Bratin (Braun  and al., 1991). These two games are also examples of best choice problem where  the player chooses the best move from a set of possible ones.  3.1 Profile of a Potential Substitute Pilot  To be able to use the neural network approach we have to identify the main fea-  tures of the potential substitute pilot and to codify them in terms of rating values  (de Gerlache and Keymeulen, 1993). We based our coding scheme on the way the  expert solves a rescheduling problem. He identifies the relevant parameters associ-  ated with the PSP and the perturbed schedule. These parameters give three types  of information. A first type describes the previous, present and furtire occupation  of the PSP. The second type represents information not in the schedule such as  the human relationship factors. The associated vaines of these two types of pa-  rameters differ for each PSP. The last type of pa.rameters describes the context  of the reschednling, namely the characteristics of the schedule. This last type of  parameters are the same for all the PSP. All these paranleters form the profile of  a PSP associated to a perturbed schedule. At each rescheduling problem corre-  sponds one perturbed schedule j and a group of, PSP i to which we associate a  Profile _= (PSP i, Perturbed_Schedule j). Implicitly, the expert associates a rat-  ing value between 0 and 1 to each parameter of the Profi! based on respectively  its little or important impact on the resulting schedule if the PSP i was chosen.  The rating value reflects the relative ilnportance of the parameters on the stability  and the equity of the resulting schedule obtained after the pilots substitution.  3.2 Dual Neural Network  It would have been possible to get more information froill the expert than only the  best profile. One of the possibilities is to ask him to score every profile associated  with a perturbed planning. From this association we could immediately construct  a scoring function which couples each profile with a specific vahle, namely its score.  Another possibility is to ask the expert to rank all profiles associated with a per-  turbed schedule. The corresponding ranki,.g function couples each profile with a  value snch that the vahles associated with the profiles of the same perturbed sched-  ule order the profiles according to their rank. The decision making process used by  the rescheduler team for the aircrew rescheduling problem does not consist in the  evaluation of a scoring or ranking function. Indeed only the knowledge of the best  profile is useful for the rescheduling process.  From a neural network architectural point of view, because the ranking problem is a  generalization of the best choice problem, a same neural network architecture can be  used. But the difference between the best choice problem and the scoring problem  is such that two different neural network architectures are associated to them. As  we show in this section, although a backpropagation network is sufficient to learn a  scoring function, its architecture, its learning and its retrieval procedures must be  804 Keyrneulen and de Gerlache  adapted to learn the best profile. Through a mathematical formulation of the best  choice problem, we show that the comparison paradigm of Tesauro (Tesauro, 1989)  is suited to the best choice problem and we suggest how to improve the learning  convergence.  3.2.1 Comparing Function  For the best choice problem the expert gives the best profile Profile .Bet associated  $  with the perturbed schedule j and that for m perturbed schedules. The problem  consists then to learn the mapping of the m. n profiles associated with the m  perturbed schedules into the rn best profiles, one for each perturbed schedule. One  way to represent this association is through a corn, paring function. This function  has as input a profile, represented by a vector Xj, and returns a single value. When  a set of profiles associated with a perturbed schedule are evaluated by the function,  it returns the lowest value for the best profile. This comparing timetlon integrates  the information given by the expert and is sufficient to reschedule any perturbed  schedule solved in the past by the expert. Formally it is defined by:  Co,,pa,'e. = (Profile) (1)  Compareff *t <Com. parc. { Vj with j = 1,...,m  ViBest with i=l,...,n  The value of Compare are not known a priori and have only a meaning when they  are compared to the value Compare * of the comparing filnction for the best  profile.  3.2.2 Geometrical Interpretation  To illustrate the difference between the neural network learning of a scoring function  and a comparing function, we propose a geometrical interpretation in the case of  a linear network having as input vectors (profiles) ,..., X,..., associated  with a perturbed schedule j.  The learning of a scoring filnction which associates a score Score with each input  vector X consists in finding a hyperplane in the input vector space which is tangent  to the circles of center ./ and ra(lius 8co,' (Fig. 1). On the contrary the learning  of a comparing function consists to obtain the equation of an hyperplane such that  the end-point of the vector Sff* is nearer the hyperplane than the end-points of  the other input vectors  associated with the same perturbed schedule j (Fig. 1).  3.2.3 Learning  We use a neural network approach to build the comparing filnction and the mean  squared error as a measure of the quality of the approximation. The comparing  function is approximated by a non-linear function' C(Profile) =  where I is the weight vector of the neural network (e.g backpropagation network).  The problem of finding C which has the property of (1) is equivalent to finding the  function C that minimizes the following error filnction (Braun and al., 1991) where   is the sigmoid timetlon:  Comparison Training for a Rescheduling Problem in Neural Networks 805  Figtare 1' Geometrical Interpretation of the learning of a Scoring Function  (Rigth) and a Comparing Function (Left)  j=l  i y Best  (q [C( rrofile '') - (rrofilej )])2  (2)  To obtain the weight vector which minimizes the error filnction (2), we use the  property that the -grM,(ff/) points in the direction in which the error function  will decrease at the fastest possible rate. To update the weight we have thus to  calculate the partial derivative of (2) with each components of the weight vector  1' it is made of a prodtact of three factors. The evaluation of the first two factors  (the sigmoid and the derivative of the sigmoid) is immediate. The third factor is  the partial derivative of the non-linear function A/' which is generally calculated  by using the generalized delta rule learning law (Rumelhart and McClelland, 1986).  Unlike the linear associator network, for the backpropagation network, the error  function (2) is not equivalent to the error function where the difference ..Bet _ )j  is associated with the input vector of the backpropagation network becatse:  5r(, - 27 ~'  _.) # -  (3)  By consequence to calculate the three factors of the partial derivative of (2), we  have to introduce separately at the bottom of the network the input vector of the  best profile ,.,t and the input vector of a less good profile X-'j. Then we have to  memorize their partial contribution at each node of the network and multiply their  contributions before tipdating the weight. Using this way to evaluate the derivative  of (2) and to update the weight, the simplicity of the generalized delta rule learning  law has disappeared.  806 Keymeulen and de Gerlache  3.2.4 Architecture  Wesauro (Tesauro and Sejnowski, 1989) proposes an architecture, that we call dual  neural network, and a learning procedure such that the simplicity of the generalized  delta rule learning law can still be used (Fig. 2). The same kind of architecture,  called siamese network, was recently used by Bromley for the signature verification  (Bromley and al., 1994). The dual neural network architecture and the learning  strategy are justified mathematically at one hand by the decomposition of the partial  derivative of the error function (2) in a slim of two terms and at the other hand by  the asymmetry property of the sigmoid and its derivative.  The architecture of the dual neural network consists to duplicate the multi-layer  network approximating the comparing function (1) and to connect the output of  both to a unique output node through a positive unit weight for the left network and  negative unit weight, for the right network. Dnring the learning a couple of profiles  is presented to the dual neural network: a best profile e.t and a less good profile  . The desired value at the output node of the dual neural network is 0 when the  left network has for input the best profile and the right network has for input a less  good profile and 1 when these profiles are permuted. During the recall we work only  with one of the two multi-layer networks, suppose the left one (the choice is of no  importance because they are exactly the same). The profiles Xj associated with a  perturbed schedule j are presented at the input of the left, multi-layer network. The  best profile is the one having the lowest value at the output of the left multi-layer  network.  Through this mathelnatical formulation we can use the suggestion of Braun to  improve the learning convergence (Bratin and al., 1991). They propose to replace  the positive and negative unit weight between the output node of the multi-layer  networks and the outplat node of the dtlal neural network by respectively a weight  value eqnal to 2 for the left network and -2 for the right network. They modify the  value of 2 by applying the generalized delta rule which has no significant impact on  the learning convergence. By manually increasing the factor 2 during the learning  procedure, we improve considerably the learning convergence due to its asymmetric  impact on the derivative of ,v(k) with I: the modification of the weight vector  is greater for couples not yet learned than for couples already learned.  4 Results  The experiments show the ability of onr model to help the CPR team of the Sabena  Belgian Airline company to choose the best profile in a group of PSPs based on  the learned expertise of the team. To codify the profile we identify 15 relevant  parameters. They constitute the input of our neural network. The training data  set was obtained by analyzing the CPR team at work during 15 days from which  we retain our training and test perturbed schedules.  We consider that the network has learned when the coinparing value of the best  profile is less than the comparing value of the other profiles and that for all training  perturbed schedules. At that time (I/7) is less than .5 for every couple of profiles.  The left graph of Figure 3 shows the evohltion of the mean error over the couples  Comparison Training for a Rescheduling Problem in Neural Networks 807  Dual Neural Network  Figure 2: The training of a dual neural network.  during the training. The right graph shows the improvement of the convergence  when the weight 12 is increased regularly during the training process.   Dual Neural Network hror  0.4 0.4  0,2 02  Incnsing V  of the Dual Neural l',ktwork  Numb of  a  Figtire 3: Convergence of the dual neural network architecture.  The network does not converge when we introduce contradictory decisions in our  training set. It is possible to resolve them by adding new context parameters in the  coding scheme of the profile.  After learning, our network shows generalization capacity by retrieving the best  profile for a new perturbed schedule that is similar to one which has already been  learned. The degree of similarity required for the generalization remains a topic for  further study.  808 Keyrneulen and de Gerlache  15 Conclusion  In conclusion, we have shown that the rescheduling problem of an airline crew pool  can be stated as a decision making problem, namely the identification of the best  potential substitute pilot. We have stressed the importance of the codification of the  information used by the expert to evaluate the best candidate. We have applied the  neural network learning approach to help the rescheduler team in the rescheduling  process by using the experience of already solved rescheduling problems. By a  mathematical analysis we have proven the efficiency of the dual neural network  architecture. The mathematical analysis permits also to improve the convergence  of the network. Finally we have illustrated the method on rescheduling problems  for the Sabena Belgian Airline company.  Acknowledgments  We thank the Scheduling and Rescheduling team of Mr. Verworst at Sabena for  their valuable information given all along this study; Professors Steels and D'Hondt  from the VUB and Professors Pasttin, Leysen and Declerck from the Military Royal  Academy who supported this research; Mr. Horner and Mr. Pau from the Digital  Europe organization for their funding. We specially thank Mr. Decuyper and Mr.  de Gerlache for their advices and attentive reading.  References  H. Braun, J. Faulner & V. Uilrich. (1991) Learning strategies for solving the prob-  lem of planning using backpropagation. In Proceedings of Fourth International  Conference on Neural Networks and their Applications, 671-685. Nimes, France.  J. Bromley, I. Guyon, Y. Lecun, E. Sackinger, R. Shah. (1994). Signature verifica-  tion using a siamese delay neural network. In J. Cowan, G. Tesauro & J. Alspector  (eds.), Advances in Neural Information Processing Systems 1. San Mateo, CA:  Morgan Kaufmann.  M. de Gerlache & D. Keymeulen. (1993) A neural network learning strategy adapted  for a rescheduling problem. In Proceedings of Fourth International Conference on  Neural Networks and their Applications, 33-42. Nimes, France.  D. Rumelhart & J. McClelland. (1986) Parallel Distributed Processing: Eplo-  rations in the Microstructure of Uognition I 64 II. Cambridge, MA: MIT Press.  L. Steels. (1990) Components of expertise. AI Magazine, 11(2):29-49.  G. Tesauro. (1989) Connectionist learning of expert preferences by comparison  training. In D. S. Touretzky (ed.), Advances in Neural Information Processing  Systems 1, 99-106. San Mateo, CA: Morgan Kaufmann.  G. Tesauro & T.J. Sejnowski. (1989) A parallel network that learns to play  backgammon. Artificial Intelligence, 39:357-390.  
When Will a Genetic Algorithm  Outperform Hill Climbing?  Melanie Mitchell  Santa Fe Institute  1660 Old Pecos Trail, Suite A  Santa Fe, NM 87501  John H. Holland  Dept. of Psychology  University of Michigan  Ann Arbor, MI 48109  Stephanie Forrest  Dept. of Computer Science  University of New Mexico  Albuquerque, NM 87131  Abstract  We analyze a simple hill-climbing algorithm (IMHC) that was pre-  viously shown to outperform a genetic algorithm (GA) on a simple  "Royal load" function. We then analyze an "idealized" genetic  algorithm (IGA) that is significantly faster than IMHC and that  gives a lower bound for GA speed. We identify the features of the  IGA that give rise to this speedup, and discuss how these features  can be incorporated into a real GA.  1 INTRODUCTION  Our goal is to understand the class of problems for which genetic algorithms (GA)  are most suited, and in particular, for which they will outperform other search  algorithms. Several studies have empirically compared GAs with other search and  optimization methods such as simple hill-climbing (e.g., Davis, 1991), simulated  annealing (e.g., Ingber & losen, 1992), linear, nonlinear, and integer programming  techniques, and other traditional optimization techniques (e.g., De Jong, 1975).  However, such comparisons typically compare one version of the GA with a second  algorithm on a single problem or set of problems, often using performance criteria  which may not be appropriate. These comparisons typically do not identify the  features that led to better performance by one or the other algorithm, making it  hard to distill general principles from these isolated results. In this paper we look in  depth at one simple hill-climbing method and an idealized form of the GA, in order  to identify some general principles about when and why a GA will outperform hill  climbing.  51  52 Mitchell, Holland, and Forrest  Figure 1: Royal Road function R.  In previous work we have developed a class of fitness landscapes (the "Royal Road"  functions; Mitchell, Forrest, & Holland, 1992; Forrest & Mitchell, 1993) designed to  be the simplest class containing the features that are most relevant to the perfor-  mance of the GA. One of our purposes in developing these landscapes is to carry  out systematic comparisons with other search methods.  A simple Royal Road function, R, is shown in Figure 1. R consists of a list of  partially specified bit strings (schemas) si in which '.' denotes a wild card (either  0 or 1). Each schema si is given with a coefficient ci. The order of a schema is  the number of defined (non-'.') bits. A bit string x is said to be an instance of a  schema s, x 6 s, if x matches s in the defined positions. The fitness R (x) of a bit  string x is defined as follows:  1 if z  s  R(z) = y. cii(z), where i(z) = 0 otherwise.  i  For example, if z is an instance of exactly two of the order-8 schemas, R (z) = 16.  Likewise, R(111... 1) = 64.  The Building Block Hypothesis (Holland, 1975/1992) states that the GA works well  when instances of low-order, short schemas ("building blocks") that confer high fit-  ness can be recombined to form instances of larger schemas that confer even higher  fitness. Given this hypothesis, we initially expected that the building-block struc-  ture of R would lay out a "royal road" for the GA to follow to the optimal string.  We also expected that simple hill-climbing schemes would perform poorly since a  large number of bit positions must be optimized simultaneously in order to move  from an instance of a lower-order schema (e.g., 11111111'*...*) to an instance of a  higher-order intermediate schema (e.g., 11111111'******'11111111'*...*). How-  ever both these expectations were overturned (Forrest & Mitchell, 1993). In our  experiments, a simple GA (using fitness-proportionate selection with sigma scaling,  single-point crossover, and point mutation) optimized R quite slowly, at least in  part because of "hitchhiking": once an instance of a higher-order schema is discov-  ered, its high fitness allows the schema to spread quickly in the population, with Os  in other positions in the string hitchhiking along with the ls in the schema's defined  positions. This slows down the discovery of schemas in the other positions, espe-  cially those that are close to the highly fit schema's defined positions. Hitchhiking  can in general be a serious bottleneck for the GA, and we observed similar effects  When Will a Genetic Algorithm Outperform Hill Climbing? 53  200 runs GA SAHC NAHC RMHC  Mea 61,334 (2304) > 256,000 (0) > 256,000 (0) 6179 (106)  Median 54,208 > 256,000 > 256,000 5775  Table 1: Mean and median number of function evaluations to find the optimum  string over 200 runs of the GA and of various hill-climbing algorithms on R1. The  standard error is given in parentheses.  in several variations of our original GA.  Our other expectation--that the GA would outperform simple hill-climbing on  these functions--was also proved wrong. Forrest and Mitchell (1993) compared  the GA's performance on a variation of R with three different hill-climbing meth-  ods: steepest ascent hill-climbing (SAHC), next-ascent hill-climbing (NAHC), and a  zero-temperature Monte Carlo method, which Forrest and Mitchell called "random  mutation hill-climbing" (RMHC). In RMHC, a string is chosen at random and its  fitness is evaluated. The string is then mutated at a randomly chosen single locus,  and the new fitness is evaluated. If the mutation leads to an equal or higher fitness,  the new string replaces the old string. This procedure is iterated until the optimum  has been found or a maximum number of function evaluations has been performed.  Here we have repeated these experiments for R1. The results (similar to those given  for R in Forrest & Mitchell, 1993) are given in Table 1. We compare the mean  and median number of function evaluations to find the optimum string rather than  mean and median absolute run time, because in almost all GA applications (e.g.,  evolving neural-network architectures), the time to perform a function evaluation  vastly dominates the time required to execute other parts of the algorithm. For this  reason, we consider all parts of the algorithm excluding the function evaluations to  take negligible time.  The results on SAHC and NAHC were as expected--while the GA found the opti-  mum on R in an average of 61,334 function evaluations, neither SAHC nor NAHC  ever found the optimum within the maximum of 256,000 function evaluations. How-  ever, RMHC found the optimum on R in an average of 6179 function evaluations--  nearly a factor of ten faster than the GA. This striking difference on landscapes orig-  inally designed to be "royal roads" for the GA underscores the need for a rigorous  answer to the question posed earlier: "Under what conditions will a GA outperform  other search algorithms, such as hill climbing?"  2 ANALYSIS OF RMHC AND AN IDEALIZED GA  To begin to answer this question, we analyzed the RMHC algorithm with respect to  R. Suppose the fitness function consists of N adjacent blocks of K ls each (in R,  N = 8 and K = 8). What is theexpected time (number of function evaluations)  E(K, N) to find the optimum string of all ls? We can first ask a simpler question:  what is the expected time E(K, 1) to find a single block of K ls? A Markov-chain  analysis (not given here) yields E(K, 1) slightly larger than 2 :, converging slowly  to 2 K from above as K -- oo (Richard Palmer, personal communication). For  54 Mitchell, Holland, and Forrest  example, for K = 8, E(K, 1) = 301.2.  Now suppose we want RMHC to discover a string with N blocks of K ls. The  time to discover a first block of K ls is E(K, 1), but, once it has been found, the  time to discover a second block is longer, since many of the function evaluations are  "wasted" on testing mutations inside the first block. The proportion of non-wasted  mutations is (KN - K)/KN; this is the proportion of mutations that occur in the  KN - K positions outside the first block. The expected time E(K, 2) to find a  second block is E(K, 1)+ E(K, 1)[KN/(KN- K)]. Similarly, the total expected  time is:  N N  E(K,N) = E(K, 1)+E(K, 1)N+...+E(K, 1)N_(N_i)  = E(K, 1)N 1+++...+ . (1)  (The actual value may be a bit larger, since E(K, 1) is the expected time to the first  block, whereas E(K, N) depends on the worst time for the N blocks.) Expression  (1) is approximately E(K, 1)N(IogN + 7), where 7 is Euler's constant. For K =  8, N = 8, the value of expression (1) is 6549. When we ran RMHC on the R  function 200 times, the average number of function evaluations to the optimum was  6179, which agrees reasonably well with the expected value.  Could a GA ever do better than this? There are three reasons why we might expect  a GA to perform well on R. First, at least theoretically the GA is fast because  of implicit parallelism (Holland, 1975/1992): each string in the population is an  instance of many different schemas, and if the population is large enough and is  initially chosen at random, a large number of different schemas--many more than  the number of strings in the population--are being sampled in parallel. This should  result in a quick search for short, low-order schemas that confer high fitness. Second,  fitness-proportionate reproduction under the GA should conserve instances of such  schemas. Third, a high crossover rate should quickly combine instances of low-order  schemas on different strings to create instances of longer schemas that confer even  higher fitness. Our previous experiments (Forrest k Mitchell, 1993) showed that  the simple GA departed from this "in principle" behavior. One major impediment  was hitchhiking, which limited implicit parallelism by luring certain schema regions  suboptimally. But if the GA worked exactly as described above, how quickly could  it find the optimal string of R ?  To answer this question we consider an "idealized genetic algorithm" (IGA) that  explicitly has the features described above. The IGA knows ahead of time what the  desired schemas are, and a "function evaluation" is the determination of whether a  given string contains one or more of them. In the IGA, at each time step a single  string is chosen at random, with uniform probability for each bit. The string is  "evaluated" by determining whether it is an instance of one or more of the desired  schemas. The first time such a string is found, it is sequestered. At each subsequent  discovery of an instance of one or more not-yet-discovered schemas the new string  is instantaneously crossed over with the sequestered string so that the sequestered  string contains all the desired schemas that have been discovered so far.  This procedure is unusable in practice, since it requires knowing a priori which  schemas are relevant, whereas in general an algorithm such as the GA or RMHC  When Will a Genetic Algorithm Outperform Hill Climbing? 55  directly measures the fitness of a string, and does not know ahead of time which  schemas contribute to high fitness. However, the idea behind the GA is to do  implicitly what the IGA is able to do explicitly. This idea will be elaborated below.  Suppose again that our desired schemas consist of N blocks of K ls each. What is  the expected time (number of function evaluations) until the saved string contains  all the desired schemas? Solutions have been suggested by G. Huber (personal com-  munication), and A. Shevoroskin (personal communication), and a detailed solution  is given in (Holland, 1993). The main idea is to note that the probability of finding  a single desired block s on a random string is p = 1/2 K, and the probability of  finding s by time f is 1 - (1 - p)t. Then the probability PN(t) that all N blocks  have been found by time t is:  7N(t) = (1 - (1 -  and the probability PN(t) that all N blocks are found at exactly time t is:  PN(t) = [1 -- (1 -- r)t] N - [1 - (1 - r)t-] N.  The expected time is then  EN = E t ([1 - (1 - r)'] N - [1 - (1 -  1  This sum can be expanded and simplified, and with some work, along with the  approximation (1 _p)n m 1-np for small p, we obtain the following approximation:  N  1  -  2K(log N + 7)-  EN -, (l/p)  n  The major point is that the IGA gives an expected time that is on the order of  2 K log N, where RMHC gives an expected time that is on the order of 2KN log N,  a factor of N slower. This kind of analysis can help us predict how and when the  GA will outperform hill climbing.  What makes the IGA faster than RMHC? A primary reason is that the IGA per-  fectly implements implicit parallelism: each new string is completely independent  of the previous one, so new samples are given independently to each schema region.  In contrast, RMHC moves in the space of strings by single-bit mutations from an  original string, so each new sample has all but one of the same bits as the previ-  ous sample. Thus each new string gives a new sample to only one schema region.  The IGA spends more time than RMHC constructing new samples, but since we  are counting only function evaluations, we ignore the construction time. The IGA  "cheats" on each function evaluation, since it knows exactly the desired schemas,  but in this way it gives a lower bound on the number of function evaluations that  the GA will need on this problem.  Independent sampling allows for a speed-up in the IGA in two ways: it allows for  the possibility of more than one desirable schema appearing simultaneously on a  given sample, and it also means that there are no wasted samples as there are  in RMHC. Although the comparison we have made is with RMHC, the IGA will  also be significantly faster on R1 (and similar landscapes) than any hill-climbing  56 Mitchell, Holland, and Forrest  Level 1: $1 $2 $3 $4 $5 $6 $? $8 $9 $10 $11 $12 $13 $14 $15 $16  Level 2: (s si) (sa st) (s sa) (s? ss) (s, so) (s si) (sa s,) (s, sa)  Level 3: (s, si sa st) (s, s6 sr ss) } s,o s,, sin) (sa s,, s, s6)  Figure 2: Royal Road Function R4.  method that works by mutating single bits (or a small number of bits) to obtain  new samples.  The hitchhiking effects described earlier also result in a loss of independent samples  for the real GA. The goal is to have the real GA, as much as possible, approximate  the IGA. Of course, the IGA works because it explicitly knows what the desired  schemas are; the real GA does not have this information and can only estimate  what the desired schemas are by an implicit sampling procedure. But it is possible  for the real GA to approximate a number of the features of the IGA. Independent  samples: The population size has to be large enough, the selection process has to  be slow enough, and the mutation rate has to be sufficient to make sure that no  single locus is fixed at a single value in every (or even a large majority) of strings in  the population. Sequesterin desired schemas: Selection has to be strong enough to  preserve desired schemas that have been discovered, but it also has to be slow enough  (or, equivalently, the relative fitness of the non-overlapping desirable schemas has  to be small enough) to prevent significant hitchhiking on some highly fit schemas,  which can crowd out desired schemas in other parts of the string. Instantaneous  crossover: The crossover rate has to be such that the time for a crossover to occur  that combines two desired schemas is small with respect to the discovery time for  the desired schemas. Speed-up over RMHC: The string length (a function of N) has  to be large enough to make the N speed-up factor significant.  These mechanisms are not all mutually compatible (e.g., high mutation works  against sequestering schemas), and thus must be carefully balanced against one  another. A discussion of how such a balance might be achieved is given in Holland  (1993).  3 RESULTS OF EXPERIMENTS  As a first step in exploring these balances, we designed R3, a variant of our previous  function R (Forrest & Mitchell, 1993), based on some of the features described  above. In R3 the desired schemas are s-ss (shown in Fig. 1) and combinations  of them, just as in R2. However, in R3 the lowest-level order-8 schemas are each  separated by "introns" (bit positions that do not contribute to fitness .see Forrest  z Mitchell, 1993; Levenick, 1991) of length 24.  In R3, a string that is not an instance of any desired schema receives fitness 1.0.  Every time a new level is reached--i.e., a string is found that is an instance of one  or more schemas at that level--a small increment u is added to the fitness. Thus  strings at level I (that are instances of at least one level-1 schema) have fitness  I q- u, strings at level 2 have fitness I q- 2u, etc. For our experiments we set u - 0.2.  When Will a Genetic Algorithm Outperform Hill Climbing? 57  Level 1 Level 2 Level 3  GA evais 500 {0) 4486 (478) 86,078 {17,242)  % runs 100 100 86  RMHC evais 230 (36) 8619 (1013) 95,027 (17,948)  % runs 100 100 41  Table 2: R4: Mean function evaluations (over 37 runs) to attain each level for  the GA and for RMHC. In the GA runs, the number of function evaluations is  sampled every 500 evaluations, so each value is actually an upper bound for an  interval of length 500. The standard errors are in parentheses. The percentage of  runs which reached each level is shown next to the heading "% runs." Only runs  which successfully reached a given level were included in the function evaluation  calculations for that level.  The purpose of the introns was to help maintain independent samples in each schema  position by preventing linkage between schema positions. The independence of  samples was also helped by using a larger population (2000) and the much slower  selection scheme given by the function. In preliminary experiments on R3 (not  shown) hitchhiking in the GA was reduced significantly, and the population was  able to maintain instances of all the lowest-level schemas throughout each run.  Next, we studied R4 (illustrated in Figure 2). R4 is identical to R3, except that it  does not have introns. Further, R4 is defined over 128-bit strings, thus doubling the  size of the problem. In preliminary runs on R4, we used a population size of 500,  a mutation rate of 0.005 (mutation always flips a bit), and multipoint crossover,  where the number of crossover points for each pair of parents was selected from a  Poisson distribution with mean 2.816.  Table 2 gives the mean number of evaluations to reach levels 1, 2, and 3 (neither  algorithm reached level 4 within the maximum of 106 function evaluations). As  can be seen, the time to reach level one is comparable for the two algorithms, but  the GA is much faster at reaching levels 2 and 3. Further, the GA discovers level  3 approximately twice as often as RMHC. As was said above, it is necessary to  balance the maintenance of independent samples with the sequestering of desired  schemas. These preliminary results suggest that R4 does a better job of maintaining  this balance than the earlier Royal Road functions. Working out these balances in  greater detail is a topic of future work.  4 CONCLUSION  We have presented analyses of two algorithms, RMHC and the IGA, and have used  the analyses to identify some general principles of when and how a genetic algorithm  will outperform hill climbing. We then presented some preliminary experimental  results comparing the GA and RMHC on a modified Royal Road landscape. These  analyses and results are a further step in achieving our original goals--to design the  simplest class of fitness landscapes that will distinguish the GA from other search  methods, and to characterize rigorously the general features of a fitness landscape  that make it suitable for a GA.  58 Mitchell, Holland, and Forrest  Our modified Royal Road landscape R4, like R, is not meant to be a realistic  example of a problem to which one might apply a GA. Rather, it is meant to be  an idealized problem in which certain features most relevant to GAs are explicit,  so that the GA's performance can be studied in detail. Our claim is that in order  to understand how the GA works in general and where it will be most useful, we  must first understand how it works and where it will be most useful on simple yet  carefully designed landscapes such as these. The work reported here is a further  step in this direction.  Acknowledgments  We thank R. Palmer for suggesting the RMHC algorithm and for sharing his careful  analysis with us, and G. Huber for his assistance on the analysis of the IGA. We  also thank E. Baum, L. Booker, T. Jones, and R. Riolo for helpful comments and  discussions regarding this work. We gratefully acknowledge the support of the Santa  Fe Institute's Adaptive Computation Program, the Alfred P. Sloan Foundation  (grant B1992-46), and the National Science Foundation (grants IRI-9157644 and  IRI-9224912).  References  L. D. Davis (1991). Bit-climbing, representational bias, and test suite design. In R.  K. Belew and L. B. Booker (eds.), Proceedings of the Fourth International Confer-  ence on Genetic Algorithms, 18-23. San Mateo, CA: Morgan Kaufmann.  K. A. De Jong (1975). An Analysis of the Behavior of a Class of Genetic Adaptive  Systems. Unpublished doctoral dissertation. University of Michigan, Ann Arbor,  MI.  S. Forrest and M. Mitchell (1993). Relative building-block fitness and the building-  block hypothesis. In D. Whitley (ed.), Foundations of Genetic Algorithms , 109-  126. San Mateo, CA: Morgan Kaufmann.  J. H. Holland (1975/1992). Adaptation in Natural and Artificial Systems. Cam-  bridge, MA: MIT Press. (First edition 1975, Ann Arbor: University of Michigan  Preas.)  J. H. 1tolland (19911). Innovation in complez adaptive systems: Some mathematical  sketches. Working Paper 93-10-062, Santa Fe Institute, Santa Fe, NM.  L. Ingber and B. Rosen (1992). Genetic algorithms and very fast simulated rean-  healing: A comparison. Mathematical Computer Modelling, 16 (11), 87-100.  J. R. Levenick (1991). Inserting introns improves genetic algorithm success rate:  Taking a cue from biology. In R. K. Belew and L. B. Booker (eds.), Proceedings of  the Fourth International Conference on Genetic Algorithms, 123-127. San Mateo,  CA: Morgan Kaufmann.  M. Mitchell, S. Forrest, and J. H. Holland (1992). The royal road for genetic algo-  rithms: Fitness landscapes and GA performance. In F. J. Varela and P. Bourgine  (eds.), Proceedings of the First European Conference on Artificial Life, 245-254.  Cambridge, MA: MIT Press.  
Identifying Fault-Prone Software  Modules Using Feed-Forward Networks:  A Case Study  N. Karunanithi  Room 2E-378, Bellcore  435 South Street  Morristown, NJ 07960  E-mail: karun@faline. bellcore. com  Abstract  Functional complexity of a software module can be measured in  terms of static complexity metrics of the program text. Classify-  ing software modules, based on their static complexity measures,  into different fault-prone categories is a difficult problem in soft-  ware engineering. This research investigates the applicability of  neural network classifiers for identifying fault-prone software mod-  ules using a data set from a commercial software system. A pre-  liminary empirical comparison is performed between a minimum  distance based Gaussian classifier, a perceptton classifier and a  multilayer layer feed-forward network classifier constructed using  a modified Cascade-Correlation algorithm. The modified version  of the Cascade-Correlation algorithm constrains the growth of the  network size by incorporating a cross-validation check during the  output layer training phase. Our preliminary results suggest that  a multilayer feed-forward network can be used as a tool for iden-  tifying fault-prone software modules early during the development  cycle. Other issues such as representation of software metrics and  selection of a proper training samples are also discussed.  793  794 Karunanithi  1 Problem Statement  Developing reliable software at a low cost is an important issue in the area of soft-  ware engineering (Karunanithi, Whitley and Malaiya, 1992). Both the reliability  of a software system and the development cost can be reduced by identifying trou-  blesome software modules early during the development cycle. Many measurable  program attributes have been identified and studied to characterize the intrinsic  complexity and the fault proneness of software systems. The intuition behind soft-  ware complexity metrics is that complex program modules tend to be more error  prone than simple modules. By controlling the complexity of software modules  during development, one can produce software systems that are easy to maintain  and enhance (because simple program modules are easy to understand). Static  complexity metrics are measured from the passive program texts early during the  development cycle and can be used as a valuable feedback for allocating resources  in future development efforts (future releases or new projects).  Two approachs can be applied to relate static complexity measures with faults  found or program changes made during testing. In the estimative approach regres-  sions models are used to predict the actual number of faults that will be disclosed  during testing (Lipow, 1982; Gaffney, 1984; Shen et al., 1985; Crawford et al., 1985;  Munson and Khoshgoftaar, 1992). Regression models assume that the metrics that  constitute independent variables are independent and normally distributed. How-  ever, most practical measures often violate the normality assumptions and exhibit  high correlation with other metrics (i.e., multicollinearity). The resulting fit of the  regression models often tend to produce inconsistent predictions.  Under the classification approach software modules are categorized into two or more  fault-prone classes (Rodriguez and Tsai, 1987; Munson and Khoshgoftaar, 1992;  Karunanithi, 1993; Khoshgoftaar et al., 1993). A special case of the classifica-  tion approach is to classify software modules into either low-fault (non-complex) or  high-fault (complex) categories. The main rationale behind this approach is that  the software managers are often interested in getting some approximate feedback  from this type of models rather than accurate predictions of the number of faults  that will be disclosed. Existing two-class categorization models are based on lin-  ear discriminant principle (Rodriguez and Tsai, 1987; Munson and Khoshgoftaar,  1992). Linear discriminant models assume that the metrics are orthogonal and that  they follow a normal distribution. To reduce multicollinearity, researchers often use  principle component analysis or some other dimensionality reduction techniques.  However, the reduced metrics may not explain all the variability if the original  metrics have nonlinear relationship.  In this paper, the applicability of neural network classifiers for identifying fault  proneness of software modules is examined. The motivation behind this research is  to evaluate whether classifiers can be developed without usual assumptions about  the input metrics. In order to study the usefulness of neural network classifiers, a  preliminary comparison is made between a simple minimum distance based Gaus-  sian classifier, a single layer perceptton and a multilayer feed-forward network devel-  oped using a modified version of Fahlman's Cascade Correlation algorithm (Fahlman  and Lebiere, 1990). The modified algorithm incorporates a cross-validation for con-  straining the growth of the size of the network. In this investigation, other issues  Identifying Fault-Prone Software Modules Using Feed-Forward Networks: A Case Study 795  such as selection of proper training samples and representation of metrics are also  considered.  2 Data Set Used  The metrics data used in this study were obtained from a research conducted by Lind  and Vairavan (Lind and Vairavan, 1989) for a Medical Imaging System software.  The complete system consisted of approximately 4500 modules amounting to about  400,000 lines of code written in Pascal, FORTRAN, PL/M and assembly level.  From this set, a random sample of 390 high level language routines was selected  for the analysis. For each module in the sample, program changes were recorded  as an indication of software fault. The number of changes in the program modules  varied from zero to 98. In addition to changes, 11 software complexity metrics  were extracted from each module. These metrics range from total lines of code  to Belady's bandwidth metric. (Readers curious about these metrics may refer to  Table I of Lind and Vairavan, 1989.) For the purpose of our classification study,  these metrics represent 11 input (both real and integer) variables of the classifier.  A software module is considered as a low fault-prone module (Category I) if there  are 0 or 1 changes and as a high fault-prone module (Category II) if there are 10  or more changes. The remaining modules are considered as medium fault category.  For the purpose of this study we consider only the low and high fault-prone modules.  Our extreme categorization and deliberate discarding of program modules is similar  to the approach used in other studies (Rodriguez and Tsai, 1987; Munson and  Khoshgoftaar, 1992). After discarding medium fault-prone modules, there are 203  modules left in the data set. Of 203 modules, 114 modules belong to the low  fault-prone category while the remaining 89 modules belong to the high fault-prone  category. The output layer of the neural nets had two units corresponding to two  fault categories.  3 Training Data Selection  We had two objectives in selecting training data: 1) to evaluate how well a neural  network classifier will perform across different sized training sets and 2) to select  the training data as much unbiased as possible. The first objective was motivated  by the need to evaluate whether a neural network classifier can be used early in the  software development cycle. Thus the classification experiments were conducted  using training samples of sizeS= ,-,l 2 a 9   3' ' ' if0 fractmn of 203 samples belonging  to Categories I and II. The remaining (I-S) fraction of the samples were used for  testing the classifiers. In order to avoid bias in the training data, we randomly  selected 10 different training samples for each fraction S. This resulted in 6 X 10  (=60) different training and test sets.  796 Karunanithi  4 Classifiers Compared  4.1 A Minimum Distance Classifier  In order to compare neural network classifiers and linear discriminant classifiers we  implemented a simple minimum distance based two-class Gaussian classifier of the  form (Nilsson, 1990):  IX - C,I = ((X - C,)(X - C,)')  where Ci, i = 1, 2 represent the prototype points for the Categories I and II, X is  a 11 dimensional metrics vector, and t is the transpose operator. The prototype  points C and C2 are calculated from the training set based on the normality as-  sumption. In this approach a given arbitrary input vector X is placed in Category  I if IX - C[ < I X - C21 and in Category II otherwise.  All raw component metrics had distributions that are asymmetric with a positive  skew (i.e., long tail to the right) and they had different numerical ranges. Note  that asymmetric distributions do not conform to the normality assumption of a  typical Gaussian classifier. First, to remove the extreme asymmetry of the original  distribution of the individual metric we transformed each metric using a natural  logarithmic base. Second, to mask the influence of individual component metric on  the distance score, we divided each metric by its standard deviation of the training  set. These transformations considerably improved the performance of the Gaussian  classifier. To be consistent in our comparison we used the log transformed inputs  for other classifiers also.  4.2 A Perceptron Classifier  A perceptron with a hard-limiting threshold can be considered as a realization of a  non-parametric linear discriminant classifier. If we use a sigmoidal unit, then the  continuous valued output of the perceptton can be interpreted as a likelihood or  probability with which inputs are assigned to different classes. In our experiment we  implemented a perceptton with two sigmoidal units (outputs 1 and 2) corresponding  to two categories. A given arbitrary vector X is assigned to Category I if the value  of the output unit 1 is greater than the output of the unit 2 and to Category II  otherwise. The weights of the network are determined iteratively using least square  error minimization procedure. In almost all our experiments, the perceptton learned  about 75 to 80 percentages of the training set. This implies that the rest of the  training samples are not linearly separable.  4.3 A Multilayer Network Classifier  To evaluate whether a multilayer network can perform better than the other two  classifiers, we repeated the same set of experiments using feed-forward networks  constructed by Fahlman's Cascade-Correlation algorithm. The Cascade-Correlation  algorithm is a constructive training algorithm which constructs a suitable network  architecture by adding one hidden (layer) unit at a time. (Refer to Fahlman and  Lebiere, 1990 for more details on the Cascade-Correlation algorithm.) Our initial  results suggested that the multilayer layer networks constructed by the Cascade-  Correlation algorithm are not capable of producing a better classification accuracy  Identifying Fault-Prone Software Modules Using Feed-Forward Networks: A Case Study 797  than the other two classifiers. An analysis of the network suggested that the re-  sulting networks had too many free variables (i.e., due to too many hidden units).  A further analysis of the rate of decrease of the residual error versus the number  of hidden units added to the networks revealed that the Cascade-Correlation algo-  rithm is capable of adding more hidden units to learn individual training patterns  at the later stages of the training phase than in the earlier stages. This happens  if the training set contains patterns that are interspersed across different decision  regions or what might be called "border patterns" (Ahmed, S. and Tesauro, 1989).  In an effort to constrain the growth of the size of the network, we modified the  Cascade-Correlation algorithm to incorporate a cross-validation check during the  output layer training phase. For each training set of size S, one third was used  for cross-validation and the remaining two third was used to train the network.  The network .construction was stopped as soon as the residual error of the cross-  validation set stopped decreasing from the residual error at the end of the previous  output layer training phase. The resulting network learned about 95% of the train-  ing patterns. However, the cross-validated construction considerably improved the  classification performance of the networks on the test set. Table 1 presented in  the next section provides a comparison between the networks developed with and  without cross-validation.  Training Hidden Unit Error Statistics  Set Size Statistics Type I Error I Type II Error  S in% Mean I Std Mean I Std Mean I Std  Without Cross-Validation  25 5.1 1.5 24.64 7.2 16.38 6.4  33 6.2 1.8 20.24 8.4 17.27 5.5  50 7.4 1.8 18.30 7.4 18.65 6.4  67 9.7 1.7 15.78 6.5 18.05 7.1  75 10.4 1.8 14.54 7.6 16.85 7.3  90 11.2 1.6 10.33 7.2 17.73 8.3  With Cross-Validation  25 1.9 1.3 20.19 5.4 12.11 4.7  33 2.2 1.0 18.24 5.5 12.40 4.1  50 2.0 0.9 17.41 5.6 15.04 5.2  67 2.7 1.1 14.32 5.8 14.08 5.5  75 2.7 1.3 13.27 7.0 13.84 5.4  90 2.9 1.2 9.77 9.4 15.47 5.1  Table 1' A Comparison of Nets With and Without Cross-Validation.  5 Results  In this section we present some preliminary results from our classification experi-  ments. First, we provide a comparison between the multilayer networks developed  with and without cross-validation. Next, we compare different classifiers in terms  of their classification accuracy. Since a neural network's performance can be af-  fected by the weight vector used to initialize the network, we repeated the training  experiment 25 times with different initial weight vectors for each training set. This  798 Karunanithi  resulted in a total of 250 training trials for each value of S. The results reported here  for the neural network classifiers represent a summary statistics for 250 experiments.  The performance of the classifiers are reported in terms of classification errors.  There are two type of classification errors that a classifier can make: a Type I error  occurs when the classifier identifies a low fault-prone (Category I) module as a high  fault-prone (Category II) module; a Type II error is produced when a high fault-  prone module is identified as a low fault-prone module. From a software manager's  point of view, these classification errors will have different implications. Type I  misclassification will result in waste of test resources (because modules that are less  fault-prone may be tested longer than what is normally required). On the other  hand, Type II misclassification will result in releasing products that are of inferior  quality. From reliability point of view, a Type II error is a serious error than a Type  I error.  No. of Patterns Error Statistics  sl Oussin  % Set Set Mean I Std Mean I Std Mean I Std  Type I Error Statistics  25 50 86 13.16 4.7 16.17 5.5 20.19 5.4  33 66 77 11.44 4.0 11.74 3.9 18.24 5.5  50 101 57 12.45 3.2 11.58 3.2 17.41 5.6  67 136 37 9.46 4.1 10.14 3.9 14.32 5.8  75 152 28 8.57 5.4 9.15 5.8 13.27 7.0  90 182 12 14.17 7.9 4.03 4.3 9.77 9.4  Type II Error Statistics  25 50 67 15.61 4.2 15.98 7.8 12.11 4.7  33 66 60 15.46 4.6 15.78 6.6 12.40 4.1  50 101 45 16.01 5.1 16.97 6.8 15.04 5.2  67 136 30 16.00 5.4 16.11 7.6 14.08 5.5  75 152 23 17.39 5.8 18.39 6.3 13.84 5.4  90 182 9 21.11 6.3 19.11 5.6 15.47 5.1  Table 2: A Summary of Type I and Type II Error Statistics.  Table 1 compares the complexity and the performance of the multilayer networks  developed with and without cross-validation. Columns 2 through 7 represent the  size and the performance of the networks developed by the Cascade-Correlation  without cross-validation. The remaining six columns correspond to the networks  constructed with cross-validation. Hidden unit statistics for the networks suggest  that the growth of the network can be constrained by adding a cross-validation  during the output layer training. The corresponding error statistics for both the  Type I and Type II errors suggest that an improvement classification accuracy can  be achieved by cross-validating the size of the networks.  Table 2 illustrates the preliminary results for different classifiers. The first two  columns in Table 2 represent the size of the training set in terms of S as a per-  centage of all patterns and the number of patterns respectively. The third column  represents the number of test patterns in Categories I (1st half) and the II (2nd half).  The remaining six columns represent the error statistics for the three classifiers in  Identifying Fault-Prone Software Modules Using Feed-Forward Networks: A Case Study 799  terms of percentage mean errors and standard deviations. The percentages errors  were obtained by dividing the number of misclassifications by the total number of  test patterns in that Category. The Type I error statistics in the first half of the  table suggest that the Gaussian and the Perceptton classifiers may be better than  multilayer networks at early stages of the software development cycle. However,  the difference in performance of the Gaussian classifier is not consistent across all  values of S. The neural network classifiers seem to improve their performance with  an increase in the size of the training set. Among neural networks, the perceptton  classifier seems to perform classification than a multilayer net. However, the Type  II error statistics in the second half of the table suggest that a multilayer network  classifier may provide a better classification of Category II modules than the other  two classifiers. This is an important results from the reliability perspective.  6 Conclusion and Work in Progress  We demonstrated the applicability of neural network classifiers for identifying fault-  prone software modules. We compared the classification efficacy of three different  pattern classifiers using a data set from a commercial software system. Our pre-  liminary empirical results are encouraging in that there is a role for multilayer  feed-forward networks either during the software development cycle of a subsequent  release or for a similar product.  The cross-validation implemented in our study is a simple heuristics for constraining  the size of the networks constructed by the Cascade-Correlation algorithm. Though  this improved the performance of the resulting networks, it should be cautioned that  cross-validation may be needed only if the training patterns exhibit certain charac-  teristics. In other circumstances, the networks may have to be constructed using  the entire training set. At this stage we have not performed complete analysis  on what characteristics of the training samples would require cross-validation for  constraining the network growth. Also we have not used other sophisticated struc-  ture reduction techniques. We are currently exploring different loss functions and  structure reduction techniques.  The Cascade-Correlation algorithm always constructs a deep network. Each addi-  tional hidden unit develops an internal representation that is a higher order sig-  moidal computation than those of previously added hidden units. Such a complex  internal representation may not be appropriate in a classification application such  as the one studied here. We are currently exploring alternatives to construct shallow  networks within the Cascade-Correlation frame work.  At this stage, we have not performed any analysis on how the internal represen-  tations of a multilayer network correlate with the input metrics. This is currently  being studied.  References  Ahmed, S. and G. Tesauro (1989). "Scaling and Generalization in Neural Networks:  A Case Study", Advances in Neural Information Processing Systems 1, pp 160-168,  D. Touretzky, ed. Morgan Kaufmann.  800 Karunanithi  Crawford, S. G., Mcintosh, A. A. and D. Pregibon (1985). "An Analysis of Static  Metrics and Faults in C Software", The Journal of Systems and Software, Vol. 5,  pp. 37-48.  Fahlman, S. E. and C. Lebiere (1990). "The Cascaded-Correlation Learning Ar-  chitecture," Advances in Neural Information Processing Systems 2, pp 524-532, D.  Touretzky, ed. Morgan Kaufmann.  Gaffney Jr., J. E. (1984). "Estimating the Number of Faults in Code", IEEE Trans.  on Software Eng., Vol. SE-10, No. 4, pp. 459-464.  Karunanithi, N, Whitley, D. and Y. K. Malaiya (1992). "Prediction of Software  Reliability Using Connectionist Models", IEEE Trans. on Software Eng., Vol. 18,  No. 7, pp. 563-574.  Karunanithi, N. (1993). "Identifying Fault-Prone Software Modules Using Con-  nectionist Networks", Proc. of the 1st Int'l Workshop on Applications of Neural  Networks to Telecommunications, (IWANNT'93), pp. 266-272, J. Alspector et al.,  ed., Lawrence Erlbaum, Publisher.  Khoshgoftaar, T. M., Lanning, D. L. and A. S. Pandya (1993). "A Neural Network  Modeling Methodology for the Detection of High-Risk Programs", Proc. of the Jth  Int'l Symp. on Software Reliability Eng. pp. 302-309.  Lind, R. K. and K. Vairavan (1989). "An Experimental Investigation of Software  Metrics and Their Relationship to Software Development Effort", IEEE Trans. on  Software Eng., Vol. 15, No. 5, pp. 649-653.  Lipow, M. (1982). "Number of Faults Per Line of Code", IEEE Trans. on Software  Eng., Vol. SE-8, No. 4, pp. 437-439.  Munson, J. C. and T. M. Khoshgoftaar (1992). "The Detection of Fault-Prone  Programs", IEEE Trans. on Software Eng., Vol. 18, No. 5, pp. 423-433.  Nilsson, J. Nils (1990). The Mathematical Foundations of Learning Machines, Mor-  gan Kaufmann, Chapters 2 and 3.  Rodriguez, V. and W. T. Tsai (1987). "A Tool for Discriminant Analysis and  Classification of Software Metrics", Information and Software Technology, Vol. 29,  No. 3, pp. 137-149.  Shen, V. Y., Yu, T., Thebaut, S. M. and T. R. Paulsen (1985). "Identifying Error-  Prone Software: An Empirical Study", IEEE Trans. on Software Eng., Vol. SE-11,  No. 4, pp. 317-323.  
Inverse Dynamics  of Speech Motor Control  Makoto Hirayama Eric Vatikiotis-Bateson Mitsuo Kawato*  ATR Human Information Processing Research Laboratories  2-2 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-02, Japan  Abstract  Progress has been made in COml)utational implementation of speech  production based on physiological data. An inverse dynamics  model of the speech articulator's musculo-skeletal system. which  is tile maI)ping from articulator trajectories to electromyographic  (EMG) signals, was modeled using tile acquired forward dynamics  model and temporal (smoothness of EMG activation) and range  constraints. This inverse dynamics model allows the use of a faster  speech motor control scheme, which can be applied to phoneme-to-  speech synthesis via musclo-skeletal system dynamics, or to future  use in speech recognition. Tile forward acoustic model, which is the  mapping from articulator trajectories to tile acoustic parameters,  was improved by adding velocity and voicing information inputs  to distinguish acoustic parameter differences caused by cha%es in  source characteristics.  1 INTRODUCTION  Modeling speech articulator dynamics is important not only for speech science,  but also for speech processing. This is because many issues in speech phenomena,  such as coarticulation or generation of aperiodic sources, are caused by temporal  properties of speech articulator behavior due to musculo-skeletal system dynamics  and constraints on neuro-motor command activation.  *Also, Laboratory of Parallel Distributed Processing, Research Institute for Electronic  Science, ttokkaido University, Sapporo, Hokkaido 060, Japan  1043  1044 Hirayama, Vatikiotis-Bateson, and Kawato  We haw proposed using neural networks for a computational implementation of  speech production based on physiological activities of speech articulator muscles.  In previous works (ttirayama, Vatikiotis-Bateson, Kawato and Jordan 1992; Hi-  rayarea, Vatikiotis-Bateson, Itonda, Koike and Kawato 1993), a neural network  learned the forward dynamics, relating motor commands to muscles and the ensu-  ing articulator behavior. From movement trajectories, the forward acoustic network  generated the acoustic PARCOR parameters (Itakura and Saito, 1969) that were  then used to synthesize the speech acoustics. A cascade neural network containing  the forward dynamics model along with a suitable smoothness crilerion was used  to produce a continuous motor command from a sequence of discrete articulatory  targets corresponding to tile phoneme input string.  Along the same line, we have extended our model of speech motor control. In this  paper, we focus on modeling tile inverse dynamics of tile musculo-skeletal system.  Having an inverse dynamics model allows us to use a faster control scheme, which  permits l)honeme-to-speech synthesis via musculo-skeletal system dynamics, and  ultimately may be useful in speech recognition. Tile final section of this paper  reports inprovements in the forward acoustic model, which were made by incor-  porating articulator velocity and voicing information to distinguish tile acoustic  parameter differences caused by changes in source characteristics.  2  INVERSE DYNAMICS MODELING OF  MUSCULO-SKELETAL SYSTEM  From the viewpoint of control theory, an inverse dynamics model of a controlled  object plays an essential role in feedforward control. That is, all accurate inverse dy-  namics model outputs an appropriate control sequence that realizes a given desired  trajectory by using only feedforward control without any feedback information, so  long as there is no perturbation from the environment. For speech articulators, the  main control scheme callnot rely upon feedback control because of sensory feedback  delays. Thus, we believe that tile inverse dynamics model is essential for biological  motor control of speech and for any eflicient speech synthesis algorithm based on  physiological data.  However, the speech articulator system is an excess-degrees-of-fi'eedom system,  thus the mapping from articulator trajectory (position, velocity, acceleration) to  electromyographic (EMG) activity is one-to-many. That is, different EMG com-  binations exist for the same articulator trajectory (for example, co-contraction of  agonist and antagonist muscle pairs). Consequently, we applied the forward mod-  eling approach to learning all inverse model (Jordan and Runaelhart, 1992), i.e.,  constrained supervised learning, as shown in Figure 1. The inpuls of tile inverse  Trajectory I Controll ::rajecto  1 Inverse [ , Forward  I Mode',,[,:: I Mode'  -- Error  Figure 1: Inverse dynamics modeling using a forward dynamics model (Jordan and  Rumelhart, 1992).  Inverse Dynamics of Speech Motor Control 1045  ..... Actual EMG  1.0-   -- "optimal" EMG by IDM  : 0.8-  - 0.6-   0.4-   0.2--  0.0--  0 1 2 3 4  Time (s)  Figure 2: After learning, the iuverse model output "optimal" EMG (anterior belly of  the digas[rio) [or jaw lowering is compared wih actual EMG [or the test trajectory.  dynamics model are articulator positions, velocities, and accelerations; the outputs  are rectified, integrated, and filtered EMG for relevant nmscles. The forward dy-  namics model previously reported (Ilirayama et al., 1993) was used for determining  the error signals of the inverse dynamics model.  To choose a realistic EMG pattern from among diverse possible sclutions, we use  both temporal and range constraints. The temporal constraint is related to the  smoothness of EMG activation, i.e., minimizing EMG activation change (Uno,  Suzuki, and Kaxvat. o, 1989). The minimum and maximum values of the range  constraint were chosen using values obtained from the experimental data. Direct  inverse modeling (Albus, 1975) was used to determine weights, which were then sup-  plied as initial weights to the constrained supervised learning algorithm of Jordan  and Rumelhart's (1992) inverse dynamics modeling method.  Figure 2 shows an example of the inverse dynamics model output after learning,  when a real articulator trajectory, not included in the training set, was given as  the input. Note that the network output cannot be exactly the same as the actual  EMG, as the network chooses a unique "optimal" EMG from many possible EMG  patterns that appear in the actual EMG for the trajectory.  .... Experimental data  -0.3  --- Direct inverse modeling  -- Inverse modeling using FDM  c "' ' ;"': .:'"'. ..'"" .-"-. . .."" '  O -0.5 -- / ' : ""'' ." ", '- '" ,"'<" ".  r ], . ,. ...,, ,,,, ..  O  o_ -0.6 --  -0.7 -- :'"  I I I I  0 1 2 3 4  Time (s)  Figure 3: Trajectories generated by the forward dynamics network for the two  methods of inverse dynamics modeling compared with the desired trajectory (ex-  perimental data).  1046 Hirayama, Vatikiotis-Bateson, and Kawato  Since the inverse dynamics model was obtained by learning, when the desired tra-  jectory is given to the inverse dynamics model, an articulator trajectory can be  generated with the forward dynamics network previously reported (IIirayama et al.,  1993). Figure 3 compares trajectories generated by the forward dynamics network  using EMG derived fi'om the direct inverse dynamics method or the constrained su-  pervised learning algorithm (which uses the forward dynamics model to determine  the inverse dynmnics model's "optimar' EMG). The latter method yielded a 30.0 %  average reduction in acceleration prediction error over the direct method, thereby  bringing the model output trajectory closer to the experimental data.  3  TRAJECTORY FORMATION USING FORWARD  AND INVERSE RELAXATION MODEL  Previously, to generate a. trajectory from discrete phoneme-specific via-points, we  used a cascade neural network (c.f., IIirayama et al., 1992). The inverse dynamics  model allows us to use an alternative network proposed by Wada and Kawato (1993)  (Figure 4). The network uses both the forward and inverse models of the controlled  object, and updates a giwm initial rough trajectory passing through the via-points  according to the dynamics of the controlled object and a smoothness constraint on  the control input. The computation time of the network is much shorter than that  of the cascade neural network (Wada and Kawato, 1993).  Figure 5 shows a forward dynamics model output trajectory driven by the model-  generated motor control signals. Unlike Wada and Kawato's original model (1993)  in which generated trajectories always pass through via-points, our trajectories were  generated from smoothed motor control signals (i.e., after applying the smoothness  constraint) and, consequently, do not, pass through the exact via-points. In this  paper, a typical value for each phoneme from experimental data was chosen as the  target via-point and xvas given in Cartesian coordinates relative to the maxillary  incisor. Although firther investigation is needed to refine the phoneme-specific  target specifications (e.g. lip aperture targets), reasonable coarticulated trajectories  were obtained fi'om series of discrete via-point targets (Figure 5). For engineering  applications such as text-to-speech synthesizers using articulatory synthesis, this  kind of technique is necessary because realistic coarticulated trajectories must serve  as input [o [he articulatory synthesizer.  /  / / u / // / s / Articu/atory Targets  I Trajectory Approximation I  ( Trajectory   Ilnvers i Dynamics Model I I Forward Dynamics Model !   ISmoothness Constraint J )   Motor Control Signal  Figure 4: Speech trajectory formatiou scheme modified from the forward and inverse  relaxation neural network model (Wada and Kawa[o, 1993).  Inverse Dynamics of Speech Motor Control 1047  >- -o.4 -  v  -- Network output  ....... Experimental data    - Phoneme specific targets  I I I I I I  0.0 0.2 0.4 0.6 0.8 1.0 1.2  Time (s)  Figure 5: Jaw trajectory generated by the forward and inverse relaxation model.  The output of the forward dynamics model is used for this plot.  A further advantage of this network is that it can be used t.o predict phoneme-  specific via-points kom the realized trajectory (Wada, Koike, Valikiotis-Bateson  and Kawato, 1993). This capability will allow us to use our forward and inverse  dynamics models for speech recognition in future, through acoustic to articula-  tory mapping (Shirai and Kobayashi, 1991; Papcun, Hochberg, Thomas, Laroche,  Zacks and Levy, 1992) and the articulatory to phoneme specific via-points map-  ping discussed above. Because trajectories may be recovered from a small set of  phoneme-specific via-points, this approach should be readily applicable to problems  of speech data compression.  4 DYNAMIC MODELING OF FORWARD ACOUSTICS  The second area of progress is the improvement in the forward acoustic network.  Previously (Hirayama et al., 1993), we demonstrated that acoustic signals can be  obtained using a neural network that learns the mapping between articulator posi-  tions and acoustic PARCOR coefficients (Itakura and Saito, 1969; See also, Markel  and Gray, 1976).  However, this modeling was effective only for vowels and a limited nmnber of conso-  nants because the architecture of the model was basically the same as that of static  articulatory synthesizers (e.g, Mermelstein, 1973). For natural speech, aperiodic  sources for plosive and sibilant consonants result in multiple sets of acoustic pa-  rameters for the same articulator configuration (i.e., the mapping is one-to-many);  hence, learning did not fully converge. One approach to solving lifts problem is  to make source modeling completely separate from the vocal tract area modeling.  However, for synthesis of natural sentences, the vocal tract. transfer function model  requires another model for the non-glottal sources associated with consonant pro-  duction. Since these sources are located at various points along the vocal tract,  their interaction is extremely complex.  Our approach to solving this one-to-many mapping is to have the neural network  learn the acoustic parameters along with the sound source characteristic specific  to each phoneme. Thus, we put articulator positions with their velocities and  voiced/voiceless information (e.g., Markel and Gray, 1976) into the input (Figure 6)  because the sound source characteristics are made not only by the articulator posi-  1048 Hirayama, Vatikiotis-Bateson, and Kawato  Articulator Positions, Velocities  & Voiced/Voiceless  GIottal Source  Acoustic Wave  Figure t3: Improved forward acoustic network. Inputs to the network are articulator  positions and velocities and voiced/voiceless information.  tion but also by the dynamic movement of articulators.  For simulations, horizontal and vertical motions of jaw, upper and lower lips, and  tongue tip and blade were used for the inputs and 12 dimensional PARCOlt pa-  rameters were used for the outputs of the network. Figure 7(a) shows position-  velocity-voiced/voiceless network output compared with position-only network and  experimentally obtained PARCOR parameters for a natural test sentence. Only the  first two coefficients are shown. The first part of the test sentence, "Sam sat on top  of the potato cooker and waited for Tommy to cut up a bag of tiny tomatoes and  pop the beat tips into the pot," is shown in this plot. Figure 7(b)(c) show a part  of the synthesized speech driven by fundamental frequency pulses for voiced sounds  and random noises for voiceless sounds.  By using velocity and voiced/voiceless inputs, the performance was improved for  natural utterances which include many vowels and consonants. The average val-  ues of the LPC-cepstrum distance measure between original and synthesized, were  5.17 (dB) for the position-only network and 4.18 (dB) for the position-velocity-  voiced/voiceless network. When listening to the output, the sentence can be un-  derstood, and almost all vowels and many of the consonants can be classified. The  overall clarity and the classification of some consonants is about as difficult as ex-  perienced in noisy international telephone calls.  Although there are other potential means to achieve further improvement (e.g.  adding more tongue channels, using more balanced training patterns, incorporating  nasality information, implementation of better glottal and non-glottal sources), the  network synthesizes quite smooth and reasonable acoustic signals by incorporating  aspects of the articulator dynamics.  5 CONCLUSION  We are modeling the information transfer from phoneme-specific articulatory targets  to acoustic wave via the nmsculo-skclctal system, using a series of neural networks.  Electromyographic (EMG) signals are used as the reflection of motor control com-  mands. In this paper, we have focused on the inverse dynamics modeling of the  Inverse Dynamics of Speech Motor Control 1049  It   o.o """---..,/' I I \'"';'"'/'" I I I  -1.0  0.0 0.2 0.4 0.6 0.8 1.0  b  -- Position+Velocity+Voiced/Voiceless Network  --- Position-only Network  1.0 PARCOR for Test  ,,,,,  -1.o ..-I I ..... : I .... I ---'" -- ...... ---"-'!  0.0 0.2 0.4 0.6 0.8 1.0  Original  Source J  (Noise + Pulse)  Synthesized -  ..,,,111111hlltlL_  2000  ,2000  I I [  Time (s)   ........................  .  I i rr, ( seconds )  Figure 7: (a) Model output PARCOR parameters. Only El and k2 are shown. (b)  Original, source model, and syn[hesized acous[ic signals. (c) Wideband spec[rogram  for [he original and syn[hesized speech. U[[erance shown is "Sam sat on lop" from  ates[ sen[ence.  1050 Hirayama, Vatikiotis-Bateson, and Kawato  musculo-skeletal system, its control for the transform from discrete linguistic infor-  mation to continuous motor control signals, and articulatory speech synthesis using  the articulator dynamics. We believe that modeling the dynamics of articulatory  motions is a key issue both for elucidating mechanisms of speech motor control and  for synthesis of natural utterances.  Acknowledgemexits  We thank Yoh'ichi Toh'kura for continuous encouragement. Further support was  provided by HFSP grants to M. Kawato.  References  Albus, J. S. (1975) A new approach to manipulator control: The ccrebellar model  articulation controller (CMAC). Transactimps of the ASME Journal of Dynamic  System, Measurement, and Control, 220-227.  Hirayama, M., E. Vatikiotis-Bateson, M. Kawato, and M. I. Jordan ;1992) Forward  dynamics modeling of speech motor control using physiological data. In Moody,  J. E., Hanson, S. J., and Lippmann, R. P. (cds.) Advaces in Neural Information  Processing System. s 4. San Mateo, CA: Morgan Kauhnann Publishers, 191-198.  Hirayama, M., E. Vatikiotis-Bateson, K. Itonda, Y. Koike, and M. Kawato (1993)  Physiologically based speech synthesis. In Giles, C. L., Hanson, S. J., and Cowan,  J. D. (eds.) Advances in Neural Iformatio Processiug Systems 5. San Mateo,  CA: Morgan Kaufmann Publishers, 658-665.  Itakura, ?. and S. Saito (1969) Speech analysis and synthesis by partial correlation  parameters. Proceeding of Japan Acoustic Society, 2-2-6 (In Japanese).  Jordan, M. I. and D. E. Rumelhart (1992) Forward models: Supervised learning  with a distal teacher. Cognitive Sciece, 16,307-354.  Mermelstein, P. (1973) Articulatory model for tile study of speech production. Jour-  nal of Acoustical Society of America, 53, 1070-1082.  Papcun, J., J. Hochberg, T. R. Thoma.s, T. Laroche, J. Zacks, and S. Levy (1992)  Inferring articulation and recognizing gestures from acoustics with a neural network  rained on x-ray microbeam data. Journal of Acoustical Society of America, 92 (2)  Pt. 1.  Shirai, K. and T. Kobayashi (1991) Estimation of articulatory motion using neural  networks. Journal of Phonetics, 19, 379-385.  Uno, Y., R. Suzuki, and M. Kawato (1989) Tile mininmm muscle tension change  model which reproduces arm movement trajectories. Proceedi,g of the dth Sympo-  sium on Biological and Physiological Engineerig, 299-302 (In Japanese).  Wada, Y. and M. Kawato (1993) A neural network model for arm trajectory forma-  tion of using forward and inverse dynamics models. Neural Networks, 6, 919-932.  Wada, Y., Y. Koike, E. Vatikiotis-Bateson, and M. Kawato (1993) Movement Pat-  tern Recognition Based on the Minimization Principle. Technical R'port of IEICE,  NC93-23, 85-92 (In Japanese).  
Grammatical Inference by  Attentional Control of Synchronization  in an Oscillating Elman Network  Bill Baird  Dept Mathematics,  U.C.Berkeley,  Berkeley, Ca. 94720,  baird@math.berkeley. edu  Todd Troyer  Dept of Phys.,  U.C.San Francisco,  513 Parnassus Ave.  San Francisco, Ca. 94143,  todd@phy. ucsf. edu  Frank Eeckman  Lawrence Livermore  National Laboratory,  P.O. Box 808 (L-270),  Livermore, Ca. 94550,  eeckman@.llnl.gov  Abstract  We show how an "Ehnan" network architecture, constructed from  recurrently connected oscillatory associative memory network mod-  ules, can employ selective "attentional" control of synchronization  to direct the flow of communication and computation within the  architecture to solve a grammatical inference problem.  Previously we have shown how the discrete time "Ehnan" network  algorithm can be implemented in a network completely described  by continuous ordinary differential equations. The time steps (ma-  chine cycles) of the system are implemented by rhythmic variation  (clocking) of a bifurcation parameter. In this architecture, oscilla-  tion amplitude codes the information content or activity of a mod-  ule (unit), whereas phase and frequency are used to "softwire" the  network. Only synchronized modules communicate by exchang-  ing amplitude information; the activity of non-resonating modules  contributes incoherent crosstalk noise.  Attentional control is modeled as a special subset of the hidden  modules with ouputs which affect the resonant frequencies of other  hidden modules. They control synchrony among the other mod-  ules and direct the flow of computation (attention) to effect transi-  tions between two subgraphs of a thirteen state automaton which  the system emulates to generate a Reber grammar. The internal  crosstalk noise is used to drive the required random transitions of  the automaton.  67  68 Baird, Troyer, and Eeckman  1 Introduction  Recordings of local field potentials have revealed 40 to 80 Hz oscillation in vertebrate  cortex [Freeman and Baird, 1987, Gray and Singer, 1987]. The amplitude patterns  of such oscillations have been shown to predict the olfactory and visual pattern  recognition responses of a trained animal. There is further evidence that although  the oscillatory activity appears to be roughly periodic, it is actually chaotic when  exainined in detail. This preliminary evidence suggests that oscillatory or chaotic  network modules may form the cortical substrate for many of the sensory, motor,  and cognitive functions now studied in static networks.  It remains be shown how networks with more complex dynamics can performs these  operations and what possible advantages are to be gained by such complexity. We  have therefore constructed a parallel distributed processing architecture that is in-  spired by the structure and dynalnics of cerebral cortex, and applied it to the prob-  lem of grammatical inference. The construction views cortex as a set of coupled  oscillatory associative memories, and is guided by the principle that attractors must  be used by macroscopic systems for reliable computation in the presence of noise.  This system must function reliably in the midst of noise generated by crosstalk from  it's own activity. Present day digital computers are built of flip-flops which, at the  level of their transistors, are continuous dissipative dynamical systems with differ-  ent attractors underlying the symbols we call "0" and "1". In a similar manner, the  network we have constructed is a symbol processing system, but with analog input  and oscillatory snbsymbolic representations.  The architecture operates as a thirteen state finite automaton that generates the  symbol strings of a Reber grammar. It is designed to demonstrate and study the  following issues and principles of neural computation: (1) Sequential computation  with coupled associative memories. (2) Computation with attractors for reliable  operation in the presence of noise. (3) Discrete time and state symbol processing  arising from continuum dynamics by bifurcations of attractors. (4) Attention as  selective synchronization controling conmmnication and temporal program flow. (5)  chaotic dynamics in some network modules driving randoran choice of attractors in  other network modules. The first three issues have been fully addressed in a previous  paper [Baird et al., 1993], and are only briefly reviewed. We focus here on the last  two.  1.1 Attentional Processing  An important element of intra-cortical communication in the brain, and between  modules in this architecture, is the ability of a module to detect and respond to  the proper input signal from a particular module, when inputs from other modules  irrelevant to the present computation are contributing crosstalk noise. This is smilar  to the problem of coding messages in a computer architecture like the Connection  Machine so that they can be picked up from the common comnmnication buss line  by the proper receiving module.  Periodic or nearly periodic (chaotic) variation of a signal introduces additional de-  grees of freedom that can be exploited in a computational architecture. We investi-  gate the principle that selective control of synchronization, which we hypopthesize  to be a model of "attention", can be used to solve this coding problem and control  communication and program flmv in an architecture with dynamic attractors.  The architecture illustrates the notion that synchronization not only "binds" sen-  Grammatical Inference by Attentional Control of Synchronization 69  sory inputs into "objects" [Gray and Singer, 1987], but binds the activity of selected  cortical areas into a functional whole that directs behavior. It is a model of "at-  tended activity" as that subset which has been included in the processing of the  moment by synchronization. This is both a spatial and temporal binding. Only the  inputs which are synchronized to the internal oscillatory activity of a module can  effect previously learned transitions of attractors within it. For example, consider  two objects in the visual field separately bound in primary visual cortex by synchro-  nization of their components at different phases or frequencies. One object may be  selectively attended to by its entralmnent to oscillatory processing at higher levels  such as V4 or IT. These in turn are in synchrony with oscillatory activity in motor  areas to select the attractors there which are directing motor output.  In the architecture presented here, we have constrained the network dynamics so  that there exist well defined notions of amplitude, phase, and frequency. The net-  work has been designed so that amplitude codes the information content or activity  of a module, whereas phase and frequency are used to "softwire" the network. An  oscillatory network module has a passband outside of which it will not synchro-  nize with an oscillatory input. Modules can therefore easily be desynchronized  by perturbing their resonant frequencies. Furthermore, only synchronized modules  communicate by exchanging amplitude information; the activity of non-resonating  modules contributes incoherant crosstalk or noise. The flow of communication be-  tween modules can thus be controled by controlling synchrony. By changing the  intrinsic frequency of modules in a patterned way, the effective connectivity of the  network is changed. The same hardware and connection matrix can thus subserve  many different computations and patterns of interaction between modules without  crosstalk problems.  The crosstalk noise is actually essential to the function of the system. It serves as  the noise source for making random choices of output symbols and automaton state  transitions in this architecture, as we discuss later. In cortex there is an issue as to  what may constitute a source of randomness of sufficient magnitude to perturb the  large ensemble behavior of neural activity at the cortical network level. It does not  seem likely that the well known molecular fluctuations which are easily averaged  within one or a few neurons can do the job. The architecture here models the  hypothesis that deterministic chaos in the macroscopic dynamics of a network of  neurons, which is the same order of magnitude as the coherant activity, can serve  this purpose.  In a set of modules which is alesynchronized by perturbing the resonant frequencies  of the group, coherance is lost and "random" phase relations result. The character  of the model time traces is irregular as seen in real neural ensemble activity. The be-  havior of the time traces in different modules of the architecture is similar to the tem-  porary appearance and sxvitching of synchronization between cortical areas seen in  observations of cortical processing during sensory/motor tasks in monkeys and hu-  mans [Bressler and Nakanmra, 1993]. The structure of this apparently chaotic sig-  nal and its use in network learning and operation are currently under investigation.  2 Normal Form Associative Memory Modules  The mathenmtical foundation for the construction of network inodules is contained  in the normal form projection algorithm [Baird and Eeckman, 1993]. This is a  learning algorithm for recurrent analog neural networks which allows associative  memory storage of analog patterns, continuous periodic sequences, and chaotic  70 Baird, Troyer, and Eeckman  attractors in the same network. An N node module can be shown to function  as an associative memory for up to N/2 oscillatory, or N/3 chaotic memory at-  tractors [Baird and Eeckman, 1993]. A key feature of a net constructed by this  algorithm is that the underlying dynamics is explicitly isomorphic to any of a  class of standard, well understood nonlinear dynamical systems - a normal .form  [Guckenheimer and Holmes, 1983].  The network modules of this architecture were developed previously as models of  olfactory cortex with distributed patterns of activity like those observed experimen-  tally [Baird, 1990, Freeman and Baird, 1987]. Such a biological network is dynami-  cally equivalent to a network in normal form and may easily be designed, simulated,  and theoretically evaluated in these coordinates. When the intramodule competi-  tion is high, they are "memory" or winner-take-all cordinates where attractors have  one oscillator at maximum amplitude, with the other amplitudes near zero. In fig-  ure two, the input and output modules are demonstrating a distributed amplitude  pattern ( the symbol "T"), and the hidden and context modules are two-attractor  modules in normal form coordinates showing either a right or left side active.  In this paper all networks are discussed in normal form coordinates. By analyz-  ing the network in these coordinates, the amplitude and phase dynamics have a  particularly simple interaction. When the input to a module is synchronized with  its intrinsic oscillation, the amplitude of the periodic activity may be considered  separately from the phase rotation. The module may then be viewed as a static  network with these amplitudes as its activity.  To illustrate the behavior of individual network modules, we examine a binary (two-  attractor) module; the behavior of modules with more than two attractors is similar.  Such a unit is defined in polar normal form coordinates by the following equations  of the Hopf normal form:  i'll '-- ?l, irli -- crlai + (d - bsin(Wclockt))rliro2i + Z wi Ij cs(05 - 01i)  J  i'oi = uiroi - croai + (d- bsin(Wctockt))roir2i + Z wIics(05 - Ooi)  li -" Odi -]- Z W '(Ijl?'li) Sin(Oj -- Oil)  OOi ---- Wi q- y w(Ij/roi)siu(Oj -- Ooi)  J  The clocked parameter bsin(wctockt) is used to implement the discrete time machine  cycle of the Ehnan architecture as discussed later. It has lower frequency (1/10)  than the intrinsic frequency of the unit wi.  Examination of the phase equations shows that a unit has a strong tendency  to synchronize with an input of similar frequency. Define the phase difference  c)=0o-Oi=0o-wlt between a unit 00 and it's input 0i. For either side of a  nnit driven by an input of the same frequency, wt = w0, There is an attractor  at zero phase difference 6 = 00 - 0t = 0 and a repellor at 6 = 180 degrees. In  simulations, the interconnected network of these units described below synchro-  nizes robustly within a few cycles following a perturbation. If the frequencies of  some modules of the architecture are randomly dispersed by a significant amount,  wt - w0  0, phase-lags appear first, then synchronization is lost in those units. An  oscillating module therefore acts as a band pass filter for oscillatory inputs.  Grammatical Inference by Attentional Control of Synchronization 71  When the oscillators are sychronized with the input, Oj - Oi - 0, the phase terms  cos(0j - Oi) = cos(O) = i dissappear. This leaves the amplitude equations  and i'oi with static inputs '.j wiI j and '.j wIj. Thus we have network modules  which emulate static network units in their amplitude activity when fully phase-  locked to their input. Amplitude information is transmitted between modules, with  an oscillatory carrier.  For fixed values of the competition, in a completely synchronized system, the in-  ternal amplitude dynamics define a gradient dynamical system for a fourth order  energy function. External inputs that are phase-locked to the module's intrinsic  oscillation simply add a linear tilt to the landscape.  For low levels of competition, there is a broad circular valley. When tilted by  external input, there is a unique equilibrium that is determined by the bias in tilt  along one axis over the other. Thinking of rli as the "acitivity" of the unit, this  acitivity becomes a monotonically increasing function of input. The module behaves  as an analog connectionist unit whose transfer function can be approximated by a  sigmoid. We refer to this as the "analog" mode of operation of the module.  With high levels of competition, the unit will behave as a binary (bistable) digital  flip-flop element. There are two deep potential wells, one on each axis. Hence the  module performs a winner-take-all choice on the coordinates of its initial state and  maintains that choice "clamped" and independent of external input. This is the  "digital" or "quantized" mode of operation of a module. We think of one attractor  within the unit as representing "1" (the right side in figure two) and the other as  representing"0".  3 Elman Network of Oscillating Associative Memories  As a benchmark for the capabilities of the system, and to create a point of con-  tact to standard network architectures, we have constructed a discrete-time recur-  rent "Ehnan" network [Ehnan, 1991] from oscillatory modules defined by ordinary  differential equations. Previously we con-  structed a system which functions as the six  state finite automaton that perfectly recog-  nizes or generates the set of strings defined by  the Reber grammar described in Cleeremans  et. al. [Cleeremans et al., 1989]. We found  the connections for this network by using the  backpropagation algorithm in a static network  that approxilnates the behavior of the ampli-  tudes of oscillation in a fully synchronized dy-  namic network [Baird et al., 1993].  Here we construct a system that emulates  the larger 13 state automata similar (less one  state) to the one studied by Cleermans, et al  in the second part of their paper. The graph  of this automaton consists of two subgraph  branches each of which has the graph struc-  ture of the automaton learned as above, but  with different assignments of transition out-  put symbols (see fig. 1).  Figure 1. x  Subgraph 1  StaState 3  Subgraph 2  P T  72 Baird, Troyer, and Eeckman  We use two types of modules in implementing the Elman network architecture shown  in figure two below. The input and output layer each consist of a single associative  memory module with six oscillatory attractors (six competing oscillatory modes),  one for each of the six symbols in the grammar. The hidden and context layers  consist of the binary "units" above composed of a two oscillatory attractors. The  architecture consists of 14 binary modules in the hidden and context layers - three  of which are special frequency control modules. The hidden and context layers are  divided into four groups: the first three correspond to each of the two subgraphs plus  the start state, and the fourth group consists of three special control modules, each  of which has only a special control output that perturbs the resonant frequencies of  the modules (by changing their values in the program) of a particular state coding  group when it is at the zero attractor, as illustrated by the dotted control lines in  figure two. This figure shows control unit two is at the one attractor (right side  of the square active) and the hidden units coding for states of subgraph two are  in synchrony with the input and output modules. Activity levels oscillate up and  down through the plane of the paper. Here in midcycle, competition is high in all  modules.  Figure 2.  INPUT  OSCILLATING ELMAN NETWORK  OUTPUT  , , .' /.  .,,. /, ? ..'.  ':?''  subgraph2states ste  ... .  subgranh I states      .: ./'::'?j;; CONTEXT  - - .............. - ..... SYNCHRIZATION  CONTROL UNITS  The discrete machine cycle of the Ehnan algorithm is implemented by the sinusoidal  variation (clocking) of the bifurcation parameter in the normal form equations that  determines the level of intramodule competition [Baird et al., 1993]. At the begin-  ning of a machine cycle, when a network is generating strings, the input and context  layers are at high competition and their activity is clamped at the bottom of deep  basins of attraction. The hidden and output modules are at low competition and  therefore behave as a traditional feedforward network free to take on analog values.  In this analog mode, a real valued error can be defined for the hidden and output  units and standard learning algorithms like backpropagation can be used to train  the connections.  Then the situation reverses. For a Reber grammar there are always two equally pos-  sible next symbols being activated in the output layer, and we let the crosstalk noise  Grammatical Inference by Attentional Control of Synchronization 73  break this symmetry so that the winner-take-all dynamics of the output module can  chose one. High competition has now also "quantized" and clamped the activity in  the hidden layer to a fixed binary vector. Meanwhile, competition is lowered in the  input and context layers, freeing these modules from their attractors. An identity  mapping from hidden to context loads the binarized activity of the hidden layer  into the context layer for the next cycle, and an additional identity mapping from  the output to input module places the chosen output symbol into the input layer  to begin the next cycle.  4 Attentional control of Synchrony  We introduce a model of attention as control of program flow by selective synchro-  nization. The attentional controler itself is modeled in this architecture as a special  set of three hidden modules with ouputs that affect the resonant frequencies of the  other corresponding three subsets of hidden modules. Varying levels of intramodule  competition control the large scale direction of information flow between layers of the  architecture. To direct information flow on a finer scale, the attention mechanism  selects a subset of modules within each layer whose output is effective in driving the  state transition behavior of the system.  By controling the patterns of synchronization within the network we are able to  generate the grammar obtained from an automaton consisting of two subgraphs  connected by a single transition state (figure 1). During training we enforce a seg-  regation of the hidden layer code for the states of the separate subgraph branches of  the automaton so that different sets of synchronized modules learn to code for each  subgraph of the automaton. Then the entire automaton is hand constructed with  an additional hidden module for the start state between the branches. Transitions  in the system from states in one subgraph of the automaton to the other are made  by "attending" to the corresponding set of nodes in the hidden and context layers.  This switching of the focus of attention is accomplished by changing the patterns  of synchronization within the network which changes the flow of communication  betxveen modules.  Each control module modulates the intrinsic frequency of the units coding for the  states a single subgraph or the unit representing the start state. The control modules  respond to a particular input symbol and context to set the intrinsic frequency of  the proper subset of hidden units to be equal to the input layer frequency. As  described earlier, modules can easily be desynchronized by perturbing their resonant  frequencies. By perturbing the frequencies of the remaining modules away from the  input frequency, these modules are no longer communicating with the rest of the  network. Thus coherent information flows from input to output only through one  of three channels. Viewing the automata as a behavioral program, the control  of synchrony constitutes a control of the program flow into its subprograms (the  subgraphs of the automaton).  When either exit state of a subgraph is reached, the "B" (begin) symbol is then  emitted and fed back to the input where it is connected through the first to second  layer weight matrix to the attention control modules. It turns off the synchrony  of the hidden states of the subgraph and allows entrainment of the start state to  begin a new string of symbols. This state in turn activates both a "T" and a "P' in  the output module. The symbol selected by the crosstalk noise and fed back to the  input module is now connected to the control modules through the weight matrix.  It desynchronizes the start state module, synchronizes in the subset of hidden units  74 Baird, Troyer, and Eeckman  coding for the states of the appropriate subgraph, and establishes there the start  state pattern for that subgraph.  Future xvork will investigate the possibilities for self-organization of the patterns of  synchrony and spatially segregated coding in the hidden layer during learning. The  weights for entire automata, including the special attention control hidden units,  should be learned at once.  4.1 Acknowledgments  Supported by AFOSR-91-0325, and a grant from LLNL. It is a pleasure to acknowl-  edge the invaluable assistance of Morris Hirsch, and Walter Freeman.  References  [Baird, 1990] Baird, B. (1990). Bifurcation and learning in network models of oscil-  lating cortex. In Forest, S., editor, Emergent Computation, pages 365-384. North  Holland. also in Physica D, 42.  [Baird and Eeckman, 1993] Baird, B. and Eeckman, F. H. (1993). A normal form  projection algorithm for associative memory. In Hassoun, M. H., editor, Asso-  ciative Neural Memories: Theory and Implementation, New York, NY. Oxford  University Press.  [Baird et al., 1993] Baird, B., Troyer, T., and Eeckman, F. H. (1993). Synchro-  nization and gramatical inference in an oscillating ehnan network. In Hanson,  S., Cowan, J., and Giles, C., editors, Advances in Neural Information Processin9  Systems 5, pages 236-244. Morgan Kaufman.  [Bressler and Nakamura, 1993] Bressler, S. and Nakamura. (1993). Interarea syn-  chronization in Macaque neocortex during a visual discrimination task. In Eeck-  man,F. H., and Bower, J., editors, Computation and Neural Systems, page 515.  Kluwer.  [Cleeremans et al., 1989] Cleeremans, A., Servan-Schreiber, D., and McClelland, J.  (1989). Finite state automata and simple recurrent networks. Neural Computa-  tion, 1(3):372-381.  [Ehnan, 1991] Ehnan, J. (1991). Distributed representations, simple recurrent net-  works and grammatical structure. Machine Learnin9, 7(2/3):91.  [Freeman and Baird, 1987] Freeman, W. and Baird, B. (1987). Relation of olfactory  EEG to behavior: Spatial analysis. Behavioral Neuroscience, 101:393-408.  [Gray and Singer, 1987] Gray, C. M. and Singer, W. (1987). Stimulus dependent  neuronal oscillations in the cat visual cortex area 17. Neuroscience [Suppl],  22:1301P.  [Guckenheimer and Hohnes, 1983] Guckenheimer, J. and Holmes, D. (1983). Non-  linear Oscillations, Dynamical Systems, and Bifurcations of Vector Fields.  Springer, New York.  
Digital Boltzmann VLSI for  constraint satisfaction and learning  Michael Murrayt  Ming-Tak Leungt  Kan Boonyanit  Kong Kritayakiranai  James B. Burr*  Gregory J. Wolff:  Takahiro Watanabe$ Edward Schwartz$  David G. Stork;  Allen M. Petersont  Department of Electrical Engineering  Stanford University  Stanford, CA 94305-4055  Ricoh California Research Center  2882 Sand Hill Road Suite 115  Menlo Park, CA 94025-7022  and  * Sun Microsystems  2550 Garcia Ave., MTV-29, room 203  Mountain View, CA 94043  Abstract  We built a high-speed, digital mean-field Boltzmann chip and SBus  board for general problems in constraint satisfaction and learning.  Each chip has 32 neural processors and 4 weight update processors,  supporting an arbitrary topology of up to 160 functional neurons.  On-chip learning is at a theoretical maximum rate of 3.5 x 10 s con-  nection updates/sec; recall is 12000 patterns/sec for typical condi-  tions. The chip's high speed is due to parallel computation of inner  products, limited (but adequate) precision for weights and activa-  tions (5 bits), fast clock (125 MHz), and several design insights.  896  Digital Boltzmann VLSI for Constraint Satisfaction and Learning 897  i INTRODUCTION  A vast number of important problems can be cast into a form of constraint satisfac-  tion. A crucial difficulty when solving such problems is the fact that there are local  minima in the solution space, and hence simple gradient descent methods rarely suf-  fice. Simulated annealing via the Boltzmann algorithm (/].4) is attractive because it  can avoid local minima better than many other methods (Aarts and Korst, 1989).  It is well known that the problem of learning also generally has local minima in  weight (parameter) space; a Boltzmann algorithm has been developed for learning  which is effective at avoiding local minima (Ackley and Hinton, 1985). The B.4  has not received extensive attention, however, in part because of its slow operation  which is due to the annealing stages in which the network is allowed to slowly relax  into a state of low error. Consequently there is a great need for fast and efficient  special purpose VLSI hardware for implementing the algorithm. Analog Boltzmann  chips have been described by Alspector, Jayakumar and Luna (1992) and by Arima  et al. (1990); both implement stochastic BA. Our digital chip is the first to imple-  ment the deterministic mean field B.4 algorithm (Hinton, 1989), and although its  raw throughput is somewhat lower than the analog chips just mentioned, ours has  unique benefits in capacity, ease of interfacing and scalability (Burr, 1991, 1992).  2 BOLTZMANN THEORY  The problems of constraint satisfaction and of learning are unified through the  Boltzmann learning algorithm. Given a partial pattern and a set of constraints,  the /].4 completes the pattern by means of annealing (gradually lowering a com-  putational "temperature" until the lowest energy state is found) an example  of constraint satisfaction. Over a set of training patterns, the learning algorithm  modifies the constraints to model the relationships in the data.  2.1 CONSTRAINT SATISFACTION  A general constraint satisfaction problem over variables xi (e.g., neural activations)  _ _1 -ij WijXiXj,  is to find the set xi that minimize a global energy function E -- 2  where wij are the (symmetric) connection weights between neurons i and j and  represent the problem constraints.  There are two versions of the BA approach to minimizing E. In one version -- the  stochastic BA -- each binary neuron xi E {-1, 1} is polled randomly, independently  and repeatedly, and its state is given a candidate perturbation. The probability of  acceptance of this perturbation depends upon the amount of the energy change  and the temperature. Early in the annealing schedule (i.e., at high temperature)  the probability of acceptance is nearly independent of the change in energy; late in  annealing (i.e., at low temperature), candidate changes that lead to lower energy  are accepted with higher probability.  In the deterministic mean field BA, each continuous valued neuron (-1 _< X i __  1) is updated simultaneously and in parallel, its new activation is set to xi =  f('.j wijxj), where f(.) is a monotonic non-linearity, typically a sigmoid which  corresponds to a stochastic unit at a given temperature (assuming independent  898 Murray, Leung, Boonyanit, Kritayakirana, Burr, Wolff, Watanabe, Schwartz, Stork, and Peterson  inputs). The inverse slope of the non-linearity is proportional to the temperature; at  the end of the anneal the slope is very high and f(-) is effectively a step function. It  has been shown that if certain non-restrictive assumtJtions hold, and if the annealing  schedule is sufficiently slow, then the final binary states (at 0 temperature) will be  those of minimum E (Hinton, 1989, Peterson and Hartman, 1989).  2.2 LEARNING  The problem of Boltzmann learning is the following: given a network topology  of input and output neurons, interconnected by hidden neurons, and given a set of  training patterns (input and desired output), find a set of weights that leads to high  probability of a desired output activations for the corresponding input activations.  In the Boltzmann algorithm such learning is achieved using two main phases --  the Teacher phase and the Student phase -- followed by the actual Weight update.  During the Teacher phase the network is annealed with the inputs and outputs  clamped (held at the values provided by the omniscient teacher). During the anneal  of the Student phase, only the inputs are clamped -- the outputs are allowed to  vary. The weights are updated according to:  Awij t t s s  = e(<xixj> - <xixj>) (1)  where e is a learning rate and t t  (xixj) the coactivations of neurons i and j at the end  of the Teacher phase and (xxj) in at the end of the Student phase (Ackley and  Hinton, 1985). Hinton (1989) has shown that Eq. 1 effectively performs gradient  descent on the cross-entropy distance between the probability of a state in the  Teacher (clamped) and the Student (free-running) phases.  Recent simulations by Galland (1993) have shown limitations of the deterministic  BA for learning in networks having hidden units directly connected to other hidden  units. While his results do not cast doubt on the deterministic BA for constraint  satisfaction, they do imply that the deterministic BA for learning is most successful  in networks of a single hidden layer. Fortunately, with enough hidden units this  topology has the expressive power to represent all but the most pathological input-  output mappings.  3 FUNCTIONAL DESIGN AND CHIP OPERATION  Figure 1 shows the functional block diagram of our chip. The most important units  are the Weight memory, Neural processors, Weight update processors, Sigmoid and  Rotating Activation Storage (RAS), and their operation are best explained in terms  of constraint satisfaction and learning.  3.1 CONSTRAINT SATISFACTION  For constraint satisfaction, the weights (constraints) are loaded into the Weight  memory, the form of the transfer function is loaded into the Sigmoid Unit, and  the values and duration of the annealing temperatures (the annealing schedule) are  loaded into the Temperature Unit. Then an input pattern is loaded into a bank  of the RAS to be annealed. Such an anneal occurs as follows: At an initial high  Digital Boltzmann VLSI for Constraint Satisfaction and Learning 899  temperature, the 32 Neural processors compute xi = 5-'j wijxj in parallel for the  hidden units. A 4 x multiplexing here permits networks of up to 128 neurons to  be annealed, with the remaining 32 neurons used as (non-annealed) inputs. Thus  our chip supports networks of up to 160 neurons total. These activations are then  stored in the Neural Processor Latch and then passed sequentially to the Sigmoid  unit, where they are multiplied by the reciprocal of the instantaneous temperature.  This Sigmoid unit employs a lookup table to convert the inputs to neural outputs  by means of non-linearity f(.). These outputs are sequentially loaded back into the  activation store. The temperature is lowered (according to the annealing sched-  ule), and the new activations are calculated as before, and so on. The final set of  activations xi (i.e., at the lowest temperature) represent the solution.  Rotating  Activation  Storage  4 weight update processors  weight update cache  Weight  memory  NP cache  Sigmoid  I Temperature unit I  Figure 1: Boltzmann VLSI block diagram. The rotating activation storage (black)  consists of three banks, which for learning problems contain the last pattern (al-  ready annealed), the current pattern (being annealed) and the next pattern (to be  annealed) read onto the chip through the external interface.  3.2 LEARNING  When the chip is used for learning, the weight memory is initialized with random  weights and the first, second and third training patterns are loaded into the RAS.  The three-bank RAS is crucial for our chip's speed because it allows a three-fold  900 Murray, Leung, Boonyanit, Kritayakirana, Burr, Wolff, Watanabe, Schwartz, Stork, and Peterson  concurrency: 1) a current pattern of activations is annealed, while 2) the annealed  last pattern is used to update the weights, while 3) the next pattern is being loaded  from off-chip. The three banks form a circular buffer, each with a Student and a  Teacher activation store.  During the Teacher anneal phase (for the current pattern), activations of the input  and output neurons are held at the values given by the teacher, and the values of  the hidden units found by annealing (as described in the previous subsection). After  the last such annealling step (i.e., at the lowest temperature), the final activations  are left in the Teacher activation store -- the Teacher phase is then complete. The  annealing schedule is then reset to its initial temperature, and the above process is  then repeated for the Student phase; here only the input activations are clamped  to their values and the outputs are free to vary. At the end of this Student anneal,  the final activations are left in the Student activation storage.  In steady state, the MUX then rotates the storage banks of the RAS such that the  next, current, and last banks are now called the current, last, and next, respectively.  To update the weights, the activations in the Student and Teacher storage bank  for the pattern just annealed (now called the "last" pattern) are sent to the four  Weight update processors, along with the weights themselves. The Weight update  processors compute the updated weights according to Eq. 1, and write them back  to the Weight memory. While such weight update is occuring for the last pattern,  the current pattern is annealing and the next pattern is being loaded from off chip.  After the chip has been trained with all of the patterns, it is ready for use in  recall. During recall, a test pattern is loaded to the input units of an activation  bank (Student side), the machine performs a Student anneal and the final output  activations are placed in the Student activation store, then read off the chip to  the host computer as the result. In a constraint satisfaction problem, we merely  download the weights (constraints) and perform a Student anneal.  4 HARDWARE IMPLEMENTATION  Figure 2 shows the chip die. The four main blocks of the Weight memory are at  the top, surrounded by 32 Neural processors (above and below this memory), and  four Weight update processors (between the memory banks). The three banks of  the Rotating Activation Store are at the bottom of the chip. The Sigmoid processor  is at the lower left, and instruction cache and external interface at the lower right.  Most of the rest of the chip consists of clocking and control circuitry.  4.1 VLSI  The chip mixes dynamic and static memory on the same die. The Activation and  Temperature memories are static RAM (which needs no refresh circuitry) while the  Weight memory is dynamic (for area efficiency). The system clock is distributed to  various local clock drivers in order to reduce the global clock capacitance and to se-  lectively disable the clocks in inactive subsystems for reducing power consumption.  Each functional block has its own finite state machine control which communicates  Digital Boltzmann VLSI for Constraint Satisfaction and Learning 901  ' ! II II .I .I i , ,lllll I IlUl,Zla I11   I  """a"""" ' " i  I  : 111jllll, LL 11LL.II !, I JILII JIllIll  Figure 2: Boltzmann VLSI chip die.  asynchronously. For diagnostic purposes, the State Machines and counters are ob-  servable through the External Interface. There is a Single Step mode which has  been very useful in verifying sub-system performance. Figure 3 shows the power  dissipation throughout a range of frequencies. Note that the power is less than  2 Watts throughout.  Extensive testing of the first silicon revealed two main classes of chip error: electrical  and circuit. Most of the electrical problems can be traced to fast edge rates on  the DRAM sense-amp equalization control signals, which cause inductive voltage  transients on the power supply rails of roughly 1 Volt. This appears to be at least  partly responsible for the occasional loss of data in dynamic storage nodes. There  also seems to be insufficient latchup protection in the pads, which is aggravated by  the on-chip voltage surges. The circuit problems can be traced to having to modify  the circuits used in the layout for full chip simulation.  In light of these problems, we have simulated the circuit in great detail in order to  explore possible corrective steps. We have modified the design to provide improved  electrical isolation, resized drivers and reduced the logic depth in several compo-  nents. These corrections solve the problems in simulation, and give us confidence  that the next fab run will yield a fully working chip.  4.2 BOARD AND SBus INTERFACE  An SBus interface board was developed to allow the Boltzmann chip to be used  with a SparcStation host. The registers and memory in the chip can be memory  mapped so that they are directly accessible to user software. The board can support  902 Murray, Leung, Boonyanit, Kritayakirana, Burr, Wolff, Watanabe, Schwartz, Stork, and Peterson  Table 1' Boltzmann VLSI chip specifications  Architecture  Size  Neurons  Weight memory  Activation store  Technology  Transistors  Pins  Clock  I/O rate  Learning rate  Recall rate  Power dissipation  n-layer, arbitrary intercorlnnections  9.5 mm x 9.8 mm  32 processors -, 160 virtual  20,480 5-bit weights (on chip)  3 banks, 160 teacher & 160 student values in each  1.2/m CMOS  4OO,O0O  84  125 MHz (on chip)  3 x 107 activations/sec (sustained)  3.5 x 10 s connection updates/sec (on chip)  12000 patterns/sec  _2 Watts (see Figure 3)  20-bit transfers to the chip at a sustained rate in excess of 8 Mbytes/second. The  board uses reconfigurable Xilinx FPGAs (field-programmable gate arrays) to allow  flexibility for testing with and without the chip installed.  4.3 SOFTWARE  The chip control program is written in C (roughly 1,500 lines of code) and commu-  nicates to the Boltzmann interface card through the virtual memory. The user can  read/write to all activation and weight memory locations and all functions of the  chip (learning, recall, annealing, etc.) can thus be specified in software.  5 CONCLUSIONS AND FUTURE WORK  The chip was designed so that interchip communications could be easily incorpo-  rated by means of high-speed parallel busses. The SBus board, interface and soft-  ware described above will require only minor changes to incorporate a multi-chip  module (MCM) containing several such chips (for instance 16). There is minimal  2  1.75  1.5  1.25  1  O.75  0.5  0.25  0  50 60 70 80 90 100 110  frequency,MHz  Figure 3: Power dissipation of the chip during full operation at 5 Volts.  Digital Boltzmann VLSI for Constraint Satisfaction and Learning 903  interchip communication delay (< 3% overhead), and thus MCM versions of our  system promise to be extremely powerful learning systems for large neural network  problems (Murray et al., 1992).  Acknowledgements  Thanks to Martin Boliek and Donald Wynn for assistance in design and construc-  tion of the SBus board. Research support by NASA through grant NAGW419 is  gratefully acknowledged; VLSI fabrication by MOSIS. Send reprint requests to Dr.  Stork: stork@crc.ricoh.com.  References  E. Aarts & J. Korst. (1989) Simulated Annealing and Boltzmann Machines: A  stochastic approach to combinatoriaI optimization and neural computing. New York:  Wiley.  D. H. Ackley & G. E. Hinton. (1985) A learning algorithm for Boltzmann machines.  Cognitive Science 9, 147-169.  J. Alspector, A. Jayakumar & S. Luna. (1992) Experimental evaluation of learning  in a neural microsystem. Advances in Neural Information Processing Systems-d,  J. E. Moody, S. J. Hanson & R. P. Lippmann (eds.), San Mateo, CA: Morgan  Kaufmann, 871-878.  Y. Arima, K. Mashiko, K. Okada, T. Yamada, A. Maeda, H. Kondoh & S. Kayano.  (1990) A self-learning neural network chip with 125 neurons and 10K self-  organization synapses. In Symposium on VLSI Circuits, Solid State Circuits Council  Staff, Los Alamitos, CA: IEEE Press, 63-64.  J. B. Burr. (1991) Digital Neural Network Implementations. Neural Networks:  Concepts, Applications, and Implementations, Volume 2, P. Antognetti & V. Mi-  lutinovic (eds.) 237-285, Englewood Cliffs, N J: Prentice Hall.  J. B. Burr. (1992) Digital Neurochip Design. Digital Parallel Implementations of  Neural Networks. K. Wojtek Przytula & Viktor K. Prasanna (eds.), Englewood  Cliffs, N J: Prentice Hall.  C. C. Galland. (1993) The limitations of deterministic Boltzmann machine learning.  Network 4, 355-379.  G. E. Hinton. (1989) Deterministic Boltzmann learning performs steepest descent  in weight-space. Neural Computation 1, 143-150.  C. Peterson & E. Hartman. (1989) Explorations of the mean field theory learning  algorithm. Neural Networks 2, 475-494.  M. Murray, J. B. Burr, D. G. Stork, M.-T. Leung, K. Boonyanit, G. J. Wolff  & A.M. Peterson. (1992) Deterministic Boltzmann machine VLSI can be scaled  using multi-chip modules. Proc. of the International Conference on Application  Specific Array Processors. Berkeley, CA (August 4-7) Los Alamitos, CA: IEEE  Press, 206-217.  
Neural Network Definitions of Highly  Predictable Protein Secondary Structure  Classes  Alan Lapedes  Complex Systems Group (T13)  LANL, MS B213 Los Alamos N.M. 87545  and The Santa Fe Institute, Santa Fe, New Mexico  Evan Steeg  Department of Computer Science  University of Toronto, Toronto, Canada  Robert Farbet  Complex Systems Group (T13)  LANL, MS B213 Los Alamos N.M. 87545  Abstract  We use two co-evolving neural networks to determine new classes  of protein secondary structure which are significantly more pre-  dictable from local amino sequence than the conventional secondary  structure classification. Accurate prediction of the conventional  secondary structure classes: alpha helix, beta strand, and coil, from  primary sequence has long been an important problem in compu-  tational molecular biology. Neural networks have been a popular  method to attempt to predict these conventional secondary struc-  ture classes. Accuracy has been disappointingly low. The algo-  rithm presented here uses neural networks to similtaneously exam-  ine both sequence and structure data, and to evolve new classes  of secondary structure that can be predicted from sequence with  significantly higher accuracy than the conventional classes. These  new classes have both similarities to, and differences with the con-  ventional alpha helix, beta strand and coil.  809  810 Lapedes, Steeg, and Farber  The conventional classes of protein secondary structure, alpha helix and beta  sheet, were first introduced in 1951 by Linus Pauling and Robert Corey  [Pauling, 1951] on the basis of molecular modeling. Prediction of secondary  structure from the amino acid sequence has long been an important problem  in computational molecular biology. There have been numerous attempts to  predict locally defined secondary structure classes using only a local window  of sequence information. The prediction methodology ranges from a combina-  tion of statistical and rule-based methods [Chou, 1978] to neural net methods  [Qian, 1988], [Maclin, 1992], [Kneller, 1990], [Stolorz, 1992]. Despite a variety of  intense efforts, the accuracy of prediction of conventional secondary structure is  still distressingly low.  In this paper we will use neural networks to generalize the notion of protein sec-  ondary structure and to find new classes of structure that are significantly more  predictable. We define protein "secondary structure" to be any classification of  protein structure that can be defined using only local "windows" of structural in-  formation about the protein. Such structural information could be, e.g., the classic    angles [Schulz, 1979] that describe the relative orientation of peptide units along  the protein backbone, or any other representation of local backbone structure. A  classification of local structure into "secondary structure classes", is defined to be  the result of any algorithm that uses a representation of local structure as Input,  and which produces discrete classification labels as Output. This is a very general  definition of local secondary structure that subsumes all previous definitions.  We develop classifications that are more predictable than the standard classifica-  tions [Pauling, 1951] [Kabsch, 1983] which were used in previous machine learning  projects, as well as in other analyses of protein shape. We show that these new,  predictable classes of secondary structure bear some relation to the conventional  category of "helix", but also display significant differences.  We consider the definition, and prediction from sequence, of just two classes of  structure. The extension to multiple classes is not difficult, but will not be made  explicit here for reasons of clarity. We won't discuss details concerning construction  of a representative training set, or details of conventional neural network train-  ing algorithms, such as backpropagation. These are well studied subjects that  are addressed in e.g., [Stolorz, 1992] in the context of protein secondary struc-  ture prediction. We note in passing that one can employ complicated network  architectures containing many output neurons (e.g. three output neurons for pre-  dicting alpha helix, beta chain, random coil), or many hidden units etc. (c.f.  [Stolorz, 1992], [Qian, 1988], [Kneller, 1990]). However, explanatory figures pre-  sented in the next section employ only one output unit per net, and no hidden  units, for clarity.  Neural Network Definitions of Highly Predictable Protein Secondary Structure Classes 811  A widely adopted definition of protein secondary structure classes is due to Kabsch  and Sander [Kabsch, 1983]. It has become conventional to use the Kabsch and  Sander definition to define, via local structural information, three classes of sec-  ondary structure: alpha helix, beta strand, and a default class called random coil.  The Kabsch and Sander alpha helix and beta strand classification captures in large  part the classification first introduced by Pauling and Corey [Pauling, 1951]. Soft-  ware implementing the Kabsch and Sander definitions, which take a local window  of structural information as Input, and produce the Kabsch and Sander secondary  structure classification of the window as Output, is widely available.  The key ideas of this paper are contained in Fig. (1).  E: E,Correlation(()[('),O  Left Net  Maps AA sequence to  structure".  "secondary  Ri2ht Net  Maps (l).q  to "secondary  structure".  In this figure the Kabsch and Sander rules are represented by a second neural net-  work. The Kabsch and Sander rules are just an Input/Output mapping (from a  local window of structure to a classification of that structure) and may in princi-  ple be replaced with an equivalent neural net representing the same Input/Output  mapping. We explicitly demonstrated that a simple neural net is capable of repre-  senting rules of the complexity of the Kabsch and Sander rules by training a network  to perform the same structure classification as the Kabsch and Sander rules, and  obtained high accuracy.  The representation of the structure data in the right-hand network uses q)gt angles.  The right-hand net sees a window of q)gt angles corresponding to the window of  amino acids in the left-hand network. Problems due to the angular periodicity of  the gt angles (i.e., 360 degrees and 0 degrees are different numbers, but represent  the same angle) are eliminated by utilizing both the sin and cos of each angle.  812 Lapedes, Steeg, and Farber  The representation of the amino acids in the left-hand network is the usual unary  representation employing twenty bits per amino acid. Results quoted in this paper  do not use a special twenty-first bit to represent positions in a window extending  past the ends of a protein.  Note that the right-hand neural network could implement extremely general def-  initions of secondary structure by changing the weights. We next show how to  change the weights in a fashion so that new classifications of secondary structure  are derived under the important restriction that they be predictable from amino  acid sequence. In other words, we require that the synaptic weights be chosen so  that the output of the left-hand network and the output of the right-hand network  agree for each sequence-structure pair that is input to the two networks.  To achieve this, both networks are trained simultaneously, starting from random  initial weights in each net, under the sole constraint that the outputs of the two  networks agree for each pattern in the training set. The mathematical implemen-  tation of this constraint is described in various versions below. This procedure  is a general, effective method of evolving predictable secondary structure classifi-  cations of experimental data. The goal of this research is to use two mutually  self-supervised networks to define new classes of protein secondary structure which  are more predictable from sequence than the standard classes of alpha helix, beta  sheet or coil.  3 CONSTRAINING THE TWO NETS TO AGREE  One way to impose agreement between the outputs of the two networks is to require  that they covary when viewed as a stream of real numbers. Note that it is not  sufficient to merely require that the outputs of the left-hand and right-hand nets  agree by, e.g., minimizing the following objective function  E = y.(LeftO  - RightO(r)) 2 (1)  p  Here, LeftO (r) and RightO  represent the outputs of the left-hand and right-  hand networks, respectively, for the ph pair of input windows: (sequence window  -left net) and (structure window -right net). It is necessary to avoid the trivial  minimum of E obtained where the weights and thresholds are set so that each net  presents a constant Output regardless of the input data. This is easily accomplished  in Eqn (1) by merely setting all the weights and thresholds to 0.0.  Demanding that the outputs vary, or more explicitly co-vary, is a viable solution  to avoiding trivial local minima. Therefore, one can maximize the correlation, p,  between the left-hand and right-hand network outputs. The standard correlation  measure between two objects, LeftO (r) and RightO  is:  p = (Le/to(p)- e/to)(nigatO(p)- nigatO)  (3)  where LeftO denotes the mean of the left net's outputs over the training set, and  respectively for the right net. p is zero if there is no variation, and is maximized  Neural Network Definitions of Highly Predictable Protein Secondary Structure Classes 813  if there is simultaneously both individual variation and joint agreement. In our  situation it is equally desirable to have the networks maximally anti-correlated  as it is for them to be correlated. (Whether the networks choose correlation, or  anti-correlation, is evident from the behavior on the training set). Hence the min-  imization of E = _p2 would ensure that the outputs are maximally correlated  (or anti-correlated). While this work was in progress we received a preprint by  Schmidhuber [Schmidhuber, 1992] who essentially implemented Eqn. (1) with an  additional variance term (in a totally different context). Our results using this mea-  sure seem quite susceptible to local minima and we prefer alternative measures to  enforce agreement.  One alternative to enforce agreement, since one ultimately measures predic-  tive performance on the basis of the Mathews correlation coefficient (see, e.g.,  [Stolorz, 1992]), is to simultaneously train the two networks to maximize this mea-  sure. The Mathews coefficient, Ci, for the i th state is defined as:  Pini - ttioi  [(ni + + oi)(p + +  where pi is the number of examples where the left-hand net and right-hand net  both predict class i, ni is the number of examples where neither net predicts i, ui  counts the examples where the left net predicts i and the right net does not, and oi  counts the reverse. Minimizing E = -Ci  optimizes Ci.  Other training measures forcing agreement of the left and right networks may be  used. Particularly suitable for the situation of many outputs (i.e., more than two-  class discrimination) is "mutual information". Use of mutual information in this  context is related to the IMAX algorithm for unsupervised detection of regularities  across spatial or temporal data [Becker, 1992]. The mutual information is defined  M = Epii log P'J (4)  i,j Pi.P.j  where Pij is the joint probability of occurrence of the states of the left and right  networks. (In previous work [Stolorz, 1992] we showed how Pij may be defined  in terms of neural networks). Minimizing E = -M maximizes M. While M has  many desirable properties as a measure of agreement between two or more variables  [Stolorz, 1992] [Farber, 1992] [Lapedes, 1989] [Korber, 1993], our preliminary sim-  ulations show that maximizing M is often prone to poor local maxima.  Finally, an alternative to using mutual information for multi-class, as opposed to  dichotomous classification, is the Pearson correlation coefficient, X 2. This is defined  in terms of Pij as  X'2 -- E (Pij -- Pi.P.j) 2  i,j Pi.P.j  (5)  Our simulations indicate that X a, Ci and p are all less susceptible to local minima  814 Lapedes, Steeg, and Farber  than M. However, these other objective functions suffer the defect that predictabil-  ity is emphasized at the expense of utility. In other words, they can be maximal  for the peculiar situation where a structural class is defined that occurs very rarely  in the data, but when it occurs, it is predicted perfectly by the other network. The  utility of this classification is therefore degraded by the fact that the predictable  class only occurs rarely. Fortunately, this effect did not cause difficulties in the  simulations we performed. Our best results to date have been obtained using the  Mathews objective function (see Results).  4 RESULTS  The database we used consisted of 105 proteins and is identical to that used in  previous investigations [Kneller, 1990] [Stolorz, 1992]. The proteins were divided  into two groups: a set of 91 "training" proteins, and a distinct "prediction" set  of 14 proteins. The resulting database is similar to the database used by Qian &  Sejnowski [Qian, 1988] in their neural network studies of conventional secondary  structure prediction. When comparison to predictability of conventional secondary  structure classes was needed, we defined the conventional alpha, beta and coil states  using the Kabsch and Sander definitions and therefore these states are identical to  those used in previous work [Kneller, 1990] [Stolorz, 1992]. A window size of 13  residues resulted in 16028 train set examples and 3005 predict set examples. Effects  of other windows sizes have not yet been extensively tested. All results, including  conventional backpropagation training of Kabsch and Sander classifications, as well  as two-net training of our new secondary structure classifications, did not employ  an extra symbol denoting positions in a window that extended past the ends of a  protein. Use of such a symbol could further increase accuracy.  We found that random initial conditions are necessary for the development of in-  teresting new classes. However, random initial conditions also suffer to a certain  extent from local minima. The mutual information function, in particular, often  gets trapped quickly in uninteresting local minima when evolved from random initial  conditions. More success was obtained with the other objective functions discussed  above. We have not exhaustively investigated strategies to avoid local minima,  and usually just chose new initial conditions if an uninteresting local minimum was  encountered.  Results were best for two class discrimination using the Mathews objective function  and a layer of five hidden units in each net. If one assigns the name "Xclass" to the  newly defined structural class, then the Mathews coefficient on the prediction set  for the Xclass dichotomy is -0.425. The Mathews coefficient on the train set for the  Xclass dichotomy is -0.508. For comparison, the Mathews coefficient on the same  predict set data for dichotomization (using standard backpropagation training with  no hidden units), into the standard secondary structure classes Alpha/NotAlpha,  Beta/NotBeta, and Coil/NotCoil is 0.33, 0.26, and 0.39, respectively. Adding hid-  den units gives negligible accuracy increase in predicting the conventional classes,  but is important for improved prediction of the new classes. The negative sign of  the two-net result indicates anti-correlation - a feature allowed by our objective  function. The sign of the correlation is easily assessed on the train set and then can  be trivially compensated for in prediction.  Neural Network Definitions of Highly Predictable Protein Secondary Structure Classes 815  A natural question to ask is whether the new classes are simply related to the more  conventional classes of alpha helix, beta, coil. A simple answer is to compute the  Mathews correlation coefficient of the new secondary structure classes with each  of the three Kabsch and Sander classes, for those examples in which the sequence  network agreed with the structure network's classification. The correlation with  Kabsch and Sander's alpha helix is highest: a Mathews coefficient of 0.248 was  obtained on the train set, while a Mathews coefficient of 0.247 was obtained on  the predict set. There is therefore a significant degree of correlation with the con-  ventional classification of alpha helix, but significant differences exist as well. The  new classes are a mixture of the conventional classes, and are not solely dominated  by either alpha, beta or coil. Conventional alpha-helices comprise roughly 25% of  the data (for both train and predict sets), while the new Xclass comprises 10%. It  is quite interesting that an evolution of secondary structure classifications starting  from random initial conditions, and hence completely unbiased towards the conven-  tional classifications, results in a classification that has significant relationship to  conventional helices but is more predictable from amino acid sequence than conven-  tional helices. Graphical analysis (not shown here) of the new Xclass shows that  the Xclass that is most closely related to helix typically extends the definition of  helix past the standard boundaries of an alpha-helix.  5 CONCLUSIONS  A primary goal of this investigation is to evolve highly predictable secondary struc-  ture classes. Ultimately, such classes could be used, e.g., to provide constraints  on tertiary structure calculations. Further work remains to derive even more pre-  dictable classes and to analyze their physical meaning. However, it is now clear  that the use of two, co-evolving, adaptive networks defines a novel and useful ma-  chine learning paradigm that allows the evolution of new definitions of secondary  structure that are significantly more predictable from primary amino acid sequence  than the conventional definitions.  Related work is that of [Hunter, 1992], [Hunter, 1992], [Zhang, 1992], [Zhang, 1993]  in which clustering either only in sequence space, or only in structure space, is  attempted. However, no condition on the compatibility of the clustering is required,  so new classes of structure are not guaranteed to be predictable from sequence.  Finally, we note that the methods described here might be usefully applied to  other cognitive/perceptual or engineering tasks in which correlation of two or more  different representations of the same data is required. In this regard the relation of  our work to that of independent work of Becker [Becker, 1992], and of Schmidhuber  [Schmidhuber, 1992], should be noted.  Acknowledgements  We are grateful for useful discussions with Geoff Hinton, Sue Becker, and Joe Bryn-  gelson. Sue Becker's contribution of software that was used in the early stages of  this project is much appreciated. The research of Alan Lapedes and Robert Farbet  was supported in part by the U.S. Department of Energy. The authors would like  to acknowledge the hospitality of the Santa Fe Institute, where much of this work  816 Lapedes, Steeg, and Farber  was performed.  References  [Becker, 1992]  [Becker, 1992]  [Chou, 1978]  [Farber, 1992]  [Hunter, 1992]  [Hunter, 1992]  [Kabsch, 1983]  [Kneller, 1990]  [Korber, 1993]  [Lapedes, 1989]  [Maclin, 1992]  [Pauling, 1951]  [Qian, 1988]  [Schmidhuber, 1992]  [Schulz, 1979]  [Stolorz, 1992]  [Zhang, 1992]  [Zhang, 1993]  S. Becker. An Information-theoretic Unsupervised Learning  Algorithm for Neural Networks. PhD thesis, University of  Toronto (1992)  S. Becker, G. Hinton, Nature 355, 161-163 (1992)  P. Chou, G. Fasman Adv. Enzymol. 47, 45 (1978)  R. Farbet, A. Lapedes J. Mol. Biol. 226 , 471, (1992)  L. Hunter, N. Harris, D. States Proceedings of the Ninth Inter-  national Conference on Machine Learning, San Mateo, Cali-  fornia, Morgan Kaufmann Associates (1992)  L. Hunter, D. States, IEEE Epert, 7(4) 67-75 (1992)  W. Kabsch, C. Sander Biopolymers 22, 2577 (1983)  D. KnelleL F. Cohen, R. Langridge J. Mol. Biol. 214, 171  (1990)  B. Korber, R. Farbet, D. Wolpert, A. Lapedes P.N.A.S. - in  press (1993)  A. Lapedes, C.Barnes, C. Burks, R. Farber, K. Sirotkin in  Computers and DNA editors: G.Bell, T. Mart, (1989)  R. Maclin, J. W. Shavlik Proceedings of the Tenth National  Conference on Artificial Intelligence, San Jose, California,  Morgan Kauffman Associates (1992)  L. Pauling, R. Corey Proc. Nat. Acad. Sci. 37, 205 (1951)  N. Qian, T. Sejnowski J. Moi. Biol. 202, 865 (1988)  J. Schmidhuber Discovering Predictable Classifications, Tech-  nical report CU-CS-626-92, Department of Computer Science,  University of Colorado (1992)  G. Schulz, R. Schirmer Principles of Protein Structure  Springer Verlag, New York, (1979)  P. Stolorz, A. Lapedes, X. Yuan J. Moi. Biol. 225,363 (1992)  X. Zhang, D. Waltz in Artificial Intelligence and Molecular  Biology, editor: L. Hunter, AAAI Press (MIT Press) (1992)  X. Zhang, J. Fetrow, W. Rennie, D. Waltz, G. Berg, in Pro-  ceedings: First International Conference on Intelligent Sys-  tems For Molecular Biology, p. 438, editors: L. Hunter, D.  Seads, J. Shavlik, AAAI Press, Menlo Park, CA. (1993)  
Supervised Learning with Growing Cell  Structures  Bernd Fritzke  Institut fiir Neuroinformatik  Ruhr-Universitit Bochum  Germany  Abstract  We present a new incremental radial basis function network suit-  able for classification and regression problems. Center positions  are continuously updated through soft competitive learning. The  width of the radial basis functions is derived from the distance  to topological neighbors. During the training the observed error  is accumulated locally and used to determine where to insert the  next unit. This leads (in case of classification problems) to the  placement of units near class borders rather than near frequency  peaks as is done by most existing methods. The resulting networks  need few training epochs and seem to generalize very well. This is  demonstrated by examples.  I INTRODUCTION  Feed-forward networks of localized (e.g., Gaussian) units are an interesting alter-  native to the more frequently used networks of global (e.g., sigmoidal) units. It  has been shown that with localized units one hidden layer suffices in principle to  approximate any continuous function, whereas with sigmoidal units two layers are  necessary.  In the following we are considering radial basis function networks similar to those  proposed by Moody & Darken (1989) or roggio & Girosi (1990). Such networks  consist of one layer L of Gaussian units. Each unit c G L has an associated vector  wc  R n indicating the position of the Gaussian in input vector space and a standard  255  256 Fritzke  deviation rc. For a given input datum   R'* the activation of unit c is described  by  o' c  (1)  On top of the layer L of Gaussian units there are m single layer percepttons.  Thereby, m is the output dimensionality of the problem which is given by a number  of input/output pairs 1 ((,() G (R n x R'). Each of the single layer perceptrons  computes a weighted sum of the activations in L:  Oi() = y] wiDj() i  {1,..., m} (2)  jL  With wij we denote the weighted connection from local unit j to output unit i.  Training of a single layer perceptron to minimize square error is a very well under-  stood problem which can be solved incrementally by the delta rule or directly by  linear algebra techniques (Moore-Penrose inverse). Therefore, the only (but severe)  difficulty when using radial basis function networks is choosing the number of local  units and their respective parameters, namely center position w and width  One extreme approach is to use one unit per data points and to position the units  directly at the data points. If one chooses the width of the Gaussians sufficiently  small it is possible to construct a network which correctly classifies the training  data, no matter how complicated the task is (Fritzke, 1994). However, the network  size is very large and might even be infinite in the case of a continuous stream of  non-repeating stochastic input data. Moreover, such a network can be expected to  generalize poorly.  Moody & Darken (1989), in contrast, propose to use a fixed number of local units  (which is usually considerably smaller than the total number of data points). These  units are first distributed by an unsupervised clustering method (e.g., k-means).  Thereafter, the weights to the output units are determined by gradient descent.  Although good results are reported for this method it is rather easy to come up  with examples where it would not perform well: k-means positions the units based  on the density of the training data, specifically near density peaks. However, to ap-  proximate the optimal Bayesian a posterJori classifier it would be better to position  units near class borders. Class borders, however, often lie in regions with a particu-  larly low data density. Therefore, all methods based on k-means-like unsupervised  placement of the Gaussians are in danger to perform poorly with a fixed number of  units or - similarly undesirable - to need a huge number of units to achieve decent  performance.  From this one can conclude that - in [he case of radial basis function networks  - it is essential to use the class labels not only for the training of the connection  weights but also for the placement of the local units. Doing this forms the core of  the method proposed below.  1Throughout this article we assume a classification problem and use the corresponding  terminology. However, the described method is suitable for regression problems as well.  Supervised Learning with Growing Cell Structures 257  2 SUPERVISED GROWING CELL STRUCTURES  In the following we present an incremental radial basis function network which  is able to simultaneously determine a suitable number of local units, their center  positions and widths as well as the connection weights to the output units. The  basic idea is a very simple one:  0. Start with a very small radial basis function network.  1. Train the current network with some I/O-pairs from the training data.  2. Use the observed accumulated error to determine where in input vector  space to insert new units.  3. If network does not perform well enough goto 1.  One should note that during the training phase (Step 1.) error is accumulated  over several data items and this accumulated error is used to determine where to  insert new units (Step 2.). This is different from the approach of Platt (1991) where  insertions are based on single poorly mapped patterns. In both cases, however, the  goal is to position new units in regions where the current network does not perform  well rather than in regions where many data items stem from.  In our model the center positions of new units are interpolated from the positions  of existing units. Specifically, after some adaptation steps we determine the unit  q which has accumulated the maximum error and insert a new unit in between q  and one of its neighbors in input vector space. The interpolation procedure makes  it necessary to allow the center positions of existing units to change. Otherwise, all  new units would be restricted to the convex hull of the centers of the initial network.  We do not necessarily insert a new unit in between q and its nearest neighbor.  Rather we like to choose one of the units with adjacent Voronoi regions 2. In the  two-dimensional case these are the direct neighbors of q in the Delaunay triangu-  lation (Delaunay-neighbors) induced by all center positions. In higher-dimensional  spaces there exists an equivalent based on hypertetrahedrons which, however, is  very hard to compute. For this reason, we arrange our units in a certain topological  structure (see below) which has the property that if two units are direct neighbors  in that structure they are mostly Delaunay-neighbors. By this we get with very  little computational effort an approximate subset of the Delaunay-neighbors which  seems to be sufficient for practical purposes.  2.1 NETWORK STRUCTURE  The structure of our network is very similar to standard radial basis function net-  works. The only difference is that we arrange the local units in a k-dimensional  topological structure consisting of connected simplices a (lines for k = 1, triangles  2The Voronoi region of a unit c denotes the part of the input vector space which consists  of points for which c is the nearest unit.  3A historical reason for this specific approach is the fact that the model was developed  from an unsupervised network (see Fritzke, 1993) where the k-dimensional neighborhood  was needed to reduce dimensionality. We currently investigate an alternative (and more  258 Ftzke  for k = 2, tetrahedrons for k = 3 and hypertetrahedrons for larger k). This ar-  rangement is done to facilitate the interpolation and adaptation steps described  below. The initial network consists of one k-dimensional simplex (k q- 1 local units  fully connected with each other). The neighborhood connections are not weighted  and do not directly influence the behavior of the network. They are, however, used  to determine the width of the Gaussian functions associated with the units. Let  for each Gaussian unit c denote Nc the set of direct topological neighbors in the  topological structure. Then the width of c is defined as  dENt  (3)  which is the mean distance to the topological neighbors. If topological neighbors  have similar center positions (which will be ensured by the way adaptation and  insertion is done) then this leads to a covering of the input vector space with partially  overlapping Gaussian functions.  2.2 ADAPTATION  It was mentioned above that several adaptation steps are done before a new unit is  inserted. One single adaptation step is done as follows (see fig. 1)'  * Chose an I/O-pair (,(), C R'*,( C R") from the training data.  * determine the unit s closest to  (the so-called best-matching unit).  e Move the centers of s and its direct topological neighbors towards  Aw -- eb( - w), Aw -- e,,( -- w) for allc  N  eb and en are small constants with eb >>  e Compute for each local unit c 6 L the activation Dc() (see eqn. 1)  * Compute for each output unit i the activation Oi (see eqn. 2)   Compute the square error by  i--1  * Accumulate error at best-matching unit s:  Aerr, = SE  * Make Delta-rule step for the weights (c denotes the learning rate)'  Awij = aDj(i- Oi) i 6 {1,..., m}, j 6 L  Since together with the best-matching unit always its direct topological neighbors  are adapted, neighboring units tend to have similar center positions. This prop-  erty can be used to determine suitable center positions for new units as will be  demonstrated in the following.  Supervised Learning with Growing Cell Structures 259  a) Before...  b) during, and ...  c) ... after adaptation  Figure 1: One adaptation step. The center positions of the current network are  shown and the change caused by a single input signal. The observed error SE for  this pattern is added to the local error variable of the best-matching unit.  a) Before ...  b) ... and after insertion  Figure 2: Insertion of a new unit. The dotted lines indicate the Voronoi fields.  The unit q has accumulated the most error and, therefore, a new unit is inserted  between q and one of its direct neighbors.  2.3 INSERTION OF NEW UNITS  After a constant number A of adaptation steps a new unit is inserted. For this  purpose the unit q with maximum accumulated error is determined. Obviously,  q lies in a region of the input vector space where many misclassifications occur.  One possible reason for this is that the gradient descent procedure is unable to find  suitable weights for the current network. This again might be caused by the coarse  resolution at this region of the input vector space: if data items from different  classes are covered by the same local unit and activate this unit to about the same  degree then it might be the case that their vectors of local unit activations are  nearly identical which makes it hard for the following single layer perceptrons to  distinguish among them. Moreover, even if the activation vectors are sufficiently  different they still might be not linearly separable.  accurate) approximation of the Delaunay triangulation which is based on the "Neural-Gas"  method proposed by Martinetz gz Schulten (1991).  260 Fritzke  o  o  o  o  o  o  o  o  o  o  a) two spiral problem: 194 points in two  classes  b) decision regions for Cascade-  Correlation (reprinted with permission  from Fahlman & Lebiere, 1990)  Figure 3: Two spiral problem and learning results of a constructive network.  The insertion of a new local unit near q is likely to improve the situation: This unit  will probably be activated to a different degree by the data items in this region and  will, therefore, make the problem easier for the single layer perceptrons.  What exactly are we doing? We choose one of the direct topological neighbors of  q, say a unit f (see also fig. 2). Currently this is the neighbor with the maximum  accumulated error. Other choices, however, have shown good results as well, e.g., the  neighbor with the most distant center position or even a randomly picked neighbor.  We insert a new unit r in between q and f and initialize its center by  = (% +  (4)  We connect the new unit with q and f and with all common neighbors of q and f.  The original connection between q and f is removed. By this we get a structure of  k-dimensional simplices again. The new unit gets weights to the output units which  are interpolated from the weights of its neighbors. The same is done for the initial  error variable which is linearly interpolated from the variables of the neighbors of r.  After the interpolation all the weights of r and its neighbors and the error variables  of these units are multiplied by a factor IN,.I/(IN,.I + 1)1. This is done to disturb  the output of the network as less as possible 4. However, the by far most important  4The redistribution of the error variable is again a relict from the unsupervised version  (Fritzke, 1993). There we count signals rather than accumulate error. An elaborate  scheme for redistributing the signal counters is necessary to get good local estimates of the  probability density. For the supervised version this redistribution is harder to justify since  the insertion of a new unit in general makes previous error information void. However,  even though there is still some room for simplification, the described scheme does work  very well already in its present form.  Supervised Learning with Growing Cell Structures 261  o  o  a) final network with 145 cells b) decision regions  Figure 4: Performance of the Growing Cell Structures on the two spiral benchmark.  decision seems to be to insert the new unit near the unit with maximum error. The  weights and the error variables adjust quickly after some learning steps.  2.4 SIMULATION RESULTS  Simulations with the two spiral problem (fig. 3a) have been performed. This clas-  sification benchmark has been widely used before so that results for comparison  are readily available. Figure 3b) shows the result of another constructive algorithm.  The data consist of 194 points arranged on two interlaced spirals in the plane. Each  spiral corresponds to one class. Due to the high nonlinearity of the task it is partic-  ular difficult for networks consisting of global units (e.g., multi-layer percepttons).  However, the varying density of data points (which is higher in the center of the  spirals) makes it also a challenge for networks of local units.  As for most learning problems the interesting aspect is not learning the training  examples but rather the performance on new data which is often denoted as gen-  eralization. Baum & Lang (1991) defined a test set of 576 points for this problem  consisting of three equidistant test points between each pair of adjacent same-class  training points. They reported for their best network 29 errors on the test set in  the mean.  In figure 4 a typical network generated by our method can be seen as well as the  corresponding decision regions. No errors on the test set of Baum and Lang are  made. Table i shows the necessary training cycles for several algorithms. The new  growing network uses far less cycles than the other networks.  Other experiments have been performed with a vowel recognition problem (Fritzke,  1993). In all simulations we obtained significantly better generalization results  than Robinson (1989) who in his thesis investigated the performance of several  connectionist and conventional algorithms on the same problem. The necessary  262 Fritzke  Table 1: Training epochs necessary for the two spiral problem  network model epochs test error reported in  Backpropagation 20000 yes Lung & Witbrock (1989)  Cross Entropy BP 10000 yes Lung & Witbrock (1989)  Cascade-Correlation 1700 yes Fahlman &; Lebiere (1990)  Growing Cell Structures 180 no Fritzke (1993)  number of training cycles for our method was lower by a factor of about 37 than  the numbers reported by Robinson (1993, personal communication).  REFERENCES  Baum, E. B. & K. E. Lung [1991], "Constructing hidden units using examples and queries,"  in Advances in NeurM Information Processing Systems 3, R.P. Lippmann, J.E.  Moody & D.S. Touretzky, eds., Morgan Kaufmann Publishers, San Mateo, 904-  910.  Fahlman, S. E. & C. Lebiere[1990], "The Cascade-Correlation Learning Architecture,"  in Advances in NeurM Information Processing Systems 2, D.S. Touretzky, ed.,  Morgan Kaufmann Publishers, San Mateo, 524-532.  Fritzke,  B. [1993], "Growing Cell Structures - a self-organizing network for unsupervised  and supervised learning," International Computer Science Institute, TR-93-026,  Berkeley.  Fritzke,  B. [1994], "Making hard problems linearly separable -incremental radial basis  function approaches," (submitted to ICANN'9d: International Con]erence on Ar-  tificial Neural Networks), Sorrento, Italy.  Lung, K. J. & M. J. Witbrock [1989], "Learning to tell two spirals apart," in Proceedings  of the 1988 Connectionist Models Summer School, D. Touretzky, G. Hinton & T.  Sejnowski, eds., Morgan Kaufmann, San Mateo, 52-59.  Martinetz, T. M. & K. J. Schulten [1991], "A "neural-gas" network learns topologies," in  Artiticial NeurM Networks, T. Kohonen, K. M/kisara, O. Simula & J. Kangas,  eds., North-Holland, Amsterdam, 397-402.  Moody, J. & C. Darken [1989], "Learning with Localized Receptive Fields," in Proceedings  of the 1988 Connectionist Models Summer School, D. Touretzky, G. Hinton & T.  Sejnowski, eds., Morgan Kaufmann, San Mateo, 133-143.  Platt, J. C. [1991], "A Resource-Allocating Network for Function Interpolation," NeurM  Computation 3,213-225.  Poggio, T. & F. Girosi [1990], "Regularization Algorithms for Learning That Are Equiva-  lent to Multilayer Networks," Science 247, 978-982.  Robinson, A. J. [1989], "Dynamic Error Propagation Networks," Cambridge University,  PhD Thesis, Cambridge.  
Bayesian Self-Organization  Alan L. Yuille  Division of Applied Sciences  Harvard University  Cambridge, MA 02138  Stelios M. Smirnakis  Lyman Laboratory of Physics  Harvard University  Cambridge, MA 02138  Lei Xu *  Dept. of Computer Science  HSH ENG BLDG, Room 1006  The Chinese University of Hong Kong  Shatin, NT  Hong Kong  Abstract  Recent work by Becker and Hinton (Becker and Hinton, 1992)  shows a promising mechanism, based on maximizing mutual in-  formation assuming spatial coherence, by which a system can self-  organize itself to learn visual abilities such as binocular stereo. We  introduce a more general criterion, based on Bayesian probability  theory, and thereby demonstrate a connection to Bayesian theo-  ries of visual perception and to other organization principles for  early vision (Atick and Redlich, 1990). Methods for implementa-  tion using variants of stochastic learning are described and, for the  special case of linear filtering, we derive an analytic expression for  the output.  1 Introduction  The input intensity patterns received by the human visual system are typically  complicated functions of the object surfaces and light sources in the world. It  * Lei Xu was a research scholar in the Division of Applied Sciences at Harvard University  while this work was performed.  lOOl  1002 Yuille, Smirnakis, and Xu  seems probable, however, that humans perceive the world in terms of surfaces and  objects (Nakayama and Shimojo, 1992). Thus the visual system must be able to  extract information from the input intensities that is relatively independent of the  actual intensity values. Such abilities may not be present at birth and hence must  be learned. It seems, for example, that binocular stereo develops at about the age  of two to three months (Held, 1987).  Becker and Hinton (Becker and Hinton, 1992) describe an interesting mechanism  for self-organizing a system to achieve this. The basic idea is to assume spatial  coherence of the structure [o be extracted and to train a neural network by maxi-  mizing the mutual information between neurons with disjoint receptive fields. For  binocular stereo, for example, the surface being viewed is assumed flat (see (Becker  and Hinton, 1992) for generalizations of this assumption) and hence has spatially  constant disparity. The intensity patterns, however, do not have any simple spatial  behaviour. Adjusting the synaptic strengths of the network to maximize the mutual  information between neurons with non-overlapping receptive fields, for an ensem-  ble of images, causes the neurons to extract features that are spatially coherent -  thereby obtaining the disparity [fig. l].  maximize I (a;b)  0 1 1 1  1 0 1 1  0 0 I 1  1 0 0 1  Figure 1: In Hinton and Becker's initial scheme (Becker and Hinton, 1992), max-  imization of mutual information between neurons with spatially disjoint receptive  fields leads to disparity tuning, provided they train on spatially coherent patterns  (i.e. those for which disparity changes slowly with spatial position)  Workers in computer vision face a similar problem of estimating the properties of  objects in the world from intensity images. It is commonly stated that vision is ill-  posed (Poggio et al, 1985) and that prior assumptions about the world are needed  to obtain a unique perception. It is convenient to formulate such assumptions by  the use of Bayes' theorem P(SID) = P(DIS)P(S)/P(D). This relates the proba-  Bayesian Self-Organization 1003  bility P(SID) of the scene $ given the data D to the prior probability of the scene  P(S) and the imaging model P(DIS ) (P(D) can be interpreted as a normalization  constant). Thus a vision theorist (see (Clark and Yuille, 1990), for example) deter-  mines an imaging model P(DIS), picks a set of plausible prior assumptions about  the world P(S) (such as natural constraints (Marr, 1982)), applies Bayes' theorem,  and then picks an interpretation $* from some statistical estimator of P(S[D) (for  example, the maximum a postertort (MAP) estimator S* = ARG{MAXsP(SI D) }.)  An advantage of the Bayesian approach is that, by nature of its probabilistic formu-  lation, it can be readily related to learning with a teacher (Kersten et al, 1987). It is  unclear, however, whether such a teacher will always be available. Moreover, from  Becker and Hinton's work on self-organization, it seems that a teacher is not always  necessary. This paper proposes a way for generalizing the self-organization ap-  proach, by starting from a Bayesian perspective, and thereby relating it to Bayesian  theories of vision. The key idea is to force the activity distribution of the outputs to  be close to a pre-specified prior distribution Pp(S). We argue that this approach is  in the same spirit as (Becker and Hinton, 1992), because we can choose the prior dis-  tribution to enforce spatial coherence, but it is also more general since many other  choices of the prior are possible. It also has some relation to the work performed by  Atick and Redlich (Atick and Redlich, 1990) for modelling the early visual system.  We will take the viewpoint that the prior Pp(S) is assumed known in advance by  the visual system (perhaps by being specified genetically) and will act as a self-  organizing principle. Later we will discuss ways that this might be relaxed.  2 Theory  We assume that the input D is a function of a signal Y, that the system wants  to determine and a distractor N [fig.2]. For example E might correspond to the  disparities of a pair of binocular stereo images and N to the intensity patterns. The  distribution of the inputs is PD(D) and the system assumes that the signal E has  distribution Pp (E).  Let the output of the system be S - C(D,'7) where C is a function of a set  of parameters '7 to be determined. For example, the function C(D,7) could be  represented by a multi-layer perceptron with the -7's being the synaptic weights.  By approximation theory, it can be shown that a large variety of neural networks  can approximate any input-output function arbitrarily well given enough hidden  nodes (Hornik et al, 1989).  The aim of self-organizing the network is to ensure that the parameters '7 are chosen  so that the outputs $ are as close to the E as possible. We claim that this can be  achieved by adjusting the parameters '7 so as to make the derived distribution of the  outputs Poo(S : 7) = f 5(S - G(D, 7))Po(D)[dD] as close as possible to Pp(S).  This can be seen to be a consistency condition for a Bayesian theory as from Bayes  formula we obtain the equation:  P(S]D)Pr>(D)[dD] = / P(DIS)Pp(S)[dD] = Pp(S). (1)  1004 Yuille, Smirnakis, and Xu  which is equivalent to our condition, provided we choose to identify P(SID) with  6(S- G(D,')')).  To make this more precise we must define a measure of similarity between the two  distributions Pp(S) and PDD(S'')'). An attractive measure is the Kullback-Leibler  distance (the entropy of PD relative to Pp):  log  Pp(s) [dS].  (2)  D=F(E,N)  g(z)  S=G(D, 7)  v ?o.(s.r)  Pot> ( S : 7)log(  ?.. (s : r).)a s  Figure 2: The parameters 7 are adjusted to minim,ze the Kullback-Leibler dis-  tance between the prior (Pp) distribution of the true signal (E) and the derived  distribution (PDD) of the network output (S).  This measure can be divided into two parts: (i) -f PDD(S'')')Iog Pr(S)[dS] and  (ii) f Poo(S: 7)log Poo(S'7)[dS]. The second term encourages variability of the  output while the first term forces similarity to the prior distribution.  Suppose that Pp(S) can be expressed as a Markov random field (i.e. the spatial  distribution of Pp($) has a local neighbourhood structure, as is commonly assumed  in Bayesian models of vision). Then, by the Hammersely-Clifford theorem, we can  write Pt(S) = e-tz,(S)/z where Er(S ) is an energy function with local connections  (for example, Ep(S) = Ei($i - Si+l?),  is an inverse temperature and Z is a  normalization constant.  Then the first term can be written (Yuille et al, 1992) as  - f Poo(S' 7)log Pp(S)[dS] = i(Er(G(D , ')')))o + log Z.  Bayesian Self-Organization 1005  We can ignore the logZ term since it is a constant (independent of 7). Mini-  mizing the first term with respect to '7 will therefore try to minimize the energy  of the outputs averaged over the inputs- {Ep(G(D, 7))}D - which is highly desir-  able (since it has a close connection to the minimal energy principles in (Poggio  et al, 1985, Clark and Yuille, 1990)). It is also important, however, to avoid the  trivial solution G(D, 7) - 0 as well as solutions for which G(D, 7) is very small  for most inputs. Fortunately these solutions are discouraged by the second term:  f PDD(D, ,7)log PD(D, ,7)[dD], which corresponds to the negative entropy of the  derived distribution of the network output. Thus, its minimization with respect to  ,7 is a maximum entropy principle which will encourage variability in the outputs  G(D, ,7) and hence prevent the trivial solutions.  3 Reformulating for Implementation.  Our theory requires us to minimize the Kullback-Leibler distance, equation 2, with  respect to ,7. We now describe two ways in which this could be implemented using  variants of stochastic learning. First observe that by substituting the form of the  derived distribution into equation 2 and integrating out the $ variable we obtain:  f ,7)  KL(,7) = PD(D)log Pp(G(D,,7))  (4)  Assuming a representative sample { D":/ e A} of inputs we can approximate KL(,7)  by -],^ log[Pt)t)(G(D", ,7): ,7)/Pp(G(D", ,7))]. We can now, in principle, perform  stochastic learning using backpropagation: pick inputs D, at random and update  the weights '7 using log[PDD(G(D , ,7): ,7)/Pp(G( D", ,7))] as the error function.  To do this, however, we need expressions for PDD(G(D",,7) : ,7) and its deriva-  tive with repect to ,7- If the function G(D,,7) can be restricted to being 1-1 (in-  creasing the dimensionality of the output space if necessary) then we can obtain  (Yuille et al, 1992) analytic expressions Pt)t)(G(D, ,7): ,7) - Pt)(D)/] det(OG/OD)l  and ((1OgPDD(G(D,,7): ,7)/C'},7) =-((G/(D)-(c2G/cD(,7), where [-1] denotes  the matrix inverse. Alternatively we can perform additional sampling to estimate  PDD(G(D, ,7): ,7) and ((logPDD(G(D, ,7): ,7)/c,7) directly from their integral rep-  resentations. (This second approach is similar to (Becker and Hinton, 1992) though  they are only concerned with estimating the first and second moments of these  distributions.)  4 Connection to Becker and Hinton.  The Becker and Hinton method (Becker and Hinton, 1992) involves maximizing the  mutual information between the output of two neuronal units $, $2 [fig.l]. This is  given by:  I(S, $2): - < log ?DD($t)  where the first two terms correspond to maximizing the entropies of  while the last term forces  1006 Yuille, Smirnakis, and Xu  By contrast, our version tries to minimize the quantity   < log ?z>z>(&, s2) > - f logPp(S,S2)Pz>z>(S,S2)[d].  If we then ensure that Pp(S, S2) = 5(S - S2) our second term will force S  S2  and our first term will maximize the entropy of the joint distribution of S, S2. We  argue that this is effectively the same as (Becker and Hinton, 1992) since maxi-  mizing the joint entropy of $, $2 with $ constrained to equal $2 is equivalent to  maximizing the individual entropies of $ and $2 with the same constraint.  To be more concrete, we consider Becker and Hinton's implementation of the mutual  information maximization principle in the case of units with continuous outputs.  They assume that the outputs of units 1,2 are Gaussian  and perform steepest  descent to maximize the symmetrized form of the mutual information between $  and S2:  v(s) v(s2)  Ism;s2 = log V(S - S2) +log V(S - S2): log V(S)+log V(S2)-2 log V(S - S2)  (5)  where V() stands for variance over the set of inputs. They assume that the difference  between the two outputs can be expressed as uncorrelated additive noise, S =  S2 + N. We reformalize their criterion as maximizing EsH(V(S2), V(N)) where  E,H(V(S2),V(N))=log{V(S2)+ V(N)}+logV(S2)-21ogV(N). (6)  For our scheme we make similar assumptions about the distributions of S and  S2. We see that < logPDD(S,S2) >= -log{< S >< S 2 > - < SS2 >2} __  -log{V(S2)V(N)} (since < SS2 >:< (S2 + N)S2 >= V(S2) and < S 2 >=  V(S2) + V(N)). Using the prior distribution Pp(S,S2)   -r(s-s2) our criterion  corresponds to minimizing Ersx(V(S2), V(N)) where:  EYSX(V(S2), V(N)): -log V(S2)- log V(N)q- rV(N).  (7)  It is easy to see that maximizing EBn(V(S2), V(N)) will try to make V(S2) as  large as possible and force V(N) to zero (recall that, by definition, V(N) _> 0).  Minimizing our energy will try to make V($2) as large as possible and will force  V(N) to 1/r (recall that r appears as the inverse of the variance of a Gaussian  prior distribution for S - S2 so making r large will force the prior distribution to  approach 5($ - $2).) Thus, provided r is very large, our method will have the  same effect as Becker and Hinton's.  5 Application to Linear Filtering.  We now describe an analysis of these ideas for the case of linear filtering.  approach will be contrasted with the traditional Wiener filter approach.  We assume for simplicity that these Gaussians have zero mean.  Our  Bayesian Self-Organization 1007  Consider a process of the form D(): E()+N() where D() denotes the input to  the system, E() is the true signal which we would like to predict, and N() is the  noise corrupting the signal. The resulting Wiener filter Aw () has fourier transform  fiw - s,z/(z,z + N,N) where z,z and N,N are the power spectrum of the  signal and the noise respectively.  By contrast, let us extract a linear filter Ab by applying our criterion. In the case  that the noise and signal are independent zero mean Gaussian distributions this  filter can be calculated explicitly (Yuille et al, 1992). It has fourier transform with  squared magnitude given by ]fibl 2 - s,s/(s,s + Ndv). Thus our filter can be  thought of as the square root of the Wiener filter.  It is important to realize that although our derivation assumed additive Gaussian  noise our system would not need to make any assumptions about the noise distribu-  tion. Instead our system would merely need to assume that the filter was linear and  then would automatically obtain the "correct" result for the additive Gaussian noise  case. We conjecture that the system might detect non-Gauusian noise by finding it  impossible to get zero Kullback-Liebler distance with the linear ansatz.  6 Conclusion  The goal of this paper was to introduce a Bayesian approach to self-organization  using prior assumptions about the signal as an organizing principle. We argued that  it was a natural generalization of the criterion of maximizing mutual information  assuming spatial coherence (Becker and Hinton, 1992). Using our principle it should  be possible to self-organize Bayesian theories of vision, assuming that the priors  are known, the network is capable of representing the appropriate functions and  the learning algorithm converges. There will also be problems if the probability  distributions of the true signal and the distractor are too similar.  If the prior is not correct then it may be possible to detect this by evaluating  the goodness of the Kullback-Leibler fit after learning 2. This suggests a strategy  whereby the system increases the complexity of the priors until the Kullback-Leibler  fit is sufficiently good (this is somewhat similar to an idea proposed by Mumford  (Mumford, 1992)). This is related to the idea of competitive priors in vision (Clark  and Yuille, 1990). One way to implement this would be for the prior probability  itself to have a set of adjustable parameters that would enable it to adapt to different  classes of scenes. We are currently (Yuille et al, 1992) investigating this idea and  exploring its relationships to Hidden Markov Models.  Ways to implement the theory, using variants of stochastic learning, were described.  We sketched the relation to Becker and Hinton.  As an illustration of our approach we derived the filter that our criterion would give  for filtering out additive Gaussian noise (possibly the only analytically tractable  case). This had a very interesting relation to the standard Wiener filter.  2This is reminiscent of Barlow's suspicious coincidence detectors (Barlow, 1993), where  we might hope to determine if two variables x & y are independent or not by calculating  the Kullback-Leibler distance between the joint distribution P(x,y) and the product of  the individual distributions P(x)P(y).  1008 Yuille, Smimakis, and Xu  Acknowledgement s  We would like to thank DARPA for an Air Force contract F49620-92-J-0466. Con-  versations with Dan Kersten and David Mumford were highly appreciated.  References  J.J. Atick and A.N. Redlich. "Towards a Theory of Early Visual Processing".  Neural Computation. Vol. 2, No. 3, pp 308-320. Fall. 1990.  H.B. Barlow. "What is the Computational Goal of the Neocortex?" To appear in:  Large scale neuronal theories of the brain. Ed. C. Koch. MIT Press. 1993.  S. Becker and G.E. Hinton. "Self-organizing neural network that discovers surfaces  in random-dot stereograms". Nature, Vol 355. pp 161-163. Jan. 1992.  J.J. Clark and A.L. Yuille. Data Fusion for Sensory Information Processing  Systems. Kluwer Academic Press. Boston/Dordrecht/London. 1990.  R. Held. "Visual development in infants". In The encyclopedia of neurosclence,  vol. 2. Boston: Birkhauser. 1987.  K. Hornik, S. Stinchocombe and H. White. "Multilayer feed-forward networks are  universal approximators". Neural Networks 4, pp 251-257. 1991.  D. Kersten, A.J. O'Toole, M.E. Sereno, D.C. Knill and J.A. Anderson. "Associative  learning of scene parameters from images". Optical Society of America, Vol. 26,  No. 23, pp 4999-5006. 1 December, 1987.  D. Marr. Vision. W.H. Freeman and Company. San Francisco. 1982.  D. Mumford. "Pattern Theory: a unifying perspective". Dept. Mathematics  Preprint. Harvard University. 1992.  K. Nakayama and S. Shimojo. "Experiencing and Perceiving Visual Surfaces".  Science. Vol. 257, pp 1357-1363.4 September. 1992.  T. Poggio, V. Torre and C. Koch. "Computational vision and regularization the-  ory". Nature, 317, pp 314-319. 1985.  A.L. Yuille, S.M. Smirnakis and L. Xu. "Bayesian Self-Organization". Harvard  Robotics Laboratory Technical Report. 1992.  PART IX  SPEECH AND SIGNAL  PROCESSING  
Optimal Stopping and Effective Machine  Complexity in Learning  Changfeng Wang  Department of Systems Sci. and Eng.  University of Pennsylvania  Philadelphia, PA, U.S.A. 19104  Santosh S. Venkatesh  Department of Electrical Engineering  University of Pennsylvania  Philadelphia, PA, U.S.A. 19104  J. Stephen Judd  Siemens Corporate Research  755 College Rd. East,  Princeton, N J, U.S.A. 08540  Abstract  We study the problexn of when to stop learning a class of feedforward networks  - networks with linear outputs neuron and fixed input weights - when they are  trained with a gradient descent a.lgorithm on a finite number of examples. Under  general regularity conditions, it is shown that there are in general three distinct  phases in the generalization performance in the learning process, and in particular,  the network has better generalization performance when learning is stopped at a  certain time before the global minimum of the empirical error is reached. A notion  of effective size of a machine is defined and used to explain the trade-off between  the complexity of the machine and the training error in the learning process.  The study leads naturally to a network size selection criterion, which turns out to  be a generalization of Akaike's Infornmtion Criterion for the learning process. It is  shown tha. t stopping learning before the global minimum of the empirical error has  the effect of network size selection.  1 INTRODUCTION  The primary goal of learning in neural nets is to find a network that gives valid generalization. In  achieving this goal, a central issue is the trade-off between the training error and network cmnplexity.  This usually reduces to a problem of network size selection, which has drawn much research effort in  recent years. Various principles, theories, and intuitions, including Occam's razor, statistical model  selection criteria such as Akaike's Information Criterion (AIC) [1] and many others [5, 1, 10, 3, 11] all  quantitatively support the following PAC prescription: between two machines which have the same  mnpirical error, the machine with smaller VC-dimension generalizes better. However, it is noted  that these methods or criteria do not necessarily lead to optimal (or nearly optimal) generalization  performance. Furthermore, all of these methods are valid only at the global minimum of the enpirical  error function (e.g, the likelihood timetlon for AIC), and it is not clear by these methods how the  generalization error is effected by network complexity or, more generally, how a network generalizes  during the learning process. This paper addresses these issues.  303  304 Wang, Venkatesh, and Judd  Recently, it has often been observed that when a network is 'traiued by a gradient descent  algorithm, there exists a critical region in the training epochs where the trained network generalizes  best, and after that region the generalization error will increase (frequently called over-training). Our  numerical experiments with gradient-type algorithms in training feedforward networks also indicate  that in this critical region, as long as the network is large enough to learn the examples, the size  of the network plays little role in the (best) generalization performance of the network. Does this  mean we must revise Occam's principle? How should one define the complexity of a network and go  about tuning it to optinize generalization performance? When should one stop learning? Although  releva.nt learning processes were treated by numerous authors [2, 6, 7, 4], the formal theoretical  studies of these problexns are abeyant.  Under rather general regularity conditions (Section 1), we give in Section 2 a theorem which  relates the generalization error at each epoch of learning to that at the global minimum of the  training error. Its consequence is that for any linear machine whose VC-dimension is finite but large  enough to learn the target concept, the number of iterations needed for the best generalization to  occur is at the order of the logarithm of the sainple size, rather than at the global minimum of  the training error; it also provides bounds on the improvement expected. Section 3 deals with the  relation between the size of the machine and generalization error by appealing to the concept of  effective size. Section 4 concerns the application of these results to the problem of network size  selection, where the AIC is generalized to cover the time evolution of the learning process. Finally,  we conclude the paper with comments on practical implementation and further research in this  direction.  2 THE LEARNING MACHINE  The machine we consider accepts input vectors X from an arbitrary input space and produces scalar  outputs  d  Here, a* = (a*,..., a'a)' is a fixed vector of real weights, for each i, b,(X) is a fixed real hmction  of the inputs, with b(X) = ((X),..., (X))' the corresponding vector of fuuctions, and  is a  random noise term. The machine (1) can be thought of as a feedforward neural network with a fixed  front end and variable weights at the output. In particular, the functions Pi can represent fixed  polynomials (higher-order or sigma-pi neural networks), radial basis functions with fixed centers, a  fixed hidden-layer of sigmoidal neurons, or simply a linear map. In this context, N.J. Nilsson [8]  has called similar structures -machines.  We consider the problem of learning from examples a relationship between a random variable Y  and an n-dimensional random vector X. We assume that this function is given by (1) for some fixed  integer d, the random vector X and random variable  are defined on the same probability space,  that E [lX] -- 0, and a2(X) = Var(lX ) = constant < c almost surely. The smallest eigenvalue of  the matrix b(x)b(x) is assumed to be bounded from below by the inverse of some square integrable  finction.  Note that it can be shown that the VC-dinension of the class of -machines with d neurons  is d under the last assumption. The learning-theoretic properties of the system will be determined  largely by the eigen structure of . Accordingly, let A _> A2 _> '-  _> A denote the eigenvalues of .  The goal of the learning is to find the true concept a given independently drawn examples (X, y)  from (1). Given any hypothesis (vector) w = (w,...,w)' for consideration as an approximation  to the true concept a, the performance measure we use is the mean-square prediction (or ensemble)  error  = E (.v - (2)  Note that the true concept a* is the mean-square solution  r* = arg nin (w) = -'E (l,(X)y), (3)  Optimal Stopping and Effective Machine Complexity in Learning 305  and the minionurn prediction error is given by (c) = minw (w) = a s.  Let n be the number of samples of (X, ). We assulne that an independent, identically  distributed sample (X(),y()), ..., (X(n),y('O), generated according to the joint distribution  of (X,Y) induced by (1), is provided to the learner. To sinplify notation, define the matrix  62 _= [b(X ()) ... b(X("'))] and the corresponding vector of outputs y - (y(),...,y("))'. In  analogy with (2) define the empirical error on the sample by  Let & denote the hypothesis vector for ;vhich the empirical error on the sanple is minimized:  V(&) = 0. Analogously with (3) we can then show that  5=(kI")-y=-t (y),  (4)  where  = o' is the empirical covariance matrix, which is almost surely nonsingular for large n.  The terms in (4) are the empirical counterparts of the ensemble averages in (3).  The gradient descent algorithm is given by:  (5)  where a = (a,a2,...,a3)', t is the number of iterations, and e is the rate of learning. From this  we can get  c, = (I - A(t))& + A(t)c0, (6)  where A(t) = (I - eta) t, and (0 is the initial weight vector.  The limit of a is A when t goes to infinity, provided p is positive definite and the learning rate   is small enough (i.e., smaller than the smallest eigenvalue of $). This implies that the gradient  descent algorithm converges to the least squares solution, starting from any point in 7 '.  3 GENERALIZATION DYNAMICS AND STOPPING TIME  3.1 MAIN THEOREM OF GENERALIZATION DYNAMICS  Even if the true concept (i.e., the precise relation between Y and X in the current problem) is in the  class of models we consider, it is usually hopeless to find it using only a finite number of examples,  except in some trivial cases. Our goal is hence less ambitious; we seek to find the best approximation  of the true concept, the approach entailing a minimization of the training or empirical error, and  then taking the global minimum of the empirical error & as the approximation. As we have seen the  procedure is unbiased and consistent. Does this then imply that training should always be carried  out to the limit? Surprisingly, the answer is no. This assertion follows from the next theorem.  Theorem 3.1 Let M > 0 be an arbitrary real constant (possibly depending on n), and suppose  assumptions AI to A3 are satisfied; then the generalization dynamics in the training process are  governed by the followin.q equation:  71,  n  uniformly for all initial weight vectors, ao in the d-dimensional ball {(* + 5: 11611 _< M, 6   and for all t > O. []  306 Wang, Venkatesh, and Judd  3.2 THREE PHASES IN GENERALIZATION  By Theorem 3.1, the mean generalization error at each epoch of the training process is characterized  by the following function:  d  - -;- - .  The analysis of the evoution of generalization with training is facilitated by treating q(.) as a  function of a continuous time parame[er t. Ve will show that, there are three distinct phases iu  generalization dynanics. These results are given in the following in form by several corollaries of  Theerein 3.1.  Without loss of generality, we assume the initial weight vector is picked up in a region with  hq/[-x'd)t then for all 0 < t < t   II*l 5 M,, = 0(,,), and in particular, I1 = 0(."). Lot r,: ,,,, , _  2h,(/[-x,,]), we have 0  r < , and thus  d  d 1 n  The quantity (1 - i) TM in the first term of the above inequalities is related to the elimination of  initial error, and can be defined a.s the approximation error (or fitting error); the last term is related  to the effective complexity of the network at t (in hct, an order O() shift of the complexity error).  The definition and observatious here will be discussed in more detail iu the uext section.  We call the learning process during the time interval 0 < t < tl the first phase of learning.  Since in this interval d(t) = O(,-2"' ) is a monotouica.lly decreasing function of t, tile generalizatiou  error decreases mouotonically in the first phase of learning. At the end of first phase of learning  d(t) = O(), therefore the generalization error is (ct ) = (ro) + O(). As a summary of these  statements we have the following corollary.  Corollary 3.2 In the first phase of learning, the complexity error is dominated by the approximation  error, and within an order of 0(, ), the generalization error decreases monotonically in the learning  process to + 0(,{) at the end of firt phase. []  For t > tl, we can show by Theorem 3.1 titat the generalization dynanics is given by tile following  equation, where &, = c(t ) -  c;(t,+t)=w(o) 2a2 y(1- eAi)t 1-(l+pi)(1-eXi)  where p _= A*2(t )n/a 2, which is, with probability approaching one, of order O(n).  Without causing confusion, we still use q(.) for the new time-varying part of the generalizatkm  error. The function q(.) has much more complex behavior after tl than in the first phase of learning.  As we will see, it decreases for some time, and finally begins to increase again. In particular, we  found the best generalization at that t where b(t) is ninimized. (It is noted that 5t is a random  variable now, and the following statements of the generalization dynamics are in the sense of with  probability approaching one as n  Define the optimal stopping tinto: t,nin ---- a, rgmin{S(ct) : t  [0, o]}, i.e., tile epoch corre-  sponding to the smallest generalization error. Then we can prove the followiug corollaries:  Corollary 3.3 The optimal stoppin.q time tmi,, = O(lnn), provided a 2 > O. In particular, the  following inequalities hold:  I,(t +p 2 ) t.(+p)  1. t t _< tmi n _< tu; where t -- ti + rain, m(1/[ -:,]) and tu --- tl + nax hq/D-,]) are both  finite real numbers. That is, the smallest generalization occurs before the global minimum  of the empirical error is reached.  Optimal Stopping and Effective Machine Complexity in Learning 307  2. (.) (tracking the generalization error) decreases monotonically for t < te and increases  monotonically to zero for t > tu; furthermore, tmin is unique if tt + In 2  h,(/[-A]) -->  In accordance with our earlier definitions, we call the learning process during the tiine interval  between tl and t, the second phase of learning; and the rest of tine the third phase of learning.  According to Corollary 3.3, for t > t,, sufficiently large, the generalization error is uniformly  better than at the glohal minimum, &, of the empirical error, although lninimum generalization error  is achieved between te and t,. The generalization error is reduced by at least  over that for & if we stop training at, a proper tine. For a fixed nmnber of learning examples,  the larger is the ratio d/,, the larger is the improvmnent in generalization error if the algorithn is  stopped before the global ninimum rt* is reached.  4 THE EFFECTIVE SIZE OF THE MACHINE  Our concentration on dynmnics and our seenting disregard for complexity do not conflict with the  learning-theoretic focus on VC-dimension; in fact, the two attitudes fit nicely together. This section  explains the generalization dynanics by introducing the the concept of effective complexity of the  machine. It is argued that early stopping in effect sets the effective size of the network to a value  smaller than its VC-dimension.  d  The effective size of the machine at time t is defined to be d(t) = ]]],= [1 - (1 - ,,)t], which  increases monotonically to d, the VC-dinension of the network, as t --4 c. This definition is justified  after the following theorem:  Theorem 4.1  all o such that )51 <_ M,  = e(.*, t) + + o(  n IT   .  Under the assumptions of Theorem 3.1, the following equation holds uniformly for  (7)  In the limit of learning, we have by letting t --} o in the above equation,  (a): (,*) + d + O(n-) (8)  n  Hence, to an order of O(n-'s), the generalization error at the limit of training breaks into two parts:  the approximation error (a*), and the complexity error rr 2. Clearly, the latter is proportional to  d, the VC-dimension of the network. For all d's larger than necessary, (a*) remains a constant,  and the generalization error is determined solely by _a The term (a* t) differs from (a*) only in  terms of initial error, and is identified to be the approximation error at t. Comparison of the above  two equations thus shows titat it is reasonable to define d(t) as the complexity error at t, and  justifies the definition of d(t) as the effective size of the machine at the same time. The quantity  d(t) captures the notion of the degree to which the capacity of the machine is used at t. It depends  on the machine parameters, the algoritlnn being used, and the marginal distribution of X. Thus, we  see from (7) that the generalization error at epoch t falls into the same two parts as it does at the  linit: the approximation error (fitting error) and the complexity error (determined by the effective  size of the machine).  As we have show in the last section, during the first phase of learning, the complexity error is of  higher order in n compared to the fitting error during the first phase of learning, if the initial error  is of order O(n ) or larger. Thus decrease of the fitting error (which is proportional to the training  error, as we will see in the next section) inplies the decrease of the generalization error. However,  308 Wang, Venkatesh, and Judd  when the fitting error is brought down to the order O(,-!/), the decreds& of fitting error will no longer  imply the decrease of the generalization error. In fact, by the above theorton, the generalization  error at t + t can be written as  d  c(.,,,) = c(,-,*) +  x,,,(t,)( -x,) TM  2  + --,.(t) + o(,,.-  ).  The fitting error and the complexity error compete at order O(&) during tile second phase of learning.  After the second the phase of learning, the complexity error dominates the fitting error, still at the  order of O(,{). Furthermore, if we define , -- i [-' p]*, the,, by the above equation and (3.3),  we have  Corollary 4.2 At the optimal stopping time, the following upper bound on the generalization error  holds,  2  c(.,,,,,,,) _< (.) + --(1 - )d + 0(..-).  Since n is a quantity of order O(n), (1 - n)d is strictly snaller thau d. Thus stopping training at  tml, has the same effect as using a snaller nachine of size less than (1 - n)d and carrying training  out to the limit! A more detailed analysis reveals how the effective size of the machine is affected  by each neuron in the learning process (onfitted due to the space limit).  REMARK: The concept of effective size of the machine can be defined sinilarly for an arbitrary  starting point. However, to compare the degree to which the capacity of the machine has been used  at t, one must specify at what distance between the hypothesis a and the truth t* is such compari-  son started. While each point in tile d-dimensional Euclidean space can be regarded as a hypothesis  (machine) about t*, it is intuitively clear that each of these machines has a different capacity to  approximate it. But it is reasonable to think that all of the machines that are ou the same sphere  {t: [t - t*[ = r}, for each r > 0, have the same capacity in approximating a*. Thus, to compare  the capacity being used at t, we must specify a specific sphere as the starting point; defining the  effective size of the machine at t without specifying the starting sphere is clearly meaningless. As  we have seen, r   is found to be a good choice for our purposes.  5 NETWORK SIZE SELECTION  The next theorem relates the generalization error and training error at each epoch of learning, and  forms the basis for choosing the optimal stopping time as well as the best size of the machine during  the learning process. In the limit of the learning process, the criteriou reduces to the well-known  Akaike Information Criterion (AIC) for statistical model selection. Comparison of the two criteria  reveals that our criterion will result in better generalization than AIC, since it incorporates the  information of each individual neuron rather than just the total number of neurons as in the AIC.  Theorem 5.1 Assuming the learning algorithm converges, and the conditions of Theorem 3.1 are  satisfied; then the following equation holds.'  (c,) = (1 + o(1))E,,(c,) + c(d,t) + o(-) (9)  ,,,her 4,t, t) = 2-5,:-E,C, li -( ---x,)'l cn  According to this theorem, we find all asymptotically unbiased estimate of (tt) to be ,(ct)+  C(d, t) when a 2 is known. This results in the following criterion for finding the optional stopping  time and network size:  min{.(ct) + C(d,t): d,t = 1,2,...} (10)  Optimal Stopping and Effective Machine Complexity in Learning 309  When t goes to infinity, the above criterion becomes:  22d  min{,,(&) + --: d = 1,2,...} (11)  n  which is the AIC for choosing the best size of networks. Therefore, (10) can be viewed as an  extension of the AIC to the learning process. To understand the differences, consider the case  when , has standard normal distribution N(0, a2). Under this assumption, the Maxinum Likelihood  (ML) estimation of the weight vectors is the same  the Mean Square estimation. The AIC was  togy ()  obtained by minimizing E , the Kullback-Leibler distance of the density function f(X)  with aMr being the ML estimation of  and that of the true density f. This is equivalent to  minimizinglim,E(Y f(X)) 2 E(Y  2  -- = -- faL (X)) (suming the limit and the expectation  are interchangeable). Now it is clear that while AIC chooses networks only at the limit of learning,  (10) does this in the whole learning process. Observe that the matrix  is now exactly the Fisher  Information Matrix of the density function fo(X), and Ai is a meure of the capacity of i in  fitting the relation between X and Y. Therefore our criterion incorporates the information about  each specific neuron provided by the Fisher Information Matrix, which is a meure of how well  the data fit the model. This implies that there are two pects in finding the trade-off between the  model complexity and the empirical error in order to minimize the generalization error: one is to  have the smallest number of neurons and the other is to minimize the utilization of each neuron.  The AIC (and in fact most statistical model selection criteria) are aimed at the former, while our  criterion incorporates the two aspects at the same time. We have seen in the earlier discussions that  for a given number of neurons, this is done by using the capacity of each neuron in fitting the data  only to the degree I - (1 - eA) t'"" rather than to its limit.  6 CONCLUDING REMARKS  To the best of our knowledge, the results described in this paper provide for the first time a precise  language to describe overtraining phenomena in learning machines such as neural networks. We  have studied formally the generalization process of a linear nachine when it is trained with a  gradient descent algorithm. The concept of effective size of a machine was introduced to break the  generalization error into two parts: the approximation error and the error caused by a complexity  term which is proportional to effective size; the former decreases monotonically and the later increases  monotonically in the learning process. When the machine is trained on a finite number of examples,  there are in general three distinct phases of learning according to the relative magnitude of the  fitting and complexity errors. In particular, there exists an optimal stopping time t,i, = O(lnn)  for minimizing generalization error which occurs before the global minionurn of the empirical error  is reached. These results lead to a generalization of the AIC in which the effect of certain network  parameters and time of learning are together taken into account in the network size selection process.  For practical application of neural networks, these results demoustrate that training a network  to its limits is not desirable. From the learning-theoretic point of view, the concept of effective  dimension of a network tells us that we need more than the VC-dimensiou of a machine to describe  the generalization properties of a machine, except in the limit of learning.  The generalization of the AIC reveals some unknown facts in statistiral model selection theory:  namely, the generalization error of a network is affected not only by the number of parameters  but also by the degree to which each parmneter is actually used in tile leariling process. Occam's  principle therefore stands in a subtler forn: Make minimal use of the capacity of a network for  encoding the information provided by learning samples.  Our results hold for weaker assumptions than were made herein about the distributions of X  and ,. The case of machines that have vector (rather than scalar) outputs is a simple generalization.  Also, our theorems have recently been generalized to the case of general nonlinear machines and are  not restricted to the squared error loss hmction.  While the problem of inferring a rule from the observational data has been studied for a long  time in learning theory as well as in other context such as in Linear and Nonlinear Regression, the  310 Wang, Venkatesh, and Judd  study of the problem as a dynamical process seems to open a flew avenue for looking at the problem.  Many problems are open. For example, it is interesting to know what could be learned from a finite  number of examples in a finite number of iterations in the case where the size of the machine is not  small compared to the sample size.  Acknowledgments  C. Wang thanks Siemens Corporate Research for support during the summer of 1992 when this  research was initiated. The work of C. Wang and S. Venkatesh has been supported in part by tile  Air Force Office of Scientific Research nnder grant F49620-93-1-0120.  References  [1] Akaike, H. (1974) Information theory and an extension of the naximum likelihood principle.  Second International Symposium on Information Theory, Ed. B.N. Krishnaiah, North Holland,  Amsterdam, 27-41.  [2] Baldi, P. and Y. Chauvin (1991) Temporal evolution of generalization during learning in linear  networks. Neural Communication. 3,589-603.  [3] Chow, G. C. (1981) A comparison of the information and posterior probability criteria for model  selection. Journal of Econometrics 16, 21-34.  [4] Hansen, Lars Kai (1993) Stochastic linear learning: exact test and training error averages.  Neural Networks, 4,393-396.  [5] Haussler, D. (1989) Decision theoretical generalization of the PAC model for neural networks  and other learuing applications. Preprint.  [6] Heskes, Tom M. and Bert Kappen (1991) Learning processes in neural networks. Physical Review  A, Vol 44, No. 4, 2718- 2726.  [7] Kroght, Anders and John A. Hefts Generalization in a linear perceptron in the presence of  noise. Preprint.  [8] Nilsson, N.J. Learning Machines. New York: McGraw Hill.  [9] Pinelis, I., and S. Utev (1984) Estimates of noments of sums of independent random variables.  Theory of Probability and Its Applications. 29 (1984) 574-577.  [10] Rissanen, J. (1987) Stochastic complexity. J. Royal Statistical Society, Series B, Vol. 49, No. 3,  223-265.  [11] Schwartz, G. (1978) Estinating tile dimension of a nodel. Annals of Statistics 6, 461-464.  [12] Sazonov, V. (1982). On the accuracy of normal approximation. Journal of multiva?ate analysis.  12, 371-384.  [13] Senatov, V. (1980) Uniform estimates of the rate of convergence in the multi-dimensional central  limit theorem. Theory of Probability and Its Applications. 25 (1980) 745-758.  [14] Vapnik, V. (1992) Measuring the capacity of learning machines (I). Preprint.  [15] Weigend, S.A. and Rumelhart (1991). Generalization through minimal networks with applica-  tion to forcasting. INTERFA CE'91-23rd Symposium on the Interface: Computing Science and  Statistics, ed. E. M., Keramidas, pp362-370. Interface Foundation of North America.  
Amplifying and Linearizing Apical  Synaptic Inputs  to Cortical Pyramidal Cells.  jvind Bernander, Christof Koch *  Computation and Neural Systems Program,  California Institute of Technology, 139-74  Pasadena, Ca 91125, USA.  Rodney J. Douglas  Anatomical Neuropharmacology Unit,  Dept. Pharmacology,  Oxford, UK.  Abstract  Intradendritic electrophysiological recordings reveal a bewildering  repertoire of complex electrical spikes and plateaus that are dif-  ficult to reconcile with conventional notions of neuronal function.  In this paper we argue that such dendritic events are just an exu-  berant expression of a more important mechanism - a proportional  current amplifier whose primary task is to offset electrotonic losses.  Using the example of functionally important synaptic inputs to the  superficial layers of an anatomically and electrophysiologically re-  constructed layer 5 pyramidal neuron, we derive and simulate the  properties of conductances that linearize and amplify distal synap-  tic input current in a graded manner. The amplification depends  on a potassium conductance in the apical tuft and calcium conduc-  tances in the apical trunk.  *To whom all correspondence should be addressed.  519  520 Bernander, Koch, and Douglas  1 INTRODUCTION  About half the pyramidal neurons in layer 5 of neocortex have long apical dendrites  that arborize extensively in layers 1-3. There the dendrites receive synaptic input  from the inter-areal feedback projections (Felleman and van Essen, 1991) that play  an important role in many models of brain function (Rockland and Virga, 1989).  At first sight this seems to be an unsatisfactory arrangement. In light of traditional  passive models of dendritic function the distant inputs cannot have a significant  effect on the output discharge of the pyramidal cell. The distal inputs are at least  one to two space constants removed from the soma in layer 5 and so only a small  fraction of the voltage signal will reach there. Nevertheless, experiments in cortical  slices have shown that synapses located in even the most superficial cortical layers  can provide excitation strong enough to elicit action potentials in the somata of  layer 5 pyramidal cells (Cauller and Connors, 1992, 1994). These results suggest  that the apical dendrites are active rather than passive, and able to amplify the  signal en route to the soma. Indeed, electrophysiological recordings from cortical  pyramidal cells provide ample evidence for a variety of voltage-dependent dendritic  conductances that could perform such amplification (Spencer and Kandel, 1961;  Regehr et al., 1993; Yuste and Tank, 1993; Pockberger, 1991; Amitai et al., 1993;  Kim and Connors, 1993).  Although the available experimental data on the various active conductances pro-  vide direct support for amplification, they are not adequate to specify the mecha-  nism by which it occurs. Consequently, notions of dendritic amplification have been  informal, usually favoring voltage gain, and mechanisms that have a binary (high  gain) quality. In this paper, we formalize what conductance properties are required  for a current amplifier, and derive the required form of their voltage dependency by  analysis.  We propose that current amplification depends on two active conductances: a  voltage-dependent K + conductance, gK, in the superficial part of the dendritic  tree that linearizes synaptic input, and a voltage-dependent Ca 2+ conductance,  gca, in layer 4 that amplifies the result of the linearization stage. Spencer and Kan-  del (1961) hypothesized the presence of dendritic calcium channels that amplify  distal inputs. More recently, a modeling study of a cerebellar Purkinje cell suggests  that dendritic calcium counteracts attenuation of distal inputs so that the somatic  response is independent of synaptic location (De Schutter and Bower, 1992). A  gain-control mechanism involving both potassium and calcium has also been pro-  posed in locust non-spiking interneurons (Laurent, 1993). In these cells, the two  conductances counteract the nonlinearity of graded transmitter release, so that the  output of the interneuron was independent of its membrane voltage. The principle  that we used can be explained with the help of a highly simplified three compart-  ment model (Fig. 1A). The leftmost node represents the soma and is clamped to  -50 mV. The justification for this is that the time-averaged somatic voltage is re-  markably constant and close to -50 mV for a wide range of spike rates. The middle  node represents the apical trunk containing gca, and the rightmost node represents  the apical tuft with a synaptic induced conductance change gsyn in parallel with  gK. For simplicity we assume that the model is in steady-state, and has an infinite  membrane resistance,  Amplifying and Linearizing Apical Synaptic Inputs to Cortical Pyramidal Cells 521  Isoma  g  K syn  Vsoma  ECa  EK  1- Esyn  B Passive response and targets C Activation curves  Linearized  100 200  gsyn (nS)  3O   2O  10  o  -50  o lO  v (ray)  Figure 1: Simplified model used to demonstrate the concepts of saturation,  linearization, and amplification. (A) Circuit diagram. The somatic compart-  ment was clamped to Voma = -50 rnV with Eca= 115 mV, EK = --95 rnV,  Eu = 0 rnV, and g - 40 nS. The membrane capacitance was ignored, since  only steady state properties were studied, and membrane leak was not included for  simplicity. (B) Somatic current, Ioma, in response to synaptic input. The pas-  sive response (thin dashed line) is sublinear and saturates for low values of g,u.  The linearized response (thick solid line) is obtained by introducing an inactivating  potassium conductance, GK ("gA" in c). A persistent persistent GK results in a  somewhat sub-linear response (thick dashed line; "gM" in c). The addition of a cal-  cium conductance amplifies the response (thin solid line). (C) Analytically derived  activation curves. The inactivating potassium conductance ("IA") was derived, but  the persistent version ("IM") proved to be more stable.  522 Bernander, Koch, and Douglas  2 RESULTS  Fig. lB shows the computed rela{onship between the excitatory synaptic input  conductance and the axial current, Io,a, flowing into the somatic (leftmost) com-  partment. The synaptic input rapidly saturates; increasing g,yn beyond about 50 nS  leads to little further increase in Iso,a. This saturation is due to the EPSP in the  distal compartment reducing the effective synaptic driving potential. We propose  that the first goal of dendritic amplification is to linearize this relationship, so that  the soma is more sensitive to the exact amount of excitatory input impinging on  the apical tuft, by introducing a potassium conductance that provides a hyper-  polarizing current in proportion to the degree of membrane depolarization. The  voltage-dependence of such a conductance can be derived by postulating a linear  relationship between the synaptic current flowing into the somatic node and the  synaptic input, i.e. Io,,a = constant 'gun. In conjunction with Ohm's law and  current conservation, this relation leads to a simple fractional polynominal for the  voltage dependency of gK (labeled "gA" in Fig. 1C). As the membrane potential  depolarizes, gK activates and pulls it back towards EK. At large depolarizations gK  inactivates, similar to the "A" potassium conductance, resulting overall in a linear  relationship between input and output (Fig. lB). As the slope conductance of this  particular form of gK can become negative, causing amplification of the synaptic  input, we use a variant of 9K that is monotonized by leveling out the activation  curve after it has reached its maximum, similar to the "M" current (Fig. 1C). Incor-  porating this non-inactivating K + conductance into the distal compartment results  in a slightly sublinear relationship between input and output (Fig. lB).  With #K in place, amplification of Io,,a is achieved by introducing an inward  current between the soma and the postsynaptic site. The voltage-dependency of the  amplification conductance can be derived by postulating Io,,a = 9ain  constant.  #sun. This leads to the non-inactivating #ca shown in Fig. 1C, in which the overall  relationship between synaptic input and somatic output current (Fig. lB) reflects  the amplification.  We extend this concept of deriving the form of the required conductances to a  detailed model of a morphologically reconstructed layer 5 pyramidal cell from cat  visual cortex (Douglas et al., 1991, Fig. 2A;). We assume a passive dendritic tree,  and include a complement of eight common voltage-dependent conductances in its  soma. 500 non-NMDA synapses are distributed on the dendritic tuft throughout  layers 1, 2 and 3, and we assume a proportionaltry between the presynaptic firing  frequency fin and the time-averaged synaptic induced conductance change. When  fin is increased, the detailed model exhibits the same saturation as seen in the  simple model (Fig. 2B). Even if all 500 synapses are activated at fin = 500 Hz only  0.65 nA of current is delivered to the soma. This saturation is caused when the  synaptic input current flows into the high input resistances of the distal dendrites,  thereby reducing the synaptic driving potential. Layer 1 and 2 input together can  contribute a maximum of 0.25 nA to the soma. This is too little current to cause  the cell to spike, in contrast with the experimental evidence (Cauller and Connors,  1994), in which spike discharge was evoked reliably. Electrotonic losses make only  a minor contribution to the small somatic signal. Even when the membrane leak  current is eliminated by setting Rrn to infinity, Ioma only increases a mere 2% to  0.66 nA.  Amplifying and Linearizing Apical Synaptic Inputs to Cortical Pyramidal Cells 523  Layer 3  Layer 4  Layer 5  B  2  Current delivered to soma  0 o  ' Layer 5] '  ssive dendrlte  Layers 1-3:  Llnearlzed  and amplified  Layers 1-3:  ?assve dendrte  lOO  fln (Hz)  2OO  o  C Activation Curves  gK  960 -40 -20 0  V (mV)  o  Input-Output behavior  Act 1ve,  predicted / Active  5O 150 20O  fin (Hz)  Figure 2: Amplification in the detailed model. (A) The morphology of this  layer V pyramidal cell was reconstructed from a HRP-stained cell in area 17 of the  adult cat (Douglas el al., 1991). The layers are marked in alternating black and  grey. The boundaries between superficial layers are not exact, but rough estimates  and were chosen at branch points; a few basal dendrites may reach into layer 6.  Axon not shown. (B) Current delivered to the soma by stimulation of 500 AMPA  synapses throughout either layer 5 or layers 1-3. (C) Derived activation curves for  gs: and Sea. Sigmoidal fits of the form g(V) = 1/(1 + e(v'a'-v)/K), resulted in  KK = 3.9 rnV, VaaIf,K -- --51 mV, Kca = 13.7 mV, Vaalf,Ca ---- --14 reV. (D)  Output spike rate as a function of input activation rate of 500 AMPA synapses  in layers 1-3, with and without the derived conductances. The dashed line shows  the fou rate predicted by using the linear target Loma as a function of fi, in  combination with the somatic f - I relationship.  524 Bernander, Koch, and Douglas  Vm  ' '  t (msec)  Figure 3: Dendritic calcium spikes. All-or-nothing dendritic Ca 2+ calcium  spikes can be generated by adding a voltage-independent but Ca2+-dependent K +  conductance to the apical tree with gmax -- 11.4 nS. The trace shown is in response  to sustained intradendritic current injection of 0.5 nA. For clamp currents of 0.3 nA  or less, no calcium spikes are triggered and only single somatic spikes are obtained  (not shown). These currents do not substantially affect the current amplifier effect.  By analogy with the simple model of Fig. 1, we eliminate the saturating response by  introducing a non-inactivating form of gz spread evenly throughout layers 1-3. The  resulting linearized response is amplified by a Ca + conductance located at the base  of the apical tuft, where the apical dendrite crosses from layer 4 to layer 3 (Fig. 2A).  This is in agreement with recent calcium imaging experiments, which established  that layer 5 neocortical pyramidal cells have a calcium hot spot in the apical tree  about 500-600/rn away from the soma (Tank et al., 1988). Although the derivation  of the voltage-dependency of these two conductances is more complicated than in  the three compartment model, the principle of the derivation is similar (Bernander,  1993, Fig. 2C;). We derive a Ca + conductance, for a synaptic current gain of  two, resembling a non-inactivating, high-threshold calcium conductance. The curve  relating synaptic input frequency and axial current flowing into the soma (Fig. 2B)  shows both the linearized and amplified relationships.  Once above threshold, the model cell has a linear current-discharge relation with a  slope of about 50 spikes per second per nA, in good agreement with experimental  observations in vitro (Mason and Larkman, 1990) and in vivo (Ahmed et al., 1993).  Given a sustained synaptic input frequency, the somatic f-I relationship can be used  to convert the synaptic current flowing into the soma Isoma into an equivalent output  frequency (Abbott, 1991; Powers et al., 1992; Fig. 2D). This simple transformation  accounts for all the relevant nonlinearities, including synaptic saturation, interaction  and the threshold mechanism at the soma or elsewhere. We confirmed the validity  of our transformation method by explicitly computing the expected relationship  between fin and four, without constraining the somatic potential, and comparing  the two. Qualitatively, both methods lead to very similar results (Fig. 2D): in the  Amplifying and Linearizing Apical Synaptic Inputs to Cortical Pyramidal Cells 525  presence of dendritic gc, superficial synaptic input can robustly drive the cell, in a  proportional manner over a large input range.  The amplification mechanism derived above is continuous in the input rate. It does  not exhibit the slow calcium spikes described in the literature (Pockberger, 1991;  Amitai et al., 1993; Kim and Connors, 1993). However, it is straightforward to add a  calcium-dependent potassium conductance yielding such spikes. Incorporating such  a conductance into the apical trunk leads to calcium spikes (Fig. 3) in response to  an intradendritic current injection of 0.4 nA or more, while for weaker inputs no  such events are seen. In response to synaptic input to the tuft of 120 Hz or more,  these spikes are activated, resulting in a moderate depression (25% or less) of the  average output rate, fou (not shown).  In our view, the function of the dendritic conductances underlying this all-or-none  voltage event is the gradual current amplification of superficial input, without am-  plifying synaptic input to the basal dendrites (Bernander, 1993). Because gca  depolarizes the membrane, further activating gca, the gain of the current amplifier  is very sensitive to the density and shape of the dendritic gca. Thus, neuromod-  ulators that act upon gc control the extent to which cortical feedback pathways,  acting via superficial synaptic input, have access to the output of the cell.  Acknowledgements  This work was supported by the Office of Naval Research, the National Institute of  Mental Health through the Center for Neuroscience, the Medical Research Council  of the United Kingdom, and the International Human Frontier Science Program.  References  [1] L.F. Abbott. Realistic synaptic inputs for model neuronal networks. Network,  2:245-258, 1991.  [2] B. Ahmed, J.C. Anderson, R.J. Douglas, K.A.C. Martin, and J.C. Nelson.  The polyneuronal innervation of spiny steallate neurons in cat visual cortex.  Submitted, 1993.  [3] Y. Amitai, A. Friedman, B.W. Connors, and M.J. Gutnick. Regenerative ac-  tivity in apical dendrites of pyramidal cells in neocortex. Cerebral Cortex,  3:26-38, 1993.  [4] ( Bernander. Synaptic integration and its control in neocortical pyramidal  cells. May 1993. Ph.D. thesis, California Institute of Technology.  [5] L.J. Cauller and B.W. Connors. Functions of very distal dendrites: experi-  mental and computational studies of layer I synapses on neocortical pyramidal  cells. In T. McKenna, J. Javis, and S.F. Zarnetzer, editors, Single Neuron  Computation, chapter 8, pages 199-229. Academic Press, Boston, MA, 1992.  [6] L.J. Cauller and B.W. Connors. J. Neuroscience, In Press.  [7] E. De Schutter and J.M. Bower. Firing rate of purkinje cells does not depend  on the dendritic location of parallel fiber inputs. Eur. J. of Neurosci., S5:17,  1992.  526 Bernander, Koch, and Douglas  [10]  [11]  [12]  [13]  [14]  [15]  [16]  [17]  [8] R.J. Douglas, K.A.C. Martin, and D. Whitteridge. An intracellular analysis of  the visual responses of neurones in cat visual cortex. J. Physiology, 440:659-  696, 1991.  [9] D.J. Felleman and D.C. Van Essen. Distributed hierarchical processing in the  primate cerebral cortex. Cerebral Cortex, 1:1-47, 1991.  H.G. Kim and B.W. Connors. Apical dendrites of the neocortex: Correlation  between sodium- and calcium-dependent spiking and pyramidal cell morphol-  ogy. J. Neuroscience, In press.  G. Laurent. A dendritic gain-control mechanism in axonless neurons of the  locust, schistocerca americana. J Physiology (London), 470:45-54, 1993.  A. Mason and A.U. Larkman. Correlations between morphology and electro-  physiology of pyramidal neurons in slices of rat visual cortex. II. Electrophysi-  ology. J. Neuroscience, 10(5):1415-1428, 1990.  H. Pockberger. Electrophysiological and morphological properties of rat motor  cortex neurons in vivo. Brain Research, 539:181-190, 1991.  P.K. Powers, R.F. Tobinson, and M.A. Konodi. Effective synaptic current can  be estimated from measurements of neuronal discharge. J. Neurophysiology,  68(3):964-968, 1992.  W.G. Regehr, J. Kehoe, P. Ascher, and C.M. Armstrong. Synaptically triggered  action-potentials in dendrites. Neuron, 11(1):145-151, 1993.  K.S. Rockland and A. Virga. Terminal arbors of individual "feedback" ax-  ons projecting from area V2 to V1 in the macaque monkey: a study us-  ing immunohistochemistry of anterogradely transported phaseoulus vulgaris-  leucoagglutinin. J. Comp. Neurol., 285:54-72, 1989.  W.A. Spencer and E.R. Kandel. Electrophysiology of hippocampal neurons.  IV fast prepotentials. J. Neurophysiology, 24:272-285, 1961.  D.W. Tank, M. Sugimori, J.A. Connor, and R.R. Llin.s. Spatially resolved  calcium dynamics of mammalian purkinje cells in cerebellar slice. Science,  242:773-777, 1988.  [19] R. Yuste, K.R. Delaney, M.J. Gutnick, and D.W. Tank. Spatially localized  calcium accumulations in apical dendrites of layer 5 neocortical neurons. In  Neuroscience Abstr. 19, page 616.2, 1993.  
Development of Orientation and Ocular  Dominance Columns in Infant Macaques  Klaus Obermayer  Howard Hughes Medical Institute  SMk-Institute  La Jolla, CA 92037  Lynne Kiorpes  Center for Neural Science  New York University  New York, NY 10003  Gary G. Blasdel  Department of Neurobiology  Harvard Medical School  Boston, MA 02115  Abstract  Maps of orientation preference and ocular dominance were recorded  optically from the cortices of 5 infant macaque monkeys, ranging in  age from 3.5 to 14 weeks. In agreement with previous observations,  we found that basic features of orientation and ocular dominance  maps, as well as correlations between them, are present and robust  by 3.5 weeks of age. We did observe changes in the strength of  ocular dominance signals, as well as in the spacing of ocular dom-  inance bands, both of which increased steadily between 3.5 and 14  weeks of age. The latter finding suggests that the adult spacing  of ocular dominance bands depends on cortical growth in neonatal  animals. Since we found no corresponding increase in the spacing  of orientation preferences, however, there is a possibility that the  orientation preferences of some cells change as the cort;.cal surface  expands. Since correlations between the patterns of orientation  selectivity and ocular dominance are present at an age, when the  visual system is still immature, it seems more likely that their de-  velopment may be an innate process and may not require extensive  visual experience.  543  544 Obermayer, Kiorpes, and Blasdel  I INTRODUCTION  Over the past years, high-resolution images of the simultaneous representation of  orientation selectivity and ocular dominance have been obtained in large areas of  macaque striate cortex using optical techniques [3, 4, 5, 6, 12, 18]. These studies  confirmed that ocular dominance and orientation preference are organized in large  parts in slabs. While optical recordings of ocular dominance are in accordance with  previous findings, it turned out that iso-orientation slabs are much shorter than  expected, and that the orientation map contains several other important elements  of organization - singularities, fractures, and saddle-points.  A comparison between maps of orientation preference and ocular dominance, which  were derived from the same region of adult monkey striate cortex, showed a pro-  nounced relationship between both patterns [5, 12, 13, 15, 17]. Fourier analyses,  for example, reveal that orientation preferences repeat at closer intervals along the  ocular dominance slabs than they do across them. Singularities were found to align  with the centers of ocular dominance bands, and the iso-orientation bands, which  connect them, intersect the borders of ocular dominance bands preferably at angles  close to 90  .  Given the fact that these relationships between the maps of orientation and ocular  dominance are present in all maps recorded from adult macaques, one naturally won-  ders how this organization matures. If the ocular dominance slabs were to emerge  initially, for example, the narrower slabs of iso-orientation might later develop in  between. This might seem likely given the anatomical segregation which is apparent  for ocular dominance but not for orientation [9]. However, this possibility is contra-  dicted by physiological studies that show normal, adult-like sequences of orientation  preference in the early postnatal weeks in macaque when ocular dominance slabs  are still immature [19]. The latter findings suggest a different developmental hy-  pothesis; that the organization into regions of different orientation preferences may  precede or even guide ocular dominance formation. A third possibility, consistent  with both previous results, is that orientation and ocular dominance maps form  independently and align in later stages of development.  In order to provide evidence for one or the other hypothesis, we investigated the  relationship between ocular dominance and orientation preference in very young  macaque monkeys. Results are presented in the remainder of this paper. Section 2  contains an overview about the experimental data, and section 3 relates the data  to previous modelling efforts.  2  ORIENTATION AND OCULAR DOMINANCE  COLUMNS IN INFANT MACAQUES  2.1 THE OVERALLSTRUCTURE  Figure 1 shows the map of orientation preference (Fig. la) and ocular dominance  (Fig. lb) recorded from area 17 of a 3.5 week old macaqueJ Both maps look similar   For all animals orientation and ocular dominance were recorded from a region close to  the border to area 18 and close to midline.  Development of Orientation and Ocular Dominance Columns in Infant Macaques 545  a b  '"':'1'"'-":':::'"' - -' :::: ::::'-" :.::i;'::::::: ::::': '"':" :':':':':'  x..%'-.'q-: ..... eeo'.'.o.  '-'-  '.- '.'o' .' '.' '.'   '.<..'.:.:.:.- . .I.:.:..'-..'..'.X..I-:..'. '.i.i.:q.:iI::. ::i1 I  :::::::::::'::: ::::::::::::::::::::::::::::::::::: -  ..:.:.:.) "::"' :':"  is' '>'-':. '.   .::.:'.::::::'--- "-"""'"'-' '"' "' '. : '- .'..... -::'.:"' _ .:   :::::::::::::::::::::::::::::::::::: --  ================================== .... :.::::::::::::::::::  '-':::::::: ::::::::;:.;:::. .::5:;::::: :':5:::::'::::::::  '-:::::::::::.:::: . :::::::::::::::::: :::::::::::2:::  :::::::::::::::.. :::.::.:::::::::. ::: ::::: ...:......  .:/::::: :::: ........ r... ..........,..:...::....:::::.,..   o.-'.-.... '.:::.:o...: .::.'..;:   ". "-:'o,':l.. -'..'-:.:.'.-:..'-I .i.:.>:-N.' .  ;..'-:o:.. . it:: I:: i:.'>  .i.z..:.;'-  '"':":': .... :':':': ...... .:.i.'"'"' :.:.:-:.it:::.  Figure 1: Spatial pattern of orientation preference and ocular dominance recorded  from area 17 of a macaque, 3.15 weeks of age. Figures (a) and (b) show orientation  preferences and ocular dominance bands within the same 3.1 mm x 4.3 mm large re-  gion of striate cortex. Brightness values in Fig. (a) indicate orientation preferences,  where the interval of 180  is represented by the progression in colors frmn black  to white. Brightness values in Fig. (b) indicate ocular dominance, where bright  and dark denote ipsi- and contralateral eye-preference. respectively. The data was  recorded from a region close to the border to area 18 and close to midline. Figure  (c) shows an enlarged section of this map in the preference (left) and the in contour  plot (right) representations. Iso-orientation lines on the right indicate intervals of  11.215 . Letters indicate linear zones (L), saddle points (H), singularities (S), and  fractures (F).  to maps which have been recorded from adults. The orientation map exhibits all of  the local elements which have been described [12, 13]: linear zones, saddle points,  546 Obermayer, Kiorpes, and Blasdel  a  .1= 6121un  to area 17/18 b 1.2  boiler 1.0 = Opm   o  0.2  ::: ::: :::: X. = 7241m 0.0 - . , .  0 I 2 3 4 5  spatial frequency [1/mm]  c e 0.5  .o [  o.o  -0.5  0 200 400 600 800  distance [gm]  Figure 2: Fourieranalysis of the ori-  entation map shown in Figure la.  (a) Complex 2D-Fouriertransform.  Each pixel corresponds to one Fouri-  ermode and its blackness indicates  the corresponding energy. A dis-  tance of one pixel corresponds to  0.23/mm. (b) Power as a function  of radial spatial frequency. (c) Au-  tocorrelations $ii as a function of  distance. The indices i,j E {3,4}  denote the two cartesian coordinates  of the orientation preference vector.  For details on the calculation see  [13, 15].  singularities, and fractures (Fig. lc). The ocular dominance map shows its typical  pattern of alternating bands.  Figure 23 shows the result of a complex 2D Fourier transform of the orientation  map shown in Figure la. Like for maps recorded from adult monkeys [13] the  spectrum is characterized by a slightly elliptical band of modes which is centered  at the origin. The major axis approximately aligns with the axis parallel to the  border to area 18 as well as with the ocular dominance bands. Therefore, like in  the adults, the orientation map is stretched perpendicular to the ocular dominance  bands, apparently to adjust to the wider spacing.  When one neglects the slight anisotropy of the Fourier spectra one can estimate a  power spectrum by averaging the squared Fourier amplitudes for similar frequen-  cies. The result is a pronounced peak whose location is given by the characteristic  frequency of the orientation map (Fig. 2b). As a consequence, autocorrelation func-  tions have a Mexican-hat shape (Fig. 2c), much like it has been reported for adults  [13, 15].  In summary, the basic features of the patterns of orientation and ocular dominance  are established as early as 3.5 weeks of age. Data which were recorded from four  Development of Orientation and Ocular Dominance Columns in Infant Macaques 547  Table 1: Characteristic wavelengths ("OD) and signal strengths (O'OD) for the ocular  dominance pattern, as well as characteristic wavelengths (AoP), density of +180 -  singularities (p+), density of - 180-singularities (p_), total density of singularities  (p), and percentage of area covered by linear zones (ali,) for the orientation pattern  as a function of age.  age (rOD AOD AOp p+ p- p aim  (weeks) (pm) (pm)(mm -2) (mm -2) (mm -2) (%area)  3.5 0.92 686 660 3.9 3.9 7.8 47  5.5 0.96 730 714 3.7 3.7 7.4 49  7.5 0.66 870 615 4.5 4.5 9.0 45  14 1.23 917 700 3.9 3.8 7.7 36  adult 1.36 950 768 3.9 3.8 7.7 43  other infants ranging from 5.5 to 14 weeks (not shown) confirm the above findings.  2.2  CHARACTERISTIC WAVELENGTHS AND SIGNAL  STRENGTH  A more detailled analysis of the recorded patterns, however, reveals changes of  certain features with age. Table 1 shows the changes in the typical wavelength of  the orientation and ocular dominance patterns as well as the (normalized) ocular  dominance signal strength with age. The strength of the ocular dominance signal  increases by a factor of 1.5 between 3.5 weeks and adulthood, a fact, which could  be explained by the still ongoing segregation of fibers within layer IVc.  The spacing of the ocular dominance columns increases by approximately 30% be-  tween 3.5 weeks and adulthood. This change in spacing would be consistent with  the growth of cortical surface area during this period [16] if one assumes that cor-  tex grows anisotropically in the direction perpendicular to the ocular dominance  bands. Interestingly, the characteristic wavelengths of the orientation patterns do  not exhibit such an increase. The wavelengths for the patterns recorded from the  different infants are close to the "adult" values. More evidence for a stable orienta-  tion pattern is provided by the fact, that the density ofsingularities is approximately  constant with age 2 and that the percentage of cortical area covered by linear zones  does neither increase nor decrease. Hence we are left with the puzzle that at least  the pattern of orientation does not follow cortical growth.  2.3  CORRELATIONS BETWEEN THE ORIENTATION AND  OCULAR DOMINANCE MAPS  Figure 3 shows a contour plot representation of the pattern of orientation preference  in overlay with the borders of the ocular dominance bands for the 3.5 week old  animal. Iso-orientation contours (thin lines) indicate intervals of 15 . Thick lines  indicate the border of the ocular dominance bands. From visual inspection it is  2Note that both types of singularities appear in equal numbers.  548 Obermayer, Kiorpes, and Blasdel  Figure 3: Contour plot representa-  tion of the orientation map shown in  Figure la in overlay with the borders  of the ocular dominance bands taken  from Figure lb. Iso-orientation lines  (thin lines) indicate intervals of 15  .  The borders of the ocular dominance  bands are indicated by thick lines.  already apparent that singularities have a strong tendency to align with the center  of the ocular dominance bands (arrow 1) and that in the linear zones (arrow 2),  where iso-orientation bands exist, these bands intersect ocular dominance bands at  angles close to 90  most of the time.  Table 2 shows a quantitative analysis of the local intersection angle. Percentage  of area covered by linear zones (cf. [12] for details of the calculation) is given for  regions, where orientation bands intersect ocular dominance bands within 18  of  perpendicular, and regions where they intersect within 18  of parallel. For all of  the animals investigated the percentages are two to four times higher for regions,  where orientation bands intersect ocular dominance bands at angles close to 90 ,  much like it has been observed in adults [12]. In particular there is no consistent  trend with age: the correlations between the orientation and ocular dominance  maps are established as early as 3.5 weeks of age.  age p e rp ap a r  alin lin  (weeks) (%area)(% area)  3.5 15.9 4.1  5.5 12.2 6.8  7.5 13.3 6.2  14 12.4 3.7  adult 18.0 2.7  Table 2: Percentage of area covered by  linear zones as a function of age for re-  gions, where orientation bands inter-  sect ocular dominance bands within  18  of perpendicular (%Pin, P), and re-  gions where they intersect within 18   of parallel ta p"" (cf. [12] for details of  \ lin ]  the calculation).  Development of Orientation and Ocular Dominance Columns in Infant Macaques 549  3 CONCLUSIONS AND RELATION TO MODELLING  In summary, our results provide evidence that the pattern of orientation is estab-  lished at a time when the pattern of ocular dominance is still developing. However,  they provide also evidence for the fact that the pattern of orientation is not linked  to cortical growth. This latter finding still needs to be firmly established in studies  where the development of orientation is followed in one and the same animal. But if  it is taken seriously the consequence would be that orientation preferences may shift  and that pairs of singularities are formed. The early presence of strong correlations  between both maps indicate that the development of orientation and ocular dom-  inance are not independen processes. Both paerns have to adjust to each oher  while cortex is growing. It, therefore, seems as if the third hypothesis is true (see  Introduction) which states that both patterns develop independently and adjust to  each other in the late stages of development. As has been shown in [13, 15] and is  suggested in [7, 14] these processes are certainly in the realm of models based on  Hebbian learning.  Many features of the orientation and ocular dominance maps are present at an age  when the visual system of the monkey is still immature [8, 11]. In particular, they  are present at a time when spatial vision is strongly impared. Consequently, it  seems unlikely that the development of these features as well as of the correlations  between both patterns requires high acuity form vision, and models which try to  predict the structure of these maps from the structure of visual images [1, 2, 10]  have to take this fact into account. The early development of orientation prefer-  ence and its correlations with ocular dominance make it also seem more likely that  their development may me an innate process and may not require extensive visual  experience. Further experiments, however, are needed to settle these questions.  Acknowledgement s  This work was funded in part by the Klingenstein Foundation, the McKnight Foun-  dation, the New England Primate Research Center (P51RRO168-31), the Seaver  Institute, and the Howard Hughes Medical Institute. We thank Terry Sejnowski,  Peter Dayan, and Rich Zemel for useful comments on the manuscript. Linda. As-  comb, Jaqueline Mack, and Gina Quinn provided excellent technical assistance.  References  [1]  [2]  [3]  H. G. Barrow and A. J. Bray. Activity induced color blob formation. In  I. Alexander and J. Taylor, editors, Artificial Neural Networks H, pages 5-9.  Elsevier Publishers, 1992.  H. G. Barrow and A. J. Bray. A model of the adaptive development of complex  cortical cells. In I. Alexander and J. Taylor, editors, Artificial Neural Networks  //, pages 1-4. Elsevier Publishers, 1992.  E. Barfield and A. Grinvald. Relationships between orientation-preference pin-  wheels, cytochrome oxidase blobs, and ocular-dominance columns in primate  striate cortex. Proc. Natl. Acad. $ci. USA, 89:11905-11909, 1992.  550 Obermayer, Kiorpes, and Blasdel  [4] G. G. Biasdel. Differential imaging of ocular dominance and orientation selec-  tivity in monkey striate cortex. J. Neurosci., 12:3117-3138, 1992.  [5] G. G. Blasdel. Orientation selectivity, preference, and continuity in monkey  striate cortex. J. Neurosc,., 12:3139-3161, 1992.  [6] G. G. Blasdel and G. Salama. Voltage sensitive dyes reveal a modular organi-  zation in monkey striate cortex. Nature, 321:579-585, 1986.  [7] R. Durbin and G. Mitchison. A dimension reduction framework for under-  standing cortical maps. Nature, 343:644-647, 1990.  [8] L. Kiorpes and T. Movshon. Behavioural analysis of visual development. In  J. R. Coleman, editor, Development of Sensory Systems n Mammals, pages  125-154. John Wiley, 1990.  [9] S. LeVay, D. H. Hubel, and T. N. Wiesel. The development of ocular dominance  columns in normal and visually deprived monkeys. J. Comp. Neurol., 191:1-51,  1980.  [10] Y. Liu and H. Shouval. Principal component analysis of natural images - an  analytic solution. Preprint.  [11] T. Movshon and L. Kiorpes. Biological limits on visual development in pri-  mates. In K. Simon, editor, Handbook of bfant Vision. Oxford University  Press, 199.3. in press.  [12] K. Obermayer and G. G. Biasdel. Geometry of orientation and ocular domi-  nance columns in monkey striate cortex. J. Neurosci., 13:4114-4129, 1993.  [13] K. Obermayer, G. G. Blasdel, and K. Schulten. A statistical mechanical analy-  sis of self-organization and pattern formation during the development of visual  maps. Phys. Rev. A15, 45:7568-7589, 1992.  [14] K. Obermayer, H. Ritter, and K. Schulten. A principle for the formation of  the spatial structure of cortical feature maps. Proc. Natl. Acad. Sci. USA,  87:8345-8349, 1990.  [15] K. Obermayer, K. Schulten, and G. G. Blasdel. A comparison of a neural  network model for the formation of brain maps with experimental data. In  D. S. Touretzky and R. Lippman, editors, Advances in Neural Information  Processing Systems 4, pages 83-90. Morgan Kaufmann Publishers, 1992.  [16] D. Purves and A. LaMantia. Development of blobs in the visual cortex of  macaques. J. Comp. Neurol., 332:1-7, 1993.  [17] N. Swindale. A model for the coordinated development of columnar systems  in primate striate cortex. Biol. Cybern., 66:217-230, 1992.  [18] D. Y. Tso, R. D. Frostig, E. E. Lieke, and A. Grinvald. Functional organization  of primate visual cortex revealed by high resolution optical imaging. Science,  249:417-420, 1990.  [19] T. N. Wiesel and D. H. Hubel. Ordered arrangement of orientation columns  in monkeys lacking visual experience. J. Comp. Neurol., 158:307-318, 1974.  
Unsupervised Learning of Mixtures of  Multiple Causes in Binary Data  Eric Saund  Xerox Palo Alto Research Center  3333 Coyote Hill Rd., Palo Alto, CA, 94304  Abstract  This paper presents a formulation for unsupervised learning of clus-  ters reflecting multiple causal structure in binary data. Unlike the  standard mixture model, a multiple cause model accounts for ob-  served data by combining assertions from many hidden causes, each  of which can pertain to varying degree to any subset of the observ-  able dimensions. A crucial issue is the mixing-function for combin-  ing beliefs from different cluster-centers in order to generate data  reconstructions whose errors are minimized both during recognition  and learning. We demonstrate a weakness inherent to the popular  weighted sum followed by sigmoid squashing, and offer an alterna-  tive form of the nonlinearity. Results are presented demonstrating  the algorithm's ability successfully to discover coherent multiple  causal representations of noisy test data and in images of printed  characters.  I Introduction  The objective of unsupervised learning is to identify patterns or features reflecting  underlying regularities in data. Single-cause techniques, including the k-means al-  gorithm and the standard mixture-model (Duda and Hart, 1973), represent clusters  of data points sharing similar patterns of ls and Os under the assumption that each  data point belongs to, or was generated by, one and only one cluster-center; output  activity is constrained to sum to 1. In contrast, a multiple-cause model permits more  than one cluster-center to become fully active in accounting for an observed data  vector. The advantage of a multiple cause model is that a relatively small number  27  28 Saund  of hidden variables can be applied combinatorially to generate a large data set. Fig-  ure i illustrates with a test set of nine 121-dimensional data vectors. This data set  reflects two independent processes, one of which controls the position of the black  square on the left hand side, the other controlling the right. While a single cause  model requires nine cluster-centers to account for this data, a perspicuous multiple  cause formulation requires only six hidden units as shown in figure 4b. Grey levels  indicate dimensions for which a cluster-center adopts a "don't-know/don't-care"  assertion.  Figure 1' Nine 121-dimensional test data samples exhibiting multiple cause  structure. Independent. processes control the position of the black rectangle  on the left and right hand sides.  While principal components analysis and its neural-network variants (Bourlard and  Kamp, 1988; Sanger, 1989) as well as the Harmonlure Boltzmann Machine (Freund  and Haussler, 1992) are inherently multiple cause models, the hidden represen-  tations they arrive at are for many purposes intuitively unsatisfactory. Figure 2  illustrates the principal components representation for the test data set presented  in figure 1. Principal components is able to reconstruct the data without error  using only four hidden units (plus fixed centroid), but these vectors obscure the  compositional structure of the data in that they reveal nothing about the statistical  independence of the left and right hand processes. Similar results obtain for multi-  ple cause unsupervised learning using a Harmonlure network and for a feedforward  network using the sigmoid nonlinearity. We seek instead a multiple cause formula-  tion which will deliver coherent representations exploiting "don't-know/don't-care"  weights to make explicit the statistical dependencies and independencies present  when clusters occur in lower-dimensional subspaces of the full J-dimensional data  space.  Data domains differ in ways that underlying causal processes interact. The present  discussion focuses on data obeying a WRITE-WHITE-AND-BLACK model, under which  hidden causes are responsible for both turning "on" and turning "off" the observed  variables.  b  Figure 2: Principal components representation for the test data from figure  1. (a) centroid (white: -1, black: 1). (b) four component vectors sufficient  to encode the nine data points. (lighter shadings: cj, < 0; grey: cj, = 0;  darker shading: cj,t. > 0).  Unsupervised Learning of Mixtures of Multiple Causes in Binary Data 29  2 Mixing Functions  A large class of unsupervised learning models share the architecture shown in figure  3. A binary vector Di -= (di,l,di,2, ...di,j, ...di,j) is presented at the data layer, and  a measurement, or response vector rni = (rni,1, mi,2, ...rni,k, ...mi,K) is computed at  the encoding layer using "weights" cj,. associating activity at data dimension j with  activity at hidden cluster-center k. Any activity pattern at the encoding layer can  be turned around to compute a prediction vector ri ---- (ri,1,, vi,2, ...vi,j, ...vi,J) at the  data layer. Different models employ different functions for performing the measure-  ment and prediction mappings, and give different interpretations to the weights.  Common to most models is a learning procedure which attempts to optimize an  objective finct. ion on errors between data vectors in a training set, and predictions  of these data vectors under their respective responses at the encoding layer.  encoding layer  (cluster-centers)  data layer  m k  d j (observed data)  r. (predicted)  J  Figure 3: Architecture underlying a large class of unsupervised learning models.  The key issue is the mizin.q function which specifies how sometimes conflicting pre-  dictions from individual hidden units combine to predict values on the data dimen-  sions. Most neural-network formulations, including principal components variants  and the Boltzmann Machine, employ linearly weighted sum of hidden unit activity  followed by a squashing, bump, or other nonlinearity. This form of mixing function  permits an error in prediction by one cluster center to be cancelled out by correct  predictions from others without consequence in terms of error in the net prediction.  As a result, there is little global pressure for cluster-centers to adopt don't-know  values when they are not quite confident in their predictions.  Instead, a multiple cause formulation delivering coherent cluster-centers requires a  form of nonlinearity in which active disagreement must result in a net "uncertain"  or neutral prediction that results in nonzero error.  30 Saund  3 Multiple Cause Mixture Model  Our formulation employs a zero-based representation at the data layer to simplify  the mathematical expression for a suitable mixing function. Data values are either 1  or -1; the sign of a weight cj, indicates whether activity in cluster-center k predicts  a 1 or -1 at data dimension j, and its magnitude (Icj,l _ 1) indicates strength of  belief; cj,: 0 corresponds to "don't-know/don't-care" (grey in figure 4b).  The mixing function takes the form,  This formula is a computationally tractable approximation to an idealized mixing  function created by linearly interpolating boundary values on the extremes of mi,k E  {0, 1} and cj,  {-1,0, 1} rationally designed to meet the criteria outlined above.  Both learning and measurement operate in the context of an objective function on  predictions equivalent to log-likelihood. The weights cj, are found through gradient  ascent in this objective function, and at each training step the encoding mi of an  observed data vector is found by gradient ascent as well.  4 Experimental Results  Figure 4 shows that the model converges to the coherent multiple cause represen-  tation for the test data of figure i starting with random initial weights. The model  is robust with respect to noisy training data as indicated in figure 5.  In figure 6 the model was trained on data consisting of 21 x 21 pixel images of  registered lower case characters. Results for K = 14 are shown indicating that the  model has discovered statistical regularities associated with ascenders, descenders,  circles, etc.  b  Figure 4: Multiple Cause Mixture Model representation for the test data  from figure 1. (a) Initial random cluster-centers. (b) Cluster-centers after  seven training iterations (white: cj, -- -1; grey: cj, - 0; black: cj, -- 1).  Unsupervised Learning of Mixtures of Multiple Causes in Binary Data 31  5 Conclusion  Ability to compress data, and statistical independence of response activities (Bar-  low, 1989), are not the only criteria by which to judge the success of an encoder  network paradigm for unsupervised learning. For many purposes, it is equally im-  portant that hidden units make explicit statistically salient structure arising from  causally distinct processes.  The difficulty lies in getting the internal knowledge-bearing entities sensibly to  divvy up responsibility for training data not just pointwise, but dimensionwise.  Mixing functions based on linear weighted sum of activities (possibly followed by  a nonlinearity) fail to achieve this because they fail to pressure the hidden units  into giving up responsibility (adopting "don't know" values) for data dimensions  on which they are prone to be incorrect. We have outlined criteria, and offered  a specific functional form, for nonlinearly combining beliefs in a predictive mixing  function such that statistically coherent hidden representations of multiple causal  structure can indeed be discovered in binary data.  References  Barlow, H.; [1989], "Unsupervised Learning," Neural Computation, 1: 295-311.  Bourlard, H., and Kamp, Y.; [1988], Auto-Association by Multilayer Perceptrons and  Singular Value Decomposition," Biological Cybernetics, 59:4-5,291-294.  Duda, R., and Hart, P.; [1973], Pattern Classification and Scene Analysis, Wiley,  New York.  FSldik, P.; [1990], "Forming sparse representations by local anti-Hebbian learning,"  Biological Cybernetics, 64:2, 165-170.  Freund, Y., and Haussler, D.; [1992], "Unsupervised learning of distributions on  binary vectors using twodayer networks," in Moody, J., Hanson, S., and Lippman,  R., eds, Advances in Neural Information Processing Systems d, Morgan Kauffman,  San Mateo, 912-919.  Nowlan, S.; [1990], "Maximum Likelihood Competitive Learning," in Touretzky, D.,  ed., Advances in Neural Information Processing Systems 2, Morgan Kauffman, San  Mateo, 574-582.  Sanger, T.; [1989], "An Optimality Principle for Unsupervised Learning," in Touret-  zky, D., ed., Advances in Neural Information Processing Systems, Morgan Kauff-  man, San Mateo, 11-19.  32 Saund  b  c  observod data d, ineasuiements  .3265 .822  1. {} .24fi4  284 1 . 0  1109  predictions  Figure 5: Multiple Cause Mixture Model results for noisy training data. (a)  Five test data sample suites with 10% bit-flip noise. Twenty suites were  used to train from random initial cluster-centers, resulting in the represen-  tation shown in (b). (c) Left: Five test data samples di; Middle: Numer-  ical activities rni,k for the most active cluster-centers (the corresponding  cluster-center is displayed above each rni,k value); Right: reconstructions  (predictions) ri based on the activities. Note how these "clean up" the  noisy samples from which they were computed.  Unsupervised Learning of Mixtures of Multiple Causes in Binary Data 33  b  LI II11 B Ill lllllJlll I  ................. III  ........... tll  .......... I, [.-,}111  .......... IIIJ  ............  ........... IIIII  .............. IIIII  .................. ill  ,,jjjlj,j  aJ JJ  ........ JJ  JJJJJJJJJ  Figure 6: (a) Training set of twenty-six 441-dimensional binary vectors. (b)  Multiple Cause Mixture Model representation at K = 14. (c) Left: Five  test data samples di; Middle: Numerical activities mi, for the most active  cluster-centers (the corresponding cluster-center is displayed above each  mi,k value); Right: reconstructions (predictions) ri based on the activities.  34 Saund  observed data di  meurements 77i,k  1.[3  ....  ,  predictions  c  
Counting function heorem for  multi-layer networks  Adam Kowalczyk  Telecom Australia, Research Laboratories  770 Blackburn Road, Clayton, Vic. 3168, Australia  (a.kowalczyk@trl.oz.au)  Abstract  We show that a randomly selected N-tuple  of points of R ' with  probability > 0 is such that any multi-layer perceptron with the  first hidden layer composed of hi threshold logic units can imple-  of. >  ment exactly 2 z_,i:0 -  then such a perceptron must have all units of the first hidden layer  fully connected to inputs. This implies the maximal capacities (in  the sense of Cover) of 2n input patterns per hidden unit and 2 input  patterns per synaptic weight of such networks (both capacities are  achieved by networks with single hidden layer and are the same as  for a single neuron). Comparing these results with recent estimates  of VC-dimension we find that in contrast to the single neuron case,  for sufficiently large n and hi, the VC-dimension exceeds Cover's  capacity.  I Introduction  In the course of theoretical justification of many of the claims made about neural  networks regarding their ability to learn a set of patterns and their ability to gen-  eralise, various concepts of maximal storage capacity were developed. In particular  Cover's capacity [4] and VC-dimension [12] are two expressions of this notion and  are of special interest here. We should stress that both capacities are not easy  to compute and are presently known in a few particular cases of feedforward net-  works only. VC-dimension, in spite of being introduced much later, has been far  375  376 Kowalczyk  more researched, perhaps due to its significance expressed by a well known relation  between generalisation and learning errors [12, 3]. Another reason why Cover's ca-  pacify gains less attention, perhaps, is that for the single neuron case it is twice  higher than VC-dimension. Thus if one would hypothesise a similar relation to be  true for other feedforward networks, he would judge Cover's capacity to be quite  an unattractive parameter for generalisation estimates, where VC-dimenslon is be-  lieved to be unrealistically big. One of the aims of this paper is to show that this  last hypothesis is not true, at least for some feedforward networks with sufficiently  large number of hidden units. In the following we will always consider multilayer  perceptrons with r continuously-valued inputs, a single binary output, and one or  more hidden layers, the first of which is made up of threshold logic units only.  The derivation of Cover's capacity for a single neuron in [4] is based on the so-called  Function Counting Theorem, proved for the linear function in the sixties (c.f. [4]),  which states that for an N-tuple  of points in general position one can implement  C'(N, n) ae 2 Y-i=0 Jv- different dichotomies of. Extension of this result to the  multilayer case is still an open problem (c.f.T. Cover's address at NIPS'92). One of  the complications arising there is that in contrast to the single neuron case even for  percepttons with two hidden units the number of implementable dichotomies may  be different for different N-tuples in general position [8]. Our first main result states  that this dependence on  is relatively weak, that for a multilayer perceptron the  number of implementable dichotomies (counting function) is constant on each of a  finite number of connected components into which the space of N-tuples in general  position can be decomposed. Then we show that for one of these components  C'(N,rh) different dichotomies can be implemented, where h is the number of  hidden units in the first hidden layer (all assumed to be linear threshold logic  units). This leads to an upper bound on Cover's capacity of 2n input patterns per  (hidden) neuron and 2 patterns per adjustable synaptic weight, the same as for a  single neuron. Comparing this result with a recent lower bound on VC-dimension  of multilayer perceptrons [10] we find that for for sufficiently large n and h the  VC-dimension is higher than Cover's capacity (by a factor log2(hl)).  The paper extends some results announced in [5] and is an abbreviated version of  a forthcoming paper [6].  2 Results  2.1 Standing assumptions and basic notation  We recall that in this paper a multilayer ioercelotror means a layered feedforward  network with one or more hidden layers, and the first hidden layer built exclusively  from threshold logic units.  A dickotomy of an N-tuple  = (xx,...,xnr)  (Rr) nr is a function $: {xx,...,xnr}   {0,1}. For a multilayer perceptron F: R r ---} {0, 1} let  -* C'F() denote the  number of different dichotomies of  which can be implemented for all possible  selections of synaptic weights and biases. We shall call C'F() a courttire# furctior  following the terminology used in [4].  Counting Function Theorem for Multi-Layer Networks 377  )  Example 1. C(g) = C(N, n) de__f 2 Ei=0 N-I for a single threshold logic unit  R" --, {0, x)  Points of an N-tuple  6 (R') N are said to be in general position if there does not  exist an l de= f min(N,n- 1)-dimensional affine hyperplane in R ' containing (l + 2)  of them. We use a symbol {7P(n, N) C (R') N to denote that set of all N-tuples   in general position.  Throughout this paper we assume to be given a probability measure dp dc__f f dx on  R ' such that the density f: R ' --, R is a continuous function.  2.2 Counting function is locally constant  We start with a basic characterisations of the subset 679(n, N) C (R') N.  Theorem 1 (i) (77)(n,N) is an open and dense subset of (Rn) N with a finite  number of connected components.  (it) Any of these components is unbounded, has an infinite Lebesgue measure and  has a positive probability measure.  Proof outline. (i) The key point to observe is that G73(n, N) = { : p() =/: 0},  where p: (R') N ---} R is a polynomial on (R') N. This implies immediately that  G73(n, N) is open and dense in (R') N. The finite number of connected components  follows from the results of Milnor [7] (c.f. [2]).  (it) This follows from an observation that each of the connected components C/has  the property that if (x, ...,XN) 6 Ci and a > 0, then (axe, ...,aXN) 6 i. []  As Example 1 shows, for a single neuron the counting function is constant on  G7)(n, N). However, this may not be the case even for perceptrons with two hidden  units and two inputs (c.f. [8, 0] for such examples and Corollary 8). Our first main  result states that this dependence on  is relatively weak.  Theorem 2 CF(X) is constant on connected components of G7)(n, N).  Proof outline. The basic heuristic behind the proof of this theorem is quite simple.  If we have an N-tuple  6 (R') N which is split into two parts by a hyperplane,  then this split is preserved for any sufficiently small perturbation f  (R') N of ,  and vice versa, any split of : corresponds to a split of . The crux is to show that  if  is in general position, then a minute perturbation f of  cannot allow a bigger  number of splits than is possible for . We refer to [0] for details. []  The following corollary outlines the main impact of Theorem 2 on the rest of the  paper. It reduces the problem of investigation of the function CF(X) on G7)(n, N) to  a consideration of a set of individual, special cases of N-tuples which, in particular,  are amenable to be solved analytically.  Corollary 3 If   Q73(n,N), then CF() = CF(f) for a randomly selected N-  tuple 2 6 (Rn) N with a probability > O.  378 Kowalczyk  2.3 A case of special component of gyP(n, N)  The following theorem is the crux of the paper.  Theorem 4 There exists a connected component CC C gYP(n, N) C (R') N such  that  cF() = = 2 - (for CC)  i=o i  with equality iff the input and first hidden/dyer are fully connected. The synaptlc  weights to units not in the first hidden layer can be constant.  Using now Corollary 3 we obtain:  Corollary 5 C',() = C(N, nh) for   (R') r with a probabilit.l > O.  The component C'C C gyP(n, N) in Theorem 4 is defined as the connected compo-  nent containing  N de__f (c(t),c(t.),...,c(tN)) e (R') N, (1)  where c : R  R ' is the curve defined as c(t) ae (t,t.,...,t) for t  R and  0 < tx < t < -.. < tr are some numbers (this example has been considered  previously in [11]). The essential part of the proof of Theorem 4 is showing the  basic properties of the N-tuple 15v which will be described by the Lemma below.  Any dichotomy  of the N-tuple fiv (c.f. 1) is uniquely defined by its value at  (2 options) and the set of indices 1 < i < i. < .-- < ia < N of all transitional  pairs (c(ti), c(ti+x)), i.e. all indices ii such that 5(c(ti)) y 5(c(ti+x)), where j =  1,..., k, (additional (v-x) options). Thus it is easily seen that there exist altogether  2(v -) different dichotomies of 15v for any given number k of transitional pairs,  where0<k<N.  Lemma6 Given integers n,N,h > 0, k > 0 and a dichotomy  Of N with k  transitional pairs.  (i) Ilk _< nh, then there exist hyperplanes H(w,,,,), (w, b) 6 R n x It, such that  5(p) = O bo+ viO(wi .pj +hi) , (2)  + 0, (3)  der der  for i = 1, ..., h and j = 1, ..., N; here vi = 1 if n is even and vi = (--1) i if n is odd,  der  bo = -0.5 ifn is odd, h is even and 5(Po) = 1, and bo  = 0.5, otherwise.  (ii) Ilk = nh, then w  0 for j = 1,...,n and i = 1,...,h, where w =  (wi, wia, ..., wi,).  (iii) If k > nh, *hen (2) and (3) cannot be satised.  The proof of Lemma  relies on usage of the Vandermonde determinant and its  derivatives. It is quite technical and thus not included here (c.f. [] for details).  Counting Function Theorem for Multi-Layer Networks 379  10 4  10 3  10 2  10  2  (Theorem 7  (Mitchison & Burbin [10]].  (Huang & Huang  [2], Sakurai [1 ..'  (Akaho & Amari -'" ..  ts]) --"  I 2 5 10 102 103 104  Number of hidden units (hi)  Figure 1: Some estimates of capacity.  3 Discussion  3.1 An upper bound on Cover's capacity  The Cover's capacity (or just capacity) of a neural network F : R ' --, {0,1},  Gap(F), is defined as the maximal N such that for a randomly selected N-tuple   = (xt,...,xv) 6 (R') v of points of R ', the network can implement 1/2 of all  dichotomies of  with probability 1 [4, 8].  Corollary 5 implies that Gap(F) is not greater than maximal N such that  CF(IN)/2 N -- C(N, nhx) _> 1/2. (4)  since any property which holds with probability 1 on (R') N must also hold prob-  ability 1 on CC (c.f Theorem 4). The left-hand-side of the above equation is just  the sum of the binomial expansion of (1/2 + 1/2) 'v-t up to h. xn-th term, so, using  the symmetry argument, we find that it is >_ 1/2 if and only if it has at least half  of the all terms, i.e. when N- 1+1 <_ 2(hxn+ 1). Thus the 2(htn+l) is the  maximal value of N satisfying (4). t Now let us recall that a multilayer perceptron  as in this paper can implement any dichotomy of any N-tuple  in general position  if N _< nht + 1 [1, 11]. This leads to the following result:  Theorem 7  + 1 _< Cap(F) <_ + 1).  ;Note that for large N the choice of cutoff value 1/2 is not critical, since the probability  of a dichotomy being implementable drops rapidly as ht n approaches 2r/2.  380 Kowalczyk  N/  #w lO  ._o 8  r-,   6  E  -. 4  a. 2   0  t:::  I 2 5 10 10 2 10 3 10 4  Number of hidden units (hi)  #w  dvdF) / [11]   k.(Sakurai  ap(F) / #w 'h  eorem 7)/  Figure 2: Comparison of estimates of the ratios of Cover's capacity per synaptic  weight (Cap(F)/#w) and VC-dimension per synaptic weight (dvc(F)/w). (Note  that the upper bound for VC-dimension has so far been proved for low number of  hidden layers [9,10].)  for any multilayer perceptton F: R ' -* {0, 1} with the rst hidden layer built from  the hi threshold logic units. For the most efficient networks in this class, with a  single hidden layer, we thus obtain the following result:  1- _< Cap(F)/#w < 2,  where w denotes the number of synaptic weights and biases.  3.2 A relation to C-dimension  The VC-dimension, dye(F), is defined as the largest N such that there exists an  N-tuple [ = (xl, ..., xv) E (R') v for which the network can implement all possible  2 r dichotomies. Recent results of Sakurai [10] imply  dye(F) > (1/2)hal(log2 hi + o(]og 2 hi) q- o((log 2 hl)2/r)). (5)  For sufficiently large n and hi this estimate exceeds 2(nhz + 1) which is an upper  bound on Cap(F). Thus, in contrast to the single threshold logic unit case we have  the following (c.f. Fig. 3):  Corollary 8 Cap(F) < ave(F) if hz >> 1.  3.3 Memorisation ability of multilayer perceptton  Corollary 8 combined with Theorem 7 and Figure 2 imply that for some cases of  patters in general position multilayer perceptton can memorise and reliably retrieve  Counting Function Theorem for Multi-Layer Networks 381  (even with 100% accuracy) much more ( log.(h) times more) than 2 patterns  per connection, as is the case for a single neuron [4]. This proves that co-operation  between hidden units can significantly improve the storage efficiency of neural net-  works.  3.4 A relation to PAC learning  Vapnik's estimate of generalisation error [12] (an error rate on independent test set)  E.(F) _< EL(F)+  holds for N > dye(F) with probability larger that (1  (i) learning error EL(F) and (ii) confidence interval  where  (6)  It contains two terms:  qt(N, dvc, r/) = (ln 2N .dvc In r/  The ability of obtaining small learning error EL(F) is, in a sense, controlled by  Cap(F), while the size of the confidence interval D is controlled by both dye(F)  and Cap(F) (through EL(F)). For a multilayer perceptron as in Theorem 7 when  dvc(F) >> Cap(F) (Fig. 2) it can turn out that actually the capacity rather than  the VC-dimension is the most critical factor in obtaining low generalisation error  E(F). This obviously warrants further research into the relation between capacity  and generalisation.  The theoretical estimates of generalisation error based on VC-dimension are believed  to be too pessimistic in comparison with some experiments. One may hypothesise  that this is caused by too high values of dye(F) used in estimates such as (6). Since  Cover's capacity in the case multilayer perceptron with h >> i turned up to be  much lower than VC-dimension, one may hope that more realistic estimates could  be achieved with generalisation estimates linked directly to capacity. This subject  will obviously require further research. Note that some results along these lines can  be found in Cover's paper [4].  3.5 Some open problems  Theorem 7 gives estimates of capacity per variable connection for a network with  the minimal number of neurons in the first hidden layer showing that these neurons  have to be fully connected. The natural question arises at this point as to whether  a network with a bigger number but not fully connected neurons in the first hidden  layer can achieve a better capacity (per adjustable synaptic weight).  The values of the counting function  -} C,() are provided in this paper for the  particular class of points in general position, for   CC C (R') N. The natural  question is whether they may be by chance a lower or upper bound for the counting  function for the general case of   (R') N ? The results of Sakurai [11] seem  to point to the former case: in his case, the sequences 15N = (p, ...,pN) turned  out to be "the hardest" in terms of hidden units required to implement 100% of  382 Kowalczyk  dichotomies. Corollary 8 and Figure 1 also support this lower bound hypothesis.  They imply in particular that there exists a N'-tuple  = (Yl, Y2, ..., yJv,)  (R') N',  where N'  VC-dimension > N, such that Cr(:) = 2 N' >> 2 N > Cr(fir) for  sufficiently large n and h.  4 Acknowledgement  The permission of Managing Director, Research and Information Technology, Tele-  com Australia, to publish this paper is gratefully acknowledged.  References  [lo]  [11]  [12]  [1] E. Baum. On the capabilities of multilayer perceptrons. Journal of Complezity,  4:193-215, 1988.  [2] S. Ben-David and M. Lindenbaum. Localization rs. identification of semi-  algebraic sets. In Proceedings of the Sixth Annual Workshop or Computational  Learning Theory (to appear), 1993.  [3] A. Blumer, A. Ehrenfeucht, D. Haussler, and M.K. Warmuth. Learnability and  the Vapnik-Chernovenkis dimensions. Journal of the ACM, 36:929-965, (Oct.  1989).  [4] T.M. Cover. Geometrical and statistical properties of linear inequalities with  applications to pattern recognition. IEEE Trans. Elec. Comp., EC-14:326-  334, 1965.  [5] A. Kowalczyk. Some estimates of necessary number of connections and hidden  units for feed-forward networks. In S.J. Hanson et al., editor, Advances in Neu-  ral Information Processing Systems, volume 5. Morgan Kaufman Publishers,  Inc., 1992.  [6] A. Kowalczyk. Estimates of storage capacity of multi-layer perceptton with  threshold logic hidden units. In preparation, 1994.  [7] J. Milnor. On Betti numbers of real varieties. Proceedings of AMS, 15:275-280,  1964.  [8] G.J. Mitchison and R.M. Durbin. Bounds on the learning capacity of some  multi-layer networks. Biological Cybernetics, 60:345-356, (1989).  [9] A. Sakurai. On the VC-dimension of depth four threshold circuits and the  complexity of boolean-valued functions. Manuscript, Advanced Research Lab-  oratory, Hitachi Ltd., 1993.  A. Sakurai. Tighter bounds of the VC-dimension of three-layer networks. In  WCNN93, 1993.  A. Sakurai. n-h-1 networks store no less n. h + 1 examples but sometimes no  more. In Proceedings of IJCNN9, pages III-936-III-941. IEEE, June 1992.  V. Vapnik. Estimation of Dependences Based on Empirical Data. Springer-  Verlag, 1982.  
Two Iterative Algorithms for Computing  the Singular Value Decomposition from  Input[Output Samples  Terence D. Sanger  Jet Propulsion Laboratory  MS 303-310  4800 Oak Grove Drive  Pasadena, CA 91109  Abstract  The Singular Value Decomposition (SVD) is an important tool for  linear algebra and can be used to invert or approximate matrices.  Although many authors use "SVD" synonymously with "Eigen-  vector Decomposition" or "Principal Components Transform", it  is important to realize that these other methods apply only to  symmetric matrices, while the SVD can be applied to arbitrary  nonsquare matrices. This property is important for applications to  signal transmission and control.  I propose two new algorithms for iterative computation of the SVD  given only sample inputs and outputs from a matrix. Although  there currently exist many algorithms for Eigenvector Decomposi-  tion (Sanger 1989, for example), these are the first true sample-  based SVD algorithms.  1 INTRODUCTION  The Singular Value Decomposition (SVD) is a method for writing an arbitrary  nonsquare matrix as the product of two orthogonal matrices and a diagonal matrix.  This technique is an important component of methods for approximating near-  singular matrices and computing pseudo-inverses. Several efficient techniques exist  for finding the SVD of a known matrix (Golub and Van Loan 1983, for example).  144  Singular Value Decomposition 145  I I  I I  I I  u  Figure 1: Representation of the plant matrix P as a linear system mapping inputs  u into outputs y. L'$_ is the singular value decomposition of P.  However, for certain signal processing or control tasks, we might wish to find the  SVD of an unknown matrix for which only input-output samples are available.  For example, if we want to model a linear transmission channel with unknown  properties, it would be useful to be able to approximate the SVD based on samples  of the inputs and outputs of the channel. If the channel is time-varying, an iterative  algorithm for approximating the SVD might be able to track slow variations.  2 THE SINGULAR VALUE DECOMPOSITION  The SVD of a nonsymmetric matrix P is given by P = L'S_R where L and / are  matrices with orthogonal rows containing the left and right "singular vectors", and  $ is a diagonal matrix of "singular values". The inverse of P can be computed by  inverting $, and approximations to P can be formed by setting the values of the  smallest elements of $ to zero.  For a memoryless linear system with inputs u and outputs y - Pu, we can write  y = L'S_Ru which shows that / gives the "input" transformation from inputs  to internal "modes", $ gives the gain of the modes, and L " gives the "output"  transformation which determines the effect of each mode on the output. Figure 1  shows a representation of this arrangement.  The goal of the two algorithms presented below is to train two linear neural networks  N and G to find the SVD of P. In particular, the networks attempt to invert P  by finding orthogonal matrices N and G such that NG  p-l, or PNG = I. A  particular advantage of using the iterative algorithms described below is that it is  possible to extract only the singular vectors associated with the largest singular  values. Figure 2 depicts this situation, in which the matrix $ is shown smaller to  indicate a small number of significant singular values.  There is a close relationship with algorithms that find the eigenvalues of a symmetric  matrix, since any such algorithm can be applied to ppT = LTS2L and pTp =  _RTS2_R in order to find the left and right singular vectors. But in a behaving animal  or operating robotic system it is generally not possible to compute the product with  P", since the plant is an unknown component of the system. In the following, I will  present two new iterative algorithms for finding the singular value decomposition  146 Sanger  Figure 2: Loop structure of the singular value decomposition for control. The  plant is P -- L'$_R, where R determines the mapping from control variables to  system modes, and L T determines the outputs produced by each mode. The optimal  sensory network is G - L, and the optimal motor network is N -/iT$ -1. /i and L  are shown as trapezoids to indicate that the number of nonzero elements of $ (the  "modes") may be less than the number of sensory variables y or motor variables u.  of a matrix P given only samples of the inputs u and outputs y.  3  THE DOUBLE GENERALIZED HEBBIAN  ALGORITHM  The first algorithm is the Double Generalized Hebbian Algorithm (DGHA), and it  is described by the two coupled difference equations  AG = ?(zy ' - LT[zzT]G) (1)  : '7(zu - LT[z]v (2)  where LT[ ] is an operator that sets the above diagonal elements of its matrix  argument to zero, y - Pu, z = Gy, and '7 is a learning rate constant.  Equation 1 is the Generalized Hebbian Algorithm (Sanger 1989) which finds the  eigenvectors of the autocorrelation matrix of its inputs y. For random uncorrelated  inputs u, the autocorrelation of y is E[yy r] = LrS2L, so equation 1 will cause  G to converge to the matrix of left singular vectors L. Equation 2 is related to  the Widrow-Hoff (1960) LMS rule for approximating u ' from z, but it enforces  orthogonality of the columns of N. It appears similar in form to equation 1, except  that the intermediate variables z are computed from y rather than u. A graphical  representation of the algorithm is given in figure 3. Equations 1 and 2 together  cause N to converge to /Tq-1 SO that the combination NG - trTS-1L is an  approximation to the plant inverse.  Theorem 1: (Sanger 1993) If y - Pu, z: Gy, and E[uu T]: I, then equations 1  and 2 converge to the left and right singular vectors of P.  Singular Value Decomposition 147  U ..I Y  "l, P  Figure 3: Graphic representation of the Double Generalized Ilebbian Algorithm.  G learns according to the usual GIlA rule, while N learns using an orthogonalized  form of the Widrow-Iloff LMS Rule.  Proof:  After convergence of equation 1, E[zz r] will be diagonal, so that E[LT[zzr]] =  E[zz:r]. Consider the Widrow-Iloff LMS rule for approximating u " from z:  zxv = ?(zu _ zrvr). (3)  After convergence of G, this will be equivalent to equation 2, and will converge to  the same attractor. The stable points of 3 occur when E[uz :r - Nzz r] = 0, for  which N = _R'S -1  The convergence behavior of the Double Generalized Hebbian Algorithm is shown in  figure 4. Results are measured by computing B -- GPN and determining whether  B is diagonal using a score  The reduction in e is shown as a function of the number of (u, y) examples given to  the network during training, and the curves in the figure represent the average over  100 training runs with different randomly-selected plant matrices P.  Note that the Double Generalized Hebbian Algorithm may perform poorly in the  presence of noise or uncontrollable modes. The sensory mapping G depends only on  the outputs y, and not directly on the plant inputs u. So if the outputs include noise  or autonomously varying uncontrollable modes, then the mapping G will respond  to these modes. This is not a problem if most of the variance in the output is due  the inputs u, since in that case the most significant output components will reflect  the input variance transmitted through P.  4 THE ORTHOGONAL ASYMMETRIC ENCODER  The second algorithm is the Orthogonal Asymmetric Encoder (OAE) which is de-  scribed by the equations  AG - ?(y' - LT[5"]G) (4)  148 Sanger  '? 1 Double Generalized Hebbian Algorithm  0.6  0.5 10x10  0.3   0.2   0,1   50 100 150 200 250 300 350 400 450 500 550 500 650 700 750 800 850 gO0 9  Example  Figure 4: Convergence of the Double Generalized Itebbian Algorithm averaged over  100 random choices of 3x3 or 10x10 matrices P.  where i = N"u.  AN r = 7(Gy- LT[GGr]i)u r  (5)  This algorithm uses a variant of the Backpropagation learning algorithm (Rumelhart  et al. 1986). It is named for the "Encoder" problem in which a three-layer network is  trained to approximate the identity mapping but is forced to use a narrow bottleneck  layer. I define the "Asymmetric Encoder Problem" as the case in which a mapping  other than the identity is to be learned while the data is passed through a bottleneck.  The "Orthogonal Asymmetric Encoder" (OAE) is the special case in which the  hidden units are forced to be uncorrelated over the data set. Figure 5 gives a  graphical depiction of the algorithm.  Theorem 2: (Sanger 1993) Equations  and 5 converge to the left and right singular  vectors of P.  Proof:  Suppose z has dimension m. If P = LTSi where the elements of S are distinct,  and E[uu T] = I, then a well-known property of the singular value decomposition  (Golub and Van Loan 1983, , for example) shows that  Z[llPu - c Null] (6)  is minimized when G " = LmU, N " = V-Rm, and U and V are any m x m matrices  for which UV -- ImS[r. (Lm and -Rm signify the matrices of only the first m  columns of L " or rows of R.) If we want E[zz '] to be diagonal, then U and V must  be diagonal. OAE accomplishes this by training the first hidden unit as if m -- 1,  the second as if m - 2, and so on.  For the case m -- 1, the error 6 is minimized when G is the first left singular vector  of P and N is the first right singular vector. Since this is a linear approximation  problem, there is a single global minimum to the error surface 6, and gradient  descent using the backpropagation algorithm will converge to this solution.  Singular Value Decomposition 149  Figure 5: The Orthogonal Asymmetric Encoder algorithm computes a forward ap-  proximation to the plant P through a bottleneck layer of hidden units.  After convergence, the remaining error is E[II(P-G r Nr)ull]. If we decompose the  plant matrix as  where li and ri are the rows of L and _R, and si are the diagonal elements of S, then  the remaining error is  i=2  which is equivalent to the original plant matrix with the first singular value set to  zero. If we train the second hidden unit using P2 instead of P, then minimization of  E[llpau-G r:vrull] will yield the second left and right singular vectors. Proceeding  in this way we can obtain the first m singular vectors.  Combining the update rules for all the singular vectors so that they learn in parallel  leads to the governing equations of the OAE algorithm which can be written in  matrix form as equations 4 and 5.  (Bannour and Azimi-Sadjadi 1993) proposed a similar technique for the symmet-  ric encoder problem in which each eigenvector is learned to convergence and then  subtracted from the data before learning the succeeding one. The orthogonal asym-  metric encoder is different because all the components learn simultaneously. After  convergence, we must multiply the learned N by S -2 in order to compute the plant  inverse. Figure 6 shows the performance of the algorithm averaged over 100 random  choices of matrix P.  Consider the case in which there may be noise in the measured outputs y. Since  the Orthogonal Asymmetric Encoder algorithm learns to approximate the forward  plant transformation from u to t, it will only be able to predict the components of  t which are related to the inputs u. In other words, the best approximation to t  based on u is t) m Pu, and this ignores the noise term. Figure 7 shows the results  of additive noise with an SNR of 1.0.  150 Sanger  0.7 '  0.6  0.5  0.4  0.3  Orthogonal Asymmetric Encoder    10x10  100 10 00 0 $00 $0 400 450 00 0 600 60 700 70 800 60 eO0 eO  Example  Figure 6: Convergence of the Orthogonal Asymmetric Encoder averaged over 100  random choices of 3x3 or 10x10 matrices P.  Acknowledgement s  This report describes research done within the laboratory of Dr. Emilio Bizzi in the  department of Brain and Cognitive Sciences at MIT. The author was supported dur-  ing this work by a National Defense Science and Engineering Graduate Fellowship,  and by NIH grants 5R37AR26710 and 5R01NS09343 to Dr. Bizzi.  References  Bannour S., Azimi-Sadjadi M. R., 1993, Principal component extraction using re~  cursive least squares learning, submitted to IEEE Transactions on Neural Networks.  Golub G. H., Van Loan C. F., 1983, Matrix Computations, North Oxford Academic  P., Oxford.  Rumelhart D. E., Hinton G. E., Williams R. J., 1986, Learning internal represen-  tations by error propagation, In Parallel Distributed Processing, chapter 8, pages  318-362, MIT Press, Cambridge, MA.  Sanger T. D., 1989, Optimal unsupervised learning in a single-layer linear feedfor-  ward neural network, Neural Networks, 2:459-473.  Sanger T. D., 1993, Theoretical Elements of Hierarchical Control in Vertebrate  Motor Systems, PhD thesis, MIT.  Widrow B., Hoff M. E., 1960, Adaptive switching circuits, In IRE WESCON Conv.  Record, Part , pages 96-104.  Singular Value Decomposition 151  OAE with 50% Added Noise   3x3 [  10x'10  50 100 150 200 250 300 350 400 4.;0 500 550 660 700 750 900 950  Example  Figure 7: Convergence of the Orthogonal Asymmetric Encoder with 50% additive  noise on the outputs, averaged over 100 random choices of 3x3 or 10x10 matrices  P.  
Generation of Internal Representation  by c-Transformation  Ryotaro Kamimura  Information Science Laboratory  Tokai University  1117 Kitakaname Hiratsuka Kanagawa 259-12, Japan  Abstract  In the present paper, we propose an entropy method to transform  the internal representation. The entropy function is defined with  respect to the state of hidden unit, that is, internal representation.  The internal representation can be transformed by changing the  parameter c for the entropy function. Thus, the transformation is  referred to as -transformation. The internal representation can  be transformed according to given problems. By transforming the  internal representation into the minimum entropy representation,  we can obtain kernel networks, smaller networks with explicit in-  terpretation. On the other hand, by changing appropriately the  parameter a, we can obtain intermediate internal representations  for the improved generalization. We applied the entropy method  to an autoencoder and we succeeded in obtaining kernel networks  with small internal entropy. In addition, we applied the method  to the frequency identification problem and we could obtain de-  rived networks whose generalization performance was significantly  superior to the performance by standard back-propagation.  I Introduction  1.1 Creation of Internal Representation  One of the most important characteristics in the learning by neural networks is  that networks can create appropriate internal representations in the course of the  271  272 Kamimura  learning[6]. By these internal representations, networks can solve multiple problems.  However, little attention has been given to the problem, regarding what kinds of  internal representations networks can create or more strongly what kind of internal  representation networks should make. There have been some works [2], [4] [5] and  [7] concerning the learning of the internal representation, in which the internal rep-  resentation is not automatically generated. However, there has been little discussion  regarding the quality or characteristics of obtained internal representations.  1.2 Objective  In this context, the objective of my paper is to define an entropy function for  the internal representation and to formulate an entropy method to transform the  internal representation according to given problems or targets, for example, the  improvement of the interpretability of networks' behaviors or coding strategies, and  the improvement of the generalization performance. To interpret explicitly the  internal representation, and networks' behaviors, the entropy should be minimized.  On the other hand, for the improved generalization performance, the entropy should  appropriately be changed according to given problems.  1.3 Internal Entropy  Let us explain the entropy function, used in this paper. Entropy H is defined with  respect to the hidden unit activity,  M  i=1  where pi is a normalized activity of ith hidden unit and the summation is only  over all the hidden units (M hidden units). This entropy is referred to as internal  entropy, because the entropy function is defined with respect to the internal rep-  resentation. If this entropy is minimized, only one hidden unit is turned on, while  all the other hidden units are turned off by multiple strong inhibitory connections  [3]. On the other hand, if entropy is maximized, all the hidden units are equally  activated. If entropy is sufficiently decreased, only a small number of hidden units  are turned on, while all the other units are off and not used for producing outputs.  Thus, this entropy function can be used to detect unnecessary hidden units to be  eliminated, and to construct simple networks.  1.4 Transformation of Internal Representation  Let us briefly outline our ideas of the transformation of internal representtions by  the internal entropy. If the internal entropy is minimized, only one hidden unit  is activated by an input pattern. With maximum internal entropy, all the hidden  units are activated by an input pattern. Let us look at Figure 1, representing the  state of internal representation. If we minimize the internal entropy, we can obtain  the internal representation with minimum entropy. On the other hand, if entropy is  maximized, all the hidden units are equally activated. In addition to two extreme  representations, we can have multiple intermediate internal representations.  Generation of Internal Representation by a-Transformation 273  Ninimum Entropy  IR  Intermediate Representations  Flax imum Entrop9  Figure 1: a-Transformation of internal representation into minimum  entropy representation for the good interpretation, into maximum en-  tropy representation and intermediate representations for the improved  generalization.  For the good interpretability of the internal representation, the internal entropy  should be minimized. When the internal entropy is minimized, only one hidden  unit is turned on, while all the other units are of[. Since only one input pattern  can activate the hidden unit, it is sure that we can easily understand the meaning  of the hidden unit. The hidden unit represents the information regarding the given  input pattern. In a state of minimum entropy, only a small number of hidden units  tend to be used. Thus, by minimizing the internal entropy, we can obtain the  smallest network architecture. We call these hidden units kernel hidden nits, and  the derived network is referred to as kernel network. The procedure to obtain the  kernel network is called kernel hidden unit analysis.  Concerning the generalization performance, we think that to reduce the network  size is not enough to improve the generalization, as demonstrated in several exper-  imental results, for example [1], [8]. It is necessary to adjust appropriately network  architectures to given problems for the improved generalization performance. For  example, in the case of the minimum entropy representation, the generalization  performance is not expected to be improved because the minimum entropy repre-  sentation tends to be a local representation and can not appropriately represent  the similarity of input patterns. If we need the good generalization performance,  the information concerning input patterns should be distributed over several hid-  den units. We think that we need intermediate representations for the improved  generalization (see Figure 1).  274 Kamimura  2 Theory and Computational Methods  2.1 Entropy Method  We have applied entropy minimization method to recurrent back-propagation[3].  In this section, we formulate the entropy method for standard back-propagation.  Suppose that a network is composed of three layers: input, competitive hidden and  output layers. Hidden units are denoted by vi and input terminals by Ed. Then,  connections from inputs to hidden units are denoted by wij and connections from  hidden units to output units are denoted by Wij. A hidden unit produces an output  =  where  N  j=l  where i is a ith element of an input pattern, N is the number of elements in the  pattern and f is the sigmoid function, defined by  1  f(i) --  1 + e -'  An entropy function on competitive hidden layer is defined by  M  - (2)  i--1  where  vi  Vr  and M is the number of competitive hidden units.  Differentiating entropy function with respect to connections from input to hidden  layer, we have  where  OH OH Ovi  OWij OVi OWij  = (a)  qi = (logpi + 1)pi(1 - pi)(1 - vl).  (4)  By using this q function, update rules can be summarized as follows. First, for  connections from competitive hidden units to output units, only delta rule must be  used. For connections from input units to competitive hidden units, in addition to  delta rule, the q5 function must be incorporated as  OH OE  Awij '- --oz OtOi j --/ OWij  This update rule means that in addition to the error minimization, entropy must  be minimized or maximized in the course of the learning by changing the parameter  Generation of Internal Representation by c-Transformation 275  2.2 Kernel Hidden Unit Analysis  As already mentioned, we have used the entropy method for obtaining simple net-  works, called kernel networks. Let us briefly explain the procedure of kernel hidden  unit analysis. We have observed that by minimizing entropy, we can obl, ain small  network architectures, compared with original oversized network architecture. In  a state of minimum entropy, a smaller number of hidden units tended to be used  to produced targets. We call these units kernel hidden units. To deterlnine kernel  hidden units, we have introduced the variance of input-hidden connections. The  variance of input-hidden connections is used to measure how a given hidden units  is important for producing targets correctly. This variance is used because it has  extensively been observed in our experiments that hidden units playing the impor-  tant roles tend to have the large variance, compared with other unimportant hidden  units. The variance (s) of ith hidden unit is defined by  M  S;=M_ 1 ,  j=l  where M is the number of hidden units, W is an average over all the connections  into ith hidden units. With these kernel hidden units, small networks, called kernel  networks, can be obtained, whose performance with respect to the error minimiza-  tion is completely equivalent to the original networks with a large number of hidden  units. Moreover, we can easily interpret the behaviors of kernel networks, because  the network size is small. Finally, this process of obtaining a small network archi-  tecture is just the kernel hidden unit analysis.  3 Results  3.1 Transformation into Kernel Networks  We applied the method to a network in which thirty-five input, hidden and output  units were employed. The network must exactly reproduce five alphabet letters: B,  C, D, E, F, G at output units. Since the difference between these letters are small,  compared with the difference between other letters, these letters are expected to be  compressed into a smaller number of hidden units.  A minimum entropy was searched by changing the parameter a. Entropy was  decreased gradually as the parameter was increased as shown in Figure 2. By  using kernel hidden unit analysis, we observed only three major hidden units for  entropy method. Thus, a kernel network is composed of three kernel hidden units,  compared with thirty-five hidden units of the original network. On the other hand,  by using standard back-propagation, many hidden units are activated, and thus the  information upon input patterns are distributed over many hidden units. Figure  3 shows original network and kernel network for the autoencoder. As you can see  from the figure, the number of hidden units is decreased from thirty-five to three  by using the kernel hidden unit analysis.  Finally, to see clearly the meaning of hidden units, networks were constructed only  with kernel hidden units, and the outputs, generated by the networks were carefully  examined. We could see that the role of hidden units could explicitly be determined.  276 Kamimura  Entropy  0.9  0.8  0.7  0.6  0.5  0.4  0.3  0.2  0.1  0  Figure 2  function  0 0.05 0.1 0.15 0.2 0.25 0.3 0.35  : Entropies, computed with four different initial values as a  of the parameter a for an autoencoder.  Original Network  C) Input Unit  Hidden Unit  Output Unit  c  I{ernel Net, uork  Figure 3: Original network and kernel network for the autoencoder.  Generation of Internal Representation by a-Transformation 277  Table 1: Summary of experiments for networks with fifteen hidden  units. Values in this table were averages over ten initial conditions.  ..c(x10 -a) Entropy HD(%) SSE  0 0.623 6.296 4.118  -2.031 0.778 1.111  -3.250 0.785 1.687  3.2 Transformation for the Improved Generalization  As already mentioned, we think that the generalization can be improved by adjust-  ing network architectures to given problems appropriately. In this section, we try  to change the parameter c and thus to change the internal entropy to obtain the  improved generalization performance. Our main result, presented in this section  is that the generalization performance can be improved by increasing the entropy  function.  We applied our method to the identification of frequencies[8]. Networks must iden-  tify three frequencies of sine waves with phases, different from those of training  data sets. First, training data were divided into three classes with three different  frequencies (2, 4, 6). Each class has ten exemplars with sixty-four samples from  sine waves. Thus, the number of input units was sixty-four. The number of output  units was three and specific target values were assigned to each output unit, accord-  ing to the frequency of a class. Experiments were performed with different initial  values for weights, ranging between -0.5 and 0.5. The learning was considered to be  finished when the absolute differences between targets and outputs were all below  0.1. Finally, the learning rate fi was set to 0.1 for all the experiments.  Let us see Table 1, showing the summary of experimental results with ten and  fifteen units by ten different initial conditions. Values in the second row in the  table could give the lowest values of Hamming distance. In the same way, values in  the third row could give the lowest SSE. The number of hidden units was fifteen.  As you can see from the table, when entropy was increased from 0.623 to 0.778,  Hamming distance was decreased greatly from 6.296 to 1.111. In addition, five out  of ten initial conditions could produce zero Hamming distance, meaning that the  generalization performance was perfect. By the criterion of SSE, when entropy was  increased from 0.623 to 0.785, SSE was decreased from 4.118 to 1.687. All the  results are statistical significant. These results show that by using entropy method,  the generalization performance was significantly improved.  4 Conclusion  In this paper, we have proposed an entropy method to transform the internal rep-  resentation. The internal entropy has been defined with respect to the hidden unit  activity or the internal representation. If the internal entropy is minimized, only  one hidden unit tends to respond to a specific input pattern, thus, the internal rep-  resentation is as local as possible. In this case, it is easy to understand the meaning  or the function of the hidden unit, because the hidden unit tends to respond to  278 Kamimura  only one specific input pattern. However, we have observed that in the case of tile  minimum entropy representation, the generalization performance is not improved.  To obtain the improved generalization performance, the information regarding an  input pattern should be distributed over several hidden units. We have applied the  entropy method to an autoencoder. In this problem, we have observed that by milli-  mizing the internal entropy, we can obtain smaller networks, called kernel networks.  These kernel networks are so small that we can easily understand the meaning of  the internal representation or networks' behaviors. Then, we have attempted to  change the internal entropy for the improve generalization. We have observed that  up to a certain point, the generalization performance can be improved by increasing  the internal entropy. Finally, if relations between kernel and intermediate networks  can explicitly be determined, we can obtain networks with the good interpretability  and the improved generalization performance.  References  [1] Y. Chauvin, "A backpropagation algorithm with optimal use of hidden units,"  in Advances in Neural Information Processing Systems, D. S. Touretzky, Eds,  San Mateo: CA, pp.519-526, 1989.  [2] T. Grossman, "The CHIR. algorithm for feed forward networks with binary  weights," in Advances in Neural Information Processing Systems, D. S. Touret-  zky, Eds, Vol.3, San Mateo: CA, pp.516-523, 1991.  [3] I{. Kamimura, "Minimum entropy method in neural networks," in P.,'oceeding  of 1993 ItlE International Conference on Neural Networks, Vol.l, pp.219-225,  1993.  [4] A. Kroph, G.I. Thorb%sson and J. A. Hertz, "A cost function of internal  representation," in Advances in Neural Information Processing Systems, D. S.  Touretzky, Eds, Vol.3, San Mateo: CA, pp.733-740, 1991.  [5] I{. R.ohwer, "The moving target training algorithm," in Advances in Neural  Infqrmation Processing Systems, D. S. Touretzky, Eds, Vol.3, San Mateo: CA,  pp.558-565, 1991.  [6] D. E. lumelhart, G. E. Hinton and I{. J. Williams, "Learning internal rep-  resentation by error propagation," in Parallel Distributed Processig, D. E.  Pumelhart, J. L. McClelland, and the PDP Research Group, Cambridge, Mas-  sachusetts: the MIT Press, Vol.l, pp.318-362, 1986.  [7] D. Saad and E. Marom, "Learning by choice of internal representttion: an  energy minimization approach," Complez Systems, Vol.4, pp.107-118, 1990.  [8] J. Sietsma and R.. J. F. Dow, "Creating artificial neural networks that gener-  alize," Neural Networks, Vol.4, pp.67-79, 1991.  
COMBINED NEURAL NETWORKS  FOR TIME SERIES ANALYSIS  Iris Ginzburg and David Horn  School of Physics and Astronomy  Raymond and Beverly Sackler Faculty of Exact Science  Tel-Aviv University  Tel-Aviv 96678, Israel  Abstract  We propose a method for improving the performance of any net-  work designed to predict the next value of a time series. We advo-  cate analyzing the deviations of the network's predictions from the  data in the training set. This can be carried out by a secondary net-  work trained on the time series of these residuals. The combined  system of the two networks is viewed as the new predictor. We  demonstrate the simplicity and success of this method, by apply-  ing it to the sunspots data. The small corrections of the secondary  network can be regarded as resulting from a Taylor expansion of  a complex network which includes the combined system. We find  that the complex network is more difficult to train and performs  worse than the two-step procedure of the combined system.  1 INTRODUCTION  The use of neural networks for computational tasks is based on the idea that the  efficient way in which the nervous system handles memory and cognition is worth  immitating. Artificial implementations are often based on a single network of math-  ematical neurons. We note, however, that in biological systems one can find collec-  tions of consecutive networks, performing a complicated task in several stages, with  later stages refining the performance of earlier ones. Here we propose to follow this  strategy in artificial applications.  224  Combined Neural Networks for Time Series Analysis 225  We study the analysis of time series, where the problem is to predict the next ele-  ment on the basis of previous elements of the series. One looks then for a functional  relation  yn= f(yn_x,yn_2,...,yn_m). (1)  This type of representation is particularly useful for the study of dynamical sys-  tems. These are characterized by a common continuous variable, time, and many  correlated degrees of freedom which combine into a set of differential equations.  Nonetheless, each variable can in principle be described by a lag-space representa-  tion of the type I . This is valid even if the y = y(t) solution is unpredictable as in  chaotic phenomena.  Weigend Huberman and Rumelhart (1990) have studied the experimental series  of yearly averages of sunspots activity using this approach. They have realized  the lag-space representation on an (m, d, 1) network, where the notation implies a  hidden layer of d sigmoidal neurons and one linear output. Using m = 12 and a  weight-elimination method which led to d = 3, they obtained results which compare  favorably with the leading statistical model (Tong and Lim, 1980). Both models do  well in predicting the next element of the sunspots series. Recently, Nowlan and  Hinton (1992) have shown that a significantly better network can be obtained if the  training procedure includes a complexity penalty term in which the distribution of  weights is modelled as a mixture of multiple gaussians whose parameters vary in an  adaptive manner as the system is being trained.  We propose an alternative method which is capable of improving the performance  of neural networks: train another network to predict the errors of the first one, to  uncover and remove systematic correlations that may be found in the solution given  by the trained network, thus correcting the original predictions. This is in agreement  with the general philosophy mentioned at the beginning, where we take from Nature  the idea that the task does not have to be performed by one complicated network; it  is advantageous to break it into stages of consecutive analysis steps. Starting with  a network which is trained on the sunspots data with back-propagation, we show  that the processed results improve considerably and we find solutions which match  the performance of Weigend et. al.  2 CONSTRUCTION OF THE PRIMARY NETWORK  Let us start with a simple application of back-propagation to the construction of  a neural network describing the sunspots data which are normalized to lie between  0 and 1. The network is assumed to have one hidden layer of sigmoidal neurons,  hi i - 1,..., d, which receives the input of the nth vector:  m  = - o,)  j=l  The output of the network, p, is constructed linearly,  d  p, = Y wihi - O.  i=1  226 Ginzburg and Horn  The error-function which we minimize is defined by  N  1 )2  E= ' (p,y,(4)  where we try to equate p,, the prediction or output of the network, with y,, the  nth value of the series. This is the appropriate formulation for a training set of N  data points which are viewed as N - m strings of length m used to predict the point  following each string.  We will work with two sets of data points. One will be labelled T and be used for  training the network, and the other P will be used for testing its predictive power.  Let us define the average error by  = - (5)  where the set $ is either T or P. An alternative parameter was used by Weigend et.  al. , in which the error is normalized by the standard deviation of the data. This  leads to an average relative variance (arv) which is related to the average error  through  arvs = a'. (6)  Following Weigend et. al. we choose m = 12 neurons in the first layer and  [ITII = 220 data points for the training set. The following IIPII = 35 years are  used for testing the predictions of our network. We use three sigmoidal units in the  hidden layer and run with a slow convergence rate for 7000 periods. This is roughly  where cross-validation would indicate that a minimum is reached. The starting  parameters of our networks are chosen randomly. Five examples of such networks  are presented in Table 1.  3 THE SECONDARY NETWORK  Given the networks constructed above, we investigate their deviations from the  desired values  q. = y. - p.. (7)  A standard statistical test for the quality of any predictor is the analysis of the  correlations between consecutive errors. If such correlations are found, the predictor  must be improved. The correlations reflect a systematic deviation of the primary  network from the true solution. We propose not to improve the primary network  by modifying its architecture but to add to it a secondary network which uses the  residuals qn as its new data. The latter is being trained only after the training  session of the primary network has been completed.  Clearly one may expect some general relation of the type  qn = f(qn-x,qn-2,''',qn-l;Yn-,Yn-2,''',Yn-l) (8)  to exist. Looking for a structure of this kind enlarges considerably the original  space in which we searched for a solution to 1 . We wish the secondary network  Combined Neural Networks for Time Series Analysis 227  to do a modest task, therefore we assume that much can be gained by looking at  the interdependence of the residuals q, on themselves. This reduces the problem to  finding the best values of  rn = fl(qn-l,qn-2,'",qn-l) (9)  which would minimize the new error function  N  1  E2=  y (rn-qn) 2. (10)  Alternatively, one may try to express the residual in terms of the functional values  r = f2(Y-l,Y2," ', Yn-l) (11)  minimizing again the expression 10 .  When the secondary network completes its training, we propose to view  (12)  as the new prediction of the combined system. We will demonstrate that a major  improvement can be obtained already with a linear perceptron. This means that  the linear regression  = +  i=1  or  rn  y oq2 yn-i q- f12  (14)  i---1  is sufficient to account for a large fraction of the systematic deviations of the primary  networks from the true function that they were trained to represent.  4 NUMERICAL RESULTS  We present in Table 1 five examples of results of (12,5,1) networks, i.e. m = 12  inputs, a hidden layer of three sigmoidal neurons and a linear output neuron. These  five examples were chosen from 100 runs of simple back-propagation networks with  random initial conditions by selecting the networks with the smallest R values  (Ginzburg and Horn, 1992). This is a weak constraint which is based on letting  the network generate a large sequence of data by iterating its own predictions, and  selecting the networks whose distribution of function values is the closest to the  corresponding distribution of the training set.  The errors of the primary networks, in particular those of the prediction set ee, are  quite higher than those quoted by Weigend et. al. who started out from a (12,8,1)  network and brought it down through a weight elimination technique to a (12,5,1)  structure. They have obtained the values eT = 0.059 er = 0.06. We can reduce our  errors and reach the same range by activating a secondary network with I = 11 to  perform the linear regression (3.6) on the residuals of the predictions of the primary  network. The results are the primed errors quoted in the table. Characteristically  we observe a reduction of eT by 3 - 4% and a reduction of er by more than 10%.  228 Ginzburg and Horn  1 0.0614 0.0587 0.0716 0.0620  2 0.0600 0.0585 0.0721 0.0663  3 0.0611 0.0580 0.0715 0.0621  4 0.0621 0.0594 0.0698 0.0614  5 0.0616 0.0589 0.0681 0.0604  Table 1  Error parameters of five networks. The unprimed errors are those of the primary  networks. The primed errors correspond to the combined system which includes  correction of the residuals by a linear perceptton with I = 11 , which is an autore-  gressions of the residuals. Slightly better results for the short term predictions are  achieved by corrections based on regression of the residuals on the original input  vectors, when the regression length is 13 (Table 2).  1 0.061 0.059 0.072 0.062  2 0.060 0.059 0.072 0.065  3 0.061 0.058 0.072 0.062  4 0.062 0.060 0.070 0.061  5 0.062 0.059 0.068 0.059  Table 2  Error parameters for the same five networks. The primed errors correspond to the  combined system which includes correction of the residuals by a linear perceptron  based on original input vectors with I = 13.  5 LONG TERM PREDICTIONS  When short term prediction is performed, the output of the original network is  corrected by the error predicted by the secondary network. This can be easily gen-  eralized to perform long term predictions by feeding the corrected output produced  by the combined system of both networks back as input to the primary network. The  corrected residuals predicted by the secondary network are viewed as the residuals  needed as further inputs if the secondary network is the one performing autore-  gression of residuals. We run both systems based on regression on residuals and  regression on functional values to produce long term predictions.  In table 3 we present the results of this procedure for the case of a secondary  network performing regression on residuals. The errors of the long term predictions  are averaged over the test set P of the next 35 years. We see that the errors of  the primary networks are reduced by about 20%. The quality of these long term  predictions is within the range of results presented by Weigend et. al. Using the  regression on (predicted) functional values, as in Eq. 14 , the results are improved  by up to 15% as shown in Table 4.  Combined Neural Networks for Time Series Analysis 229  1 0.118 0.098 0.162 0.109 0.150 0.116  2 0.118 0.106 0.164 0.125 0.131 0.101  3 0.117 0.099 0.164 0.112 0.136 0.099  4 0.116 0.099 0.152 0.107 0.146 0.120  5 0.113 0.097 0.159 0.112 0.147 0.123  Table 3  Long term predictions into the future. en denotes the average error of n time steps  predictions over the P set. The unprimed errors are those of the primary networks.  The primed errors correspond to the combined system which includes correction of  the residuals by a linear perceptron.  1 0.118 0.098 0.162 0.107 0.150 0.101  2 0.118 0.104 0.164 0.117 0.131 0.089  3 0.117 0.098 0.164 0.108 0.136 0.086  4 0.117 0.098 0.152 0.105 0.146 0.105  5 0.113 0.096 0.159 0.110 0.147 0.109  Table 4  Long term predictions into the future. The primed errors correspond to the com-  bined system which includes correction of the residuals by a linear perceptron based  on the original inputs.  6 THE COMPLEX NETWORK  Since the corrections of the secondary network are much smaller than the charac-  teristic weights of the primary network, the corrections can be regarded as resulting  from a Taylor expansion of a complex network which include the combined system.  This can be simply implemented in the case of Eq. 14 which can be incorporated in  the complex network as direct linear connections from the input layer to the output  neuron, in addition to the non-linear hidden layer, i.e.,  d  i-1 j=l  We train such a complex network on the same problem to see how it compares with  the two-step approach of the combined networks described in the previous chapters.  The results depend strongly on the training rates of the direct connections, as  compared with the training rates of the primary connections (i.e. those of the  primary network). When the direct connections are trained faster than the primary  ones, the result is a network that resembles a linear perceptton, with non-linear  230 Ginzburg and Horn  corrections. In this case, the assumption of the direct connections being small  corrections to the primary ones no longer holds. The training error and prediction  capability of such a network are worse than those of the primary network. On the  other hand, when the primary connections are trained using a faster training rate,  we expect the final network to be similar in nature to the combined system. Still,  the quality of training and prediction of these solutions is not as good as the quality  of the combined system, unless a big effort is made to find the correct rates. Typical  results of the various systems are presented in Table 5.  type of network  (t p  primary network 0.061  learning rate of linear weights = 0.1 0.062  learning rate of linear weights = 0.02 0.061  combined system 0.058  0.072  0.095  0.068  0.062  Table 5  Short term predictions of various networks. The learning rate of primary weights  is 0.04.  The performance of the complex network can be better than that of the primary  network by itself, but it is surpassed by the achievements of the combined system.  7 DISCUSSION  It is well known that increasing the complexity of a network is not the guaranteed  solution to better performance (Geman et. al. 1992). In this paper we propose an  alternative which increases very little the number of free parameters, and focuses on  the residual errors one wants to eliminate. Still one may raise the question whether  this cannot be achieved in one complex network. It can, provided we are allowed to  use different updating rates for different connections. In the extreme limit in which  one rate supersedes by far the other one, this is equivalent to a disjoint architecture  of a combined two-step system. This emphasizes the point that a solution of a  feedforward network to any given task depends on the architecture of the network  as well as on its training procedure.  The secondary network which we have used was linear, hence it defined a simple  regression of the residual on a series of residuals or a series of function values. In  both cases the minimum which the network looks for is unique. In the case in  which the residual is expressed as a regression on function values, the problem can  be recast in a complex architecture. However, the combined procedure guarantees  that the linear weights will be small, i.e. we look for a small linear correction to the  prediction of the primary network. If one trains all weights of the complex network  at the same rate this condition is not met, hence the worse results.  We advocate therefore the use of the two-step procedure of the combined set of  networks. We note that combined set of networks. We note that the secondary  networks perform well on all possible tests: they reduce the training errors, they  Combined Neural Networks for Time Series Analysis 231  improve short term predictions and they do better on long term predictions as well.  Since this approach is quite general and can be applied to any time-series forecasting  problem, we believe it should be always tried as a correction procedure.  REFERENCES  Geman, S., Bienenstock, E., & Doursat, R., 1992. Neural networks and the  bias/variance dilemma. Neural Comp. 4, 1-58.  Ginzburg, I. & Horn, D. 1992. Learning the rule of a time series. Int. Journal of  Neural Systems 3, 167-177.  Nowlan, S. J.  Hinton, G. E. 1992. Simplifying neural networks by soft weight-  sharing. Neural Comp. 4,473-493.  Tong, H.,  Lira, K. S., 1980. Threshold autoregression, limit cycles and cyclical  data. J. P. Star. Soc. B 42,245.  Weigend, A. S., Huberman, B. A. & Rumelhart, D. E., 1990. Predicting the Future:  A Connectionist Approach, Int. Journal of Neural Systems 1,193-209.  
Developing Population Codes By  Minimizing Description Length  Richard S. Zemel  CNL, The Salk Institute  10010 North Torrey Pines Rd.  La Jolla, CA 92037  Geoffrey E. Hinton  Department of Computer Science  University of Toronto  Toronto M5S 1A4 Canada  Abstract  The Minimum Description Length principle (MDL) can be used to  train the hidden units of a neural network to extract a representa-  tion that is cheap to describe but nonetheless allows the input to  be reconstructed accurately. We show how MDL can be used to  develop highly redundant population codes. Each hidden unit has  a location in a low-dimensional implicit space. If the hidden unit  activities form a bump of a standard shape in this space, they can  be cheaply encoded by the center of this bump. So the weights from  the input units to the hidden units in an autoencoder are trained  to make the activities form a standard bump. The coordinates of  the hidden units in the implicit space are also learned, thus allow-  ing flexibility, as the network develops a discontinuous topography  when presented with different input classes. Population-coding in  a space other than the input enables a network to extract nonlinear  higher-order properties of the inputs.  Most existing unsupervised learning algorithms can be understood using the Min-  imum Description Length (MDL) principle (Rissanen, 1989). Given an ensemble  of input vectors, the aim of the learning algorithm is to find a method of coding  each input vector that minimizes the total cost, in bits, of communicating the input  vectors to a receiver. There are three terms in the total description length:   The code-cost is the number of bits required to communicate the code  that the algorithm assigns to each input vector.  ll  12 Zemel and Hinton   The model-cost is the number of bits required to specify how to recon-  struct input vectors from codes (e.g., the hidden-to-output weights).   The reconstruction-error is the number of bits required to fix up any  errors that occur when the input vector is reconstructed from its code.  Formulating the problem in terms of a communication model allows us to derive an  objective function for a network (note that we are not actually sending the bits).  For example, in competitive learning (vector quantization), the code is the identity  of the winning hidden unit, so by limiting the system to 7/ units we limit the  average code-cost to at most log 2 7/bits. The reconstruction-error is proportional  to the squared difference between the input vector and the weight-vector of the  winner, and this is what competitive learning algorithms minimize. The model-cost  is usually ignored.  The representations produced by vector quantization contain very little information  about the input (at most log s 7/bits). To get richer representations we must allow  many hidden units to be active at once and to have varying activity levels. Principal  components analysis (PCA) achieves this for linear mappings from inputs to codes.  It can be viewed as a version of MDL in which we limit the code-cost by only  having a few hidden units, and ignoring the model-cost and the accuracy with which  the hidden activities must be coded. An autoencoder (see Figure 2) that tries to  reconstruct the input vector on its output units will perform a version of PCA if the  output units are linear. We can obtain novel and interesting unsupervised learning  algorithms using this MDL approach by considering various alternative methods of  communicating the hidden activities. The algorithms can all be implemented by  backpropagating the derivative of the code-cost for the hidden units in addition to  the derivative of the reconstruction-error backpropagated from the output units.  Any method that communicates each hidden activity separately and independently  will tend to lead to factorial codes because any mutual information between hidden  units will cause redundancy in the communicated message, so the pressure to keep  the message short will squeeze out the redundancy. In (Zemel, 1993) and (Hinton  and Zemel, 1994), we present algorithms derived from this MDL approach aimed  at developing factoriM codes. Although factoriM codes are interesting, they are not  robust against hardware failure nor do they resemble the population codes found in  some parts of the brain. Our aim in this paper is to show how the MDL approach  can be used to develop population codes in which the activities of hidden units are  highly correlated. For a more complete discussion of the details of this algorithm,  see (Zemel, 1993).  Unsupervised algorithms contain an implicit assumption about the nature of the  structure or constraints underlying the input set. For example, competitive learning  algorithms are suited to datasets in which each input can be attributed to one of  a set of possible causes. In the algorithm we present here, we assume that each  input can be described as a point in a low-dimensional continuous constraint space.  For instance, a complex shape may require a detailed representation, but a set of  images of that shape from multiple viewpoints can be concisely represented by first  describing the shape, and then encoding each instance as a point in the constraint  space spanned by the viewing parameters. Our goal is to find and represent the  constraint space underlying high-dimensional data samples.  Developing Population Codes by Minimizing Description Length 13  size    0       0X0 *  orientatior  Figure 1: The population code for an instance in a two-dimensional implicit space.  The position of each blob corresponds to the position of a unit within the population,  and the blob size corresponds to the unit's activity. Here one dimension describes  the size and the other the orientation of a shape. We can determine the instantiation  parameters of this particular shape by computing the center of gravity of the blob  activities, marked here by an "X".  I POPULATION CODES  In order to represent inputs as points drawn from a constraint space, we choose  a population code style of representation. In a population code, each code unit is  associated with a position in what we call the implicit space, and the code units'  pattern of activity conveys a single point in this space. This implicit space should  correspond to the constraint space. For example, suppose that each code unit  is assigned a position in a two-dimensional implicit space, where one dimension  corresponds to the size of the shape and the second to its orientation in the image  (see Figure 1). A population of code units broadly-tuned to different positions can  represent any particular instance of the shape by their relative activity levels.  This example illustrates that population codes involve three quite different spaces:  the input-vector space (the pixel intensities in the example); the hidden-vector space  (where each hidden, or code unit entails an additional dimension); and this third,  low-dimensional space which we term the implicit space. In a learning algorithm  for population codes, this implicit space is intended to come to smoothly represent  the underlying dimensions of variability in the inputs, i.e., the constraint space.  For instance, the Kohonen (1982) algorithm defines the implicit space topology  through fixed neighborhood relations, and the algorithm then manipulates hidden-  vector space so that neighbors in implicit space respond to similar inputs.  This form of coding has several computational advantages, in addition to its signif-  icance due to its prevalence in biological systems. Population codes contain some  redundancy and hence have some degree of fault-tolerance, and they reflect under-  lying structure of the input, in that similar inputs are mapped to nearby implicit  positions. They also possess a hyperacuity property, as the number of implicit  positions that can be represented far exceeds the number of code units.  14 Zemel and Hinton  2 LEARNING POPULATION CODES WITH MDL  Autoencoders are a general way of addressing issues of coding, in which the hidden  unit activities for an input are the codes for that input which are produced by the  input-hidden weights, and in which reconstruction from the code is done by the  hidden-output mapping. In order to allow an autoencoder to develop population  codes for an input set, we need some additional structure in the hidden layer that  will allow a code vector to be interpreted as a point in implicit space. While most  topographic-map formation algorithms (e.g., the Kohonen and elastic net (Durbin  and Willshaw, 1987) algorithms) define the topology of this implicit space by fixed  neighborhood relations, in our algorithm we use a more explicit representation.  Each hidden unit has weights coming from the input units that determine its activity  level. But in addition to these weights, it has another set of adjustable parameters  that represent its coordinates in the implicit space. To determine what implicit  position is represented by a vector of hidden activities, we can average together the  implicit coordinates of the hidden units, weighting each coordinate vector by the  activity level of the unit.  Suppose, for example, that each hidden unit is connected to an 8x8 retina and has  2 implicit coordinates that represent the size and orientation of a particular kind of  shape on the retina, as in our earlier example. If we plot the hidden activity levels  in the implicit space (not the input space), we would like to see a bump of activity  of a standard shape (e.g., a Gaussian) whose center represents the instantiation  parameters of the shape (Figure 2 depicts this for a 1D implicit space). If the  activities form a perfect Gaussian bump of fixed variance we can communicate  them by simply communicating the coordinates of the mean of the Gaussian; this  is very economical if there are many less implicit coordinates than hidden units.  It is important to realize that the activity of a hidden unit is actually caused by the  input-to-hidden weights, but by setting these weights appropriately we can make  the activity match the height under the Gaussian in implicit space. If the activity  bump is not quite perfect, we must also encode the bump-error--the misfit between  the actual activity levels and the levels predicted by the Gaussian bump. The  cost of encoding this misfit is what forces the activity bump in implicit space to  approximate a Gaussian.  The reconstruction-error is then the deviation of the output from the input. This  reconstruction ignores implicit space; the output activities only depend on the vector  of hidden activities and weights.  2.1 The objective function  Currently, we ignore the model-cost, so the description length to be minimized is:  E t = B t + R t  7-t N  j=l k----1  where a, b, c are the activities of units in the input, hidden, and output layers,  respectively, VB and Vn are the fixed variances of the Gaussians used for coding the  Developing Population Codes by Minimizing Description Length 15  NETWOIII<  IMPLICIT SPACE (u = 1)  Output  (t...N)  lliddcn  (I...H)  hip.t,  (L..N)  0 0 "' 0 0  Activity (b)  X3 XI X6  x: t x4 xs x7 x.  l'osition (x)  best-fit  Gaussian  Figure 2' Each of the 7/hidden units in the autoencoder has an associated position  in implicit space. Here we show a 1D implicit space. The activity b of each hidden  unit j on case t is shown by a solid line. The network fits the best Gaussian to this  pattern of activity in implicit space. The predicted activity, , of unit j under this  Gaussian is based on the distance from xj to the mean ttt; it serves as a target for  bump-errors and the reconstruction-errors, and the other symbols are explained in  the caption of Figure 2.  We compute the actual activity of a hidden unit, b, as a normalized exponential  of its total input.  Note that a unit's actual activity is independent of its position  in implicit space. Its expected activity is its normalized value under the predicted  Gaussian bump:  ^t  b5 = exp(-(xj - )/2o'2)/ exp(-(x i -/t)e/2e) (2)  i=1  where r is the width of the bump, which we assume for now is fixed throughout  training.  We have explored several methods for computing the mean of this bump. Simply  computing the center of gravity of the representation units' positions, weighted  by their activity, produces a bias towards points in the center of implicit space.  Instead, on each case, a separate minimization determines ttt; it is the position in  implicit space that minimizes B t given {xj, b }. The network has full inter-layer  connectivity, and linear output units. Both the network weights and the implicit  coordinates of the hidden units are adapted to minimize E.  b = exp(net)/,% exp(net[), where net is the net input into unit j on case  16 Zemel and Hinton  0,08  0.06  Activity0.04  0.02  Unit 18 - Epoch 0  Unit 18 - Epoch 23  o.2 :2  0.15  X position 8 position  10 10  Figure 3: This figure shows the receptive field in implicit space for a hidden unit.  The left panel shows that before learning, the unit responds randomly to 100 differ-  ent test patterns, generated by positioning a shape in the image at each point in a  10x10 grid. Here the 2 dimensions in implicit space correspond to x and y positions.  The right panel shows that after learning, the hidden unit responds to objects in  a particular position, and its activity level falls off smoothly as the object position  moves away from the center of the learned receptive field.  3 EXPERIMENTAL RESULTS  In the first experiment, each 8x8 real-valued input image contained an instance of a  simple shape in a random (x, y)-position. The network began with random weights,  and each of 100 hidden units in a random 2D implicit position; we trained it using  conjugate gradient on 400 examples. The network converged after 25 epochs. Each  hidden unit developed a receptive field so that it responded to inputs in a limited  neighborhood that corresponded to its learned position in implicit space (see Figure  3). The set of hidden units covered the range of possible positions.  In a second experiment, we also varied the orientation of the shape and we gave  each hidden unit three implicit coordinates. The network converged after 60 epochs  of training on 1000 images. The hidden unit activities formed a population code  that allowed the input to be accurately reconstructed.  A third experiment employed a training set where each image contained either a  horizontal or vertical bar, in some random position. The hidden units formed an  interesting 2D implicit space in this case: one set of hidden units moved to one  corner of the space, and represented instances of one shape, while the other group  moved to an opposite corner and represented the other (Figure 4). The network  was thus able to squeeze a third dimension (i.e., which shape) into the 2D implicit  space. This type of representation would be difficult to learn in a Kohonen network;  the fact that the hidden units learn their implicit coordinates allows more flexibility  than a system in which these coordinates are fixed in advance.  Developing Population Codes by Minimizing Description Length 17  Y  6.50  6.00  5.50  5.00  4.50  4.00  3.50 !  3.00  2.50  2.00  1.5o  1.oo  0..0  0.00  Implicit Space (Epoch O)  6.50 -----T  6.00  5.50  5.00  4.50  4.00  3.50 x  3.00  x  250  x  12;0 x  x  1.00  x  0.50 x  o.oo 1 .oo  Implicit Space (Epoch 120)  x  5.110 6,110  Xposn  mea!l.V  mean. H  X  Figure 4: This figure shows the positions of the hidden units and the means in the 2D  implicit space before and after training on the horizontal/vertical task. The means  in the top right of the second plot all correspond to images containing vertical bars,  while the other set correspond to horizontal bar images. Note that some hidden  units are far from all the means; these units do not play a role in the coding of the  input, and are free to be recruited for other types of input cases.  4 RELATED WORK  This new algorithm bears some similarities to several earlier algorithms. In the  experiments presented above, each hidden unit learns to act as a Radial Basis  Function (RBF) unit. Unlike standard RBFs, however, here the RBF activity serves  as a target for the activity levels, and is determined by distance in a space other  than the input space.  Our algorithm is more similar to topographic map formation algorithms, such as the  Kohonen and elastic-net algorithms. In these methods, however, the population-  code is in effect formed in input space. Population coding in a space other than  the input enables our networks to extract nonlinear higher-order properties of the  inputs.  In (Saund, 1989), hidden unit patterns of activity in an autoencoder are trained to  form Gaussian bumps, where the center of the bump is intended to correspond to  the position in an underlying dimension of the inputs. In addition to the objective  functions being quite different in the two algorithms, another crucial difference  exists: in his algorithm, as well as the other earlier algorithms, the implicit space  topology is statically determined by the ordering of the hidden units, while units in  our model learn their implicit coordinates.  18 Zemel and Hinton  5 CONCLUSIONS AND CURRENT DIRECTIONS  We have shown how MDL can be used to develop non-factorial, redundant repre-  sentations. The objective function is derived from a communication model where  rather than communicating each hidden unit activity independently, we instead  communicate the location of a Gaussian bump in a low-dimensional implicit space.  If hidden units are appropriately tuned in this space their activities can then be  inferred from the bump location.  Our method can easily be applied to networks with multiple hidden layers, where  the implicit space is constructed at the last hidden layer before the output and  derivatives are then backpropagated; this allows the implicit space to correspond  to arbitrarily high-order input properties. Alternatively, instead of using multiple  hidden layers to extract a single code for the input, one could use a hierarchical  system in which the code-cost is computed at every layer.  A limitation of this approach (as well as the aforementioned approaches) is the  need to predefine the dimensionality of implicit space. We are currently working  on an extension that will allow the learning algorithm to determine for itself the  appropriate number of dimensions in implicit space. We start with many dimensions  but include the cost of specifying ttt in the description length. This obviously  depends on how many implicit coordinates are used. If all of the hidden units have  the same value for one of the implicit coordinates, it costs nothing to communicate  that value for each bump. In general, the cost of an implicit coordinate depends  on the ratio between its variance (over all the different bumps) and the accuracy  with which it must be communicated. So the network can save bits by reducing  the variance for unneeded coordinates. This creates a smooth search space for  determining how many implicit coordinates are needed.  Acknowledgements  This research was supported by grants from NSERC, the Ontario Information Technology  Research Center, and the Institute for Robotics and Intelligent Systems. Geoffrey Hinton  is the Noranda Fellow of the Canadian Institute for Advanced Research. We thank Peter  Dayan for helpful discussions.  References  Durbin, R. and Willshaw, D. (1987). An analogue approach to the travelling salesman  problem. Nature, 326:689-691.  Hinton, G. and Zemel, R. (1994). Autoencoders, minimum description length, and  Helmholtz free energy. To appear in Cowan, J.D., Tesauro, G., and Alspector,  J. (eds.), Advances in Neural Information Processing Systems 6. San Francisco,  CA: Morgan Kaufmann.  Kohonen, T. (1982). Self-organized formation of topologically correct feature maps.  Biological Cybernetics, 43:59-69.  Rissanen, J. (1989). Stochastic Complexity in StatisticalInquiry. World Scientific Pub-  lishing Co., Singapore.  Saund, E. (1989). Dimensionality-reduction using connectionist networks. IEEE  Transactions on Pattern Analysis and Machine Intelligence, 11(3):304-314.  Zemel, R. (1993). A Minimum Description Length Framework for Unsupervised Learn-  ing. Ph.D. Thesis, Department of Computer Science, University of Toronto.  
Assessing the Quality of Learned Local Models  Stefan Schaal Christopher G. Atkeson  Department of Brain and Cognitive Sciences & The Artifical Intelligence Laboratory  Massachusetts Institute of Technology  545 Technology Square, Cambridge, MA 02139  email: sschaal@ ai.mit.edu, cga ai.mit.edu  Abstract  An approach is presented to learning high dimensional functions in the case  where the learning algorithm can affect the generation of new data. A local  modeling algorithm, locally weighted regression, is used to represent the learned  function. Architectural parameters of the approach, such as distance metrics, are  also localized and become a function of the query point instead of being global.  Statistical tests are given for when a local model is good enough and sampling  should be moved to a new area. Our methods explicitly deal with the case where  prediction accuracy requirements exist during exploration: By gradually shifting  a "center of exploration" and controlling the speed of the shift with local pre-  diction accuracy, a goal-directed exploration of state space takes place along the  fringes of the current data support until the task goal is achieved. We illustrate  this approach with simulation results and results from a real robot learning a  complex juggling task.  1  INTRODUCTION  Every learning algorithm faces the problem of sparse data if the task to be learned is suf-  ficiently nonlinear and high dimensional. Generalization from a limited number of data  points in such spaces will usually be strongly biased. If, however, the learning algorithm  has the ability to affect the creation of new experiences, the need for such bias can be re-  duced. This raises the questions of (1) how to sample data the most efficient, and (2) how  to assess the quality of the sampled data with respect to the task to be leamed. To address  these questions, we represent the task to be learned with local linear models. Instead of  constraining the number of linear models as in other approaches, infinitely many local  models are permitted. This corresponds to modeling the task with the help of (hyper-)  tangent planes at every query point instead of representing it in a piecewise linear fash-  ion. The algorithm applied for this purpose, locally weighted regression (LWR), stems  from nonparametric regression analysis (Cleveland, 1979, Miiller, 1988, Hirdle 1990,  Hastie&Tibshirani, 1991). In Section 2, we will briefly outline LWR. Section 3 discusses  160  Assessing the Quality of Learned Local Models 161  several statistical tools for assessing the quality of a learned linear LWR model, how to  optimize the architectural parameters of LWR, and also how to detect outliers in the data.  In contrast to previous work, all of these statistical methods are local, i.e., they depend on  the data in the proximity of the current query point and not on all the sampled data. A  simple exploration algorithm, the shifting setpoint algorithm (SSA), is used in Section 4  to demonstrate how the properties of LWR can be exploited for learning control. The  SSA explicitly controls prediction accuracy during learning and samples data with the  help of optimal control techniques. Simulation results illustrate that this method work  well in high dimensional spaces. As a final example, the methods are applied to a real  robot learning a complex juggling task in Section 5.  2 LOCALLY WEIGHTED REGRESSION  Locally linear models constitute a good compromise between locally constant models  such as nearest neighbors or moving average and locally higher order models; the former  tend to introduce too much bias while the latter require fitting many parameters which is  computationally expensive and needs a lot of data. The algorithm which we explore here,  locally weighted regression (LWR) (Atkeson, 1992, Moore, 1991, Schaal&Atkeson,  1994), is closely related to versions suggested by Cleveland et al. (1979, 1988) and  Farmer&Siderowich (1987). A LWR model is trained by simply storing every experi-  ence as an input/output pair in memory. If an output yq is to be generated from a given  input xq, the it is computed by fitting a (hyper-) tangent plane at xe by means of weight-  ed regression:  3(x ) = [(WX)V WX ]4 (WX)* Wy (1)  where X is an mx(n+ 1) matrix of inputs to the regression, y the vector of corresponding  outputs, 3(x) the vector of regression parameters, and W the diagonal mxm matrix of  weights. The requested yresults from evaluating the tangent plane at xq, i.e., y =  The elements of W give points which are close to the current query point x a larger n-  fluence than those which are far away. They are determined by a Gaussian kernel:  wi(x) = exp((x i - x)rD(xq)(xi- x) / 2k(x) 2) (2)  w, is the weight'for the i th data point (xi, Yi) in memory given query point x. The ma-  Wix D(x) weights the contribution of the individual input dimensions, and the factor  k(xq) determines how local the regression will be. D and k are architectural parameters  of LWR and can be adjusted to optimize the fit of the local model. In the following we  will just focus on optimizing k, assuming that D normalizes the inputs and needs no fur-  ther adjustment; note that, with some additional complexity, our methods would also hold  for locally tuning D.  3  ASSESSING THE LOCAL FIT  In order to measure the goodness of the local model, several tests have been suggested.  The most widely accepted one is leave-one-out cross validation (CV) which calculates the  prediction error of every point in memory after recalculating (1) without this point  (Wahba&Wold 1975, Maron&Moore 1994). As an alternative measure, Cleveland et al.  (1988) suggested Mallow's Ct,-test, originally developed as a way to select covariates in  linear regression analysis (Mallow, 1966). Hastie&Tibshirani (1991) showed that CV and  the Cp-test are closely related for certain classes of analyses. Hastie&Tibshirani (1991)  162 Schaal and Atkeson  also presented pointwise standard-error bands to assess the confidence in a fitted value  which correspond to confidence bands in the case of an unbiased fit. All these tests are  essentially global by requiring statistical analysis over the entire range of data in mem-  ory. Such a global analysis is computationally costly, and it may also not give an ade-  quate measure at the current query site xq: the behavior of the function to be approxi-  mated may differ significantly in different places, and an averaging over all these behav-  iors is unlikely to be representative for all query sites (Fan&Gijbels, 1992).  It is possible to convert some of the above measures to be local. Global cross validation  has a relative in linear regression analysis, the PRESS residual error (e.g., Myers, 1990),  here formulated as a mean squared local cross validation error:  1 , w,(y,- x/r[3) , n'= E w2 p'= n' p (3)  MSEcss(xq)- n'--p ,= 1-w,xT(xrwrwx)-x,w, ,= ' n  n is the number of data points in memory contributing with a weight w e greater than  some small constant (e.g., we > 0.01) to the regression, and p is the dimensionality of .  The PRESS statistic performs leave-one-out cross validation computationally very effi-  cient by not requiring the recalculation of [3 (Eq.(1)) for every excluded point.  Analogously, prediction intervals from linear regression analysis (e.g., Myers, 1990) can  be transformed to be a local measure too:  I = xr[3 + t,/2..._ v, s/1 + x r (xrwrwx)- x, (4)  where s 2 is an estimate of the variance at x:  s2(x,) = (X[3- y)r WrW(X[3- y)  n'- p' (5)  and t,/.,_v, is Student's t-value of n'-p' degrs of frdom for a 1(1-a)% predic-  tion bound. e dkt intecremfion of (4)  pricfion bounds is only possible if y is  an unbi estimate, which is usually hd m demine.  Finally, e PRESS statistic c flso be used for locfl outlier detfion. For is puose it  is refoulamd  a smddized individufl PRESS residual:  e,.c,s (x) = w'(Y - xr[3) (6)  s ]l- wixr (xrwrwx)4x,w,  This measure has zero mean and unit variance. If it exceeds a certain threshold for a point  x,, the point can be called an outlier.  An important ingredient to forming the measures (3)-(6) lies in the definition of n' and  p' as given in (3). Imagine that the weighting function (2) is not Gaussian but rather a  function that clips data points whose distance from the current query point exceeds a cer-  tain threshold and that the remaining r data points all contribute with unit weight. This  reduced data regression coincides correctly with a r-data regression since n' = r. In the  case of the soft-weighting (2), the definition of n' ensures the proper definition of the  moments of the data. However, the definition of p', i.e., the degrees of freedom of the re-  gression, is somewhat arbitrary since it is unclear how many degrees of freedom have ac-  Assessing the Quality of Learned Local Models 163  tually been used. Defining p' as in (3) guarantees that p'< n' and renders all results  more pessimistic when only a small number of data points contribute to the regression.  2  1,5-  1-  0.5-  0-  -0.5  -0.2  2  :',";. .. "q  :,; l: 1.' '  ,;'.' ',.' !  0.2 0.4 0.6 0.8  1.2  1,5.  -0.5  -0.2  2  0 0.2 0.4 0.6 0.8  1.2  ()  1.5-  y-predicted .  i    ..... Iedtn mteal  0.5- . ......... ;   .  0-  -0.5  Fige 1: ting e L fit usg: (a)  glob cross vidation; (b) local oss valida-  tion; (c) local prection ts.  The statistical tests (3) and (4) can not only be  used as a diagnostic tool, but they can also  serve to optimize the architectural parameters  of LWR. This results in a function fitting tech-  nique which is called supersmoothing in statis-  tics (Hastie&Tibshirani, 1991). Fan&Gijbels  (1992) investigated a method for this purpose  that required estimation of the second deriva-  tive of the function to be approximated and the  data density distribution. These two measures  are not trivially obtained in high dimensions  and we would like to avoid using them. Figure  1 shows fits of noisy data from the function  y = x- sin3(2/rx 3) cos(2/rx 3) exp(x 4) with  95% prediction intervals around the fitted val-  ues. In Figure l a, global one-leave-out cross  validation was applied to optimize k (cf.  Eq.(2)). In the left part of the graph the fit  starts to follow noise. Such behavior is to be  expected since the global optimization of k  also took into account the quickly changing  regions on the right side of the graph and thus  chose a rather small k. In Figure lb mini-  mization of the local one-leave-out cross vali-  dation error was applied to fit the data, and in  Figure lc prediction intervals were mini-  mized. These two fits cope nicely with both  the high frequency and the low frequency re-  gions of the data and recover the true function  rather well. The extrapolation properties of lo-  cal cross validation are the most appropriate  given that the we know the true function.  Interestingly, at the right end of Figure lc, the minimization of the prediction intervals  suddenly detects that global regression has a lower prediction interval than local regres-  sion and jumps into the global mode by making k rather large. In both local methods  there is always a competition between local and global regression. But sudden jumps take  place only when the prediction interval is so large that the data is not trustworthy anyway.  To some extend, the statistical tests (3)-(6) implicitly measure the data density at the cur-  rent query point and are thus sensitive towards little data support, characterized by a  small n'. This property is deskable as a diagnostic tool, particularly if the data sampling  process can be directed towards such regions. However, if a fixed data set is to be analyz-  ed which has rather sparse and noisy data in several regions, a fit of the data with local  optimization methods may result in too jagged an approximation since the local fitting  mistakes the noise in such regions as high frequency portion of the data. Global methods  avoid this effect by biasing the function fitting in such unfavorable areas with knowledge  from other data regions and will produce better results if this bias is appropriate.  164 Schaal and Atkeson  4 THE SHIFTING SETPOINT EXPLORATION ALGORITHM  In this section we want to give an example of how LWR and its statistical tools can be  used for goal directed data sampling in learning control. If the task to be learned is high  dimensional it is not possible to leave data collection to random exploration; on the one  hand this would take too much time, and on the other hand it may cause the system to en-  ter unsafe or costly regions of operation. We want to develop an exploration algorithm  which explicitly avoids with such problems. The shifting setpoint algorithm (SSA) at-  tempts to decompose the control problem into two separate control tasks on different time  scales. At the fast time scale, it acts as a nonlinear regulator by trying to keep the con-  trolled system at some chosen setpoints in order to increase the data density at these set-  points. On a slower time scale, the setpoints are shifted by controlling local prediction ac-  curacy to accomplish a desired goal. In this way the SSA builds a narrow tube of data  support in which it knows the world. This data can be used by more sophisticated control  algorithms for planning or further exploration.  The algorithm is graphically illustrated in the example of a mountain car in Figure 2. The  task of the car is to drive at a given constant horizontal speed iaeatea frown the left to the  right of Figure 2a. 5ca,,,u need not be met precisely; the car should also minimize its fuel  consumption. Initially, the car knows nothing about the world and cannot look ahead, but  it has noisy feedback of its position and velocity. Commands, which correspond to the  thrust F of the motor, can be generated at 5Hz. The mountain car starts at its start point  with one arbitrary initial action for the first time step; then it brakes and starts all over  again, assuming the system can be reset somehow. The discrete one step dynamics of the  car are modeled by an LWR forward model:  n,, = ?(Xc,,,,,F), where x = (i,x) r (7)   ' T F T xT  After a few trials, the SSA searches the data in memory for the point tXc.e,, ,xn,)b,,   whose outcome :,,., can be predicted with the smallest local prediction interval. This  best point is declared the setpoint of this stage:  -- (Xcurren t  )b,,, (8)  , F s , Xs, o. , ) , F. x n,t  and its local linear model results from a corresponding LWR lookup:  Xs.o, = [(Xs.i,F s) -- Axs., + BF s + c (9)  Based on this lit:ear model, an optimal LQ controller (e.g., Dyer&McReynolds, 1970) can  be constructed. This results in a control law of the form:  r*=-K(Xc,,,,-Xs.,,,)+r s. (10)  After these calculations, the mountain car learned one controlled action for the first time  step. However, since the initial action was chosen arbitrarily, Xs.o, , will be significantly  away from the desired speed 2,,r, a. A reduction of this error is achieved as follows.  First, the SSA repeats one step actions with the LQ controller until suf, fjcient data is col-  lected to reduce the prediction intervals of LWR lookups for (xsr.,,F s) (Eq.(9)) below a  certain threshold. Then it shifts the setpoint towards the goal according to the procedure:  1) calculate the error of the predicted output state: errs.o, , = x,,, a - Xso,  2) take the deri,v/ative of the error with respect to the command F s 'iiom a IT, WR lookup  for (xsr.,,., Fs)(cf. (9)):  Assessing the Quality of Learned Local Models 165  3)  4)  errs'"' - Jerrs't Xs'"' - xs'' =-B  aFs axs.o., aFs aFs  and calculate a correction AF s from solving: -BAF s = q errs..,; q e [0,1] deter-  mines how much of the error should be compensated for in one step.  update F s: F s = F s - AF s and calculate the new x s o, with LWR (Eq.(9)).  assess the fit for the updated setpoint with predictio'i intervals. If the quality is above  a certain threshold, continue with 1), otherwise terminate shifting.  (d)  ===================================================================== ============================================ =========================  Figure 2: The mountain car: (a) landscape across which the car has to drive at constant velocity  of 0.8 m/s, (b) contour plot of data density in phase space as generated by using multistage  SSA, (c) contour plot of data density in position-action space, (d) 2-dimensional mountain car  0.1  T  'i:i:i r  10 20 $O 40 50  Figure 3: Mean prediction error of local models  In this way, the output state of the setpoint  shifts towards the goal until the data support  falls below a threshold. Now the mountain  car performs several new trials with the new  setpoint and the correspondingly updated  LQ controller. After the quality of fit statis-  tics rise above a threshold, the setpoint can  be shifted again. As soon as the first stage's  setpoint reduces the error x,,d - Xs.o , suf-  ficiently, a new stage is created and the  mountain car tries to move one step further in its world. The entire procedure is repeated  for each new stage until the car knows how to move across the landscape. Figure 2b and  Figure 2c show the thin band of data which the algorithm collected in state space and po-  sition-action space. These two pictures together form a narrow tube of knowledge in the  input space of the forward model.  166 Schaal and Atkeson  The example of the mountain car can easily be scaled up to arbitrarily high dimensions by  making the mountain a multivariate function. We tried versions up to a 5-dimensional  mountain corresponding to a 9l 5 --> 9l  forward model; Figure 2d shows the 2-dimen-  sional version. The results of learning had the same quality as in the 1D example. Figure  3 shows the prediction errors of the local models after learning for the 1D, 2D,..., and 5D  mountain car. To obtain these errors, the car was started at random positions within its  data support from where it drove along the desired trajectory. The difference between the  predicted next state and the actual outcome at each time step was averaged. Position er-  rors stayed within 2-4 cm on the 10m long landscape, and velocity errors within 0.02-  0.05 m/s. The dimensionality of the problem did not affect the outcome significantly.  (a)  Tal Number  (c)  Figure 4: (a) illustration of devilsticking, (b) a  devilsticking robot, (c) learning curve of robot  5 ROBOT JUGGLING  To test our algorithms in a real world exper-  iment, we implemented them on a juggling  robot. The juggling task to be performed,  devil sticking, is illustrated in Figure 4a. For  the robot, devil sticking was slightly simpli-  fied by attaching the devil stick to a boom,  as illustrated in Figure 4b. The task state was  encoded as a 5-dimensional state vector,  taken at the moment when the devilstick hit  one of the hand sticks; the throw action was  parameterized as 5-dimensional action vec-  tor. This resulted in a 9l--> 9l 5 discrete  forward model of the task. Initially the robot  was given default actions for the left-hand  and right-hand throws; the quality of these  throws, however, was far away from achiev-  ing steady juggling. The robot started with  no initial experiences and tried to build con-  trollers to perform continuous juggling. The  goal states for the SSA developed automarl-  cally from the requirement that the left hand  had to learn to throw the devilstick to a place  where the right hand had sufficient data sup-  port to control the devilstick, and vice versa.  Figure 4c shows a typical learning curve for  this task. It took about 40 trials before the  left and the right hand learned to throw the  devilstick such that both hands were able to  cooperate. Then, performance quickly went  up to long runs up to 1200 consecutive hits.  Humans usually need about one week of one  hour practicing per day before they achieve decent juggling performance. In comparison  to this, the learning algorithm performed very well. However, it has to be pointed out that  the learned controllers were only local and could not cope with larger perturbations. A de-  tailed description of this experiment can be found in Schaal&Atkeson (1994).  Assessing the Quality of Learned Local Models 167  CONCLUSIONS  One of the advantages of memory-based nonparametric learning methods lies in the least  commitment strategy which is associated with them. Since all data is kept in memory, a  lookup can be optimized with respect to the architectural parameters. Parametric ap-  proaches do not have this ability if they discard their training data; if they retain it, they  essentially become memory-based. The origin of nonparametric modeling in traditional  statistics provides many established statistical methods to inspect the quality of what has  been learned by the system. Such statistics formed the backbone of the SSA exploration  algorithm. So far we have only examined some of the most obvious statistical tools which  directly relate to regression analysis. Many other methods from other statistical frame-  works may be suitable as well and will be explored by our future work.  Acknowledgements  Support was provided by the Air Force Office of Scientific Research, by the Siemens  Corporation, the German Scholarship Foundation and the Alexander von Humboldt  Foundation to Stefan Schaal, and a National Science Foundation Presidential Young  Investigator Award to Christopher G. Atkeson. We thank Gideon Stein for implementing  the first version of LWR on a DSP board, and Gerrie van Zyl for building the devil  sticking robot and implementing the first version of learning of devil sticking.  References  Atkeson, C.G. (1992), "Memory-Based Approaches to Approximating Continuous Functions", in:  Casdagli, M.; Eubank, S. (eds.): Nonlinear Modeling and Forecasting. Redwood City, CA: Addi-  son Wesley (1992).  Cleveland, W.S., Devlin, S.J., Grosse, E. (1988), "Regression by Local Fitting: Methods, Proper-  ties, and Computational Algorithms". Journal of Econometrics 37, 87-114, North-Holland (1988).  Cleveland, W.S. (1979), "Robust Locally-Weighted Regression and Smoothing Scatterplots".  Journal of the American Statistical Association, no.74, pp.829-836 (1979).  Dyer, P., McReynolds, S.R. (1970), The Computation and Theory of Optimal Control, New York:  Academic Press (1970).  Fan, J., Gijbels, I. (1992), "Variable Bandwidth And Local Linear Regression Smoothers", The  Annals of Statistics, vol.20, no.4, pp.2008-2036 (1992).  Farmer, J.D., Sidorowich, J.J. (1987), "Predicting Chaotic Dynamics", Kelso, J.A.S., Mandell, A.J.,  Shlesinger, M.F., (eds.):Dynamic Patterns in Complex Systems, World Scientific Press (1987).  H'dle, W. (1991), Smoothing Techniques with Implementation in S, New York, NY: Springer.  Hastie, T.J.; Tibshirani, R.J. (1991), Generalized Additive Models, Chapman and Hall.  Mallows, C.L. (1966), "Choosing a Subset Regression", unpublished paper presented at the annual  meeting of the American Statistical Association, Los Angles (1966).  Maron, O., Moore, A.W. (1994), "Hoeffding Races: Accelerating Model Selection Search for  Classification and Function Approximation", in: Cowan, J., Tesauro, G., and Alspector, J. (eds.)  Advances in Neural Information Processing Systems 6, Morgan Kaufmann (1994).  Milllet, H.-G. (1988), Nonparametric Regression Analysis of Longitudinal Data, Lecture Notes in  Statistics Series, vol.46, Berlin: Springer (1988).  Myers, R.H. (1990), Classical And Modern Regression With Applications, PWS-KENT (1990).  Schaal, S., Atkeson, C.G. (1994), "Robot Juggling: An Implementation of Memory-based  Learning", to appear in:Control Systems Magazine, Feb. (1994).  Wahba, G., Wold, S. (1975), "A Completely Automatic French Curve: Fitting Spline Functions By  Cross-Validation", Communications in Statistics, 4(1) (1975).  
Structured Machine Learning For 'Soft'  Classification with Smoothing Spline  ANOVA and Stacked Tuning, Testing  and Evaluation  Grace Wahba  Dept of Statistics  University of Wisconsin  Madison, WI 53706  Yuedong Wang  Dept of Statistics  University of Wisconsin  Madison, WI 53706  Chong Gu  Dept of Statistics  Purdue University  West Lafayette, IN 47907  Ronald Klein, MD  Dept of Ophthalmalogy  University of Wisconsin  Madison, WI 53706  Barbara Klein, MD  Dept of Ophthalmalogy  University of Wisconsin  Madison, WI 53706  Abstract  We describe the use of smoothing spline analysis of variance (SS-  ANOVA) in the penalized log likelihood context, for learning  (estimating) the probability p of a '1' outcome, given a train-  ing set with attribute vectors and outcomes. p is of the form  p(t) - el(t)/(1 + el(t)), where, if t is a vector of attributes, f  is learned as a sum of smooth functions of one attribute plus a  sum of smooth functions of two attributes, etc. The smoothing  parameters governing f are obtained by an iterative unbiased risk  or iterative GCV method. Confidence intervals for these estimates  are available.  1. Introduction to 'soft' classification and the bias-variance tradeoff.  In medical risk factor analysis records of attribute vectors and outcomes (0 or 1)  for each example (patient) for n examples are available as training data. Based on  the training data, it is desired to estimate the probability p of the i outcome for any  415  416 Wahba, Wang, Gu, Klein, and Klein  new examples in the future, given their attribute vectors. In 'soft' classification, the  estimate p of p is of particular interest, and might be used, say, by a physician to  tell a patient that if he reduces his cholesterol from t to t', then he will reduce his  risk of a heart attack from p(t) to/5(t'). We assume here that p varies 'smoothly'  with any continuous attribute (predictor variable).  It is long known that smoothness penalties and Bayes estimates are intimately re-  lated (see e.g. Kimeldorf and Wahba(1970, 1971), Wahba(1990) and references  there). Our philosophy with regard to the use of priors in Bayes estimates is to  use them to generate families of reasonable estimates (or families of penalty func-  tionals) indexed by those smoothing or regularization parameters which are most  relevant to controlling the generalization error. (See Wahba(1990) Chapter 3, also  Wahba(1992)). Then use cross-validation, generalized cross validation (GCV), un-  biased risk estimation or some other performance oriented method to choose these  parameter(s) to minimize a computable proxy for the generalization error. A person  who believed the relevant prior might use maximum likelihood (ML) to choose the  parameters, but ML may not be robust against an unrealistic prior (that is, ML  may not do very well from the generalization point of view if the prior is off), see  Wahba(1985). One could assign a hyperprior to these parameters. However, except  in cases where real prior information is available, there is no reason to believe that  the use of hyperpriors will beat out a performance oriented criterion based on a good  proxy for the generalization error, assuming, of course, that low generalization error  is the true goal.  O'Sullivan et a/(1986) proposed a penalized log likelihood estimate of f, this work  was extended to the SS-ANOVA context in Wahba, Gu, Wang and Chappell(1993),  where numerous other relevant references are cited. This paper is available by  ftp from ftp. stat.wisc.edu, cd pub/wahba in the file soft-class.ps.Z. An  extended bibliography is available in the same directory as ml-bib.ps. The SS-  ANOVA allows a variety of interpretable structures for the possible relationships  between the predictor variables and the outcome, and reduces to simple relations  in some of the attributes, or even, to a two-layer neural net, when the data suggest  that such a representation is adequate.  2. Soft classification and penalized log likelihood risk factor estimation  To describe our 'worldview', let t be a vector of attributes, t  g2  T, where g2 is  some region of interest in attribute space T. Our 'world' consists of an arbitrarily  large population of potential examples, whose attribute vectors are distributed in  some way over g2 and, considering all members of this 'world' with attribute vectors  in a small neighborhood about t, the fraction of them that are 1's is p(t). Our  training set is assumed to be a random sample of n examples from this population,  whose outcomes are known, and our goal is to estimate p(t) for any t  fl. In 'soft'  classification, we do not expect one outcome or the other to be a 'sure thing', that  is we do not expect p(t) to be 0 or i for large portions of g2.  Next, we review penalized log likelihood risk estimates. Let the training data be  {Yi, t(i), i = 1, ...n} where yi has the value 1 or 0 according to the classification of  example i, and t(i) is the attribute vector for example i. If the n examples are a  random sample from our 'world', then the likelihood function of this data, given  "Soft" Classification with Smoothing Spline ANOVA 417  v('), is  l ikel ihood { y, p } = II= xp( t ( i) )u' (1 - p( t ( i) ) ) -u ' , (1)  which is the product of n Bernoulli likelihoods. Define the logit f(t) by f(t) =  log[p(t)/(1 - p(t))], then p(t) = e.t(t)/(1 + e.t(t)). Substituting in f and taking logs  gives  - log likelihood{y,f) -- (y,f) =  log(1 + e l(t(i))) - yif(t(i)). (2)  i=1  We estimate f assuming that it is in some space 7/of smooth functions. (Technically,  7/ is a reproducing kernel Hilbert space, see Vahba(1990), but you don't need to  know what this is to read on). The fact that f is assumed 'smooth' makes the  methods here very suitable for medical data analysis. The penalized log likelihood  estimate fx of f will be obtained as the minimizer in 7/of  (y, f) + xJ(f)  (a)  where J(f) is a suitable 'smoothness' penalty. A simple example is, T = [0, 1] and  J(f) = fo(f(m)(t)?dt, in which case fx is a polynomial spline of degree 2m- 1. If  Jam(f)= -- ax!:7]aa! '" ,Ox...Oxj d Ildxj. (4)  o +...+o,=m c c j  then fx is a thin plate spline. The thin plate spline is a linear combination of  polynomials of degree m or less in d variables, and certain radial basis functions.  For more details and other penalty functionals which result in rbf's, see Wahba(1980,  1990, 1992).  The likelihood function (y, f) will be maximized if p(t(i)) is 1 or 0 according as  yi is i or 0. Thus, in the (full-rank) spline case, as A -+ 0, fx tends to  at the data points. Therefore, by letting A be small, we can come close to fitting  the data points exactly, but unless the 1's and O's are well separated in attribute  space, fx will be a very 'wiggly' function and the generalization error (not precisely  defined yet) may be large.  The choice of A represents a tradeoff betwcen overfitting and underfitting the data  (bias-variance tradeoff). It is important in practice good value of A. We now define  what we mean by a good value of X. Given the family px, A >_ 0, we want to choose  X so that px is close to the 'true' but unknown p so that, if new examples arrive with  attribute vector in a neighborhood of t, px(t) will be a good estimate of the fraction  of them that are 1's. 'Closeness' can be defined in various reasonable ways. We use  the Kullbach-Leibler (KL) distance (not a real distance!). The KL distance between  two probability measures (g,) is defined as KL(g,) = Ea[log(g/)] , where E a  means expectation given g is the true distribution. If (t) is some probability  measure on T, (say, a proxy for the distribution of the attributes in the population),  then define KL (p, Px) (for Bernoulli random variables) with respect to  as  KL.(p, px) = f [p(t)log (p(t)  (1-p(t)  [,p-) ] + (1 -- p(t))log )] dr(t).  ,1 - px(t)  (5)  418 Wahba, Wang, Gu, Klein, and Klein  Since KLv is not computable from the data, it is necessary to develop a computable  proxy for it. By a computable proxy is meant a function of A that can be calculated  from the training set which has the property that its minimizer is a good estimate  of the minimizer of KLv. By letting px('t) - eld(t)/(1 q- e l(t)) it is seen that to  minimize KL, it is only necessary to minimize  [log(1 q- e Ix(t)) - p(t)f(t)]dy(t) (6)  over A since (5) and (6) differ by something that does not depend on A. Leaving-  out-half cross validation (CV) is one conceptually simple and generally defensible  (albeit possibly wasteful) way of choosing A to minimize a proxy for KL(p, px).  The n examples are randomly divided in half and the first n/2 examples are used  to compute Px for a series of trial values of A. Then, the remaining n/2 examples  are used to compute  A 2 Z [log(1 q- e lx(t(i))) -- yifx(t(i))] (7)  KLcv( ) = .  ,=+1  for the trial values of A. Since the expected value of Yi is p(t(i)), (7) is, for each A an  unbiased estimate of () with dr the sampling distribution of the {t(1), ..., t(n/2)}.  A would then be chosen by minimizing (7) over the trial values. It is inappropriate to  just evaluate (7) using the same data that was used to obtain fx, as that would lead  to overfitting the data. Variations on (7) are obtained by successively leaving out  groups of data. Leaving-out-one versions of (7) may be defined, but the computation  may be prohibitive.  3. Newton-Raphson Iteration and the Unbiased Risk estimate of A.  We use the unbiased risk estimate given in Craven and Wahba(1979) for smoothing  spline estimation with Gaussian errors, which has been adapted by Gu(1992a) for  the Bernoulli case. To describe the estimate we need to describe the Newton-  Raphson iteration for minimizing (3). Let b(f) -- log(1 q- el), then (y,f) -  y4n=l[b(f(t(i)) - yif(t(i))]. It is easy to show that Eyi = f(t(i)) = b'(f(t(i)) and  vat yi = p(t(i))(1- p(t(i)) = b"(f(t(i)). Represent f either exactly by using a  basis for the (known) n-space of functions containing the solution, or approximately  by suitable approximating basis functions, to get  N  f  k=l  Then we need to find c = (Cl,..., cs)' to minimize  n N N  n  Ix(c) = Z b( cB(t( i) ) ) - yi( cB(t( i) ) ) q- - Ac 2c,  i=1 k=l k--1  where Z is the necessarily non-negative definite matrix determined  J( c:B) = c'Zc. The gradient VIx and the Hessian V2Ix of Ixare given by  i oI; I   = X'(p - y) + nAZc,  (8)  (o)  by  (10)  "Soft" Classification with Smoothing Spline ANOVA 419  {V2Ix}/, - O=Ix = X'WcX + nAE, (11)  OcjOc  where X is the matrix with ijth entry Bj (t(i)), Pc is the vector with ith entry  do('(')) where f(.)  = = = cB(.), and We is the diagonal  given by pc(t(i)) (l+e('(')))  matrix with iith entry pc(t(i))(1 - pc(t(i))). Given the tth Newton-Raphson iterate  c , c (t+x) is given by  c (t+) = c  -- (X'Wc(oX + nAE)-l(X'(pdo -- y) + nAEc ) (12)  and c(t+) is the minimizer of  1/2 2  I?(c) = IIz()- w:(oXcl[ + nc'c. (13)  where z(/), the sincalled pseudmdata, is given by  z  = W -12' z/-(t) (14)  c(t) I,--Pc(O) 'c(O ....  The 'predicted' value (t) -,/2 -  = c(o c, where c is the minimizer of (13), is related to  the pseudo-data z(t) by  () = A()()(), (la)  where A(t) () is the smoother matrix given by  wl/ ' W(,)X + nS)-  X' w/(16)  A(t) () = "() '  ....  In Wahba(1990), Section 9.2 , it w proposed to obtain a GCV score for  in (9)  follows: For fixed X, iterate (12) to convergence. Define V(t)()  [[(I- t(O())z(Oll/(t(I-1()())). Letting L be the converged value of  compute  ll(I - A() ())z() l[2 ll?(y- pc(=))ll 2  v()(x) = ((- A()()))  ((- A()())) (1)  and minimize V () with respect to . Gu(1992a) showed tha (since the variance  is known once the mean is known here) that the unbied risk estimate U() in  Craven and Wahba can also be adapted to this problem   1 _/e [e 2  v(o() =-II(,) (v-n(,))l +- A(O(). (18)  He also proposed an alternating iteration, different than that described in  Wahba(1990), namely, given cU) = c()(U)), find  = (+x) to minimize (18).  Given (+), do a Newton step to get c (+x), get X(+e) by minimizing (18), continue  until convergence. He showed that the alternating ierafion gave better estimates of   using V than the iteration in Wahba(1990),  menured by the KL-distance. His  results (with the alternating iteration) suggested U had somewhat of an advantage  over V, and that is what we are using in the present work. Zhao et al, his volume,  have used V successfully with the alternating iteration.  The defion of  ghere &fiers from ghe defion here by a facgor of n/2. Please  noge he gypoapcM eor in (9.2.18) ghere where  shoed be 2X.  420 Wahba, Wang, Gu, Klein, and Klein  4. Smoothing spline analysis of variance (SS-ANOVA)  In SS-ANOVA, f(t) = f(t, ...,ta) is decomposed as  f(t) = ll + E fa(ta) + ]] fa(ta,t) + '" (19)  where the terms in the expansion are uniquely determined by side conditions which  generalize the side conditions of the usual ANOVA decompositions. Let the logitf (t)  be of the form (19) where the terms are summed over a 6 , a,  6 , etc. where   indexes terms which are chosen to be retained in the model after a model selec-  tion procedure. Then fx,, an estimate of f, is obtained  the minimizer of  C(y, fx,o)+ A&(f) (20)  where  = X] + X] + ...  (21)  The J, JAB,'" are quadratic 'smoothness' penalty functionMs, and the O's satisfy  a single constraint. For certain spline-like smoothness penalties, the minimizer of  (20) is known to be in the span of a certain set of n functions, and the vector  c of coefficients of these functions can (for fixed (A,0)) be chosen by the Newton  Raphson iteration. Both A and the O's are estimated by the unbiased risk estimate  of Gu using RKPACK(available from ne;libCresearch. a;;. cora) as a subroutine  at each Newton iteration. Details of smoothing spline ANOVA decompositions may  be found in Wahba(1990) and in Gu and Wahba(1993) (also available by ftp to  f;p.s;a;.wiac.edu, cd to pub/wahba , in the file aaanova.pa.Z). In Wahba et  a/(1993) op cit, we estimate the risk of diabetes given some of the attributes in the  Pima-Indian data base. There JM was chosen partly by a screening process using  paramteric GLIM models and partly by a leaving out approximately 1/3 procedure.  Continuing work involves development of confidence intervals based on Gu(1992b),  development of numerical methods suitable for very large data sets based on Gi-  rard's(1991) randomized trace estimation, and further model selection issues.  In the Figures we provide some preliminary analyses of data from the Wiscon-  sin Epidemiological Study of Diabetic Retinopathy (WESDR, Klein et al 1988).  The data used here is from people with early onset diabetes participating in the  WESDR study. Figure l(left) gives a plot of body mass index (brai) (a mea-  sure of obesity) vs age (age) for 669 instances (subjects) in the WESDR study  that had no diabetic retinopathy or nonproliferative retinopathy at the start of  the study. Those subjects who had (progressed) retinopathy four years later, are  marked as  and those with no progression are marked as .. The contours are  lines of estimated posterior standard deviation of the estimate /5 of the proba-  bility of progression. These contours are used to delineate a region in which /5  is deemed to be reliable. Glycosylated hemoglobin (gly), a measure of blood  sugar control. was also used in the estimation of p. A model of the form  p = e l/(1 + el), f(age, gly, brai) = p + fl (age) + b. gly + fa(bmi) + fla(age, bmi)  was selected using some of the screening procedures described in Wahba et a/(1993),  along with an examination of the estimated multiple smoothing parameters, which  indicated that the linear term in gly was sufficient to describe the (quite strong)  dependence on gly. Figure l(right) shows the estimated probability of progression  "Soft" Classification with Smoothing Spline ANOVA 421  given by this model. Figure 2(left) gives cross sections of the fitted model of Figure  l(right), and Figure 2(right) gives another cross section, along with its confidence  interval. Interesting observations can be made, for example, persons in their late  20's with higher gly and bmJ. are at greatest risk for progression of the disease.  Figure 1: Left: Data and contours of constant posterior standard deviation at  the median gly, as a function of age and brai. Right: Estimated probability of  progression at the median gly, as a function of age and brai.  o. ql bmi gly=q2   ...... q2bmi  c - - q3 bmi   -- - q4 bmi  o  10 20 30 40 )   age (Y0  gly----q3  x',..:-,  10 20 30 40 50 60  age (yr)  10 20 30 40 50 60  age (yr)  Figure 2: Left: Eight cross sections of the right panel of Figure 1, Estimated prob-  ability of progression as a function of age, at four levels of brai by two of gly.  ql,...q4 are the quartiles at .125, .375, .625 and .875. Right: Cross section of the  right panel of Figure I for brai and gly at their medians, as a function of age,  with Bayesian 'condifidence interval' (shaded) which generalizes Gu(1992b) to the  multivariate case.  422 Wahba, Wang, Gu, Klein, and Klein  Acknowledgement s  Supported by NSF DMS-9121003 and DMS-9301511, and NEI-NIH EY09946 and  EY03083  References  Craven, P. & Wahba, G. (1979), 'Smoothing noisy data with spline functions:  estimating the correct degree of smoothing by the method of generalized cross-  validation', Numer. Math. 31,377-403.  Girard, D. (1991), 'Asymptotic optimality of the fast randomized versions of GCV  and CL in ridge regression and regularization', Ann. Statist. 19, 1950-1963.  Gu, C. (1992a), 'Cross-validating non-Gaussian data', J. Cornput. Graph. Stats.  1, 169-179.  Gu, C. (1992b), 'Penalized likelihood regression: a Bayesian analysis', Statistica  Sinica 2,255-264.  Gu, C. & Wahba, G. (1993), 'Smoothing spline ANOVA with component-wise  Bayesian "confidence intervals" ', J. Computational and Graphical Statistics 2, 1-21.  Kimeldorf, G. &; Wahba, G. (1970), 'A correspondence between Bayesian estimation  of stochastic processes and smoothing by splines', Ann. Math. Statist. 41,495-502.  Klein, R., Klein, B., Moss, S. Davis, M.,  DeMets, D. (1988), Glycosylated  hemoglobin predicts the incidence and progression of diabetic retinopathy, JAMA  260, 2864-2871.  O'Sullivan, F., Yandell, B. &; Raynor, W. (1986), 'Automatic smoothing of regres-  sion functions in generalized linear models', J. Am. 5'tat. Soc. 81, 96-103.  Wahba, G. (1980), Spline bases, regularization, and generalized cross validation for  solving approximation problems with large quantities of noisy data, in W. Cheney,  ed., 'Approximation Theory III', Academic Press, pp. 905-912.  Wahba, G. (1985), 'A comparison of GCV and GML for choosing the smoothing  parameter in the generalized spline smoothing problem', Ann. Statist. 13, 1378-  1402.  Wahba, G. (1990), Spline Models for Observational Data, SIAM. CBMS-NSF Re-  gional Conference Series in Applied Mathematics, vol. 59.  Wahba, G. (1992), Multivariate function and operator estimation, based on smooth-  ing splines and reproducing kernels, in M. Casdagli &; S. Eubank, eds, 'Nonlinear  Modeling and Forecasting, SFI Studies in the Sciences of Complexity, Proc. Vol  XII', Addison-Wesley, pp. 95-112.  Wahba, G., Gu, C., Wang, Y. & Chappell, R. (1993), Soft classification, a. k.  a. risk estimation, via penalized log likelihood and smoothing spline analysis of  variance, to appear, Proc. Santa Fe Workshop on Supervised Machine Learning, D.  Wolpert and A. Lapedes, eds, and Proc. CLNL92, T. Petsche, ed, with permission  of all eds.  
Locally Adaptive Nearest Neighbor  Algorithms  Dietrich Wettschereck Thomas G. Dietterich  Department of Computer Science  Oregon State University  Corvallis, OR 97331-3202  escds o ors. edu  Abstract  Four versions of a k-nearest neighbor algorithm with locally adap-  tive k are introduced and compared to the basic k-nearest neigh-  bor algorithm (kNN). Locally adaptive kNN algorithms choose the  value of k that should be used to classify a query by consulting the  results of cross-validation computations in the local neighborhood  of the query. Local kNN methods are shown to perform similar to  kNN in experiments with twelve commonly used data sets. Encour-  aging results in three constructed tasks show that local methods  can significantly outperform kNN in specific applications. Local  methods can be recommended for on-line learning and for appli-  cations where different regions of the input space are covered by  patterns solving different sub-tasks.  1 Introduction  The k-nearest neighbor algorithm (kNN, Dasarathy, 1991) is one of the most ven-  erable algorithms in machine learning. The entire training set is stored in memory.  A new example is classified with the class of the majority of the k nearest neighbors  among all stored training examples. The (global) value of k is generally determined  via cross-validation.  For certain applications, it might be desirable to vary the value of k locally within  184  Locally Adaptive Nearest Neighbor Algorithms 185  different parts of the input space to account for varying characteristics of the data  such as noise or irrelevant features However, for lack of an algorithm, researchers  have assumed a global value for k in all work concerning nearest neighbor algorithms  to date (see, for example, Bottou, 1992, p. 895, last two paragraphs of Section 4.1).  In this paper, we propose and evaluate four new algorithms that determine different  values for k in different parts of the input space and apply these varying values to  classify novel examples. These four algorithms use different methods to compute  the k-values that are used for classification.  We determined two basic approaches to compute locally varying values for k. One  could compute a single k or a set of k values for each training pattern, or training  patterns could be combined into groups and k value(s) computed for these groups. A  procedure to determine the k to be used at classification time must be given in both  approaches. Representatives of these two approaches are evaluated in this paper  and compared to the global kNN algorithm. While it was possible to construct  data sets where local algorithms outperformed kNN, experiments with commonly  used data sets showed, in most cases, no significant differences in performance. A  possible explanation for this behavior is that data sets which are commonly used  to evaluate machine learning algorithms may all be similar in that attributes such  as distribution of noise or irrelevant features are uniformly distributed across all  patterns. In other words, patterns from data sets describing a certain task generally  exhibit similar properties.  Local nearest neighbor methods are comparable in computational complexity and  accuracy to the (global) k-nearest neighbor algorithm and are easy to implement. In  specific applications they can significantly outperform kNN. These applications may  be combinations of significantly different subsets of data or may be obtained from  physical measurements where the accuracy of measurements depends on the value  measured. Furthermore, local kNN classifiers can be constructed at classification  time (on-line learning) thereby eliminating the need for a global cross-validation  run to determine the proper value of k o  1.1 Methods compared  The following nearest neighbor methods were chosen as representatives of the pos-  sible nearest neighbor methods discussed above and compared in the subsequent  experiments:  k-nearest neighbor (kNN)  This algorithm stores all of the training examples. A single value for k is  determined from the training data. Queries are classified according to the  class of the majority of their k nearest neighbors in the training data.  localKNNk unrestricted  This is the basic local kNN algorithm. The three subsequent algorithms  are modifications of this method. This algorithm also stores all of the  training examples. Along with each training example, it stores a list of  those values of k that correctly classify that example under leave-one-out  cross-validation. To classify a query q, the M nearest neighbors of the  query are computed, and that k which classifies correctly most of these M  186 Wettschereck and Dietterich  neighbors is determined. Call this value kM,q. The query q is then classified  with the class of the majority of its kM,q nearest neighbors. Note that kM,q  can be larger or smaller than M. The parameter M is the only parameter  of the algorithm, and it can be determined by cross-validation.  1ocalKNNks pruned  The list of k values for each training example generally contains many val-  ues. A global histogram of k values is computed, and k values that appear  fewer than L times are pruned from all lists (at least one k value must,  however, remain in each list). The parameter L can be estimated via cross-  validation. Classification of queries is identical to localKNNks unrestricted.  1ocalKNNone  per class  For each output class, the value of k that would result in the correct (leave-  one-out) classification of the maximum number of training patterns from  that class is determined. A query q is classified as follows: Assume there  are two output classes, C' and C2. Let k and k2 be the k value computed  for classes C' and C'2, respectively. The query is assigned to class C' if the  percentage of the k nearest neighbors of q that belong to class C' is larger  than the percentage of the k nearest neighbors of q that belong to class  C. Otherwise, q is assigned to class C. Generalization of that procedure  to any number of output classes is straightforward.  localKNNone  per cluter  An unsupervised cluster algorithm (RPCL,  Xu et al., 1993) is used to  determine clusters of input data. A single k value is determined for each  cluster. Each query is classified according to the k value of the cluster it is  assigned to.  2 Experimental Methods and Data sets used  To measure the performance of the different nearest neighbor algorithms, we em-  ployed the training set/test set methodology. Each data set was randomly par-  titioned into a training set containing approximately 70% of the patterns and a  test set containing the remaining patterns. After training on the training set, the  percentage of correct classifications on the test set was measured. The procedure  was repeated a total of 25 times to reduce statistical variation. In each experi-  ment, the algorithms being compared were trained (and tested) on identical data  sets to ensure that differences in performance were due entirely to the algorithms.  Leave-one-out cross-validation (Weiss & Kulikowski, 1991) was employed in all ex-  periments to estimate optimal settings for free parameters such as k in kNN and  M in localKNN.   Rival Penalized Competitive Learning is a straightforward modification of the well  known k-means clustering algorithm. RPCL's main advantage over k-means clustering is  that one can simply initialize it with a sufficiently large number of clusters. Cluster centers  are initialized outside of the input range covered by the training examples. The algorithm  then moves only those cluster centers which are needed into the range of input values and  therefore effectively ehminates the need for cross-validation on the number of clusters in  k-means. This paper employed a simple version with the number of initial clusters always  set to 25, ac set to 0.05 and ar to 0.002.  Locally Adaptive Nearest Neighbor Algorithms 187  We report the average percentage of correct classifications and its standard error.  Two-tailed paired t-tests were conducted to determine at what level of significance  one algorithm outperforms the other. We state that one algorithm significantly  outperforms another when the p-value is smaller than 0.05.  3 Results  3.1 Experiments with Constructed Data Sets  Three experiments with constructed data sets were conducted to determine the  ability of local nearest neighbor methods to determine proper values of k. The data  sets were constructed such that it was known before experimentation that varying  k values should lead to superior performance. Two data sets which were presumed  to require significantly different values of k were combined into a single data set  for each of the first two experiments. For the third experiment, a data set was  constructed to display some characteristics of data sets for which we assume local  kNN methods would work best. The data set was constructed such that patterns  from two classes were stretched out along two parallel lines in one part of the  input space. The parallel lines were spaced such that the nearest neighbor for most  patterns belongs to the same class as the pattern itself, while two out of the three  nearest neighbors belong to the other class. In other parts of the input space, classes  were well separated, but class labels were flipped such that the nearest neighbor of a  query may indicate the wrong pattern while the majority of the k nearest neighbors  (k > 3) would indicate the correct class (see also Figure 4).  Figure 1 shows that in selected applications, local nearest neighbor methods can  lead to significant improvements over kNN in predictive accuracy.  Experiment 1  Letter Led- 16  Recogn. Display Combined  - 66.4::1:0.4% ]69.14-0.3%i59.9-I-0.4.%  5001500/16 i 50015001161  0o0/ 0o0/ e  Experiment 2  Sine-21 Wave-21 Combined  54.1-1-0.9% 179.94-0.6%i77.24-0.2%  280/120/21 [ 280/120/2 ' 560/240/21  o  -4  Experiment 3 i  Constructed  70.04-0.6%  *kNN  480/120/2  ]l ks pruned I ks unrestricted [] one k per class [] one k per cluster I  Figure 1: Percent accuracy of local kNN methods relative to kNN on separate test sets.  These differences (*) were statistically significant (p < 0.05). Results are based on 25  repetitions. Shown at the bottom of each graph are sizes of training sets/sizes of test  sets/number of input features. The percentage at top of each graph indicates average  accuracy of kNN q- standard error.  The best performing lqcal methods are localKNNks rru,ed, localKNNks unrestricted,  188 Wettschereck and Dietterich  and localKNNone k rer ct, ster. These methods were outperformed by kNN in two  of the original data sets. However, the performance of these methods was clearly  superior to kNN in all domains where data were collections of significantly distinct  subsets.  3.2 Experiments with Commonly Used Data Sets  Twelve domains of varying sizes and complexities were used to compare the perfor-  mance of the various nearest neighbor algorithms. Data sets for these domains were  obtained from the UC-Irvine repository of machine learning databases (Murphy  Aha, 1991, Aha, 1990, Detrano et al., 1989). Results displayed in Figure 2 indicate  that in most data sets which are commonly used to evaluate machine learning algo-  rithms, local nearest neighbor methods have only minor impact on the performance  of kNN. The best local methods are either indistinguishable in performance from  kNN (localKNNo, e k rer custer) or inferior in only one domain (localKNN},  Iris Glass Wine Hungarian Cleveland Voting  2  95.6-I-0.5% { 67.4:t:1.2% ; 96.2+0.5% ;82.o-i.o% [ 83.4-I-0.5% i 92.0-I-0.o  105/50/4 i 150/64/9 ; 125/53/13 i 206/88/13 ; 212/91/13 ; 305/130/16  0 kNN  -2  Led Display Waveform  7 Features 24 Features 21 Features 40 Features Isolet Letter Letter recog.  72.3-1-0.6% i 52.94-0.7% ; 82.4-1-0.8% i80.7:1:1.1% { 76.5:t:0.5%  77.0:1:0.3%  :200/500/7 ; 200/500/24 ; 300/100/21 ; 300/100/40 i ]000/1000/617; ]000/1000/16  kNN  I I ks pruned I ks unrestricted :-::D one k per class [] one k per cluster ]  Figure 2: Percent accuracy of local kNN methods relative to kNN on separate test sets.  These differences (*) were statistically significant (p < 0.05). Results are based on 25  repetitions. Shown at the bottom of each graph are sizes of training sets/sizes of test  sets/number of input features. The percentage at top of each graph indicates average  accuracy of kNN + standard error.  The number of actual k values used varies significantly for the different local meth-  ods (Table 1). Not surprisingly, localKNN}s unrestricted uses the largest number of  distinct k values in all domains. Pruning of ks significantly reduced the number of  values used in all domains. However, the method using the fewest distinct k values  is localKNNone : rer ctuster, which also explains the similar performance of kNN and  localKNNone : rer ctuster in most domains. Note that several clusters computed by  localKNNone : per custer may use the same k.  Locally Adaptive Nearest Neighbor Algorithms 189  Table 1: Average number of distinct values for k used by local kNN methods.  Task kNN local kNN methods  ks ks one k per one k per  pruned unrestricted class cluster  Letter recog. 1 7.64-1.1 10.84-1.5 6.44-0.3 1.84-0.2  Led-16 I 16.44-2.5 43.34.0.9 9.24-0.1 9.24-0.5  CombinedLL 1 52.04-3.8 71.44-1.2 14.74-0.4 3.04-0.2  Sine-21 I 6.64-1.0 27.54-1.1 2.04-0.0 1.04-0.0  Waveform-21 I 9.14-1.4 28.04-1.5 2.94-0.1 4.24-0.2  Combinedsw I 13.54-1.5 30.84'1.6 3.04.0.0 4.84.0.2  Constructed 1 11.84.0.9 15.74.0.5 2.04-0.0 5.44-0.2  Iris 1 1.64-0.2 2.04-0.2 2.44.0.1 2.34-0.1  Glass 1 7.74-0.8 11.24-0.7 3.34-0.2 1.94-0.2  Wine I 2.24-0.4 3.84-0.4 2.04.0.1 2.64-0.1  Hungarian I 4.14-0.6 12.64-0.6 2.04-0.0 1.04-0.0  Cleveland I 8.04-1.0 17.24-1.1 1.84-0.1 4.64-0.2  Voting I 4.14-0.4 6.44-0.3 2.04-0.0 1.34-0.1  Led-7 Display I 5.64.0.4 7.64.0.4 6.14.0.2 1.04.0.0  Led-24 Display i 16.04-2.9 37.44-1.6 9.04-0.2 1.64-0.2  Waveform-21 I 9.74-1.3 27.84-1.2 3.04-0.0 4.34-0.1  Waveform-40 I 8.44.2.0 29.94.1.5 3.04-0.0 4.84-0.1  Isolet Letter I 11.54.2.1 43.94.0.6 16.54-0.5 7.14-0.3  Letter reco. I 9.44-1.9 17.04-2.3 6.04-0.3 2.4-1-0.2  Figure 3 shows, for one single run of Experiment 2 (data sets were combined as  described in Figure 1), which k values were actually used by the different local  methods. Three clusters of k values can be seen in this graph, one cluster at k = 1,  one at k = 7,9,11,12 and the third at k = 19,20,21. It is interesting to note that  the second and the third cluster correspond to the k values used by kNN in the  separate experiments. Furthermore, kNN did not use k = 1 in any of the separate  runs. This gives insight into why kNN's performance was inferior to that of the  local methods in this experiment: Patterns in the combined data set belong to  one of three categories as indicated by the k values used to classify them (k = 1,  k , 10, k m 20). Hence, the performance difference is due to the fact that kNN  must estimate at training time which single category will give the best performance  while the local methods make that decision at classification time for each query  depending on its local neighborhood.  k =1 I I I 2 2 2 3 3 5 6 6 7 7 7 8 8 9 9 9 1010 IIII 1212 13 14 15 17 18 1919  21   4545     61 67  74 75 76    [l ks pruned I ks unrestricted ;[] one k per class [] one k per cluster ]  Figure 3: Bars show number of times local kNN methods used certain k values to classify  test examples in Experiment 2 (Figure 1 (Combined), numbers based on single run). KNN  used k = 1 in this experiment.  190 Wettschereck and Dietterich  4 Discussion  Four versions of the k-nearest neighbor algorithm which use different values of k  for patterns which belong to different regions of the input space were presented and  evaluated in this paper. Experiments with constructed and commonly used data  sets indicate that local nearest neighbor methods may have superior classification  accuracy than kNN in specific domains.  Two methods can be recommended for domains where attributes such as noise or  relevance of attributes vary significantly within different parts of the input space.  The first method, called localKNNks pruned, computes a list of "good" k values for  each training pattern, prunes less frequent values from these lists and classifies a  query according to the list of k values of a pre-specified number of neighbors of  the query. Leave-one-out cross-validation is used to estimate the proper amount of  pruning and the size of the neighborhood that should be used.  The other method, 1ocalKNNone k per cluster, uses a cluster algorithm to determine  clusters of input patterns. One k is then computed for each cluster and used to  classify queries which fall into this cluster. LocalKNNo,e  per cluster performs in-  distinguishable from kNN in all commonly used data sets and outperforms kNN  on the constructed data sets. This method compared with all other local methods  discussed in this paper introduces a lower computational overhead at classification  time and is the only method which could be modified to eliminate the need for  leave-one-out cross-validation.  The only purely local method, localKNNs unrestricted, performs well on constructed  data sets and is comparable to kNN on non-constructed data sets. Sensitivity stud-  ies (results not shown) showed that a constant value of 25 for the parameter M  gave results comparable to those where cross-validation was used to determine the  value of M. The advantage of localKNNs unrestricted over the other local meth-  ods and kNN is that this method does not require any global information what-  soever (if a constant value for M is used). It is therefore possible to construct a  localKNNs unrestricted classifier for each query which makes this method an attrac-  tive alternative for on-line learning or extremely large data sets.  If the researcher has reason to believe that the data set used is a collection of  subsets with significantly varying attributes such as noise or number of irrelevant  features, we recommend the construction of a classifier from the training data using  localKNNone k per cluster and comparison of its performance to kNN. If the classifier  must be constructed on-line then localKNNs unrestricted should be used instead of  kNN.  We conclude that there is considerable evidence that local nearest neighbor meth-  ods may significantly outperform the k-nearest neighbor method on specific data  sets. We hypothesize that local methods will become relevant in the future when  classifiers are constructed that simultaneously solve a variety of tasks.  Acknowledgements  This research was supported in part by NSF Grant IRI-8657316, NASA Ames Grant  NAG 2-630, and gifts from Sun Microsystems and Hewlett-Packard. Many thanks  Locally Adaptive Nearest Neighbor Algorithms 191  to Kathy Astrahantseff and Bill Langford for helpful comments during the revision  of this manuscript.  References  Aha, D.W. (1990). A Study of Instance-Based Algorithms for Supervised Learning  Tasks. Technical Report, University of California, Irvine.  Bottou, L., Vapnik, V. (1992). Local Learning Algorithms. Neural Computation,  4(6), 888-900.  Dasarathy, B.V. (1991). Nearest Neighbor(NN) Norms: NN Pattern Classification  Techniques. IEEE Computer Society Press.  Detrano, R., Janosi, A., Steinbrunn, W., Pfisterer, M., Schmid, K., Sandhu, S.,  Guppy, K., Lee, S. & Froelicher, V. (1989). Rapid searches for complez patterns in  biological molecules. American Journal of Cardiology, 64,304-310.  Murphy, P.M. & Aha, D.W. (1991). UCI Repository of machine learning databases  [Machine-readable data repository]. Technical Report, University of California,  Irvine.  Weiss, S.M., & Kulikowski, C.A. (1991). Computer Systems that learn. San Mateo  California: Morgan Kaufmann Publishers, INC.  Xu, L., Krzyzak, A., & Oja, E. (1993). Rival Penalized Competitive Learning for  Clustering Analysis, RBF Net, and Curve Detection IEEE Transactions on Neural  Networks, 4(4),636-649.  kNN correct: 69.3%  local kNN ct: .9%  Total coect:  $O dam points  Noise fr dam  Noise fr data  0 data points  kNN: 70.0%  $1.0%  84.6%  local kNN: 74.8%  77.5%  78.3%  Size of training set: 480  tost set: 120  Figure 4: Data points for the Constructed data set were drawn from either of the two  displayed curves (i.e. all data points lie on either of the two curves). Class labels were  flipped with increasing probabilities to a maximum noise level of approximately 45% at  the respective ends of the two lines. Listed at the bottom is performance of kNN and  localKNN,m,-trlctea within different regions of the input space and for the entire input  space.  
Event-Driven Simulation of Networks of  Spiking Neurons  Lloyd Watts  Synaptics Inc.  2698 Orchard Parkway  San Jose CA 95134  11oydsynapt ics. com  Abstract  A fast event-driven software simulator has been developed for sim-  ulating large networks of spiking neurons and synapses. The prim-  itive network elements are designed to exhibit biologically realis-  tic behaviors, such as spiking, refractoriness, adaptation, axonal  delays, summation of post-synaptic current pulses, and tonic cur-  rent inputs. The efficient event-driven representation allows large  networks to be simulated in a fraction of the time that would be  required for a full compartmental-model simulation. Correspond-  ing analog CMOS VLSI circuit primitives have been designed and  characterized, so that large-scale circuits may be simulated prior  to fabrication.  1 Introduction  Artificial neural networks typically use an abstraction of real neuron behaviour,  in which the continuously varying mean firing rate of the neuron is presumed to  carry the information about the neuron's time-varying state of excitation [1]. This  useful simplification allows the neuron's state to be represented as a time-varying  continuous-amplitude quantity. However, spike timing is known to be important in  many biological systems. For example, in nearly all vertebrate auditory systems,  spiral ganglion cells from the cochlea are known to phase lock to pure-tone stimuli  for all but the highest perceptible frequencies [2]. The barn owl uses axonal delays  to compute azimuthal spatial localization [3]. Studies in the cat [4] have shown that  927  928 Watts  relative timing of spikes is preserved even at the highest cortical levels. Studies in  the visual system of the blowfly [5] have shown that the information contained in  just three spikes is enough for the fly to make a decision to turn, if the visual input  is sparse.  Thus, it is apparent that biological neural systems exploit the spiking and time-  dependent behavior of the neurons and synapses to perform system-level computa-  tion. To investigate this type of computation, we need a simulator that includes  detailed neural behavior, yet uses a signal representation efficient enough to allow  simulation of large networks in a reasonable time.  2 Spike: Event-Driven Simulation  Spike is a fast event-driven simulator optimized for simulating networks of spiking  neurons and synapses. The key simplifying assumption in Spike is that all currents  injected into a cell are composed of piecewise-constant pulses (i.e., boxcar pulses),  and therefore all integrated membrane voltage trajectories are piecewise linear in  time. This very simple representation is capable of surprisingly complex and realistic  behaviors, and is well suited for investigating system-level questions that rely on  detailed spiking behavior.  The simulator operates by maintaining a queue of scheduled events. The occurrence  of one event (i.e., a neuron spike) usually causes later events to be scheduled in the  queue (i.e., end of refractory period, end of post-synaptic current pulse). The total  current injected into a cell is integrated into the future to predict the time of firing,  at which time a neuron spike event is scheduled. If any of the current components  being injected into the cell subsequently change, the spike event is rescheduled. The  simulator runs until the queue is empty or until the desired run-time has elapsed.  A similar event-driven neural simulator was developed by Pratt [6].  The simulator output may be plotted by a number of commercially available plotting  programs, including Gnuplot, Mathematica, Xvgr, and Cview.  3 NeuraLOG: Neural Schematic Capture  NeuraLOG is a schematic entry tool, which allows the convenient entry of "neural"  circuit diagrams, consisting of neurons, synapses, test inputs, and custom symbols.  NeuraLOG is a customization of the program AnaLOG, by John Lazzaro and Dave  Gillespie.  The parameters of the neural elements are entered directly on the schematic dia-  gram; these parameters include the neuron refractory period, duration and intensity  of the post-synaptic current pulse following an action potential, saturation value of  summating post-synaptic currents, tonic input currents, axonal delays, etc. Cus-  tom symbols can be defined, so that arbitrarily complex hierarchical designs may  be made. It is common to create a complex "neuron" containing many neuron and  synapse primitive elements. Spiking inputs may be supplied as external stimuli for  the circuit in a number of different formats, including single spikes, periodic spike  trains, periodic bursts, poisson random spike trains, and gaussian-jittered periodic  Event-Driven Simulation of Networks of Spiking Neurons 929  spike trains. Textual input to Spike is also supported, to allow simulation of circuit  topologies that would be too time-consuming to enter graphically.  4 A Simple Example  A simple example of a neural circuit is shown in Figure 1. This circuit consists of  two neurons (the large disks), several synapses (the large triangles), and two tonic  inputs (the small arrows). The text strings associated with each symbol define  that symbol's parameters: neuron parameters are identifier labels (i.e., nl) and  refractory period in milliseconds (ms); synapse parameters are the value of the post-  synaptic current in nA, and the duration of the current pulse in ms, and an optional  saturation parameter, which indicates how many post-synaptic current pulses may  be superposed before saturation; the tonic input parameter is the injected current  in nA. Filled symbols (tonic inputs and synapses) indicate inhibitory behavior.  -.8 10 8 >-  -8 15)-  6.4)-  .13 2.5  -.001 >-  Figure 1: Graphical input representation of a simple neural circuit, as en-  tered in NeuraLOG.  The simulated behavior of the circuit is shown in Figure 2. The neuron n! exhibits  an adapting bursting behavior, as seen in the top trace of the plot.  The excitatory tonic current input to neuron n! causes nl to fire repeatedly. The  weakly excitatory synapse from n! to neuron n2 causes n2 to fire after many spikes  from nl. The synaptic current in the synapse from n! to n2 is plotted in the trace  labeled snln2. The strongly inhibitory synapse from n2 to n! causes n! to stop  firing after n2 fires a spike. The synaptic current in the synapse from n2 to n! is  plotted in the trace labeled sn2nl. The combination of the excitatory tonic input  to n! and the inhibitory feedback from n2 to n! causes the bursting behavior.  The adapting behavior is caused by the self-inhibitory accumulating feedback from  neuron n! to itself, via the summating inhibitory synapse in the top left of the  diagram. Each spike on n! causes a slightly increased inhibitory current into  which gradually slows the rate of firing with each successive pulse. The synaptic  current in this inhibitory synapse is plotted in the trace znlnl; it is similar to the  930 Watts  nl  n2  snlnl  sn2nl  snln2  0 10 20 30 40 50 60 70  Time (ms)  Figure 2: Simulation results for the circuit of Figure 1, showing adapting  bursting behavior.  current that would be generated by a cMcium-dependent potassium channel.  This simple example demonstrates that the summating synapse primitive can be  used to model a behavior that is not strictly synaptic in origin; it can be thought  of as a general time-dependent state variable. This example also illustrates the  principle that proper network topology (summating synapse in a negative feedback  loop) can lead to realistic system-level behavior (gradual adaptation), even though  the basic circuit elements may be rather primitive (boxcar current pulses).  5 Applications of the Simulation Tools  NeuraLOG and Spike have been used by the author to model spiking associative  memories, adaptive structures that learn to predict a time delay, and chaotic spiking  circuits. Researchers at Caltech [7, 8] and the Salk Institute have used the tools in  their studies of locust central pattern generators (CPGs) and cortical oscillations.  The cortical oscillation circuits contain a few hundred neurons and a few thousand  synapses. A CPG circuit, developed by Sylvie Ryckebusch, is shown in Figure 3;  the corresponding simulation output is shown in Figure 4.  NeuraLOG and Spike are distributed at no charge under the GNU licence. They  are currently supported on HP and Sun workstations. The tools are supplied with  a user's manual and working tutorial examples.  Event-Driven Simulation of Networks of Spiking Neurons 931  i11)  levi)  t #d;  levr   dsl levi  ilr)  levr)  levr pirr.     der levr)   Figure 3: Sylvie lyckebusch's locust CPG circuit. For clarity, the synapse  parameters have been omitted.  932 Watts  in  dfl  dsl  ill  levi  pirl  dfr  dsr  ilr  levr  pirr  J .I J J J J   J J J .I J J J  J J J J J J J J J  J J J J J J J J J !'  0 20 40 60 80 100  Time (ms)  Figure 4: Simulation results for Sylvie Pyckebusch's locust CPG circuit.  6 The Link to Analog VLSI  Analog VLSI circuit primitives that can be modelled by Spike have been designed  and tested. The circuits are shown in Figure 5, and have been described previously  [9, 10]. These circuits have been used by workers at Caltech to implement VLSI  models of central pattern generators. The software simulation tools allow simulation  of complex neural circuits prior to fabrication, to improve the likelihood of success  on first silicon, and to allow optimization of shared parameters (bias wires).  7 Conclusion  NeuraLOG and Spike fill a need for a fast neural simulator that can model large  networks of biologically realistic spiking neurons. The simple computational prim-  itives within Spike can be used to create complex and realistic neural behaviors  in arbitrarily complex hierarchical designs. The tools are publicly available at no  charge. NeuraLOG and Spike have been used by a number of research labs for  detailed modeling of biological systems.  Acknowledgements  NeuraLOG is a customization of the program AnaLOG, which was written by John  Lazzaro and David Gillespie. Lloyd Watts gratefully acknowledges helpful dis-  cussions with Carver Mead, Sylvie Ryckebusch, Misha Mahowald, John Lazzaro,  Event-Driven Simulation of Networks of Spiking Neurons 933  Figure 5: CMOS Analog VLSI circuit primitives. The neuron circuit models  a voltage-gated sodium channel and a delayed rectifier potassium channel  to produce a spiking mechanism. The tonic circuit allows constant currents  to be injected onto the membrane capacitance Cm. The synapse circuit  creates a boxcar current pulse in response to a spike input.  David Gillespie, Mike Vanier, Brad Minch, Rahul Sarpeshkar, Kwabena Boahen,  John Platt, and Steve Nowlan. Thanks to Sylvie Ryckebusch for permission to use  her CPG circuit example.  References  [1] J. Hertz, A. Krogh and R. Palmer, Introduction to the Theory of Neural Com-  putation, Addison-Wesley, 1991.  [2] N. Y-S. Kiang, T. Watanabe, E. C. Thomas, L. F. Clark, "Discharge Patterns  of Single Fibers in the Cat's Auditory Nerve", MIT Res. Monograph No. 35,  (MIT, Cambridge, MA).  [3] M. Konishi, T.T. Takahashi, H. Wagner, W.E. Sullivan, C.E. Carl "Neuro-  physiological and Anatomical Substrates of Sound Localization in the Owl",  In Auditory Function, G.M. Edelman, W.E. Gall, and W.M. Cowan, eds., pp.  721-745, Wiley, New York.  [4] D. P. Phillips and S. E. Hall, "Response Timing Constraints on the Cortical  Representation of Sound Time Structure", Journal of the Acoustical Society of  America, 88 (3), pp. 1403-1411, 1990.  [5] R.R. de Ruyter van Steveninck and W. Bialek, "Real-time Performance of a  movement-sensitive neuron in the blowfly visual system: Coding and infor-  934 Watts  [10]  mation transfer in short spike sequences", Proceedings of the Royal Society of  London, Series B, 234, 379-414.  [6] G. A. Pratt, Puke Computation, Ph.D. Thesis, Massachusetts Institute of  Technology, 1989.  [7] M. Wehr, S. Ryckebusch and G. Laurent, Western Nerve Net Conference, Seat-  tle, Washington, 1993.  [8] S. Ryckebusch, M. Wehr, and G. Laurent, "Distinct rhythmic locomotor pat-  terns can be generated by a simple adaptive neural circuit: biology, simulation,  and VLSI implementation", in review, Journal of Computational Neuroscience.  [9] R. Sarpeshkar, L. Watts, C.A. Mead, "Refractory Neuron Circuits", Inter-  nal Memorandum, Physics of Computation Laboratory, California Institute of  Technology, 1992.  L. Watts, "Designing Networks of Spiking Silicon Neurons and Synapses", Pro-  ceedings of Computation and Neural Systems Meeting CNS'92, San Francisco,  CA, 1992.  PART VIII  VISUAL PROCESSING  
Encoding Labeled Graphs by Labeling  RAAM  Alessandro Sperduti*  Department of Computer Science  Pisa University  Corso Italia 40, 56125 Pisa, Italy  Abstract  In this paper we propose an extension to the RAAM by Pollack.  This extension, the Labeling RAAM (LRAAM), can encode la-  beled graphs with cycles by representing pointers explicitly. Data  encoded in an LRAAM can be accessed by pointer as well as by  content. Direct access by content can be achieved by transform-  ing the encoder network of the LRAAM into an analog Hopfield  network with hidden units. Different access procedures can be  defined depending on the access key. Sufficient conditions on the  asymptotical stability of the associated Hopfield network are briefly  introduced.  1 INTRODUCTION  In the last few years, several researchers have tried to demonstrate how symbolic  structures such as lists, trees, and stacks can be represented and manipulated in a  connectionist system, while still preserving all the computational characteristics of  connectionism (and extending them to the symbolic representations) (Hinton, 1990;  Plate, 1991; Pollack, 1990; Smolensky, 1990; Touretzky, 1990). The goal is to high-  light the potential of the connectionist approach in handling domains of structured  tasks. The common background of their ideas is an attempt to achieve distal access  and consequently compositionality. The RAAM model, proposed by Pollack (Pol-  lack, 1990), is one example of how a neural network can discover compact recursive  *Work partially done while at the International Computer Science Institute, Berkeley.  1125  1126 Sperduti  Output Layer  Hidden Layer  Input Layer  Label P 1 P 2 P n  Decoder  Encoder  Figure 1: The network for a general LRAAM. The first layer of the network imple-  ments an encoder; the second layer, the corresponding decoder.  distributed representations of trees with a fixed branching factor.  This paper presents an extension of the RAAM, the Labeling RAAM (LRAAM).  An LRAAM allows one to store a label for each component of the structure to be  represented, so as to generate reduced representations of labeled graphs. Moreover,  data encoded in an LRAAM can be accessed not only by pointer but also by content.  In Section 2 we present the network and we discuss some technical aspects of the  model. The possibility to access data by content is presented in Section 3. Some  stability results are introduced in Section 4, and the paper is closed by discussion  and conclusions in Section 5.  2 THE NETWORK  The general structure of the network for an LRAAM is shown in Figure 1. The  network is trained by backpropagation to learn the identity function. The idea is to  obtain a compressed representation (hidden layer activation) of a node of a labeled  graph by allocating a part of the input (output) of the network to represent the  label (Nt units) and the remaining part to represent one or more pointers. This  representation is then used as pointer to the node. To allow the recursive use of these  compressed representations, the part of the input (output) layer which represents  a pointer must be of the same dimension as the hidden layer (NH units). Thus, a  general LRAAM is implemented by a N - NH - N feed-forward network, where  N = Nt + nNH, and n is the number of pointer fields.  Labeled graphs can be easily encoded using an LRAAM. Each node of the graph  only needs to be represented as a record, with one field for the label and one  field for each pointer to a connected node. The pointers only need to be logical  pointers, since their actual values will be the patterns of hidden activation of the  network. At the beginning of learning, their values are set at random. A graph is  represented by a list of these records, and this list constitutes the initial training set  for the LRAAM. During training the representations of the pointers are consistently  updated according to the hidden activations. Consequently, the training set is  dynamic. For example, the network for the graph shown in Figure 2 can be trained  as follows:  Encoding Labeled Graphs by Labeling RAAM 1127  input hidden output  (Lx d2 d4 ds) --* d'   (L2 d3 d4 nil)  d2   (L3 dc nil nil)  d3   (L d dn3 nil)  d4   (Ls d d nil)  ds   (L nil nil nil)  d   I!  (L nil" nil" nil")  where Li and dni are respectively the label and the pointer (reduced descriptor to  the i-th node. For the sake of simplicity, the void pointer is represented by a single  symbol, nil, but each instance of it must be considered as being different. This  statement will be made clear in the next section.  Once the training is complete, the patterns of activation representing pointers can be  used to retrieve information. Thus, for example, if the activity of the hidden units of  the network is clamped to d, the output of the network becomes (L ,d2,dn4 ,dns),  enabling further retrieval of information by decoding d2, or dn4, or dns, and so on.  Note that more labeled graphs can be encoded in the same LRAAM.  2.1 THE VOID POINTER PROBLEM  In the RAAM model there is a termination problem in the decoding of a compressed  representation: due to approximation errors introduced during decoding, it is not  clear when a decoded pattern is a terminal or a nonterminal. One solution is to test  for "binary-ness", which consists in checking whether all the values of a pattern are  above I - r or below r, r > 0, r << 1. However, a nonterminal may also pass the  test for "binary-ness".  One advantage of LRAAM over RAAM is the possibility to solve the problem by  allocating one bit of the label for each pointer to represent if the pointer is void or  not. This works better than fixing a particular pattern for the void pointer, such  as a pattern with all the bits to I or 0 or -1 (if symmetrical sigmoids are used).  Simulations performed with symmetrical sigmoids showed that the configurations  with all bits equal to I or -1 were also used by non void pointers, whereas the  configuration with all bits set to zero considerably reduced the rate of convergence.  Using a part of the label to solve the problem is particularly efficient, since the  pointer fields are free to take on any configuration when they are void, and this  increases the freedom of the system. To facilitate learning, the output activation  of the void pointers in one epoch is used as an input activation in the next epoch.  Experimentation showed fast convergence to different fixed points for different void  Figure 2: An example of a labeled graph.  1128 Sperduti  pointers. For this reason, we claimed that each occurrence of the void pointer is  different, and that the nil symbol can be considered as a "don't care" symbol.  2.2 REPRESENTATION OF THE TRAINING SET  An important question about the way a graph is represented in the training set  is which aspects of the representation itself can make the encoding task harder  or easier. In (Sperduti, 1993a) we made a theoretical analysis on the constraints  imposed by the representation on the set of weights of the LRAAM, under the  hypotheses of perfect learning (zero total error after learning) and linear output  units. Our findings were:  i) pointers to nodes belonging to the same cycle of length k and represented in  the same pointer field p, must be eigenvectors of the matrix (W )k, where  W (p) is the connection matrix between the hidden layer and the output  units representing the pointer field p;  confluent pointers, i.e., pointers to the same node represented in the same  pointer field p (of different nodes), contribute to reducing the rank of the  matrix W , the actual rank is however dependent on the constraints im-  posed by the label field and the other pointer fields.  We have observed that different representations of the same structure can lead to  very different learning performances. However, representations with roughly the  same number of non void pointers for each pointer field, with cycles represented in  different pointer fields and with confluent pointers seem to be more effective.  3 ACCESS BY CONTENT  Retrieval of coded information is performed in RAAM through the pointers. All the  terminals and nonterminals can be retrieved recursively by the pointers to the whole  tree encoded in a RAAM. If direct access to a component of the tree is required,  the pointer to the component must be stored and used on demand.  Data encoded in an LRAAM can also be accessed directly by content. In fact, an  LRAAM network can be transformed into an analog Hopfield network with one  hidden layer and asymmetric connection matrix by feeding back its output into its  input units.  Because each pattern is structured in different fields, different access  1Experimental results have shown that there is a high correlation between elements of  W  (the set of weights from the input to the hidden layer) and the corresponding elements  in W ()T (the set of weights from the hidden to the output layer). This is particularly true  for weights corresponding to units of the label field. Such result is not a total surprise,  since in the case of a static training set, the error function of a linear encoder network has  been proven to have a unique global minimum corresponding to the projection onto the  subspace generated by the first principal vectors of a covariance matrix associated with the  training set (Baldi &: Hornik, 1989). This implies that the weights matrices are transposes  of each other unless there is an invertible transformation between them (see also (Bourlard  &: Kamp, 1988)).  Encoding Labeled Graphs by Labeling RAAM 1129  Figure 3: The labeled graph encoded in a 16-3-16 LRAAM (5450 epochs), and the  labeled tree encoded in a 18-6-18 LRAAM (1719 epochs).  procedures can be defined on the Hopfield network according to the type of access  key. An access procedure is defined by:  1. choosing one or more fields in the input layer according to the access key(s);  2. clamping the output of such units to the the access key(s);  3. setting randomly the output of the remaining units in the network;  4. letting the remaining units of the network to relax into a stable state.  A validation test of the reached stable state can be performed by:  1. unfreezing the clamped units in the input layer;  2. if the stable state is no longer stable the result of the procedure is considered  wrong and another run is performed;  3. otherwise the stable state is considered a success.  This validation test, however, sometimes can fail to detect an erroneous retrieval  (error) because of the existence of spurious stable states that share the same known  information with the desired one.  The results obtained by the access procedures on an LRAAM codifying the graph  and on an LRAAM coddying the tree shown in Figure 3 are reported in Table  1. For each procedure 100 trials were performed. The "mean" column in the  table reports the mean number of iterations employed by the Hopfield network to  converge. The access procedure by outgoing pointers was applied only for the tree.  It can be seen from Table I that the performances of the access procedures were  high for the graph (no errors and no wrong retrievals), but not so good for the  tree, in particular for the access by label procedure, due to spurious memories. It is  interesting to note that the access by label procedure is very ettlcient for the leaves  of the tree. This feature can be used to build a system with two identical networks,  one accessed by pointer and the other by content. The search for a label proceeds  simultaneously into the two networks. The network accessed by pointer will be very  fast to respond when the label is located on a node at lower levels of the tree, and  the network accessed by content will be able to respond correctly and very fast 2  when the label is located on a node at higher levels of the tree.  2Assuming an analog implementation of the Hopfield network.  1130 Sperduti  GRAPH: Access by Label TREE: Access by Label  key(s) success wrong error mean key success wrong error mean  l0 100% 0% 0% 7.35 l0 0% 100% 0% 16.48  l 100% 0% 0% 36.05 l 94% 6% 0% 14.57  12 100% 0% 0% 6.04 12 47% 53% 0% 16.92  13 100% 0% 0% 3.99 la 100% 0% 0% 18.07  14 100% 0% 0% 23.12 14 97% 0% 3% 32.64  ls 100% 0% 0% 18.12 15 100% 0% 0% 16.03  16 100% 0% 0% 29.26 16 49% 51% 0% 27.50  TREE: Access by Children Pointers 17 42% 58% 0% 27.10  (d, d2) 49% 51% 0% 6.29 18 57% 43% 0% 62.45  (d3, d4) 10% 90% 0% 8.55 19 20% 0% 80% 14.75  (ds,d6) 40% 60% 0% 12.48 /lO 100% 0% 0% 19.11  (d?, d8) 78% 22% 0% 6.57 /11 100% 0% 0% 10.83  (d9,dxo) 9% 91% 0% 6.22 /12 100% 0% 0% 19.12  d? 14% 86% 0% 14.01 l3 29% 71% 0% 23.87  (d.2, d3) 14% 86% 0% 7.87 14 100% 0% 0% 12.09  (d4,d5) 28% 72% 0% 6.07 15 100% 0% 0% 13.11  (*) one pointer  Table 1: Results obtained by the access procedures.  4 STABILITY RESULTS  In the LRAAM model two stability problems are encountered. The first one arises  when considering the decoding of a pointer along a cycle of the encoded structures.  Since the decoding process suffers, in general, of approximation errors, it may hap-  pen that the decoding diverges from the correct representations of the pointers  belonging to the cycle. Thus, it is fundamental to discover under which conditions  the representations obtained for the pointers are asymptotically stable with respect  to the pointer transformation. In fact, if the representations are asymptotically  stable, the errors introduced by the decoding function are automatically corrected.  The following theorem can be proven (Sperduti, 1993b):  Theorem 1 A decoding sequence  d (iJ+') -- F(P'J)(J(i)), j = 0,...,L (1)  with c  = c  , satisfying  Y.lbkl < 1, i= 1,...,m (2)  k=l  for some index Pi,, q = 0,..., L, is asymptotically stable, where b,k is the (i, k)th  element of a matrix B, given by  -' p  In the statement of the theorem, F (p.')(t) - F(D (p")d+t )) is the transformation  of the reduced descriptor (pointer) t{by the pointer field pj, and J(P)(t) is its  Encoding Labeled Graphs by Labeling RAAM 1131  Jacobian matrix. As a corollary of this theorem we have that if at least one pointer  belonging to the cycle has saturated components, then the cycle is asymptotically  stable with respect to the decoding process. Moreover, the theorem can be applied  with a few modifications to the stability analysis of the fixed points of the associated  Hopfield network.  The second stability problem consists into the discovering of sufficient conditions  under which the property of asymptotical stability of a fixed point in one particular  constrained version of the associated Hopfield network, i.e., an access procedure,  can be extended to related fixed points of different constrained versions of it, i.e.,  access procedures with more information or different information. The result of  Theorem I was used to derive three theorems regarding this issue (see (Sperduti,  1993b)).  5 DISCUSSION AND CONCLUSIONS  The LRAAM model can be seen from various perspectives. It can be considered as  an extension of the RAAM model, which allows one to encode not only trees with  information on the leaves, but also labeled graphs with cycles. On the other hand,  it can be seen as an approximate method to build analog Hopfield networks with  a hidden layer. An LRAAM is probably somewhere in between. In fact, although  it extends the representational capabilities of the RAAM model, it doesn't possess  the same synthetic capabilities as the RAAM, since it explicitly uses the concept  of pointer. Different subsets of units are thus used to codify labels and pointers.  In the RAAM model, using the same set of units to codify labels and reduced  representations is a more natural way of integrating a previously developed reduced  descriptor as a component of a new structure. In fact, this ability was Pollack's  original rationale behind the RAAM model, since with this ability it is possible to fill  a linguistic role with the reduced descriptor of a complex sentence. In the LRAAM  model the same target can be reached, but less naturally. There are two possible  solutions. One is to store the pointer of some complex sentence (or structure, in  general), which was previously developed, in the label of a new structure. The  other solution would be to have a particular label value which tells us that the  information we are looking for can be retrieved using one conventional or particular  pointer among the current ones.  An issue strictly correlated with this is that, even if in an LRAAM it is possible  to encode a cycle, what we get from the LRAAM is not an explicit reduced repre-  sentation of the cycle, but several pointers to the components of the cycle forged  in such a way that the information on the cycle is only represented implicitly in  each of them. However, the ability to synthesize reduced descriptors for structures  with cycles is what makes the difference between the LRAAM and the RAAM. The  only system that we know of which is able to represent labeled graphs is the DUAL  system proposed by Dyer (Dyer, 1991). It is able to encode small labeled graphs  representing relationships among entities. However, the DUAL system cannot be  considered as being on the same level as the LRAAM, since it devises a reduced  representation of a set of functions relating the components of the graph rather  than a reduced representation for the graph. Potentially also Holographic Reduced  Representations (Plate, 1991) are able to encode cyclic graphs.  1132 Sperduti  The LRAAM model can also be seen as an extension of the Hopfield networks  philosophy. A relevant aspect of the use of the Hopfield network associated with an  LRAAM, is that the access procedures defined on it can efficiently exploit subsets  of the weights. In fact, their use corresponds to generating several smaller networks  from a large network, one for each kind of access procedure, each specialized on a  particular feature of the stored data. Thus, by training a single network, we get  several useful smaller networks.  In conclusion an LRAAM has several advantages over a standard RAAM. Firstly,  it is more powerful, since it allows to encode directed graphs where each node has  a bounded number of outgoing arcs. Secondly, an LRAAM allows direct access to  the components of the encoded structure not only by pointer, but also by content.  Concerning the applications where LRAAMs can be exploited, we believe there are  at least three possibilities: in knowledge representation, by encoding Conceptual  Graphs (Sowa, 1984); in unification, by representing terms in restricted domains  (Knight, 1989); in image coding, by storing Quadtrees (Samet, 1984);  References  P. Baldi & K. Hornik. (1989) Neural networks and principal component analysis: Learning  from examples without local minima. Neural Networks, 2:53-58.  H. Bourlard & Y. Kamp. (1988) Auto-association by multilayer perceptrons and singular  value decomposition. Biological Cybernetics, 59:291-294.  M. G. Dyer. (1991) Symbolic NeuroEngineering for Natural Language Processing: A Multi-  level Research Approach., volume i of Advances in Connectionist and Neural Computation  Theory, pages 32-86. Ablex.  G. E. Hinton. (1990) Mapping part-whole hierarchies into connectionist networks. Artifi-  cial Intelligence, 46:47-75.  K. Knight. (1989) Unification: A multidisciplinary survey. ACM Computing Surveys,  21:93-124.  T. Plate. (1991) Holographic reduced representations. Technical Report CRG-TR-91-1,  Department of Computer Science, University of Toronto.  J. B. Pollack. (1990) Recursive distributed representations. Artificial Intelligence, 46(1-  2):77-106.  H. Samet. (1984) The quadtree and related hierarchical data structures. ACM Computing  Surveys, 16:187-260.  P. Smolensky. (1990) Tensor product variable binding and the representation of symbolic  structures in connectionist systems. Artificial Intelligence, 46:159-216.  J.F. Sowa. (1984) Conceptual Structures: Information Processing in Mind and Machine.  Addison-Wesley.  A. Sperduti. (1993a) Labeling RAAM. TR 93-029, ICSI, Berkeley.  A. Sperduti. (1993b) On some stability properties of the LRAAM model. TR 93-031,  ICSI, Berkeley.  D. S. Touretzky. (1990) Boltzcons: Dynamic symbol structures in a connectionist network.  Artificial Intelligence, 46:5-46.  PART XI  ADDENDA TO NIPS 5  
A Comparison of Dynamic Reposing and  Tangent Distance for Drug Activity  Prediction  Thomas CI. Dietterich  Arris Pharmaceutical Corporation and Oregon State University  Corvallis, OR 97331-3202  Ajay N. Jain  Arris Pharmaceutical Corporation  385 Oyster Point Blvd., Suite 3  South San Francisco, CA 94080  Richard H. Lathrop and Tomas Lozano-Perez  Arris Pharmaceutical Corporation and MIT Artificial Intelligence Laboratory  545 Technology Square  Cambridge, MA 02139  Abstract  In drug activity prediction (as in handwritten character recogni-  tion), the features extracted to describe a training example depend  on the pose (location, orientation, etc.) of the example. In hand-  written character recognition, one of the best techniques for ad-  dressing this problem is the tangent distance method of Simard,  LeCun and Denker (1993). Jain, et al. (1993a; 1993b) introduce a  new technique--dynamic reposing--that also addresses this prob-  lem. Dynamic reposing iteratively learns a neural network and then  reposes the examples in an effort to maximize the predicted out-  put values. New models are trained and new poses computed until  models and poses converge. This paper compares dynamic reposing  to the tangent distance method on the task of predicting the bio-  logical activity of musk compounds. In a 20-fold cross-validation,  216  A Comparison of Dynamic Reposing and Tangent Distance for Drug Activity Prediction 217  dynamic reposing attains 91% correct compared to 79% for the  tangent distance method, 75% for a neural network with standard  poses, and 75% for the nearest neighbor method.  1 INTRODUCTION  The task of drug activity prediction is to predict the activity of proposed drug  compounds by learning from the observed activity of previously-synthesized drug  compounds. Accurate drug activity prediction can save substantial time and money  by focusing the efforts of chemists and biologists on the synthesis and testing of  compounds whose predicted activity is high. If the requirements for highly active  binding can be displayed in three dimensions, chemists can work from such displays  to design new compounds having high predicted activity.  Drug molecules usually act by binding to localized sites on large receptor molecules  or large enyzme molecules. One reasonable way to represent drug molecules is  to capture the location of their surface in the (fixed) frame of reference of the  (hypothesized) binding site. By learning constraints on the allowed location of  the molecular surface (and important charged regions on the surface), a learning  algorithm can form a model of the binding site that can yield accurate predictions  and support drug design.  The training data for drug activity prediction consists of molecules (described by  their structures, i.e., bond graphs) and measured binding activities. There are two  complications that make it difficult to learn binding site models from such data.  First, the bond graph does not uniquely determine the shape of the molecule. The  bond graph can be viewed as specifying a (possibly cyclic) kinematic chain which  may have several internal degrees of freedom (i.e., rotatable bonds). The confor-  mations that the graph can adopt, when it is embedded in 3-space, can be assigned  energies that depend on such intramolecular interactions as the Coulomb attraction,  the van der Waal's force, internal hydrogen bonds, and hydrophobic interactions.  Algorithms exist for searching through the space of conformations to find local  minima having low energy (these are called "conformers"). Even relatively rigid  molecules may have tens or even hundreds of low energy conformers. The training  data does not indicate which of these conformers is the "bioactive" one--that is,  the conformer that binds to the binding site and produces the observed binding  activity.  Second, even if the bioactive conformer were known, the features describing the  molecular surface--because they are measured in the frame of reference of the bind-  ing site--change as the molecule rotates and translates (rigidly) in space.  Hence, if we consider feature space, each training example (bond graph) induces a  family of 6-dimensional manifolds. Each manifold corresponds to one conformer as  it rotates and translates (6 degrees of freedom) in space. For a classification task,  a positive decision region for "active" molecules would be a region that intersects  at least one manifold of each active molecule and no manifolds of any inactive  molecules. Finding such a decision region is quite difficult, because the manifolds  are difficult to compute.  218 Dietterich, Jain, Lathrop, and Lozano-Perez  A similar "feature manifold problem" arises in handwritten character recognition.  There, the training examples are labelled handwritten digits, the features are ex-  tracted by taking a digitized gray-scale picture, and the feature values depend on  the rotation, translation, and zoom of the camera with respect to the character.  We can formalize this situation as follows. Let xi, i -- 1,..., N be training exam-  ples (i.e., bond graphs or physical handwritten digits), and let f(:ri) be the label  associated with :ri (i.e., the measured activity of the molecule or the identity of the  handwritten digit). Suppose we extract n real-valued features V(zi) to describe ob-  ject :ri and then employ, for example, a multilayer sigmoid network to approximate  f(:r) by ](x) = g(V(x)). This is the ordinary supervised learning task.  However, the feature manifold problem arises when the extracted features depend  on the "pose" of the example. We will define the pose to be a vector p of parameters  that describe, for example, the rotation, translation, and conformation of a molecule  or the rotation, translation, scale, and line thickness of a handwritten digit. In this  case, the feature vector V(x,p) depends on both the example and the pose.  Within the handwritten character recognition community, several techniques have  been developed for dealing with the feature manifold problem. Three existing ap-  proaches are standardized poses, the tangent-prop method, and the tangent-distance  method. Jain et al. (1993a, 1993b) describe a new method--dynamic reposing--  that applies supervised learning simultaneously to discover the "best" pose p? of  each training example a:i and also to learn an approximation to the unknown func-  tion f(:r) as ](:ri) = g(V(:ri,p?)). In this paper, we briefly review each of these  methods and then compare the performance of standardized poses, tangent dis-  tance, and dynamic reposing to the problem of predicting the activity of musk  molecules.  2  FOUR APPROACHES TO THE FEATURE  MANIFOLD PROBLEM  2.1 STANDARDIZED POSES  The simplest approach is to select only one of the feature vectors V(:ri,pi) for each  example by constructing a function, Pi = ,S'(:ri), that computes a standard pose  for each object. Once Pi is chosen for each example, we have the usual super-  vised learning task--each training example has a unique feature vector, and we can  approximate f by ](:r)= g(V(x,,S'(:r))).  The difficulty is that S can be very hard to design. In optical character recognition,  S typically works by computing some pose-invariant properties (e.g., principal axes  of a circumscribing ellipse) of xi and then choosing pi to translate, rotate, and scale  :ri to give these properties standard values. Errors committed by OCR algorithms  can often be traced to errors in the S function, so that characters are incorrectly  positioned for recognition.  In drug activity prediction, the standardizing function S must guess which con-  former is the bioactive conformer. This is exceedingly difficult to do without addi-  tional information (e.g., 3-D atom coordinates of the molecule bound in the binding  A Comparison of Dynamic Reposing and Tangent Distance for Drug Activity Prediction 219  site as determined by x-ray crystallography). In addition, $ must determine the  orientation of the bioactive conformers within the binding site. This is also quite  difficult--the bioactive conformers must be mutually aligned so that shared poten-  tial chemical interactions (e.g., hydrogen bond donors) are superimposed.  2.2 TANGENT PROPAGATION  The tangent-prop approach (Simard, Victorri, LeCun, & Denker, 1992) also em-  ploys a standardizing function $, but it augments the learning procedure with the  constraint that the output of the learned function g(V(x,p)) should be invariant  with respect to slight changes in the poses of the examples:  Ilvr g(V(x,p))I:s<.ll = 0,  where [l' 11 indicates Euclidean norm. This constraint is incorporated by using the  left-hand-side as a regularizer during backpropagation training.  Tangent-prop can be viewed as a way of focusing the learning algorithm on those  input features and hidden-unit features that are invariant with respect to slight  changes in pose. Without the tangent-prop constraint, the learning algorithm  may identify features that "accidentally" discriminate between classes. However,  tangent-prop still assumes that the standard poses are correct. This is not a safe  assumption in drug activity prediction.  2.3 TANGENT DISTANCE  The tangent-distance approach (Simard, LeCun & Denker, 1993) is a variant of the  nearest-neighbor algorithm that addresses the feature manifold problem. Ideally,  the best distance metric to employ for the nearest-neighbor algorithm with feature  manifolds is to compute the "manifold distance"--the point of nearest approach  between two manifolds:  manifold-dist (x, x2) = min IIV(x,px) - v(2,p2)ll.  P  This is very expensive to compute, however, because the manifolds can have highly  nonlinear shapes in feature space, so the manifold distance can have many local  minima.  The tangent distance is an approximation to the manifold distance. It is computed  by approximating the manifold by a tangent plane in the vicinity of the standard  poses. Let Ji be the Jacobian matrix defined by (Ji)jk = OV(xi,Pi)j/O(pi)k, which  gives the plane tangent to the manifold of molecule xi at pose pi. The tangent  distance is defined as  tangent-dist (x,, x2): minll[V(xl,pl ) + Jla]- [V(x2,p2) + Jab]ll  i;lb '  where p, = S(Xl) and p2 = $(x2). The column vectors a and b give the change  in the pose required to minimize the distance between the tangent planes approx-  imating the manifolds. The values of a and b minimizing the right-hand side can  be computed fairly quickly via gradient descent (Simard, personal communication).  In practice, only poses close to $(xl) and $(x2) are considered, but this provides  220 Dietterich, Jain, Lathpop, and Lozano-Perez  more opportunity for objects belonging to the same class to adopt poses that make  them more similar to each other.  In experiments with handwritten digits, Simard, LeCun, and Denker (1993) found  that tangent distance gave the best performance of these three methods.  2.4 DYNAMIC REPOSING  All of the preceding methods can be viewed as attempts to make the final predicted  output f(x) invariant with respect to changes in pose. Standard poses do this by  not permitting poses to change. Tangent-prop adds a local invariance constraint.  Tangent distance enforces a somewhat less local invariance constraint.  In dynamic reposing, we make f invariant by defining it to be the maximum value  (taken over all poses p) of an auxiliary function g:  ](x) = max g(V(x,p)).  p  The function g will be the function learned by the neural network.  Before we consider how g is learned, let us first consider how it can be used to  predict the activity of a new molecule x t. To compute f(xt), we must find the pose  pt. that maximizes g(V(xt,pt*)). We can do this by performing a gradient ascent  starting from the standard pose $(x) and moving in the direction of the gradient  of g with respect to the pose: 7p,g(V(xt,pt)).  This process has an important physical analog in drug activity prediction. If x t is  a new molecule and g is a learned model of the binding site, then by varying the  pose pt we are imitating the process by which the molecule chooses a low-energy  conformation and rotates and translates to "dock" with the binding site.  In handwritten character recognition, this would be the dual of a deformable tem-  plate model: the template (g) is held fixed, while the example is deformed (by  rotation, translation, and scaling) to find the best fit to the template.  The function g is learned iteratively from a growing pool of feature vectors. Initially,  the pool contains only the feature vectors for the standard poses of the training ex-  amples (actually, we start with one standard pose of each low energy conformation  of each training example). In iteration j, we apply backpropagation to learn hy-  pothesis gj from selected feature vectors drawn from the pool. For each molecule,  one feature vector is selected by performing a forward propagation (i.e., computing  g(V(xi, Pi))) of all feature vectors of that molecule and selecting the one giving the  highest predicted activity for that molecule.  After learning gj, we then compute for each conformer the pose +l that maximizes  gj(V(xi,p)):  +1 _ argmax gj(V(xi,p)).  p  From the chemical perspective, we permit each of the molecules to "dock" to the  current model gJ of the binding site.  The feature vectors V(xi,p +1) corresponding to these poses are added to the pool  of poses, and a new hypothesis gj+l is learned. This process iterates until the poses  A Comparison of Dynamic Reposing and Tangent Distance for Drug Activity Prediction 221  cease to change. Note that this algorithm is analogous to the EM procedure (Redner  &; Walker, 1984) in that we accomplish the simultaneous optimization of g and the  poses {pi} by conducting a series of separate optimizations of g (holding {pi} fixed)  and {pi} (holding g fixed).  We believe the power of dynamic reposing results from its ability to identify the  features that are critical for discriminating active from inactive molecules. In the  initial, standard poses, a learning algorithm is likely to find features that "acciden-  tally" discriminate actives from inactives. However, during the reposing process,  inactive molecules will be able to reorient themselves to resemble active molecules  with respect to these features. In the next iteration, the learning algorithm is  therefore forced to choose better features for discrimination.  Moreover, during reposing, the active molecules are able to reorient themselves so  that they become more similar to each other with respect to the features judged  to be important in the previous iteration. In subsequent iterations, the learning  algorithm can "tighten" its criteria for recognizing active molecules.  In the initial, standard poses, the molecules are posed so that they resemble each  other along all features more-or-less equally. At convergence, the active molecules  have changed pose so that they only resemble each other along the features impor-  tant for discrimination.  3 AN EXPERIMENTAL COMPARISON  3.1 MUSK ACTIVITY PREDICTION  We compared dynamic reposing with the tangent distance and standard pose meth-  ods on the task of musk odor prediction. The problem of musk odor prediction has  been the focus of many modeling efforts (e.g., Bersuker, et al., 1991; Fehr, et al.,  1989; Narvaez, Lavine & Jurs, 1986). Musk odor is a specific and clearly iden-  tifiable sensation, although the mechanisms underlying it are poorly understood.  Musk odor is determined almost entirely by steric (i.e., "molecular shape") effects  (Ohloff, 1986). The addition or deletion of a single methyl group can convert an  odorless compound into a strong musk. Musk molecules are similar in size and  composition to many kinds of drug molecules.  We studied a set of 102 diverse structures that were collected from published studies  (Narvaez, Lavine & Jurs, 1986; Bersuker, et al., 1991; Ohloff, 1986; Fehr, et al.,  1989). The data set contained 39 aromatic, oxygen-containing molecules with musk  odor and 63 homologs that lacked musk odor. Each molecule was conformation-  ally searched to identify low energy conformations. The final data set contained  6,953 conformations of the 102 molecules (for full details of this data set, see Jain,  et al., 1993a). Each of these conformations was placed into a starting pose via a  hand-written $ function. We then applied nearest neighbor with Euclidean dis-  tance, nearest neighbor with the tangent distance, a feed-forward network without  reposing, and a feed-forward network with the dynamic reposing method. For dy-  namic reposing, five iterations of reposing were sufficient for convergence. The time  required to compute the tangent distances far exceeds the computation times of  the other algorithms. To make the tangent distance computations feasible, we only  222 Dietterich, Jain, Lathrop, and Lozano-Perez  Table 1: Results of 20-fold cross-validation on 102 musk molecules.  Method  Nearest neighbor (Euclidean distance)  Neural network (standard poses)  Nearest neighbor (Tangent distance)  Neural network (dynamic reposing)  Percent Correct  75  75  79  91  Table 2: Neural network cross-class predictions (percent correct)  N  Molecular class:  13 21 27 14  Standard poses 85 76 74 57  Dynamic reposing 100 90 85 71  computed the tangent distance for th 200 neighbors that were nearest in Euclidean  distance. Experiments with a subset of the molecules showed that this heuristic in-  troduced no error on that subset.  Table i shows the results of a 20-fold cross-validation of all four methods. The  tangent distance method does show improvement with respect to a standard neu-  ral network approach (and with respect to the standard nearest neighbor method).  However, the dynamic reposing method outperforms the other two methods sub-  stantially.  An important test for drug activity prediction methods is to predict the activity  of molecules whose molecular structure (i.e., bond graph) is substantially different  from the molecules in the training set. A weakness of many existing methods for  drug activity prediction (Hansch &: Fujita, 1964; Hansch, 1973) is that they rely on  the assumption that all molecules in the training and test data sets share a common  structural skeleton. Because our representation for molecules concerns itself only  with the surface of the molecule, we should not suffer from this problem. Table 2  shows four structural classes of molecules and the results of "class holdout" exper-  iments in which all molecules of a given class were excluded from the training set  and then predicted. Cross-class predictions from standard poses are not particularly  good. However, with dynamic reposing, we obtain excellent cross-class predictions.  This demonstrates the ability of dynamic reposing to identify the critical discrimi-  nating features. Note that the accuracy of the predictions generally is determined  by the size of the training set (i.e., as more molecules are withheld, performance  drops). The exception to this is the right-most class, where the local geometry of  the oxygen atom is substantially different from the other three classes.  A Comparison of Dynamic Reposing and Tangent Distance for Drug Activity Prediction 223  4 CONCLUDING REMARKS  The "feature manifold problem" arises in many application tasks, including drug  activity prediction and handwritten character recognition. A new method, dynamic  reposing, exhibits performance superior to the best existing method, tangent dis-  tance, and to other standard methods on the problem of musk activity prediction.  In addition to producing more accurate predictions, dynamic reposing results in a  learned binding site model that can guide the design of new drug molecules. Jain,  et al., (1993a) shows a method for visualizing the learned model in the context of  a given molecule and demonstrates how the model can be applied to guide drug  design. Jain, et al., (1993b) compares the method to other state-of-the-art meth-  ods for drug activity prediction and shows that feed-forward networks with dynamic  reposing are substantially superior on two steroid binding tasks. The method is cur-  rently being applied at Arris Pharmaceutical Corporation to aid the development  of new pharmaceutical compounds.  Acknowledgement s  Many people made contributions to this project. The authors thank Barr Bauer,  John Burns, David Chapman, Roger Critchlow, Brad Katz, Kimberle Koile, John  Park, Mike Ross, Teresa Webster, and George Whitesides for their efforts.  References  Bersuker, I. B., Dimoglo, A. S., Yu. Gorbachov, M., Vlad, P. F., Pesaro, M. (1991).  New Journal of Chemistry, 15, 307.  Fehr, C., Galindo, J., Haubrichs, R., Perret, R. (1989). Heir. Chim. Acta, 72, 1537.  Hansch, C. (1973). In C. J. Cavallito (Ed.), Structure-Activity Relationships. Ox-  ford: Pergamon.  Hansch, C., Fujita, T. (1964). J. Am. Chem. Soc., 86, 1616.  Jain, A. N., Dietterich, T. G., Lathrop, R. H., Chapman, D., Critchlow, R. E.,  Bauer, B. E., Webster, T. A., Lozano-Perez, T. (1993a). A shape-based method for  molecular design with adaptive alignment and conformational selection. Submitted.  Jain, A., Koile, K., Bauer, B., Chapman, D. (1993b). Compass: A 3D QSAR  method. Performance comparisons on asteroid benchmark. Submitted.  Narvaez, J. N., Lavine, B. K., Jurs, P. C. (1986). Chemical Senses, 11, 145-156.  Ohloff, G. (1986). Chemistry of odor stimuli. E:perientia, J, 271.  Redher, R. A., Walker, H. F. (1984). Mixture densities, maximum likelihood, and  the EM algorithm. SIAM Review, 26 (2) 195-239.  Simard, P. Victorri, B., Le Cun, Y. Denker, J. (1992). Tangent Prop A formalism  for specifying selected invariances in an adaptive network. In Moody, J. E., Hanson,  S. J., Lippmann, R. P. (Eds.) Advances in Neural Information Processing Systems  J. San Mateo, CA: Morgan Kaufmann. 895-903.  Simard, P. Le Cun, Y., Denker, J. (1993). Efficient pattern recognition using a  new transformation distance. In Hanson, S. J., Cowan, J. D., Giles, C. L. (Eds.)  Advances in Neural Information Processing Systems 5, San Mateo, CA: Morgan  Kaufmann. 50-58.  
Optimal Brain Surgeon:  Extensions and performance comparisons  Babak Hassibi*  David G. Stork  Gregory Wolff  Takahiro Watanabe  Ricoh California Research Center  2882 Sand Hill Road Suite 115  Menlo Park, CA 94025-7022  and  *Department of Electrical Engineering  105B Durand Hall  Stanford University  Stanford, CA 94305-4055  Abstract  We extend Optimal Brain Surgeon (OBS) -- a second-order  method for pruning networks -- to allow for general error mea-  sures, and explore a reduced computational and storage implemen-  tation via a dominant eigenspace decomposition. Simulations on  nonlinear, noisy pattern classification problems reveal that OB$  does lead to improved generalization, and performs favorably in  comparison with Optimal Brain Damage (OBD). We find that the  required retraining steps in OBD may lead to inferior generaliza-  tion, a result that can be interpreted as due .to injecting noise back  into the system. A common technique is to stop training of a large  network at the minimum validation error. We found that the test  error could be reduced even further by means of OB$ (but not  OBD) pruning. Our results justify the t - o approximation used  in OB$ and indicate why retraining in a highly pruned network  may lead to inferior performance.  263  264 Hassibi, Stork, Wolff, and Watanabe  1 INTRODUCTION  The fundamental theory of generalization favors simplicity. For a given level of  performance on observed data, models with fewer parameters can be expected to  perform better on test data. In practice, we find that neural networks with fewer  weights typically generalize better than large networks with the same training error.  To this end, LeCun, Denker and Solla's (1990) Optimal Brain Damage method  (OBD) sought to delete weights by keeping the training error as small as possible.  Hassibi and Stork (1993) extended OBD to include the off-diagonal terms in the  network's Hessian, which were shown to be significant and important for pruning  in classical and benchmark problems.  OBD and Optimal Brain Surgeon (OBS) share the same basic approach of training  a network to (local) minimum in error at weight w*, and then pruning a weight  that leads to the smallest increase in the training error. The predicted functional  increase in the error for a change in full weight vector 5w is:  OE )T 1 02E  6E =  6w 6w v   0 =H   + o([[wll 3) , (1)     m0  where H is the Hessian matrix. The first term vanishes because we are at a local  minimum in error; we ignore third- and higher-order terms (Gorodkin et al., 1993).  Hassibi and Stork (1993) first showed that the general solution for minimizing this  function given the constraint of deleting one weight was:  2  Wq 1 Wq  W---- [$_l]q q $ -1 .eq and Lq-- 2 [$-l]qq (2)  Here, eq is the unit vector along the qth direction in weight space and Lq is the  saliency of weight q -- an estimate of the increase in training error if weight q is  pruned and the other weights updated by the left equation in Eq. 2.  2  GENERAL ERROR MEASURES AND FISHER'S  METHOD OF SCORING  In this section we show that the recursive procedure for computing the inverse  Hessian for sum squared errors presented in Hassibi and Stork (1993) generalizes  to any twice differentiable distance norm and that the key approximation based on  Fisher's method of scoring is still valid.  Consider an arbitrary twice differentiable distance norm d(t, o) where t is the de-  sired output (teaching vector) and o = F(w, in) the actual output. Given a weight  vector w, F maps the input vector in to the output; the total error over P patterns  1 P , o[kl).  is E =  k=l d( t[k] It is straightforward to show that for a single output  unit network the Hessian is:  Optimal Brain Surgeon: Extensions and Performance Comparisons 265  1 P cgF(w, in [k]) c92d(t[k],o []) cgFT(w, in [])  H =   Ow 002 Ow  k=l  1 P cgd(t [1, o [1) c92F(w, in [k])  7 0o 0w 2  k=l  (3)  The second term is of order O(]lt - oil); using Fisher's method of scoring (Sever &  Wild, 1989), we set this term to zero. Thus our Hessian reduces to:  I OF(w, in 02d(t[l,o 0FT(w, in  H: 0w 002 0w (4)  k=l  c92 d(t [kl ,o[l )  We define X  (w'in[) and a  and following the logic of Hsibi  0W 002  and Sork (1993) we can easily show tha the recursion for computing the inverse  Hessian becomes:  T  --1 Hi ' Xk+l 'Xk+l ' Hi , H 1: ct-1I, and H 1 = H -1  Hk+ 1 = H 1 - ___p + XkL 1  H 1  Xk+ 1 '  ak  (5)  where  is a small parameter -- effectively a weight decay constant. Note how  different error measures d(t, o) scale the gradient vectors Xk forming the Hessian  (Eq. 4). For the squared error d(t, o) -- (t - 0) 2, we have a: 1, and all gradient  vectors are weighted equally. The cross entropy or Kullback-Leibler distance,  o (1 -o) 0 _< o,t < 1 (6)  d(t,o) = olog  + (1 - o) log (1 - t) ' -  i Hence if o [] is close to zero or one, X is given a large  yields ak = Otkl(1--OIkl)'  weight in the Hessian; conversely, the smallest value of a occurs when o [] = 1/2.  This is desirable and makes great intuitive sense, since in the cross entropy norm  the value of o [] is interpreted as the probability that the kth input pattern belongs  to a particular class, and therefore we give large weight to X whose class we are  most certain and small weight to those which we are least certain.  3 EIGENSPACE DECOMPOSITION  Although OBS has been shown to be a powerful method for small and interme-  diate sized networks -- Hassibi, Stork and Wolff (1993) applied OBS successfully  to NETtalk -- its use in larger problems is difficult because of large storage and  computation requirements. For a network of n weights, simply storing the Hessian  requires 0(n2/2) elements and O(Pn 2) computations are needed for each pruning  step. Reducing this computational burden requires some type of approximation.  Since OBS uses the inverse of the Hessian, any approximation to OBS will at some  level reduce to an approximation of H. For instance OBD uses a diagonal approx-  imation; magnitude-based methods use an isotropic approximation; and dividing  the network into subsets (e.g., hidden-to-output and input-to-hidden) corresponds  to the less-restrictive block diagonal approximation. In what follows we explore the  dominant eigenspace decomposition of the inverse Hessian as our approximation. It  should be remembered that all these are subsets of the full OBS approach.  266 Hassibi, Stork, Wolff, and Watanabe  3.1 Theory  The dominant eigendecomposition is the best low-rank approximation of a matrix  (in an induced 2-norm sense). Since the largest eigenvalues of H-1 are the smallest  eigenvalues of H, this method will, roughly speaking, be pruning weights in the  approximate nullspace of H. Dealing with a low rank approximation of H -1 will  drastically reduce the storage and computational requirements.  Consider the eigendecomposition of H:  0 ) , ,  H = (UsUN) N UN = UsYlsUs + UNNUN'  (7)  where Es contains the largest eigenvalues of H and N the smallest ones. (We use  the subscripts q and N to loosely connote signal and noise.) The dimension of the  noise subspace is typically ra << n. Us and UN are n x (n -- ra) and n x ra matrices  that span the dominant eigenspace of H and H-l, and * denotes matrix transpose  and complex conjugation. If, as suggested above, we restrict the weight prunings to  lie in UN, we obtain the following saliency and full weight change when removing  the qth weight:  2  I Wq (8)  Jq ----  eq T' UN' "]r 1  rv-e q  Wq -- 1   (V = eqT'UN' [1. Uv .eq N UNeq , (9)  where we have used 'bars' to indicate that these are approximations to Eq. 2. Note  now that we need only to store ]N and UN, which have roughly nm elements.  Likewise the computation required to estimate N and UN is O(Pnm).  The bound on Lq is:  Lq _ Lq _ Lq q- 2 LqLq 1  wq  ' er(s)' (10)  where er(s) is the smallest eigenvalue of $. Moreover if er(s) is large enough so  1 we have the following simpler form:  that rr(s) > [H_l]q q  Lq < .q < Lq (11)  -- --1--  [H-1lqq_(s)  In either case Eqs. 10 and 11 indicate that the larger _(s) is, the tighter the bounds  are. Thus if the subspace dimension m is such that the eigenvalues in Us are large,  then we will have a good approximation.  LeCun, Simard and Pearlmutter (1993) have suggested a method that can be used  to estimate the smallest eigenvectors of the Hessian. However, for OBS (as we shall  see) it is best to use the Hessian with the t - o approximation, and their method  is not appropriate.  Optimal Brain Surgeon: Extensions and Performance Comparisons 267  3.2 Simulations  We pruned networks trained on the three Monk's problems (Thrun et al., 1991) us-  ing the full OBS and a 5-dimensional eigenspace version of OBS, using the validation  error rate for stopping criterion. (We chose a 5-dimensional subspace, because this  reduced the computational complexity by an order of magnitude.) The Table shows  the number of weights obtained. It is clear that this eigenspace decomposition was  not particularly successful. It appears as though the the off-diagonal terms in H be-  yond those in the eigenspace are important, and their omission leads to bad pruning.  However, this warrants further study.  Monkl  Monk2  Monk3  unpruned OBS 5-d eigenspace  58  39  39  14  16  4  28  27  11  4 OBS/OBD COMPARISON  General criteria for comparing pruning methods do not exist. Since such meth-  ods amount to assuming a particular prior distribution over the parameters, the  empirical results usually tell us more about the problem space, than about the  methods themselves. However, for two methods, such as OBS and OBD, which  utilize the same cost function, and differ only in their approximations, empirical  comparisons can be informative. Hence, we have applied both OBS and OBD to  several problems, including an artificially generated statistical classification task,  and a real-world copier voltage control problem. As we show below, the OBS algo-  rithm usually results in better generalization performance.  4.1 MULTIPLE GAUSSIAN PRIORS  We created a two-catagory classification problem with a five-dimensional in-  put space. Category A consisted of two Gaussian distributions with mean  vectors /A1 ---- (1,1,0,1,.5) and /A2 = (0,0,1,0,.5) and covariances nl =  Diag[0.99, 1.0, 0.88, 0.70, 0.95] and 2: Diag[1.28, 0.60, 0.52, 0.93, 0.93] while cat-  egory B had means /B -- (0,1,0,0,.5) and /B2 = (1,0,1,1,.5) and covariances  B1 = Dia910.84, 0.68, 1.28, 1.02, 0.89] and B2 = Diag[0.52, 1.25, 1.09, 0.64, 1.13].  The networks were feedforward with 5 input units, 9 hidden units, and a single  output unit (64 weights total). The training and the test sets consisted of 1000  patterns each, randomly chosen from the equi-probable categories. The problem  was a difficult one: even with the somewhat large number of weights it was not  possible to obtain less than 0.15 squared error per training pattern. We trained the  networks to a local error minimum and then applied OBD (with retraining after  each pruning step using backpropagation) as well as OBq.  Figure 1 (left) shows the training errors for the network as a function of the number  of remaining weights during pruning by OBq and by OBD. As more weights are  pruned the training errors for both OBq and OBD typically increase. Comparing  the two graphs for the first pruned weights, the training error for OBD and OBq  are roughly equal, after which the training error of OBq is less until the 24th weight  268 Hassibi, Stork, Wolff, and Watanabe  E  .165  .16  .155  .15  30  Train  OBD  OBS,,%  E  22  21!  21  20!  2  19'.  30  Test  OBS  35 40 45 50 55 60 65 35 40 45 50 55 60 65  number of weights number of weights  Figure 1: OBS and OBD training error on a sum of Gaussians prior pattern clas-  sification task as a function of the number of weights in the network. (Pruning  proceeds right to left.) OBS pruning employed o = 10 -6 (cf., Eq. 5); OBD em-  ployed 60 retraining epochs after each pruning.  is removed. The reason OBD training is initially slightly better is that the network  was not at an exact local minimum; indeed in the first few stages the training error  for OBD actually becomes less than its original value. (Training exhaustively to  the true local minimum took prohibitively long.) In contrast, due to the t - o  approximation OBS tries to keep the network response close to where it was, even  if that isn't the minimum w*. We think it plausible that if the network were at an  exact local minimum OBS would have had virtually identical performance.  Since OBD is using retraining the only reason why OBS can outperform after the  first steps is that OBD has removed an incorrect weight, due to its diagonal approx-  imation. (The reason OBS behaves poorly after removing 24 weights -- a radically  pruned net -- may be that the second-order approximation breaks down at this  point.) We can see that the minimum on test error occurs before this breakdown,  meaning that the failed approximation (Fig. 2) does not affect our choice of the  optimal network, at least for this problem.  The most important and interesting result is the test error for these pruned networks  (Figure 1, right). The test error for OBD does not show any consistent behaviour,  other than the fact that on the average it generally goes up. This is contrary to what  one would expect of a pruning algorithm. It seems that the retraining phase works  against the pruning process, by tending to reinforce overfitting, and to reinject the  training set noise. For 'OBS, however, the test error consistently decreases until  after removing 22 weights a minimum is reached, because the t - o approximation  avoids reinjecting the training set noise.  4.2 OBS/OBD PRUNING AND "STOPPED" NETWORKS  A popular method of avoiding overfitting is to stop training a large net when the  validation error reaches a minimum. In order to explore whether pruning could  improve the performance on such a "stopped" network (i.e., not at w*), we mon-  itored the test error for the above problem and recorded the weights for which a  minimum on the test set occured. We then applied OBS and OBD to this network.  Optimal Brain Surgeon: Extensions and Performance Comparisons 269  E  204  202  200  198  196  194  OBS  35 40 45 50 55 60  number of weights  Figure 2: A 64-weight network was trained to minimum validation error on the  Gaussian problem -- not w* -- and then pruned by OBD and by OBS. The test  error on the resulting network is shown. (Pruning proceeds from right to left.) Note  especially that even though the network is far from w*, OBS leads lower test error  over a wide range of prunings, even through OBD employs retraining.  The results shown in Figure 2 indicate that with OB$ we were able to reduce the  test error, and this reached a minimum after removing 17 weights. OBD was not  able to consistently reduce the test error.  This last result and those from Fig. 2 have important consequences. There are no  universal stopping criteria based on theory (for the reasons mentioned above), but  it is a typical practice to use validation error as such a criterion. As can be seen  in Figure 2, the test error (which we here consider a validation error) consistantly  decreases to a unique miniumum for pruning by OBS. For the network pruned (and  continuously retrained) by OBD, there is no such structure in the validation curves.  There seems to be no reliable clue that would permit the user to know when to stop  pruning.  4.3 COPIER CONTROL APPLICATION  The quality of an image produced by a copier is dependent upon a wide variety of  factors: time since last copy, time since last toner cartridge installed, temperature,  humidity, overall graylevel of the source document, etc. These factors interact in a  highly non-linear fashion, so that mathematical modelling of their interrelationships  is difficult. Morita et al. (1992) used backpropagation to train an 8-4-8 network (65  weights) on real-world data, and managed to achieve an RMS voltage error of 0.0124  on a critical control plate. We pruned his network with both OBD with retraining  as well as with OBS. When the network was pruned by OBD with retraining, the  test error continually increased (erratically) such that at 34 remaining weights, the  RMS error was 0.023. When also we pruned the original net by OBS, and the test  error gradually decreased such that at the same number of weights the test error  was 0.012 -- significantly lower than that of the net pruned by OBD.  270 Hassibi, Stork, Wolff, and Watanabe  5 CONCLUSIONS  We compared pruning by OBS and by OBD with retraining on a difficult non-linear  statistical pattern recognition problem and found that OBS led to lower generaliza-  tion error. We also considered the widely used technique of training large nets to  minimum validation error. To our surprise, we found that subsequent pruning by  OBS lowered generalization error, thereby demonstrating that such networks still  have overfitting problems. We have found that the dominant eigenspace approach  to OBS leads to poor performance. Our simulations support the claim that the  t - o approximation used in OBS avoids reinjecting training set noise into the net-  work. In contrast, including such t - o terms in OBS reinjects training set noise  and degrades generalization performance, as does retraining in OBD.  Acknowledgements  Thanks to T. Kailath for support of B.H. through grants AFOSR 91-0060 and  DAAL03-91-C-0010. Address reprint requests to Dr. Stork: stork@crc.ricoh.com.  References  J. Gorodkin, L. K. Hansen, A. Krogh, C. Svarer and O. Winther. (1993) A quanti-  tative study of pruning by Optimal Brain Damage. International Journal of Neural  Systems 4(2) 159-169.  B. Hassibi & D. G. Stork. (1993) Second order derivatives for network pruning:  Optimal Brain Surgeon. In S. J. Hanson, J. D. Cowan and C. L. Giles (eds.),  Advances in Neural Information Processing Systems 5, 164-171. San Mateo, CA:  Morgan Kaufmann.  B. Hassibi, D. G. Stork & G. Wolff. (1993) Optimal Brain Surgeon and general  network pruning. Proceedings of ICNN 93, San Francisco I IEEE Press. 293-299.  Y. LeCun, J. Denker & S. Solla. (1990) Optimal Brain Damage. In D. Touretzky  (ed.), Advances in Neural Information Processing Systems 2, 598-605. San Mateo,  CA: Morgan Kaufmann.  Y. LeCun, P. Simard & B. Pearlmutter. (1993) Automatic learning rate max-  imization by on-line estimation of the Hessian's eigenvectors. In S. J. Hanson,  J. D. Cowan & C. L. Giles (eds.), Advances in Neural Information Processing Sys-  tems 5, 156-163. San Mateo, CA: Morgan Kaufmann.  T. Morita, M. Kanaya, T. Inagaki, H. Murayama & S. Kato. (1992) Photo-copier  image density control using neural network and fuzzy theory. Second International  Workshop on Industrial Fuzzy Control 4 Intelligent Systems December 2-4, College  Station, TX, 10.  S. Thrun and 23 co-authors. (1991) The Monk's Problems -- A performance com-  parison of different learning algorithms. CMU-CS-91-197 Carnegie-Mellon Univer-  sity Dept. of Computer Science Technical Report.  
An Analog VLSI Model of Central Pattern  Generation in the Leech  Micah S. Siegel*  Department of Electrical Engineering  Yale University  New Haven, CT 06520  Abstract  I detail the design and construction of an analog VLSI model of the  neural system responsible for swimming behaviors of the leech. Why  the leech? The biological network is small and relatively well  understood, and the silicon model can therefore span three levels of  organization in the leech nervous system (neuron, ganglion, system); it  represents one of the first comprehensive models of leech swimming  operating in real-time. The circuit employs biophysically motivated  analog neurons networked to form multiple biologically inspired silicon  ganglia. These ganglia are coupled using known interganglionic  connections. Thus the model retains the flavor of its biological  counterpart, and though simplified, the output of the silicon circuit is  similar to the output of the leech swim central pattern generator. The  model operates on the same time- and spatial-scale as the leech nervous  system and will provide an excellent platform with which to explore  real-time adaptive locomotion in the leech and other "simple"  invertebrate nervous systems.  1. INTRODUCTION  A Central Pattern Generator (CPG) is a network of neurons that generates rhythmic  output in the absence of sensory input (Rowat and Selverston, 1991). It has been  Present address: Micah Siegel, Computation and Neural Systems, Mail Stop 139-74  California Institute of Technology, Pasadena, CA 91125.  622  An Analog VLSI Model of Central Pattern Generation in the Leech 623  I tonic  width  Figure 1. Silicon neuromime. The circuit includes tonic excitation, inhibitory synapses  and an inhibitory recovery time. Note that there are two inhibitory synapses per device.  Iionic sets the level of tonic excitatory input; Vinhib sets the synaptic strength; Irecoy  .determines the inhibitory recovery time.  suggested that invertebrate central pattern generation may represent an excellent theatre  within which to explore silicon implementations of adaptive neural systems: invertebrate  CPG networks are orders of magnitude smaller than their vertebrate counterparts, much  detailed information is available about them, and they guide behaviors that may be of  technological interest (Ryckebusch et al., 1989). Furthermore, CPG networks are  typically embedded in larger neural circuits and are integral to the neural correlates of  adaptive behavior in many natural organisms (Friesen, 1989).  On strategy for modeling "simple" adaptive behaviors is first to evolve a biologically  plausible framework within which to include increasingly more sophisticated and  verisimilar adaptive mechanisms; because the model of leech swimming presented in this  paper encompasses three levels of organization in the leech central nervous system, it  may provide an ideal such structure with which to explore potentially useful adaptive  mechanisms in the leech behavioral repertoire. Among others, these mechanisms include:  habituation of the swim response (Debski and Friesen, 1985), the local bending reflex  (Lockery and Kristan, 1990), and conditioned learning of the stepping and shortening  behaviors (Sahley and Ready, 1988).  624 Siegel  B  phase  $0   100   123  t L.  282 t ,  c  Figure 2. The individual ganglion. (A) Cycle phases of the  oscillator neurons in the biological ganglion (from Friesen,  1989). (B) Somatic potential of the simplified silicon  ganglion. (C) Circuit diagram of silicon ganglion using cells  and synaptic connections identified in the leech ganglion.  2. LOCOMOTORY CPG IN THE LEECH  As a first step toward modeling a full repertoire of adaptive behavior in the medicinal  leech (Hirundo medicinalis), I have designed, fabricated, and successfully tested an analog  silicon model of one critical neural subsystem w the coupled oscillatory central pattern  generation network responsible for swimming. A leech swims by undulating its  segmented body to form a rearward-progressing body wave. This wave is analogous to  the locomotory undulations of most elongated aquatic animals (e.g. fish), and some  terrestrial amphibians and reptiles (including salamanders and snakes) (Friesen, 1989).  The moving crests and troughs in the body wave are produced by phase-delayed contractile  rhythms of the dorsal and ventral body wall along successive segments (Stent and Kristan,  1981). The interganglionic neural subsystem that subserves this behavior constitutes an  important modeling platform because it guides locomotion in the leech over a wide range  of frequencies and adapts to varying intrinsic and extrinsic conditions (Debski and Friesen,  1985).  In the medicinal leech, interneurons that coordinate the rearward-progressing swimming  contractions undergo oscillations in membrane potential and fire impulses in bursts. It  appears that the oscillatory activity of these interneurons arises from a network rhythm  that depends on synaptic interaction between neurons rather than from an endogenous  polarization rhythm arising from inherently oscillatory membrane potentials in individual  An Analog VLSI Model of Central Pattern Generation in the Leech 625  A  ganglion: 9 10 11  head tail  B  28  9{27  123 :_  123  28  103 ms  Figure 3. The complete silicon model. (A) Coupled oscillatory ganglia. As in the leech  nervous system, interganglionic connections employ conduction delays. (B) Somatic  recording of cells (28, 27, 123) from three midbody ganglia (9,10,11) in the silicon  model. Notice the phase-delay in homologous cells of successive ganglia. (The apparent  "beat" frequencies riding on the spike bursts are an aliasing artifact of the digital  oscilloscope measurement and the time-scale; all spikes are approximately the same  height.)  neurons (Friesen, 1989). The phases of the oscillatory interneurons form groups clustered  about three phase points spaced equally around the activity cycle. To first  approximation, all midbody ganglia of the leech nerve cord express an identical activity  rhythm. However, activity in each ganglion is phase-delayed with respect to more  anterior ganglia (Friesen, 1989); presumably this is responsible for the undulatory body  wave characteristic of leech swimming.  626 Siegel  3. THE SILICON MODEL  The silicon analog model employs biophysically realistic neural elements (neuromimes),  connected into biologically realistic ganglion circuits. These ganglion circuits are  coupled together using known interganglionic connections. This silicon model thus  spans three levels of organization in the nervous system of the leech (neuron,  ganglion, system), and represents one of the first comprehensive models of leech  swimming (see also Friesen and Stent, 1977). The hope is that this model will provide a  framework for the implementation of adaptive mechanisms related to undulatory  locomotion in the leech and other invertebrates.  The building block of the model CPG is the analog neuromime (see figure 1); it exhibits  many essential similarities to its biological counterpart. Like CPG interneurons in the  leech swim system, the silicon neuromime integrates current across a somatic  "capacitance" and uses positive feedback to generate action potentials whose frequency is  determined by the magnitude of excitatory current input (Mead, 1989). In the leech swim  system, nearly tonic excitatory input is transformed by a system of inhibition to produce  the swim pattern (Friesen, 1989); adjustable tonic excitation is therefore included in  the individual silicon neuromime.  Inhibitory synapses with adjustable weights are also implemented. Like its  biological counterpart, the silicon neuromime includes a characteristic recovery time from  inhibition. From theoretical and experimental studies, such inhibition recovery time is  thought to play an important functional role in the interneurons that constitute the leech  swim system (Friesen and Stent, 1977). Axonal delays have been demonstrated in the  intersegmental interaction between ganglia in the leech. Similar axonal delays have been  implemented in the silicon model using shifting delay lines.  The building block of the distributed model for the leech swim system is the ganglion.  These biologically motivated silicon ganglia are constructed using only (though not all)  identified cells and synaptic connections between cells in the biological system. Cells  27, 28, and 123 constitute a central inhibitory loop within each ganglion. Figure 2  exhibits the simplified diagram and the cycle phases of oscillatory interneurons in both  the biological and the silicon ganglion. As in the leech ganglion, the phase relationships  in the model ganglion fall into three groups, with cells 27, 28, and 123 participating each  in the appropriate group of the oscillatory cycle. It is interesting that, though the silicon  model captures the spirit of the tri-phasic output, the model is imprecise with respect to  the exact phase locations of cells 27, 28, and 123 within their respective groups. This  discrepancy between the silicon model and the biological system may point to the  significance of other swim interneurons for swim pattern generation in the leech.  Undoubtedly, the additional oscillatory interneurons sculpt this tri-phasic output  significantly.  The silicon model of coupled successive segments in the leech is implemented using  these silicon neurons and biologically motivated ganglia. The model employs  interganglionic connections known to exist in the biological system and generates  qualitatively similar output at the same time-scale as the leech system. It appears in the  leech that synchronization between ganglia is governed by the interganglionic synaptic  interaction of interneurons involved in the oscillatory pattern rather than by autonomous  An Analog VLSI Model of Central Pattern Generation in the Leech 627  coordinating neurons (Friesen, 1989). In the silicon model, interganglionic interaction is  represented by a projection from more anterior cell 123 to more posterior cell 28; this  B  100 ms  7igure 4. Phase lag between more anterior and more posterior  segments in both systems. (A) Intersegmental phase lag in  the leech swim system (from Friesen, 1989). (B)  Intersegmental phase lag in the silicon model. Though not  shown in the figure, this cycle repeats at the same frequency as  the cycle in A. (Note change of time scale.)  projection is also observed between cells 123 and 28 of successive ganglia in the leech  (Friesen, 1989), however it is by no means the only such interganglionic connection. In  addition, the biological system utilizes conduction delays in its interganglionic  projections; each of these is modeled in the silicon system by a delay line (Friesen and  Stent, 1977) analogous to an active cable with adjustable propagation speed. Figure 3  demonstrates the silicon model of three coupled ganglia with transmission delays. Notice  that neuromimes in each successive ganglion are phase-delayed from homologous  neuromimes in more anterior ganglia. Figure 4 shows this phase delay more explicitly.  4. DISCUSSION  The analog silicon model of central pauern generation in the leech successfully captures  design principles from three levels of organization in the leech nervous system and has  been tested over a wide range of network parameter values. It operates on the same time-  scale as its biological counterpart and gives rise to ganglionic activity that is qualitatively  similar to activity in the leech ganglion. Furthermore, it maintains biologically  plausible phase relationship between homologous elements of successive ganglia. The  design of the silicon model is intentionally compatible with analog Very Large Scale  Integration (VLSI) technology, making its integrated spatial-scale close to that of the  leech nervous system. It is interesting that this highly simplified model captures  qualitatively the output both within and between ganglia of the leech; it may be  illuminating to explore the functional significance of other swim interneurons by their  inclusion in similar silicon networks. The current model provides an important platform  for future implementations of invertebrate adaptive behaviors, especially those behaviors  related to swim and other locomotory pattern generation. The hope is that such behaviors  628 Siegel  can be evolved incrementally using neuromime models of identified adaptive interneurons  to modulate the swim central pattern generating network.  Acknowledgments  I would like to thank the department of Electrical Engineering at Yale University for  encouraging and generously supporting independent undergraduate research.  References  Rowat, P.F. and Selverston, A.I. (1991). Network, 2, 17-41.  Ryckebusch, S., Bower, J.M., Mead, C., (1989). In D.Touretzky (ed.), Advances in  Neural Information Processing Systems, 384-393. San Mateo, CA: Morgan Kaufmann.  Friesen, W.O. (1989). In J. Jacklet (ed), Neuronal and Cellular Oscillators, 269-316.  New York: Marcel Dekker.  Debski, E.A. and Friesen, W.O. (1985). Journal of Experimental Biology, 116, 169-  188.  Lockery, S.R. and Kristan, W.B. (1990). Journal of Neuroscience, 10(6), 1811-1815.  Sahley, C.L. and Ready, D.F. (1988). Journal of Neuroscience, 8(12), 4612-4620.  Stent, G.S. and Kristan, W.B. (1981). In K.Muller, J Nicholls, and G. Stent (eds),  Neurobiology of the Leech, 113-146. Cold Spring Harbor: Cold Spring Harbor  Laboratory.  Mead, C.A. (1989). Analog VLSI and Neural Systems, Reading, MA: Addison-Wesley.  Friesen, W.O. and Stent, G.S. (1977). Biological Cybernetics, 28, 27-40.  
Robust Reinforcement Learning in  Motion Planning  Satinder P. Singh*  Department of Brain and Cognitive Sciences  Massachusetts Institute of Technology  Cambridge, MA 02139  singh@psyche.mit.edu  Andrew G. Barto, Roderic Grupen, and Christopher Connolly  Department of Computer Science  University of Massachusetts  Amherst, MA 01003  Abstract  While exploring to find better solutions, an agent performing on-  line reinforcement learning (RL) can perform worse than is accept-  able. In some cases, exploration might have unsafe, or even catas-  trophic, results, often modeled in terms of reaching 'failure' states  of the agent's environment. This paper presents a method that uses  domain knowledge to reduce the number of failures during explo-  ration. This method formulates the set of actions from which the  RL agent composes a control policy to ensure that exploration is  conducted in a policy space that excludes most of the unacceptable  policies. The resulting action set has a more abstract relationship  to the task being solved than is common in many applications of  RL. Although the cost of this added safety is that learning may  result in a suboptimal solution, we argue that this is an appropri-  ate tradeoff in many problems. We illustrate this method in the  domain of motion planning.  *This work was done while the first author was finishing his Ph.D in computer science  at the University of Massachusetts, Amherst.  655  656 Singh, Barto, Grupen, and Connolly  An agent using reinforcement learning (Sutton et al., 1991; Barto et al., to appear)  (RL) to approximate solutions to optimal control problems has to search, or explore,  to improve its policy for selecting actions. Although exploration does not directly  affect performance (Moore & Atkeson, 1993) in off-line learning with a model of  the environment, exploration in on-line learning can lead the agent to perform  worse than is acceptable. In some cases, exploration might have unsafe, or even  catastrophic, results, often modeled in terms of reaching 'failure' states of the agent's  environment. To make on-line RL more practical, especially if it involves expensive  hardware, task-specific minimal levels of performance should be ensured during  learning, a topic not addressed by prior RL research.  Although the need for exploration cannot be entirely removed, domain knowledge  can sometimes be used to define the set of actions from which the RL agent composes  a control policy so that exploration is conducted in a space that excludes most of  the unacceptable policies. We illustrate this approach using a simulated dynamic  mobile robot in two different environments.  I Closed-loop policies as actions  RL agents search for optimal policies in a solution space determined in part by  the set of actions available to the agent. With a few exceptions (e.g., Mahadevan  & Connell, 1990; Singh, 1992), researchers have formulated RL tasks with actions  that are primitive in the sense that they are low-level, are available in very state,  are executed open-loop, and last a single time-step. We propose that this is an  arbitrary, and self-imposed, restriction, and that in general the set of actions can  have a much more abstract relationship to the problem being solved. Specifically,  what are considered 'actions' by the RL algorithm can themselves be closed-loop  control policies that meet important subgoals of the task being solved.  In this paper, the following general advantages afforded by using closed-loop policies  as actions are demonstrated in the domain of motion planning:  1. It is possible to design actions to meet certain hard constraints so that RL  maintains acceptable performance while simultaneously improving perfor-  mance over time.  2. It is possible to design actions so that the action space for the learning  problem has fewer dimensions than the actual dimension of the physical  action space.  The robustness and greatly accelerated learning resulting from the above factors  can more than offset the cost of designing the actions. However, care has to be  taken in defining the action space to ensure that the resulting policy space contains  at least one policy that is close to optimal.  2 RL with Dirichlet and Neumann control policies  The motion planning problem arises from the need to give an autonomous robot  the ability to plan its own motion, i.e., to decide what actions to execute in order  to achieve a task specified by initial and desired spatial arrangements of objects.  Robust Reinforcement Learning in Motion Planning 657  First consider geometric path planning, i.e., the problem of finding safe paths for a  robot with no dynamical constraints in an environment with stationary obstacles.  A safe path in our context is one that avoids all obstacles and terminates in a  desired configuration. Connolly (1992) has developed a method that generates safe  paths by solving Laplace's equation in configuration space with boundary conditions  determined by obstacle and goal configurations (also see, Connolly & Grupen, 1993).  Laplace's equation is the partial differential equation  i= Oxi - O, (1)  whose solution is a harmonic function, b, with no interior local minima. In practice,  a finite difference approximation to Equation I is solved numerically via Gauss Sidel  relation on a mesh over configuration space. Safe paths are generated by gradient  descent on the resulting approximate harmonic function. In the general motion  planning problem, we are interested in finding control policies that not only keep  the robot safe but also extremize some performance criterion, e.g., minimum time,  minimum jerk, etc.  Two types of boundary conditions, called Dirichlet and Neumann boundary condi-  tions, give rise to two different harmonic functions, D and N, that generate dif-  ferent types of safe paths. Robots following paths generated from D tend to be re-  pelled perpendicularly from obstacles. In controt, robots following paths generated  from N tend to skirt obstacles by moving parallel to their boundaries. Although  the state space in the motion planning problem for a dynamic robot in a planar  environment is {x, k, y, fi), harmonic functions are derived in two-dimensional posi-  tion space. These functions are inexpensive to compute (relative to the expense of  solving the optimal control problem) because they are independent of the robot dy-  namics and criterion of optimal control. The closed-loop control policies that follow  the gradient of the Dirichlet or Neumann solutions, respectively denoted D and  N, are defined approximately as follows: D(S) = VD(g), and N(S) =  where g is the projection of the state s onto position space.   Instead of formulating the motion planning problem  a RL task in which a control  policy maps states into primitive control actions, consider the formulation in which  a policy maps each state s to a mixing parameter k(s) so that the actual action is   [1 - k(s)]D(S) + [k(s)](s), where 0  k(s)  1. Figure lB shows the structure  of this kind of policy. In Appendix B, we present conditions guaranteeing that for  a robot with no dynamical constraints, this policy space contains only acceptable  policies, i.e., policies that cause the robot to reach the goal configuration without  contacting an obstacle. Although this guarantee does not strictly hold when the  robot h dynamical constraints, e.g., when there are bounds on acceleration, this  formulation still reduces the risk of unacceptable behavior.  3 Simulation Results  In this paper we present a brief summary of simulation results for the two envi-  ronments shown in Figures 2A and 3A. See Singh (1993) for details. The first  In practice, the gradients of the harmonic functions act as reference signMs to a PD-  controller.  658 Singh, Barto, Grupen, and Connolly  environment consists of two rooms connected by a corridor. The second environ-  ment is a horseshoe-shaped corridor. The mobile robot is simulated as a unit-mass  that can accelerate in any direction. The only dynamical constraint is a bound on  the maximum acceleration.  Q(stato, action)  A B (s)  k(s) :Low-Level Action  State Policy   (s) T ,ction  1 - k(s)-Low. Level Action   . I Policy2  X X Y Y k Neumann  state  (s) (s)  mixing  coefficient  Figure 1' Q-learning Network and Policy Structure. Panel A: 2-layer Connectionist  Network Used to Store Q-values. Network inversion was used to find the maximum  Q-value (Equation 2) at any state and the associated greedy action. The hidden  layer consists of radial-basis units. Panel B: Policy Structure. The agent has to learn  a mapping from state s to a mixing coefficient 0 _< k(s) _ I that determines the  proportion in which to mix the actions specifies by the pure Dirichlet and Neumann  policies.  The learning task is to approximate minimum time paths from every point inside  the environment to the goal region without contacting the boundary wall. A rein-  forcement learning algorithm called Q-learning (Watkins, 1989) (see Appendix A)  was used to learn the mixing function, k. Figure 1A shows the 2-layer neural net-  work architecture used to store the Q-values. The robot was trained in a series  of trials, each trial starting with the robot placed at a randomly chosen state and  ending when the robot enters the goal region. The points marked by stars in Fig-  ures 2A and 3A were the starting locations for which statistics were collected to  produce learning curves.  Figures 2B, 2C, 3A and 3B show three robot trajectories from two randomly chosen  start states; the black-filled circles mark the Dirichlet trajectory (labeled D), the  white-filled circles mark the Neumann trajectory (labeled N), and the grey-filled  circles mark the trajectory after learning (labeled Q). Trajectories are shown by  taking snapshots of the robot at every time step; the velocity of the robot can  be judged by the spacing between successive circles on the trajectory. Figure 2D  shows the mixing function for zero-velocity states in the two-room environment,  while Figure 3C shows the mixing function for zero velocity states in the horseshoe  environment. The darker the region, the higher the proportion of the Neumann  Robust Reinforcement Learning in Motion Planning 659  policy in the mixture. In the two-room environment, the agent learns to follow  the Neumann policy in the left-hand room and to follow the Dirichlet policy in the  right-hand room.  Figure 2E shows the average time to reach the goal region as a function of the  number of trials in the two-room environment. The solid-line curve shows the  performance of the Q-learning algorithm. The horizontal lines show the average  time to reach the goal region for the designated pure policies. Figure 3D similarly  presents the results for the horseshoe environment. Note that in both cases the  RL agent learns a policy that is better than either the pure Dirichlet or the pure  Neumann policies. The relative advantage of the learned policy is greater in the  two-room environment than in the horseshoe environment.  On the two-room environment we also compared Q-learning using harmonic func-  tions, as described above, with Q-learning using primitive accelerations of the mobile  robot as actions. The results are summarized along three dimensions: a) speed of  learning: the latter system took more than 20,000 trials to achieve the same level  of performance achieved by the former in 100 trials, b) safety: the simulated robot  using the latter system crashed into the walls more than 200 times, and c) asymp-  totic performance: the final solution found by the latter system was 6% better than  the one found by the former.  4 Conclusion  Our simulations show how an RL system is capable of maintaining acceptable per-  formance while simultaneously improving performance over time. A secondary mo-  tivation for this work was to correct the erroneous impression that the proper, if  not the only, way to formulate RL problems is with low-level actions. Experience  on large problems formulated in this fashion has contributed to the perception that  RL algorithms are hopelessly slow for real-world applications. We suggest that a  more appropriate way to apply RL is as a "component technology" that uses expe-  rience to improve on partial solutions that have already been found through either  analytical techniques or the cumulative experience and intuitions of the researchers  themselves. The RL framework is more abstract, and hence more flexible, than  most current applications of RL would lead one to believe. Future applications of  RL should more fully exploit the flexibility of the RL framework.  A Q-learning  On executing action a in state st at time t, the following update on the Q-value  function is performed:  Qt+l(st,a)-Qt(st,a)-bot(st,a)[R(st,a)-b'/(am, 2Qt(st+l,a))-Qt(st,a)] (2)  where R(st, a) is the payoff, 0 _ '/ _ I is the discount factor, and c is a learning  rate parameter. See Watkins (1989) for further details.  660 Singh, Barto, Ginpen, and Connolly  A   *[  GOAL  B  c  7000  8000  2o N  GOAL i  O  i. o  D  E  Q-learning  --- Neumann  ......... Dirichlet  oo I I I I I  0 9000 18000 27000 36000 45000  Number of Trials  Figure 2: Results for the Two-Room Environment. Panel A: Two-Room Environ-  ment. The stars mark the starting locations for which statistics were computed.  Panel B: Sample Trajectories from one Starting Location. The black-filled circles  labeled D show a pure Dirichlet trajectory, the white-filled circles labeled N show a  pure Neumann trajectory, and the grey-filled circles labeled Q show the trajectory  after learning. The trajectories are shown by taking snapshots at every time step;  the velocity of the robot can be judged by the distance between successive points  on the trajectory. Panel C: Three Sample Trajectories from a Different Starting  Location. Panel D: Mixing Function Learned by the Q-learning Network for Zero  Velocity States. The darker the region the higher the proportion of the Neumann  policy in the resulting mixture. Panel E: Learning Curve. The curve plots the time  taken by the robot to reach the goal region, averaged over the locations marked  with stars in Panel A, as a function of the number of Q-learning trials. The dashed  line shows the average time using the pure Neumann policy; the dotted line for the  pure Dirichlet policy; and the solid line for Q-learning. The mixed policy formed  by Q-learning rapidly outperforms both pure harmonic function policies.  Robust Reinforcement Learning in Motion Planning 661  A  B  GOAL  GOAL  c  15OOO  10O0O  00  D  Q-learning  --- Neumann  ......... Dirichlet  ------ -.--._- ......................  I I I I  Number of,,.Trials  Figure 3: Results for the Horseshoe Environment. Panel A: Horseshoe-Shaped  Environment. Locations marked with stars are the starting locations for which  statistics were computed. It also shows sample trajectories from one starting loca-  tion; the black-filled circles marked D show a Dirichlet trajectory, the white-filled  circles marked N show a Neumann trajectory, and the grey-filled circles marked Q  show the trajectory after learning. The trajectories are shown by taking snapshots  at every time step; the velocity of the robot can be judged by the distance between  successive points on the trajectory. Panel B: Three Sample Trajectories from a  Different Starting Location. Panel C: Mixing Function Learned by the Q-learning  Network for Zero Velocity States. The darker the region the higher the proportion  of the Neumann policy in the resulting mixture. Panel D: Learning Curve. The  curve plots the time taken by the robot to reach the goal region, averaged over the  locations marked with stars in Panel A, as a function of the number of Q-learning  trials. The dashed line shows the average time for the pure Neumann policy; the  dotted line for the pure Dirichlet policy; and the solid line for Q-learning. Q-learning  rapidly outperforms both pure harmonic function policies.  662 Singh, Barto, Grupen, and Connolly  B Safety  Let L denote the surface whose gradients at any point are given by the closed-loop  policy under consideration. Then for there to be no minima in L, the gradient of L  should not vanish in the workspace, i.e., (1- k(s))VrI)D ()+ k(s)VrlMr()  0. The  only way it can vanish is if Vi  1-k(s) - [ i'  where [.]i is the ith component of vector [.]. The algorithm can explicitly check for  that possibility and prevent it. Alternatively, note that due to the finite precision  in any practical implementation, it is extremely unlikely that Equation 3 will hold  in any state. Also note that ,r(s) for any point s on the boundary will always point  away from the boundary because it is the convex sum of two vectors, one of which  is normal to the boundary, and the other of which is parallel to the boundary.  Acknowledgement s  This work was supported by a grant ECS-9214866 to Prof. A. G. Barto from the  National Science Foundation, and by grants IRI-9116297, IRI-9208920, and CDA-  8922572 to Prof. R. Grupen from the National Science Foundation.  References  Barto, A.G., Bradtke, S.J., & Singh, S.P. (to appear). Learning to act using real-  time dynamic programming. Artificial Intelligence.  Cormoily, C. (1992). Applications of harmonic functions to robotics. In The 199  International Symposium on Intelligent Control. IEEE.  Cormoily, C. & Grupen, R. (1993). On the applications of harmonic functions to  robotics. Journal of Robotic Systems, 10(7), 931-946.  Mahadevan, S. & Cormell, J. (1990). Automatic programming of behavior-based  robots using reinforcement learning. Technical report, IBM Research Division,  T.J.Watson Research Center, Yorktown Heights, NY.  Moore, A.W. & Atkeson, C.G. (1993). Prioritized sweeping: Reinforcement learning  with less data and less real time. Machine Learning, 13(1).  Singh, S.P. (1992). Transfer of learning by composing solutions for elemental se-  quential tasks. Machine Learning, 8(3/4), 323-339.  Singh, S.P. (1993). Learning to Solve MarkovJan Decision Processes. PhD thesis,  Department of Computer Science, University of Massachusetts. also, CMPSCI  Technical Report 93-77.  Sutton, R.S., Barto, A.G., & Williams, R.J. (1991). Reinforcement learning is direct  adaptive optimal control. In Proceedings of the American Control Conference,  pages 2143-2146, Boston, MA.  Watkins, C.J.C.H. (1989). Learning from Delayed Rewards. PhD thesis, Cambridge  Univ., Cambridge, England.  
Clustering with a Domain-Specific  Distance Measure  Steven Gold, Eric Mjolsness and Anand Rangarajan  Department of Computer Science  Yale University  New Haven, CT 06520-8285  Abstract  With a point matching distance measure which is invariant under  translation, rotation and permutation, we learn 2-D point-set ob-  jects, by clustering noisy point-set images. Unlike traditional clus-  tering methods which use distance measures that operate on feature  vectors - a representation common to most problem domains - this  object-based clustering technique employs a distance measure spe-  cific to a type of object within a problem domain. Formulating  the clustering problem as two nested objective functions, we derive  optimization dynamics similar to the Expectation-Maximization  algorithm used in mixture models.  I Introduction  Clustering and related unsupervised learning techniques such as competitive learn-  ing and self-organizing maps have traditionally relied on measures of distance, like  Euclidean or Mahalanobis distance, which are generic across most problem domains.  Consequently, when working in complex domains like vision, extensive preprocess-  ing is required to produce feature sets which reflect properties critical to the domain,  such as invariance to translation and rotation. Not only does such preprocessing  increase the architectural complexity of these systems but it may fail to preserve  some properties inherent in the domain. For example in vision, while Fourier de-  composition may be adequate to handle reconstructions invariant under translation  and rotation, it is unlikely that distortion invariance will be as amenable to this  technique (von der Malsburg, 1988).  Clustering with a Domain-Specific Distance Measure 97  These problems may be avoided with the help of more powerful, domain-specific  distance measures, including some which have been applied successfully to visual  recognition tasks (Simard, Le Cun, and Denker, 1993; Huttenlocher el al., 1993).  Such measures can contain domain critical properties; for example, the distance  measure used here to cluster 2-D point images is invariant under translation, rota-  tion and labeling permutation. Moreover, new distance measures may constructed,  as this was, using Bayesian inference on a model of the visual domain given by a  probabilistic grammar (Mjolsness, 1992). Distortion invariant or graph matching  measures, so formulated, can then be applied to other domains which may not be  amenable to description in terms of features.  Objective functions can describe the distance measures constructed from a proba-  bilistic grammar, as well as learning problems that use them. The clustering prob-  lem in the present paper is formulated as two nested objective functions: the inner  objective computes the distance measures and the outer objective computes the  cluster centers and cluster memberships. A clocked objective function is used, with  separate optimizations occurring in distinct clock phases (Mjolsness and Miranker,  1993). The optimization is carried out with coordinate ascent/descent and deter-  ministic annealing and the resulting dynamics is a generalization of the Expectation-  Maximization (EM) algorithm commonly used in mixture models.  2 Theory  2.1 The Distance Measure  Our distance measure quantifies the degree of similarity between two unlabeled  2-D point images, irrespective of their position and orientation. It is calculated  with an objective that can be used in an image registration problem. Given two  sets of points {Xj } and {Yk }, one can minimize the following objective to find the  translation, rotation and permutation which best maps Y onto X   E,a(m, t, O) - mjkllX j - t - R(O).  with constraints: j : rnj = 1 , k j rnjk = 1.  Such a registration permits the matching of two sparse feature images in the presence  of noise (Lu and Mjolsness, 1994). In the above objective, m is a permutation matrix  which matches one point in one image with a corresponding point in the other image.  The constraints on m ensure that each point in each image corresponds to one and  only one point in the other image (though note later remarks regarding fuzziness).  Then given two sets of points {Xj} and {Y} the distance between them is defined  as:  D({Xj}, {Y})= min(E,.,a(m,t,O) l constraints on m) (1)  m,t,O  This measure is an example of a more general image distance measure derived in  (Mjolsness, 1992):  d(x,y) = mind(x,T(y)) e [0,  T  where T is a set of transformation parameters introduced by a visual grammar. In  (1) translation, rotation and permutation are the transformations, however scaling  98 Gold, Mjolsness, and Rangarajan  or distortion could also have been included, with consequent changes in the objective  function.  The constraints are enforced by applying the Potts glass mean field theory ap-  proximations (Peterson and Soderberg,1989) and then using an equivalent form of  the resulting objective, which employs Lagrange multipliers and an x log x barrier  function (as in Yuille and Kosowsky, 1991):  1 1)  Ereg(m, t, O) = E mJk[[XJ -- t-- R(O) o yk[}2 +  E mjk(log mjk --  jk jk  j k k j  In this objective we are looking for a saddle point. (2) is minimized with respect to  m, t, and , which are the correspondence matrix, translation,and rotation, and is  maximized with respect to  and , the Lagrange multipliers tat enforce te row  and column constrMnts for m.  2.2 The Clustering Objective  The learning problem is formulated as follows: Given a set of I images, {Xi), with  each image consisting of J points, find a set of A cluster centers {Ya) and match  variables {Mia) defined as  1 ifXi is in Y.'s cluster  Mia = 0 otherwise,  such that each image is in only one cluster, and the total distance of all the images  from their respective cluster centers is minimized. To find {Ya) and {Mia) minimize  the cost function,  with the constraint that Vi . Mi = 1. D(Xi,Y), the distance function, is  defined by (1).  The constraints on M are enforced in a manner similar to that described for the  distance measure, except that now only the rows of the matrix M need to add to  one, instead of both the rows and the columns The Potts glass mean field theory  method is applied and an equivalent form of the resulting objective is used:  1  Eca,.t,.(Y,M) = Z Mi.D(Xi,Y.) +  Z Mi.(logMi. - 1)+ Z Ai(Z Mi. - 1)  ia ia i a  (3)  Replacing the distance measure by (2), we derive:  Edu.,er(Y, M, t, O,m) - Z Mi. E m'"llXq - t,. - R(O,.) .  ia jk  jk j k  . 1  ) ,Viak(E miajk -- 1)] +   Mi.(logMi. - 1)+ ' Ai(y. Mi. - 1)  k j za i a  Clustering with a Domain-Specific Distance Measure 99  A saddle point is required. The objective is minimized with respect to Y, M, m,  t, O, which are respectively the cluster centers, the cluster membership matrix, the  correspondence matrices, the rotations, and the translations. It is maximized with  respect to A, which enforces the row constraint for M, and tt and v which enforce  the column and row constraints for m. M is a cluster membership matrix indicating  for each image i, which cluster a it falls within, and mia is a permutation matrix  which assigns to each point in cluster center Ya a corresponding point in image Xi.  Oia gives the rotation between image i and cluster center a. Both M and m are  fuzzy, so a given image may partially fall within several clusters, with the degree of  fuzziness depending upon/, and/M.  Therefore, given a set of images, X, we construct Eust,. and upon finding the  appropriate saddle point of that objective, we will have Y, their cluster centers,  and M, their cluster memberships.  3 The Algorithm  3.1 Overview- A Clocked Objective Function  The algorithm to minimize the above objective consists of two loops - an inner  loop to minimize the distance measure objective (2) and an outer loop to minimize  the clustering objective (3). Using coordinate descent in the outer loop results  in dynamics similar to the EM algorithm for clustering (Hathaway, 1986). (The  EM algorithm has been similarly used in supervised learning [Jordan and Jacobs,  1993].) All variables occurring in the distance measure objective are held fixed  during this phase. The inner loop uses coordinate ascent/descent which results in  repeated row and column projections for m. The minimization of m, t and O occurs  in an incremental fashion, that is their values are saved after each inner loop call  from within the outer loop and are then used as initial values for the next call to  the inner loop. This tracking of the values of m, t, and O in the inner loop is  essential to the efficiency of the algorithm since it greatly speeds up each inner loop  optimization. Each coordinate ascent/descent phase can be computed analytically,  further speeding up the algorithm. Local minima are avoided, by deterministic  annealing in both the outer and inner loops.  The resulting dynamics can be concisely expressed by formulating the objective as  a clocked objective function, which is optimized over distinct sets of variables in  phases,  Eclockecl -- Ectuser( ( ((la, m)A, (v, m)A), oA, tA)b, (.,, M)A, yA) b  with this special notation employed recursively:  E(x, y),  coordinate descent on x, then y, iterated (if necessary)  x A  use analytic solution for x phase  The algorithm can be expressed less concisely in English, as follows:  Initialize t, 13 to zero, Y to random values  Begin Outer Loop  Begin Inner Loop  Initialize t, 13 with previous values  100 Gold, Mjolsness, and Rangarajan  Find m, t, 13 for each ia pair:  Find m by softmax, projecting across j, then k, iteratively  Find 13 by coordinate descent  Find t by coordinate descent  End Inner Loop  If first time through outer loop T/- and repeat inner loop  Find M,Y using fixed values of m, t, 13 determined in inner loop:  Find M by softmax, across i  Find Y by coordinate descent  End Outer Loop  When the distances are calculated for all the X - Y pairs the first time time through  the outer loop, annealing is needed to minimize the objectives accurately. However  on each succeeding iteration, since good initial estimates are available for t and 13  (the values from the previous iteration of the outer loop) annealing is unnecessary  and the minimization is much faster.  The speed of the above algorithm is increased by not recalculating the X - Y distance  for a given ia pair when its Mia membership variable drops below a threshold.  3.2 Inner Loop  The inner loop proceeds in three phases. In phase one, while t and 13 are held fixed,  m is initialized with the softmax function and then iteratively projected across its  rows and columns until the procedure converges. In phases two and three, t and 13  are updated using coordinate descent. Then/, is increased and the loop repeats.  In phase one m is updated with softmax:  exp(-/m[[Xij - tia -- R(13ia) ' Yall =)  rniaj: : 5-' exp(-.llXij - - R(13ia) . I[ 2)  Then m is iteratively normalized across j and k until yj Amiajk < e '  mi aj k  miajk -- Ej t miaj k  miaj k  ; miajk -- Ek t miaj k'  Using coordinate descent 13 is calculated in phase two:  Y/, ,,s, ( ( x,s-t, ),-( x,s -t ), )  13ia = tan - Ys'ms'((xs-t')'-(x's-t))  And t in phase three:  tial -- Ejk rn*Jk(X*5-t'-Y& cos,+Y,2 sin ,)  tia2 ----  my&(XiJ2-t*2-Yn sin -Y cosi)  Ejk mtajk  Finally 3, is increased and the loop repeats.  Clustering with a Domain-Specific Distance Measure 101  0 to zero,  By setting the partial derivatives of (2) to zero and initializing p? and k  the algorithm for phase one may be derived. Phases two and three may be derived  by taking the partial derivative of (2) with respect to O, setting it to zero, solving  for O, and then solving for the fixed point of the vector (tl, t2).  Beginning with a small m allows minimization over a fuzzy correspondence matrix  m, for which a global minimum is easier to find. Raising m drives the m's closer  to 0 or 1, as the algorithm approaches a saddle point.  3.3 Outer Loop  The outer loop also proceeds in three phases: (1) distances are calculated by calling  the inner loop, (2) M is projected across a using the softmaxfunction, (3) coordinate  descent is used to update Y o  Therefore, using softmax M is updated in phase two:  exp(--M jk miajlllXij - tia - R(Oi,) ' kll 2)  Mia = Ea' exp(--m Ejk mi='jllxo - ti=' - R(Oga') ' ra'11 =)  Y, in phase three is calculated using coordinate descent:  Ei Mia Yj mij(cosOi(Xij - ti) + sin Oi(Xij2 -tia2))  Yak l  Ei Mi Ej miaj(-sin ei(Xij - ti) q- cos ei(Xij -  Yak2 --  Then M is increased and the loop repeats.  4 Methods and Experimental Results  In two experiments (Figures la and lb) 16 and 100 randomly generated images of  15 and 20 points each are clustered into 4 and 10 clusters, respectively.  A stochastic model, formulated with essentially the same visual grammar used to  derive the clustering algorithm (Mjolsness, 1992), generated the experimental data.  That model begins with the cluster centers and then applies probabilistic trans-  formations according to the rules laid out in the grammar to produce the images.  These transformations are then inverted to recover cluster centers from a starting  set of images. Therefore, to test the algorithm, the same transformations are ap-  plied to produce a set of images, and then the algorithm is run in order to see if it  can recover the set of cluster centers, from which the images were produced.  First, n - 10 points are selected using a uniform distribution across a normalized  square. For each of the n: 10 points a model prototype (cluster center) is created  by generating a set of k - 20 points uniformly distributed across a normalized  square centered at each orginal point. Then, m - l0 new images consisting of  k = 20 points each are generated from each model prototype by displacing all k  model points by a random global translation, rotating all k points by a random  global rotation within a 54  arc, and then adding independent noise to each of the  translated and rotated points with a Gaussian distribution of variance er 2.  102 Gold, Mjolsness, and Rangarajan  I 0.2 O.i 0.6 0.8 I 1.2 1.  0.5 0.75 1 1.25 1.  Figure 1: (a): 16 images, 15 points each (b)'100 images, 20 points each  The p = n x m = 100 images so generated is the input to the algorithm. The  algorithm, which is initially ignorant of cluster membership information, computes  n = 10 cluster centers as well as n x p = 1000 match variables determining the  cluster membership of each point image. cr is varied and for each cr the average  distance of the computed cluster centers to the theoretical cluster centers (i.e. the  original n = 10 model prototypes) is plotted.  Data (Figure la) is generated with 20 random seeds with constants of n = 4, k =  15, m = 4, p = 16, varying cr from .02 to .14 by increments of .02 for each seed.  This produces 80 model prototype-computed cluster center distances for each value  of cr which are then averaged and plotted, along with an error bar representing  the standard deviation of each set. 15 random seeds (Figure lb) with constants  of n = 10, k = 20, m = 10,p = 100, cr varied from 02 to .16 by increments of  .02 for each seed, produce 150 model prototype-computed cluster center distances  for each value of or. The straight line plotted on each graph shows the expected  model prototype-cluster center distances, /5 = ko'/x/ , which would be obtained if  there were no translation or rotation for each generated image, and if the cluster  memberships were known. It can be considered a lower bound for the reconstruction  performance of our algorithm. Figures la and lb together summarize the results of  280 separate clustering experiments.  For each set of images the algorithm was run four times, varying the initial randomly  selected starting cluster centers each time and then selecting the run with the lowest  energy for the results. The annealing rate for/m and /, was a constant factor of  1.031. Each run of the algorithm averaged ten minutes on an Indigo SGI workstation  for the 16 image test, and four hours for the 100 image test. The running time of  the algorithm is O(pnk2). Parallelization, as well as hierarchical and attentional  mechanisms, all currently under investigation, can reduce these times.  5 Summary  By incorporating a domain-specific distance measure instead of the typical generic  distance measures, the new method of unsupervised learning substantially reduces  the amount of ad-hoc pre-processing required in conventional techniques. Critical  features of a domain (such as invariance under translation, rotation, and permu-  Clustering with a Domain-Specific Distance Measure 103  tation) are captured within the clustering procedure, rather than reflected in the  properties of feature sets created prior to clustering. The distance measure and  learning problem are formally described as nested objective functions. We derive  an efficient algorithm by using optimization techniques that allow us to divide up  the objective function into parts which may be minimized in distinct phases. The  algorithm has accurately recreated 10 prototypes from a randomly generated sample  database of 100 images consisting of 20 points each in 120 experiments. Finally, by  incorporating permutation invariance in our distance measure, we have a technique  that we may be able to apply to the clustering of graphs. Our goal is to develop  measures which will enable the learning of objects with shape or structure.  Acknowledgement s  This work has been supported by  ONR/DARPA grant N00014-92-J-4048.  AFOSR grant F49620-92-J-0465 and  References  R. Hathaway. (1986) Another interpretation of the EM algorithm for mixture  distributions. Statistics and Probability Letters 4:53:56.  D. Huttenlocher, G. Klanderman and W. Rucklidge. (1993) Comparing im-  ages using the Hausdorff Distance. Pattern Analysis and Machine Intelligence  15(9):850:863.  A. L. Yuille and J.J. Kosowsky. (1992). Statistical physics algorithms that converge.  Technical Report 92-7, Harvard Robotics Laboratory.  M.I. Jordan and R.A. Jacobs. (1993). Hierarchical mixtures of experts and the  EM algorithm. Technical Report 9301, MIT Computational Cognitive Science.  C. P. Lu and E. Mjolsness. (1994). Two-dimensional object localization by coarse-  to-fine correlation matching. In this volume, NIPS 6.  C. yon der Malsburg. (1988). Pattern recognition by labeled graph matching.  Neural Networks,l:141:148.  E. Mjolsness and W. Miranker. (1993). Greedy Lagrangians for neural networks:  three levels of optimization in relaxation dynamics. Technical Report 945, Yale  University, Department of Computer Science.  E. Mjolsness. Visual grammars and their neural networks. (1992) SPIE Conference  on the Science of Artificial Neural Networks, 1710:63:85.  C. Peterson and B. SSderberg. A new method for mapping optimization problems  onto neural networks. (1989) International Journal of Neural Systems,l(1):3:22.  P. Simard, Y. Le Cun, and J. Denker. Efficient pattern recognition using a new  transformation distance. (1993). In S. Hanson, J. Cowan, and C. Giles, (eds.),  NIPS 5. Morgan Kaufmann, San Mateo CA.  
Statistics of Natural Images:  Scaling in the Woods  Daniel L. Ruderman* and William Bialek  NEC Research Institute  4 Independence Way  Princeton, N.J. 08540  Abstract  In order to best understand a visual system one should attempt  to characterize the natural images it processes. We gather images  from the woods and find that these scenes possess an ensemble scale  invariance. Further, they are highly non-Gaussian, and this non-  Gaussian character cannot be removed through local linear filter-  ing. We find that including a simple "gain control" nonlinearity in  the filtering process makes the filter output quite Gaussian, mean-  ing information is maximized at fixed channel variance. Finally, we  use the measured power spectrum to place an upper bound on the  information conveyed about natural scenes by an array of receptors.  I Introduction  Natural stimuli are playing an increasingly important role in our understanding of  sensory processing. This is because a sensory system's ability to perform a task is a  statistical quantity which depends on the signal and noise characteristics. Recently  several approaches have explored visual processing as it relates to natural images  (Atick & Redlich '90, Bialek et al '91, van Hateten '92, Laughlin '81, Srinivasan  et al '82). However, a good characterization of natural scenes is sorely lacking. In  this paper we analyze images from the woods in an effort to close this gap. We  *Current address:  CB2 3EG, England.  The Physiological Laboratory, Downing Street, Cambridge  551  552 Ruderman and Bialek  further attempt to understand how a biological visual system should best encode  these images.  2 The Images  Our images consist of 256 x 256 pixels I(x) which are calibrated against luminance  (see Appendix). We define the image contrast logarithmically as  b(x) = ln(I(x)/Io),  where I0 is a reference intensity defined for each image. We choose this constant  such that x b(x) = 0; that is, the average contrast for each image is zero. Our  analysis is of the contrast data b(x).  3 Scaling  Recent measurements (Field '87, Burton & Moorhead '87) suggest that ensembles  of natural scenes are scale-invariant. This means that and any quantity defined on  a given scale has statistics which are invariant to any change in that scale. This  seems sensible in light of the fact that the images are composed of objects at all  distances, and so no particular angular scale should stand out. (Note that this does  not imply that any particular image is fractal! Rather, the ensemble of scenes has  statistics which are invariant to scale.)  3.1 Distribution of Contrasts  We can test this scaling hypothesis directly by seeing how the statistics of various  quantities change with scale. We define the contrast averaged over a box of size  N x N (pixels) to be  N  1  j).  i,j=l  We now ask: "How does the probability P(b:v) change with N?"  In the left graph offigure 1 we plot log(P(v/qSrMS)) for N = 1, 2, 4, 8, 16, 32 along  with the parabola corresponding to a Gaussian of the same variance. By dividing  out the RMS value we simply plot all the graphs on the same contrast scale. The  graphs all lie atop one another, which means the contrast scales--the distribution 's  shape is invariant to a change in angular scale. Note that the probability is far from  Gaussian, as the graphs have linear, and not parabolic, tails. Even after averaging  nearly 1000 pixels (in the case of 32 x 32), it remains non-Gaussian. This breakdown  of the central limit theorem implies that the pixels are correlated over very long  distances. This is analogous to the physics of a thermodynamic system at a critical  point.  3.2 Distribution of Gradients  As another example of scaling, we consider the probability distribution of image  gradients. We define the magnitude of the gradient by a discrete approximation  Statistics of Natural Images: Scaling in the Woods 553  o  .35  1 2 3 4 5  Figure 1: Left' Semi-log plot of P(c)N/c) Ms) for N = 1,2,4,8,16,32 with a  Gaussian of the same variance for comparison (solid line). Right: Semi-log plot of  P(GN/ON) for same set of N's with a Rayleigh distribution for comparison (solid  line).  such that  G(x) -IG(x)l  We examine this quantity over different scales by first rescaling the images as above  and then evaluating the gradient at the new scale. We plot log(P(GN/N)) for N =  1, 2, 4, 8, 16, 32 in the right graph of figure 1, along with the Rayleigh distribution,  P m G exp(-cG2). If the images had Gaussian statistics, local gradients would be  Rayleigh distributed. Note once again scaling of the distribution.  3.3 Power Spectrum  Scaling can also be demonstrated at the level of the power spectrum. If the ensemble  is scale-invariant, then the spectrum should be of the form  A  where k is measured in cycles/degree, and S is the power spectrum averaged over  orientations.  The spectrum is shown in figure 2 on log-log axes. It displays overlapping data from  the two focal lengths, and shows that the spectrum scales over about 2.5 decades in  spatial frequency. We determine the parameters as A = (6.47+0.13) x 10-3deg. ('t9)  and r/= 0.19 q-0.01. The integrated power spectrum up to 60 cycles/degree (the  human resolution limit) gives an RMS contrast of about 30%.  4 Local Filtering  The early stages of vision consist of neurons which respond to local patches of  images. What do the statistics of these local processing units look like? We convolve  images with the filter shown in the left of figure 3, and plot the histogram of its  output on a semi-log scale on the right of the figure.  554 Ruderman and Bialek  -1  -2  -3  -4  -5  -6  -1.5 -1  -0.5 o 0.5 1 1.5  Log10 [Spatial Frequency (cycles/degree) ]  Figure 2: Power spectrum of the contrast of natural scenes (log-log plot).  The distribution is quite exponential over nearly 4 decades in probability. In fact,  almost any local linear filter which passes no DC has this property, including center-  surround receptive fields. Information theory tells us that it is best to send signals  with Gaussian statistics down channels which have power constraints. It is of in-  terest, then, to find some type of filtering which transforms the exponential distri-  butions we find into Gaussian quantities.  Music, as it turns out, has some similar properties. An amplitude histogram from 5  minutes of "The Blue Danube" is shown on the left of figure 4. It is almost precisely  exponential over 4 decades in probability. We can guess what causes the excesses  over a Gaussian distribution at the peak and the tails; it's the dynamics. When  a quiet passage is played the amplitudes lie only near zero, and create the excess  in the peak. When the music is loud the fluctuations are large, thus creating the  Figure 3: Left: 2 x 2 local filter.  when filtering natural scenes.  o  -o5  -25  -35  .at -2 o 2 4  Right: Semi-log plot of histogram of its output  Statistics of Natural Images: Scaling in the Woods 555  o  Figure 4' Left: Semi-log histogram of "The Blue Danube" with a Gaussian for  comparison (dashed). Right: 5 x 5 center-surround filter region.  tails. Most importantly, these quiet and loud passages extend coherently in time;  so to remove the peak and tails, we can simply slowly adjust a "volume knob" to  normalize the fluctuations. The images are made of objects which have coherent  structure over space, and a similar localized dynamic occurs. To remove it, we need  some sort of gain control.  To do this, we pass the images through a local filter and then normalize by the local  standard deviation of the image (analogous to the volume of a sound passage):  (x) =  Here (3(x) is the mean image contrast in the N x N region surrounding x, and c(x)  is the standard deviation within the same region (see the right of figure 4).  -o 5  Figure 5: Left: Semi-log plot of histogram of b, with Gaussian for comparison  (dashed). Right' Semi-log plot of histogram of gradients of b, with Rayleigh dis-  tribution shown for comparison (dashed).  We find that for a value N = 5 (ratio of the negative surround to the positive  center), the histograms of b are the closest to Gaussian (see the left of figure 5).  Further, the histogram of gradients of b is very nearly Rayleigh (see the right of  556 Ruderman and Bialek  figure 5). These are both signatures of a Gaussian distribution. Functionally, this  "variance normalization" procedure is similar to contrast gain control found in the  retina and LGN (Benardete et al, '92). Could its role be in "Gaussianizing" the  image statistics?  5 Information in the Retina  From the measured statistics we can place an upper bound on the amount of in-  formation an array of photoreceptors conveys about natural images. We make the  following assumptions:  Images are Gaussian with the measured power spectrum. This places an  upper bound on the entropy of natural scenes, and thus an upper bound  on the information represented.  The receptors sample images in a hexagonal array with diffraction-limited  optics. There is no aliasing.  Noise is additive, Gaussian, white, and independent of the image.  The output of the r/th receptor is thus given by  Yn = / d x b(x) M(x- Xn) + r/n,  where x,, is the location of the receptor, M(x) is the point-spread function of the  optics, and r/,, is the noise. For diffraction-limited optics,  M(k) m 1- Ikl/kc,  where kc is the cutoff frequency of 60 cycles/degree.  In the limit of an infinite lattice, Fourier components are independent, and the total  information is the sum of the information in each component:  = ... dk klog 1+ Aa2 IM(k)12(k) .  4  Here  is the information per receptor, A is the area of the unit cell in he lattice,  and a2 is he variance of the noise.  We take S(k) = A/k 2-v, with A and  taking their measured values, and express  the noise level in terms of the signal-to-noise ratio in the receptor. In figure 6 we  plot the information per receptor as a function of SNR along wi[h the information  capacity (per receptor) of the photoreceptor lattice at that SNR, which is  1  c = og[1 +  The information conveyed is less than 2 bits per receptor per image, even at SNR -  1000. The redundancy of this representation is quite high, as seen by the gap  between the curves; at least as much of the information capacity is being wasted as  is being used.  Statistics of Natural Images: Scaling in the Woods 557  (bits)  5  4  3  2  1  "  Log10 [SNR]  Figure 6: Information per receptor per image (in bits) as a function of log(SNR)  (lower line). Information capacity per receptor (upper line).  6 Conclusions  We have shown that images from the forest have scale-invariant, highly non-  Gaussian statistics. This is evidenced by the scaling of the non-Gaussian histograms  and the power-law form of the power spectrum. Local linear filtering produces val-  ues with quite exponential probability distributions. In order to "Gaussianize," we  must use a nonlinear filter which acts as a gain control. This is analogous to contrast  gain control, which is seen in the mammalian retina. Finally, an array of receptors  which encodes these natural images only conveys at most a few bits per receptor  per image of information, even at high SNR. At an image rate of 50 per second,  this places an information requirement of less than about 100 bits per second on a  foveal ganglion cell.  Appendix  Snapshots were gathered using a Sony Mavica MVC-5500 still video camera  equipped with a 9.5-123.5mm zoom lens. The red, green, and blue signals were  combined according to the standard CIE formula Y = 0.59 G + 0.30 R + 0.11 B  to produce a grayscale value at each pixel. The quantity Y was calibrated against  incident luminance to produce the image intensity I(x). The images were cropped  to the central 256 x 256 region.  The dataset consists of 45 images taken at a 15ram focal length (images subtend  150 of visual angle) and 25 images at an 80mm focal length (30 of visual angle). All  images were of distant objects to avoid problems of focus. Images were chosen by  placing the camera at a random point along a path and rotating the field of view  until no nearby objects appeared in the frame. The camera was tilted by less than  100 up or down in an effort to avoid sky and ground. The forested environment  (woods in New Jersey in springtime) consisted mainly of trees, rocks, hillside, and  a stream.  558 Ruderman and Bialek  Acknowledgements  We thank H. B. Barlow, B. Gianulis, A. J. Libchaber, M. Potters, R. R. de Ruyter  van Stevenink, and A. Schweitzer. Work was supported in part by a fellowship from  the Fannie and John Hertz Foundation (to D.L.R.).  References  J.J. Atick and N. Redlich. Towards a theory of early visual processing Neural  Computation, 2:308, 1990.  E. A. Benardete, E. Kaplan, and B. W. Knight. Contrast gain control in the primate  retina: P cells are not X-like, some M-cells are. Vis. Neuosci., 8:483-486, 1992.  W. Bialek, D. L. Ruderman, and A. Zee. The optimal sampling of natural im-  ages: a design principle for the visual system?, in Advances in Neural Information  Processing systems, 3, R. P. Lippman, J. E. Moody and D. S. Touretzky, eds., 1991.  G. J. Burton and I. R. Moorhead. Color and spatial structure in natural scenes.  Applied Optics, 26:157-170, 1987.  D. J. Field. Relations between the statistics of natural images and the response  properties of cortical cells. J. Opt. $oc. Am. A, 4:2379, 1987.  J. H. van Hateren. Theoretical predictions of spatiotemporal receptive fields of fly  LMCs, and experimental validation. J. Comp. Physiol. A, 171:157-170, 1992.  S. B. Laughlin. A simple coding procedure enhances a neuron's information capac-  ity. Z. Naturforsh., 36c:910-912, 1981.  M. V. Srinivasan, S. B. Laughlin, and A. Dubs. Predictive coding: a fresh view of  inhibition in the retina. Proc. R. $oc. Lond. B, 216:427-459, 1982.  
Classification of Electroencephalogram using  Artificial Neural Networks  A C Tsoi*, D S C So*, A Sergejew**  *Department of Electrical Engineering  **Department of Psychiatry  University of Queensland  St Lucia, Queensland 4072  Australia  Abstract  In this paper, we will consider the problem of classifying electroencephalo-  gram (EEG) signals of normal subjects, and subjects suffering from psychi-  atric disorder, e.g., obsessive compulsive disorder, schizophrenia, using a  class of artificial neural networks, viz., multi-layer perceptton. It is shown  that the multilayer perceptton is capable of classifying unseen test EEG  signals to a high degree of accuracy.  1 Introduction  The spontaneous electrical activity of the brain was first observed by Caton in 1875.  Although considerable investigations on the electrical activity of the non-human  brain have been undertaken, it was not until 1929 that a German neurologist Hans  Berger first published studies on the electroencephalogram (EEG) recorded on the  scalp of human. He lay the foundation of clinical and experimental applications of  EEG between 1929 and 1938.  Since then EEG signals have been used in both clinical and experimental work  to discover the state which the brain is in (see e.g., Herrmann, 1982, Kolb and  Whishaw, 1990, Lindsay and Holmes, 1984). It has served as a direct indication of  any brain activities. It is routinely being used in clinical diagnosis of epilepsy (see  e.g., Basar, 1980; Cooper, 1980).  Despite advances in technology, the classification of EEG signals at present requires  a trained personnel who either "eyeballs" the direct EEG recordings over time,  1151  1152 Tsoi, So, and Sergejew  or studies the contour maps representing the potentials generated from the "raw"  electrical signal (see e.g., Cooper, 1980). This is both a highly skillful job, as well as  a laborious task for a neurologist. With the current advances in computers, a logical  question to ask: can we use the computer to perform an automatic classification of  EEG signals into different classes denoting the psychiatric states of the subjects?  This type of classification studies is not new. In fact, in the late 1960's there were  a number of attempts in performing the automatic classification using discriminant  analysis techniques. However, this work was largely abandoned as most researchers  concluded that classification based on discriminant techniques does not generalise  well, i.e., while it has very good classification accuracies in classifying the data which  is used to train the automatic classification system, it may not have high accuracy  in classifying the unseen data which are not used to train the system in the first  instance.  Recently, a class of classification techniques, called artificial neural network (ANN),  based on nonlinear models, has become very popular (see e.g., Touretzky, 1989,  1990, Lippmann et al, 1991). This type of networks claims to be inspired by bi-  ological neurons, and their many inter-connections. This type of artificial neural  networks has limited pattern recognition capabilities. Among the many applications  which have been applied so far are sonar signal classification (see e.g., Touretzky,  1989), handwritten character recognition (see .e.g., Touretzky, 1990), facial expres-  sion recognition (see e.g., Lippmann et al. 1991).  In this paper, we will investigate the possibility of using an ANN for EEG classifica-  tions. While it is possible to extract features from the time series using either time  domain or frequency domain techniques, from some preliminary work, it is found  that the time domain techniques give much better results.  The structure of this paper is as follows: In section 2, we will give a brief discussion  on a popular class of ANNs, viz., multi-layer percepttons (MLP). In section 3, we  will discuss various feature extractions using time domain techniques. In section 4,  we will present results in classifying a set of unseen EEG signals.  2 Multi-layer Perceptrons  Artificial neural network (ANN) consists of a number of artificial neurons inter-  connected together by synaptic weights to form a network (see e.g, Lippmann,  1987). Each neuron is modeled by the following mechanical model:  y -- f(y wixi q- O) (1)  i=1  where y is the output of the neuron, wi, i = 1,2,..., n are the synaptic weights,  xi,i = 1,2...,n are the inputs, and 0 is a threshold function. The nonlinear  function f(.) can be a sigmoid function, or a hyperbolic tangent function. An ANN  is a network of inter-connected neurons by synapses (Hertz, Krogh and Palmer,  1991).  There are many possible ANN architectures (Hertz, Krogh, Palmer, 1991). A pop-  Classification of Electroencephalogram Using Artificial Neural Networks 1153  ular architecture is the multi-layer perceptron (MLP) (see e.g., Lippmann, 1987).  In this class of ANN, signal travels only in a forward direction. Hence it is also  known as a feedforward network. Mathematically, it can be described as follows:  y = f(Az +  z = f(Bu +  (2)  where y is a m x I vector, representing the output of the output layer neurons; z  is a p x I vector, representing the outputs of the hidden layer neurons; u is a n x 1  vector, representing the input feature vector; 0y is a m x i vector, known as the  threshold vector for the output layer neurons; O, is a p x I vector, representing  the threshold vector for the hidden layer neurons; A and B are matrices of m x p  and p x n respectively. The matrices A, and B are the synaptic weights connecting  the hidden layer neuron to the output layer neuron; and the input layer neurons,  and the hidden layer neurons respectively. For simplicity sake, we will assume the  nonlinearity function to be a sigmoid function, i.e.,  1  ,f(a) - I q- e -' (4)  The unknown parameters A, B, Oy, O, can be obtained by minimizing an error cri-  terion:  P  i=1  (5)  where P is the total number of examplars, di, i = 1,2,..., P are the desired outputs  which we wish the MLP to learn.  By differentiating the error criterion J with respect to the unknown parameters,  learning algorithms can be obtained.  The learning rules are as follows:  A new = A Id q_ r/A(y)ez '  (6)  where A new is the next estimate of the matrix A, T denotes the transpose of a  vector or a matrix. r/ is a learning constant. A(y) is a m x rn diagonal matrix,  whose diagonal elements are f'(yi),i = 1,2,...,m. The vector e is m x 1, and it is  given by d z' = [(dr - y,),(d2 - y2),...,(din - ym)] T.  The updating equation for the B matrix is given by the following  B new = B old q_ rlA(z)Su T  where  is a p x 1 vector, given by  (7)  1154 Tsoi, So, and Sergejew  5 = A TA(y)e (8)  and the other parameters are as defined above.  The threshold vectors can be obtained as follows:  and  y  (9)  o, = o, *d +  (10)  Thus it is observed that once a set of initial conditions for the unknown parameters  are given, this algorithm will find a set of parameters which will converge to a value,  representing possibly a local minimum of the error criterion.  3 Pre-processing of the EEG signal  A cursory glance at a typical EEG signal of a normal subject, or a psychiatrically ill  subject would convince anyone that one cannot hope to distinguish the signal just  from the raw data alone. Consequently, one would need to perform considerable  feature extraction (data pre-processing) before classification can be made. There  are two types of simple feature extraction techniques, viz., frequency domain and  time domain (see e.g., Kay, 1988, Marple, 1987). In the frequency domain, one  performs a fast Fourier transform (FFT) on the data. Often it is advantageous  to modify the signal by a window function. This will reduce the sidelobe leakage  (Kay, and Marpie, 1981, Harris, 1978). it is possible to use the average spectrum,  obtained by averaging the spectrum over a number of frames, as the input feature  vector to the MLP.  In the time domain, one way to pre-process the data is to fit a parametric model to  the underlying data. There are a number of parametric models, e.g., autoregressive  (AR) model, an autoregressive moving average (ARMA) model (see e.g., Kay, 1988,  Marple, 1987).  The autoregressive model can be described as follows:  N  st = E tist-i + {;t (11)  j=l  where st is the signal at time t; ct is assumed to be a zero mean Gaussian vari-  able with variance a 2. The unknown parameters or1, j = 1,2,...,N describe the  spectrum of the signal. They can be obtained by using standard methods, e.g.,  Yule-Walker equations, or Levinson algorithm (Kay, 1988, Marpie, 1987).  The autoregressive moving average (ARMA) model can be seen as a parsimonious  model for an AR model with a large N. Hence, as long as we are not concerned  Classification of Electroencephalogram Using Artificial Neural Networks 1155  about the interpretation of the AR model obtained, there is little advantage to  use the more complicated ARMA model. Subsequently, in this paper, we will only  consider the AR models.  Once the AR parameters are determined, then they can be used as the input fea-  tures to the MLP. It is known that the AR parametric model basically produces  a smoothed spectral envelope (Kay, 1988, Marple, 1987). Thus, the model param-  eters of AR is another way to convey the spectral information to the MLP. This  information is different in quality to that given by the FFT technique in that the  FFT transforms both signal and noise alike, while the parametric models tend to  favor the signal more and is more effective in suppressing the noise effect.  In some preliminary work, we find that the frequency domain extracted features do  not give rise to good classification results using MLP. Henceforth we will consider  only the AR parameters as input feature vectors.  4 Classification Results  In this section, we will summarise the results of the experiments in using the AR  parametric method of feature extraction as input parameters to the MLP.  We obtained EEG data pertaining to normal subjects, subjects who have been di-  agnosed as suffering from severe obsessive compulsive disorder (OCD), and subjects  who have been diagnosed as suffering from severe schizophrenia. Both the OCD and  the schizophrenic subjects are under medication. The subjects are chosen so that  their medication as well as their medical conditions are at a steady state, i.e., they  have not changed over a long period of time. The diagnosis is made by a number of  trained neurologists. The data files are chosen only if the diagnosis from the experts  concur.  We use the standard 10-20 recording system (Cooper, 1980), i.e., there are 19  channels of EEG recording, each sampled at 128 Hz. The recording were obtained  while the subject is at rest. Some data screening has been performed to screen out  the segment of data which contains any artifact. In addition, the data is anti-aliased  first by a low pass filter before being sampled. The sampled data is then low pass  filtered at 30 1tz to get rid of any higher frequency components.  We have chosen one channel, viz., the C channel (the channel which is the recording  of the signal at the azimuth of the scalp). This channel can be assumed to be  representative of the brain state from the overall EEG recording of the scalp.   This time series is employed for feature extraction purposes.  For time domain feature extraction, we first convert the time series into a zero  mean one. Then a data frame of one second duration is chosen : as the basic time  segmentation of the series. An AR model is fitted to this one second time frame to  From some preliminary work, it can be shown that this channel can be considered as  a hnear combination of the other channels, in the sense that the prediction error variance  is small.  2It has been found that the EEG signal is approximately stationary for signal length of  one second. Hence employing a data frame width of one second ensures that the underlying  assumptions in the AR modelling technique are valid (Marpie, 1988)  1156 Tsoi, So, and Sergejew  extract a feature vector formed by the resulting AR coefficients.  An average feature vector is acquired from the first 250 seconds, as in practice, the  first 250 seconds usually represent a state of calm in the patient, and therefore the  EEG is less noisy. After the first 250 seconds, the patient may enter an unstable  condition, such as breathing faster and muscle contraction which can introduce  artifacts. We use an AR model of length between 8 to 15.  We have chosen 15 such data file to form our training data set. This consists of 5  data files from normal subjects, 5 from OCD subjects, and 5 from subjects suffering  from schizophrenia.  In the time domain extracted feature vectors, we use a MLP with 8 input neurons,  15 hidden layer neurons, and 3 output neurons. The MLP's are trained accordingly.  We use a learning gain of 0.01. Once trained, the network is used to classify unseen  data files. These unseen data files were pre-classified by human experts. Thus the  desired classification of the unseen data files are known. This can then be used to  check the usefulness of the MLP in generalising to unseen data files.  The results 3 are shown in table 1.  The unseen data set consists of 6 normal subjects, 8 schizophrenic subjects, and  10 obsessive compulsive disorder subjects. It can be observed that the network  correctly classifies all the normal cases, makes one mistake in classifying the  schizophrena cases, and one mistake in classifying the OCD cases.  Also we have experimented on varying the number of hidden neurons. It is found  that the classification accuracy does not vary much with the variation of hidden  layer neurons from 15 to 50.  We have also applied the MLP on the frame by frame data, i.e., before they are being  averaged over the 250 second interval. However, it is found that the classification  results are not as good as the ones presented. We were puzzled by this result as  intuitively, we would expect the frame by frame results to be better than the ones  presented.  A plausible explanation for this puzzle is given as follows: the EEG data is in  general quite noisy. In the frame by frame analysis, the features extracted may  vary considerably over a short time interval, while in the approach taken here, the  noise effect is smoothed out by the averaging process.  One may ask: why would the methods presented work at all? In traditional EEG  analysis (Lindsay & Holmes, 1984), FFT technique is used to extract the frame  by frame frequency responses. The averaged frequency response is then obtained  over this interval. Traditionally only four dominant frequencies are observed, viz.,  the "alpha", "beta", "delta", and "theta" frequencies. It is a basic result in EEG  research that these frequencies describe the underlying state of the subject. For  example, it is known that the "alpha" wave indicates that the subject is at rest. An  EEG technologist uses data in this form to assist in the diagnosis of the subject.  On the other hand, it is relatively well known in signal processing literature (Kay,  3The results shown are typical results. We have used different data files for training  and testing. In most cases, the classification errors on the unseen data files are small,  similar to those presented here.  Classification of Electroencephalogram Using Artificial Neural Networks 1157  original activation of activation of I activation of predicted  classes normal schiz ocd classes  normal1 0.905 0.008 0.201 normal  normal2 0.963 0.006 0.103 normal  normal3 0.896 0.021 0.086 normal  normal4 0.870 0.057 0.020 normal  normal5 0.760 0.237 0.000 normal  normal6 0.752 0.177 0.065 normal  schizl 0.000 0.981 0.042 schiz  schiz2 '0.000 0.941 0.163 schiz  schiz3 0.002 0.845 0.050 schiz  schiz4 0.015 0.989 0.004 schiz  schiz5 0.000 0.932 0.061 schiz  schiz6 0.377 0.695 0.014 schiz  schiz7 0.062 0.898 0'.000 schiz  schiz8 0.006 0.086 0.921 ocd  ocdl 0.017 0.134 0.922 ocd  cd2 0.027 0.007 0.940 ocd  ocd3 0.000 0.033 0.993 ocd  ocd4 0.000 0.014 0.997 ocd  ocd5 0.015 0.138 0.889 ocd  ocd6 0.000 0.150 0.946 ocd  ocd7 0.002' 0.034 0.985 ocd  ocd8 0.006 0.960 0.003 schiz  ocd9 0.045 0.005 0.940 ocd  ocdl0 0.085 0.046 0.585 ocd  Table 1: Classification of unseen EEG data files  1988, Marple, 1987) to view the AR model as indicative of the underlying frequency  content of the signal. In fact, an 8th order AR model indicates that the signal  can be considered to consist of 4 underlying frequencies. Thus, intuitively, the  8th order AR model averaged over the first 250 seconds represents the underlying  dominant frequencies in the signal. Given this interpretation, it is not surprising  that the results are so good. The features extracted are similar to those used in the  diagnosis of the subjects. The classification technique, which in this case, the MLP,  is known to have good generalisation capabilities (Hertz, Krogh, Palmer, 1991).  This contrasts the techniques used in previous attempts in the 1960's, e.g., the  discriminant analysis, which is known to have poor generalisation capabilities. Thus,  one of the reasons why this approach works may be attributed to the generalisation  capabilities of the MLP.  5 Conclusions  In this paper, a method for classifying EEG data obtained from subjects who are  normal, OCD or schizophrenia has been obtained by using the AR parameters as  1158 Tsoi, So, and Sergejew  input feature vectors. It is found that such a network has good generalisation  capabilities.  6 Acknowledgments  The first and third author wish to acknowledge partial financial support from the  Australian National Health and Medical Research Council. In addition, the first  author wishes to acknowledge partial financial support from the Australian Research  Council.  7 References  Basar, E. (1980). EEG-Brain Dynamics - Relation between EEG and Brain Evoked  Potentials. Elsevier/North Holland Biomedical Press.  Cooper, R. (1980). EEG Technology. Butterworths. Third Editions.  Harris, F.J. (1978). "On the Use of windows for Harmonic Analysis with the  Discrete Fourier Transform". Proceedings IEEE. Vol. 66, pp 51-83.  Herrmann, W.M. (1982). Electroencephalography in Drug Research. Butterworths.  Hertz, J. Krogh, A, Palmer, R. (1991) Introduction to The Theory of Neural Com-  putation. Addison Wesley, Redwood City, Calif.  Kay, S.M., Marple, S.L., Jr. (1981). "Spectrum Analysis - A Modern Perspective".  Proceeding IEEE. Vol. 69, No. 11, Nov. pp 1380 o 1417.  Kay, S.M. (1988) Modern Spectral Estimation - Theory and Applications Prentice  hall.  Kolb, B., Whishaw, I.Q. (1990). Fundamentals of Human Neuropsychology. Free-  man, New York.  Lindsay, D.F., Holmes, J.E. (1984). Basic Human Neurophysiology. Elsevier.  Lippmann, R.P. (1987) "An introduction to computing with neural nets"  Acoustics Speech and Signal Processing Magazine. Vol. 4, No. 2, pp 4-22.  IEEE  Lippmann, R.P., Moody, J., Touretzky, D.S. (Ed.) (1991). Advances in Neural  Information Processing Systems 3. Morgan Kaufmann, San Mateo, Calif.  Marpie, S.L., Jr. (1987). Digital Spectral Analysis with Applications. Prentice Hall.  Touretzky, D.S. (Ed.) (1989). Advances in Neural Information Processing Systems  1. Morgan Kaufmann, San Mateo, Calif.  Touretzky, D.S. (Ed.) (1990). Advances in Neural Information Processing Systems  Morgan Kaufmann, San Mateo, Calif.  PART XII  WORKSHOPS  
Training Neural Networks with  Deficient Data  Volker Tresp  Siemens AG  Central Research  81730 Miinchen  Germany  tresp@zfe.siemens.de  Subutai Ahmad  Interval Research Corporation  1801-C Page Mill Rd.  Palo Alto, CA 94304  ahmad@interval.com  Ralph Neuneier  Siemens AG  Central Research  81730 Miinchen  Germany  ralph@zfe.siemens. de  Abstract  We analyze how data with uncertain or missing input features can  be incorporated into the training of a neural network. The gen-  eral solution requires a weighted integration over the unknown or  uncertain input although computationally cheaper closed-form so-  lutions can be found for certain Gaussian Basis Function (GBF)  networks. We also discuss cases in which heuristical solutions such  as substituting the mean of an unknown input can be harmful.  I INTRODUCTION  The ability to learn from data with uncertain and missing information is a funda-  mental requirement for learning systems. In the "real world", features are missing  due to unrecorded information or due to occlusion in vision, and measurements are  affected by noise. In some cases the experimenter might want to assign varying  degrees of reliability to the data.  In regression, uncertainty is typically attributed to the dependent variable which is  assumed to be disturbed by additive noise. But there is no reason to assume that  input features might not be uncertain as well or even missing competely.  In some cases, we can ignore the problem: instead of trying to model the rela-  tionship between the true input and the output we are satisfied with modeling the  relationship between the uncertain input and the output. But there are at least two  128  Training Neural Networks with Deficient Data 129  reasons why we might want to explicitly deal with uncertain inputs. First, we might  be interested in the underlying relationship between the true input and the output  (e.g. the relationship has some physical meaning). Second, the problem might be  non-stationary in the sense that for different samples different inputs are uncertain  or missing or the levels of uncertainty vary. The naive strategy of training networks  for all possible input combinations explodes in complexity and would require suffi-  cient data for all relevant cases. It makes more sense to define one underlying true  model and relate all data to this one model. Ahmad and Tresp (1993) have shown  how to include uncertainty during recall under the assumption that the network  approximates the "true" underlying function. In this paper, we first show how in-  put uncertainty can be taken into account in the training of a feedforward neural  network. Then we show that for networks of Gaussian basis functions it is possible  to obtain closed-form solutions. We validate the solutions on two applications.  2 THE CONSEQUENCES OF INPUT UNCERTAINTY  Consider the task of predicting the dependent variable  y  R from the input  vector x  M consisting of M random variables. We assume that the input  data {(xkl k -- 1,2,...,K} are selected independently and that P(x) is the joint  probability distribution of x. Outputs {(yk I k = 1, 2, ..., K} are generated following  the standard signal-plus-noise model  y} = f(x }) + (}  where {(} I k = 1, 2, ..., K} denote zero-mean random variables with probability den-  sity P(). The best predictor (in the mean-squared sense) of y given the input x  is the regressor defined by E(ylx ) = f y P(yIx) dx = f(x), where E denotes the  expectation. Unbied neural networks ymptotically (K  ) converge to the  regreor.  To account for uncertainty in the independent variable we sume that we do not  have access to x but can only obtain samples from another random vector z  M  with  z  = x  + 5   where {5 } k = 1, 2, ..., K} denote independent random vectors containing M random  variables with joint density P, ().  A neural network trained with data {(z},y})lk = 1,2,..., K} approximates  E(ylz) = P[z) y P(ylx) P(zlx) P(x) dydx = P(z) f(x) P,(z- x) P(x) dx.  (1)  thu, in general (ylz)  (z) nd we obtain  bied olution. Conide the  ce that the noise processes can be described by Gaussians P(e) = G(e; 0, a) and  P(6) = (,; 0, ) where, in ou notation, (; , ,) tnd  I 1  (; ,,) = (2)" H= * ep[-  j=l  Our notation does not stinguh between a random variable and its rezation.  2At this point, we sume that P, is independent of z.  130 Tresp, Ahmad, and Neuneier    f(x) Y  E (yl x)  E (yl z)  Figure 1: The top half of the figure shows the probabilistic model. In an example,  the bottom half shows E(ylz ) = f(z) (continuous), the input noise distribution  (dotted) and E(ulz) (dashed).  where m, s are vectors with the same dimensionality as z (here M). Let us take a  closer look at four special cases.  Certain input. If a = 0 (no input noise), the integral collapses and E(ylz ) = f(z).  fincertain input. If P(z) varies much more slowly than P(zlz), Equation 1 de-  scribed the convolution of f(z) with the noise process P6(z- z). Typical noise  processes will therefore blur or smooth the original mapping (Figures 1). It is  somewhat surprising that the error on the input results in a (linear) convolution  integral. In some special cases we might be able to recover f(z) from an network  trained on deficient data by deconvolution, although one should use caution since  deconvolution is very error sensitive.  finknown input. If aj - o then the knowledge of zj does not give us any infor-  mation about zi and we can consider the jth input to be unknown. Our formalism  therefore includes the case of missing inputs as special case. Equation 1 becomes  an integral over the unknown dimensions weighted by P(z) (Figure 2).  Linear approzimation. If the approximation  = f(z - + +  is valid, the input noise can be transformed into output noise and E(ylz ) = f(z).  This results can also be derived using Equation 1 if we consider that a convolution of  a linear function with a symmetrical kernel does not change the function. This result  tells us that if f(x) is approximately linear over the range where P6() has significant  Training Neural Networks with Deficient Data 131  Figure 2: Left: samples y' = f(x, x) are shown (no output noise). Right: with  one input missing, P(ylxl) appears noisy.  amplitude we can substitute the noisy input and the network will still approximate  f(x). Similarly, the mean mean(x1) of an unknown variable can be substituted  for an unknown input, if f(x) is linear and xj is independent of the remaining  input variables. But in all those cases, one should be aware of the potentially large  additional variance (Equation 2).  3 MAXIMUM LIKELIHOOD LEARNING  In this section, we demonstrate how deficient data can be incorporated into the  training of feedforward networks. In a typical setting, we might have a number of  complete data, a number of incomplete data and a number of data with uncertain  features. Assuming independent samples and Gaussian noise, the log-likelihood l  for a neural network NN, with weight vector w becomes  K K  i= ElogP(zk,yk) = log/G(yk;NNtox),o 'v) G(z};x,o ') P(x) dx.  k=l k=l  Note that now, the input noise variance is allowed to depend on the sample k. The  gradient of the log-likelihood with respect to an arbitrary weight wi becomes 3  c91 K K  Owi= Z OlgP(z'Y) --   1  = c9wi - (o.V)2 = p(z,y)  x  ONN,,(x) a(z;x,o.k ) P(x) dz.  - NNw(z)) Owi  (3)  First, realize that for a certain sample k (a  - 0)' OlogP(za,ya)/Owi =  (yk - NNw(za))/(o'V)  c9NNw(za)/c9wi which is the gradient used in normal back-  propagation. For uncertain data, this gradient is replaced by an averaged gra-  dient. The integral averages the gradient over possible true inputs x weighted  by the probability of P(xlz,y ) = P(zklx) P(ylx) P(x)/P(z,y). The term  SThis equation can also be obtained via the EM formalism. A similar equation was  obtained by Buntine and Weigend (1991) for binary inputs.  132 Tresp, Ahmad, and Neuneier  P(y}l x) = G(y}; NNox),a ) is of special importance since it weights the gradi-  ent higher when the network prediction NNo(x) agrees with the target y}. This  term is also the main reason why heuristics such as substituting the mean value  for a missing variable can be harmful: if, at the substituted input, the difference  between network prediction and target is large, the error is also large and the data  point contributes significantly to the gradient although it is very unlikely that the  substitutes value was the true input.  In an implementation, the integral needs to be approximated by a finite sum (i. e.  Monte-Carlo integration, finite-difference approximation etc.). In the experiment  described in Figure 3, we had a 2-D input vector and the data set consisted of both  complete data and data with one missing input. We used the following procedure  Train the network using the complete data. Estimate (a) . We used (a)    (Ec/(K- H)), where Zc is the training error after the network was trained with  only the complete data, and H is the number of hidden units in the network.  Estimate the input density P(z) using Gaussian mixtures (see next section).  Include the incomplete training patterns in the training.  For every incomplete training pattern  k be the certain input and let z be the missing input, and z k (z, z).   Let zc =   Approximate (assuming -1/2 < zj < 1/2, the hat stands for estimate)  0log P(z, y) 1 1 1  where  ONN(z,j/J)  9w,  J/  1  j=--Z/2  '  4 GAUSSIAN BASIS FUNCTIONS  The required integration in Equation I is computationally expensive and one would  prefer closed form solutions. Closed form solutions can be found for networks which  are based on Gaussian mixture densities. 4 Let's assume that the joint density is  given by  N  =  i=1  where ci is the location of the center of the ith Gaussian and and sij corresponds to  the width of the ith Gaussian in the jth dimension and P(wi) is the prior probability  of wi. Based on this model we can calculate the expected value of any unknown  Gaussian mixture learning with missing inputs is also addressed by Ghahramani and  Jordan (1993). See also their contribution in this volume.  Training Neural Networks with Deficient Data 133  0.1  0.08  0.1  0.04  28c 28c, 125c 125c, 28c 225m 28c, mean  225m 128m 225m subst  Figure 3: Regression. Left: We trained a feedforward neural network to predict the  housing price from two inputs (average number of rooms, percent of lower status  population (Tresp, Hollatz and Ahmad (1993)). The training data set contained  varying numbers of complete data points (c) and data points with one input missing  (m). For training, we used the method outlined in Section 3. The test set consisted  of 253 complete data. The graph (vertical axis: generalization error) shows that by  including the incomplete patterns in the training, the performance is significantly  improved. Right: We approximated the joint density by a mizture of Gaussians.  The incomplete patterns were included by using the procedure outlined in Sec-  tion 4. The regression was calculated using Equation 4. As before, including the  incomplete patterns in training improved the performance. Substituting the mean  for the missing input (column on the right) on the other hand, resulted in worse  performance than training of the network with only complete data.  0.86  0.84  0.82   0.8  0.78  0.74  0.72  0.7  0.68  0.7% 2o'oo o.%  # of data with miss. feat.  '  4 5  # of missing features  Figure 4: Left: Classification performance as a function of the number of missing  features on the task of 3D hand gesture recognition using a Gaussian mixtures  classifier (Equation 5). The network had 10 input units, 20 basis functions and 7  output units. The test set contained 3500 patterns. (For a complete description  of the task see (Ahmad and Tresp, 1993).) Class-specific training with only 175  complete patterns is compared to the performance when the network is trained with  an additional 350, 1400, and 3325 incomplete patterns. Either I input (continuous)  or an equal number of 1-3 (dashed) or 1-5 (dotted) inputs where missing. The  figure shows clearly that adding incomplete patterns to a data set consisting of  only complete patterns improves performance. Right: the plot shows performance  when the network is trained only with 175 incomplete patterns. The performance  is relatively stable as the number of missing features increases.  134 Tresp, Ahmad, and Neuneier  variable a: u from any set of known variables a: ' using (Tresp, Hollatz and Ahmad,  1993)  E,: cG(x" c?, 87) P(,)  Note, that the Gaussians are projected onto the known dimensions. The last equa-  tion describes the normalized basis function network introduced by Moody and  Darken (1989).  Classifiers can be built by approximating the class-specific data distributions  P(xlclassi ) by mixtures of Gaussians. Using Bayes formula, the posterior class  probability then becomes  P(c.8,)P(lc.**,)  P(classlar) = E.i P(class.i )P(arlclass.i ) '  (5)  We now assume that we do not have access to x but to z where, again, P(zlx ) =  G(z; x, a). The log-likelihood of the data now becomes  K N K N  l:  log / y] G(x; ci, s,) P(;o,) G(zk;x, a ) dx: ' log y. G(z};ci,  k=l i=1 k:l i=1  where (S/) 2: s/ i + (a) 2. We can use the EM approach (Dempster, Laird and  Rubin, 1977) to obtain the following update equations. Let ci.i, si.i and P(oi) denote  current parameter estimates and let C = (ci.i(a:)  + zjsi)/(Si)  and Dj =  k 2 2 k 2  ((0') sij)/(Sj ) . The new estimates (indicated by a hat) can be obtained using  G(z; c,,s2) P(,)  P('lz}) - EY=, 6(zk;c,s) p()  K  1  P(,) =  Y] P(,lz })  (6)  (7)  (s)  (9)  These equations can be solved by alternately using Equation 6 to estimate P(wi Iz )  and Equations 7 to 9 to update the parameter estimates. If a  = 0 for all k (only  certain data) we obtain the well known EM equations for Gaussian mixtures (Duda  and Hart (1973), page 200). Setting a = oo represents the fact that the jth input  k  k 2  is missing in the kth data point and C. - ci', D i. = s i. Figure 3 and Figure 4  show experimental results for a regression and a classificatmn problem.  Training Neural Networks with Deficient Data 135  5 EXTENSIONS AND CONCLUSIONS  We can only briefly address two more aspects. In Section 3 we only discussed  regression. We can obtain similar results for classification problems if the cost-  function is a log-likelihood function (e.g. the cross-entropy, the signal-plus-noise  model is not appropriate). Also, so far we considered the true input to be unobserved  data. Alternatively the true inputs can be considered unknown parameters. In this  case, the goal is to substitute the maximum likely input for the unknown or noisy  input. We obtain as log-likelihood function  + log P(x})].  The learning procedure consists of finding optimal values for network weights w and  true inputs x .  6 CONCLUSIONS  Our paper has shown how deficient data can be included in network training. Equa-  tion 3 describes the solution for feedforward networks which includes a computa-  tionally expensive integral. Depending on the application, relatively cheap approx-  imations might be feasible. Our paper hinted at possible pitfalls of simple heuris-  tics. Particularly attractive are our results for Gaussian basis functions which allow  closed-form solutions.  References  Ahmad, S. and Tresp, V. (1993). Some solutions to the missing feature problem in vision.  In S. J. Hanson, J. D. Cowan and C. L. Giles, (Eds.), Neural Information Processing  Systems 5. San Mateo, CA: Morgan Kaufmann.  Buntine, W. L. and Weigend, A. S. (1991). Bayesian Back-Propagation. Complex systems,  Vol. 5, pp. 605-643.  Dempster, A. P., Laird, N.M. and Rubin, D. B. (1977). Maximum likelihood from incom-  plete data via the EM algorithm. J. Royal Statistical Society Series B, 39, pp. 1-38.  Duda, R. O. and Hart, P. E. (1973). Pattern Classification and Scene Analysis.  Wiley and Sons, New York.  John  Ghahramani, Z. and Jordan, M. I. (1993). Function approximation via density estimation  using an EM approach. MIT Computational Cognitive Sciences, TR 9304.  Moody, J. E. and Darken, C. (1989). Fast learning in networks of locally-tuned processing  units. Neural Computation, Vol. 1, pp. 281-294.  Tresp, V., Hollatz J. and Ahmad, S. (1993). Network structuring and training using rule-  based knowledge. In S. J. Hanson, J. D. Cowan and C. L. Giles, (Eds.), Neural Information  Processing Systems 5. San Mateo, CA: Morgan Kaufmann.  Tresp, V., Ahmad, S. and Neuneier, R. (1993). Uncertainty in the Inputs of Neural  Networks. Presented at Neural Networks for Computing 1993.  
Backpropagation without Multiplication  Patrice Y. Simard  AT&T Bell Laboratories  Holmdel, NJ 07733  Hans Peter Graf  AT&T Bell Laboratories  Itohndel, NJ 07733  Abstract  The back propagation algorithm has been modified to work with-  out any multiplications and to tolerate computations with a low  resolution, which makes it more attractive for a hardware imple-  mentation. Numbers are represented in floating point format with  i bit mantissa and 3 bits in the exponent for the states, and i bit  mantissa and 5 bit exponent for the gradients, while the xveights are  16 bit fixed-point numbers. In this way, all the computations can  be executed with shift and add operations. Large netxvorks with  over 100,000 weights were trained and demonstrated the same per-  formance as networks computed with full precision. An estimate of  a circuit implementation shows that a large network can be placed  on a single chip, reaching more than 1 billion weight updates per  second. A speedup is also obtained on any machine where a mul-  tiplication is slower than a shift operation.  i INTRODUCTION  One of the main problems for implementing tile backpropagation algorithm in hard-  ware is the large number of multiplications that have to be executed. Fast multipli-  ers for operands with a high resolution require a large area. Hence the multipliers  are the elements dominating the area of a circuit.. Many researchers have tried to  reduce the size of a circuit bv limiting the resolution of the computation. Typically,  this is done by simply reducfng the number of bits utilized for the computation. For  a forward pass a reduction to just a few, 4 to 6, bits, often degrades the performance  very little, but learning requires considerably more resolution. Requirements rang-  ing anywhere fi'om 8 bits to more than 16 bits were reported to be necessary to make  learning converge reliably (Sakaue et al., 1993; Asanovic, Morgan and Wawrzynek,  1993; Reyneri and Filippi, 1991). But there is no general theory, how much resolu-  tion is enough, and it depends oil several factors, such as the size and architecture  of the netxvork as well as on the type of problem to be solved.  232  Backpropagation without Multiplication 233  Several researchers have tried t.o train networks where the weights are limited to  powers of two (Kwan and Tang, 1993; White and Ehnasry, 1992; Marchesi et. al.,  1993). In this way all the nmltiplications can be reduced to shift operations, an  operation that can be implemented with much less hardware than a multiplication.  But restricting the weight values severely impacts the performance of a network, and  it is tricky to make the learning procedure converge. In fact, some researchers keep  weights with a full resolution off-line aud update these weights in the backward pass,  while the weights with reduced resolution are used in the forward pass (Marchesi  et al., 1993). Silnilar tricks are usually used when networks implemented in analog  hardware are trained. Weights with a high resolution are stored in an external,  digital memory while the analog network with its limited resolution is used in the  forward pass. If a high resolution copy is not stored, the weight update process  needs to be modified. This is t. ypically done by using a stochastic update technique,  such as weight dithering (Vincent and Myers, 1992), or weight perturbation (Jabri  and Flower, 1992).  We present here an algorithm that instead of reducing the resolution of the weights,  reduces the resolution of all the other values, namely those of the states, gradients  and learning rates, to powers of two. This eliminates multiplications without af-  fecting the learning capabilities of the network. Therefore we obtain the benefit of  a much compacter circuit without any compromises on the learning performance.  Simulations of large networks with over 100,000 weights show that this algorithm  perfor. ms as well as standard i)ackpropagation computed with 32 bit floating point  precision.  2 THE ALGORITHM  The forward propagatiou for each unit i, is given by the equation:  where f is the unit function, wi is the weight from unit i t.o unit j, and xi is the  activation of unit i. The backpropagation algorithm is robust with regard to the  unit filnction as long as the function is nonlinear, monotonically increasing, and a  derivative exists (the most commonly used fimction is depicted iu Figure 1, left.  A saturated ramp function (see Figure 1, middle), for instance, performs as well  as the differentiable sigmoid. The binary threshold function, however, is too much  of a simplification and results in poor performance. The clioice of our function is  dictated by the fact that we would like t.o have only powers of two for the unit values.  This function is depicted in Figure 1, right. It giw_s performances comparable to  the sigmoid or the saturated ramp. Its values can be represented by a 1 bit mantissa  (the sign) with a 2 or 3 bit exponent (uegative powers of two).  The derivative of this fimction is a sum of Dirac delta functions, but we take instead  the derivative of a piecewise linear ramp function (see Figure 1). One could actually  consider this a low pass filtered version of the real derivative. After the gradients  of all the units have been cornpuled using the equation,  we will discretize the values to be a power of two (with sign). This introduces noise  into the gradient and its effect on the learning has to be considered carefully. This  234 Simard and Graf  Sigmoid  Piecewise linear  Power of two  uncCon  0  Fnctlon derivative (approxlmaton)  1.$  Figure 1: Left' sigmoid function xvith its derivative. Middle: piecewise linear  function with its derivative. Right: Saturated power of two function with a power  of two approximation of its derivative (identical to the piece,vise linear derivative).  problem will be discussed in section 4. The backpropagation algorithm can now be  implemented with additions (or subtractions) and shifting only. The weight update  is given by the equation:  A Wj i -- -- ?]gj 32 i (3)  Since both gj and xi are powers of two, the veight update also reduces to additions  and shifts.  3 RESULTS  A large structured network with five layers and overall more than 100,000 weights  was used to test this algorithm. The application analyzed is recognizing handwritten  character images. A database of 800 digits was used for training and 2000 hand-  written digits were used for testing. A description of this netxvork can be found in  (Le Cun et al., 1990). Figure 2 shows the learning curves on the test set for various  unit functions and discretization processes.  First, it should be noted that the results given by the sigmoid function and the  saturated ramp with full precision on unit values, gradients, and weights are very  similar. This is actually a well known behavior. The surprising result comes from  the fact that reducing the precision of the unit values and the gradients to a 1 bit  mantissa does not reduce the classification accuracy and does not even slow down the  learning process. During these tests the learning process was interrupted at various  stages to check that both the unit values (including the input layer, but excluding  the output layer) and the gradients (all gradients) were restricted to powers of two.  It was flirther confirmed that only 2 bits were sufficient for the exponent of the unit  Backpropagation without Multiplication 235  Training  oo. error  o  sigmoid  o pecewise lin  eo ! o powerof2  7O  60 ,  10  0 2 4 6 8 10 12 14 18 18 20 22 24  age (in 1000)  Testing  ,oo I error  sigmoid  9o | o pecewise lin  7O  3O  10  0  0 2 4 6 8 10 12 14 16 16 20 22 24  age (in 1000)  Figure 2: Training and testing error during learning. The filled squares (resp.  empty squared) represent t. he points obtained with the vanilla backpropagation and  a sigmoid function (resp. piecewise linear function) used as an activation function.  The circles represent the same experiment done wit. h a power of t. wo function used  as the activation fimction, and xvit. h all unit. gradient. s discretized to the nearest  power of two.  values (from 20 t.o 2 -3) and 4 bit.s vere sufficient for the exponent. of the gradients  (from 20 to 2-1).  To test whether there was any asympt.otic limit on performance, we ran a long  term experiment (several days) with our largest network (17,000 h'ee parameters)  for handwritten character recognition. The training set (60,000 patterns) was made  out 30,000 patterns of tile original NIST t. raining set, (easy) and 30,000 patterns  out of the original NIST testing set (hard). Using the most basic backpropagat,ion  algorithm (with a guessed constant learning rate) we got the training raw error rate  down to under 1% in 20 epochs which is comparable to our standard learning time.  Performance on the t,est set was not as good wit,h t,he discrete network (it took twice  as long to reach equal performance xvith the discrete network). This was at,tribut,ed  to the unnecessary discretization of t,he output unit. s 1  These results show that gradients and unit activat.ions can be discretized t.o powers  of two with negligible loss in performance and convergence speed! The next section  will present, theoretical explanations for why this is at. all possible and why it, is  generally the case.  Since the output units are not multiplied by anything, there is no need to use a  discrete activation function. As a matter of fact the coutinuous sigmoid functiou can be  implemented by just changing the target, values (using the inverse sigmoid function) and  by using no activation function for the output units. Titis modification was not introduced  but we believe it vould improves the performance on the test set, especially when fancy  decision rules (with confidence evaluation) are used, since they require high precision on  the output units.  236 Simard and Graf  2000,  1600,  1200  1000,  600.  400.  200,  2000  1800  1600  1400  1200  600  histogram  -0,002  histogram  0,002 .004  W1 4  W:  Best case: Noise  is uncorrelated and  all weights are equal  .  . -Vw2  Worse case: Nose  is correlated or the  weights are unequal  o  -o , Go4 -0 . 002 o G . OG2  . 004  Figure 3: Top left: histogram of the gradients of one output unit after more than  20 epochs of learning over a training set of 60.000 patterns. Bottom left: same  histogram assuming that the distribution is constant between powers of two. Right:  simplified network architectures for noise effect analysis.  4 DISCUSSION  Discretizing the gradient is potentially very dangerous. Convergence may no longer  be guaranteed, learning may become prohibitively slow, and final performance after  learning may be be too poor to be interesting. We will now explain why these  problems do not arise for our choice of discretization.  Let gi(P) be the error gradient at. weight i and pattern p. Let tti and rr i be the mean  and standard deviation of gi(P) over the set of patterns. The mean /_ti is what is  driving the weights to their final values, the standard deviation rr i represents the  amplitudes of the variations of the gradients from pattern to pattern. In batch  learning, only Pi is used for the weight update, while in stochastic gradient descent,  each gi(P) is used for the weight update. If the learning rate is small enough the  effects of the noise (measured by rri) of the stochastic variable rr i(p) are negligible,  but the frequent weight updates in stochastic gradient. descent result in important  speedups.  To explain why the discretization of the gradient to a power of two has negligible  effect on the performance, consider that in stochastic gradient descent, the noise on  the gradient is already so large that it is minimally affected by a rounding (of the  gradient) to the nearest power of two. Indeed asymptotically, the gradient average  (Yi) tends to be negligible compared to its standard deviation (lYi) , meaning that  from pattern to pattern the gradient can undergo sign reversals. Rounding to the  nearest power of two in comparison is a change of at most aa%, but never a change  in sign. This additional noise can therefore be compensated by a slight decrease in  the learning rate which xvill hardly affect the learning process.  Backpropagation without Multiplication 237  The histogram of gi(P) after learning in the last experiment described in the result  section, is shown in Figure 3 (over the training set of 60,000 patterns). It is easy to  see in the figure that/i is small with respect to ai (in this experiment p,i was one to  two orders of magnitude smaller than ai depending on the layer). We can also see  that rounding each gradient to the nearest power of two will not affect significantly  the variance eri and therefore the learning rate xvill uot need to be decreased to  achieve the same performance.  We will now try to evaluate the rounding to the nearest power of two effect more  quantitatively. The standard deviation of the gradient for any weight can be written   , i E(gi(p) _ tq)2 = 1 _ i_t2 1  a' =  - Z gi(p)   - Z g,(p)  (4)  p p p  This approximation is very good asymptotically (aFter a few epochs of learning).  For instance if ]Yi] < ai/10, the above formula gives the standard deviation to 1%.  Rounding the gradient gi to the nearest power of two (while keeping the sign) can  be viewed as the effect of a multiplicative noise ni in the equation  gi = = gi(l + hi) for sortie k (5)  where gi' is the nearest power of two from gi. It can be easily verified that this  implies that ni ranges from -1/a and 1/3. From now on, we will view ni as a  random variable which models as noise the effect of discretization. To simplify the  computation we will assume that -i has uniform distribntion. The effect. of this  sumption is depicted in figure 3, where the bottom histogram has been assumed  constant between any two powers of two.  To evaluate the effect of the noise ni in a multilayer network, let nil be the multi-  plicative noise introduced at layer I (1 = 1 for the output, and I = L for the first  layer above the input) for weight i. Let's further assume that there is only one unit  per layer (a simplified diagram of the network architecture is shown on figure 3.  This is the worst case analysis. If there are several units per layer, the gradients  will be summed to units in a lower layer. The gradients within a layer are corre-  lated from unit to unit (they all originate from the same desired values), but the  noise introduced by the discretization can only be less correlated, not more. The  summation of the gradient in a lower layer can therefore only decrease the effect of  the discretization. The worst case analysis is therefore when there is only one unit  per layer as depicted in figure 3, extreme right. We will further assume that the  noise introduced by the discretization in oue layer is independent from the noise  introduced in the next layer. This is not really true but it greatly simplifies the  derivation.  ' be the mean and standard deviation of gi(P)'. Since nti has a zero  Let Y'i and a i  mean, Y'i = yi and t. is negligible with respect to gi(P). In the worst case, when the  radient has to be backpropagated all the way to the input, the standard deviation  IS'  .(,,(,) II( - ",) - / lid",,  p ,,  I 1  L   E gi(P)"- II (1 + "t,)'"el",; -  p I J-I/a  o' 7 1+ (6)  238 Simard and Graf  As learning progresses, the minimum average distance of each weight to the weight  corresponding to a. local minimum becomes proportional to the variance of the noise  on that weight, divided by the learning rate. Therefore, asymptotically (which is  where most of the time is spent), for a given convergence speed, the learning rate  should be inversely proportional to the variance of the noise in the gradient. This  means that to compensate the effect of the discretization, the learning rate should  be divided by  1.02  (7)  Even for a 10 layer network this value is only 1.2, (a is 20 % larger than ai).  The assumption that the noise is independent from layer to layer tends to slightly  underestimate this number while the assumption that the noise fi'om tinit to unit  in the same layer is completely correlated tends to overestimate it.  All things considered, we do not expect that the learning rate should be decreased  by more than 10 to 20% for any practical application. In all our simulations it was  actually left unchanged!  5 HARDWARE  This algorithm is well suited for integrating a large network on a single chip. The  weights are implemented with a resolution of 16 bits, while the states need only 1  bit in the mantissa and 3 bits in the exponent, the gradient 1 bit in the mantissa  and 5 bits in the exponent, and for the learning rate 1 bits mantissa and 4 bits  exponent suffice. In this way, all the multiplications of weights with states, and of  gradients with learning rates and states, reduce to add operations of the exponents.  For the forward pass the weights are multiplied with the states and then summed.  The multiplication is executed as a shift operation of the weight values. For sum-  ming two products their mantissae have to be aligned, again a shift operation, and  then they can be added. The partial sums are kept. at full resolution until the end of  the summing process. This is necessary to avoid losing the influence of many small  products. Once the snm is computed, it is then quantized simply by checking the  most significant bit in the mantissa. For the backward propagation the computation  runs in the same way, except that now the gradient is propagated through the net,  and the learning rate has to be taken into account..  The only operations reqnired for this algorithm are 'shift' and 'add'. An ALU  implementing these operations with the resolution mentioned can be built with  less than 1,000 transistors. In order to execute a network fast, its weights have to  be stored on-chip. Otherwise, the time required to transfer weights from external  memory onto the chip boundary makes the high compute power all but useless. If  storage is provided for 10,000 weights plus 2,000 states, this requires less than 256  kbit of memory. Together with 256 ALUs and circuitry for routing the data, this  leads to a circuit with about 1.7 million transistors, where over 80% of them are  contained in the memory. This assumes that the memory is implemented with static  cells, if dynamic memory is used instead the transistor count drops considerably..  An operating speed of 40 MHz results in a compnte rate of 10 billion operations  per second. With such a. chip a network may be trained at. a speed of more than 1  billion weight updates per second.  Backpropagation without Multiplication 239  This algorithm has been optiinized for an implementation on a chip, but it can  also provide a considerable speed up when executed on a standard computer. Due  to the small resolution of the numbers, several states can be packed into a 32 bit  number and hence many more fit into a chache. Moreover on a machine without  a hardware multiplier, where the multiplication is execnted with microcode, shift  operations may be much faster than multiplications. Hence a substancial speedup  may be observed.  References  Asanovic, K., Morgan, N., and Wawrzynek, J. (1993). Using Sinrelations of Re-  duced Precision Arithmetic to Design a Neuro- Microprocessor. J. VLSI Signal  Processing, 6(1) :33-44.  Jabri, M. and Flower, B. (1992). Weight Perturbation: An optimal architecture  and learning technique for analog VLSI feedforward and recurrent multilayer  networks. IEEE Trans. Neural Networks, 3(3):154-157.  Kwan, H. and Tang, C. (1993). Multipyerless Multilayer Feedforward Neural Net-  work Design Suitable for Cont. inuous Input-Output Mapping. Electronic Let-  ters, 29(14):1259-1260.  Le Gun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard,  W., and Jackel, L. D. (1990). tlandwritten Digit Recognition with a Back-  Propagation Network. In Touretzky, D., editor, Neural Information Processing  Systems, volume 2, (Denver, 1989). Morgan Kaufinan.  Marchesi, M., Orlando, G., Piazza, F., and Uncini, A. (1993). Fast Neural Networks  without Multipliers. IEEE Tra,s. Neural Networks, 4(1):53-62.  Reyneri, L. and Filippi, E. (1991). An analysis on the Performance of Silicon hn-  plementations of Backpropagation Algorithms for Artificial Neural Networks.  IEEE Trans. Computers, 40(12):1380-1389.  Sakaue, S., Kohda., T., Yamamoto, H., Maruno, S., and Shimeki, Y. (1993). Re-  duction of Reqnired Precision Bits for Back-Propagation Applied to Pattern  Recognition. IEEE Trans. Neural Networks, 4(2):270-275.  Vincent, J. and Myers, D. (1992). Weight dithering and Wordlength Selection for  Digital Backpropagation Networks. BT Technology J., 10(3):124-133.  White, B. and Elmasry, M. (1992). The Digi-Neocognitron: A Digital Neocognitron  Neural Network Model for VLSI. IEEE Trans. Neutral Networks, 3(1):73-85.  
Synchronization, oscillations, and 1/f  noise in networks of spiking neurons  Martin Stemmler, Marius Usher, and Christof Koch  Computation and Neural Systems, 139-74  California Institute of Technology  Pasadena, CA 91125  Zeev Olami  Dept. of Chemical Physics  Weizmann Institute of Science  Rehovot 76100, Israel  Abstract  We investigate a model for neural activity that generates long range  temporal correlations, 1If noise, and oscillations in global activity.  The model consists of a two-dimensional sheet of leaky integrate-  and-fire neurons with feedback connectivity consisting of local ex-  citation and surround inhibition. Each neuron is independently  driven by homogeneous external noise. Spontaneous symmetry  breaking occurs, resulting in the formation of "hotspots" of activ-  ity in the network. These localized patterns of excitation appear  as clusters that coalesce, disintegrate, or fluctuate in size while si-  multaneously moving in a random walk constrained by the interac-  tion with other clusters. The emergent cross-correlation functions  have a dual structure, with a sharp peak around zero on top of  a much broader hill. The power spectrum associated with single  units shows a 1If decay for small frequencies and is flat at higher  frequencies, while the power spectrum of the spiking activity aver-  aged over many cells--equivalent to the local field potential--shows  no 1If decay but a prominent peak around 40 Hz.  629  630 Stemmler, Usher, Koch, and Olami  1 The model  The model consists of a 100-by-100 lattice of integrate-and-fire units with cyclic  lattice boundary conditions. Each unit represents the nerve cell membrane as a  simple RC circuit (r = 20 msec) with the addition of a reset mechanism; the  refractory period Tr,! is equal to one iteration step (1 msec).  Units are connected to each other within the layer by local excitatory and inhibitory  connections in a center-surround pattern. Each unit is excitatorily connected to  N = 50 units chosen from a Gaussian probability distribution of r = 2.5 (in terms  of the lattice constant), centered at the unit's position N inhibitory connections  per unit are chosen from a uniform probability distribution on a ring eight to nine  lattice constants away.  Once a unit reaches the threshold voltage, it emits a pulse that is transmitted in  one iteration (1 msec) to connected neighboring units, and the potential is reset by  subtracting the threshold from resting potential.  V/(t + 1) = (exp(-1/r)(t) + Ii(t)) 0[h - V(t)]. (1)  Ii is the input current, which is the sum of lateral currents from presynaptic units  and external current. The lateral current leads to an increase (decrease) in the  membrane potential of excitatory (inhibitorily) connected cells. The weight of  the excitation and inhibition, in units of voltage threshold, is  and /3. The  values ct = 1.275 and /3 = 0.67 were used for simulations. The external input is  modeled independently for each cell as a Poisson process of excitatory pulses of  magnitude i/N, arriving at a mean rate ,t. Such a simple cellular model mimics  reasonably well the discharge patterns of cortical neurons [Bernander et al., 1994,  Softky and Koch, 1993].  2 Dynamics and Pattern Formation  In the mean-field approximation, the firing rate of an integrate-and-fire unit is a  function of the input current [Amit and Tsodyks, 1991] given by  f(I) = (Tre! - r ln[1 - 1/(I r)]) -, (2)  where Tref is the refractory period and r the membrane time constant.  In this approximation, the dynamics associated with eq. i simplify to  dli = -I, + y. WI(I) + I? t (3)  dt j '  where W/j represents the connection strength matrix from unit j to unit i.  Homogeneous firing activity throughout the network will result as long as the con-  nectivity pattern satisfies ITV(k)-i < 0 for all k, where ITV(k) is the Fourier transform  of W/j. As one increases the total strength of lateral connectivity, clusters of high  firing activity develop. These clusters form a hexagonal grid across the network; for  even stronger lateral currents, the clusters merge to form stripes.  The transition from a homogeneous state to hexagonal clusters to stripes is generic  to many nonequilibrium systems in fluid mechanics, nonlinear optics, reaction-  diffusion systems, and biology. (The classic theory for fluid mechanics was first  Synchronization, Oscillations, and l/f Noise in Networks of Spiking Neurons 631  developed by [Newell and Whitehead, 1969], see [Cross and Hohenberg, 1993] for  an extensive review. Cowan (1982) was the first to suggest applying the techniques  of fluid mechanics to neural systems.)  The richly varied dynamics of the model, however, can not be captured by a mean-  field description. Clusters in the quasi-hexagonal state coalesce, disintegrate, or  fluctuate in size while simultaneously moving in a random walk constrained by the  interaction with other clusters.  Random Walk of Clusters  18 ,  16  14  2  0 I  0 2  4 6 8 1% 12 14 116 18  x (lattlce units)  Figure 1: On the left, the summed firing activity for the network over 50 msec of  simulation is shown. Lighter shades denote higher firing rates (maximum firing rate  120 Hz). Note the nearly hexagonal pattern of clusters or "hotspots" of activity.  On the right, we illustrate the motion of a typical cluster. Each vertex in the graph  represents a tracked cluster's position averaged over 50 msec. Repulsive interactions  with surrounding clusters generally constrain the motion to remain within a certain  radius. This vibratory motion of a cluster is occasionally punctuated by longer-  range diffusion.  Statistical fluctuations, diffusion and synchronization of clusters, and noise in the  external input driving the system lead to 1/f-noise dynamics, long-range correla-  tions, and oscillations in the local field potential. These issues shall be explored  next.  S 1If Noise  The power spectra of spike trains from individual units are similar to those pub-  lished in the literature for nonbursting cells in area MT in the behaving mon-  key [Bair et al., 1994]. Power spectra were generally flat for all frequencies above  100 Hz. The effective refractory period present in an integrate-and-fire model in-  troduces a dip at low frequencies (also seen in real data). Most noteworthy is the  1/f 's component in the power spectrum at low frequencies. Notice that in order  to see such a decay for very low frequencies in the spectrum, single units must be  recorded for on the order of 10-100 sec, longer than the recording time for a typical  trial in neurophysiology.  We traced a cluster of neuronal activity as it diffused through the system, and  632 Stemmler, Usher, Koch, and Olami  3  2'5 1  2  1.5  1  0.5  0  0  Spike Train Power Spectrum  I$I distribution  20 40 60 80 100  30. 50. 70. 100. 150. 200  Hz  msec  Figure 2: Typical power spectrum and ISI distribution of single units over 400 sec  of simulation. At low frequencies, the power spectrum behaves as f-0.84-0.017 up  to a cut-off frequency of m 8 Hz. The ISI distribution on the right is shown on a  log-log scale. The ISI histogram decays as a power law P(t) or t-1'74-'2 between  25 and 300 msec. In contrast, a system with randomized network connections will  have a Poisson-distributed ISI histogram which decays exponentially.  measured the ISI distribution at a fixed point relative to the cluster center. In the  cluster frame of reference, activity should remain fairly constant, so we expect and  do find an interspike interval (ISI) distribution with a single characteristic relaxation  time:  Pt(t) = A(r)exp(-tA(r)),  where the firing rate A(r) is now only a function of the distance r in cluster coordi-  nates. Thus Pr(t) is always Poisson for fixed r.  If a cluster diffuses slowly compared to the mean interspike interval, a unit at a  fixed position samples many ISI distributions of varying A(r) as the cluster moves.  The ISI distribution in the fixed frame reference is thus  P(t) - / (r)2 exp(-t (r))dr.  (4)  Depending on the functional form of (r), P(t) (the ISI distribution for a unit at  a fixed position) will decay as a power law, and not as an exponential. Empirically,  the distribution of firing rates as a function of r can be approximated (roughly) by  a Gaussian. A Gaussian (r) in eq. 4 leads to P(t) ~ t -2 for t at long times. In  turn, a power-law (fractal) P(t) generates 1If noise (see Table 1).  4 Long-Range Cross-Correlations  Excitatory cross-correlation functions for units separated by small distances consist  of a sharp peak at zero mean time delay followed by a slower decay characterized  by a power law with exponent -0.21 until the function reaches an asymptotic level.  Nelson et al. (1992) found this type of cross-correlation between neurons-a "castle  on a hill"-to be the most common form of correlation in cat visual cortex. Inhibitory  Synchronization. Oscillations, and l/f Noise in Networks of Spiking Neurons 633  cross-correlations show a slight dip that is much less pronounced than the sharp  excitatory peak at short time-scales.  looo  750  500  250  Cross--Correlation at d = 1  I I I I I I  --300 --200 --100 0 100 200 300  1000  75O  500  250  Cross--Correlation at d = 9  I I I I I I I  --300 --200 --100 0 100 200 300  nsec  Figure 3: Cross-correlation functions between cells separated by d units of the  lattice. Given the center-surround geometry of connections, the upper curve corre-  sponds to mutually excitatory coupling and the lower to mutually inhibitory cou-  pling. Correlations decay as 1/t '2, consistent with a power spectrum of single  spike trains that behaves as 1If 'a.  Since correlations decay slowly in time due to the small exponent of the power,  long temporal fluctuations in the firing rate result, as the i/f-type power spectra of  single spike trains demonstrate. These fluctuations in turn lead to high variability  in the number of events over a fixed time period.  In fact, the decay in the auto-correlation and power spectrum, as well as the rise  in the variability in the number of events, can be related back to the slow de-  cay in the interspike interval (ISI) distribution. If the ISI distribution decays  as a power law P(t) ., t -u, then the point process giving rise to it is fractal  with a dimension D =  - 1 [Mandelbrot, 1983]. Assuming that the simula-  tion model can be described as a fully ergodic renewal process, all these quanti-  ties will scale together [Cox and Lewis, 1966, Teich, 1989, Lowen and Teich, 1993,  Usher et ah, 1994]:  634 Stemmler, Usher, Koch, and Olami  Table 1: Scaling Relations and Empirical Results  Vat(N) Auto-correlation Power Spectrum ISI Distribution  Vat(N) ~ N  A(t) ~ t -2 S(f) ~ f-+i P(t) ~ t -  Var(N) ~ N TM A(t) ~/-0.2 $(f) ~ f-0.s P(t) ~ t  These relations will be only approximate if the process is nonrenewal or nonergodic,  or if power-laws hold over a limited range. The process in the model is clearly non-  renewal, since the presence of a cluster makes consecutive short interspike intervals  for units within that cluster more likely than in a renewal process. Hence, we expect  some (slight) deviations from the scaling relations outlined above.  5 Cluster Oscillations and the Local Field Potential  The interplay between the recurrent excitation that leads to nucleation of clusters  and the "firewall" of inhibition that restrains activity causes clusters of high activity  to oscillate in size. Fig 4 is the power spectrum of ensemble activity over the size  of a typical cluster.  25  20  15  10  5  0  Power Spectrum of Cluster Activity within radmus d=9  0 20 40 60 80 100  Hz  Figure 4: Power spectrum of the summed spiking activity over a circular area the  size of a single cluster (with a radius of 9 lattice constants) recorded from a fixed  point on the lattice for 400 seconds. Note the prominent peak centered at 43 Hz  and the loss of the 1If component seen in the single unit power spectra (Fig. 2).  These oscillations can be understood by examining the cross-correlations between  cells. By the Wiener-Khinchin theorem, the power spectrum of cluster activity is the  Fourier transform of the signal's auto-correlation. Since the cluster activity is the  sum of all single-unit spiking activity within a cluster of N cells, the autocorrelation  of the cluster spiking activity will be the sum of N auto-correlations functions of the  Synchronization, Oscillations, and l/f Noise in Networks of Spiking Neurons 635  individual cells and N x (N - 1) cross-correlation functions among individual cells  within the cluster. The ensemble activity is thus dominated by cross-correlations.  In general, the excitatory "castles" are sharp relative to the broad dip in the cross-  correlation due to inhibition (see Fig. 3). In Fourier space, these relationships  are reversed: broader Fourier transforms of excitatory cross-correlations are paired  with narrower Fourier transforms of inhibitory cross-correlations. Superposition of  such transforms leads to a peak in the 30-70 Hz range and cancellation of the 1If  component which was present the single unit power spectrum.  Interestingly, the power spectra of spike trains of individual cells within the network  (Fig. 2) show no evidence of a peak in this frequency band. Diffusion of clusters  disrupts any phase relationship between single unit firing and ensemble activity.  The ensemble activity corresponds to the local field potential in neurophysiological  recordings. While oscillations between 30 and 90 Hz have often been seen in the  local field potential (or sometimes even in the EEG) measured in cortical areas in  the anesthetized or awake cat and monkey, these oscillations are frequently not or  only weakly visible in multi- or single-unit data (e.g., [Eeckman and Freeman, 1990,  Kreiter and Singer, 1992, Gray et al., 1990, Eckhorn et al., 1993]). We here offer a  general explanation for this phenomenon.  Acknowledgments: We are indebted to William Softky, Wyeth Bair, Terry Se-  jnowski, Michael Cross, John Hopfield, and Ernst Niebur, for insightful discus-  sions. Our research was supported by a Myron A. Bantrell Research Fellowship,  the Howard Hughes Medical Institute, the National Science Foundation, the Office  of Naval Research and the Air Force Office of Scientific Research.  References  [Amit and Tsodyks, 1991] Amit, D. J. and Tsodyks, M. V. (1991). Quantitative  study of attractor neural network retrieving at low rates: 1. substrate spikes, rates  and neuronal gain. Network Corn., 2(3):259-273.  [Bait et al., 1994] Bait, W., Koch, C., Newsome, W., and Britten, K. (1994). Power  spectrum analysis of MT neurons in the behaving monkey. J. Neurosci., in press.  [Bernander et al., 1994] Bernander, O., Koch, C., and Usher, M. (1994). The effect  of synchronized inputs at the single neuron level. Neural Computation, in press.  [Cowan, 1982] Cowan, J. D. (1982). Spontaneous symmetry breaking in large scale  nervous activity. Int. J. Quantum Chemistry, 22:1059-1082.  [Cox and Lewis, 1966] Cox, D. and Lewis, P. A. W. (1966). The Statistical Analysis  of Series of Events. Chapman and Hall, London.  [Cross and Hohenberg, 1993] Cross, M. C. and Hohenberg, P. C. (1993). Pattern  formation outside of equilibrium. Rev. Mod. Phys., 65(3):851-1112.  [Eckhorn et al., 1993] Eckhorn, R., Frien, A., Bauer, R., Woelbern, T., and Harald,  K. (1993). High frequency (60-90 hz) oscillations in primary visual cortex of awake  monkey. Neuroreport, 4:243-246.  636 Stemmler, Usher, Koch, and Olami  [Eeckman and Freeman, 1990] Eeckman, F. and Freeman, W. (1990). Correlations  between unit firing and EEG in the rat olfactory system. Brain Res., 528(2):238-  244.  [Gray et al., 1990] Gray, C. M., Engel, A. K., KSnig, P., and Singer, W. (1990).  Stimulus dependent neuronal oscillations in cat visual cortex: receptive field  properties and feature dependence. Europ. J. Neurosci., 2:607-619.  [Kreiter and Singer, 1992] Kreiter, A. K. and Singer, W. (1992). Oscillatory neu-  ronal responses in the visual cortex of the awake macaque monkey. Europ. J.  Neurosci., 4:369-375.  [Lowen and Teich, 1993] Lowen, S. B. and Teich, M. C. (1993). Fractal renewal  processes generate 1/f noise. Phys. Rev. E, 47(2):992-1001.  [Mandelbrot, 1983] Mandelbrot, B. B. (1983). The fractal geometry of nature. W.  H. Freeman, New York.  [Nelson et al., 1992] Nelson, J. I., Salin, P. A., Munk, M. H.-J., Arzi, M., and  Bullier, J. (1992). Spatial and temporal coherence in cortico-cortical connections:  A cross-correlation study in areas 17 and 18 in the cat. Visual Neuroscience,  9:21-38.  [Newell and Whitehead, 1969] Newell, A. C. and Whitehead, J. A. (1969). Finite  bandwidth, finite amplitude convection. J. Fluid Mech., 38:279-303.  [Softky and Koch, 1993] Softky, W. R. and Koch, C. (1993). The highly irregular  firing of cortical cells is inconsistent with temporal integration of random EPSPs.  J. Neurosci., 13(1):334-350.  [Teich, 1989] Teich, M. C. (1989). Fractal character of the auditory neural spike  train. IEEE Trans. Biotaed. Eng., 36(1):150-160.  [Usher et al., 1994] Usher, M., Stemmler, M., Koch, C., and Olami, Z. (1994). Net-  work amplification of local fluctuations causes high spike rate variability, fractal  firing patterns, and oscillatory local field potentials. Neural Computation, in  press.  PART V  CONTROL,  NAVIGATION, AND  PLANNING  
Computational Elements of the Adaptive  Controller of the Human Arm  Reza Shadmehr and Ferdinando A. Mussa-Ivaldi  Dept. of Brain and Cognitive Sciences  M. I. T., Cambridge, MA 02139  Eraall: reza@ai.mit.edu, sandro@ai.mit.edu  Abstract  We consider the problem of how the CNS learns to control dynam-  ics of a mechanical system. By using a paradigm where a subject's  hand interacts with a virtual mechanical environment, we show  that learning control is via composition of a model of the imposed  dynamics. Some properties of the computational elements with  which the CNS composes this model are inferred through the gen-  eralization capabilities of the subject outside the training data.  1 Introduction  At about the age of three months, children become interested in tactile exploration  of objects around them. They attempt to reach for an object, but often fail to  properly control their arm and end up missing their target. In the ensuing weeks,  they rapidly improve and soon they can not only reach accurately, they can also  pick up the object and place it. Intriguingly, during this period of learning they  tend to perform rapid, flailing-like movements of their arm, as if trying to "excite"  the plant that they wish to control in order to build a model of its dynamics.  From a control perspective, having a model of the arm's skeletal dynamics seems  necessary because of the relatively low gain of the fast acting feedback system  in the spinal neuro-muscular controllers (Crago et al. 1976), and the long delays in  transmission of sensory information to the supra-spinal centers. Such a model could  be used by the CNS to predict the muscular forces that need to be produced in order  to move the arm along a desired trajectory. Yet, this model by itself is not sufficient  1077  1078 Shadmehr and Mussa-Ivaldi  for performing a contact task because most objects which our hand interacts with  change the arm's dynamics significantly. We are left with a situation in which we  need to be able to quickly acquire a model of an object's dynamics so that we can  incorporate it in the control system for the arm. How we learn to construct a model  of a dynamical system and how our brains represent the composed model are the  subjects of this research.  2 Learning Dynamics of a Mechanical System  To make the idea behind learning dynamics evident, consider the example of con-  trolling a robotic arm. The arm may be seen as an inertially dominated mechanical  admitance, accepting force as input and producing a change in state as its output:  U(q) (F - C(q, 4)) (1)  where q is the configuration of the robot, H is the inertia tensor, F is the input  force from some controllable source (e.g., motors), and C is the coriolis/centripetal  forces. In learning to control the arm, i.e., having it follow a certain state trajectory  or reach a final state, we form a model which has as its input the desired change in  the state of the arm and receive from its output a quantity representing the force  that should be produced by the actuators. Therefore, what needs to be learned is  a map from state and desired changes in state to force:  3(q, 4, d) =/(q)d + (q, 4) (2)  Combine the above model with a simple PD feedback system,  and the dynamics of the system in Eq. (1) can now be written in terms of a new  variable s = q - qa, i.e., the error in the trajectory. It is easy to see that if we have    H and   C, and if K and B are positive definite, then s will be a decreasing  function of time, i.e., the system will be globally stable.  Learning dynamics means forming the map in Eq. (2). The computational elements  which we might use to do this may vary from simple memory cells that each have an  address in the state space (e.g., Albus 1975, Raibert & Wimberly 1984, Miller et al.  1987), to locally linear functions restricted to regions where we have data (Moore  & Atkeson 1994), to sigmoids (Gomi & Kawato 1990) and radial basis functions  which can broadly encode the state space (Botros & Atkeson 1991). Clearly, the  choice that we make in our computational elements will affect how the learned map  will generalize its behavior to regions of the state space outside of the training data.  Furthermore, since the task is to learn dynamics of a mechanical system (as opposed  to, for example, dynamics of a financial market), certain properties of mechanical  systems can be used to guide us in our choice for the computational elements. For  example, the map from states to forces for any mechanical system can be linearly  parameterized in terms of its mass properties (Stotine and Li, 1991). In an inertially  dominated system (like a multi-joint arm) these masses may be unknown, but the  fact that the dynamics can be linearized in terms of the unknowns makes the task  of learning control much simpler and orders of magnitude faster than using, for  example, an unstructured memory based approach.  Computational Elements of the Adaptive Controller of the Human Arm 1079  /  /  O.lm[  .... ,.-,., : .... / '., :, ,...""' , , , .f"' .,. . .. ...:. '.,  I'  B 1. see  2.5  'o ' :'": }!  "':,,9>/'":..,,...,5.'[.,/,.,.,-::,. -:...:.:.; ....   fi:....:>..,,.:  I' 0  m 2 4 6 8 10  1. sec Time (sec)  Figure 1: Dynamics of a real 2 DOF robot was learned so to produce a desired trajectory.  A: Schematic of the robot. The desired trajectory is the quarter circle. Performance of a  PD controller is shown by the gray line, as well as in B, where joint trajectories are drawn:  the upper trace is the shoulder joint and the lower trace is the elbow joint. Desired joint  trajectory is solid line, actual trajectory is the gray line. C: Performance when the PD  controller is coupled with an adaptive model. D: Error in trajectory. Solid line is PD,  Gray line is PD+adaptation.  To illustrate this point, consider the task of learning to control a real robot arm.  Starting with the assumption that the plant has 2 degrees of freedom with rota-  tional joints, inertial dynamics of Eq. (2) can be written as a product of a known  matrix-function of state-dependent geometric transformations Y, and an unknown  (but constant) vector a, representing the masses, center of masses, and link lengths:  1)(q,,a) -- Y(q,,a) . The matrix Y serves the function of referring the un-  known masses to their center of rotation and is a geometric transformation which  can be derived from our assumption regarding the structure of the robot. It is  these geometric transformations that can guide us in choosing the computational  elements for encoding the sensory data (q and ).  We used this approach to learn to control a real robot. The adaptation law was  derived from a Lyapunov criterion, as shown by Slotine and Li (1991):   _- _yr (q, , ) ( _ Oa(t) + q - qa(t))  The system converged to a very low trajectory tracking error within only three pe-  riods of the movement (Fig. 1). This performance is achieved despite the fact that  our model of dynamics ignores frictional forces, noise and delay in the sensors, and  dynamics of the actuators. In contrast, using a sigmoid function as the basic corn-  1080 Shadmehr and Mussa-Ivaldi  putational element of the map and training via back-propagation led to comparable  levels of performance in over 4000 repetitions of the training data (Shadmehr 1990).  The difference in performance of these two approaches was strictly due to the choice  of the computational elements with which the map of Eq. (2) was formed.  Now consider the task of a child learning dynamics of his arm, or that of an adult  picking up a hammer and pounding a nail. We can scarcely afford thousands of  practice trials before we have built an adequate model of dynamics. Our proposal  is that because dynamics of mechanical systems are distinctly structured, perhaps  our brains also use computational elements that are particularly suited for learning  dynamics of a motor task (as we did in learning to control the robot in Fig. 1). How  to determine the structure of these elements is the subject of the following sections.  3 A Virtual Mechanical Environment  To understand how humans represent learned dynamics of a motor task, we designed  a paradigm where subjects reached to a target while their hand interacted with a  virtual mechanical environment. This environment was a force field produced by a  manipulandum whose end-effector was grasped by the subject. The field of forces  depended only on the velocity of the hand, e.g., F = B, as shown in Fig. 2A,  and significantly changed the dynamics of the limb: When the robot's motors were  turned off (null field condition), movements were smooth, straight line trajectories  to the target (Fig. 2B). When coupled with the field however, the hand's trajectory  was now significantly skewed from the straight line path (Fig. 2C).  It has been suggested that in making a reaching movement, the brain formulates  a kinematic plan describing a straight hand path along a smooth trajectory to the  target (Morasso 1981). Initially we asked whether this plan was independent of the  dynamics of the moving limb. If so, as the subject practiced in the environment,  the hand path should converge to the straight line, smooth trajectory observed in  the null field. Indeed, with practice, trajectories in the force field did converge to  those in the null field. This was quantified by a measure of correlation which for all  eight subjects increased monotonically with practice time.  If the CNS adapted to the force field by composing a model of its dynamics, then  removal of the field at the onset of movement (un-be-known to the subject) should  lead to discrepancies between the actual field and the one predicted by the subject's  model, resulting in distorted trajectories which we call after-effects. The expected  dynamics of these after-effects can be predicted by a simple model of the upper  arm (Shadmehr and Mussa-Ivaldi 1994). Since the after-effects are a by-product  of the learning process, we expected that as subjects adapted to the field, their  performance in the null field would gradually degrade. We observed this gradual  growth of the after-effects, leading to grossly distorted trajectories in the null field  after subjects had adapted to the force field (Fig. 2D). This evidence suggested that  the CNS composed a model of the field and used this model to compensate for the  forces which it predicted the hand would encounter during a movement.  The information contained in the learned model is a map whose input is the state  and the desired change in state of the limb, and whose output is force (Eq. 2). How  is this map implemented by the CNS? Let us assume that the approximation is via  Computational Elements of the Adaptive Controller of the Human Arm 1081  0.5  -r -0.5  -1  A -1 -0.5 0 0.5 I B  Hand x-velocy (m/s)  150  50 "'."...  0  ........ ",a;r,, ' .... ..:.;,,::'..97!.:'..F.-  -50 [  - 150  -100 -50 0 50 1 O0 150  Displacement (mm)  Figure 2: A: The virtual mechanical environment as a force field. B: Trajectory of reach-  ing movements (center-out) to 8 targets in a null field. C: Average+standard-deviation  of reaches to same targets when the field was on, before adaptation. D: After-affects of  adaptation, i.e., when moving in a null field but expecting the field.  a distributed set of computational elements (Poggio 1990). What are the properties  of these elements.'? An important property may be the spatial bandwidth, i.e., the  size of the receptive field in the input space (the portion of the input space where  the element generates a significant output). This property greatly influences how  the CNS might interpolate between states which it has visited during training, and  whether it can generalize to regions beyond the boundary of the training data.  Pot example, in eye movements, it has been suggested that a model of dynamics of  the eye is stored in the cerebellum (Shidara et al. 1992). Cells which encode this  model (Purkinje cells) vary their firing rate as a linear function of the state of the  eye, and the sum of their outputs (firing rates) correlates well with the force that the  muscles need to produce to move the eye. Therefore, the model of eye's dynamics  is encoded via cells with very large receptive fields. On the other hand, cells which  take part in learning a visual hyperacuity task may have very small receptive fields  (Poggio et al. 1992), resulting in a situation where training in a localized region  does not lead to generalization.  In learning control of our limbs, one possibility for the computational elements is the  neural control circuits in the spinal cord (Mussa-Ivaldi 1992). Upon activation of  1082 Shadmehr and Mussa-Ivaldi  Test workspace Workspace  ned  A  x  150  IO0  5O  o  -50  - 1 O0  -150  0.5  B -100 -50 0 50 100 150 C -1 -0.5 0 0.5 1  Displacement (mm) Hand x-velocity (m/s)  Figure 3: A: Schematic of subject's arm and the trained region of the workspace where  the force field was presented and the "test" region where the transferred effects were  measured. B: After-effects at the test region. (2: A joint-based translation of the force  field shown in Fig. 2A to the novel workspace. This is the field that the subject expected  at the test region.  one such circuit, muscles produce a time varying force field, i.e., forces which depend  on the state of the limb (position and velocity) and time (Mussa-Ivaldi et al. 1990).  Let us call the force function produced by one such motor element fi(q, ,t). It  turns out that as one changes the amount of activation to a motor element, the  output forces essentially scale. When two such motor elements are activated, the  resulting force field is a linear combination of the two individual fields (Bizzi et al.  2  1991): f = -i= cifi(q,,t).  Now consider the task of learning to move in the field shown in Fig. 2A. The  model that the CNS builds is a map from state of the limb to forces imposed by the  environment. Following the above scenario, the task is to find coefficients ci for each  element such that the output field is a good approximation of the enviromnental  field. Unlike the computational elements of a visual task however, we may postulate  that the motor elements are characterized by their broad receptive fields. This is  because muscular force changes gradually as a function of the state of the limb  and therefore its output force is non zero for wide region of the state space. It  follows that if learning dynamics was accomplished through formation of a map  whose computational elements were these motor functions, then because of the large  spatial bandwidth of the elements the composed model should be able to generalize  to well beyond the region of the training data.  Computational Elements of the Adaptive Controller of the Human Arm 1083  To test this, we limited the region of the input space for which training data was  provided and quantified the subject's ability to generalize to a region outside the  training set. Specifically, we limited the workspace where practice movements in the  force field took place and asked whether localexposure to the field led to after-effects  in other regions (Fig. 3A). We found that local training resulted in after-effects in  parts of the workspace where no exposure to the field had taken place (Fig. 3B). This  indicated that the model composed by the CNS predicted specific forces well outside  the region in which it had been trained. The existence of this generalization showed  that the computational elements with which the internal model was implemented  had broad receptive fields.  The transferred after-effects (Fig. 3B) show that at the novel region of the  workspace, the subject's model of the environment predicted very different forces  than the one on which the subject had been trained on (compare with Fig. 2D).  This rejected the hypothesis that the composed model was a simple mapping (i.e.,  translation invariant) in a hand-based coordinate system, i.e., from states of the  arm to forces on the hand. The alternate hypothesis was that the composed model  related observed states of the arm to forces that needed to be produced by the  muscles and was translation invariant in a coordinate system based on the joints  and muscles. This would be the case, for example, if the computational elements  encoded the state of the arm linearly (analogous to Purkinje cells for the case of  eye movements) in joint space.  To test this idea, we translated the field in which the subject had practiced to the  novel region in a coordinate system defined based on the joint space of the subject's  arm, resulting in the field shown in Fig. 3C. We recorded the performance of the  subjects in this new field at the novel region of the workspace (after they had been  trained on field of Fig. 2A) and found that performance was near optimum at the  first exposure. This indicated that the geometric structure of the composed model  supported transfer of information in an intrinsic, e.g., joint based, coordinate sys-  tem. This result is consistent with the hypothesis that the computational elements  involved in this learning task broadly encode the state space and represent their  input in a joint-based coordinate system and not a hand-based one.  4 Conclusions  In learning control of an inertially dominated mechanical system, knowledge of the  system's geometric constraints can direct us to choose our computational elements  such that learning is significantly faciliated. This was illustrated by an example  of a real robot arm: starting with no knowledge of its dynamics, a reasonable  model was learned within 3 periods of a movements (as opposed to thousands of  movements when the computational elements were chosen without regard to the  geometric properties). We argued that in learning to control the human arm, the  CNS might also make assumption regarding geometric properties of its links and  use specialized computational elements which facilitate learning of dynamics.  One possibility for these elements are the discrete neuronal circuits found in the  spinal cord. The function of these circuits can be mathematically formulated such  that a map representing inverse dynamics of the arm is formed via a combination  of the elements. Because these computational elements encode their input space  1084 Shadmehr and Mussa-Ivaldi  broadly, i.e., has significant output for a wide region of the input space, we expected  that if subjects learned a dynamical process from localized training data, then the  formed model should generalize to novel regions of the state space. Indeed we found  that the subjects transferred the training information to novel regions of the state  space, and this transfer took place in a coordinate system similar to that of the  joints and muscles. We therefore suggest that the CNS learns control of the arm  through formation of a model whose computational elements broadly encode the  state space, and that these elements may be neuronal circuits of the spinal cord.  Acknowledgments: Financial support was provided in part by the NIH (AR26710) and  the ONR (N00014/90/J/1946). R. S. was supported by the McDonnell-Pew Center for  Cognitive Neurosciences and the Center for Biological and Computational Learning.  References  Albus JS (1975) A new approach to manipulator control: The cerebellar model articulation  controller (CMAC). Trans ASME J Dyn Syst Meas Contr 97:220-227.  Bizzi E, Mussa-Ivaldi FA, Giszter SF (1991) Computations underlying the execution of  movement: a novel biological perspective. Science 253:287-291.  Botros SM, Atkeson CG (1991) Generalization properties of radial basis functions. In:  Lippmann et al., Adv. in Neural Informational Processing Systems 3:707-713.  Crago, Houk JC, Hasan Z (1976) Regulatory actions of human stretch reflex. J Neuro-  physiol 39:5-19.  Gomi H, Kawato M (1990) Learning control for a closed loop system using feedback error  learning. Proc IEEE Conf Decision Contr.  Miller WT, Glanz FH, Kraft LG (1987) Application of a general learning algorithm to the  control of robotic manipulators. Int J Robotics Res 6(2):84-98.  Moore AW, Atkeson CG (1994) An investigation of memory-based function approximators  for learning control. Machine Learning, submitted.  Mussa-Ivaldi FA, Giszter SF (1992) Vector field approximation: a computational paradigm  for motor control and learning. Biol Cybern 67:491-500.  Mussa-Ivaldi FA, Giszter SF, Bizzi E (1990) Motor-space coding in the central nervous  system. Cold Spring Harbor Syrup Quant Biol 55:827-835.  Poggio T (1990) A theory of how the brain might work. Cold Spring Harbor Syrup Quant  Biol 55:899-910.  Poggio T, FaMe M, Edelman S (1992) Fast perceptual learning in visual hyperacuity.  Science 256:1018-1021.  Raibert MH, Wimberly FC (1984) Tabular control of balance in a dynamic legged system.  IEEE Trans Systems, Man, Cybernetics SMC-14(2):334-339.  Shadmehr R (1990) Learning virtual equilibrium trajectories for control of a robot arm.  Neural Computation 2:436-446.  Shadmehr R, Mussa-Ivaldi FA (1994) Adaptive representation of dynamics during learning  of a motor task. J Neuroscience, in press.  Shidara M, Kawano K, Gomi H, Kawato M (1993) Inverse-dynamics model eye movement  control by Purkinje cells in the cerebellum. Nature 365:50-52. Slotine JJE, Li W (1991)  Applied Nonlinear Control, Prentice Hall, Englewood Cliffs, New Jersey.  
Feature Densities are Required for  Computing Feature Correspondences  Subutai Ahmad  Interval Research Corporation  1801-C Page Mill Road, Palo Alto, CA 94304  E-mail: ahmadnterval. com  Abstract  The feature correspondence problem is a classic hurdle in visual  object-recognition concerned with determining the correct mapping  between the features measured from the image and the features ex-  pected by the model. In this paper we show that determining good  correspondences requires information about the joint probability  density over the image features. We propose "likelihood based  correspondence matching" as a general principle for selecting op-  timal correspondences. The approach is applicable to non-rigid  models, allows nonlinear perspective transformations, and can op-  timally deal with occlusions and missing features. Experiments  with rigid and non-rigid 3D hand gesture recognition support the  theory. The likelihood based techniques show almost no decrease  in classification performance when compared to performance with  perfect correspondence knowledge.  I INTRODUCTION  The ability to deal with missing information is crucial in model-based vision sys-  tems. The feature correspondence problem is an example where the correct map-  ping between image features and model features is unknown at recognition time.  For example, imagine a network trained to map fingertip locations to hand gestures.  Given features extracted from an image, it becomes important to determine which  features correspond to the thumb, to the index finger, etc. so we know which input  units to clamp with which numbers. Success at the correspondence matching step  961  962 Ahmad  x 2  Class boundary  I Pl I  Class 1  Class 2   p2  o I  Xl  Figure 1: An example 2D feature space. Shaded regions denote high probabil-  ity. Given measured values of 0.2 and 0.9, the points p and P2 denote possible  instantiations but p is much more likely.  is vital for correct classification. There has been much previous work on this topic  (Connell and Brady 1987; Segen 1989; Huttenlocher and Ullman 1990; Pope and  Lowe 1993) but a general solution has eluded the vision community. In this paper  we propose a novel approach based on maxinfizing the probability of a set of mod-  els generating the given data. We show that neural networks trained to estimate  the joint density between image features can be successfully used to recover the  optimal correspondence. Unlike other techniques, the likelihood based approach is  applicable to non-rigid models, allows perspective 3D transformations, and includes  a principled method for dealing with occlusions and missing features.  1.1 A SIMPLE EXAMPLE  Consider the idealized example depicted in Figure 1. The distribution of features is  highly non-uniform (this is typical of non-rigid objects). The classification boundary  is in general completely unrelated to the feature distribution. In this case, the  class (posterior) probability approaches 1 as feature x approaches 0, and 0 as it  approaches 1. Now suppose that two feature values 0.2 and 0.9 are measured from  an image. The task is to decide which value gets assigned to x and which value  gets assigned to x2. A common strategy is to select the correspondence which gives  the maximal network output (i.e. mammal posterior probability). In this example  (and in general) such a strategy will pick point p2, the wrong correspondence. This  is because the classifer output represents the probability of a class given a specific  feature assignment and specific values. The correspondence problem however, is  something completely different: it deals with the probability of getting the feature  assignments and values in the first place.  Feature Densities Are Required for Computing Feature Correspondences 963  2  LIKELIHOOD BASED CORRESPONDENCE  MATCHING  We can formalize the intuitive arguments in the previous section. Let C denote the  set of classes under consideration. Let X denote the list of features measured from  the image with correspondences unknown. Let A be the set of assignments of the  measured values to the model features. Each assignment a 6 A reflects a particular  choice of feature correspondences. ,Ve consider two different problems: the task of  choosing the best assignment a and the task of classifying the object given X.  Selecting the best correspondence is equivalent to selecting the permutation that  maximizes p(a[X, C). This can be re-written as:  p(alX, C) = p(X]a, O)p(alC)  p(XlC)  (1)  p(XIC ) is a normalization factor that is constant across all a and can be ignored.  Let xa denote a specific feature vector constructed by applying permutation a to  X. Then (1) is equivalent to maximizing:  p(alX, C) = P(xalC)p(alC)  (2)  p(alC) denotes our prior knowledge about possible correspondences. (For example  the knowledge that edge features cannot be matched to color features.) When no  prior knowledge is available this term is constant. We denote the assignment that  maximizes (2) the maximum likelihood correspondence match. Such a correspon-  dence maximizes the probability that a set of visual models generated a given set  of image features and will be the optimal correspondence in a Bayesian sense.  2.1 CLASSIFICATION  In addition to computing correspondences, we would like to classify a model from  the measured image features, i.e. compute p(CilX, C). The maximal-output based  solution is equivalent to selecting the class Ci that maximizes p(Cilxa, C) over all  assignments a and all classes Ci. It is easy to see that the optimal strategy is actually  to compute the following weighted estimate over all candidate assignments:  p(Ci[X,C) = Eap(OilX, a, C)p(Xla, C)p(alC)  p(XlC)  Classification based on (3) is equivalent to selecting the class that maximizes:  (3)  p(Clx, C)p(xlC)p(alC)  a  (4)  Note that the network output based solution represents quite a degraded estimate  of (4). It does not consider the input density nor perform a weighting over possible  964 Ahmad  correspondences. A reasonable approximation is to select the maximum likelihood  correspondence according to (2) and then use this feature vector in the classification  network. This is suboptimal since the weighting is not done but in our experience  it yields results that are very close to those obtained with (4).  3  COMPUTING CORRESPONDENCES WITH GBF  NETWORKS  In order to compute (2) and (4)we consider networks of normalized Gaussian basis  functions (GBF networks). The i'th output unit is computed as:  with:  y(x) = Ej wjb(x)  b(x) (5)  . rj exp[-  (xi- lUji) 2  b/(x) = rjn(x;/uj,j)= (2r) d/2 d 2o?i ]  1'-i k = 10'kj i  (6)  Here each basis function j is characterized by a mean vector pj and by crj, a vector  representing the diagonal covm'iance matrix. wji represents the weight from the  j'th Gaussian to the i'th output. rj is a weight attached to each basis function.  Such networks have been popular recently and have proven to be useful in a num-  ber of applications (e.g. (Roscheisen et al. 1992; Poggio and Edelman 1990).  For our current purpose, these networks have a number of advantages. Under  certain training regimes such as EM or "soft clustering" (Dempster et al. 1977;  Nowlan 1990) or an approximation such as K-Means (Neal and Hinton 1993),  the basis functions adapt to represent local probability densities. In particu-  lar p(xa[C)  Y',jbj(xa). If standard error gradient training is used to set the  weights wij then yi(Xa) , p(Ci[xa, C) Thus both (2) and (4) can be easilty com-  puted.(Ahmad and Tresp 1993) showed that such networks can effectively learn  feature density information for complex visual problems. (Poggio and Edelman  1990) have also shown that similar netvorks (with a different training regime) can  learn to approximate the complex mappings that arise in 3D recognition.  3.1  OPTIMAL CORRESPONDENCE MATCHING WITH  OCCLUSION  An additional advantage of GBF networks trained in this way is that it is possible  to obtain closed form solutions to the optimal classifier in the presence of missing  or noisy features. It is also possible to correctly compute the probability of feature  vectors containing missing dimensions. The solution consists of projecting each  Gaussian onto the non-missing dimensions and evaluating the resulting network.  Note that it is incorrect to simply substitute zero or any other single value for the  missing dimensions. (For lack of space we refer the reader to (Ahmad and Tresp  Feature Densities Are Required for Computing Feature Correspondences 965  'Your"  "rhumbsp" "pointing"  Figure 2: Classifiers were trained to recognize these 7 gestures. a 3D computer  model of the hand is used to generate images of the hand in various poses. For  each training example, we randomly choose a 3D orientation and depth, compute  the 3D positions of the fingertips and project them onto 2D. There were 5 features  yielding a 10D input space.  1993) for further details.) Thus likelihood based approaches using GBF networks  can simultaneously optimally deal with occlusions and the correspondence problem.  4 EXPERIMENTAL RESULTS  We have used the task of 3D gesture recognition to compare likelihood based meth-  ods to the network output based technique. (Figure 2 describes the task.) We  considered both rigid and non-rigid gesture recognition tasks. We used a GBF  network with 10 inputs, 1050 basis functions and 7 output units. For comparision  we also trained a standard backpropagation network (BP) with 60 hidden units on  the task. For this task we assume that during training all feature correspondences  are known and that during training no feature values are noisy or missing. For this  task we assume that during training all feature correspondences are known and that  during training no feature values are noisy or missing. Classification performance  with full correspondence information on an independent test set is about 92% for  the GBF network and 93% for the BP network. (For other results see (Williams  et al. 1993) who have also used the rigid version of this task as a benchmark.)  4.1 EXPERIMENTS WITH RIGID HAND POSES  Table 1 plots the ability of the various methods to select the correct correspon-  dence. Random patterns were selected from the test set and all 5! = 120 possible  combinations were tried. MLCM denotes the percentage of times the maximum  likelihood method (equation (2)) selected the correct feature correspondence. GBF-  M and BP-M denotes how often the maximal output method chooses the correct  correspondence using GBF nets and BP. "Random" denotes the percentage if cor-  respondences are chosen randomly. The substantially better performance of MLCM  suggests that, at least for this task, density information is crucial. It is also interest-  ing to examine the errors made by MLCM. A common error is to switch the features  for the pinky and the adjacent finger for gestures "one", "two", "thumbs-up" and  "pointing". These two fingertips often project very close to one another in many  poses; such a mistake usually do not affect subsequent classification.  966 Ahmad  Selection Method Percentage Correct  Random 1.2%  GBF-M 8.8%  BP-M 10.3%  MLCM 62.0%  Table 1: Percentage of correspondences selected correctly.  Classifier Classification Performance  BP-Random 28.0%  BP-Max 39.2%  GBF-Max 47.3%  GBF-WLC 86.2%  GBF-Known 91.8%  Table 2: Classification without correspondence information.  Table 2 shows classification performance when the correspondence is unknown.  GBF-WLC denotes weighted likelihood classification using GBF networks to com-  pute the feature densities and the posterior probabilities. Performance with the  output based techniques are denoted with GBF-M and BP-M. BP-R denotes per-  formance with random correspondences using the back propagation network. GBF-  known plots the performance of the GBF network when all correspondences are  known. The results are quite encouraging in that performance is only slightly de-  graded with WLC even though there is substantially less information present when  correspondences are unknown. Although not shown, results with MLCM (i.e. not  doing the weighting step but just choosing the correspondence with highest prob-  ability) are about 1% less than WLC. This supports the theory that many of the  errors of MLCM in Table i are inconsequential.  4.1.1 Missing Features and No Correspondences  Figure 3 shows error as a function of the number of missing dimensions. (The  missing dimensions were randomly selected from the test set.) Figure 3 plots the  average number of classes that are assigned higher probability than the correct class.  The network output method and weighted likelihood classification is compared to  the case where all correspondences are known. In all cases the basis functions  were projected onto the non-missing dimensions to approximate the Bayes-optimal  condition. As before, the likelihood based method outperforms the output based  method. Surprisingly, even with 4 of the 10 dimensions missing and with correspon-  dences unknown, WLC assigns highest probability to the correct class on average  (performance score < 1.0).  Feature Densities Are Required for Computing Feature Correspondences 967  Error  3.5  3  2.5  2  1.5  1  0.5  0  Error vs missing features without correspondence  GBF-M -O--  - WLC O  GBF-Known t  I I I  0 1 2 3 4 5  No. of missing features  Figure 3: Error with missing features when no correspondence information is  present. The y-axis denotes the average number of classes that are assigned higher  probability than the correct class.  4.2 EXPERIMENTS WITH NON-RIGID HAND POSES  In the previous experiments the hand configuration for each gesture remained rigid.  Correspondence selection with non-rigid gestures was also tried out. As before a  training set consisting of examples of each gesture was constructed. However, in  this case, for each sample, a random perturbation (within 20 degrees) was added to  each finger joint. The orientation of each sample was allowed to randomly vary by  45 degrees around the x, y, and z axes. When viewed on a screen the samples give  the appearance of a hand wiggling around. Surprisingly, GBF networks with 210  hidden units consistently selected the correct correspondences with a performance  of 94.9%. (The performance is actually better than the rigid case. This is because  in this training set all possible 3D orientations were not allowed.)  5 DISCUSSION  We have shown that estimates of joint feature densities can be used to successfully  deal with lack of correspondence information even when some input features are  missing. We have dealt mainly with the rather severe case where no prior informa-  tion about correspondences is available. In this particular case to get the optimal  correspondece, all n! possibilities must be considered. However this is usually not  necessary. Useful techniques exist for reducing the number of possible correspon-  dences. For example, (Huttenlocher and Ullman 1990) have argued that three fea-  968 Ahmad  ture correspondences are enough to constrah the pose of rigid objects. In this case  only O(n 3) lnatdxes need to be tested. hx addition features usually fall into incoln-  patible sets (e.g. edge features, corner features, etc.) further reducing the iratuber  of potential matdes. Finally, with hnage sequences one can use correspondence  information froIn the previous frame to constrahx the set of correspondences in the  current frame. 'Vhatever the situation, a likelihood based approach is a principled  method for evaluating the set of available lnatdxes.  Acknowledgements  Mud of this researd was conducted at Siemens Central Researd in Munid, Ger-  lnany. I would like to thank Volker Tresp at Siemens for lnany interesting discussions  and Brigitte Wirtz for providing the hand model.  References  Ahmad, S. and V. Tresp (1993). Sonhe solutions to the missing feature problem  h vision. In S. Hanson, J. Cowan, and C. Giles (Eds.), Advances i'n Neural  Info.rmatio.n Processing Systems 5, pp. 393 400. lIorgan Kaufinann Publish-  ers,  Coxmell, J. and 1I. Brady (1987). Generating and generalizing models of visual  objects. Artificial Intelligence 31, 159 183.  Delnpster, A., N. Laird, and D. Rubin (1977). 2vlaximmn-likelihood from hxcom-  plete data via the E.II algorithm. J. Royal Statistical Soc. Set. B 39, 1 38.  Huttexfiodxer; D. and S. Ulhnan (1990). Recognizing solid objects by aliglnnent  with an hnage. I.r, ternational Journal of Computer Vision 5(2), 195 212.  Neal, R. and G. Hinton (1993). A new view of the EII algorithln that justifies  increlnental and other variants. Biometrika, submitted.  Nowlan, S. (1990). lIaximmn likelihood competitive learnhg. h D. Touretzky  (Ed.), Advances i.r, Neural Information Processing Systems 2, pp. 574 582.  San lIateo, CA: lIorgan Kaufmmm Publishers.  Poggio, T. and S. Edehnan (1990). A network that learns to recognize three-  dilnensional objects. Nature 33 (6225), i 3.  Pope, A. and D. Lowe (1993, May). Learnhg object recognition models from im-  ages. In Fourth Internatio.ral Confererce o' Computer Vision, Berlin. IEEE  Computer Society Press.  Rosdxeisen, 1I., R. Hofinann, and V. Tresp (1992). Neural control for rolling  mills: Incorporating donrain theories to overcome data deficiency. In IYL J.,  H. S.J.; and L. R. (Eds.), Advances in Neu.ral tnformatio.n Processing Systems  . lIorgan Kaufinan.  Segen, J. (1989). lIodel learning and recogfition of not,rigid objects. In IEEE  Computer Society Conference on Computer Vision and Pattern Recog.nition,  San Diego, CA.  Williams; C. K.. R. S. Zemel, and 1I. C. lIozer (1993). Unsupervised learning  of object models. la AAAI Fall 1993 Symposium o.n Machi.he Lea.rni.ng i.n  Computer Visio.n, pp. 20 24. Proceedings available as AAAI Tech Report  FSS-93-04.  
Surface Learning with Applications to  Lipreading  Christoph Bregler  *Computer Science Division  University of California  Berkeley, CA 94720  Stephen M. Omohundro **  *Int. Computer Science Institute  1947 Center Street Suite 600  Berkeley, CA 94704  Abstract  Most connectionist research has focused on learning mappings from  one space to another (eg. classification and regression). This paper  introduces the more general task of learning constraint surfaces.  It describes a simple but powerful architecture for learning and  manipulating nonlinear surfaces from data. We demonstrate the  technique on low dimensional synthetic surfaces and compare it to  nearest neighbor approaches. We then show its utility in learning  the space of lip images in a system for improving speech recognition  by lip reading. This learned surface is used to improve the visual  tracking performance during recognition.  I Surface Learning  Mappings are an appropriate representation for systems whose variables naturally  decompose into "inputs" and "outputs". To use a learned mapping, the input vari-  ables must be known and error-free and a single output value must be estimated for  each input. Many tasks in vision, robotics, and control must maintain relationships  between variables which don't naturally decompose in this way. Instead, there is  a nonlinear constraint surface on which the values of the variables are jointly re-  stricted to lie. We propose a representation for such surfaces which supports a wide  range of queries and which can be naturally learned from data.  The simplest queries are "completion queries". In these queries, the values of certain  variables are specified and the values (or constraints on the values) of remaining  44 Bregler and Omohundro  Figure 1: Using a constraint surface to reduce uncertainty in two variables  Figure 2: Finding the closest point in a surface to a given point.  variables are to be determined. This reduces to a conventional mapping query if the  "input" variables are specified and the system reports the values of corresponding  "output" variables. Such queries can also be used to invert mappings, however, by  specifying the "output" variables in the query. Figure i shows a generalization in  which the variables are known to lie with certain ranges and the constraint surface  is used to further restrict these ranges.  For recognition tasks, "nearest point" queries in which the system must return the  surface point which is closest to a specified sample point are important (Figure  2). For example, symmetry-invariant classification can be performed by taking the  surface to be generated by applying all symmetry operations to class prototypes (eg.  translations, rotations, and scalings of exemplar characters in an OCR system). In  our representation we are able to efficiently find the globally nearest surface point  in this kind of query.  Other important classes of queries are "interpolation queries" and "prediction  queries". For these, two or more points on a curve are specified and the goal is to in-  terpolate between them or extrapolate beyond them. Knowledge of the constraint  surface can dramatically improve performance over "knowledge-free" approaches  like linear or spline interpolation.  In addition to supporting these and other queries, one would like a representation  which can be efficiently learned. The training data is a set of points randomly  drawn from the surface. The system should generalize from these training points  to form a representation of the surface (Figure 3). This task is more difficult than  mapping learning for several reasons: 1) The system must discover the dimension of  the surface, 2) The surface may be topologically complex (eg. a torus or a sphere)  Surface Learning with Applications to Lipreading 45    -    *   O0 0  -  I I I I I I I I I I J  Figure 3: Surface Learning  and may not support a single set of coordinates, 3) The broader range of queries  discussed above must be supported.  Our approach starts from the observation that if the data points were drawn from  a linear surface, then a principle components analysis could be used to discover the  dimension of the linear space and to find the best-fit linear space of that dimension.  The largest principle vectors would span the space and there would be a precipitous  drop in the principle values at the dimension of the surface. A principle components  analysis will no longer work, however, when the surface is nonlinear because even  a 1-dimensional curve could be embedded so as to span all the dimensions of the  space.  If a nonlinear surface is smooth, however, then each local piece looks more and  more linear under magnification. If we consider only those data points which lie  within a local region, then to a good approximation they come from a linear surface  patch. The principle values can be used to determine the most likely dimension of  the surface and that number of the largest principle components span its tangent  space (Omohundro, 1988). The key idea behind our representations is to "glue"  these local patches together using a partition of unity.  We are exploring several implementations, but all the results reported here come  from a represenation based on the "nearest point" query. The surface is repre-  sented as a mapping from the embedding space to itself which takes each point  to the nearest surface point. K-means clustering is used to determine a initial set  of "prototype centers" from the data points. A principle components analysis is  performed on a specified number of the nearest neighbors of each prototype. These  "local PCA" results are used to estimate the dimension of the surface and to find  the best linear projection in the neighborhood of prototype i. The influence of these  local models is determined by Gaussians centered on the prototype location with a  variance determined by the local sample density. The projection onto the surface  is determined by forming a partition of unity from these Gaussians and using it to  form a convex linear combination of the local linear projections:  ?(x) = El  (1)  This initial model is then refined to minimize the mean squared error between the  46 Bregler and Omohundro  Figure 4: Learning a i-dimensional surface. a) The surface to learn b) The local  patches and the range of their influence functions, c) The learned surface  training samples and the nearest surface point using EM optimization and gradient  descent.  2 Synthetic Examples  To see how this approach works, consider 200 samples drawn from a i-dimensional  curve in a two-dimensionM space (Figure 4a). 16 prototype centers are chosen by k-  means clustering. At each center, a local principle components analysis is performed  on the closest 20 training samples. Figure 4b shows the prototype centers and the  two local principle components as straight lines. In this case, the larger principle  value is several times larger than the smaller one. The system therefore attempts  to construct a one-dimensional learned surface. The circles in Figure 4b show the  extent of the Gaussian influence functions for each prototype. Figure 4c shows the  resulting learned suface. It was generated by randomly selecting 2000 points in the  neighborhood of the surface and projecting them according to the learned model.  Figure 5 shows the same process applied to learning a two-dimensional surface  embedded in three dimensions.  To quantify the performance of this learning algorithm, we studied the effect of the  different parameters on learning a two-dimensional sphere in three dimensions. It  is easy to compare the learned results with the correct ones in this case. Figure 6a  shows how the empirical error in the nearest point query decreases as a function  of the number of training samples. We compare it against the error made by a  nearest-neighbor algorithm. With 50 training samples our approach produces an  error which is one-fourth as large. Figure 6b shows how the average size of the local  principle values depends on the number of nearest neighbors included. Because  this is a two-dimensional surface, the two largest values are well-separated from the  third largest. The rate of growth of the principle values is useful for determining  the dimension of the surface in the presence of noise.  Surface Leaming with Applications to Lipreading 47  Figure 5: Learning a two-dimensional surface in the three dimensions a) 1000 ran-  dom samples on the surface b) The two largest local principle components at each  of 100 prototype centers based on 25 nearest neighbors.  Figure 6: Quantitative performance on learning a two-dimensional sphere in three  dimensions. a) Mean squared error of closest point querries as function of the num-  ber of samples for the learned surface vs. nearest training point b) The mean square  root of the three principle values as a function of number of neighbors included in  each local PCA.  48 Bregler and Omohundro  a b  Figure 7: Snakes for finding the lip contours a) A correctly placed snake b) A snake  which has gotten stuck in a local minimum of the simple energy function.  3 Modelling the space of lips  We are using this technique as a part of system to do "lipreading". To provide  features for "viseme classification" (visemes are the visual analog of phonemes), we  would like the system to reliably track the shape of a speaker's lips in video images.  It should be able to identify the corners of the lips and to estimate the bounding  curves robustly under a variety of imaging and lighting conditions. Two approaches  to this kind of tracking task are "snakes" (Kass, et. al, 1987) and "deformable  templates" (Yuille, 1991). Both of these approaches minimize an "energy function"  which is a sum of an internal model energy and an energy measuring the match to  external image features.  For example, to use the "snake" approach for lip tracking, we form the internal  energy from the first and second derivatives of the coordinates along the snake,  prefering smoother snakes to less smooth ones. The external energy is formed  from an estimate of the negative image gradient along the snake. Figure 7a shows  a snake which has correctly relaxed onto a lip contour. This energy function is  not very specific to lips, however. For example, the internal energy just causes  the snake to be a controlled continuity spline. The "lip- snakes" sometimes relax  onto undesirable local minima like that shown in Figure 7b. Models based on  deformable templates allow a researcher to more strongly constrain the shape space  (typically with hand-coded quadratic linking polynomials), but are difficult to use  for representing fine grain lip features.  Our approach is to use surface learning as described here to build a model of the  space of lips. We can then replace the internal energy described above by a quantity  computed from the distance to the learned surface in lip feature space.  Our training set consists of 4500 images of a speaker uttering random words 1.  The training images are initially "labeled" with the conventional snake algorithm.  Incorrectly aligned snakes are removed from the database by hand. The contour  shape is parameterized by the x and y coordinates of 40 evenly spaced points along  the snake. All values are normalized to give a lip width of 1. Each lip contour is  The data was collected for an earlier lipreading system described in (Bregler, Hild,  Manke, Waibel 1993)  Surface Learning with Applications to Lipreading 49   a  Figure 8: Two principle axes in a local patch in lip space. a, b, and c are configu-  rations along the first principle axis, while d, e, and f are along the third axis.  a b c  Figure 9: a) Initial crude estimate of the contour b) An intermediate step in the  relaxation c) The final contour.  therefore a point in an 80-dimensional "lip- space". The lip configurations which  actually occur lie on a lower dimensional surface embedded in this space. Our  experiments show that a 5-dimensional surface in the 80-dimensional lip space is  sufficient to describe the contours with single pixel accuracy in the image. Figure 8  shows some lip models along two of the principle axes in the local neighborhood of  one of the patches. The lip recognition system uses this learned surface to improve  the performance of tracking on new image sequences.  The tracking algorithm starts with a crude initial estimate of the lip position and  size. It chooses the closest model in the lip surface and maps the corresponding  resized contour back onto the estimated image position (Figure 9a). The external  image energy is taken to be the cumulative magnitude of graylevel gradient esti-  mates along the current contour. This term has maximum value when the curve  is aligned exactly on the lip boundary. We perform gradient ascent in the contour  space, but constrain the contour to lie in the learned lip surface. This is achieved by  reprojecting the contour onto the lip surface after each gradient step. The surface  thereby acts as the analog of the internal energy in the snake and deformable tem-  plate approaches. Figure 9b shows the result after a few steps and figure 9c shows  the final contour. The image gradient is estimated using an image filter whose width  is gradually reduced as the search proceeds.  The lip contours in successive images in the video sequence are found by starting  with the relaxed contour from the previous image and performing gradient ascent  50 Bregler and Omohundro  with the altered external image energies. Empirically, surface-based tracking is far  more robust than the "knowledge-free" approaches. While we have described the  approach in the context of contour finding, it is much more general and we are  currently extending the system to model more complex aspects of the image.  The full lipreading system which combines the described tracking algorithm and a  hybrid connectionist speech recognizer (MLP/HMM) is described in (Bregler and  Konig 1994). Additionally we will use the lip surface to interpolate visual features  to match them with the higher rate auditory features.  4 Conclusions  We have presented the task of learning surfaces from data and described several im-  portant queries that the learned surfaces should support: completion, nearest point,  interpolation, and prediction. We have described an algorithm which is capable of  efficiently performing these tasks and demonstrated it on both synthetic data and  on a real-world lip-tracking problem. The approach can be made computationally  efficient using the "bumptree" data structure described in (Omohundro, 1991). We  are currently studying the use of "model merging" to improve the representation  and are also applying it to robot control.  Acknowledgement s  This research was funded in part by Advanced Research Project Agency contract  #N0000 1493 C0249 and by the International Computer Science Institute. The  database was collected with a grant from Land Baden Wuerttenberg (Landesschw-  erpunkt Neuroinformatik) at Alex Waibel's institute.  References  C. Bregler, H. Hild, S. Manke & A. Waibel. (1993) Improving Connected Letter  Recognition by Lipreading. In Proc. of lnt. Conf. on Acoustics, Speech, and Signal  Processing, Minneapolis.  C. Bregler, Y. Konig (1994) "Eigenlips" for Robust Speech Recognition. In Proc.  of Int. Conf. on Acoustics, Speech, and Signal Processing, Adelaide.  M. Kass, A. Witkin, and D. Terzopoulos. (1987) SNAKES: Active Contour Models,  in Proc. of the First Int. Conf. on Computer Vision, London.  S. Omohundro. (1988) Fundamentals of Geometric Learning. University of Illinois  at Urbana-Champaign Technical Report UIUCDCS-R-88-1408.  S. Omohundro. (1991) Bumptrees for Efficient Function, Constraint, and Classifi-  cation Learning. In Lippmann, Moody, and Touretzky (ed.), Advances in Neural  Information Processing Systems 3. San Mateo, CA: Morgan Kaufmann.  A. Yuille. (1991) Deformable Templates for Face Recognition, Journal of Cognitive  Neuroscience, Volume 3, Number 1.  
Supervised learning from incomplete  data via an EM approach  Zoubin Ghahramani and Michael I. Jordan  Department of Brain & Cognitive Sciences  Massachusetts Institute of Techuology  Cambridge, MA 02139  Abstract  Real-world learning tasks may involve high-dimensional data sets  with arbitrary patterns of missing data. In this paper we present  a framework based on maximum likelihood density estimation for  learning from such data. sets. We use mixture models for the den-  sity estimates and make two distinct appeals to the Expectation-  Maximization (EM) principle (Dempster et al., 1977) in deriving  a learning algorithm--EM is used both for the estimation of mix-  ture components and for coping with missing data. The result-  ing algorithm is applicable to a wide range of supervised as well  as unsupervised learning problems. Results kom a classification  benchmark--the iris data set--are presented.  1 Introduction  Adaptive systems generally operate in environments that are fraught with imper-  fections; nonetheless they must cope with these imperfections and learn to extract  as much relevant information as needed for their particular goals. One form of  imperfection is incompleteness in sensing information. Incompleteness can arise ex-  trinsically from the data generation process and intrinsically from failures of the  system's sensors. For example, an object recognition system must be able to learn  to classify images with occlusions, and a robotic controller must be able to integrate  multiple sensors even when only a fraction may operate at any given time.  In this paper we present a framework--derived from parametric statistics--for learn-  120  Supervised Learning from Incomplete Data via an EM Approach 121  ing from data sets with arbitrary patterns of iucompleteness. Learning in this frame-  work is a classical estimation problem requiring an explicit probabilistic model and  an algorithm for estimating the parameters of the model. A possible disadvantage  of parametric methods is their lack of flexibility when compared with nonparamet-  ric methods. This problem, however, can be largely circumvented by the use of  mixture models (McLachlan and Basford, 1988). Mixture models combine much of  the flexibility of nonparametric methods with certain of the analytic advantages of  parametric methods.  Mixture models have been utilized recently for supervised learning problems in the  form of the "mixlures of experts" architecture (Jacobs et al., 1991; Jordan and  Jacobs, 1994). This architecture is a parametric regression model with a modular  structure similar to the nonparametric decision tree and adaptive spline models  (Breiman et al., 1984; Friedman, 1991). The approach presented here differs from  these regression-based approaches in that the goal of learning is to estimate the  density of the data. No distinction is made between input and output variables; the  joint density is estimated and this estimate is then used to form an input/output  map. Similar approaches have been discussed by Specht (1991) and Tresp et al.  (1993). To estimate the vector function y -- f(x) the joint density P(x,y) is esti-  mated and, given a particular input x, the conditional density P(yl x) is formed.  To obtain a single estimate of y rather than the full conditional density one can  evaluate  = E(y[x), the expectation of y given x.  The density-based approach to learning can be exploited in several ways. First,  having an estimate of the joint density allows for the representation of any rela-  tion between the variables. From P(x,y), we can estimate ' -- f(x), the inverse   _. f-1 (y), or any other relation between two subsets of the elements of the con-  catenated vector (x, y).  Second, this density-based approach is applicable both to supervised learning and  unsupervised learning in exactly the same way. The only distinction between su-  pervised and unsupervised learning in this framework is whether some portion of  the data vector is denoted as "input" and another portion as "target".  Third, as we discuss in this paper, the density-based approach deals naturally with  incomplete data, i.e. missing values in the data set. This is because the problem  of estimating mixture densities can itself be viewed as a missing data problem (the  "labels" for the component densities are missing) and an Expectation-Maximization  (EM) algorithm (Dempster et al., 1977) can be developed to handle both kinds of  missing data.  Density estimation using EM  This section outlines the basic learning algorithm for finding the maximum like-  lihood parameters of a mixture model (Dempster et al., 1977; Duda and Hart,  1973; Nowlan, 1991). We assume that the data Y = {x,..., XN} are generated  independently froin a mixture density  M  P(xi) = E P(xilcJ;OJ )P(j)' (1)  j=l  122 Ghahramani and Jordan  where each component of the mixture is denoted cod and parametrized by Oj. From  equation (1) and the independence assumption we see that the log likelihood of the  parameters given the data set is  N M  t(01x) - log P(xilw; (2)  i=1 j=l  By the maximum likelihood principle the best model of the data has parameters  that maximize/(01l'). This function, however, is not easily maximized numerically  because it involves the log of a sum.  Intuitively, there is a "credit-assignment" problem: it is not clear which component  of the mixture generated a given data point and thus which parameters to adjust  to fit that data point. The EM algorithm for mixture models is an iterative method  for solving this credit-assignment problem. The intuition is that if one had access  to a "hidden" random variable z that indicated which data point was genera. ted  by which component, then the maximization problem would decouple into a set  of simple maximizations. Using the indicator variable z, a "complete-data" log  likelihood function can be written  N M  /,(0IX, Z) - 1ogP(x, lz,;O)P(zi;O), (3)  i=1 j=l  which does not involve a log of a summation.  Since z is unknown l, cannot be utilized directly, so we instead work with its ex-  pectation, denoted by Q(OlOk). As shown by (Dempster et al., 1977), t(01x) can be  maximized by iterating the following two steps:  E step: Q(OlOk) =  M step: 0k+ = argmax Q(OlOk). (4)  0  The E (Expectation) step computes the expected complete data log likelihood and  the M (Maximization) step finds the parameters that maximize this likelihood.  These two steps form the basis of the EM algorithm; in the next two sections we  will outline how they can be used for real and discrete density estimation.  2.1 Real-valued data: mixture of Gaussians  Real-valued data can be lnodeled as a mixture of Gaussians. For this model the  E-step simplifies to computing hij = E[zij [xi, Ok], the probability that Gaussian j,  as defined by the parameters estimated at time step k, generated data point. i.  ^ k)T  -  I1-/2exp{-(xi - tj Ej 'k(xi- )}  hij = 1 [[-1/2xp{  x . (5)  The M-step re-estimates the means and covariances of the Gaussians I using the  data set weighted by the hij'  a) ' k+l _ E4_-i hijxl  Ij -- EiN__i hij '  ^k+l E/V_.i hij(xi /.3k.' + 1 ) (xi . k+l)T  (6)  Though this derivation assumes equal priors for the Gaussians, if the priors are viewed  as mixing parameters they can also be learned in the maximization step.  Supervised Learning from Incomplete Data via an EM Approach 123  2.2 Discrete-valued data: mixture of Bernoullis  D-dimensional binary data x = (Xl,..., Xd,...ZD), Zd ( {0, 1}, can be modeled as  a mixture of M Bernoulli densities. That is,  M D  P(xlO ) = y. P(wj) H pJ(1 - jd) (1-xa).  j----1 d=l  (7)  For this model tile E-step involves computing  d=l /jd k -- Ija]  % = i -  /-l=l 11d=l tld k -- r   (8)  and the M-step again re-estimates the parameters by  /,k4-1 __ Y"-/N'-I hijXi (9)  More generally, discrete or categorical data can be modeled as generated by a mix-  ture of multinonlial densities and similar derivations for the learning algorithm can  be applied. Finally, the exteusion to data with mixed real, binary, and categorical  dimensions call be readily derived by assuming a joint density with mixed compo-  nents of the three types.  3 Learning from incomplete data  In the previous section we presented one aspect of the EM algorithm: learning  mixture models. Another important application of EM is to learning froin data  sets with missing values (Little and Rubin, 1987; Dempster et al., 1977). This  application has been pursued in the statistics literature for non-mixture density  estimation problems; in this paper we combine this application of EM with that of  learning mixture parameters.  We assume that. the data set , = {Xl,..., XN} is divided into an observed com-  ponent .o and a missing component ,l'm. Similarly, each data vector xi is divided  X   into ( i, x?) where each data vector can have different missing components--this  would be denoted by superscript mi and oi, but we have simplified the notation for  the sake of clarity.  To handle missing data we rewrite the EM algorithln as follows  E step: O(OlOk ) = E[I(OI.t',xm,z)IX,Oi]  m step: 0+1 = argmax O(O[Ok). (10)  0  Comparing to equation (4) we see that aside from the indicator variables Z we have  added a second form of incomplete data., ,m, correspouding to the missing values  in the data set. The E-step of the algorithm estimates both these forms of missing  information; in essence it uses the current estimate of the data density to complete  the missing values.  124 Ghahramani and Jordan  3.1 Real-valued data: mixture of Gaussians  We start by writing the log likelihood of the complete data,  N M N M  /,(0IX, Xm, Z) - ZZzijlogP(xilzi,O)+ ZEzijlogP(zilO). (11)  i j i j  We can ignore the second term since we will only be estimating the parameters of  the P(xilzi, 0). Using equation (11) for the mixture ot' Gaussians wc note that if  only the indicator variables zl are missing, the E step can be reduced to estimating  E[zli xi, 0]. For the ce we are interested in, with two types of missing data zi and  x, we expand equation (11) using m and o superscripts to denote subvectors and  submatrices of the parameters matching the missing and observed components of  the data,  N M  n 1 1 o i,OO(x   /e(01X,xm, z) = zij[log2w+loglsl-(xi _])Tf . _)  i j  --(X o T -1 I m  -) E 'm(x?--7)--(x ,  Note that after taking the expectation, the sufficient statistics for the parameters  involve three unknown terms, zij, zisx, and zoxx r. Thus we must compute:      .. m m  E[zalx ? 0] E[zox?lx 7 0] and E[z,ax i x i Tlx?,0 ].  One intuitive approach to dealing with missing data is to use the current estimate  of the data density to compute the expectat. ion of the missing data in an E-step,  complete the data with these expectations, and then use this completed data to re-  estimate parameters in an M-step. However, this intuition fails even when dealing  with a single two-dimensional Gaussian; the expectation of the missing data always  lies along a line, which bies the estimate of the covariance. On the other hand,  the approach arising from application of the EM algorithm specifies that one should  use the current density estimate to compute the expectation of whatever incomplete  terms appear in the likelihood maximization. For the mixture of Gaussians these  incomplete terms involve interactions between the indicator variable zi5 and the  first and second moments of x. Thus, simply computing the expectation of the  missing data zl and x fi'om our model and substituting those values iuto the M  step is not sufficient to guarantee an increde in the likelihood of the parameters.  The above terms can be computed as follows: E[zj ]x, 0] is again hj, the proba-  bility  defined in (5) measured only on the observed dimensions of xi, and  o ' EE9-(x? -- P'7)), (12)  = h[x21z5 = = + _, _,  Defining   E[xlzij = 1 x9 0] the regression of x on x? using Gaussian j  Z[z,x7x?lx? = h(s?m mooo-s?o  , - -5  + xr). (1)  The M-step uses these expectations substituted into equations (6)a aud (6)b to  re-estimate the means and covariances. To re-estimate the mean vector, /xj, we  substitute the values E[xnlzij = 1,x,0k] for the missing components of xi in  equation (6)a. To re-estimate the covariance matrix we substitute the values  E[x?x? T Izij = 1, x?, 0k] for the outer product matrices involving the ntissing com-  ponents of xi in equation (6)b.  Supervised Learning from Incomplete Data via an EM Approach 125  3.2 Discrete-valued data: mixture of Bernoullis  For the Bernoulli mixture the sufficient statistics for the M-step involve the incom-  plete terms E[zijlx?, 0k] and E[zijxlx?, 0k]. The first is equal to hij calculated over  the observed subvector of xi. The second, since we assume that within a class the  h m  individual dimensions of the Bernoulli variable are independent, is simply ijttj   The M-step uses these expectations substituted into equation (9).  4 Supervised learning  If each vector xi in the data set is composed of an "input" subvector, xii, and a  "target" or outpnt subvector, x?, then learning the joint density of thc input and  target is a form of supervised learning. In supervised learning we generally wish to  predict the output variables from the input variables. In this section we will outline  how this is achieved using the estimated density.  4.1 Function approximation  For real-valued function approximation we have assumed that the density is esti-  mated using a mixture of Gaussians. Given an input vector xii we extract all the  relevant information from the density p(xi,x ) by conditionalizing to  For a single Gaussian this conditional density is normal, and, since P(x', x ) is a  mixture of Gaussians so is P(xlxi). In principle, this conditional density is the  final output of the density estimator. That is, given a particular input the net-  work returns the complete conditional density of the output. However, since many  applications require a single estimate of the output, we note three ways to ob-  tain estimates  of x  = f(xi): the least squares estimate (LSE), which takes  :(xii) = E(x[xii); stochastic sampling (STOCH), which samples according to  the distribution .(xi)  P(xlx); single colnponent LSE (SLSE), which takes  (xii) = E(x[x},wj) where j = argmaxk P(zklx'i). For a given input, SLSE picks  the Gaussian with highest posterior and approximates the output with the LSE  estimator given by that Gaussian alone.  The conditional expectation or LSE estimator for a Gaussian mixture is  oi ii-  _ i  O(xii ) = ]])= hij[tty + E E (xi - tt})] (14)  M '  which is a convex sum of linear approximations, where the weights h;z vary non-  linearly according to equation (14) over the input space. The LSE estimator on a  Gaussian mixture has interesting relations to algorithms such as CART (Breiman  et al., 1984), MARS (Friedman, 1991), and mixtures of experts (Jacobs ctal., 1991;  Jordan and Jacobs, 1994), in that the mixture of Gaussians competitively parti-  tions the input space, and learns a linear regression surface on each partition. This  similarity h also been noted by Tresp et al. (1993) .  The stochastic estimator (STOCH) and the single component estimator (SLSE) are  better suited than any least squares method for learning non-convex inverse maps,  where the mean of several solutions to an inverse might not be a sohllion. These  126 Ghahramani and Jordan  Figure 1: Classification of the iris data  set. 100 data points were used for train-  ing and 50 for testing. Each data point  consisted of 4 real-valued attributes and  one of three class labels. The figure  shows classification performance q- 1  standard error (, = 5) as a function  of proportion missing features for the  EM algorithm and for mean imputa-  tion (MI), a common heuristic where the  missing values are replaced with their  unconditional means.  Classification with missing inputs  0 20 40 60 80 100  % missing features  estimators take advantage of the explicit representation of the input/output density  by selecting one of the several solutions to the inverse.  4.2 Classification  Classification problems involve learning a mapping from an input space into a set  of discrete class labels. The density estimation framework presented in this paper  lends itself to solving classification problems by estimating the joint density of the  input and class label using a mixture model. For example, if the inputs have real-  valued attributes and there are D class labels, a mixture model with Gaussian and  multinomial components will be used:  M  P(x,C = d[O)=  P(coj) tJd exp{ 1 )TE-I(x ttj)}, (15)  j=l (2w)n/2[jll]2 --(X- ttj --  denoting the joint probability that the data point is x and belongs to class d,  where the ttjd are the parameters for the multinomial. Once this density has been  estimated, the maximum likelihood label for a particular input x may be obtained  by computing P( = dlx , 0). Similarly, the class conditional densities can be derived  by evaluating P(xlC = d, 0). Conditionalizing over classes in this way yields class  conditional densities which are in turn mixtures of Gaussians. Figure 1 shows  the performance of the EM algorithm on an example classification problem with  varying proportions of missing features. We have also applied these algorithms to  the problems of clustering 35-dimensional greyscale images and approximating the  kinematics of a three-joint planar arm from incomplete data.  5 Discussion  Density estimation in high dimensions is generally considered to be more difficult--  requiring more parameters--than function approximation. The density-estimation-  based approach to learning, however, has two advantages. First, it permits ready in-  corporation of results from the statistical literature on missing data to yield flexible  supervised and unsupervised learning architectures. This is achieved by combining  two branches of application of the EM algorithm yielding a set of learning rules for  mixtures under incomplete sampling.  Supervised Learning from Incomplete Data via an EM Approach 127  Second, estimating the density explicitly enables us to represent any relation be-  tween the variables. Density estimation is fundamentally more general than function  approxilnation and this generality is needed for a large class of learning problems  arising from inverting causal systems (Ghahramani, 1994). These problclns cannot  be solved easily by traditional function approximation techniques since the data is  not generated from noisy samples of a function, but rather of a relation.  Acknowledgements  Thanks to D. M. Titterington and David Cohn for helpful comments. This project  was supported in part by grants from the McDonnell-Pew Foundation, ATR Andi-  tory and Visual Perception Research Laboratories, Siemens Corporation, the Na-  tional Science Foundation, and the Office of Naval Research. The iris data set was  obtained from the UCI Repository of Machine Learning Databases.  References  Breiman, L., Friedman, J. H., Olshen, R. A., and Stone, C. J. (1984). Classification  and Regression Trees. Wadsworth International Group, Behnont, CA.  Dempster, A. P., Laird, N.M., and Rubin, D. B. (1977). Maximum likelihood from  incomplete data via the EM algorithm. J. Royal Statistical Society Series B,  39:1-38.  Duda, R. O. and Hart, P. E. (1973). Pattern Classification and SceTe Analysis.  Wiley, New York.  Friedman, J. tI. (1991). Multivariate adaptive regression splines. Tbc Annls of  Statistics, 19:1-141.  Ghahramani, Z. (1994). Solving inverse problems using an EM approach to density  estimation. In Proceedings of the 1993 Connectionist Models Summer School.  Erlbaum, Hillsdale, NJ.  Jacobs, R., Jordan, M., NowInn, S., and Hinton, G. (1991). Adaptive mixtnre of  local experts. Neural Computation, 3:79-87.  Jordan, M. and Jacobs, R. (1994). Hierarchical mixtures of experts and the EM  algorithm. Neural Computation, 6:181-214.  Little, R. J. A. and Rubin, D. B. (1987). Statistical Analysis with Missing Data.  Wiley, New York.  McLachlan, G. and Basford, K. (1988). Mixture models: Inference and applications  to clustering. Marcel Dekker.  Nowlan, S. J. (1991). Soft Competitive Adaptation: Neural Network Learning Algo-  rithms based on Fitting Statistical Mixtures. CMU-CS-91-126, School of Com-  puter Science, Carnegie Mellon University, Pittsburgh, PA.  Specht, D. F. (1991). A general regression neural network. IEEE Trans. Neural  Networks, 2(6):568-576.  Tresp, V., Ho!latz, J., and Ahmad, S. (1993). Network structuring and training  using rule-based knoxvledge. In Hanson, S. J., Cowan, J. D., and Giles, C. L.,  editors, Advances in Neural Information Processing Systems 5. Morgan Kauf-  man Publishers, San Mateo, CA.  
Fool's Gold: Extracting Finite State Machines  From Recurrent Network Dynamics  John E Kolen  Laboratory for Artificial Intelligence Research  Department of Computer and Information Science  The Ohio State University  Columbus, OH 43210  kolen-j @ cis.ohio-state.edu  Abstract  Several recurrent networks have been proposed as representations for the  task of formal language learning. After training a recurrent network rec-  ognize a formal language or predict the next symbol of a sequence, the  next logical step is to understand the information processing carried out  by the network. Some researchers have begun to extracting finite state  machines from the internal state trajectories of their recurrent networks.  This paper describes how sensitivity to initial conditions and discrete  measurements can trick these extraction methods to return illusory finite  state descriptions.  INTRODUCTION  Formal language learning (Gold, 1969) has been a topic of concern for cognitive science  and artificial intelligence. It is the task of inducing a computational description of a formal  language from a sequence of positive and negative examples of strings in the target lan-  guage. Neural information processing approaches to this problem involve the use of recur-  rent networks that embody the internal state mechanisms underlying automata models  (Cleeremans et al., 1989; Elman, 1990; Pollack, 1991; Giles et al, 1992; Watrous & Kuhn,  1992). Unlike traditional automata-based approaches, learning systems relying on recurrent  networks have an additional burden: we are still unsure as to what these networks are  doing. Some researchers have assumed that the networks are learning to simulate finite state  501  502 Kolen  machines (FSMs) in their state dynamics and have begun to extract FSMs from the net-  works' state transition dynamics (Cleeremans et al., 1989; Giles et al., 1992; Watrous &  Kuhn, 1992). These extraction methods employ various clustering techniques to partition  the internal state space of the recurrent network into a finite number of regions correspond-  ing to the states of a finite state automaton.  This assumption of finite state behavior is dangerous on two accounts. First, these extrac-  tion techniques are based on a discretization of the state space which ignores the basic def-  inition of information processing state. Second, discretization can give rise to incomplete  computational explanations of systems operating over a continuous state space.  SENSITIVITY TO INITIAL CONDITIONS  In this section, I will demonstrate how sensitivity to initial conditions can confuse an FSM  extraction system. The basis of this claim rests upon the definition of information processing  state. Information processing (IP) state is the foundation underlying automata theory. Two  IP states are the same if and only if they generate the same output responses for all possible  future inputs (Hopcroft & Ullman, 1979). This definition is the fulcrum for many proofs and  techniques, including finite state machine minimization. Any FSM extraction technique  should embrace this definition, in fact it grounds the standard FSM minimization methods  and the physical system modelling of Crutchfield and Young (Crutchfield & Young, 1989).  Some dynamical systems exhibit exponential divergence for nearby state vectors, yet  remain confined within an attractor. This is known as sensitivity to initial conditions. If this  divergent behavior is quantized, it appears as nondeterministic symbol sequences (Crutch-  field & Young, 1989) even though the underlying dynamical system is completely deter-  ministic (Figure 1).  Consider a recurrent network with one output and three recurrent state units. The output  unit performs a threshold at zero activation for state unit one. That is, when the activation  of the first state unit of the current state is less than zero then the output is A. Otherwise,  the output is B. Equation 1 presents a mathematical description. S(t) is the current state of  the system O (t) is the current output.  S(t+l) = tanh( 0 2 . t )  0 2  A Sl(t ) < 0  O(t) = B S l(t)>0  (1)  Figure 2 illustrates what happens when you run this network for many iterations. The point  in the upper left hand state space is actually a thousand individual points all within a ball  of radius 0.01. In one iteration these points migrate down to the lower corner of the state  space. Notice that the ball has elongated along one dimension. After ten iterations the orig-  inal ball shape is no longer visible. After seventeen, the points are beginning to spread  along a two dimensional sheet within state space. And by fifty iterations, we see the net-  work reaching the its full extent of in state space. This behavior is known as sensitivity to  initial conditions and is one of three conditions which have been used to characterize cha-  otic dynamical systems (Devaney, 1989). In short, sensitivity to initial conditions implies  Fool's Gold: Extracting Finite State Machines from Recurrent Network Dynamics 503  x6-4x(1-x)  O(x) = { A x<0.5 B  B x>0.5  A  x 6- 2x mod 1  1  A x<  1 2  s <x<  C 2  --<32  3 Ai C  x 6- 3.68x ( 1 - x)  O(x) = { A x<0.5  B x>0.5  Figure 1: Examples of deterministic dynamical systems whose discretize trajectories  appear nondeterministic.  that any epsilon ball on the attractor of the dynamical will exponentially diverge, yet still  be contained within the locus of the attractor. The rate of this divergence is illustrated in  Figure 3 where the maximum distance between two points is plotted with respect to the  number of iterations. Note the exponential growth before saturation. Saturation occurs as  the point cloud envelops the attractor.  No matter how small one partitions the state space, sensitivity to initial conditions will  eventually force the extracted state to split into multiple trajectories independent of the  future input sequence. This is characteristic of a nondeterministic state transition. Unfortu-  nately, it is very difficult, and probably intractable, to differentiate between a nondetermin-  istic system with a small number of states or a deterministic with large number of states. In  certain cases, however, it is possible to analytically ascertain this distinction (Crutchfield &  Young, 1989).  THE OBSERVERS' PARADOX  One response to this problem is to evoke more computationally complex models such as  push-down or linear-bounded automata. Unfortunately, the act of quantization can actually  introduce both complexion and complexity in the resulting symbol sequence. Pollack and  I have focused on a well-hidden problems with the symbol system approach to understand-  ing the computational powers of physical systems. This work (Kolen & Pollack, 1993;  504 Kolen  1 1 1  output=A 1 output=B 1 output=A 1  Start (e<0.01) 1 iteration 10 iterations  1 1 1  output=A,B 1 output=A,B 1 output=A,B 1  17 iterations 25 iterations 50 iterations  Figure 2: The state space of a recurrent network whose next state transitions are  sensitive to initial conditions. The initial epsilon ball contains 1000 points. These points  first straddle the output decision boundary at iteration seven.  Kolen & Pollack, In press) demonstrated that computational complexity, in terms of Chom-  sky's hierarchy of formal languages (Chomsky, 1957; Chomsky, 1965) and Newell and  Simon's physical symbol systems (Newell & Simon, 1976), is not intrinsic to physical sys-  tems. The demonstration below shows how apparently trivial changes in the partitioning of  state space can produce symbol sequences from varying complexity classes.  Consider a point moving in a circular orbit with a fixed rotational velocity, such as the end  of a rotating rod spinning around a fixed center, or imagine watching a white dot on a spin-  ning bicycle wheel. We measure the location of the dot by periodically sampling the loca-  tion with a single decision boundary (Figure 4, left side). If the point is to the left of  boundary at the time of the sample, we write down an "17'. Likewise, we write down an ""'  when the point is on the other side. (The probability of the point landing on the boundary  is zero and can arbitrarily be assigned to either category without affecting the results  below.) In the limit, we will have recorded an infinite sequence of symbols containing long  sequences of "s and l.'s.  The specific ordering of symbols observed in a long sequence of multiple rotations is  Fool's Gold: Extracting Finite State Machines from Recurrent Network Dynamics 505  2.5  2  10 20 30 40 5  Iteration number  Figure 3: Spread of initial points across the attractor as measured by maximum distance.  1  1 r  Figure 4: On the left, two decision regions which induce a context free language. 0 is  the current angle of rotation. At the time of sampling, if the point is to the left (right) of  the dividing line, an 1 (r) is generated. On the right, three decision regions which  induce a context sensitive language.  dependent upon the initial rotational angle of the system. However, the sequence does pos-  sess a number of recurring structural regularities, which we call sentences: a run of r's fol-  lowed by a run of l's. For a fixed rotational velocity (rotations per time unit) and sampling  rate, the observed system will generate sentences of the form rnl m (n, m > 0). (The notation  r n indicates a sequence of n r's.) For a fixed sampling rate, each rotational velocity spec-  ifies up to three sentences whose number of r's and l's differ by at most one. These sen-  tences repeat in an arbitrary manner. Thus, a typical subsequence of a rotator which  produces sentences rnl n, rnln+l,rn+11 n would look like  506 Kolen  rnln+lrnlnrnln+lrn+llnrnlnrnln+l '  A language of sentences may be constructed by examining the families of sentences gener-  ated by a large collection of individuals, much like a natural language is induced from the  abilities of its individual speakers. In this context, a language could be induced from a pop-  ulation of rotators with different rotational velocities where individuals generate sentences  of the form {rnl n, rnln+l,rn+lln}, n > 0. The resulting language can be described by a  context free grammar and has unbounded dependencies; the number of 1's is a function of  the number of preceding r's. These two constraints on the language imply that the induced  language is context free.  To show that this complexity class assignment is an artifact of the observational mecha-  nism, consider the mechanism which reports three disjoint regions: 1, c, and r (Figure 4,  right side). Now the same rotating point will generate sequences of the form  ...rr...rrcc...cc11...11rr...rrcc...cc11...11 ....  For a fixed sampling rate, each rotational velocity specifies up to seven sentences, rnmlk,  when n, m, and k can differ no by no more than one. Again, a language of sentences may  be constructed containing all sentences in which the number of r's, e's, and 1's differs by  no more than one. The resulting language is context sensitive since it can be described by  a context sensitive grammar and cannot be context free as it is the finite union of several  context sensitive languages related to rncnl n.  CONCLUSION  Using recurrent neural networks as the representation underlying the language learning task  has revealed some inherent problems with the concept of this task. While formal languages  have mathematical validity, looking for language induction in physical systems is question-  able, especially if that system operates with continuous internal states. As I have shown,  there are two major problems with the extraction of a learned automata from our models.  First, sensitivity to initial conditions produces nondeterministic machines whose trajecto-  ries are specified by both the initial state of the network and the dynamics of the state trans-  formation. The dynamics provide the shape of the eventual attractor. The initial conditions  specify the allowable trajectories toward that attractor. While clustering methods work in  the analysis of feed-forward networks because of neighborhood preservation (as each layer  is a homeomorphism), they may fail when applied to recurrent network state space trans-  formations. FSM construction methods which look for single transitions between regions  will not help in this case because the network eventually separates initially nearby states  across several FSM state regions.  The second problem with the extraction of a learned automata from recurrent network is  that trivial changes in observation strategies can cause one to induce behavioral descrip-  tions from a wide range of computational complexity classes for a single system. It is the  researcher's bias which determines that a dynamical system is equivalent to a finite state  automata.  Fool's Gold: Extracting Finite State Machines from Recurrent Network Dynamics 507  One response to the first problem described above has been to remove and eliminate the  sources of nondeterminism from the mechanisms. Zeng et. al (1993) corrected the second-  order recurrent network model by replacing the continuous internal state transformation with  a discrete step function. (The continuous activation remained for training purposes.) This  move was justified by their focus on regular language learning, as these languages can be rec-  ognized by finite state machines. This work is questionable on two points, however. First,  tractable algorithms already exist for solving this problem (e.g. Angluin, 1987). Second, they  claim that the network is self-clustering the internal states. Self-clustering occurs only at the  comers of the state space hypercube because of the discrete activation function, in the same  manner as a digital sequential circuit "clusters" its states. Das and Mozer (1994), on the other  hand, have relocated the clustering algorithm. Their work focused on recurrent networks that  perform internal clustering during training. These networks operate much like competitive  learning in feed-forward networks (e.g. Rumelhart and Zipser, 1986) as the dynamics of the  learning rules constrain the state representations such that stable clusters emerge.  The shortcomings of finite state machine extraction must be understood with respect to the  task at hand. The actual dynamics of the network may be inconsequential to the final prod-  uct if one is using the recurrent network as a pathway for designing a finite state machine.  In this engineering situation, the network is thrown away once the FSM is extracted. Neural  network training can be viewed as an "interior" method to finding discrete solutions. It is  interior in the same sense as linear programming algorithms can be classified as either edge  or interior methods. The former follows the edges of the simplex, much like traditional  FSM learning algorithms search the space of FSMs. Internal methods, on the other hand,  explore search spaces which can embed the target spaces. Linear programming algorithms  employing internal methods move through the interior of the defined simplex. Likewise,  recurrent neural network learning methods swim through mechanisms with multiple finite  state interpretations. Some researchers, specifically those discussed above, have begun to  bias recurrent network learning to walk the edges (Zeng et al, 1993) or to internally cluster  states (Das & Mozer, 1994).  In order to understand the behavior of recurrent networks, these devices should be regarded  as dynamical systems (Kolen, 1994). In particular, most common recurrent networks are  actually iterated mappings, nonlinear versions of Barnsley's iterated function systems  (Barnsley, 1988). While automata also fall into this class, they are a specialization of  dynamical systems, namely discrete time and state systems. Unfortunately, information  processing abstractions are only applicable within this domain and do not make any sense  in the broader domains of continuous time or continuous space dynamical systems.  Acknowledgments  The research reported in this paper has been supported by Office of Naval Research grant  number N00014-92-J-1195. I thank all those who have made comments and suggestions for  improvement of this paper, especially Greg Saunders and Lee Giles.  References  Angluin, D. (1987). Learning Regular Sets from Queries and Counterexamples. Information  508 Kolen  and Computation, 75, 87-106.  Barnsley, M. (1988). Fractals Everywhere. Academic Press: San Diego, CA.  Chomsky, N. (1957). Syntactic Structures. The Hague: Mounton & Co.  Chomsky, N. (1965). Aspects of the Theory of Syntax. Cambridge, Mass.: MIT Press.  Cleeremans, A., Servan-Schreiber, D. & McClelland, J. L. (1989). Finite state automata and  simple recurrent networks. Neural Computation, 1,372-381.  Crutchfield, J. & Young, K. (1989). Computation at the Onset of Chaos. In W. Zurek, (Ed.),  Entropy, Complexity, and the Physics of lnformation. Reading: Addison-Wesely.  Das, R. & Mozer, M. (1994) A Hybrid Gradient-Descent/Clustering Technique for Finite  State Machine Induction. In Jack D. Cowan, Gerald Tesauro, and Joshua Alspector, (Eds.),  Advances in Neural Information Processing Systems 6. Morgan Kaufman: San Francisco.  Devaney, R. L. (1989). An Introduction to Chaotic Dynamical Systems. Addison-Wesley.  Elman, J. (1990). Finding structure in time. Cognitive Science, 14, 179-211.  Giles, C. L., Miller, C. B., Chen, D., Sun, G. Z., Chen, H. H. & C.Lee, Y. (1992). Extracting  and Learning an Unknown Grammar with Recurrent Neural Networks. In John E. Moody,  Steven J. Hanson & Richard P. Lippman, (Eds.), Advances in Neural Information Processing  Systems 4. Morgan Kaufman.  Gold, E. M. (1969). Language identification in the limit. Information and Control, 10, 372-  381.  Hopcroft, J. E. & Ullman, J. D. (1979). Introduction to Automata Theory, Languages, and  Computation. Addison-Wesely.  Kolen, J. F. (1994) Recurrent Networks: State Machines or Iterated Function Systems?. In M.  C. Mozer, P. Smolensky, D. S. Touretzky, J. L. Elman, & A. S. Weigend (Eds.), Proceedings  of the 1993 Connectionist Models Summer School. (pp. 203-210) Hillsdale, NJ: Erlbaum  Associates.  Kolen, J. F. & Pollack, J. B. (1993). The Apparent Computational Complexity of Physical  Systems. In Proceedings of the Fifteenth Annual Conference of the Cognitive Science Society.  Laurence Earlbaum.  Kolen, J. F. & Pollack, J. B. (In press) The Observers' Paradox: The Apparent Computational  Complexity of Physical Systems. Journal of Experimental and Theoretical Artificial Intelli-  gence.  Pollack, J. B. (1991). The Induction Of Dynamical Recognizers. Machine Learning, 7. 227-  252.  Newell, A. & Simon, H. A. (1976). Computer science as empirical inquiry: symbols and  search. Communications of the Association for Computing Machinery, 19, 113-126.  Rumelhart, D. E., and Zipser, D. (1986). Feature Discovery by Competitive Learning. In D.  E. Rumelhart, J. L. McClelland, and the PDP Research Group, (Eds.), Parallel Distributed  Processing. Volume 1.151-193. MIT Press: Cambridge, MA.  Watrous, R. L. & Kuhn, G. M. (1992). Induction of Finite-State Automata Using Second-  Order Recurrent Networks. In John E. Moody, Steven J. Hanson & Richard P. Lippman,  (Eds.), Advances in Neural Information Processing Systems 4. Morgan Kaufman.  Zeng, Z., Goodman, R. M., Smyth, P. (1993). Learning Finite State Machines With Self-Clus-  tering Recurrent Networks. Neural Computation, 5, 976-990  PART IV  NEUROSCIENCE  
Analyzing Cross Connected Networks  Thomas R. Shultz  Department of Psychology &  McGill Cognitive Science Centre  McGill University  Montr6al, Qu6bec, Canada H3A lB 1  sh ultz@ psych. mcgill .ca  and  Jeffrey L. Elman  Center for Research on Language  Department of Cognitive Science  University of California at San Diego  LaJolla, CA 92093-0126 U.S.A.  elman@crl.ucsd.edu  Abstract  The non-linear complexities of neural networks make network solutions  difficult to understand. Sanger's contribution analysis is here extended to  the analysis of networks automatically generated by the cascade-  correlation learning algorithm. Because such networks have cross  connections that supersede hidden layers, standard analyses of hidden  unit activation patterns are insufficient. A contribution is defined as the  product of an output weight and the associated activation on the sending  unit, whether that sending unit is an input or a hidden unit, multiplied  by the sign of the output target for the current input pattern.  Intercorrelations among contributions, as gleaned from the matrix of  contributions x input patterns, can be subjected to principal  components analysis (PCA) to extract the main features of variation in  the contributions. Such an analysis is applied to three problems,  continuous XOR, arithmetic comparison, and distinguishing between  two interlocking spirals. In all three cases, this technique yields useful  insights into network solutions that are consistent across several  networks.  1 INTRODUCTION  Although neural network researchers are typically impressed with the performance  achieved by their learning networks, it often remains a challenge to explain or even  characterize such performance. The latter difficulties stem principally from the complex  non-linear properties of neural nets and from the fact that information is encoded in a form  that is distributed across many weights and units. The problem is exacerbated by the fact  that multiple nets generate unique solutions depending on variation in both starting states  and training patterns.  Two techniques for network analysis have been applied with some degree of success,  focusing respectively on either a network's weights or its hidden unit activations. Hinton  (e.g., Hinton & Sejnowski, 1986) pioneered a diagrammatic analysis that involves  plotting a network's learned weights. Occasionally, such diagrams yield interesting  insights but often, because of the highly distributed nature of network representations, the  most notable features of such analyses are the complexity of the pattern of weights and its  variability across multiple networks learning the same problem.  1117  1118 Shultz and Elman  Statistical analysis of the activation patterns on the hidden units of three layered feed-  forward nets has also proven somewhat effective in understanding network performance.  The relations among hidden unit activations, computed from a matrix of hidden units x  input patterns, can be subjected to either cluster analysis (Elman, 1990) or PCA (Elman,  1989) to determine the way in which the hidden layer represents the various inputs.  However, it is not clear how this technique should be extended to multi-layer networks or  to networks with cross connections.  Cross connections are direct connections that bypass intervening hidden layers. Cross  connections typically speed up learning when used in static back-propagation networks  (Lang & Witbrock, 1988) and are an obligatory and ubiquitous feature of some generative  learning algorithms, such as cascade-correlation (Fahlman & Lebiere, 1990). Generative  algorithms construct their own network topologies as they learn. In cascade-correlation,  this is accomplished by recruiting new hidden units into the network, as needed, installing  each on a separate layer. In addition to layer-to-layer connections, each unit in a cascade-  correlation network is fully cross connected to all non-adjacent layers downstream.  Because such cross connections carry so much of the work load, any analysis restricted to  hidden unit activations provides a partial picture of the network solution at best.  Generafive networks seem to provide a number of advantages over static networks,  including more principled network design, leaner networks, faster learning, and more  realistic simulations of human cognitive development (Fahlman & Lebiere, 1990; Shultz,  Schmidt, Buckingham, & Mareschal, in press). Thus, it is important to understand how  these networks function, even if they seem impervious to standard analytical tools.  2 CONTRIBUTION ANALYSIS  One analytical technique that might be adapted for multi-layer, cross connected nets is  contribution analysis (Sanger, 1989). Sanger defined a contribution as the triple product  of an output weight, the activation of a sending unit, and the sign of the output target for  that input. He argued that contributions are potentially more informative than either  weights alone or hidden unit activations alone. A large weight may not contribute much  if it is connected to a sending unit with a small activation. Likewise, a large sending  activation may not contribute much if it is connected via a small weight. In contrast,  considering a full contribution, using both weight and sending activation, would more  likely yield valid comparisons.  Sanger (1989) applied contribution analysis to a small version of NETtalk, a net that  learns to convert written English into spoken English (Sejnowski & Rosenberg, 1987).  Sanger's analysis began with the construction of an output unit x hidden unit x input  pattern array of contributions. Various two-dimensional slices were taken from this three-  dimensional array, each representing a particular output unit or a particular hidden unit.  Each two-dimensional slice was then subjected to PCA, yielding information about either  distributed or local hidden unit responsibilities, depending on whether the focus was on an  individual output unit or individual hidden unit, respectively.  3 CONTRIBUTION ANALYSIS FOR MULTI-LAYER,  CROSS CONNECTED NETS  We adapted contribution analysis for use with multi-layered, cross connected cascade-  correlation nets. Assume a cascade-correlation network with j units (input units + hidden  units) and k output units, being trained with i input patterns. There are j x k output  weights in such a network, where an output weight is defined as any weight connected to  Analyzing Cross-Connected Networks 1119  an output unit. A contribution c for a particular ijk combination is defined as  Cijk = Wjk aij 2tki (1)  where Wjk is the weight connecting sending unit j with output unit k, aij is the activation  of sending unit j given input pattern i, and tki is the target for output unit k given input  pattern i. The term 2tki adjusts the sign of the contribution so that it provides a measure  of correctness. That is, positive contributions push the output activation towards the  target, whereas negative contributions push the output activation away from the target. In  cascade-correlation, sigmoid output units have targets of either -0.5 or +0.5. Hence,  multiplying a target by 2 yields a positive sign for positive targets and a negative sign for  negative targets. Our term 2tki is analogous to Sanger's (1989) term 2tik - 1, which is  appropriate for targets of 0 and 1, commonly used in back-propagation learning.  In contrast to Sanger's (1989) three-dimensional army of contributions (output unit x  hidden unit x input pattern), we begin with a two-dimensional output weight (k *j) x  input pattern (i) array of contributions. This is because we want to include all of the  contributions coming into the output units, including the cross connections from more  than one layer away. Since we begin with a two-dimensional array, we do not need to  employ the somewhat cumbersome slicing technique used by Sanger to isolate particular  output or hidden units. Nonetheless, as will be seen, our technique does allow the  identification of the roles of specific contributions.  4 PRINCIPAL COMPONENTS ANALYSIS  Correlations among the various contributions across input patterns are subjected to PCA.  PCA is a statistical technique that identifies significant dimensions of variation in a  multi-dimensional space (Flury, 1988). A component is a line of closest fit to a set of  points in multi-dimensional space. The goal of PCA is to summarize a multivariate data  set using as few components as possible. It does this by taking advantage of possible  correlations among the variables (contributions, in our case).  We apply PCA to contributions, as defined in Equation 1, taken from networks learning  three different problems: continuous XOR, arithmetic comparisons, and distinguishing  between interlocking spirals. The contribution matrix for each net, as described in section  3, is subjected to PCA using 1.0 as the minimum eigenvalue for retention. Varimax  rotation is applied to improve the interpretability of the solution. Then the scree test is  applied to eliminate components that fail to account for much of the variance (Cattell,  1966). In cases where components are eliminated, the analysis is repeated with the correct  number of components, again with a varimax rotation. Component scores for the retained  components are plotted to provide an indication of the function of the components.  Finally, component loadings for the various contributions are examined to determine the  roles of the contributions from hidden units that had been recruited into the networks.  5 APPLICATION TO THE CONTINUOUS XOR PROBLEM  The simplicity of binary XOR and the small number of training patterns (four) renders  application of contribution analysis superfluous. However, it is possible to construct a  continuous version of the XOR problem that is more suitable for contribution analysis.  We do this by dividing the input space into four quadrants. Input values are incremented  in steps of 0.1 starting from 0.0 up to 1.0, yielding 100 x, y input pairs. Values of x up  to 0.5 combined with values of y above 0.5 produce a positive output target (0.5), as do  values of x above 0.5 combined with values of y below 0.5. Input pairs in the other two  quadrants yield a negative output target (-0.5).  1120 Shultz and Elman  Three cascade-correlation nets are trained on this problem. Each of the three nets generates  a unique solution to the continuous XOR problem, with some variation in number of  hidden units recruited. PCA of contributions yields different component loadings across  the three nets and different descriptions of components. Yet with all of that variation in  detail, it is apparent that all three nets make the same three distinctions that are afforded  by the training patterns. The largest distinction is that which the nets are explicitly  mined to make, between positive and negative outputs. Two components are sufficient to  describe the representations. Plots of rotated component scores for the 100 training  patterns cluster into four groups of 25 points, each cluster corresponding to one of the  four quadrants described earlier. Component loadings for the various contributions on the  two components indicate that the hidden units play an interactive and distributed role in  separating the input patterns into their respective quadrants.  6 APPLICATION TO COMPARATIVE ARITHMETIC  A less well understood problem than XOR in neural net research is that of arithmetic  operations, such as addition and multiplication. What has a net leamed when it learns to  add, or to multiply, or to do both operations? The non-linear nature of multiplication  makes it particularly interesting as a network analysis problem. The fact that several  psychological simulations using neural nets involve problems of linear and non-linear  arithmetic operations enhances interest in this sort of problem (McClelland, 1989; Shultz  et al., in press).  We designed arithmetic comparison tasks that provided interesting similarities to some of  the psychological simulations. In particular, instead of simply adding or multiplying, the  nets learn to compare sums or products to some value and then output whether the sum or  product is greater than, less than, or equal to that comparative value.  The addition and multiplication tasks each involve three linear input units. The first two  input units each code a randomly selected integer in the range from 0 to 9, inclusive. The  third input unit codes a randomly selected comparison integer. For addition problems, the  comparison values are in the range of 0 to 19, inclusive; for multiplication the range is 0  to 82, inclusive. Two output units code the results of the comparison. Target outputs of  0.5 and -0.5 represent that the results of the arithmetic operation are greater than the  comparison value, targets of -0.5 and 0.5 represent less than, and targets of 0.5 and 0.5  represent equal to. For problems involving both addition and multiplication, a fourth  input unit codes the type of arithmetic operation to be performed: 0 for addition, 1 for  multiplication.  Nets trained on either addition or multiplication have 100 randomly selected training  patterns, with the restriction that 45 of them have correct answers of greater than, 45 have  correct answers of less than, and 10 have correct answers of equal to. The latter constraints  are designed to reduce the natural skew of comparative values in the high direction on  multiplication problems. Nets trained on both addition and multiplication have 100  randomly selected addition problems and 100 randomly selected multiplication problems,  subject to the constraints just described. We trained three nets on addition, three on  multiplication, and three on both addition and multiplication.  6.1 RESULTS FOR ADDITION  PCA of contributions in all three addition nets yield two significant components. In each  of the three nets, the component scores form three clusters, representing the three correct  answers. In all three nets, the first component distinguishes greater than from less than  answers and places equal to answers in the middle; the second component distinguishes  Analyzing Cross-Connected Networks 1121  equal to from unequal to answers. The primary role of the hidden unit in these nets is to  distinguish equality from inequality. The hidden unit is not required to perform addition  per se in these nets, which have additive activation functions.  6.2 RESULTS FOR MULTIPLICATION  PCA applied to the contributions in the three multiplication nets yields from 3 to 4  significant components. Plots of rotated component scores show that the first component  separates greater than from less than outputs, placing equal to outputs in the middle.  Other components further differentiate the problems in these categories into several  smaller groups that are related to the particular values being multiplied. Rotated  component loadings indicate that component 1 is associated not only with contributions  coming from the bias unit and the input units, but also with contributions from some  hidden units. This underscores the need for hidden units to capture the non-linearities  inherent to multiplication.  6.3 RESULTS FOR BOTH ADDITION AND MULTIPLICATION  PCA of contributions yields three components in each of the three nets taught to do both  addition and multiplication. In addition to the familiar distinctions between greater than,  less than, and equal to outputs found in nets doing either addition or multiplication, it is  of interest to determine whether nets doing both operations distinguish between adding  and multiplying.  Figure 1 shows the rotated component scores for net 1. Components 1 and 2 (accounting  for 30.2% and 21.9% of the variance, respectively) together distinguish greater than  answers from the rest. Component 3, accounting for 20.2% of the variance, separates  equal to answers from less than answers and multiplication from addition for greater than  answers. Together, components 2 and 3 separate multiplication from addition for less than  answers. Results for the other two nets learning both multiplication and addition  comparisons are essentially similar to those for net 1.  2  1  2 1 2 o -1 _ 2  -1 0  Componen  -3  Component 3  Figure 1. Rotated component scores for a net doing both addition and multiplication.  6.4 DISCUSSION OF COMPARATIVE ARITHMETIC  As with continuous XOR, there is considerable variation among networks learning  comparative arithmetic problems. Varying numbers of hidden units are recruited by the  networks and different types of components emerge from PCA of network contributions.  In some cases, clear roles can be assigned to particular components, but in other cases,  separation of input patterns relies on interactions among the various components.  1122 Shultz and Elman  Yet with all of this variation, it is apparent that the nets learn to separate arithmetic  problems according to features afforded by the training set. Nets learning either addition or  multiplication differentiate the problems according to answer types: greater than, less  than, and equal to. Nets learning both arithmetic operations supplement these answer  distinctions with the operational distinction between adding and multiplying.  7 APPLICATION TO THE TWO-SPIRALS PROBLEM  We next apply contribution analysis to a particularly difficult discrimination problem  requiring a relatively large number of hidden units. The two-spirals problem requires the  net to distinguish between two interlocking spirals that wrap around their origin three  times. The standard version of this problem has two sets of 97 continuous-valued x, y  pairs, each set representing one of the spirals. The difficulty of the two-spirals problem is  underscored by the finding that standard back-propagation nets are unable to learn it  (Wieland, unpublished, cited in Fahlman & Lebiere, 1990). The best success to date on  the two-spirals problem was reported with cascade-correlation nets, which learned in an  average of 1700 epochs while recruiting from 12 to 19 hidden units (Fahlman & Lebiere,  1990). The relative difficulty of the two-spirals problem is undoubtedly due to its high  degree of non-linearity. It suited our need for a relatively difficult, but fairly well  understood problem on which to apply contribution analysis. We ran three nets using the  194 continuous x, y pairs as inputs and a single sigmoid output unit, signaling -0.5 for  spiral 1 and 0.5 for spiral 2.  Because of the relative difficulty of interpreting plots of component scores for this  problem, we focus primarily on the extreme component scores, defined as less than -1 or  greater than 1. Those x, y input pairs with extreme component scores on the first two  components for net 1 are plotted in Figure 2 as filled points on the two spirals. There are  separate plots for the positive and negative ends of each of the two components. The filled  points in each quadrant of Figure 2 define a shape resembling a tilted hourglass covering  approximately one-half of the spirals. The positive end of component 1 can be seen to  focus on the northeast sector of spiral 1 and the southwest sector of spiral 2. The negative  end of component 1 has an opposite focus on the northeast sector of spiral 2 and the  southwest sector of spiral 1. Component 2 does precisely the opposite of component 1:  its positive end deals with the southeast sector of spiral 1 and the northwest sector of  spiral 2 and its negative end deals with the southeast sector of spiral 2 and the northwest  sector of spiral 1. Comparable plots for the other two nets show this same hourglass  shape, but in a different orientation.  The networks appear to be exploiting the symmetries of the two spirals in reaching a  solution. Examination of Figure 2 reveals the essential symmetries of the problem. For  each x, y pair, there exists a corresponding -x, -y pair 180 degrees opposite and lying on  the other spiral. Networks learn to treat these mirror image points similarly, as revealed  by the fact that the plots of extreme component scores in Figures 2 are perfectly  symmetrical across the two spirals. If a point on one spiral is plotted, then so is the  corresponding point on the other spiral, 180 degrees opposite and at the same distance out  from the center of the spirals. If a mined network learns that a given x, y pair is on spiral  1, then it also seems to know that the -x, -y pair is on spiral 2. Thus, it make good sense  for the network to represent these opposing pairs similarly.  Recall that contributions are scaled by the sign of their targets, so that all of the products  of sending activations and output weights for spiral 1 are multiplied by -1. This is to  ensure that contributions bring output unit activations close to their targets in proportion  Analyzing Cross-Connected Networks 1123  to the size of the contribution. Ignoring this scaling by target, the networks possess  sufficient information to separate the two spirals even though they represent points of the  two spirals in similar fashion. The plot of the extreme component scores in Figure 2  suggests that the critical information for separating the two spirals derives mainly from  the signs of the input activations.  Because scaling contributions by the sign of the output target appears to obscure a full  picture of network solutions to the two-spirals problem, there may be some value in  using unscaled contributions in network analysis. Use of unscaled contributions also  could be justified on the grounds that the net has no knowledge of targets as it represents  a particular problem; target information is only used in the error correction process. A  disadvantage of using unscaled contributions is that one cannot distinguish contributions  that facilitate vs. contributions that inhibit reaching a relatively error free solution.  The symmetry of these network representations suggests a level of systematicity that is,  on some accounts, not supposed to be possible in neural nets (Fodor & Pylyshyn, 1988).  Whether this representational symmetry reflects systematicity in performance is another  matter. One empirical prediction would be that as a net leams that x, y is on one spiral, it  also learns at about the same time that -x, -y is on the other spiral. If confirmed, this  would demonstrate a clear case of systematic cognition in neural nets.  8 GENERAL DISCUSSION  Performing PCA on network contributions is here shown to be a useful technique for  understanding the performance of networks constructed by the cascade-correlation learning  algorithm. Because cascade-correlation nets typically possess multiple hidden layers and  are fully cross connected, they are difficult to analyze with more standard methods  emphasizing activation patterns on the hidden units alone. Examination of their weight  patterns is also problematic, particularly in larger networks, because of the highly  distributed nature of the net's representations.  Analyzing contributions, in contrast to either hidden unit activations or weights, is a  naturally appealing solution. Contributions capture the influence coming into output  units both from adjacent hidden units and from distant, cross connected hidden and input  units. Moreover, because contributions include both sending activations and connecting  weights, they are not unduly sensitive to one at the expense of the other.  In the three domains examined in the present paper, PCA of the network contributions  both confirm some expected results and provide new insights into network performance.  In all cases examined, the nets succeed in drawing all of the important distinctions in their  representations that are afforded by the training patterns, whether these distinctions  concern the type of output or the operation being performed on the input. In combination  with further experimentation and analysis of network weights and activation patterns, this  technique could help to provide an account of how networks accomplish whatever it is  they leam to accomplish.  It might be of interest to apply the present technique at various points in the leaming  process to obtain a developmental trace of network performance. Would all networks  learning under the same constraints progress through the same stages of development, in  terms of the problem distinctions they are able to make? This would be of particular  interest to network simulations of human cognitive development, which has been claimed  to be stage-like in its progressions.  1124 Shultz and Elman  I .  commonertl I 8 - Oomponenl 1  The present technique could also be useful in predicting the results of lesioning  experiments on neural nets. If the role of a hidden unit can be identified by its association  with a particular principal component, then it could be predicted that lesioning this unit  would impair the function served by the component.  Acknowledgments  This research was supported by the Natural Sciences and Engineering Research Council of  Canada and the MacArthur Foundation. Helpful comments were provided by Scott  Fahlman, Denis Mareschal, Yuriko Oshima-Takane, and Sheldon Tetewsky.  References  Cattell, R. B. (1966). The scree test for the number of factors. Multivariate Behavioral  Research, 1,245-276.  Elman, J. L. (1989). Representation and structure in connectionist models. CRL  Technical Report 8903, Center for Research in Language, University of California at  San Diego.  Elman, J. L. (1990). Finding structure in time. Cognitive Science, 14, 179-21l.  Fahlman, S. E., & Lebiere, C. (1990.) The Cascade-Correlation learning architecture. In  D. Toureky (Ed.), Advances in neural information processing systems 2, (pp. 524-  532). Mountain View, CA: Morgan Kaufmann.  Flury, B. (1988). Common principal components and related multivariate models. New  York: Wesley.  Fodor, J., & Pylyshyn, Z. (1988). Connectionism and cognitive architecture: A critical  analysis. Cognition, 28, 3-71.  Hinton, G. E., & Sejnowski, T. J. (1986). Learning and relearning in Bolmann  machines. In D. E. Rumelhart & J. L. McClelland (Eds.), Parallel distributed  processing: Explorations in the microstructure of cognition, Volume I: Foundations,  pp. 282-317. Cainbridge, MA: MIT Press.  Lang, K. J., & Witbrock, M. J. (1988). Learning to tell two spirals apart. In D.  Toureky, G. Hinton, & T. Sejnowski (Eds)., Proceedings of the Connectionist  Models Summer School, (pp. 52-59). Mountain View, CA: Morgan Kaufmann.  McClelland, J. L. (1989). Parallel distributed processing: Implications for cognition and  development. In Morris, R. G. M. (Ed.), Parallel distributed processing: Implications  for psychology and neurobiology, pp. 8-45. Oxford University Press.  Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). Learning internal  representations by error propagation. In D. E. Rumelhart & J. L. McClelland (Eds.),  Parallel distributed processing: Explorations in the microstructure of cognition,  Volume I: Foundations, pp. 318-362. Cambridge, MA: MIT Press.  Sanger, D. (1989). Contribution analysis: A technique for assigning responsibilities to  hidden units in connectionist networks. Connection Science, 1, 115-138.  Sejnowski, T. J., & Rosenberg, C. R. (1987). Parallel networks that learn to pronounce  English text. Complex Systems, 1, 145-168.  Shul, T. R., Schmidt, W. C., Buckingham, D., & Mareschal, D. (In press). Modeling  cognitive development with a generafive connectionist algorithm. In G. Halford & T.  Simon (Eds.), Developing cognitive competence: New approaches to process  modeling. Hillsdale, N J: Erlbaum.  8 ' - cmponen!  8 ' component 2  o.. ooo  o /,,,. ao %.o  %%   ! ""'""'"'"'   ... .... ot.o o  :Figure 2. ExLreme rotated component scorns for a net on the two-spirals problem.  
Monte Carlo Matrix Inversion and  Reinforcement Learning  Andrew Barto and Michael Duff  Computer Science Department  University of Massachusetts  Amherst, MA 01003  Abstract  We describe the relationship between certain reinforcement learn-  ing (RL)methods based on dynamic programming (DP)and a class  of unorthodox Monte Carlo methods for solving systems of linear  equations proposed in the 1950's. These methods recast the solu-  tion of the linear system as the expected vlue of a statistic suitably  defined over sample paths of a Markov chain. The significance of  our observations lies in arguments (Curriss, 1954) that these Monte  Carlo methods scale better with respect to state-space size than do  standard, iterative techniques for solving systems of linear equa-  tions. This analysis also establishes convergence rate estimates.  Because methods used in RL systems for approximating the evalu-  ation function of a fixed control policy also approximate solutions  to systems of linear equations, the connection to these Monte Carlo  methods establishes that algorithms very similar to TD algorithms  (Sutton, 1988) are asymptotically more efficient in a precise sense  than other methods for ewluating policies. Further, all DP-based  RL methods have some of the properties of these Monte Carlo al-  gorithms, which suggests that although RL is often perceived to  be slow, for sufficiently large problems, it may in fact be more ef-  ficient than other known classes of methods capable of producing  the same results.  687  688 Barto and Duff  Introduction  Consider a system whose dynamics are described by a finite state Markov chain with  transition matrix P, and suppose that at each time step, in addition to making a  transition from state zt - i to zt+ - j with probability pi, the system produces  a randomly determined reward, rt+, whose expected value is R/. The evaluation  function, Ix, maps states to their expected, infinite-horizon discounted returns:  t----0  It is well known that V uniquely satifies a linear system of equations describing  local consistency:  or  (1)  The problem of computing or estimating V is interesting and important in its  own right, but perhaps more significantly, it arises as a (rather computationally-  burdensome) step in certain techniques for solving Markov Decision Problems. In  each iteration of Policy-Iteration (Howard, 1960), for example, one must determine  the evaluation function associated with some fixed control policy, a policy that  improves with each iteration.  Methods for solving (1) include standard iterative techniques and their variants--  successive approximation (Jacobi or Gauss-Seidel versions), successive over-  relaxation, etc. They also include some of the algorithms used in reinforcement  learning (RL) systems, such as the family of TD algorithms (Sutton, 1988). Here  we describe the relationship between the latter methods and a class of unorthodox  Monte Carlo methods for solving systems of linear equations proposed in the 1950's.  These methods recast the solution of the linear system as the expected value of a  statistic suitably defined over sample paths of a Markov chain.  The significance of our observations lies in arguments (Curriss, 1954) that these  Monte Carlo methods scale better with respect to state-space size than do stan-  dard, iterative techniques for solving systems of linear equations. This analysis also  establishes convergence rate estimates. Applying this analysis to particular mem-  bers of the family of TD algorithms (Sutton, 1988) provides insight into the scaling  properties of the TD family as a whole and the reasons that TD methods can be  effective for problems with very large state sets, such as in the backgammon player  of Tesauro (Tesauro, 1992).  Further, all DP-based RL methods have some of the properties of these Monte  Carlo algorithms, which suggests that although RL is often slow, for large problems  (Markov Decision Problems with large numbers of states) it is in fact far more prac-  tical than other known methods capable of producing the same results. First, like  many RL methods, the Monte Carlo algorithms do not require explicit knowledge  of the transition matrix, P. Second, unlike standard methods for solving systems  of linear equations, the Monte Carlo algorithms can approximate the solution for  some variables without expending the computational effort required to approximate  Monte Carlo Matrix Inversion and Reinforcement Learning 689  the solution for all of the variables. In this respect, they are similar to DP-based  RL algorithms that approximate solutions to Markovian decision processes through  repeated trials of simulated or actual control, thus tending to focus computation  onto regions of the state space that are likely to be relevant in actual control (Barto  at. al., 1991).  This paper begins with a condensed summary of Monte Carlo algorithms for solv-  ing systems of linear equations. We show that for the problem of determining an  evaluation function, they reduce to simple, practical implementations. Next, we  recall arguments (Curriss, 1954) regarding the scaling properties of Monte Carlo  methods compared to iterative methods. Finally, we conclude with a discussion of  the implications of the Monte Carlo technique for certain algorithms useful in RL  systems.  2  Monte Carlo Methods for Solving Systems of Linear  Equations  The Monte Carlo approach may be motivated by considering the statistical evalua-  tion of a simple sum, k ak. If {p} denotes a set of values for a probability mass  function that is arbitrary (save for the requirement that a  0 imply p  0), then  (-) p, which may be interpreted as the expected value of a random  variable Z defined by Pr {Z = -} = p.  P  om equation (1) and the Neumann series representation of the inverse it is is clear  that  V: (I- 7P)-R = R +TPR +7PR +...  whose i * component is   ..+ +... (2)  i...i  and it is this series that we wish to evMuate by statisticM means.  A technique originated by Ulam and yon-Neumann (Forsythe & Leibler, 1950) uti-  zes an arbitrarily defined Markov chn with transition matrk  and state set  {1, 2, ..., n} (V is assumed to have n components). The chain begins in state i and  is aowed to make k transitions, where k is drawn from a geometric distribution  with parameter p,,p; i.e., Pr{k state transitions}  = p,,p(1 - p,,p). The Markov  chain, governed by P and the geometricay-distributed stopping criterion, defines  a mass function assigning probabty to every trajectory of every length starting in  state i, 0 = i0 = i  : i  .-.  t: it, and to each such trajectory there  corresponds a unique term in the sum (2).  For the case of value estimation, "Z" is defined by  - H=x  690 Barto and Duff  which for/5 = p and P, tee= ' becomes  The sample average of sampled values of Z is guaranteed to converge (as the number  of samples grows lrge) to state i's expected, infinite-horizon discounted return.  In Wasow's method (Wasow, 1952), the truncated Neumann series  E  i ixia i...i  is expressed as  plus the expected value of the sum of N random variables  Z, Z, ..., ZN, the intention being that  E(Z) = 7   PiqPqi " 'Pi_i.  i...i  Let trajectories of length N be generated by he Markov chain governed by P. A  given term 7kPiiPixi a .. 'pi_i is associated with a trajectories i  i  i    ..  i  i+  ...  iN whose first k + 1 states are i,i,...,i. The measure  of this set of trajectories is just iiixia '' 'Pi_xi. Thus, the random variables Zn,  k: 1, N are defined by  PiixPixia ' ' ' Pi_xi  If P: P, then the estimate becomes an average of sample truncated, discounted  returns: i =  + , +  + '" + 7N.  The Ulam/von Neumann approach may be reconciled with that of Wasow by pro-  cessing a given trajectory a posterJori, converting it into a set of terminated paths  consistent with any choice of stopping-state transition probabties. For example,  for a stopping state transition probabty of 1 - , a path of length k has proba-  bmty Each "prefix" of the observed path z(0)  z(1)  (2) .-. can  be weighted by the probabty of a path of corresponding length, resulting in an  estimate, V, that is the sampled, discounted return:  V =  7 R(n).  =0  3 Complexity  In (Curtiss, 1954) Curtiss establishes a theoretical comparison of the complexity  (number of multiplications) required by the Ulam/von Neumann method and a  stationary linear iterative process for computing a single component of the solution  to a system of linear equations. Curtiss develops an analytic formula for bounds  on the conditional mean and variance of the Monte-Carlo sample estimate, V, and  mean and variance of a sample path's time to absorption, then appeals to the  Monte Carlo Matrix Inversion and Reinforcement Learning 691  n  1000  9oo  8oo  7oo  6oo  5oo  4oo  3oo  2oo  lOO  0  0 100 200 300 400 500 600 700 800 900 1000  Figure 1: Break-even size of state space versus accuracy.  Central Limit Theorem to establish a 95%-confidence interval for the complexity of  his method to reduce the initial error by a given factor,/. x  For the case of value-estimation, Curtiss' formula for the Monte-Carlo complexity  may be written as  1 (1 + --) (3)  This is compared to the complexity of the iterative method, which for the value-  estimation problem takes the form of the classical dynamic programming recursion,  V (n+l) -- R " 7PV('):  WORKiterati. e ---  log____{) n2 +  1 + log '7  The iterative method's complexity has the form ar,  + r,, with a > 1, while the  Monte-Carlo complexity is independent of n--it is most sensitive to the amount of  error reduction desired, signified by . Thus, given a fixed amount of computation,  for large enough r,, the Monte-Carlo method is likely (with 95% confidence level) to  produce better estimates. The theoretical "break-even" points are plotted in Figure  1, and Figure 2 plots work versus state-space size for example values of -y and/.  'That is, for the iterative method, ( is aennea via [IV() - V(")II < llV) - v0)ll,  while for the Monte Carlo method, i is defined via [V()(i)- 'MI < 11v(o) _ v(0)l[,  where rM is the average over M sample V's.  692 Barto and Duff  50000 -  45000  4OOOO  35000  30000  25000  20000  15000  10000  5000  0  I  I  I  I  I  I  I  I  I  I  I  I  I  I  I   /  0 10 20 30 40 50 60 70 80 90  ...... Iterative  Monte Carlo  --- Gauss  I  1 oo  n  Figure 2: Work versus number of states for 7 = .5 and/ = .01.  4 Discussion  It was noted that the analytic complexity Curtiss develops is for the work required  to compute one component of a solution vector. In the worst case, all components  could be estimated by constructing r, separate, independent estimators. This would  multiply the Monte-Carlo complexity by a factor of r,, and its scahng supremacy  would be only marginally preserved. A more efficient approach would utilize data  obtained in the course of estimating one component to estimate other components  as well; Rubinstein (Rubinstein, 1981) decribes one way of doing this, using the  notion of "covering paths." Also, it should be mentioned that substituting more  sophisticated iterative methods, such as Gauss-Seidel, in place of the simple suc-  cessive approximation scheme considered here, serves only to improve the condition  number of the underlying iterative operator--the amount of computation required  by iterative methods remains an 2 + n, for some a > 1.  An attractive feature of the the analysis provided by Curtiss is that, in effect, it  yields information regarding the convergence rate of the method; that is, Equation  4 can be re-arranged in terms of . Figure 3 plots  versus work for example values  of  and n.  The simple Monte Carlo scheme considered here is practically identical to the  limiting case of TD-A with A equal to one (TD-1 differs in that its averaging of  sampled, discounted returns is weighted with recency). Ongoing work (Duff) ex-  plores the connection between TD-A (Sutton, 1988), for general values of A, and  Monte Carlo methods augmented by certain variance reduction techniques. Also,  Barnard (Barnard) has noted that TD-0 may be viewed as a stochastic approxima-  Monte Carlo Matrix Inversion and Reinforcement Leaming 693  1.0  0.9  0.8  0.7  0.6  0.5  0.4  0.3  0.2  0.1  0.0  Iterative  . ......  Monte Carlo ............  0 10000 20000 30000 40000 50000  Work  Figure 3: Error reduction versus work for -y = .9 and r, = 100.  tion method for solving (1).  On-line RL methods for solving Markov Decision Problems, such as Real-Time  Dynamic Programming (RTDP)(Barto et. al., 1991), share key features with the  Monte Carlo method. As with many algorithms, RTDP does not require explicit  knowledge of the transition matrix, P, and neither, of course, do the Monte Carlo  algorithms. RTDP approximates solutions to Markov Decision Problems through  repeated trials of simulated or actual control, focusing computation upon regions of  the state space likely to be relevant in actual control. This computational "focusing"  is also a feature of the Monte Carlo algorithms. While it is true that a focusing  of sorts is exhibited by Monte Carlo algorithms in an obvious way by virtue of  the fact that they can compute approximate solutions for single components of  solution vectors without exerting the computational labor required to compute all  solution components, a more subtle form of computational focusing also occurs.  Some of the terms in the Neumann series (2) may be very unimportant and need  not be represented in the statistical estimator at all. The Monte Carlo method's  stochastic estimation process achieves this automatically by, in effect, making the  appearance of the representative of a non-essential term a very rare event.  These correspondences--between TD-0 and stochastic approximation, between TD-   and Monte Carlo methods with variance reduction, between DP-based RL al-  gorithms for solving Markov Decision Problems and Monte Carlo algorithms --  together with the comparatively favorable scaling and convergence properties en-  joyed by the simple Monte Carlo method discussed in this paper, suggest that DP-  based RL methods like TD/stochastic-approximation or RTDP, though perceived  to be slow, may actually be advantageous for problems having a sufficiently large  694 Barto and Duff  number of states.  Acknowledgement  This material is based upon work supported by the National Science Foundation  under Grant ECS-9214866.  References  E. Barnard. Temporal-Difference Methods and Markov Models. Submitted for  publication.  A. Barto, S. Bradtke, & S. Singh. (1991) Real-Time Learning and Control Using  Asynchronous Dynamic Programming. Computer Science Department, University  of Massachusetts, Tech. Rept. 91-57.  J. Curriss. (1954) A Theoretical Comparison of the Efficiencies of Two Classical  Methods and a Monte Carlo Method for Computing One Component of the Solution  of a Set of Linear Algebraic Equations. In H. A. Mayer (ed.), Symposium on Monte  Carlo Methods, 191-233. New york, NY: Wiley.  M. Duff. A Control Variate Perspective for the Optimal Weighting of Truncated,  Corrected Returns. In Preparation.  S. Forsythe & K. Leibler. (1950) Matrix Inversion by a Monte Carlo Method. Math.  Tables Other Aids Cornput., 4:127-129.  R. Howard. (1960) Dynamic Programming and Markov Proceses. Cambridge, MA:  MIT Press.  R. Rubinstein. (1981) Simulation and the Monte Carlo Method. New York, NY:  Wiley.  R. Sutton. (1988) Learning to Predict by the Method of Temporal Differences.  Machine Learning 3:9-44.  G. Tesauro. (1992) Practical Issues in Temporal Difference Learning. Machine  Learning 8:257-277.  W. Wasow. (1952) A Note on the Inversion of Matrices by Random Walks. Math.  Tables Other Aids Cornput., 6:78-81.  
Solvable  Models of Artificial Neural  Networks  Sumio Watanabe  Information and Communication R & D Center  Ricoh Co., Ltd.  3-2-3, Shin-Yokohama, Kohoku-ku, Yokohama, 222 Japan  su nfio @ip e. rd c. ricoh. co.j p  Abstract  Solvable models of nonlinear lem'ning machines m'e proposed, and  learning in artificial neural networks is studied based on the theory  of ordinary differential equations. A learning algorithm is con-  structed, by which the optimal parameter can be found without  any recursive procedure. The solvable models enable us to analyze  the reason why experimental results by the error backpropagation  often contradict the statistical learning theory.  1 INTRODUCTION  Recent studies have shown that learning in artificial neural networks can be under-  stood as statistical parametric estimation using the maxbnum likelihood method  [1], and that their generalization abilities can be estimated using the statistical  asymptotic theory [2]. However, as is often reported, even when the number of  parameters is too large, the error for the testing sample is not so large as the theory  predicts. The reason for such inconsistency has not yet been clarified, because it is  difficult for the artificial neural network to find the global optimal parameter.  On the other hand, in order to analyze the nonlinear phenomena, exactly solvable  models have been playing a central role in mathematical physics, for example, the  K-dV equation, the Toda lattice, and some statistical models that satisfy the Yang-  423  424 Watanabe  Baxter equation[3].  This paper proposes the first solvable models in the nonlinear learning problem. We  consider simple three-layered neural networks, and show that the parameters from  the inputs to the hidden units determine the function space that is characterized  by a differential equation. This fact means that optimization of the parameters  is equivalent to optimization of the differential equation. Based on this property,  we construct a learning algorithm by which the optimal parameters can be found  without any recursive procedure. Experimental result using the proposed algorithm  shows that the maximum likelihood estimator is not always obtained by the error  backpropagation, and that the conventional statistical learning theory leaves much  to be improved.  2 The Basic Structure of Solvable Models  Let us consider a function fc,w(X) given by a simple neural network with 1 input  unit, H hidden units, and 1 output unit,  H  fc,w(x) -  ciTw(x), (1)  i=1  where both c -- {ci} and tv -- {wi} are parameters to be optimized, Tw,(x) is the  output of the i-th hidden unit.  ;e assume that {i(x) -- w,(X) is a set of independent functions in CH-class.  The following theorem is the start point of this paper.  Theorem 1  Iution is {T(x) and whose H-th order coefficient is I is uniquely given by  (Owg)(x) = (-1)" W,+ (g, 1, 2, ..., H  where I/VH is the H-th order Wronskian,  The H-th order differential equation whose fundamental system of so-  (2)  WH(gz, ;z, ..., T/) = det  ! ''.  (H-I) (H-l) -1)  1 2 ' ' ' (H H  For proof, see [4]. From this theoreln, we have the following corollary.  Corollary I Let g(x) be a CH-class function. Then the following conditions for  g(x) and w = {wi} are equivalent.  here eists a set c: (c,} such that  (2) = O.  Solvable Models of Artificial Neural Networks 425  Example 1 Let us consider a case, Ow,(X) = exp0v,x).  H  g(x) =  c, exp(w,x)  i=1  is equivalent to {D n + pD n- + p2D n-2 +...+pn}g(x) = 0, where D = (d/dx)  and a set {pi} is determined from {wi} by the relation,  H  z" +z "-1 +pz "- +...+p, = (z-w,) (Vz  c).  i:1  Example 2 (RBF) A hnction g(x) is given by radial bis functions,  H  g(x) =  ci exp{-(x- wi)2},  i=1  if and only if e-'{D n +pD n- +p2D n-2 +...+pn}(e'g(x)) = 0, where aset  {pi} is deternfined from {wi} by the relation,  H  z" +pz "- +p"-+...+p,, = (z- 2,) (Vz c).  i=1  Figure 1 shows a learning algorithm for the solvable models. When a target function  g(x) is given, let us consider the following function approximation problem.  H  (') =  m,() + ().  i=1  Learning in the neural network is optinfizing both {ci} and {wi} such that (x) is  minimized for some error function. From the definition of D, eq. (3) is equivMent  to (Dmg)(z) = (D)(x), where the term (Dwg)(z) is independent of ci. Therefore,  if we adopt IID11 s the error function to be minimized, {wi} is optimized by  minimizing IIDgll, independenfiy of {ci}, where I111  = f I()ld. After  is mininfized, we have (Dm.g)(z)  0, where w* is the optimized parameter. From  the corollary 1, there exists a set {c} such that g(x)   cm(z), where {c}  can be found using the ordinary least square method.  3 Solvable Models  For a general function Tw, the differential operator Dw does not always have such  a simple form as the above examples. In this section, we consider a linear operator  L such that the differential equation of Lw has a simple form.  Definition A neural network  cioi (x) is called solvable if there exist functions  a, b, and a linear operator L such that  (Lpw,)(x) = exp(a(tvi)x + bOvi)).  The following theore,n shows that the optimal parmneter of the solvable models can  be found using the same algo,'ithm as Figure 1.  426 Watanabe  ) is givenj  g(x) = Z c q)w (x) +(x)  i=l i  It is difficult to optimize wi  independently of ci  There exits C i s.t.  H  equiv. I  g(x) ilCi%?(X) r equiv.  I  Least Square Method .-- C i  optimized  " I  g(x)   d i %, (x) ( END )  i=l i [  D w g(x)= D w (x)  I  : minimized  W: optimized  g( ) -'  D, x =0  w  Figure 1: Structure of Solvable Models  Theorem 2 For a solvable model of a neural network, the .following conditions are  equivalent when wi  wj (i  j).  (1) There exist both {ci} and {wi} such that g(x) - iH= CiTw,(X).  (2) There exists {Pl} such that {D t + pD t- + p2D t-2 +... + pt}(Lg)(x)= O.  (3) For arbitrary Q > O, we define a sequence {yn} by y, = (Lg)(nQ). Then, there  exists {qi} such that Yn + qlYn- + q2Yn-2 + '" + qHYn-H = O.  Note that {IDwLgll 2 is a quadratic form for {pi}, xvhich is easily nfininfized by the  least square method. 'n lY' + qlY,- +'" + qHY,-Y{ 2 is also a quadratic form for  Theorem 3  relations.  z H q-pz H-  The sequences {wi), {Pi), and {qi} in the theorem 2 have the following  q- p2z H-2 q-  .. q- P,  z t + qz It- + q2z ti-2 + "' + qIt =  H  I-I(z -- a(wi)) (Vz  C),  i=1  H  1-I(z- exp(a(wi)Q)) (Vz  C).  i=1  For proofs of the above theorelns, see [5]. These theorems sho,v that, if {Pi} or  Solvable Models of Artificial Neural Networks 427  {qi) is optimized for a given function g(x), then {a(wi)} can be found as a set of  solutions of the algebraic equation.  Suppose that a target function g(x) is given. Then, from the above theorems,  the globally optimal parameter w* = {w r } can be found by mininfizing  independently of {ci}. Moreover, if the hnction a(w) is a one-to-one mapping, then  there exists w* uniquely without permutation of {w }, if and only if the quadratic  form II{D" + p DH- + "' + p-}gll 2 is not degenerate[4]. (Remk that, if it is  degenerate, we can use another neural network with the smMler number of hidden  units.)  Example 3 A neurM network without scMing  H  = + (4)  i=I  is solvable when (a)(x)  0 (a.e.), where  denotes the Fourier transform. Define  a linear operator L by (Lg)(x)= (g)(x)/(a)(x), then, it follows that  H  (Lf,,c)(X) =  ci exp(- bi x). (5)  i=1  By the Theorem 2, the optimM {hi} can be obtMned by using the differentiM or the  sequential equation.  Example 4 (MLP)  is solvable.  that  A three-layered perceptron  H  fb,c(X) --  Ci tan -1 (x q- bi ), (6)  ai  i--1  Define a linear operator L by (Lg)(x) = x. (g)(x), then, it folloxvs  H  (Lfb,c)(X) =  ciexp(-(ai + V/ bi)x + (ai, bi)) (x >_ 0). (7)  i=1  where o(ai, hi) is some function of ai and hi. Since the function tan -l(x) is mono-  tone increasing and bounded, we can expect that a neural network given by eq.  (6) has the same ability in the function approximation problem as the ordinary  three-layered perceptton using the sigmoid function, tanh(x).  Example 5 (Finite Wavelet Decomposition) A finite wavelet decomposition  H  f,c(X) = x + ), (8)  ai  i----1  is solvable when a(x) - (d/dx)"(1/(1 +/2)) (n _> 1). Define a linear operator L by  (g)(x) = x -n. (.T'g)(x) then, it follows that  H  (Lfb,c)(X) = y. ciexp(--(ai + vf bi)x + t(al,bi)) (x _> 0). (9)  i--1  428 Watanabe  where (ai, bi) is some function of ai and b. Note that a(x) is an analyzing wavelet,  and that this example shows a method how to optimize parameters for the finite  wavelet decomposition.  4 Learning Algorithm  We construct a learning algorithm for solvable models, as shown in Figure 1.  < <Learning Algorithm> >  (0) A target function g(x) is given.  (1) {Ym} is calculated by Ym - (Lg)(mQ).  (2) {qi} is optimized by mininfizing Y']m lyre + qlYm- q- q2Ym-2 q-'" q- qHYm-H[ 2.  (3) {Zi} is calculated by solving z H + qlz -1 + q2 zH-2 q-... q- qH -- O.  (4) {ivi} is determined by a(wi)= (1/Q)log zi.  (5) {ci} is optimized by minimizing Yj(g(xj)- Yicio,(xj)) 2.  Strictly speaking, g(x) should be given for arbitrary x. However, in the practical  application, if the number of training samples is sufficiently large so that (Lg)(x)  can be ahnost precisely approximated, this algorithm is available. In the third  procedure, to solve the algebraic equation, the DKA method is applied, for example.  5 Experimental Results and Discussion  5.1 The backpropagation and the proposed method  For experiments, we used a probability density function and a regression function  given by  i exp(- (y - h(x))2  Q(y]x)- 2 2 2 2 )  I tan_(x - 1/3 i tan_(x - 2/3  = ' + 0.02 )  where rr = 0.2. One hundred input samples were set at the same interval in [0,1),  and output samples were taken from the above conditional distribution.  Table 1 shows the relation between the number of hidden units, training errors,  and regression en'ors. In the table, the training error in the backpropagation shows  the square error obtained after 100,000 training cycles. The training error in the  proposed method shows the square error by the above algorithm. And the regres-  sion error shows the square error between the true regression curve h(x) and the  estimated curve.  Figure 2 shows the true and estimated regression lines: (0) the true regression  line and sample points, (1) the estimated regression line with 2 hidden units, by  the BP (the error backpropagation) after 100,000 training cycles, (2) the estimated  regression line with 12 hidden units, by the BP after 100,000 training cycles, (3) the  Solvable Models of Artificial Neural Networks 429  Table 1: Training errors and regression errors  Hidden Backpropagation Proposed Method  Units Training Regression Training Regression  2 4.1652 0.7698 4.0889 0.3301  4 3.3464 0.4152 3.8755 0.2653  6 3.3343 0.4227 3.5368 0.3730  8 3.3267 0.4189 3.2237 0.4297  10 3.3284 0.4260 3.2547 0.4413  12 3.3170 0.4312 3.1988 0.5810  estimated line with 2 hidden units by the proposed method, and (4) the estimated  line with 12 hidden units by the proposed method.  5.2 Discussion  When the number of hidden units was small, the training errors by the BP were  smaller, but the regression errors were larger. 5Yhen the number of hidden units  was taken to be larger, the training error by the BP didn't decrease so much as the  proposed method, and the regression error didn't increase so much as the proposed  method.  By the error backpropagation, parameters didn't reach the maximum likelihood  estimator, or they fell into local minima. However, when the number of hidden  units was large, the neural network without the maximum likelihood estimator  attained the better generalization. It seems that parameters in the local minima  were closer to the true parameter than the nmxinum likelihood estimator.  Theoretically, in the case of the layered neural networks, the maximum likelihood  estimator may not be subject to asymptotically normal distribution because the  Fisher information matrix may be degenerate. This can be one reason why the  experimental results contradict the ordinary statistical theory. Adding such a prob-  lem, the above experimental results show that the local minimum causes a strange  problem. In order to construct the more precise learning theory for the backprop-  agation neural network, and to choose the better parameter for generalization, we  maybe need a method to analyze learning and inference with a local minimum.  6 Conclusion  We have proposed solvable models of artificial neural networks, and studied their  learning structure. It has been shown by the experimental results that the proposed  method is useful in analysis of the neural network generalization problem.  430 Watanabe  (0) True Curve and Samples.  Sample error sum = 3.6874  (1) BP after 100,000 cycles  H = 2, Etrain -- 4.1652, E,.e9 = 0.7698  (3) Proposed Method  H = 2, Etrain = 4.0889, Ec9 = 0.3301  H: the number of hidden units  Etrain: the training error  E,.eg: the regression error  (2) BP after 100,000 cycles  H = 12, Et,.ain -- 3.3170, Ereg -- 0.4312  (4) Proposed Method  H = 12, Et,.,i, = 3.1988, Eea = 0.5810  Figure 2: Experimental Results  References  [1] H. White. (1989) Learning in artificial neural networks: a statistical perspective.  Neural Computation, 1,425-464.  [2] N.Murata, S.Yoshizawa, and S.-I.Amari.(1992) Learning Curves, Model Selection  and Complexity of Neural Networks. Advances in Neural Information Processing  Systems 5, San Mateo, Morgan I<aufinan, pp.607-614.  [3] R. J. Baxter. (1982) Exactly Solved Models in Statistical Mechanics, Academic  Press.  [4] E. A. Coddington. (1955) Theory of ordinary differential equations, the McGraw-  Hill Book Company, New York.  [5] S. Watanabe. (1993) Function approximation by neural networks and solution  spaces of differential equations. Submitted to Neural Networks.  
Constructive Learning Using Internal  Representation Conflicts  Laurens R. Leerink and Marwan A. Jabri  Systems Engineering & Design Automation Laboratory  Department of Electrical Engineering  The University of Sydney  Sydney, NSW 2006, Australia  Abstract  We present an algorithm for the training of feedforward and recur-  rent neural networks. It detects internal representation conflicts  and uses these conflicts in a constructive manner to add new neu-  rons to the network. The advantages are twofold: (1) starting with  a small network neurons are only allocated when required; (2) by  detecting and resolving internal conflicts at an early stage learning  time is reduced. Empirical results on two real-world problems sub-  stantiate the faster learning speed; when applied to the training  of a recurrent network on a well researched sequence recognition  task (the Reber grammar), training times are significantly less than  previously reported.  1 Introduction  Selecting the optimal network architecture for a specific application is a nontrivial  task, and several algorithms have been proposed to automate this process. The  first class of network adaptation algorithms start out with a redundant architecture  and proceed by pruning away seemingly unimportant weights (Sietsma and Dow,  1988; Le Cun et al, 1990). A second class of algorithms starts off with a sparse  architecture and grows the network to the complexity required by the problem.  Several algorithms have been proposed for growing feedforward networks. The  upstart algorithm of Frean (1990) and the cascade-correlation algorithm of Fahlman  (1990) are examples of this approach.  279  280 Leerink and Jabri  The cascade correlation algorithm has also been extended to recurrent networks  (Fahlman, 1991), and has been shown to produce good results. The recurrent  cascade-correlation (RCC) algorithm adds a fully connected layer to the network  after every step, in the process attempting to correlate the output of the additional  layer with the error. In contrast, our proposed algorithm uses the statistical prop-  erties of the weight adjustments produced during batch learning to add additional  units.  The RCC algorithm will be used as a baseline against which the performance of  our method will be compared. In a recent paper, Chen et al (1993) presented an  algorithm which adds one recurrent neuron with small weights every N epochs.  However, no significant improvement in training speed was reported over training  the corresponding fixed size network, and the algorithm will not be further analyzed.  To the authors knowledge little work besides the two mentioned papers have applied  constructive algorithms to recurrent networks.  In the majority of our empirical studies we have used partially recurrent neural  networks, and in this paper we will focus our attention on such networks. The mo-  tivation for the development of this algorithm partly stemmed from the long training  times experienced with the problems of phoneme and word recognition from contin-  uous speech. However, the algorithm is directly applicable to feedforward networks.  The same criteria and method used to add recurrent neurons to a recurrent network  can be used for adding neurons to any hidden layer of a feed-forward network.  2 Architecture  In a standard feedforward network, the outputs only depend on the current inputs,  the network architecture and the weights in the network. However, because of the  temporal nature of several applications, in particular speech recognition, it might  be necessary for the network to have a short term memory.  Partially recurrent networks, often referred to as Jordan (1989) or Elman (1990)  networks, are well suited to these problems. The architecture examined in this  paper is based on the work done by Robinson and Fallside (1991) who have applied  their recurrent error propagation network to continuous speech recognition.  A common feature of all partially recurrent networks is that there is a special set  of neurons called context units which receive feedback signals from a previous time  step. Let the values of the context units at time t be represented by C(t). During  normal operation the input vector at time t are applied to the input nodes I(t), and  during the feedforward calculation values are produced at both the output nodes  O(t + 1) and the context units C(t + 1). The values of the context units are then  copied back to the input layer for use as input in the following time step.  Several training algorithms exist for training partially recurrent neural networks,  but for tasks with large training sets the back-propagation through time (Werbos,  1990) is often used. This method is computationally efficient and does not use  any approximations in following the gradient. For an application where the time  information is spread over T. input patterns, the algorithm simply duplicates the  network T times - which results in a feedforward network that can be trained by a  variation of the standard backpropagation algorithm.  Constructive Learning Using Internal Representation Conflicts 281  3 The Algorithm  For partially recurrent networks consisting of input, output and context neurons,  the following assertions can be made:  The role of the context units in the network is to extract and store all  relevant prior information from the sequence pertaining to the classification  problem.  For weights entering context units the weight update values accumulated  during batch learning will eventually determine what context information  is stored in the unit (the sum of the weight update values is larger than the  initial random weights).  We assume that initially the number of context units in the network is  insufficient to implement this extraction and storage of information (we  start training with a small network). Then, at different moments in time  during the recognition of long temporal sequences, a context unit could be  required to preserve several different contexts.  These conflicts are manifested as distinct peaks in the distribution of the  weight update values during the epoch.  All but the last fact follows directly from the network architecture and requires no  further elaboration. The peaks in the distribution of the weight update values are a  result of the training algorithm attempting to adjust the value of the context units in  order to provide a context value that will resolve short-term memory requirements.  After the algorithm had been developed, it was discovered that this aspect of the  weight update values had been used in the past by Wynne-Jones (1992) and in  the Meiosis Networks of Hanson (1990). The method of Wynne-Jones (1992) in  particular is very closely related; in this case principal component analysis of the  weight updates and the Hessian matrix is used to detect oscillating nodes in fully  trained feed-forward networks. This aspect of backpropagation training is fully  discussed in Wynne-Jones (1992), to which reader is referred for further details.  The above assertions lead to the proposed training algorithm, which states that if  there are distinct maxima in the distribution of weight update values of the weights  entering a context unit, then this is an indication that the batch learning algorithm  requires this context unit for the storage of more than one context.  If this conflict can be resolved, the network can effectively store all the contexts  required, leading to a reduction in training time and potentially an increase in  performance.  The training algorithm is given below (the mode of the distribution is defined as  the number of distinct maxima):  For all context units {  Set N = modality of the distribution of weight update values;  If N > 1 then {  Add N-1 new context units to the network which are identical  (in terms of weighted inputs) to the current context unit.  282 Leerink and Jabri  Adjust each of these N context units (including the  original) by the weight update value determined by each  maxima (the average value of the mode).  Adjust all weights leaving these N context units so that the  addition of the new units do not affect any subsequent layers  (division by N). This ensures that the network retains all  previously acquired knowledge.  The main problem in the implementation of the above algorithm is the automatic  detection of significant maxima in the distribution of weight updates. A standard  statistical approach for the determination of the modality (the number of maxima)  of a distribution of noisy data is to fit a curve of a certain predetermined order to  the data. The maxima (and minima) are then found by setting the derivative to  zero. This method was found to be unsuitable mainly because after curve fitting it  was difficult to determine the significance of the detected peaks.  It was decided that only instances of bi-modality and tri-modality were to be iden-  tified, each corresponding to the addition of one or two context units. The following  heuristic was constructed:  Calculate the mean and standard deviation of the weight update values.  Obtain the maximum value in the distribution.  If there are any peaks larger than 60% of the maxima outside one standard  deviation of the mean, regard this as significant.  This heuristic provided adequate identification of the modalities. The distribution  was divided into three areas using the mean 4- the standard deviation as boundaries.  Depending on the number of maxima detected, the average within each area is used  to adjust the weights.  4 Discussion  According to our algorithm it follows that if at least one weight entering a context  unit has a multi-modal distribution, then that context unit is duplicated. In the  case where multi-modality is detected in more than one weight, context units were  added according to the highest modality.  Although this algorithm increases the computational load during training, the stan-  dard deviation of the weight updates rapidly decreases as the network converges.  The narrowing of the distribution makes it more difficult to determine the modal-  ity. In practice it was only found useful to apply the algorithm during the initial  training epochs, typically during the first 20.  During simulations in which strong multi-modalities were detected in certain nodes,  frequently the multi-modalities would persist in the newly created nodes. In this  Constructive Learning Using Internal Representation Conflicts 283  manner a strong bi-modality would cause one node to split into two, the two nodes  to grow to four, etc. This behaviour was prevented by disabling the splitting of  a node for a variable number of epochs after a multi-modality had been detected.  Disabling this behaviour for two epochs provided good results.  5 Simulation Results  The algorithm was evaluated empirically on two different tasks:  Phoneme recognition from continuous multi-speaker speech using the  TIMIT (Garofolo, 1988) acoustic-phonetic database.  Sequence Recognition: Learning a finite-state grammar from examples of  valid sequences.  For the phoneme recognition task the algorithm decreased training times by a factor  of 2 to 10, depending on the size of the network and the size of the training set.  The sequence recognition task has been studied by other researchers in the past, no-  tably Fahlman (1991). Fahlman compared the performance of the recurrent cascade  correlation (RCC) network with that of previous results by Cleeremans et al (1989)  who used an Elman (1990) network. It was concluded that the RCC algorithm  provides the same or better performance than the Elman network with less training  cycles on a smaller training set. Our simulations have shown that the recurrent  error propagation network of Robinson and Fallside (1991), when trained with our  constructive algorithm and a learning rate adaptation heuristic, can provide the  same performance as the RCC architecture in 40% fewer training epochs using a  training set of the same size. The resulting network has the same number of weights  as the minimum size RCC network which correctly solves this problem.  Constructive algorithms are often criticized in terms of efficiency, i.e. "Is the in-  crease in learning speed due to the algorithm or just the additional degrees of  freedom resulting from the added neuron and associated weights?". To address this  question several simulations were conducted on the speech recognition task, com-  paring the performance and learning time of a network with N fixed context units  to that of a network with small number of context units and growing a network  with a maximum of N context units. Results indicate that the constructive algo-  rithm consistently trains faster, even though both networks often have the same  final performance.  6 Summary  In this paper the statistical properties of the weight update values obtained during  the training of a simple recurrent network using back-propagation through time  have been examined. An algorithm has been presented for using these properties to  detect internal representation conflicts during training and to use this information  to add recurrent units to the network. Simulation results show that the algorithm  decreases training time compared to networks which have a fixed number of context  units. The algorithm has not been applied to feedforward networks, but can in  principle be added to all training algorithms that operate in batch mode.  284 Leerink and Jabri  References  Chen, D., Giles, C.L., Sun, G.Z., Chen, H.H., Lee, Y.C., Goudreau, M.W. (1993).  Constructive Learning of Recurrent Neural Networks. In 1993 IEEE International  Conference on Neural Networks, III:1196-1201. Piscataway, NJ: IEEE Press.  Cleeremans, A., Servan-Schreiber, D., and McClelland, J.L. (1989). Finite State  Automata and Simple Recurrent Networks. Neural Computation 1:372-381.  Elman, J.L. (1990). Finding Structure in Time. Cognitive Science 14:179-211.  Fahlman, S.E. and C. Lebiere (1990). The Cascade Correlation Learning Architec-  ture. In D. S. Touretzky (ed.), Advances in Neural Information Processing Systems  2, 524-532. San Mateo, CA: Morgan Kaufmann.  Fahlman, S.E. (1991). The Recurrent Cascade Correlation Architecture. Technical  Report CMU-CS-91-100. School of Computer Science, Carnegie Mellon University.  Frean, M. (1990). The Upstart Algorithm: A Method for Constructing and Training  Feedforward Neural Networks. Neural Computation 2:198-209.  Garofolo, J.S. (1988). Getting Started with the DARPA TIMIT CD-ROM: an  Acoustic Phonetic Continuous Speech Database. National Institute of Standards  and Technology (NIST), Gaithersburgh, Maryland.  Hanson, S.J. (1990). Meiosis Networks. In D. S. Touretzky (ed.), Advances in Neu-  ral Information Processing Systems 2, 533-541, San Mateo, CA: Morgan Kaufmann.  Jordan, M.I. (1989). Serial Order: A Parallel, Distributed Processing Approach. In  Advances in Connectionist Theory: Speech, eds. J.L. Elman and D.E. Rumelhart.  Hillsdale: Erlbaum.  Le Cun, Y., J.S. Denker, and S.A Solla (1990). Optimal Brain Damage. In D. S.  Touretzky (ed.), Advances in Neural Information Processing Systems 2, 598-605.  San Mateo, CA: Morgan Kaufmann.  Reber, A.S. (1967). Implicit learning of artificial grammars. Journal of Verbal  Learning and Verbal Behavior 5:855-863.  Robinson, A.J. and Fallside F. (1991). An error propagation network speech recog-  nition system. Computer Speech and Language 5:259-274.  Sietsma, J. and RJ.F Dow (1988). Neural Net Pruning-Why and How. In IEEE  International Conference on Neural Networks. (San Diego 1988), 1:325-333.  Wynne-Jones, M. (1992) Node Splitting: A Constructive Algorithm for Feed-  Forward Neural Networks. In D. S. Touretzky (ed.), Advances in Neural Infor-  mation Processing Systems d, 1072-1079. San Mateo, CA: Morgan Kaufmann.  Werbos, P.J. (1990). Backpropagation Through Time, How It Works and How to  Do It. Proceedings of the IEEE, 78:1550-1560.  
Fast Pruning Using Principal  Components  Asriel U. Levin Todd K. Leen and John E. Moody  Department of Computer Science and Engineering  Oregon Graduate Institute  P.O. Box 91000  Portland, OR 97291-1000  Abstract  We present a new algorithm for eliminating excess parameters and  improving network generalization after supervised training. The  method, "Principal Cmnponents Pruning (PCP)", is based on prin-  cipal component analysis of the node activations of successive layers  of the network. It is simple, cheap to implement, and effective. It  requires no network retraining, and does not involve calculating  the full Hessian of the cost function. Only the weight and the node  activity correlation matrices for each layer of nodes are required.  We demonstrate the efficacy of the method on a regression problem  using polynomial basis functions, and on an economic time series  prediction problem using a two-layer, feedforward network.  I Introduction  In supervised learning, a network is presented with a set of training exemplars  [u(k),y(k)], k = 1... N where u(k) is the ktn input and y(k) is the correspond-  ing output. The assumption is that there exists an underlying (possibly noisy)  functional relationship relating the outputs to the inputs  y- f(u,e)  where e denotes the noise. The aim of the learning process is to approximate this  relationship based on the the training set. The success of the learned approximation  36 Levin, Leen, and Moody  is judged by the ability of the network to approximate the outputs corresponding  to inputs it was not trained on.  Large networks have more functional flexibility than small networks, so are better  able to fit the training data. However large networks can have higher parameter  variance than small networks, resulting in poor generalization. The number of  parameters in a network is a crucial factor in it's ability to generalize.  No practical method exists for determining, a priori, the proper network size and  connectivity. A promising approach is to start with a large, fully-connected network  and through pruning or regularization, increase model bias in order to reduce model  variance and improve generalization.  Review of existing algorithms  In recent years, several methods have been proposed. Skeletonization (Mozer and  Smolensky, 1989) removes the neurons that have the least effect on the output  error. This is costly and does not take into account correlations between the neuron  activities. Eliminating small weights does not properly account for a weight's effect  on the output error. Optimal Brain Damage (OBD) (Le Cun et al., 1990) removes  those weights that least affect the training error based on a diagonal approximation  of the Hessian. The diagonal assumption is inaccurate and can lead to the removal  of the wrong weights. The method also requires retraining the pruned network,  which is computationally expensive. Optimal Brain Surgeon (OBS) (Hassibi et al.,  1992) removes the "diagonal" assumption but is impractical for large nets. Early  stopping monitors the error on a validation set and halts learning when this error  starts to increase. There is no guarantee that the learning curve passes through the  optimal point, and the final weight is sensitive to the learning dynamics. Weight  decay (ridge regression) adds a term to the objective function that penalizes large  weights. The proper coefficient for this term is not known a priori, so one must  perform several optimizations with different values, a cumbersome process.  We propose a new method for eliminating excess parameters and improving network  generalization. The method, "Principal Components Pruning (PCP)", is based on  principal component analysis (PCA) and is simple, cheap and effective.  2 Background and Motivation  PCA (Jolliffe, 1986) is a basic tool to reduce dimension by eliminating redundant  variables. In this procedure one transforms variables to a basis in which the covari-  ance is diagonal and then projects out the low variance directions.  While application of PCA to remove input variables is useful in some cases (Leen  et al., 1990), there is no guarantee that low variance variables have little effect on  error. We propose a saliency measure, based on PCA, that identifies those variables  that have the least effect on error. Our proposed Principal Components Pruning  algorithm applies this measure to obtain a simple and cheap pruning technique in  the context of supervised learning.  Fast Pruning Using Principal Components 37  Special Case: PCP in Linear Regression  In unbiased linear models, one can bound the bias introduced from pruning the  principal degrees of freedom in the model. We assume that the observed system  is described by a signal-plus-noise model with the signal generated by a function  linear in the weights:  y = Wou+e  , W  ,xp, and e is a zero mean additive noise. The  where u  P, y  "  regression model is  The input correlation matrix is E = - Y-k u(k)uT(k)   It is convenient to define coordinates in which E is diagonal A -- C T E C where C is  the matrix whose columns are the orthonormal eigenvectors of E. The transformed  input variables and weights are  - Cru and  = W C respectively, and the  model output can be rewritten as  =    It is straightforward to bound the increase in trning set error resulting from re-  moving subsets of the transformed input variable. The sum squared error is  1     [y() _ ()]r[y() _ ()1  k  Let t (k) denote the model's output when the last p - 1 components of (k) are set  to zero. By the triangle inequality   [y() _ ()]r[y() _ ()]  k  1    +  [()_ ()]r[()_ ()] ()  k  The second term in (1) bounds the increase in the training set error 1. This term  can be rewritten as  p  - - =  N  k i=/+1  where i denotes the i * column of  and Ai is the i * eigenvalue. The quantity  i Ai measures the effect of the i t eigen-coordinate on the output error; it serves  as our saliency measure for the weight i.  Relying on Aike's Final Prediction error (FPE) (Aike, 1970), the average test  set error for the original model is given by  N  where pm is the number of parameters in the model. If p - 1 principal components  are removed, then the expected test set is given by  N+Im  Jt[W] = N - l t(W)  1For y  R 1 , the inequality is replaced by an equality.  38 Levin, Leen, and Moody  If we assume that N >> !  m, the last equation implies that the optimal generaliza-  tion will be achieved if all principal components for which  2mI  N  are removed. For these eigen-coordinates the reduction in model variance will more  then compensate for the increase in training error, leaving a lower expected test set  error.  3 Proposed algorithm  The pruning algorithm for linear regression described in the previous section can be  extended to multilayer neural networks. A complete analysis of the effects on gen-  eralization performance of removing eigen-nodes in a nonlinear network is beyond  the scope of this short paper. However, it can be shown that removing eigen-nodes  with low saliency reduces the effective number of parameters (Moody, 1992) and  should usually improve generalization. Also, as will be discussed in the next section,  our PCP algorithm is related to the OBD and OBS pruning methods. As with all  pruning techniques and analyses of generalization, one must assume that the data  are drawn from a stationary distribution, so that the training set fairly represents  the distribution of data one can expect in the future.  Consider now a feedforward neural network, where each layer is of the form  yi = r[w'u'] = r[x']  Here, u i is the input, x  is the weighted sum of the input, F is a diagonal operator  consisting of the activation function of the neurons at the layer, and y is the output  of the layer.  1. A network is trained using a supervised (e.g. backpropagation) training  procedure.  2. Starting at the first layer, the correlation matrix E for the input vector to  the layer is calculated.  3. Principal components are ranked by their effect on the linear output of the  layer. 2  4. The effect of removing an eigennode is evaluated using a validation set.  Those that do not increase the validation error are deleted.  5. The weights of the layer are projected onto the 1 dimensional subspace  spanned by the significant eigenvectors  W --> W C C  where the columns of C are the eigenvectors of the correlation matrix.  6. The procedure continues until all layers are pruned.  2If we assume that --F is the sigmoidal operator, relying on its contraction property,  we have that the resulting output error is bounded by [le[[ <- [[W[[[[e[[ where el is  error observed at xi and IlWll is the norm of the matrices connecting it to the output.  Fast Pruning Using Principal Components 39  As seen, the algorithm proposed is easy and fast to implement. The matrix dimen-  sions are determined by the number of neurons in a layer and hence are manageable  even for very large networks. No retraining is required after pruning and the speed  of running the network after pruning is not affected.  Note: A finer scale approach to pruning should be used if there is a large variation  between Sij for different j. In this case, rather than examine 5Tsii in one piece,  the contribution of each 5ji could be examined individually and those weights  for which the contribution is small can be deleted.  4 Relation to Hessian-Based Methods  The effect of our PCP method is to reduce the rank of each layer of weights in a  network by the removal of the least salient eigen-nodes, which reduces the effective  number of parameters (Moody, 1992). This is in contrast to the OBD and OBS  methods which reduce the rank by eliminating actual weights. PCP differs further  from OBD and OBS in that it does not require that the network be trained to a  local minimum of the error.  In spite of these basic differences, the PCP method can be viewed as intermedi-  ate between OBD and OBS in terms of how it approximates the Hessian of the  error function. OBD uses a diagonal approximation, while OBS uses a linearized  approximation of the full Hessian. In contrast, PCP effectively prunes based upon  a block-diagonal approximation of the Hessian. A brief discussion follows.  In the special case of linear regression, the correlation matrix E is the full Hessian  of the squared error. 3 For a multilayer network with Q layers, let us denote the  numbers of units per layer as {pq: q = 0... Q}.4 The number of weights (including  biases) in each layer is bq - pq(Pq-1 q- 1), and the total number of weights in the  network is B Q  = Yq=l bq. The Hessian of the error function is a B x B matrix,  while the input correlation matrix for each of the units in layer q is a much simpler  (Pq-1 q- 1) x (Pq-1 q- 1) matrix. Each layer has associated with it pq identical  correlation matrices.  The combined set of these correlation matrices for all units in layers q - 1... Q of  the network serves as a linear, block-diagonal approximation to the full Hessian of  the nonlinear network. 5 This block-diagonal approximation has Q  Eq=l Pq(Pq--1 + 1) 2  Q  non-zero elements, compared to the [q= Pq(Pq-1 q- 1)] 2 elements of the full Hessian  (used by OBS) and the Q  Y']q=l Pq(Pq-1 q- 1) diagonal elements (used by OBD). Due  to its greater richness in approximating the Hessian, we expect that PCP is likely  to yield better generalization performance than OBD.  aThe correlation matrix and Hessian may differ by a numerical factor depending on the  normalization of the squared error. If the error function is defined as one half the average  squared error (ASE), then the equality holds.  4The inputs to the network constitute layer 0.  The derivation of this approximation will be presented elsewhere. However, the cor-  respondence can be understood in analogy with the special case of linear regression.  40 Levin, Leen, and Moody  ..i 0.75  0.5  0.25  a) -''- .0 .  -1[  . .''i' 0.75  0.5  0.25  -1 -0.75 35 -0.25  , -0.25  -1  Figure 1: a) Underlying function (solid), training data (points), and l0 th order polynomial  fit (dashed). b) Underlying function, training data, and pruned regression fit (dotted).  The computational complexities of the OBS, OBD, and PCP methods are  0 N pq(pq-1 ']- 1) , 0 N-pq(pq_l ']- 1) , 0 N(pq_l + 1) 2  q----1 q----1 q----1  respectively, where we assume that N _ B. The computational cost of PCP is  therefore significantly less than that of OBS and is similar to that of OBD.  5 Simulation Results  Regression With Polynomial Basis Functions  The analysis in section 2 is directly applicable to regression using a linear combina-  tion of basis functions  = W f(u) . One simply replaces u with the vector of basis  functions f(u).  We exercised our pruning technique on a univariate regression problem using mono-  mial basis functions f(u) ( 1, u, u 2 )T with n = 10. The underlying func-   ,  . . ,U n  tion was a sum of four sigmoids. Training and test data were generated by evaluating  the underlying function at 20 uniformly spaced points in the range -1 _ u _ + 1 and  adding gaussian noise. The underlying function, training data and the polynomial  fit are shown in figure l a.  The mean squared error on the training set was 0.00648. The test set mean squared  error, averaged over 9 test sets, was 0.0183 for the unpruned model. We removed  the eigenfunctions with the smallest saliencies 52 A. The lowest average test set  error of 0.0126 was reached when the trailing four eigenfunctions were removed. c.  Figure lb shows the pruned regression fit.  The FPE criterion suggested pruning the trailing three eigenfunctions. We note that  our example does not satisfy the assumption of an unbiased model, nor are the sample  sizes large enough for the FPE to be completely reliable.  Fast Pruning Using Principal Components 41  0.9  0.85  0.8  0.75  0.7  0.65  0.6  0.55  0.5  0  2 4  8 1'0 12  Prediction Horizon (month)  Figure 2: Prediction of the  IP index 1980 - 1990. The  solid line shows the perfor-  mance before pruning and  the dotted line the perfor-  mance after the application  of the PCP algorithm. The  results shown represent av-  erages over 11 runs with  the error bars representing  the standard deviation of  the spread.  Time Series Prediction with a Sigmoidal Network  We have applied the proposed algorithm to the task of predicting the Index of In-  dustrial Production (IP), which is one of the main gauges of U.S. economic activity.  We predict the rate of change in IP over a set of future horizons based on lagged  monthly observations of various macroeconomic and financial indicators (altogether  45 inputs). 7  Our standard benchmark is the rate of change in IP for January 1980 to January  1990 for models trained on January 1960 to December 1979. In all runs, we used two  layer networks with 10 tanh hidden nodes and 6 linear output nodes corresponding  to the various prediction horizons (1, 2, 3, 6, 9, and 12 months). The networks were  trained using stochastic backprop (which with this very noisy data set outperformed  more sophisticated gradient descent techniques). The test set results with and  without the PCP algorithm are shown in Figure 2.  Due to the significant noise and nonstationarity in the data, we found it beneficial  to employ both weight decay and early stopping during training. In the above runs,  the PCP algorithm was applied on top of these other regularization methods.  6 Conclusions and Extensions  Our "Principal Components Pruning (PCP)" algorithm is an efficient tool for re-  ducing the effective number of parameters of a network. It is likely to be useful when  there are correlations of signal activities. The method is substantially cheaper to  implement than OBS and is likely to yield better network performance than OBD. s  7Preliminary results on this problem have been described briefly in (Moody et al.,  1993), and a detailed account of this work will be presented elsewhere.  SSee section 4 for a discussion of the block-diagonal Hessian interpretation of our  method. A systematic empirical comparison of computational cost and resulting net-  work performance of PCP to other methods like OBD and OBS would be a worthwhile  undertaking.  42 Levin, Leen, and Moody  Furthermore, PCP can be used on top of any other regularization method, including  early stopping or weight decay.  Unlike OBD and OBS, PCP does not require that  the network be trained to a local minimum.  We are currently exploring nonlinear extensions of our linearized approach. These  involve computing a block-diagonal Hessian in which the block corresponding to  each unit differs from the correlation matrix for that layer by a nonlinear factor. The  analysis makes use of GPE (Moody, 1992) rather than FPE.  Acknowledgements  One of us (TKL) thanks Andreas Weigend for stimulating discussions that provided some  of the motivation for this work. AUL and JEM gratefully acknowledge the support of  the Advanced Research Projects Agency and the Office of Naval Research under grant  ONR N00014-92-J-4062. TKL acknowledges the support of the Electric Power Research  Institute under grant RP8015-2 and the Air Force Office of Scientific Research under grant  F49620-93-1-0253.  References  Akaike, H. (1970). Statistical predictor identification. Ann. Inst. Star. Math., 22:203.  Hassibi, B., Stork, D., and Wolff, G. (1992). Optimal brain surgeon and general network  pruning. Technical Report 9235, RICOH California Research Center, Menlo Park,  CA.  Jolliffe, I. T. (1986). Principal Component Analysis. Springer-Verlag.  Le Cun, Y., Denker, J., and Solla, S. (1990). Optimal brain damage. In Touretzky, D.,  editor, Advances in Neural Information Processing Systems, volume 2, pages 598-605,  Denver 1989. Morgan Kaufmann, San Mateo.  Leen, T. K., Rudnick, M., and Hammerstrom, D. (1990). Hebbian feature discovery  improves classifier efficiency. In Proceedings of the IEEE/INNS International Joint  Conference on Neural Networks, pages 1-51 to 1-56.  Moody, J. (1992). The effective number of parameters: An analysis of generalization and  regularization in nonlinear learning systems. In Moody, J., Hanson, S., and Lippman,  R., editors, Advances in Neural Information Processing Systems, volume 4, pages  847-854. Morgan Kaufmann.  Moody, J., Levin, A., and Rehfuss, S. (1993). Predicting the u.s. index of industrial  production. Neural Network World, 3:791-794. in Proceedings of Parallel Applications  in Statistics and Economics '93.  Mozer, M. and Smolensky, P. (1989). Skeletonization: A technique for trimming the fat  from a network via relevance assesment. In Touretzky, D., editor, Advances in Neural  Information Processing Systems, volume 1, pages 107-115. Morgan Kaufmann.  Weigend, A. S. and Rumelhart, D. E. (1991). Generalization through minimal networks  with application to forecasting. In Keramidas, E. M., editor, INTERFACE'91 - 23rd  Symposium on the Interface: Computing Science and Statistics, pages 362-370.  9(Weigend and Rumelhart, 1991) called the rank of the covariance matrix of the node  activities the "effective dimension of hidden units" and discussed it in the context of early  stopping.  
Classification of Multi-Spectral Pixels  by the  Binary Diamond Neural Network  Yehuda Salu  Department of Physics and CSTEA, Howard University, Washington, DC 20059  Abstract  A new neural network, the Binary Diamond, is presented and its use  as a classifier is demonstrated and evaluated. The network is of the  feed-forward type. It learns from examples in the 'one shot' mode,  and recruits new neurons as needed. It was tested on the problem of  pixel classification, and performed well. Possible applications of the  network in associative memories are outlined.  1 INTRODUCTION: CLASSIFICATION BY CLUES  Classification is a process by which an item is assigned to a class. Classification is  widely used in the animal kingdom. Identifying an item as food is classification.  Assigning words to objects, actions, feelings, and situations is classification. The  purpose of this work is to introduce a new neural network, the Binary Diamond,  which can be used as a general purpose classification tool. The design and  operational mode of the Binary Diamond are influenced by observations of the  underlying mechani.qms that take place in human classification processes.  An item to be classified consists of basic features. Any arbitrary combination of basic  features will be called a clue. Generally, an item will consist of many clues. Clues are  related not only to the items which contain them, but also to the classes. Each class,  that resides in the memory, has a list of clues which are associated with it. These clues  1143  1144 Salu  are the basic building blocks of the classification rules. A classification rule for a class  X would have the following general form:  Classification rule: If an item contains clue X 1, or clue X2,... , or clue Xn, and if it  does not contain clue X 1, nor clue X2, ..., nor clue X m, it is classified as belonging  to class X.  Clues X1,...,X n are the excitatory clues of class X, and clues X1,...,Xmare the  inhibitory clues of class X.  When classifying an item, we first identify the clues that it contains. We then match  these clues with the classification rules, and find the class of the item. It may happen  that a certain item satisfies classification rules of different classes. Some of the clues  match one class, while others match another. In such cases, a second set of rules,  disambiguation rules, are employed. These rules select one class out of those tagged  by the classification rules. The disambiguation rules rely on a hierarchy that exists  among the clues, a hierarchy that may vary from one classification scheme to another.  For example, in a certain hierarchy clue A is considered more reliable than clue B, if  it contains more features. In a different hierarchy scheme, the most frequent clue is  considered the most reliable. In the disambiguation process, the most reliable clue,  out of those that has actively contributed to the classification, is identified and serves  as the pointer to the selected class. This classification approach will be called  classification by clues (CBC).  The classification rules may be 'loaded' into our memory in two ways. F'irst, the  precise rules may be spelled out and recorded (e.g. 'A red light mean stop'). Second,  we may learn the classification rules from examples presented to us, utilizing innate  common sense learning mechanism. These mechanisms enable us to deduce from the  examples presented to us, what dues should serve in the classification rules of the  adequate classes, and what clues have no specificity, and should be ignored. For  example, by pointing to a red balloon and saying red, an infant may associate each of  the stimuli red and balloon as pointers to the word red. After presenting a red car,  and saying red, and presenting a green balloon and saying green, the infant has  enough information to deduce that the stimulus red is associated with the word red,  and the stimulus balloon should not be classified as red.  2 THE BINARY DIAMOND  2.1 STRUCTURE  In order to perform a CBC in a systematic way, all the clues that are present in the  item to be classified have to be identified first, and then compared against the  classification rules. The Binary Diamond enables carrying these tasks fast and  Classification of Multi-Spectral Pixels by the Binary Diamond Neural Network 1145  effectively. Assume that there are N different basic features in the environment. Each  feature can be assigned to a certain bit in an N dimensional binary vector. An item  will be represented by turning-on (from the default value of 0 to the value of 1) all the  bits that correspond to basic features, that are present in the item. The total number  of possible clues in this environment is at most 2 N. One way to represent these  possible clues is by a lattice, in which each possible clue is represented by one node.  The Binary Diamond is a lattice whose nodes represent clues. It is arranged in layers.  The first (bottom) layer has N nodes that represent the basic features in the  environment. The second layer has N-(N-I)/2 nodes that represent clues consisting of  2 basic features. The K'th layer has nodes that represent clues, which consist of K  basic features. Nodes from neighboring layers which represent clues that differ by  exactly one basic feature are connected by a line. Figure 1 is a diagram of the Binary  Diamond for N=4.  Figure 1: The Binary Diamond of order 4. The numbers inside the nodes are the  binary codes for the feature combination that the node represents, e.g 1 < = >  (0,0,0,1), 5< = >(0,1,0,1), 14 < = > (1,1,1,0), 15 < = > (1,1,1,1).  2.2 THE BINARY DIAMOND NEURAL NETWORK  The Binary Diamond can be turned into a feed-forward neural network by treating  each node as a neuron, and each line as a synapse leading from a neuron in a lower  layer (k) to a neuron in the higher layer (k + 1). All synaptic weights are set to 0.6, and  1146 Salu  all thresholds are set to 1, in a standard Pitts McCulloch neuron. The output of a  firing neuron is 1. An item is entered into the network by turning-on the neurons in  the first layer, that represent the basic features constituting this item. Signals  propagate forward one layer at a time tick, and neurons stay active for one time tick.  It is easy to verify that all the clues that are part of the input item, and only such clues,  will be turned on as the signals propagate in the network. In other words, the network  identifies all the clues in the item to be classified. An item consisting of M basic  features will activate neurons in the first M layers. The activated neuron in the M'th  layer is the representation of the entire item. As an example, consider the input item  with feature vector (0,1,1,1), using the notations of figure 1. It is entered by activating  neurons 1, 2, and 4 in the first layer. The signals will propagate to neurons 3, 5, 6, and  7, which represent all the clues that the input item contains.  2.3 INCORPORATING CLASS INFORMATION  Each neuron in the Binary Diamond represent a possle clue in the environment  spun by N basic features. When an item is entered in the first layer, all the dues that it  contains activate their representing neurons in the upper layers. This is the first step  in the classification process. Next, these dues have to point to the appropriate class,  based upon the classification rule. The possible classes are represented by neurons  outside of the Binary Diamond. Let x denote the neuron, outside the Binary  Diamond, that represents class X. An excitatory due X i (from the Binary Diamond)  will synapse onto x with a synaptic weight of 1. An inhibitory due Xj (in the Binary  Diamond) will synapse onto x with an inhibitory weight of -z, where z is a very large  number (larger than the maximum number of dues that may point to a class). This  arrangement ensures that the classification rule formulated above is carried out. In  cases of ambiguity, where a number of classes have been activated in the process, the  class that was activated by the clue in the highest layer will prevail. This clue has the  largest number of features, as compared with the other dues that actively participated  in the classification.  2.4 GROWING A BINARY DIAMOND  A possible limitation on the processes described in the two previous sections is that, if  there are many basic features in the environment, the 2 N nodes of the Binary  Diamond may be too much to handle. However, in practical situations, not all the  clues really occur, and there is no need to actually represent all of them by nodes.  One way of taking advantage of this simplifying situation is to grow the network one  event (a training item and its classification) at a time. At the benningo there is just  the first layer with N neurons, that represent the N basic features. Each event adds its  neurons to the network, in the exact positions that they would occupy in the regular  Classification of Multi-Spectral Pixels by the Binary Diamond Neural Network 1147  Binary Diamond. A clue that has already been represented in previous events, is not  duplicated. After the new clues of the event have been added to the network, the  information about the relationships between clues and classes is updated. This is done  for all the clues that are contained in the new event. The new neurons send synapses  to the neuron that represent the class of the current event. Neurons of the current  event, that took part in previous events, are checked for consistency. If they point to  other classes, their synapses are cut-off. They have just lost their specificity. It should  be noted that there is no need to present an event more than one time for it to be  correctly recorded ('one shot learning'). A new event will never adversely interfere  with previously recorded information. Neither the order of presenting the events, nor  repetitions in presenting them will affect the final structure of the network. Figure 2  illustrates how a Binary Diamond is grown. It encodes the information contained in  two events, each having three basic features, in an environment that has four basic  features. The first event belongs to class A, and the second to class B.  (0,1,1,1) -> A (1,1,1,0) -> B  Figure 2. Growing a Binary Diamond. Left: All the feature combinations of the three-  feature item (0,1,1,1) are represented by a 3'rd order Binary Diamond, which is grown  from the basic features represented by neurons 1, 2, and 4. All these combinations,  marked by a wavy background, are, for the time being, spedtic clues to class A.  Right: The three-feature item, (1,1,1,0) is added, as another 3'rd order Binary  Diamond. At this point, only neurons 1,3,5,and 7 represent spedtic clues to class A.  Neurons 8,10,12, and 14 represent specific clues to class B, and neurons 2,4, and 6  represent non-specific clues.  3 CLASSIFICATION OF MULTI-SPECTRAL PIXELS  3.1 THE PROBLEM  Spectral information of land pixels, which is collected by satellites, is used in  preparation of land cover maps and similar applications. Depending on the satellite  and its instrumentation, the spectral information consists of the intensities of several  1148 Salu  light bands, usually in the visible and infra-red ranges, which have been reflected from  the land pixels. One method of classification of such pixels relies on independent  knowledge of the land cover of some pixels in the scene. These classified pixels serve  as the training set for a classification algorithm. Once the algorithm is trained, it  classifies the rest of the pixels.  The actual problem described here involves testing the Binary Diamond in a pixel  classification problem. The tests were done on four scenes from the vicinity of  Washington DC, each consisting of approximately 22,003 pixels. The spectral  information of each pixel consisted of intensities of four spectral bands, as collected  by the Thematic Mapper of the Landsat 4 satellite. Ground covers of these scenes  were determined independently by ground and aerial surveys. There were 17 classes  of ground covers. Th following list gives the number of pixels per class in one of the  scenes. The distributions in the other scenes were similar.  1) water (28). 2) miscellaneous crops (299). 3) corn-standing (0). 4) corn-stubble  (349). 5) shrub-land (515). 6) grass/pasture (3,184). 7) soybean (125). 8) bare-  soil, dear land (535). 9) hardwood, canopy > 50% (10,169). 10) hardwood, canopy  < 50% (945). 11) conder forest (2,051). 12) mixed wood forest (616). 13)  asphalt (390). 14) single family housing (2,220). 15) multiple family housing (26).  16) industrial/commerdal (118). 17) bare soil-plowed field (382). Total 21,952.  3.2 METHODS  Approximately 10% of the pixels in each of the four scenes were randomly selected to  become a training set. Four Bin_a_ry Diamond networks were grown, based on these  four training sets. In the evaluation phase, each network classified each scene.  The intensity of the light in each band was discretized into 64 intervals. Each interval  was considered as a basic feature. So, each pixel was characterized by four basic  features (one for each band), out of 4x64=256 possible basic features. The first layer  of the Binary Diamond consisted of 256 neurons, representing these basic features.  Pixels of the training set were treated like events. They were presented sequentially,  one at a time, for one time, and the neurons that represent their clues were added to  the network, as explained in section 2.4. After the training phase, the rest of the pixels  were presented, and the network classified them. The results of this classification  were kept for comparisons with the observed ground cover values.  The same training sets were used to train two other classification algorithms; a  backpropagation neural network, and a nearest neighbor classifier. The back-  propagation network had four neurons in the input layer, each representing a spectral  band. It had seventeen neurons in the output layer, each representing a class, and a  hidden layer of ten neurons. The nearest neighbor classifier used the pixels of the  training set as models. The Euclidean distance between the feature vector of a pixel to  Classification of Multi-Spectral Pixels by the Binary Diamond Neural Network  be classified and each model pixel was computed. The pixel was classified according  to the class of its closest model.  1149  3.3 RESULTS  In auto-classification, the pixels of a scene are classified by an algorithm that was  trained using pixels from the same scene. In cross-classification, the classification of a  scene is done by an algorithm that was trained by pixels of another scene. It was found  that in both auto-classification and cross-classification, the results depend on the  consistency of the training set. Boundary pixels, which form the boundary (on the  ground) between two classes, may contain a combination of two ground cover classes.  If boundary pixels were excluded from the scene, the results of all the classification  methods improved significantly. Table 1 compares the overall performance of the  three classification methods in auto-classification and cross-classification, when only  boundary pixels were considered. Similar ordering of the classification methods was  obtained when all the pixels were considered.  1 2 3 4  1 83 58 71 74  2 41 78 50 44  3 49 48 75 52  4 54 44 57 76  Binary Diamond  1 2 3 4  1 83 41 46 61  2 27 73 28 17  3 43 38 62 35  4 52 36 39 70  Nearest Neighbor  1 2 3 4  1 73603364  2 25553826  3 33 38 52 37  4 48434260  Back-Propagation  Table 1: The percent of correctly classified pixels for the implementations of the  three methods, for non-boundary pixels only, as tested on the four maps. Column's  index is the training map, row's index is the testing map.  Table 2 compares the performances of the three methods class by class, as obtained in  the classification of the f'u'st scene. Similar results were obtained for the other scenes.  I= 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  BD 48 10 0 33 7 44 10 53 88 37 34 5 32 69 33 34 41  BDp 57 8 0 48 10 14 10 58 87 37 35 6 30 69 42 25 27  bNN 63 54 0 72 47 19 52 77 60 63 48 60 70 43 64 62 72  BP 68 5 0 68 0 1 11 66 80 74 11 1 26 45 54 52 27  Table 2: The percent of pixels from category I that have been classified as category I.  Auto-classification of scene 1. All the pixels are included. BD = results of Binary  Diamond where the feature vectors are in the standard Cartesian representation.  BDp = results of Binary Diamond where the feature vectors are in four dimensional  polar coordinates. bNN results of nearest neighbor, and BP of back-propagation.  1150 Salu  The overall performance of the Binary Diamond was better than those of the nearest  neighbor and the back-propagation classifiers. This was the case in auto-classification  and in cross-classification, in scenes that included all the pixels, and in scenes that  consisted only of non-boundary pixels. However, when comparing individual classes, it  was found that different classes may have different best classifiers. In practical  applications, the prices of correct or the wrong classifications of each class, as well as  the frequency of the classes in the environment will determine the optimal classifier.  All the networks recruited their neurons as needed, during the training phase. They  all started with 256 neurons in the first layer, and with seventeen neuron in the class  layer, outside the Binary Diamond. At the end of the training phase of the first scene,  The Binary Diamond consisted of 5,622 neurons, in four layers. This is a manageable  number, and it is much smaller than the maximum number of possle clues,  644_-224.  4  OTHER APPLICATIONS OF THE BINARY DIAMOND  The Binary Diamond, as presented here, was the core of a network that was used as a  classifier. Because of its special structure, the Binary Diamond can be used in other  related problems, such as in associative memories. In associative memory, a presented  due has to retrieve all the basic features of an associated item. ff we start from any  node in the Binary Diamond, and cascade down in the existing lines, we reach all the  basic features of this clue in the first layer. So, to retrieve an associated item, the  signals of the input clue have first to climb up the binary diamond till they reach a  node, which is the best generali?ation of this clue, and then to cascade down and to  activate the basic features of this generalization. The synaptic weights in the upward  direction can encode information about causality relationships and the frequency of  co-activations of the pre and post-synaptic neurons. This information can be used in  the retrieval of the most appropriate generalization to the given due. An associative  memory of this kind retrieves information in ways similar to human associative  retrieval (paper submitted).  References A reference list, as well as more details about pixel classification can be found in:  Classification of Multi-Spectral Image Data by the Binary Diamond Neural Network  and by Non-Parametric Pixel-by-Pixel Methods, by Yehuda Salu and James Tilton.  IEEE Transactions On Geoscience And Remote Sensin 1993 (in press).  
Hidden Markov Models for Human  Genes  Pierre Baldi *  Jet Propulsion Laboratory  California Institute of Technology  Pasadena, CA 91109  Soren Brunak  Center for Biological Sequence Analysis  The Technical University of Denmark  DK-2800 Lyngby, Denmark  Yves Chauvin 1  Net-ID, Inc.  601 Minnesota  San Francisco, CA 94107  Jacob Engelbrecht  Center for Biological Sequence Analysis  The Technical University of Denmark  DK-2800 Lyngby, Denmark  Anders Krogh  Electronics Institute  The Technical University of Denmark  DK-2800 Lyngby, Denmark  Abstract  Human genes are not continuous but rather consist of short cod-  ing regions (exons) interspersed with highly variable non-coding  regions (introns). We apply HMMs to the problem of modeling ex-  ons, introns and detecting splice sites in the human genome. Our  most interesting result so far is the detection of particular oscilla-  tory patterns, with a minimal period of roughly 10 nucleotides, that  seem to be characteristic of exon regions and may have significant  biological implications.  *and Division of Biology, California Institute of Technology.  rand Department of Psychology, Stanford University.  761  762 Baldi, Brunak, Chauvin, Engelbrecht, and Krogh  oxon intron  EXON  3' splice site  acceptor site  5' splice site  donor site  CONSENSUS SEQUENCES  I I  I    I  I NTCAG G  CCCCCCCC  AG GTAGT  Figure 1: Structure of eukaryotic genes (not to scale: introns are typically much  longer than exons).  1 INTRODUCTION  The genes of higher organisms are not continuous. Rather, they consist of relatively  short coding regions called exons interspersed with non-coding regions of highly vari-  able length called introns (Fig. 1). A complete gene may comprise as many as fifty  exons. Very often, exons encode discrete functional or structural units of proteins.  Prior to the translation of genes into proteins, a complex set of biochemical mecha-  nisms is responsible for the precise cutting of genes at the splice junctions, i.e. the  boundaries between introns and exons, and the subsequent removal and ligation  which results in the production of mature messenger RNA. The translation ma-  chinery of the cell operates directly onto the mRNA, converting a primary sequence  of nucleotides into the corresponding primary sequence of amino acids, according  to the rules of the genetic code. The genetic code converts every three contiguous  nucleotides, or codons, into one of the twenty antino acids (or into a stop signal).  Therefore the splicing process must be exceedingly precise since a shift of only one  base pair completely upsets the codon reading frame for translation. Many details  of the splicing process are not known; in particular it is not clear how acceptor sites  (i.e. intron/exon boundaries) and donor sites (i.e. exon/intron boundaries) are rec-  ognized with extremely high accuracy. Both acceptor and donor sites are signaled  by the existence of consensus sequences, i.e. short sequences of nucleotides which  are highly conserved across genes and, to some extent, across species. For instance,  Hidden Markov Models for Human Genes 763  most introns start with GT and terminate with /lG and additional patterns can be  detected in the proximity of the splice sites. The main problem with consensus  sequences, in addition to their variability, is that by themselves they are insufficient  for reliable splice site detection. Indeed, whereas exons are relatively short with an  average length around 150 nucleotides, introns are often much longer, with several  thousand of seemingly random nucleotides. Therefore numerous false positive con-  sensus signals are bound to occur inside the introns. The GT dinucleotide constitutes  roughly 5% of the dinucleotides in human DNA, but only a very small percentage of  these belongs to the splicing donor category, in the order of 1.5%. The dinucleotide  /IG constitutes roughly 7.5% of all the dinucleotides and only around 1% of these  function as splicing acceptor sites. In addition to consensus sequences at the splice  sites, there seem to exist a number of other weak signals (Senapathy (1989), Brunak  et al. (1992)) embedded in the 100 intron nucleotides upstream and downstream of  an exon. Partial experimental evidence seems also to suggest that the recognition  of the acceptor and donor boundaries of an exon may be a concerted process.  In connection with the current exponential growth of available DNA sequences and  the human genome project, it has become essential to be able to algorithmically  detect the boundaries between exons and introns and to parse entire genes. Unfor-  tunately, current available methods are far from performing at the level of accuracy  required for a systematic parsing of the entire human genome. Most likely, gene  parsing requires the statistical integration of several weak signals, some of which are  poorly known, over length scales of a few hundred nucleotides. Furthermore, initial  and terminal exons, lacking one of the splice sites, need to be treated separately.  2 HMMs FOR BIOLOGICAL PRIMARY SEQUENCES  The parsing problem has been tackled with classical statistical methods and more  recently using neural networks (Lapedes (1988), Brunak (1991)), with encouraging  results. Conventional neural networks, however, do not seem ideally suited to han-  dle the sort of elastic deformations introduced by evolutionary tinkering in genetic  sequences. Another trend in recent years, has been the casting of DNA and protein  sequences problems in terms of formal languages using context free grammars, au-  tomata and Hidden Markov Models (HMMs). The combination of machine learning  techniques which can take advantage of abundant data together with new flexible  representations appears particularly promising. HMMs in particular have been used  to model protein families and address a number of task such as multiple alignments,  classification and data base searches (Baldi et al. (1993) and (1994); Haussler et al.  (1993); Krogh et al. (1994a); and references therein). It is the success obtained with  this method on protein sequences and the ease with which it can handle insertions  and deletions that naturally suggests its application to the parsing problem.  In Krogh et al. (1994b), HMMs are applied to the problem of detecting coding/non-  coding regions in bacterial DNA (E. coli), which is characterized by the absence  of true introns (like other prokaryotes). Their approach leads to a HMM that  integrates both genie and intergenie regions, and can be used to locate genes fairly  reliably. A sirrfilar approach for human DNA, that is not based on HMMs, but  uses dynamic programming and neural networks to combine various gene finding  techniques, is described in Snyder and Stormo (1993). In this paper we take a  764 Baldi, Brunak, Chauvin, Engelbrecht, and Krogh  Main State Entropy Values  10 20 30 40 50 60 70 80 90 100110120130140150160170  Main State Position  Figure 2: Entropy of enfission distribution of main states.  first step towards parsing the human genome with HMMs by modeling exons (and  flanking intron regions).  As in the applications of HMMs to speech or protein modeling, we use left-right  architectures to model exon regions, intron regions or their boundaries. The ar-  chitectures typically consist of a backbone of main states flanked by a sequence of  delete states and a sequence of insert states, with the proper interconnections (see  Baldi et al. (1994) and Krogh et al. (1994) for more details and Fig. 4 below). The  data base used in the experiments to be described consists of roughly 2,000 human  internal exons, with the corresponding adjacent introns, extracted from release 78  of the GenBank data base. It is essential to remark that, unlike in the previous ex-  periments on protein families, the exons in the data base are not directly related by  evolution. As a result, insertions and deletions in the model should be interpreted  in terms of formal operations on the strings rather than evolutionary events.  3 EXPERIMENTS AND RESULTS  A number of different HMM training experiments have been carried using different  classes of sequences including exons only, flanked exons (with 50 or 100 nucleotides  on each side), introns only, flanked acceptor and flanked donor sites (with 100  nucleotides on each side) and slightly different architectures and learning algorithms.  Only a few relevant examples will be given here.  Hidden Markov Models for Human Genes 765  Figure 3: Emission distribution from main states.  In an early experiment, we trained a model of length 350 using 500 flanked ex-  OhS, with 100 nucleotides on each side, using gradient descent on the negative log-  likelihood (Baldi and Chauvin (1994)). The exons themselves had variable lengths  between 50 and 300. The entropy plot (Fig. 2), after 7 gradient descent training  cycles, reveals that the HMM has learned the acceptor site quite well but appears  to have some difficulties with the donor site. One possible contributing factor is  the high variability of the length of the training exons: the model seems to learn  two donor sites, one for short exons and one for the other exons. The most striking  pattern, however, is the greater smoothness of the entropy in the exon region. In  the exon region, the entropy profile is weakly oscillatory, with a period of about  20 base pairs. Discrimination and t-tests conducted on this model show that it  is definitely capable of discrinfinating exon regions, but. the confidence level is not  sufficient yet to reliably search entire genomes.  A slightly different model was subsequently trained using again 500 flanked exons,  with the length of the exons between 100 and 200 only. The probability of emitting  each one of the four nucleotides, across the main states of the model, are plotted  in Fig. 3, after the sixth gradient descent training cycle. Again the donor site  seems harder to learn than the acceptor site. Even more striking are the clear  766 Baldi, Brunak, Chauvin, Engelbrecht, and Krogh  Figure 4: The repeated segment of the tied model. Note that position 15 is identical  to position 5.  oscillatory patterns present in the exon region, characterized by a nfinimal period  of 10 nucleotides, with/l and G in phase and  and T in anti-phase.  The fact that the acceptor site is easier to learn could result from the fact that exons  in the training sequences are always flanked by exactly 100 nucleotides upstream.  To test this hypothesis, we trained a similar model using the same sequences but  in reverse order. Surprisingly, the model still learns the acceptor site (which is  now downstream from the donor site) much better than the donor site. The os-  ciliatory pattern in the reversed exon region is still present. The oscillations we  observe could also be an artifact of the method: for instance, when presented with  random training sequences, oscillatory HMM solutions could appear naturally as  local optima of the tra.ining procedure. To test this hypothesis, we trained a model  using random sequences of similar average composition as the exons and found no  distinct oscillatory patterns. We also checked that our data base of exons does not  correspond prevalently to c-helical domains of proteins.  To further test our findings, we trained a tied exon model with a hard-wired peri-  odicity of 10. The tied model consists of 14 identical segments of length 10 and 5  additional positions in the beginning and end of the model, making a total length  of 150. During training the segments are kept identical by tying of the parameters,  i.e. the parameters are constrained to be exactly the same throughout learning, as  in the weight sharing procedure for neural networks. The model was trained on 800  exon sequences of length between 100 and 200, and it was tested on 262 different  sequences. The parameters of the repeated segment, after training, are shown in  Fig. 4. Emission probabilities are represented by horizontal bars of corresponding  proportional length. There is a lot of structure in this segment. The most promi-  nent feature is the regular expression [^T][AT]G at position 12-14. (The regular  expression means "anything but T followed by A or T followed by G".) The same  pattern was often found at positions with very low entropy in the "standard models"  described above. In order to test the significa. nce, the tied model was compared to a  standard model of the same length. The average negative log-likelihood (NNL) they  both assign to the exon sequences and to random sequences of similar composition,  as well as their number of parameters are shown in the table below.  Hidden Markov Models for Human Genes 767  Model Scores NLL training NLL testing  parameters  Standard model  203.2 200.3 2550  with random seqs  Standard model  198.8 196.4 2550  with real seqs  Tied model  198.6 195.6 340  with real seqs  The tied model achieves a level of performance comparable to the standard model  but with significantly less free parameters, and therefore a period of 10 in the exons  seems to be a strong hypothesis. Note that the period of the pattern is not strictly  10, and we found almost equally good models with a built-in period of 9 or 11.  The type of left-to-right architecture we have used is not the ideal model of an exon,  because of the large length variations. It would be desirable to have a model with  a loop structure such that the segment can be entered as many times as necessary  for any given exon (see Krogh et al. (1994b) for a loop structure used for E. coli  DNA). This is one of the future lines of research.  4 CONCLUSION  In summary, we are applying HMMs and related methods to the problems of  exon/intron modeling and human genome parsing. Our preliminary results show  that acceptor sites are intrinsically easier to learn than donor sites and that very  simple HMM models alone are not sufficient for reliable genome parsing. Most im-  portantly, interesting statistical 10 base oscillatory patterns have been detected in  the exon regions. If confirmed, these patterns could have significant biological and  algorithmic implications. These patterns could be related to the superimposition  of several simultaneous codes (such as triplet code and frame code), and/or to the  way DNA is wrapped around histone molecules (Beckmann and Trifonov (1991)).  Presently, we are investigating their relationship to reading frame effects by training  several HMM models using a data base of exons with the same reading frame.  References  Beckmann, J.S. and Trifonov, E.N. (1991) Splice Junctions Follow a 205-base Lad-  der. PNAS USA, 88, 2380-2383.  Baldi, P., Chauvin, Y., Hunkapillcr, T. and McClure, M. A. (1994) Hidden Markov  Models of Biological Primary Sequence Information. PNAS USA, 91, 3, 1059-1063.  Baldi, P., Chauvin, Y., Hunkapillcr, T. and McClure, M. A. (1993) Hidden Markov  Models in Molecular Biology: New Algorithms and Applications. Advances in  Neural Information Processing Systems 5, Morgan Kaufmann, 747-754.  Baldi, P. and Chauvin, Y. (1994) Smooth On-Line Learning Algorithms for Hidden  Markov Models. Neural Computation, 6, 2, 305-316.  Brunak, S., Engelbrecht, J. and Knudsen, S. (1991) Prediction of Human mRNA  Donor and Acceptor Sites from the DNA Sequence. Journal of Molecular Biology,  220, 49-65.  768 Baldi, Bmnak, Chauvin, Engelbrecht, and Krogh  Engelbrecht, J., Knudsen, S. and Brunak S., (1992) G/C rich tract in 5' end of  human introns, Journal of Molecular Biology, 22]', 108-113.  Haussler, D., Krogh, A., Mian, I.S. and SjSlander, K. (1993) Protein Modeling  using Hidden Markov Models: Analysis of Globins, Proceedings of the Hawaii In-  ternational Conference on System Sciences, 1, IEEE Computer Society Press, Los  Alamitos, CA, 792-802.  Krogh, A., Brown, M., Mian, I. S., SjSlander, K. and Haussler, D. (1994a) Hid-  den Markov Models in Computational Biology: Applications to Protein Modeling.  Journal of Molecular Biology, 235, 1501-1531.  Krogh, A., Mian, I. S. and Haussler, D. (1994b) A Hidden Markov Model that Finds  Genes in E. Coli DNA, Technical Report UCSC-CRL-93-33, University of California  at Santa Cruz.  Laperies, A., Barnes, C., Burks, C., Farber, R. and Sirotkin, K. Application of Neu-  ral Networks and Other Machine Learning Algorithms to DNA Sequence Analysis.  In G.I. Bell and T.G. Mart, editors. The Procceedings of the Interface Between  Computation Science and Nucleic Acid Sequencing Workshop. Proceedings of the  Santa Fe Institute, volume VII, pages 157-182. Addison Wesley, Redwood City,  CA, 1988.  Senapathy, P., Shapiro, M.B., and Harris, N.L. (1990) Splice Junctions, Branch  Point Sites, and Exons: Sequence Statistics, Identification and Applications to  Genome Project. Patterns in Nucleic Acid Sequences, Academic Press, 252-278.  Snyder, E.E. and Stormo, G.D. (1993) Identification of coding regions in genomic  DNA sequences: an application of dynamic programming and neural networks.  Nucleic Acids Research, 21,607-613.  
Segmental Neural Net Optimization for Continuous Speech  Recognition  Ying Zhao  Richard Schwartz John Makhoul  BBN System and Technologies  70 Fawcett Street  Cambridge MA 02138  George Zavaliagkos  Abstract  Previously, we had developed the concept of a Segmental Neural Net (SNN) for  phonetic modeling in continuous speech recognition (CSR). This kind of neu-  ral network technology advanced the state-of-the-art of large-vocabulary CSR,  which employs Hidden Markov Models (HMM), for the ARPA 1000-word Re-  source Management corpus. More Recently, we started porting the neural net  system to a larger, more challenging corpus - the ARPA 20,000-word Wall Street  Journal (WSJ) corpus. During the porting, we explored the following research  directions to refine the system: i) training context-dependent models with a reg-  ularization method; ii) training SNN with projection pursuit; and ii) combining  different models into a hybrid system. When tested on both a development set  and an independent test set, the resulting neural net system alone yielded a per-  formance at the level of the HMM system, and the hybrid SNN/HMM system  achieved a consistent 10-15% word error reduction over the HMM system. This  paper describes our hybrid system, with emphasis on the optimization methods  employed.  1 INTRODUCTION  Hidden Mafi.ov Models (HMM) represent the state-of-the-art for large-vocabulary con-  tinuous speech recognition (CSR). Recently, neural network technology has been shown  to advance the state-of-the-art for CSR by integrating neural nets and HMMs [1,2]. In  principle, the advance is based on the fact that neural network modeling can avoid some  limitations of the HMM modeling, for example, the conditional-independence assumption  of HMMs and the fact that segmental features are hard to incorporate. Our work has been  based on the concept of a Segmental Neural Net (SNN) [2].  1059  1060 Zhao, Schwartz, Makhoul, and Zavaliagkos  A segmental neural network is a neural network that attempts to recognize a complete  phoneme segment as a single unit. Its basic structure is shown in Figure 1. The input  to the network is a fixed length representation of the speech segment, which is obtained  from the warping (quasi-linear sampling) of a variable length phoneme segment. If the  network is trained to minimize a least squares error or a cross entropy distortion measure,  the output of the network can be shown to be an estimate of the posterior probability of  the phoneme class given the input segment [3,4].  segment score  neural  network  warping  phonetic segment  Figure 1- The SNN model samples the frames and produces a single segment score.  Our initial SNN system comprised a set of one-layer sigmoidal nets. This system is trained  to minimize a cross entropy distortion measure by a quasi-Newton error minimization  algorithm. A variable length segment is warped into a fixed length of 5 input frames.  Since each frame includes 16 features, 14 mel cepstrum, power and difference of power,  an input to the neural network forms a 16 x 5 = 80 dimensional vector.  Previously, our experimental domain was the ARPA 1000-word Resource Management  (RM) Corpus, where we used 53 output phoneme classes. When tested on three independent  evaluation sets (Oct 89, Feb 91 and Sep 92), our system achieved a consistent 10-20%  word error rate reduction over the state-of-the-art HMM system [2].  2 THE WALL STREET JOURNAL CORPUS  After the final September 92 RM corpus evaluation, we ported our neural network system  to a larger corpus -- the Wall Street Joumal (WSJ) Corpus. The WSJ corpus consists  primarily of read speech, with a 5,000- to 20,000-word vocabulary. It is the current ARPA  speech recognition research corpus. Compared to the RM corpus, it is a more challenging  corpus for the neural net system due to the greater length of WSJ utterances and the  higher perplexity of the WSJ task. So we would expect greater difficulty in improving  performance on the WSJ corpus.  Segmental Neural Net Optimization for Continuous Speech Recognition 1061  3 TRAINING CONTEXT-DEPENDENT MODELS WITH  REGULARIZATION  3.1 WHY REGULARIZATION  In contrast to the context-independent modeling for the RM corpus, we are concentrating on  context-dependent modeling for the WSJ corpus. In context-dependent modeling, instead  of using a single neural net to recognize phonemes in all contexts, different neural networks  are used to recognize phonemes in different contexts. Because of the paucity of training  data for some context models, we found that we had an overfitting problem.  Regularization provides a class of smoothing techniques to ameliorate the overfitting prob-  lem [5]. We started using regularization in our initial one-layer sigmoidal neural network  system. The regularization term added here is to regulate how far the context-dependent  parameters can move away from their initial estimates, which are context-independent pa-  rameters. Tiffs is different from the usual weight decay technique in neural net literature,  and it is designed specifically for our problem. The objective function is shown below:  I  [,log(l f,)+'logf, + [,lg r lgr0 [ [ 2 (1)  Na i  c Regularization Term     v  Distortion measure Er(W)  where fi is the net output for class i, [[W[] is the Euclidean norm of all weights in all  the networks, []W0[[ is the initial estimate of weights from a context-independent neural  network, Na is the number of data points. X is the regularization parameter which controls  the tradeoff between the "smoothness" of the solution, as measured by ]]lg r - lgro]] 2, and  the deviation from the data as measured by the distortion.  The optimal ,, which gives the best generalization to a test set, can be estimated by  generalized cross-validation [5]. If the distortion measure as shown in (2)  1  Na. IIAW- bll 2 + AllWI[ 2 (2)  is a qu3dratic function in terms of network weights W, the optimal ) is that which gives  the minin,mn of a generalized cross-validation index V()0 [6]:  ' iiAO)_ bll 2  V(X =  I - Ttr(A(,X)) (3)  where A(X) = A(AWA+Na,XI)A w. V(,X) is an easily calculated function based on singular  value decomposition (SVD):  d 2  V() = ' (4)  where A = UDV T, singular decomposition of A, z = UTb. Figure 2 shows an example  plot of V(X). A typical optimal X has an inverse relation to the number of samples in each  class, indicating that  is gradually reduced with the presence of more data.  1062 Zhao, Schwartz, Makhoul, and Zavaliagkos  o  o o  o oo c o  oOOO o c  0 5'10-7 10^-6 1.5'10^-6 2.5'10^-6  lambda  Figure 2: A typical V()  Just as the linear least squares method can be generalized to a nonlinear least squares  problem by an itemfive procedure, so selecting the optimal value of the regularization  parameter in a quadratic error criterion can be generalized to a non-quadratic error criterion  itemfively. We developed an itemfive procedure to apply the cross-validation technique to  a non-quadratic error function, for example, the cross-entropy criterion Er(W) in (1) as  follows:  1. Compute distortion Er(W.) for an estimate W..  2. Compute gradient g, and Hessian H. of the distortion Er(W.).  3. Compute the singular value decomposition of H, = V D,, V,. Set z,, =  4. Evaluate a generalized cross-validation index V,(;) similar to (2) as follows, for a  range of 's and select the . that gives the minimum V..  Segmental Neural Net Optimization for Continuous Speech Recognition 1063  Na [Er(W.)-  (a.r? ,q  v. (:) = 2 (5)  dy  [Nd -- Ej d+NaA ]  5. Set W,+ = W, - (H, + Na,X,)-g,.  6. Go to I and iterate.  Note that ) is adjusted at each iteration. The final value of ), is taken as the optimal ).  Iterative regularization parameter selection shows that ) converges, for example, to o-'  from one of our experiments.  3.2 A TWO-LAYER NEURAL NETWORK SYSTEM WITH REGULARIZATION  We then extended our regularization work from the one-layer sigmoidal network system to  a two-layer sigmoidal network system. The first layer of the network works as a feature  extractor and is shared by all phonetic classes. Theoretically, in order to benefit from  its larger capability of representing phonetic segments, the number of hidden units of a  two-layer network should be much greater than the number of input dimensions. However,  a large number of hidden units can cause serious overfitting problems when the number of  training samples is less than the number of parameters for some context models. Therefore,  regularizatioo is more useful here. Because the second layer can be trained as a one-layer  net, the regularization techrdques we developed for a one-layer net can be applied here to  train the second layer.  In our implementation, a weighted least squares error measure was used at the output layer.  First, the weights for the two-layer system were initialized with random numbers between  -1 and 1. Fixing the weights for the second layer, we trained the first layer by using  gradient descent; then fixing the weights for the first layer, we trained the second layer by  linear least squares with a regularization term, without the sigmoidal function at the output.  We stopped after one iteration for our initial experiment.  4 TRAINING SNN WITH PROJECTION PURSUIT  4.1 WHY PROJECTION PURSUIT  As we described in the previous section, regularization is especially useful in training the  second layer of a two-layer network. In order to take advantage of the two-layer layer  structure, we want to train the first layer as well. However, once the number of the hidden  units is large, the number of weights in the first layer is huge, which makes the first layer  very difficult to train. Projection pursuit presents a useful technique to use a large hidden  layer but still keep the number of weights in the first layer as small as possible.  The original pojection prosuit is a nonparametric statistical technique to find interesting  low dimensional projections of high dimensional data sets [7]. The parametric version of  it, a projection pursuit learning network (PPLN) has a structure very similar to a two-layer  sigmoidal network network [7]. In a traditional two-layer neural network, the weights in  the first layer can be viewed as hyperplanes in the input space. It has been proposed that  a special function of the first layer is to partition the input space into cells through these  hyperplanes [8]. The second layer groups these cells together to form decision regions.  1064 Zhao, Schwartz, Makhoul, and Zavaliagkos  The accuracy or resolution of the decision regions is completely specified by the size and  density of the cells which is determined by the number and placement of the first layer  hyperplanes in the input space.  In a two-layer neural net, since the weights in the first layer can go anywhere, there are no  restrictions on the placement of these hyperplanes. In contrast, a projection pursuit learning  network restricts these hyperplanes in some major "interesting" directions. In other words,  hidden units are grouped into several distinct directions. Of course, with this grouping, the  number of cells in the input space is reduced somewhat. However, the interesting point  here is that this restriction does not reduce the number of cells asymptotically [7]. In other  words, grouping hidden units does not affect the number of cells much. Consequently, for  a fixed number of hidden units, the number of parameters in the first layer in a projection  pursuit learning network is much less than in a traditional neural network. Therefore, a  projection pursuit learning network is easier to train and generalizes better.  4.2 HOW TO TRAIN A PPLN  In our implementation, the distinct projection directions were shared by all context-  dependent models, and they were trained context-independently. We then trained these  direction parameters with back-propagation. The second layer was trained with regulariza-  tion. Iterations can go back and forth between the two layers.  5 COMBINATIONS OF DIFFERENT MODELS  In the last two sections, we talked about using regularization and projection pursuit to  optimize our neural network system. In this section, we will discuss another optimization  method, combining different models into a hybrid system. The combining method is based  on the N-best rescoring paradigm [2].  The N-best rescor;.qg paradigm is a mechanism that allows us to build a hybrid system by  combining different knowledge sources. For example, in the RM corpus, we successfully  combined the HMM system, te SNN system and word-pair grammar into a single hybrid  system which achieved the state-of-the-art. We have been using this N-best rescoring  paradigm to combine different models in the WSJ corpus as well. These different models  include SNN left context, right context, and diphone models, HMM models, and a language  model known as statistical grammar. We will show how to obtain a reasonable combination  of different systems from Bayes rule.  The goal is to compute P(,5']X), the probability of the sentence $ given the observation  sequence X. From Bayes rule,  P(xI$)  P(SlX)srr = P(S)  p(x)  P(aIS)   P(S)H P(a]p,c)  p----)  Segmental Neural Net Optimization for Continuous Speech Recognition 1065  where X is a sequence of acoustic features z in each phonetic segment; p and c is the  phoneme class and context for the segment, respectively. The following three approxima-  tions are used here:   P(XIS): I-[: P(zIS).   P(zl$) = P(zlp, c).   P(clz)= P(c).  Therefore, in a SNN system, we use the following approximation from Bayes rule:  P(SIX)jvjv  P(S) H P(PlZ'c)  . P(p[c)  where  P(S): Word grammar score.  H P(Pl z, c): Neural net score.  H P(plc): Phone grammar score.  These three scores together with I-IM scores are combined in the SNN/H hybrid  system.  6 EXPERIMENTAL RESULTS  Development Set Nov92 Test  HMM 11.0 8.5  Baseline SNN 11.7 -  Regflarization and Projection Pursuit SNN 11.2 9.1  Baseline SNN/HMM 10.3 7.7  Regularization and Projection Pursuit SNN/HMM 9.5 7.2  Table 1: Word Error Rates for 5K, Bigram Grammar  Development Set Nov93 Test  HMM 14.4 14.0  Regularization and Projection Pursuit SNN 14.6 -  Regularizafion and Projection Pursuit SNN/HMM 13.0 12.3  Table 2: Word Error Rates for 20K, Trigram Grammar  Speaker-independent CSR tests were performed on the 5,000-word (5K) and 20,000-word  (20K) ARPA Wall Street Journal corpus. Bigram and trigram statistical grammars were  used. The basic neural network structure consists of 80 inputs, 500 hidden units and 46  outputs. There are 125 projection directions in the first layer. Context models consist of  1066 Zhao, Schwartz, Makhoul, and Zavaliagkos  right context models and left diphone models. In the right context models, we used 46  different networks to recognize each phoneme in each of the different right contexts. In  the left diphone models, a segment input consisted of the first half segment of the current  phone plus the second half segment of the previous phone. Word error rates are shown in  Tables 1 and 2.  Comparing the first two rows of Table I and Table 2, we can see that the two-layer neural  network system alone is at the level of state-of-the-art HMM systems. Shown in Row 3  and 5 of Table 1, regularization and projection pursuit improve the performance of neural  net system. The hybrid SNN/HMM system reduces the word error rate 10%-15% over the  HMM system in both tables.  7 CONCLUSIONS  Neural net technology is useful in advancing the state-of-the-art in continuous speech recog-  nition system. Optimization methods, like regularization and projection pursuit, improve  the performance of the neural net system. Our hybrid SNN/HMM system reduces the word  error rate 10%-15% over the HM system on 5,000-word and 20,000-word WSJ corpus.  Acknowledgments  This work was funded by the Advanced Research Projects Agency of the Department of  Defense.  References  [1] M. Cohen, H. Franco, N. Morgan, D. Rumelhart and V. Abrash, "Context-Dependent  Multiple Distribution Phonetic Modeling with MLPS", in em Advances in Neural  Information Processing Systems 5, eds. S. J. Hanson, J. D. Cowan and C. L. Giles.  Morgan Kaufmann Publishers, San Mateo, 1993.  [2] G. Zavaliagkos, Y. Zhao, R. Schwartz and J. Makhoul," A Hybrid Neural Net  System for State-of-the-Art Continuous Speech Recognition", in em Advances in  Neural Information Processing Systems 5, eds. S. J. Hanson, J. D. Cowan and C. L.  Giles. Morgan Kaufmann Publishers, San Mateo, 1993.  [3] A. Barron. "Statistical properties of artificial neural networks," IEEE Conf. Decision  and Control, Tampa, FL, pp. 280-285, 1989.  [4] H. Gish, "A probabilistic approach to the understanding and training of neural network  classifiers," IEEE Int. Conf. Acoust., Speech, Signal Processing, April 1990.  [5] G. Wahba, Spline Models for Observational Data, CBMS-NSF Regional Conference  Series in Applied Mathematics, 1990.  [6] D. M. Bates, M. J. Lindstrom, G. Wahba and B. S. Yandell, "GCVPACK-Routines  for Generalized Cross Validation", Comm. Statist.- Simula., 16(4), 1247-1253 (1987).  [7] Y. Zhao and C. G. Atkeson, "Implementing Projection Pursuit Learning", to appear  in Neural Computation, in preparation.  [8] J. Makhoul, A. EI-Jaroudi and R. Schwartz, "Partitioning Capabilities of Two-layer  Neural Networks", IEEE Transactions on Signal Processing, 39, pp. 1435-1440, 1991.  PART X  COGNITIVE SCIENCE  
Lipreading by neural networks:  Visual preprocessing, learning  and sensory integration  Gregory J. Wolff  Ricoh California Research Center  2882 Sand Hill Road Suite 115  Menlo Park, CA 94025-7022  wolff@crc.ricoh.com  K. Venkatesh Prasad  Ricoh California Research Center  2882 Sand Hill Road Suite 115  Menlo Park, CA 94025-7022  prasad@crc.ricoh.com  David G. Stork  Ricoh California Research Center  2882 Sand Hill Road Suite 115  Menlo Park, CA 94025-7022  stork@crc.ricoh.com  Marcus Hennecke  Department of Electrical Engineering  Mail Code 4055  Stanford University  Stanford, CA 94305  Abstract  We have developed visual preprocessing algorithms for extracting  phonologically relevant features from the grayscale video image of  a speaker, to provide speaker-independent inputs for an automat-  ic lipreading ("speechreading") system. Visual features such as  mouth open/closed, tongue visible/not-visible, teeth visible/not-  visible, and several shape descriptors of the mouth and its motion  are all rapidly computable in a manner quite insensitive to lighting  conditions. We formed a hybrid speechreading system consisting  of two time delay neural networks (video and acoustic) and inte-  grated their responses by means of independent opinion pooling  -- the Bayesian optimal method given conditional independence,  which seems to hold for our data. This hybrid system had an er-  ror rate 25% lower than that of the acoustic subsystem alone on a  five-utterance speaker-independent task, indicating that video can  be used to improve speech recognition.  1027  1028 Wolff, Prasad, Stork, and Hennecke  1 INTRODUCTION  Automated speech recognition is notoriously hard, and thus any predictive source  of information and constraints that could be incorporated into a computer speech  recognition system would be desirable. Humans, especially the hearing impaired,  can utilize visual information -- "speech reading" -- for improved accuracy (Dodd  & Campbell, 1987, Sanders & Goodrich, 1971). Speech reading can provide direct  information about segments, phoneroes, rate, speaker gender and identity, and sub-  tle information for segmenting speech from background noise or multiple speakers  (De Filippo & Sims, 1988, Green 85 Miller, 1985).  Fundamental support for the use of visual information comes from the complemen-  tary nature of the visual and acoustic speech signals. Utterances that are difficult to  distinguish acoustically are the easiest to distinguish. visually, and vice versa. Thus,  for example /mi/ +- /nil are highly confusable acoustically but are easily distin-  guished based on the visual information of lip closure. Conversely,/hi//pi/are  highly confusable visually ("visemes"), but are easily distinguished acoustically by  the voice-onset time (the delay between the burst sound and the onset of vocal fold  vibration). Thus automatic lipreading promises to help acoustic speech recognition  systems for those utterances where they need it most; visual information cannot  contribute much information to those utterances that are already well recognized  acoustically.  1.1 PREVIOUS SYSTEMS  The system described below differs from recent speech reading systems. Whereas  Yuhas et al. (1989) recognized static images and acoustic spectra for vowel recogni-  tion, ours recognizes dynamic consonant-vowel (CV) utterances. Whereas Petajan,  Bischoff & Bodoff (1988) used thresholded pixel based representations of speakers,  our system uses more sophisticated visual preprocessing to obtain phonologically  relevant features. Whereas Pentland and Mase (1989) used optical flow methods  for estimating the motion of four lip regions (and used no acoustic subsystem), we  obtain several other features from intensity profiles. Whereas Bregler et al. (1993)  used direct pixel images, our recognition engine used a far more compressed visual  representation; our method of integration, too, was based on statistical properties  of our data. We build upon the basic recognizer architecture of Stork, Wolff and  Levine (1992), but extend it to grayscale video input.  2 VISUAL PREPROCESSING  The sheer quantity of image data presents a hurdle to utilizing video information for  speech recognition. Our approach to video preprocessing makes use of several simple  computations to reduce the large amount of data to a manageable set of low-level  image statistics describing the region of interest around the mouth. These statistics  capture such features as the positions of the upper and lower lip, the mouth shape,  and their time derivatives. The rest of this section describes the computation of  these features.  Grayscale video images are captured at 30 frames/second with a standard NTSC  Lipreading by Neural Networks: Visual Preprocessing, Learning, and Sensory Integration 1029  lower upper Io TM  upper lip lip hp  hp  mouth  cloed mouth open  pixel position pixel position  Figure 1: (Left) The central bands of the automatically determined ROI from two frames  of the video sequence of the utterance/ba/and their associated luminance profiles along  the central marked line. Notice that the lowest valley in this profile changes drastically  in intensity as the mouth changes from closed to open. In addition, the linear separation  between the peaks adjacent to the lowest valley also increases as the mouth opens. These  features are identified on the ROI from a single frame (right). The position, intensity, and  temporal variation of these features provide input to our recognizer.  camera, and subsampled to give 150 x 150 pixel image sequence. A 64 x 64 pixel  region of interest (ROI) is detected and tracked by means of the following operations  on the full video images:   Convolve with 3 x 3 pixel low-pass filter   Convolve with 3 x 3 pixel edge detector   Convolve with 3 x 3 pixel low-pass filter   Threshold at   Triangulate eyes with mouth  (to remove spatial noise)  (to detect edges)  (to smooth edges)  (to isolate eyes and mouth)  (to obtain ROI)  We also use temporal coherence in frame-to-frame correlations to reduce the effects  of noise in the profile or missing data (such as "closed" eyes). Within the ROI the  phonological features are found by the following steps (see Figure 1):   Convolve with 16 x 16 pixel low-pass filter   Extract a vertical intensity profile   Extract a horizontal intensity profile   Locate and label intensity peaks and valleys   Calculate interframe peak motion  (to remove noise)  (mouth height)  (mouth width)  (candidates for teeth, tongue)  (speed estimates)  Video preprocessing tasks, including temporal averaging, are usually complicated  because they require identifying corresponding pixels across frames. We circumvent  this pixel correspondence problem by matching labeled features (such as intensity  extrema- peaks and valleys) on successive frames.  1030 Wolff, Prasad, Stork, and Hennecke  2.1 FEATURES  The seventeen video features which serve as input to our recognizer are:   Horizontal separation between the left and right mouth comers   Vertical separation between the top and bottom lips  For each of the three vertically aligned positions:   Vertical position: Pv   Intensity value: I   Change in intensity versus time: AI/At  For both of the mouth corner positions:   Horizontal position: Pn   Intensity value: I   Change in intensity versus time: AI/At  For each speaker, each feature was scaled have a zero mean and unit standard  deviation.  3 DATA COLLECTION AND NETWORK TRAINING  We trained the modified time delay neural network (Waibel, 1989) shown in Figure 2  on both the video and acoustic data. (See Stork, Wolff and Levine (1992) for  a complete description of the architecture.) For the video only (VO) network, the  input layer consists of 24 samples of each of the 17 features, corresponding to roughly  0.8 seconds. Each (sigmoidal) hidden unit received signals from a receptive field of  17 features for five consecutive frames. Each of the different hidden units (there  were 3 for the results reported below) is replicated to cover the entire input space  with overlapping receptive fields. The next layer consisted of 5 rows of x-units (one  row for each possible utterance), with exponential transfer functions. They received  inputs from the hidden units for 11 consecutive frames, thus they indirectly received  input from a total of 18 input frames corresponding to roughly 0.6 seconds. The  activities of the x-units encode the likelihood that a given letter occurs in that  interval. The final layer consists of five p-units (probability units), which encode  the relative probabilities of the presence of each of the possible utterances across  the entire input window. Each p-unit sums the entire row of corresponding x-units,  normalized by the sum over all x-units. (Note that "weights" from the x-units to  the p-units are fixed.)  The acoustic only (AO) network shared the same architecture, except that the input  consisted of 100 frames (1 second) of 14 reel scale coefficients each, and the x-units  received fan in from 25 consecutive hidden units.  In the TDNN architecture, weights are shared, i.e., the pattern of input-to-hidden  weights is forced to be the same at each interval. Thus the total number of inde-  pendent weights in this VO network is 428, and 593 for the AO network.  These networks were trained using Backpropagation to minimize the  Leibler distance (cross-entropy) between the targets and outputs,  E _= D(t l] P) =  ti ln( t/  i Pi)' (1)  Here the target probability is i for the target category, and 0 for all other categories.  In this case Equation i simplifies to E = -ln(pc) where c is the correct category.  Kullback-  Lipreading by Neural Networks: Visual Preprocessing, Learning, and Sensory Integration 1031  71  Outputs for utterance ma3  '-'c- I bada fa la m  o  o  0.8 sec I 5 10 15 20  Time  Figure 2: Modified time delay neural network architecture (left) and unit activities  for a particular pattern (right). The output probabilities are calculated by inte-  grating over the entire input window and normalizing across categories. Note the  temporal segmentation which naturally occurs in the layer of X-units.  3.1 SENSORY INTEGRATION  Given the output probability distributions of the two networks, we combine them  assuming conditional independence and using Bayes rule to obtain:  P(AIc,)P(Vlc,)P(c,)  P(c, lA, V): Ejv= P(AIc;)P(Vlcj)P(c)  (2)  That is, the joint probability of the utterance belonging to category ci is just the  normalized product of the outputs for category ci of each network.  This "independent opinion pooling" (Berger, 1985) offers several advantages over  other methods for combining the modalities. First, it is optimal if the two signals  really are conditionally independent, which appears to be the case for our data.  (Proving that two signals are not conditionally independent is difficult.) Moreover,  Massaro and Cohen (1983) have shown that human recognition performance is  consistent with the independence assumption. A second advantage is simplicity.  The combination adds no extra parameters beyond those used to model each signal,  thus generalization performance should be good. Furthermore, the independent  recognizers can be developed and trained separately, the only requirement is that  they both output probability estimations.  A third advantage is that this system automatically compensates for noise and  assigns more importance to the network which is most sure of its classification. For  example, if the video data were very noisy (or missing), the video network would  1032 Wolff, Prasad, Stork, and Hennecke  judge all utterances equally likely. In this case the video contribution would cancel  out, and the final output probabilities would be determined solely by the audio  network. Bregler et al. (1993) attempt to compensate for the variance between  channels by using the entropy of the output of the individual networks as a weighting  on their contribution to the final outputs. Their ad hoc method suffers several  drawbacks. For example, it does not distinguish the case where a one category is  highly likely and the rest equiprobable, from the case where several categories are  moderately likely.  A final advantage of Eq. 2 is that it does not require synchrony of the acoustic and  visual features. The registration between the two signals could be off substantially  (as long as the same utterance is present in the input to both networks). On the  contrary, methods which attempt to detect cross-modal features would be very  sensitive to the relative timing of the two signals.  4  RESULTS  Video Test  Audio Test  AV Test  ma 0  la o  fa o  da o  C)  ba  Input Input Input  54% correct 64% correct 72% correct  Figure 3: Confusion matrices for the video only (VO), acoustic only (AO), and the AV  networks. Each vertical column is labeled by the spoken CV pair presented as input; each  horizontal row represents the output by the network. The radius of each disk in the array  is proportional to the output probabihty given an input letter. The recognition accuracy  (measured as a percentage of novel test patterns properly classified by maximum network  output) is shown.  The video and audio networks were trained separately on several different con-  sonants in the same vowel context (/ha/, /da/, /fa/, /la/, /ma/) recorded from  several different speakers. (For the results reported below, there were 10 speakers,  repeating each of 5 CV pairs 5 times. Four of these were used for training, and one  for testing generalization.)  For the video only networks, the correct classification (using the Max decision rule)  on unseen data is typically 40-60%. As expected, the audio networks perform better  with classification rates in the 50-70% range on these small sets of similar utterances.  Lipreading by Neural Networks: Visual Preprocessing, Learning, and Sensory Integration 1033  Figure 3 shows the confusion matrices for the network outputs. We see that for the  video only network the confusion matrix is fairly diagonal, indicating generally good  performance. However the video network does tend to confuse utterances such as  /ba/++/ma/. The audio network generally makes fewer errors, but confuses other  utterances, such as/ba/++/da/.  The performance for the combined outputs (the AV network) is much better than  either of the individual networks, achieving classification rates above 70%. (In  previous work with only 4 speakers, classifications rates of up to 95% have been  achieved.) We also see a strongly diagonal confusion matrix for the AV network,  indicating that complementary nature of the the confusions made by the individual  networks.  5 RELATIONSHIP TO HUMAN PERCEPTION  Visual  Acoustic  I'f gh I k Irmp m;lltl'uwh  AV  Input Input Input  Figure 4' Confusion matrices from human recognition performance for video only,  acoustic only, and combined speech for CV pairs (Massaro et al., 1993).  Interestingly, our results are qualitatively similar to findings in human perception.  Massaro et al. (1993) presented Visual only, Acoustic only, and combined speech to  subjects and collected response probabilities. As can be seen in the confusion ma-  trices of Figure 4, subjects are not so bad at lipreading. The Visual only confusion  matrix shows a strong diagonal component, though confusions such as/ma/++/ba/  are common. Performance on acoustic speech is better, of course, but there are still  confusions such as /ba/ ++ Ida/. Combined speech yields even better recognition  performance, eliminating most confusions. In fact, Massaro et al. found that the  response probabilities of combined speech are accurately predicted by the product  of the two single mode response probabilities. Massaro uses this and other evidence  to argue quite convincingly that humans treat acoustic and visual speech channels  independently, combining them only at a rather late stage of processing.  1034 Wolff, Prasad, Stork, and Hennecke  6 CONCLUSIONS AND FUTURE WORK  The video pre-processing presented here represents a first pass at reducing the  amount of visual data to a manageable level in order to enable on-line process-  ing. Our results indicate that even these straightforward, computationally tractable  methods can significantly enhance speech recognition. Future efforts will concen-  trate on refining the pre-processing to capture more information, such as rounding  and f-tuck, and testing the efficacy of our recognition system on larger datasets. The  complementary nature of the acoustic and visual signals lead us to believe that a  further refined speech reading system will significantly improve the state-of-the-art  acoustic recognizers, especially in noisy environments.  References  J.O. Berger. (1985) Statistical decision theory and Bayesian analysis (nd ed.). 272-275,  New York: Springer-Verlag.  C. Bregler, S. Manke, H. Hild & A. Waibel. (1993) Bimodal Sensor Integration on the  example of "Speech-Reading". Proc. ICNN-93, Vol. II 667-677.  C. L. De Filippo & D. G. Sims (eds.), (1988) New Reflections on Speechreading (Special  issue of The Volta Review). 90(5).  B. Dodd & R. Campbell (eds.). (1987) Hearing by Eye: The Psychology of Lip-reading.  Hillsdale, N J: Lawrence Erlbaum Press.  K. P. Green & J. L. Miller. (1985) On the role of visual rate information in phonetic  perception. Perception and Psychophysics 38, 269-276.  D. W. Massaro & M. M. Cohen (1983) Evaluation and integration of visual and auditory  information in speech perception J. Exp. Psych: Human Perception and Performance 9,  753-771.  D. W. Massaro, M. M. Cohen & A. T. Gesi (1993). Long-term training, transfer, and  retention in learning to lipread. Perception  Psychophysics, 53, 549-562.  A. Pentland & K. Mase (1989) Lip reading: Automatic visual recognition of spoken words.  Proc, Image Understanding and Machine Vision, Optical Society of America, June 12-14.  E. D. Petajan, B. Bischoff & D. Bodoff. (1988) An improved automatic lipreading system  to enhance speech recognition. A CM SIGCHI-88, 19-25.  D. Sanders & S. Goodrich. (1971) The relative contribution of visual and auditory com-  ponents of speech to speech intelligibility as a function. of three conditions of frequency  distortion. J. Speech and Hearing Research 14, 154-159.  D. G. Stork, G. Wolff & E. Levine. (1992) Neural network lipreading system for improved  speech recognition. Proc. IJCNN-92, Vol. II 285-295.  A. Waibel. (1989) Modular construction of time-delay neural networks for speech recog-  nition. Neural Computation 1, 39-46.  B. P. Yuhas, M. H. Goldstein, Jr., T. J. Sejnowski & R. E. Jenkins. (1988) Neural network  models of sensory integration for improved vowel recognition. Proc. IEEE 78(10), 1658-  1668.  
Bounds on the complexity of recurrent  neural network implementations of finite  state machines  Bill G. Horne  NEC Research Institute  4 Independence Way  Princeton, NJ 08540  Don R. Hush  EECE Department  University of New Mexico  Albuquerque, NM 87131  Abstract  In this paper the efficiency of recurrent neural network implementa-  tions of m-state finite state machines will be explored. Specifically,  it will be shown that the node complexity for the unrestricted case  can be bounded above by O (x/)  It will also be shown that the  node complexity is O (x/m log m) when the weights and thresholds  are restricted to the set {- 1, 1}, and O (m) when the fan-in is re-  stricted to two. Matching lower bounds will be provided for each  of these upper bounds assuming that the state of the FSM can be  encoded in a subset of the nodes of size [log mi.  I Introduction  The topic of this paper is understanding how efficiently neural networks scale to  large problems. Although there are many ways to measure efficiency, we shall be  concerned with node comple:ity, which as its name implies, is a calculation of the  required number of nodes. Node complexity is a useful measure of efficiency since  the amount of resources required to implement or even simulate a recurrent neural  network is typically related to the number of nodes. Node complexity can also  be related to the efficiency of learning algorithms for these networks and perhaps  to their generalization ability as well. We shall focus on the node complexity of  recurrent neural network implementations of finite state machines (FSMs) when  the nodes of the network are restricted to threshold logic units.  359  360 Home and Hush  In the 1960s it was shown that recurrent neural networks are capable of imple-  menting arbitrary FSMs. The first result in this area was due to Minsky [7], who  showed that m-state FSMs can be implemented in a fully connected recurrent neu-  ral network. Although circuit complexity was not the focus of his investigation it  turns out that his construction, yields O (m) nodes. This construction was also  guaranteed to use weight values limited to the set {1, 2}. Since a recurrent neural  network with k hard-limiting nodes is capable of representing as many as 2  states,  one might wonder if an m-state FSM could be implemented by a network with  log m nodes. However, it was shown in [1] that the node complexity for a standard  fully connected network is ft ((m logre)x/3). They were also able to improve upon  Minsky's result by providing a construction which is guaranteed to yield no more  than O (m 3/q) nodes. In the same paper lower bounds on node complexity were in-  vestigated as the network was subject to restrictions on the possible range of weight  values and the fan-in and fan-out of the nodes in the network. Their investigation  was limited to fully connected recurrent neural networks and they discovered that  the node complexity for the case where the weights are restricted to a finite size set  is ft (x/m log m) . Alternatively, if the nodes in the network were restricted to have a  constant fan-in then the node complexity becomes  (m). However, they left open  the question of how tight these bounds are and if they apply to variations on the  basic architecture. Other recent work includes investigation of the node complexity  for networks with continuous valued nonlinearities [14]. However, it can also be  shown that when continuous nonlinearities are used, recurrent neural networks are  far more powerful than FSMs; in fact, they are Turing equivalent [13].  In this paper we improve the upper bound on the node complexity for the un-  restricted case to O(v/. We also provide upper bounds that match the lower  bounds above for various restrictions. Specifically, we show that a node complexity  of O (v/m log m) can be achieved if the weights are restricted to the set {- 1, 1 }, and  that the node complexity is O (m) for the case when the fan-in of each node in the  network is restricted to two. Finally, we explore the possibility that implementing  finite state machines in more complex models might yield a lower node complexity.  Specifically, we explore the node complexity of a general recurrent neural network  topology, that is capable of simulating a variety of popular recurrent neural net-  work architectures. Except for the unrestricted case, we will show that the node  complexity is no different for this architecture than for the fully connected case if  the number of feedback variables is limited to [log m], i.e. if the state of the FSM  is encoded optimally in a subset of the nodes. We leave it as an open question if a  sparser encoding can lead to a more efficient implementation.  2 Background  2.1 Finite State Machines  FSMs may be defined in several ways. In this paper we shall be concerned with  Mealy machines, although our approach can easily be extended to other formulations  to yield equivalent results.  Bounds on the Complexity of Recurrent Neural Network Implementations 361  Definition 1 A Mealy machine is a quintuple .M = (Q, q0, E, A, d), where Q is a  finite set of states; q0 is the initial state; E is the input alphabet; A is the output  alphabet; and d  Q x E > Q x A is the combined transition and output function.  Throughout this paper both the input and output alphabets will be binary (i.e. E -  A = {0, 1}). In general, the number of states, m: IQI, may be arbitrary. Since any  element of Q can be encoded as a binary vector whose minimum length is [log m],  the function b can be implemented as a boolean logic function of the form  q. {0,1) [1gm]+l  {0,1} [1gm]+l  (1)  The number, N, of different minimal FSMs with m states will be used to determine  lower bounds on the number of gates required to implement an arbitrary FSM in a  recurrent neural network. It can easily be shown that (2m) m <_ N [5]. However,  it will be convenient to reexpress N in terms of n = [log m] + I as follows  2 <_ (2)  2.2 Recurrent Neural Networks  The fundamental processing unit in the models we wish to consider is the perceptton,  which is a biased, linearly weighted sum of its inputs followed by a hard-limiting  nonlinearity whose output is zero if its input is negative and one otherwise. The  fan-in of the perceptton is defined to be the number of non-zero weights. When the  values of xi are binary (as they are in this paper), the perceptton is often referred  to as a threshold logic unit (TLU).  A count of the number of different partially specified threshold logic functions, which  are threshold logic functions whose values are only defined over v vertices of the  unit hypercube, will be needed to develop lower bounds on the node complexity  required to implement an arbitrary logic function. It has been shown that this  number, denoted L, is [15]  2//n  As pointed out in [10], many of the most popular discrete-time recurrent neural  network models can be implemented as a feedforward network whose outputs are  fed back recurrently through a set of unit time delays. In the most generic version of  this architecture, the feed forward section is lower triangular, meaning the I th node is  the only node in layer I and receives input from all nodes in previous layers (including  the input layer). A lower triangular network of k threshold logic elements is the  most general topology possible for a feedforward network since all other feedforward  networks can be viewed as a special case of this network with the appropriate weights  set equal to zero. The most direct implementation of this model is the architecture  proposed in [11]. However, many recurrent neural network architectures can be cast  into this framework. For example, fully connected networks [3] fit this model when  the the feedforward network is simply a single layer of nodes. Even models which  appear very different [2, 9] can be cast into this flamework.  362 Home and Hush  3 The unrestricted case  The unrestricted case is the most general, and thus explores the inherent power of  recurrent neural networks. The unrestricted case is also important because it serves  as a baseline from which one can evaluate the effect of various restrictions on the  node complexity.  In order to derive an upper bound on the node complexity of recurrent neural  network implementations of FSMs we shall utilize the following lemma, due to  Lupanov [6]. The proof of this lemma involves a construction that is extremely  complex and beyond the scope of this paper.  Lemma 1 (Lupanov, 1973) Arbitrary boolean logic functions with x inputs and  y outputs can be implemented in a network of percepttons with a node complexity  of  :r - log '  Theorem 1 Multilayer recurrent neural networks can implement FSMs having m  states with a node complexity of O (x/). []  Proof.' Since an m-state FSM can be implemented in a recurrent neural network  in which the multilayer network performs a mapping of the form in equation (1),  then using n = m = [log m] + 1, and applying Lemma 1 gives an upper bound of  O (x/) . Q.E.D.  Theorem 2 Multilayer recurrent neural networks can implement FSMs having m  states with a node complexity of  (x/ if the number of unit time delays is [log mi.  Proof.' In order to prove the theorem we derive an expression for the maximum  number of functions that a k-node recurrent neural network can compute and com-  pare that against the minimum number of finite state machines. Then we solve for  k in terms of the number of states of the FSM.  Specifically, we wish to manipulate the inequality  < n! ( k-1 ) h 2n(n+i)+  - n- 1 (n + i)!  i=0  (a) (b)  where the left hand side is given in equation (2), (a) represents the total number of  ways to choose the outputs and feedback variables of the network, and (b) repre-  sents the total number of logic functions computable by the feed forward section of  the network, which is lower triangular. Part (a) is found by simple combinatorial  arguments and noting that the last node in the network must be used as either  an output or feedback node. Part (b) is obtained by the following argument: If  the state is optimally encoded in [log m] nodes, then only [log m] variables need  Bounds on the Complexity of Recurrent Neural Network Implementations 363  to be fed back. Together with the external input this gives n = [log m] + 1 local  inputs to the feedforward network. Repeated application of (3) with  = 2" yields  expression (b).  Following a series of algebraic manipulations it can easily be shown that there exists  a constant c such that  n2 n < cken.  Since n = [log m] + 1 it follows that k = ft (v/-) . Q.E.D.  4 Restriction on weights and thresholds  All threshold logic functions can be implemented with perceptrons whose weight  and threshold values are integers. It is well known that there are threshold logic  functions of n variables that require a perceptron with weights whose maximum  magnitude is (2 ") and O(n '/2) [8]. This implies that if a perceptron is to be  implemented digitally, the number of bits required to represent each weight and  threshold in the worst case will be a superlinear function of the fan-in. This is  generally undesirable; it would be far better to require only a logarithmic number  of bits per weight, or even better, a constant number of bits per weight. We will be  primarily be interested in the most extreme case where the weights are limited to  values from the set {-1, 1}.  In order to derive the node complexity for networks with weight restrictions, we  shall utilize the following lemma, proved in [4].  Lemma 2 Arbitrary boolean logic functions with x inputs and y outputs can be  implemented in a network ofperceptrons whose weights and thresholds are restricted  to the set {-1, 1} with a node complexity of t3 (y) . []  This lemma is not difficult to prove, however it is beyond the scope of this paper.  The basic idea involves using a decomposition of logic functions proposed in [12].  Specifically, a boolean function f may always be decomposed into a disjunction of  2 r terms of the form 5:2...Srfi(xr+,... ,xn), one for each conjunction of the  first r variables, where j represents either a complemented or uncomplemented  version of the input variable xj and each fi is a logic function of the last n - r  variables. This expression can be implemented directly in a neural network. With  negligible number of additional nodes, the construction can be implemented in such  a way that all weights are either -1 or 1. Finally, the variable r is optimized to  yield the minimum number of nodes in the network.  Theorem 3 Multilayer recurrent neural networks that have nodes whose weights  and thresholds arc restricted to the set {-1, 1} can implement FSMs having m  states with a node complexity of O (m log m) . []  Proof.' Since an m-state FSM can be implemented in a recurrent neural network  in which the multilayer network performs a mapping of the form in equation (1),  then using n -- m -- [log m] + 1, and applying Lemma 2 gives an upper bound of  o (m log m).  364 Home and Hush  Theorem 4 Multilayer recurrent neural networks that have nodes whose weights  and thresholds are restricted to a set of size IW] can implement FSMs having m  f /rn log m   states with a node complexity of  ,VloglW I ] if the number of unit time delays is  [log mi. []  Proof: The proof is similar to the proof of Theorem 2 which gave a lower bound  for the node complexity required in an arbitrary network of threshold logic units.  Here, the inequality we wish to manipulate is given by  k-1  - n 1 [W['*+i+'  i=0  (a) (b)  where the left hand side and (a) are computed as before and (b) represents the  maximum number of ways to configure the nodes in the network when there are  only IWI choices for each weight and threshold. Following a series of algebraic  manipulations it can be shown that there exists a constant c such that  n2 ' _< ck  log [W[.  f /rologin ) Q.E.D.  Since n - [logm] + 1 it follows that k -- fl ,VloglW [ .  Clearly, for W = {-1, 1} this lower bound matches the upper bound in Theorem 3.  5 Restriction on fan-in  A limit on the fan-in of a perceptron is another important practical restriction.  In the networks discussed so far each node has an unlimited fan-in. In fact, in  the constructions described above, many nodes receive inputs from a polynomial  number of nodes (in terms of m) in a previous layer. In practice it is not possible  to build devices that have such a large connectivity. Restricting the fan-in to 2, is  the most severe restriction, and will be of primary interest in this paper.  Once again, in order to derive the node complexity for restricted fan-in, we shall  utilize the following lemma, proved in [4].  Lemma 3 Arbitrary boolean logic functions with x inputs and y outputs can be  implemented in a network of perceptrons restricted to fan-in 2 with a node com-  plexity of  y2    (x+logy) '  This proof of this lemma is very similar to the proof of Lemma 2. Here Shannon's  decomposition is used with r = 2 to recursively decompose the logic function into  a set of trees, until each tree has depth d. Then, all possible functions of the last  n - d variables are implemented in an inverted tree-like structure, which feeds into  the bottom of the trees. Finally, d is optimized to yield the minimum number of  nodes.  Bounds on the Complexity of Recurrent Neural Network Implementations 365  Theorem 5 Multilayer recurrent neural networks that have nodes whose fan-in is  restricted to two can implement FSMs having m states with a node complexity of  O(m) []  Proof: Since an m-state FSM can be implemented in a recurrent neural network  in which the multilayer network performs a mapping of the form in equation (1),  then using n - m = [log m] + 1, and applying Lemma 3 gives an upper bound of  O(m).  Theorem 6 Multilayer recurrent neural networks that have nodes whose fan-in is  restricted to two can implement FSMs having m states with a node complexity of  Q (m) if the number of unit time delays is [log mi. []  Proof: Once again the proof is similar to Theorem 2, which gave a lower bound  for the node complexity required in an arbitrary network of threshold logic units.  Here, the inequality we need to solve for is given by  k-1  - n 1 14  i=0  (a) (b)  where the left hand side and (a) are computed as before and (b) represents the max-  imum number of ways to configure the nodes in the network. The term ( n + i )  2  is used since a node in the i th layer has n + i possible inputs from which two are  chosen. The constant 14 represents the fourteen possible threshold logic functions  of two variables. Following a series of algebraic manipulations it can be shown that  there exists a constant c such that  n2'* _< ck log k  Since n = [log m] + 1 it follows that k = Q (m).  Q.E.D.  6 Summary  In summary, we provide new bounds on the node complexity of implementing FSMs  with recurrent neural networks. These upper bounds match lower bounds devel-  oped in [1] for fully connected recurrent networks when the size of the weight set  or the fan-in of each node is finite. Although one might speculate that more com-  plex networks might yield more efficient constructions, we showed that these lower  bounds do not change for restrictions on weights or fan-in, at least when the state  of the FSM is encoded optimally in a subset of [log m] nodes. When the network  is unrestricted, this lower bound matches our upper bound. We leave it as an open  question if a sparser encoding of the state variables can lead to a more efficient  implementation.  One interesting aspect of this study is that there is really not much difference  in efficiency when the network is totally unrestricted and when there are severe  restrictions placed on the weights. Assuming that our bounds are tight, then there  366 Home and Hush  is only a v/g m penalty for restricting the weights to either -1 or 1. To get some  idea for how marginal this difference is consider that for a finite state machine with  m: 18 x 10 s states, v/log m is only eight!  A more detailed version of this paper can be found in [5].  References  [10]  [12]  [13]  [14]  [15]  [1] N. Alon, A.K. Dewdney, and T.J. Ott. Efficient simulation of finite automata  by neural nets. JACM, 38(2):495-514, 1991.  [2] A.D. Back and A.C. Tsoi. FIR and IIR synapses, a new neural network archi-  tecture for time series modeling. Neural Computation, 3(3):375-385, 1991.  [3] J.J. Hop field. Neural networks and physical systems with emergent collective  computational abilities. Proc. Nat. Acad. $ci., 79:2554-2558, 1982.  [4] B.G. Horne and D.R. Hush. On the node complexity of neural networks. Tech-  nical Report EECE 93-003, Dept. EECE, U. New Mexico, 1993.  [5] B.G. Horne and D.R. Hush. Bounds on the complexity of recurrent neural  network implementations of finite state machines. Technical Report EECE  94-001, Dept. EECE, U. New Mexico, 1994.  [6] O.B. Lupanov. The synthesis of circuits from threshold elements. Problemy  Kibernetiki, 26:109-140, 1973.  [7] M. Minsky. Computation: Finite and infinite machines. Prentice-Hall, 1967.  [8] S. Muroga. Threshold Logic and Its Applications. Wiley, 1971.  [9] K.S. Narendra and K. Parthasarathy. Identification and control of dynamical  systems using neural networks. IEEE Trans. on Neural Networks, 1:4-27, 1990.  O. Nerrand et al. Neural networks and nonlinear adaptive filtering: Unifying  concepts and new algorithms. Neural Computation, 5(2):165-199, 1993.  A.J. Robinson and F. Fallside. Static and dynamic error propagation networks  with application to speech coding. In D.Z. Anderson, editor, Neural Informa-  tion Processing Systems, pages 632-641, 1988.  C. Shannon. The synthesis of two-terminal switching circuits. Bell Sys. Tech.  J., 28:59-98, 1949.  H. Siegelmann and E.D. Sontag. Neural networks are universal computing  devices. Technical Report SYCON-91-08, Rutgers Ctr. for Sys. and Cont.,  1991.  H.T. Siegelmann, E.D. Sontag, and C.L. Giles. The complexity of language  recognition by neural networks. In Proc. IFIP 12 th World Comp. Cong., pages  329-335, 1992.  R.O. Winder. Bounds on threshold gate realizability. IEEE Trans. on Elect.  Comp., EC-12:561-564, 1963.  
Exploiting Chaos to Control the Future  Gary W. Flake*  Guo-Zhen Sun t  Yee-Chun Lee t  Hsing-Hen Chen t  Institute for Advance Computer Studies  University of Maryland  College Park, MD 20742  Abstract  Recently, Ott, Grebogi and Yorke (OGY) [6] found an effective  method to control chaotic systems to unstable fixed points by us-  ing only small control forces; however, OGY's method is based on  and limited to a linear theory and requires considerable knowledge  of the dynamics of the system to be controlled. In this paper we use  two radial basis function networks: one as a model of an unknown  plant and the other as the controller. The controller is trained  with a recurrent learning algorithm to minimize a novel objective  function such that the controller can locate an unstable fixed point  and drive the system into the fixed point with no a priori knowl-  edge of the system dynamics. Our results indicate that the neural  controller offers many advantages over OGY's technique.  1 Introduction  Recently, Ott, Grebogi and Yorke (OGY) [6] proposed a simple but very good idea.  Since any small perturbation can cause a large change in a chaotic trajectory, it  is possible to use a very small control force to achieve a large trajectory modifi-  cation. Moreover, due to the ergodicity of chaotic motion, any state in a chaotic  *Department of Computer Science, peyote@umiacs.umd.edu  tLaboratory for Plasma Research  647  648 Flake, Sun, Lee, and Chen  attractor can be reached by a small control force. Since OGY published their work,  several experiments and simulations have proven the usefulness of OGY's method.  One prominent application of OGY's method is the prospect of controlling cardiac  chaos [1].  We note that there are several unfavorable constraints on OGY's method. First,  it requires a priori knowledge of the system dynamics, that is, the location of  fixed points. Second, due to the limitation of linear theory, it will not work in the  presence of large noise or when the control force is as large as beyond the linear  region from which the control law was constructed. Third, although the ergodicity  theory guarantees that any state after moving away from the desired fixed point  will eventually return to its linear vicinity, it may take a very long time for this to  happen, especially for a high dimensional chaotic attractor.  In this paper we will demons[rate how a neural network (NN) can control a chaotic  system with only a small control force and be trained with only examples from  the state-space. To solve this problem, we introduced a novel objective function  which measures the distance between the current state and its previous average.  By minimizing this objective function, the NN can automatically locate the fixed  point. As a preliminary step, a training set is used to train a forward model for  the chaotic dynamics. The work of Jordan and Rumelhart [4] has shown that  control problems-can be mapped into supervised learning problems by coupling the  outputs of a controller NN (the control signals) to the inputs of a forward model  of a plant to form a multilayer network that is indirectly recurrent. A recurrent  learning algorithm is used to train the controller NN. To facilitate learning we use an  extended radial basis function (RBF) network for both the forward model and the  controller. To benchmark with OGY's result, the Hnon map is used as a numerical  example. The numerical results have shown the preliminary success of the proposed  scheme. Details will be given in the following sections.  In the next section we give our methodology and describe the general form of the  recurrent learning algorithm used in our experiments. In Section 3, we discuss RBF  networks and reintroduce a more powerful version. In Section 4, the numerical  results are presented in detail. Finally, in Section 5, we give our conclusions.  2 Recurrent Learning for Control  Let /(.) denote a NN whose output, fit, is composed through a plant, f(.), with  unknown dynamics. The output of the unknown plant (the state), t+l, forms  part of the input for the NN at the next time step, hence the recurrency. At each  time step the state is also passed to an output function, (.), which computes the  sensation, fft+l. The time evolution of this system is more accurately described by  where Y'+x is the desired sensation for time t + 1 and t represents the trainable  weights for the network. Additionally, we define the temporally local and global  Exploiting Chaos to Control the Future 649  error functionMs  1  11y-7 -ff11 and E = Y]is__x  where N is the final time step for the system.  The real-time recurrent learning (RTRL) algorithm [9] for training the network  weights to minimize E is based on the fair assumption that minimizing the local  error functionMs with a small learning rate at each time step will correspond to  minimizing the global error. To derive the learning algorithm, we can imagine the  system consisting of the plant, controller, and error functionals as being unfolded  in time. From this perspective we can view each instance of the controller NN  as a separate NN and thus differentiate the error functionals with respect to the  network weights at different times. Hence, we now add a time index to tt to  represent this fact. However, when we use t without the time index, the term  should be understood to be time invariant.  We can now define the matrix  i=0  which further allows us to define  Odi _ Odi O ri (2)  :  i=1  Equation 2 is the gradient equation for the RTRL algorithm while Equation 3 is for  the backpropagation through time (BPTT) learning algorithm [7]. The gradients  defined by these equations are usually used with gradient descent on a multilayer  perceptron (MLP). We will use them on RBF networks.  3 The CNLS Network  The Connectionist Normalized Local Spline (CNLS) network [3] is an extension of  the more familiar radial basis function network of Moody and Darken [5]. The  forward operation of the network is defined by  () - Z(fi -n t- C . ( -- ti) ) /9i (;),  i  (4)  where  / 2  exp(-/3ill- '11 )  pi(?) = y]j exp(-/3j 11- % 112) '  (5)  All of the equations in this section assume a single output. Generalizing them for  multiple outputs merely adds another index to the terms. For all of our simulations,  we choose to distribute the centers, 5i, based on a sample of the input space.  650 Flake, Sun, Lee, and Chen  Additionally, the basis widths,/?i, are set to an experimentally determined constant.  Because the output, b, is linear in the terms fi and , training them is very fast.  To train the CNLS network on a prediction problem we, can use a quadratic error  1 : _  function of the form E = 5(y( ) q())2, where y() is the target function that  we wish to approximate. We use a one-dimensional Newton-like method [8] which  yields the update equations  OE  f/t+l _ fi t -- E  fit +  d./+l - ( - Z  = + -  The right-most update rules form the learning algorithm when using the CNLS  network for prediction, where r/is a learning rate that should be set below 1.0. The  left-most update rules describe a more general learning algorithm that can be used  when a target output is unknown.  When using the CNLS network architecture as part of a recurrent learning algorithm  we must be able to differentiate the network outputs with respect to the inputs. Note  that in Equations 1 and 2 each of the terms 0t/0fft-x, 0fft-x/0-x, 0/0_x,  and O/Oi can either be exactly solved or approximated by differentiating a CNLS  network. Since the CNLS output is highly nonlinear in its inputs, computing these  partial derivatives is not quite as elegant as it would be in a MLP. Nevertheless, it  can be done. We skip the details and just show the end result:  0z  i=1 j=l  (6)  with *7 Y.p(x)i?(  ) and qi fi + c (. i).  4 Adaptive Control  By combining the equations from the last two sections, we can construct a recurrent  learning scheme for RBF networks in a similar fashion to what has been done with  MLP networks. To demonstrate the utility of our technique, we have chosen a well-  studied nonlinear plant that has been successfully modeled and controlled by using  non-neural techniques. Specifically, we will use the Hnon map as a plant, which  has been the focus of much of the research of OGY [6]. We also adopt some of their  notation and experimental constraints.  4.1 The Hanon Map  The H&non map [2] is described by the equations  xt+ = A- xt  + Byt  Yt+ l ---- Xt ,  (7)  (8)  Exploiting Chaos to Control the Future 651  where A = A0 + p and p is a control parameter that may be modified at each time  step to coerce the plant into a desirable state. For all simulations we set A0 = 1.29  and B = 0.3 which gives the above equations a chaotic attracter that also contains  an unstable fixed point. Our goal is to train a CNLS network that can locate and  drive the map into the unstable fixed point and keep it there with only a minimal  amount of information about the plant and by using only small values of p.  The unstable fixed point (xr, Yr) in Equations 7 and 8 can be easily calculated as  x, = Yr m 0.838486. Forcing the Hanon map to the fixed point is trivial if the  controller is given unlimited control of the parameter. To make the problem more  realistic we define p* as the maximum magnitude that p can take and use the rule  below on the left  ifp>p* p iflpl<p*  P': - p* ifp<-p* P' = 0 iflp[>p*  while OGY use the rule on the right. The reason we avoid the second rule is that  it cannot be modeled by a CNLS network with any precision since it is step-like.  The next task is to define what it means to "control" the H&non map. Having  analytical knowledge of the fixed point in the attracter would make the job of the  controller much easier, but this is unrealistic in the case where the dynamics of  the plant to control are unknown. Instead, we use an error function that simply  compares the current state of the plant with an average of previous states:  e, = + (y, -  where (.) is the average of the lt r values of its argument. This function ap-  proaches zero when the map is in a fixed point for time length greater than r. This  function requires no special knowledge about the dynamics of the plant, yet it still  enforces our constraint of driving the map into a fixed point.  The learning algorithm also requires the partial derivatives of the error function with  respect to the plant state variables, which are Oet/Ox = x - (x) and Oet/Oyt =  Yt - (y). These two equations and the objective function are the only special  purpose equations used for this problem. All other equations generalize from the  derivation of the algorithm. Additionally, since the "output" representation (  discussed earlier) is identical to the state representation, training on a distinct  output function is not strictly necessary in this ce. Thus, we simplify the problem  by only using a single additional model for the unknown next-state function of the  Hnon map.  4.2 Simulation  To facilitate comparison between alternate control techniques, we now introduce  the term e't where 't is a random variable and e is a small constant which specifies  the intensity of the noise. We use a Gaussian distribution for 't such that the  distribution has a zero mean, is independent, and has a variance of one. In keeping  with [6], we discard any values of 't which are greater in magnitude than 10. For  training we set e = 0.038. However, for tests on the real controller, we will show  results for several values of e.  652 Flake, Sun, Lee, and Chen  ! !  (a) ''' ' (b) ''' ' (c)  ! !  (d) ' ' ' ' () (f)  e '  Figure 1: Experimental results from training a neural controller to drive the Hnon  map into a fixed point. From (a) to (f), the values of e are 0.035, 0.036, 0.038,  0.04, 0.05, and 0.06, respectively. The top row corresponds to identical experiments  performed in [6].  We add the noise in two places. First, when training the model, we add noise to  the target output of the model (the next state). Second, when testing the controller  on the real H&non map, we add the noise to the input of the plant (the previous  state). In the second case, we consider the noise to be an artifact of our fictional  measurements; that is, the plant evolves from the previous noise free state.  Training the controller is done in two stages: an off-line portion to tune the model  and an on-line stage to tune the controller. To train the model we randomly pick  a starting state within a region (-1.5, 1.5) for the two state variables. We then  iterate the map for one hundred cycles with p = 0 so that the points will converge  onto the chaotic attractor. Next, we randomly pick a value for p in the range of  (-p*, p*). The last state from the iteration is combined with this control parameter  to compute a target state. We then add the noise to the new state values. Thus,  the model input consists of a clean previous state and a control parameter and the  target values consist of the noisy next state. We compute 100 training patterns  in this manner. Using the prediction learning algorithm for the CNLS network  we train the model network on each of the 100 patterns (in random order) for 30  epochs. The model quickly converges to a low average error.  In the next stage, we use the model network to train the controller network in two  ways. First, the model acts as the plant for the purposes of computing a next state.  Additionally, we differentiate the model for values needed for the RTRL algorithm.  We train the controller for 30 epochs, where each epoch consists of 50 cycles. At  the beginning of each epoch we initialize the plant state to some random values  (not necessarily on the chaotic attracter,) and set the recurrent history matrix,  Exploiting Chaos to Control the Future 653  (a) ' ' , ' ' (b) '  Figure 2: Experimental results from [6]. From left to right, the values ofe are 0.035,  0.036, and 0.038, respectively.  Ft, to zero. Then, for each cycle, we feed the previous state into the controller  as input. This produces a control parameter which is fed along with the previous  state as input into the model network, which in turn produces the next state. This  next state is fed into the error function to produce the error signal. At this point  we compute all of the necessary values to train the controller for that cycle while  maintaining the history matrix.  In this way, we train both the model and control networks with only 100 data points,  since the controller never sees any of the real values from the Hnon map but only  estimates from the model. For this experiment both the control and model RBF  networks consist of 40 basis functions.  4.3 Summary  Our results are summarized by Figure 1. As can be seen, the controller is able to  drive the Hnon Map into the fixed point very rapidly and it is capable of keeping  it there for an extended period of time without transients. As the level of noise is  increased, it can be seen that the plant maintains control for quite some time. The  first visible spike can be observed when e = 0.04.  These results are an improvement over the results generated from the best non-  neural technique available for two reasons: First, the neural controller that we  have trained is capable of driving the Hnon map into a fixed point with far fewer  transients then other techniques. Specifically, alternate techniques, as illustrated  in Figure 2, experience numerous spikes in the map for values of e for which our  controller is spike-free (0.035 - 0.038). Second, our training technique has smaller  data requirements and uses less special purpose information. For example, the  RBF controller was trained with only 100 data points compared to 500 for the non-  neural. Additionally, non-neural techniques will typically estimate the location of  the fixed point with an initial data set. In the case of [6] it was assumed that the  fixed point could be easily discovered by some technique, and as a result all of their  experiments rely on the true (hard-coded) fixed point. This, of course, could be  discovered by searching the input space on the RBF model, but we have instead  allowed the controller to discover this feature on its own.  654 Flake, Sun, Lee, and Chen  5 Conclusion and Future Directions  A crucial component of the success of our approach is the objective function that  measures the distance between the current state and the nearest time average.  The reason why this objective function works is that during the control stage the  learning algorithm is minimizing only a small distance between the current point  and the "moving target." This is in contrast to minimizing the large distance  between the current point and the target point, which usually causes unstable long  time correlation in chaotic systems and ruins the learning. The carefully designed  recurrent learning algorithm and the extended RBF network also contribute to the  success of this approach. Our results seem to indicate that RBF networks hold great  promise in recurrent systems. However, further study must be done to understand  why and how NNs could provide more useful schemes to control real world chaos.  Acknowledgements  We gratefully acknowledge helpful comments from and discussions with Chris  Barnes, Lee Giles, Roger Jones, Ed Ott, and James Reggia. This research was  supported in part by AFOSR grant number F49620-92-J-0519.  References  [1] A. Garfinkel, M.L. Spano, and W.L. Ditto. Controlling cardiac chaos. Science,  257(5074):1230, August 1992.  [2] M. Hnon. A two-dimensional mapping with a strange attractor. Communica-  tions in Mathematical Physics, 50:69-77, 1976.  [3] R.D. Jones, Y.C. Lee, C.W. Barnes, G.W. Flake, K. Lee, P.S. Lewis, and S. Qian.  Function approximation and time series prediction with neural network. In  Proceedings of the International Joint Conference on Neural Networks, 1990.  [4] M.I. Jordan and D.E. Rumelhart. Forward models: Supervised learning with a  distal teacher. Technical Report Occasional Paper #40, MIT Center for Cogni-  tive Science, 1990.  [5] J. Moody and C. Darken. Fast learning in networks of locally-tuned processing  units. Neural Computation, 1:281-294, 1989.  [6] E. Ott, C. Grebogi, and J.A. Yorke. Controlling chaotic dynamical systems.  In D.K. Campbell, editor, CHAOS: Soviet-American Perspectives on Nonlinear  Science, pages 153-172. American Institute of Physics, New York, 1990.  [7] F.J. Pineda. Generalization of back-propagation to recurrent neural networks.  Physical Review Letters, 59:2229-2232, 1987.  [8] W.H. Press, B.P. Flannery, S.A. Teukolsky, and W.T. Vetterling. Numerical  Recipes. Cambridge University Press, Cambridge, 1986.  [9] R.J. Williams and D. Zipset. Experimental analysis of the real-time recurrent  learning algorithm. Connection Science, 1:87-111, 1989.  
Postal Address Block Location Using A  Convolutional Locator Network  Ralph Wolf and John C. Platt  Synaptics, Inc.  2698 Orchard Parkway  San Jose, CA 95134  Abstract  This paper describes the use of a convolutional neural network  to perform address block location on machine-printed mail pieces.  Locating the address block is a difficult object recognition problem  because there is often a large amount of extraneous printing on a  mail piece and because address blocks vary dramatically in size and  shape.  We used a convolutional locator network with four outputs, each  trained to find a different corner of the address block. A simple  set of rules was used to generate ABL candidates from the network  output. The system performs very well: when allowed five guesses,  the network will tightly bound the address delivery information in  98.2% of the cases.  1 INTRODUCTION  The U.S. Postal Service delivers about 350 milhon mail pieces a day. On this scale,  even highly sophisticated and custom-built sorting equipment quickly pays for itseft.  Ideally, such equipment would be able to perform optical character recognition  (OCR) over an image of the entire mail piece. However, such large-scale OCR is  impractical given that the sorting equipment must recognize addresses on 18 mail  pieces a second. Also, the large amount of advertising and other irrelevant text that  can be found on some mail pieces could easily confuse or overwhelm the address  recognition system. For both of these reasons, character recognition must occur  745  746 Wolf and Platt  Figure 1: Typical address blocks from our data set. Notice the wide variety in  the shape, size, justification and number of lines of text. Also notice the detached  ZIP code in the upper right example. Note: The USPS requires us to preserve the  confidentiality of the mail stream. Therefore, the name fields of all address block  figures in this paper have been scrambled for publication. However, the network  was trained and tested using unmodified images.  only on the relevant portion of the envelope: the destination address block. The  system thus requires an address block location (ABL) module, which draws a tight  bounding box around the destination address block.  The ABL problem is a challenging object recognition task because address blocks  vary considerably in their size and shape (see figure 1). In addition, figures 2 and 3  show that there is often a great deal of advertising or other information on the mail  piece which the network must learn to ignore.  Conventional systems perform ABL in two steps (Caviglione, 1990) (Palumbo,  1990). First, low-level features, such as blobs of ink, are extracted from the im-  age. Then, address block candidates are generated using complex rules. Typically,  there are hundreds of rules and tens of thousands of lines of code.  The architecture of our ABL system is very different from conventional systems.  Instead of using low-level features, we train a neural network to find high-level  abstract features of an address block. In particular, our neural network detects  the corners of the bounding box of the address block. By finding abstract features  instead of trying to detect the whole address block in one step, we build a large  degree of scale and shape invariance into the system. By using a neural network,  we do not need to develop explicit rules or models of address blocks, which yields a  more accurate system.  Because the features are high-level, it becomes easy to combine these features into  object hypotheses. We use simple address block statistics to convert the corner  features into object hypotheses, using only 200 lines of code.  Postal Address Block Location Using a Convolutional Locator Network 747  2 SYSTEM ARCHITECTURE  Our ABL system takes 300 dpi grey scale images as input and produces a list of the  5 most likely ABL candidates as output. The system consists of three parts: the  preprocessor, a convolutional locator network, and a candidate generator.  2.1 PREPROCESSOR  The preprocessor serves two purposes. First, it substantially reduces the resolution  of the input image, therefore decreasing the computational requirements of the  neural network. Second, the preprocessor enhances spatial frequencies in the image  which are associated with address text. The recipe used for the preprocessing is as  follows:  1: Clip the top 20, of the image.  2: Spatgaily filter with a passband of 0.3 to 1.4min.  3: Take the absolute value of each pixel.  4: Low-pass filter and subsample by a factor of 16 in X and Y.  5: Perform a linear contrast stretch, mapping the darkest  pixel to 1.0 and the lightest pixel to 0.0.  The effect of this preprocessing can be seen in figures 2 and 3.  2.2 CONVOLUTIONAL LOCATOR NETWORK  We use a convolutional locator network (CLN) to find the corners of the bounding  box. Each layer of a CLN convolves its weight pattern in two dimensions over the  outputs of the previous layer (LeCun, 1989) (Fukushima, 1980). Unlike standard  convolutional networks, the output of a CLN is a set of images, in which regions  of activity correspond to recognition of a particular object. We train an output  neuron of a CLN to be on when the receptive field of that neuron is over an object  or feature, and off everywhere else.  CLNs have been previously used to assist in the segmentation step for optical charac-  ter recognition, where a neuron is trained to turn on in the center of every character,  regardless of the identity of the character (Martin, 1992) (Platt, 1992). The recogni-  tion of an address block is a significantly more difficult image segmentation problem  because address blocks vary over a much wider range than printed characters (see  figure 1).  The output of the CLN is a set of four feature maps, each corresponding to one  corner of the address block. The intensity of a pixel in a given feature map represents  the likelihood that the corresponding corner of the address block is located at that  pixel.  Figure 4 shows the architecture of our convolutional locator network (CLN). It has  three layers of trainable weights, with a total of 22,800 free parameters. The network  was trained via weight-shared backpropagation. The network was trained for 23  epochs on 800 mail piece images. This required 125 hours of cpu-time on an i860  based computer. Cross validation and final testing was done with two additional  748 Wolf and Platt  ................................................................................................... '" .' ' '.E ..... -'- ...... " .....  ...............  : ............... :-.'-:-:-:-:, o:x"-'.-,x-:-.-.:- .............. :--.-.-- ............................................. 'i:..:!: ............... .."-:>'<"3.':'-'"-": ............ ': ............  .:..-.:.:.:.:.:..-..-.:.:,'-:.:.  , , :-: ....... . ............... ... ............. :-:::: ....................... .:'.:-...:!..- .... .:.:..,:.: ........................ .. ................... . ....  :.:.:..-.:.:.:.:.:.:.:.:.:..-.: ., '.,., ...:.:$t1:.:]ll:lE.$.i:-..:!........x-..:.:.:.:....:.:.:.:.:.:-:.:.:.:.:.-..:.:.-..:.......:....:-:.:-:.:.:...-:.:.:.:..?:....:.:.:.:.:.:.:.:.:.:.:.:.:.:.:....:.:.:.:.:.:.:.:.:<.:.:.:.:  ====================== '- : .:::::::::::::::::::::::E:E:i:E:i:i:i:i:i:i:i:i:i:i:E:!:i:i:i:i:!:i:E:i:i::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::.:::::::  . =====================: . ..'.' . :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::  :::::::::::::::::::::::::::::::::: .' ..  .E .. x.-  n' .-. :'..  n. - .' ..:.:.:.:.::.:.;-:.:.:-::.:.::.:.:.:.:.:.:.:..:.:.:.:.::.:.:.::::.:::.:::.:.:::---::::::.:-::::.::.:::.:.:.:.:.:.:.:.:.:.  ..................... :.: ..... .....:....-': ......... ...'....:.:..... ......... . ..................................................................... . ....................  ...........  ... ,.. .......: :..:. .....: ...x-...-...-.-.-..-..-.: ..-..-.:.:..-.: .:. :.:. :.:.:-: ..-.:..-..-.:.-..:. :..-.:.:..-..-. :..-..-.:.:.:.:.:. :..-...-: :: ::: .':: :::-::::.'.-: .-::.'.-: :: .-:: :::::::::::::::::::::::::::: :: :.'.-::: .-::'-:.-.'::.-. '...-::::::: .-.'.-: :::'.::.-'..-'.: :.-:: :.-:::::: ::-'.-:::;:.'::: :: ::::::: ::::  .::::::::::::::..::::::-...::::::::::::-:.-:..:::.-:::::::::::5:::.:.:.:---::::.:.:.:::::..--:.--<--...:-..:::..::::::.::.-.:.:.:.-.::::-'..5:.':,-...::..--:::::..-::.:::----:---:::.-..:.:.:.:::.::::-.:.:::.:::::::::-.-i:E::....-:.:---....-.::::-.:::-.::.:.:.--c.::.:...-:..-::-.:.:.:.:.:.:.:.:-...:.:.::.:.:.-.;.:-::::::::..-.-:::.:....:::::....  ::.:: ................ ' '.'-'',: ' :" ":'":' ':'"' :' ': :'"' "":'":'""":' '":E:':':': ':'"":': -:' ':':: :':':':': :':':':'":':.-.:" :'":':':':'.'-.E::E!:i:i::!E: ..': '"':':':iiE!EE!ii:'::.::':':'E!:E::!i!E:E.i:ili!  ............. "' '"'""'"'"'"'"7" :. ..... : . .:.' ..' .' '"'""" ' '. ":"' ' '"" :'"': . ' ' '";';'"'"':;-:-';o-;':;;:'::-:;'--';'-';.-;'::::':';-;:.;:?'---'-':..;'  -2::..:.-.-.--..-.-.:7.---..-.--....-.---3.:-.-:.....-..--..-..-..-.:-..-.-.-.-..-.............-......-..... "2.:'"?'"'""'-'-'-.' :'":-".-.-'-::-: '"':-:-."-:': ....... '"-' '-'-'-:-.':-:':-'"'. ':-":".'.':'..   .?...-.-.......-.-.. .:...---.---..---:..--.:.-..-.- ..... ........ -.. -...-:.:..-..--.-- -.. -..-.-.--. ......... ....- ....... '....... ...... '.......-........................ ..;..-......-.............-..........  ...:......:...:..:...:.-:......., ?.-., .:. -x.:,,-.-.,x:.:. :.,..:::::.:.$.--. ::..- ... : ..-....':.: .........:.... . .. -...-..... ....... -. .-.--. .... .............................  ..... ..'.-." -.  :. . ........ . '. ',',#_-'::-...-:.-:.-.-..-,,,:.".'. "-.': ..'""'.:'. '..'...' .'.:.......:...'....':::?'?".'"'?':'??'"'?:::::"':?:E".'"'??''C'    ....... ' ...... '":'.'::.'::.:::::::5::f..:;:. ::. :.' :::::: :-'::: :: ::::::::-'::::5.':::: :::.':: :::::.':: ::'..: ::::;: :.':.';::."-:.;::::5.': 5.' :.':::5.':'.:::::.':.'::iE ::-:-: J-:: '-: $-:-: 3 $ .-'.: .-'.:.:.'-.-'.-'..'.:.-'.-'..'.-'3.-'.:.-'.-'.:.-'.-'.-'$.:.-' ;d .:.-'.'-.-'.:.-5: : .-'.'-.-'.'-.-...'-.-.-'.....-'..., .'.-.....-...-  :!::7.. ========================== '::" ' . ...... :' ":' ':::.-.. "".'::- ':-:::!:' :.:.!::i...--.-.-.---.-.-:---..------.--..-.------.-:--.--........-.-:...--:..:-:.::.:......:..:::.:....:.::...::......:::  ""'" ' ":i'"'- ":::.. :.-:5'" ::?i ' '.': :.! :.".".':'-" "-" ::.i:i:i ' 'i'i :':: .:"--- ':'"'-'"" "'- "'"'" :'"'"':'" "-"" '-:'" ............ "'"":'""":"'":'.  ............. ..... :..!......-. . ..-,.,.-....................... :: ..... ...-.....................................................................................................................................  ...... ""': '----- '-'-. - '. - "?. :'-!i: .............. -----... - :,::. !: :..---......:....---..:-.:: ....................................... . .............................................   : :. ..... ::'::':";:'-:.6;::.',d'::.: "'""x-',:&:',.-':-',:E.. .':..::.-.-.'.'. ...... -.'.-.'. ....... '. ..... ........................-:.............................................................  -.:.:-. :........-....:,.:.:E:::::........... ... . ..............:.....::...:.:e..:i::.....:.:.:...:.:...:.:.....:.......: ........... :.:.:.:.:.:.:.:.:.:.:.:.:.:.:.:.:.:.:.:.:.:.:.:.:.:.:.:.:.:.:.:.:.:.:.:.:.:.:.:::.:.:.:.:.:.:.  y::-:::::::-:.:.:::-:-::.-,x:: '?i "-:'e:,..........':_.'i-:'..::.-..':..,:..'...':..:.:.:... '.::'.'.:...:..:.:.:-:.:v:.......:.:.:.:...:.:.:.:.:.:.:...:-:.:.:...:.: :.:v:.:.....:....v..........-..:..  -;. ,. ,,..., ,,..  Figure 2: The network operating on an example from the test set. The top image  is the original image. The middle image is the image that is fed to the CLN after  preprocessing. The preprocessing enhances the text and suppresses the background  color. The bottom image is the first candidate of the ABL system. The output of the  system is shown with a white and black rectangle. In this case, the first candidate  is correct. Notice that our ABL system does not get confused by the horizontal  lines in the image, which would confound a line-finding-based ABL system.  Postal Address Block Location Using a Convolutional Locator Network 749  :' :-'::: :::::::::::::::::::::::::::::::::::: ':':'.::::' :': ': ':' ffi i :" ii:ii:- :- ::: ':?' .'.::::: ! :: ?-:-"):: :5 :: :'-:: .'::: :: .': :::- ::: ::: ::.' ::::: :: ::::: :::::: :::  ::::-':::::::::::::::: :: :: .-:.-: :::::::::: .-: .:-:-': :: ::: :::::: ffi ::-'.':5:::::-' :..::.-'i :.: :.' ::: [:.": .':::i:-'.": !:5-'.":3 i:i :i ::'-:' :t[:i :...&l..:. -t:::::[:::::::  - ':.:. :..4-.'"':..... - '. ::'" ' ' -.4::: - ' ......... =====================================================================================================================================   :-:5.. '.::: .-..' ' ! :: : '::.:"-'! :: .... :;"i ..... i".:"-" .:- ."--:.::.!:!:",.,x:.'i:-'. :::' -':.'..:. ............................... :''":::'--'::':'-"-"-":':"-'-":"-'-':::::::::::-'-?:i:i:!:ii!:  :4:  '" '^ ..:. ..-' .:'-- '-: ' :5.": .: *-:'-' ' .c.". -- ::'-':- .. ..   ...x-'q .............. ... ....... *P -. ............. .,,- -'. .-. ........  ............... . .....  ::::::.':..'.x  :'-':: :'-":':'':[:"--' '"'"' :' .-'-"-x--"-' .-' .... ---- .-:-:-;. !-{t.- - -:-'...- *.. .  :  ' .-x-.. :  i-::::><. q.. :.::,i:....': '.:.'..-.'-.'/.:;:j :. , .-.-':::: ... ' ...': - .-., - ."&-'.-..,-::::: .... -,..&2.-:.-':.-'.'.' '"" .:x.-:.... -.-:....:-.:.?-  :-" ]tJ.  :':':'-"::: :'-":' -!j..x(i5.:..-.:..-x..-.::qq:.:.:--  :..-.:.:.:.:.:,:.:::i.::.:::. :..., ..:::..-, ._. -,.:-" .:.'.,:::.--  ::-' .:-x .:;. -:::]:-":':.,/.:]...:. :.::2-..'-:.:'.'.:::::;.-'-'-'.-::':::::::.- .: "-::::::."-.''.-'::-':::::.:::':.;. '. ::::::::-x-:.::-'-.:.:::: -' '' -:' '::-'   .; -i;i:.-.:-' ?..-.,.:,..-.:-.-:.'.:.:....,.::.::'.-:':.:":... --'.'.:--..'..-.:....-.'.-.-'-:.,:::::q: .:.:':'-:'::':- -".:;::. ::.. ,:::?:':.-:i:-'..:.-!5 .:::.-.-:::. --i::.'.:- '::::::  ,..;\' .: .......  ..... ,.:-" .......  ..... :::-.. ::'..-:.....-'-?.! . ,.,: .................. ..:.  --..:-:.'::..:>.',''x._-.-,:.-_-- - -.- .-- --,--...-.-.-.. -.-.,-..-.-.-. -.-,,-.----:-_":dqxZ. -x-:-.- .> , :x".::-' ..... :-  ,-:-g]I:.  ..--  ..  ..... -.-.:.-: .,,. ,, : .... .:..:::5..::::::....::..:::::..:i:.::::.$.:i:...:::::.:.:::./::::::..:::..::..:::::.......... ,:.. ::a::::-':-'::-'   .-.-,--.. :'.q .,.,,z.q..x> ..t, .. ,,, ........... ..;./...l{-:-}x-,,.-.-.-,,,,,- ................... .-.-.-.-.-.-.-...-.-.-.-.-.-.-.-.-.....-.,-.-.-.-..,.... ............ :.  :!:::: :'::i:-X-':.:: ::.::::. :.::.:.:.'.-':.-':!i]l '" 'z :: :::::::::::::::::::::::::::::::::::::::::::::: 8g ::::::::::::::::::::::::::::::..:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::....:::::: J'?.-.:::::i::....-.']::i:.:. ::.  ........................ :. ......... :.:..```:.:...........:.:.:.:....:....:...`..:.:.:.:.......,.:.......:....:`.`..:.:.:.:.:.:.:.:.:..:.:.:.:.:.:....:.:.......:.:.:.:....:.::.:.:.....::.:....:.......:.......:.:.:..:.:.:.....`......:...*.:.:..:.:.....:....:.:.:.:.:..........:.:.:.:.:.  ================================================================================================================================================================================================================================================================================================================================  ....  :--'--.'--- ...... .;.';..."-.'' '-" "" " "'"'- '"  ""'' "'"'"...:':'-.-:.;[!:>;.>>.'->:-:-',,:-:.:.x-:-:.>>:.>>>: :-:.:.:.:.-.-...---...-.-.'--.........:.....:.:.:...:............................:.:.::::::.:.......:....  :.:: ============================================================================================== :::::::::::::::::::::::::::::::::::::::::::::: ":8: J'/3:i;.;:-'"::::':':::;:'" .....-'.;-':  .::::::::::.'::.:'..-ix : ...-:-.'-::::x.:z: :, -': ....'-':-' ffi :::::.:-. :-- -.':.'- ffi '..::, :.:-' .::::::-.'-:-'-.-.-'--'-:.:.:-:-: ..... '- - . :.-::-:-.-:-:-"-:q '-. '-:.  -.....: .... :-x. "-  -:.:.>:,,:,, ........... .'?.....-.-.,s..-.-.:. ........ -..- :.:'.:-....::::---"'",, .... .::,-......:,-'.-- ..:o:.,,... ......... ;.:,g:i: ........ :....-'-: ...:,.:..-'7.!..,...:.-...g  .. - ..... ., ..... - '*:::-5-?.':...,:.;.:.. ............... ':-' '-... '.J:. :.' -::::::'.  .... .. .::.Ji*l.- . ." .........:'.-.'xl, .'.. '. '.:::::-:-::.1l:... ...... :......::::--  .... .'-:-:-: .... .'" ''"'...:."8.'-?.:/::.;-.'-:.:.:..- :..'..,'.'-'-- ..:-:.:.:.""":--'.,'z"'.,11ili!'- "::'..:.:-'_::.:.--.. .--' -.-.-.- ..: - .. . .:..,.. -...... ,. ' '"' "'x':':'.......':.':'rff:..-..-,x..-.:..x.-.:!  .Sii.-".-i!i!:[::'.'....:,*?.:x4.:'iiiii:ii ,::: ......:-.':'::-' '-::%..-i ...... '--'- '"'". :""::-'.'::'---'.'--. .:'-'---:--:'-:::.. ':'::::'-':---. ...... ::::-.:::.-: "-" ' '-';':' '"-- ---'::.  -...:':--:,:-!,..':..:;ii"!:  :x-y.-..-.:.:.:-'-'./s.:...--' :.:'.-? '-"---'../-'., ,. ,, :. .,, '--, ..:?......:.:.: ..... .>:....:; ........  ';::' ':+ :-':':':':':':':':'-":':':':" x-"'". .... ;::'-'-'::'.:' ..':'"-" 2:  .::' :x -'.'::::-.-.--"-"'-':':"-':':':':': ...:.. . . . :.,x.'$.. *.....::, :'-":'-' i:'   '- '-'-' -' '-'-:'-' "-'-'-'-'. --' .... >'-'-'.'--:..'-:-........-...-.-.-,,-,..-..,-,-.-.... .:..-...-.. ..:-:.:.:  :... x-:- :.:.:..: .::::::: :.-"-::. '- r:' '::-' :-.'-- ":::::5:',;-',;:::' "'-'""':"' :;:-:-:-:,>:.'-'-'-'-'-:-'-'-:;.':;:.'..'-:-.'--;-.-.;-.-..'  .:.;.?:i::::i::"-'-'-'--'-'--"-:-:..-:,.i.-.-.----...-.. :`:..{:::!::::::.:......:::..::::::::::::::.:.::.:.:.! :.:-.--.- .... :. ,.., ..: .... ., ::::::::::::::::::::::::: .......................................................  :-:-:- :-:- , : -.:.:-'. . ':. ======================================================================= ':::::S:  .:.:.: ................................. : .................. .: ................. 1 .......  Figure 3: Another example from the test set. The preprocessed image still has a  large amount of background noise. In this example, the first candidate of the ABL  system (shown in the lower left) was almost correct, but the ZIP code got truncated.  The second candidate of he system (shown in the lower right) gives the complete  address.  750 Wolf and Platt  Third layer of weights  4 36x16 windows  Second layer of weights  8 9x9 windows  First layer of weights  6 9x9 windows  Output maps  Second layer feature maps  2x2 subsampled first layer  feature maps  First layer feature maps  Input image  Figure 4: The architecture of the convolutional locator network used in our ABL  system.  data sets of 500 mail pieces each. All together, these 1800 images represent 6 Gbytes  of raw data, or 25 Mbytes of preprocessed images.  2.3 CANDIDATE GENERATOR  The candidate generator uses the following recipe to convert the output maps of  the CLN into a list of ABL candidates:  1: Find the top 10 local maxima in each feature map.  2: Construct all possible ABL candidates by combining pairs  local maxima Irom opposing corners.  3: Discard candidates which have negative length or width.  4: Compute conlidence ol each candidate.  6: Sort the candidates according to conlidence.  6: Remove duplicate and near duplicate candidates.  7: Pad the candidates by a lixed amount on all sides.  The confidence of an address block candidate is:  Caddress block -- Psizeocation H C/  i=1  where Caddr block is the confidence of the address block candidate, rsie is the  prior probability of finding an address block of the hypothesized size, /ocation is  the prior probability of finding an address block in the hypothesized location, and  Postal Address Block Location Using a Convolutional Locator Network 751  C'i are the value of each of the output maxima. The prior probabilities Psie and  Pocation were based on smoothed histograms generated from the training set and  validation set truths.  Steps 6 and 7 each contain 4 tuning parameters which we optimized using the  validation set and then froze before evaluating the final test set.  3 SYSTEM PERFORMANCE  Figures 2 and 3 show the performance of the system on two challenging mail pieces  from the final test set. We examined and classified the response of the system to all  500 test images. When allowed to produce five candidates, the ABL system found  98.2% of the address blocks in the test images.  More specifically, 96% of the images have a compact bounding box for the complete  address block. Another 2.2% have bounding boxes which contain all of the delivery  information, but omit part of the name field. The remaining 1.8% fail, either  because none of the candidates contain all the delivery information, or because  they contain too much non-address information. The average number of candidates  required to find a compact bounding box is only 1.4.  4 DISCUSSION  This paper demonstrates that using a CLN to find abstract features of an object,  rather than locating the entire object, provides a reasonable amount of insensitivity  to the shape and scale of the obj.ect. In particular, the completely identified address  blocks in the final test set had aspect ratios which ranged from 1.3 to 6.1 and their  absolute X and Y dimensions both varied over a 3:1 range. They contained anywhere  from 2 to 6 lines of text.  In the past, rule-based systems for object recognition 'were designed from scratch  and required a great deal of domain-specific knowledge. CLNs can be trained to  recognize different classes of objects without a lot of domain-specific knowledge.  Therefore, CLNs are a general purpose object segmentation and recognition archi-  tecture.  The basic computation of a CLN is a high-speed convolution, which can be cost-  effectively implemented by using parallel hardware (Sickinger, 1992). Therefore,  CLNs can be used to reduce the complexity and cost of hardware recognition sys-  tems.  5 CONCLUSIONS  In this paper, we have described a software implementation for an address block  location system which uses a convolutional locator network to detect the corners of  the destination address on machine printed mail pieces.  The success of this system suggests a general approach to object recognition tasks  where the objects vary considerably in size and shape. We suggest the following  752 Wolf and Platt  three-step approach: use a simple preprocessing algorithm to enhance stimuli which  are correlated to the object, use a CLN to detect abstract features of the objects in  the preprocessed image, and construct object hypotheses by a simple analysis of the  network output. The use of CLNs to detect abstract features enables versatile object  recognition architectures with a reasonable amount of scale and shape invariance.  Acknowledgement s  This work was funded by USPS Contract No. 104230-90-C-3441. The authors would  like to thank Dr. Binh Phan of the USPS for his generous advice and encourage-  ment. The images used in this work were provided by the USPS.  References  Caviglione, M., Scaiola, (1990), "A Modular Real-time Vision System for Address  Block Location," Proc. Jth Advanced Technology Conference, USPS, 42-56.  Fukushima, K., (1980), "Neocognitron: A Self-Organizing Neural Network Model  for a Mechanism of Pattern Recognition Unaffected by Shift in Position." Biological  Cybernetics, 36, 193-202.  LeCun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R. E., Hubbard, W.,  Jackel, L. D., (1989), "Backpropagation Applied to Handwritten Zip Code Recog-  nition" Neural Computation, 1, 541-551.  Martin, G., Rashid, M., (1992), "Recognizing Overlapping Hand-Printed Characters  by Centered-Object Integrated Segmentation and Recognition," Advances in Neural  Information Processing Systems, 4, 504-511.  Palumbo, P. W., Soh, J., Srihari, S. N., Demjanenjo, V., Sridhar, R., (1990), "Real-  Time Address Block Location using Pipelining and Multiprocessing," Proc.  Advanced Technology Conference, USPS, 73-87.  Platt, J., Decker, J. E, LeMoncheck, J. E., (1992), "Convolutional Neural Networks  for the Combined Segmentation and Recognition of Machine Printed Characters,"  Proc. 5th Advanced Technology Conference, USPS, 701-713.  Sickinger, E., Boser, B., Bromley, J., LeCun, Y., Jackel, L., (1992) "Applica-  tion of the ANNA neural network chip to high-speed character recognition," IEEE  Trans. Neural Networks, 3, (3), 498-505.  
Temporal Difference Learning of  Position Evaluation in the Game of Go  Nicol N. Schraudolph Peter Dayan Terrence J. Sejnowski  schraudosalk. edu dayansalk. edu t errysalk. edu  Computational Neurobiology Laboratory  The Salk Institute for Biological Studies  San Diego, CA 92186-5800  Abstract  The game of Go has a high branching factor that defeats the tree  search approach used in computer chess, and long-range spa-  tiotemporal interactions that make position evaluation extremely  difficult. Development of conventional Go programs is hampered  by their knowledge-intensive nature. We demonstrate a viable  alternative by training networks to evaluate Go positions via tem-  poral difference (TD) learning.  Our approach is based on network architectures that reflect the  spatial organization of both input and reinforcement signals on  the Go board, and training protocols that provide exposure to  competent (though unlabelled) play. These techniques yield far  better performance than undifferentiated networks trained by self-  play alone. A network with less than 500 weights learned within  3,000 games of 9x9 Go a position evaluation function that enables  a primitive one-ply search to defeat a commercial Go program at  a low playing level.  1 INTRODUCTION  Go was developed three to four millenia ago in China; it is the oldest and one of the  most popular board games in the world. Like chess, it is a deterministic, perfect  information, zero-sum game of strategy between two players. They alternate in  817  818 Schraudolph, Dayan, and Sejnowski  placing black and white stones on the intersections of a 19x19 grid (smaller for  beginners) with the objective of surrounding more board area (territory) with their  stones than the opponent. Adjacent stones of the same color form groups; an empty  intersection adjacent to a group is called a liberty of that group. A group is captured  and removed from the board when its last liberty is occupied by the opponent. To  prevent loops, it is illegal to make a move which recreates a prior board position. A  player may pass at any time; the game ends when both players pass in succession.  Unlike most other games of strateg35 Go has remained an elusive skill for computers  to acquire -- indeed it has been recognized as a "grand challenge" of Artificial  Intelligence (Rivest, 1993). The game tree search approach used extensively in  computer chess is infeasible: the game tree of Go has an average branching factor  of around 200, but even beginners may routinely look ahead up to 60 plies in  some situations. Humans appear to rely mostly on static evaluation of board  positions, aided by highly selective yet deep local lookahead. Conventional Go  programs are carefully (and protractedly) tuned expert systems (Fotland, 1993).  They are fundamentally limited by their need for human assistance in compiling  and integrating domain knowledge, and still play barely above the level of a  human beginner -- a machine learning approach may thus offer considerable  advantages. (Bragmann, 1993) has shown that a knowledge-free optimization  approach to Go can work in principle: he obtained respectable (though inefficient)  play by selecting moves through simulated annealing (Kirkpatrick et al., 1983) over  possible continuations of the game.  The pattern recognition component inherent in Go is amenable to connectionist  methods. Supervised backpropagation networks have been applied to the game  (Stoutamire, 1991; Enderton, 1991) but face a bottleneck in the scarcity of hand-  labelled training data. We propose an alternative approach based on the TD(,X)  predictive learning algorithm (Sutton, 1984; Sutton, 1988; Barto et al., 1983), which  has been successfully applied to the game of backgammon by (Tesauro, 1992).  His TD-Gammon program uses a backpropagation network to map preselected  features of the board position to an output reflecting the probability that the player  to move would win. It was trained by TD(0) while playing only itself, yet learned  an evaluation function that coupled with a full two-ply lookahead to pick the  estimated best move -- made it competitive with the best human players in the  world (Robertie, 1992; Tesauro, 1994).  In an early experiment we investigated a straightforward adaptation of Tesauro's  approach to the Go domain. We trained a fully connected 82-40-1 backpropagation  network by randomized  self-play on a 9x9 Go board (a standard didactic size for  humans). The output learned to predict the margin of victory or defeat for black.  This undifferentiated network did learn to squeak past Wally, a weak public domain  program (Newman, 1988), but it took 659,000 games of training to do so. We have  found that the efficiency of learning can be vastly improved through appropriately  structured network architectures and training strategies, and these are the focus of  the next two sections.   Unlike backgammon, Go is a deterministic game, so we had to generate moves stochas-  tically to ensure sufficient exploration of the state space. This was done by Gibbs sam-  pling (Geman and Geman, 1984) over values obtained from single-ply search, annealing the  temperature parameter from random towards best-predicted play.  Temporal Difference Learning of Position Evaluation in the Game of Go 819  . evaluation  reinforcement map --  symmetry  g rou ps  processed  featu res  constraint satisfaction )   f raw feature maps  .'1   Go board  connectivity map  symmetry  groups   '''  Figure 1' A modular network architecture that takes advantage of board symme-  tries, translation invariance and localized reinforcement to evaluate Go positions.  Also shown is the planned connectivity prediction mechanism (see Discussion).  2 NETWORK ARCHITECTURE  One of the particular advantages of Go for predictive learning is that there is much  richer information available at the end of the game than just who won. Unlike  chess, checkers or backgammon, in which pieces are taken away from the board  until there are few or none left, Go stones generally remain where they are placed.  This makes the final state of the board richly informative with respect to the course  of play; indeed the game is scored by summing contributions from each point on  the board. We make this spatial credit assignment accessible to the network by  having it predict the fate of every point on the board rather than just the overall  score, and evaluate whole positions accordingly. This bears some similarity with  the Successor Representation (Dayan, 1993) which also integrates over vector rather  than scalar destinies. '  Given the knowledge-based approach of existing Go programs, there is an embar-  rassment of input features that one might adopt for Go: Wally already uses about  30 of them, stronger programs disproportionately more. In order to demonstrate  reinforcement learning as a viable alternative to the conventional approach, how-  ever, we require our networks to learn whatever set of features they might need.  The complexity of this task can be significantly reduced by exploiting a number  2 Sharing information within the network across multiple outputs restricts us to ,X = 0  for efficient implementation of TD(,X). Note that although (Tesauro, 1992) did not have this  constraint, he nevertheless found ,X = 0 to be optimal.  820 Schraudolph, Dayan, and Sejnowski  of constraints that hold a priori in this domain. Specificall) patterns of Go stones  retain their properties under color reversal, reflection and rotation of the board,  and -- modulo the considerable influence of the board edges -- translation. Each  of these invariances is reflected in our network architecture:  Color reversal invariance implies that changing the color of every stone in a Go  position, and the player whose turn it is to move, yields an equivalent position from  the other player's perspective. We build this constraint directly into our networks  by using antisymmetric input values (+1 for black, -1 for white) and squashing  functions throughout, and negating the bias input when it is white's turn to move.  Go positions are also invariant with respect to the eightfold (reflection x rotation)  symmetry of the square. We provided mechanisms for constraining the network  to obey this invariance by appropriate weight sharing and summing of derivatives  (Le Cun et al., 1989). Although this is clearly beneficial during the evaluation of  the network against its opponents, it appears to impede the course of learning?  To account for translation invariance we use convolution with a weight kernel  rather than multiplication by a weight matrix as the basic mapping operation in  our network, whose layers are thus feature maps produced by scanning a fixed  receptive field across the input. One particular advantage of this technique is the  easy transfer of learned weight kernels to different Go board sizes.  It must be noted, however, that Go is not translation-invariant: the edge of the board  not only affects local play but modulates other aspects of the game, and indeed  forms the basis of opening strategy. We currently account for this by allowing each  node in our network to have its own bias weight, giving it one degree of freedom  from its neighbors. This enables the network to encode absolute position at a  modest increse in the number of adjustable parameters. Furthermore, we provide  additional redundancy around the board edges by selective use of convolution  kernels twice as wide as the input.  Figure 1 illustrates the modular architecture suggested by these deliberations. In  the experiments described below we implement all the features shown except for  the connectivity map and lateral constraint satisfaction, which are the subject of  future work.  3 TRAINING STRATEGIES  Temporal difference learning teaches the network to predict the consequences of  following particular strategies on the basis of the play they produce. The question  arises as to which strategies should be used to generate the large number of Go  games needed for training. We have identified three criteria by which we compare  alternative training strategies:  the computational efficiency of move generation,  the quality of generated pla x and  reasonable coverage of plausible Go positions.  3 We are investigating possible causes and cures for this phenomenon.  Temporal Difference Learning of Position Evaluation in the Game of Go 821  Tesauro trained TD-Gammon by self-play -- ie. the network's own position eval-  uation was used in training to pick both players' moves. This technique does not  require any external source of expertise beyond the rules of the game: the network  is its own teacher. Since Go is a deterministic game, we cannot always pick the  estimated best move when training by self-play without running the risk of trap-  ping the network in some suboptimal fixed state. Theoreticall59 this should not  happen -- the network playing white would be able to predict the idiosyncrasies  of the network playing black, take advantage of them thus changing the outcome,  and forcing black's predictions to change commensurately-- but in practice it is a  concern. We therefore pick moves stochastically by Gibbs sampling (Geman and  Geman, 1984), in which the probability of a given move is exponentially related to  the predicted value of the position it leads to through a "temperature" parameter  that controls the degree of randomness.  We found self-play alone to be rather cumbersome for two reasons: firstl the  single-ply search used to evaluate all legal moves is computationally intensive --  and although we are investigating faster ways to accomplish it, we expect move  evaluation to remain a computational burden. Secondly, learning from self-play is  sluggish as the network must bootstrap itself out of ignorance without the benefit of  exposure to skilled opponents. However, there is nothing to keep us from training  the network on moves that are not based on its own predictions  for instance, it  can learn by playing against a conventional Go program, or even by just observing  games between human players.  We use three computer opponents to train our networks: a random move generator,  the public-domain program Wally (Newman, 1988), and the commercial program  The Many Faces of Go (Fotland, 1993). The random move generator naturally doesn't  play Go very well 't, but it does have the advantages of high speed and ergodicity  -- a few thousand games of random Go proved an effective way to prime our  networks at the start of training. The two conventional Go programs, by contrast,  are rather slow and deterministic, and thus not suitable generators of training data  when playing among themselves. However, they do make good opponents for the  network, which can provide the required variety of play through its Gibbs sampler.  When training on games played between such dissimilar players, we must match  their strength so as to prevent trivial predictions of the outcome. Against Many Faces  we use standard Go handicaps for this purpose; Wally we modified to intersperse its  play with random moves. The proportion of random moves is reduced adaptively  as the network improves, providing us with an on-line performance measure.  Since, in all cases, the strategies of both players are intimately intertwined in the  predictions, one would never expect them to be correct overall when the network  is playing a real opponent. This is a particular problem when the strategy for  choosing moves during learning is different from the policy adopted for 'optimal'  network play. (Samuel, 1959) found it inadvisable to let his checker program  learn from games which it won against an opponent, since its predictions might  otherwise reflect poor as well as good play. This is a particularly pernicious form  of over-fitting -- the network can learn to predict one strategy in exquisite detail,  without being able to play well in general.  4In order to ensure a minimum of stability in the endgame, it does refuse to fill in its own  eyes -- a particular, locally recognizable type of suicidal move.  822 Schraudolph, Dayan, and Sejnowski  h0--+reinf  board-.h0 tum--*h0  hl-*reinf  architecture  {r ...... ]{r  l r ......... I'q'r  board-*hl tum--*hl board-.reinf tum--*reinf  Figure 2: A small network that learned to play 9x9 Go. Boxes in the architecture  panel represent 9x9 layers of units, except for turn which is a single bias unit.  Arrows indicate convolutions with the corresponding weight kernels. Black disks  represent excitato, white ones inhibitory weights; within each matrix, disk area  is proportional to weight magnitude.  4 RESULTS  In exploring this domain, we trained many networks by a variety of methods.  A small sample network that learned to beat Many Faces (at low playing level)  in 9x9 Go within 3,000 games of training is shown in Figure 2. This network  was grown during training by adding hidden layers one at a time; although it  was trained without the (reflection x rotation) symmetry constraint, many of the  weight kernels learned approximately symmetric features. The direct projection  from board to reinforcement layer has an interesting structure: the negative central  weight within a positive surround stems from the fact that a placed stone occupies  (thus loses) a point of territory even while securing nearby areas. Note that the wide  17x17 projections from the hidden layers have considerable fringes ostensibly a  trick the network uses to incorporate edge effects, which are also prominent in the  bias projections from the turn unit.  We compared training this architecture by self-play versus play against Wally. The  initial rate of learning is similar, but soon the latter starts to outperform the former  (measured against both Wally and Many Faces), demonstrating the advantage of  having a skilled opponent. After about 2000 games, however, it starts to overfit to  Wally and consequently worsens against Many Faces. Switching training partner  to Many Faces at this point produced (after a further 1,000 games) a network that  could reliably beat this opponent. Although less capable, the self-play network did  manage to edge past Wally after 3,000 games; this compares very favorably with  Temporal Difference Learning of Position Evaluation in the Game of Go 823  the undifferentiated network described in the Introduction. Furthermore, we have  verified that weights learned from 9x9 Go offer a suitable basis for further training  on the full-size (19x19) board.  5 DISCUSSION  In general our networks appear more competent in the opening than further into  the game. This suggests that although reinforcement information is indeed propa-  gating all the way back from the final position, it is hard for the network to capture  the multiplicity of mid-game situations and the complex combinatorics character-  istic of the endgame. These strengths and weaknesses partially complement those  of symbolic systems, suggesting that hybrid approaches might be rewarding. We  plan to further improve network performance in a number of ways:  It is possible to augment the input representation of the network in such a way  that its task becomes fully translation-invariant. We intend to do this by adding an  extra input layer whose nodes are active when the corresponding points on the Go  board are empt)6 and inactive when they are occupied (regardless of color). Such  an explicit representation of liberties makes the three possible states of a point on  the board (black stone, white stone, or empty) linearly separable to the network,  and eliminates the need for special treatment of the board edges.  The use of limited receptive field sizes raises the problem of how to account for  long-ranging spatial interactions on the board. In Go, the distance at which groups  of stones interact is a function of their arrangement in context; an important sub-  problem of position evaluation is therefore to compute the connectivity of groups  of stones. We intend to model connectivity explicitly by training the network to  predict the correlation pattern of local reinforcement from a given position. This  information can then be used to control the lateral propagation of local features in  the hidden layer through a constraint satisfaction mechanism.  Finall we can train networks on recorded games between human players, which  the Internet Go Server provides in steady quantities and machine-readable format.  We are only beginning to explore this promising supply of instantaneous (since  prerecorded), high-quality Go play for training. The main obstacle encountered so  far has been the human practice of abandoning the game once both players agree on  the outcome-- typically well before a position that could be scored mechanically is  reached. We address this issue by eliminating early resignations from our training  set, and using Wally to bring the remaining games to completion.  We have shown that with sufficient attention to network architecture and training  procedures, a connectionist system trained by temporal difference learning alone  can achieve significant levels of performance in this knowledge-intensive domain.  Acknowledgements  We are grateful to Patrice Simard and Gerry Tesauro for helpful discussions, to Tim  Casey for the plethora of game records from the Internet Go Server, and to Geoff  Hinton for tniterations. Support was provided by the McDonnell-Pew Center for  Cognitive Neuroscience, SERC, NSERC and the Howard Hughes Medical Institute.  824 Schraudolph, Dayan, and Sejnowski  References  Barto, A., Sutton, R., and Anderson, C. (1983). Neuronlike adaptive elements that  can solve difficult learning control problems. IEEE Transactions on Systems,  Man, and Cybernetics, 13.  Brtigmann, B. (1993). Monte Carlo Go. Manuscript available by Internet anony-  mous file transfer from bsdserver. ucsf.edu, file Go/comp/mcgo.tex. Z.  Dayan, P. (1993). Improving generalization for temporal difference learning: The  successor representation. Neural Computation, 5(4):613-624.  Enderton, H. D. (1991). The Goleta Go program. Technical Report CMU-CS-92-  101, Carnegie Mellon University. Report available by Internet anonymous file  transfer from bsdserver. ucsf.edu, file Go/comp/golem.sh.Z.  Fotland, D. (1993). Knowledge representation in the Many Faces of Go. Manuscript  available by Internet anonymous file transfer from bsdserver. ucsf.edu, file  Go/comp/mfg.Z.  Geman, S. and Geman, D. (1984). Stochastic relaxation, gibbs distributions, and  the bayesian restoration of images. IEEE Transactions on Pattern Analysis and  Machine Intelligence, 6.  Kirkpatrick, S., Gelatt Jr., C. D., and Vecchi, M.P. (1983). Optimization by simulated  annealing. Science, 220:671-680.  Le Cun, Y., Boser, B., Denker, J., Henderson, D., Howard, R., Hubbard, W., and  Jackel, L. (1989). Backpropagation applied to handwritten zip code recognition.  Neural Computation, 1:541-551.  Newman, W. H. (1988). Wall a Go playing program. Shareware C program  available by Internet anonymous file transfer from bsdserver. ucsf.edu, file  Go / com p / w ally. sh.Z.  Rivest, R. (1993). MIT Press, forthcoming. Invited talk: Computational Learning  Theory and Natural Learning Systems, Provincetown, MA.  Robertie, B. (1992). Carbon versus silicon: Matching wits with TD-Gammon. Inside  Backgammon, 2(2):14-22.  Samuel, A. L. (1959). Some studies in machine learning using the game of checkers.  IBM Journal of Research and Development, 3:211-229.  Stoutamire, D. (1991). Machine learning applied to Go. Master's thesis, Case  Western Reserve University. Reprint available by Internet anonymous file  transfer from bsdserver. ucsf.edu, file Go/comp/report.ps.Z.  Sutton, R. (1984). Temporal Credit Assignment in Reinforcement Learning. PhD thesis,  University of Massachusetts, Amherst.  Sutton, R. (1988). Learning to predict by the methods of temporal differences.  Machine Learning, 3:9-44.  Tesauro, G. (1992). Practical issues in temporal difference learning. Machine Learn-  ing, 8:257-278.  Tesauro, G. (1994). TD-Gammon, a self-teaching backgammon program, achieves  master-level play. Neural Computation, 6(2):215-219.  
Optimal signalling in Attractor Neural  Networks  Isaac Meilijson Eytan Ruppin *  School of Mathematical Sciences  Raymond and Beverly Sackler Faculty of Exact Sciences  Tel-Aviv University, 69978 Tel-Aviv, Israel.  Abstract  In [Meilijson and Ruppin, 1993] we presented a methodological  framework describing the two-iteration performance of Hopfield-  like attractor neural networks with history-dependent, Bayesian  dynamics. We now extend this analysis in a number of directions:  input patterns applied to small subsets of neurons, general con-  nectivity architectures and more efficient use of history. We show  that the optimal signal (activation) function has a slanted sigmoidal  shape, and provide an intuitive account of activation functions with  a non-monotone shape. This function endows the model with some  properties characteristic of cortical neurons' firing.  I Introduction  It is well known that a given cortical neuron can respond with a different firing pat-  tern for the same synaptic input, depending on its firing history and on the effects  of modulator transmitters (see [Connors and Gutnick, 1990] for a review). The time  span of different channel conductances is very broad, and the influence of some ionic  currents varies with the history of the membrane potential [Lytton, 1991]. Moti-  vated by the history-dependent nature of neuronal firing, we continue.our previous  investigation [Meilijson and Ruppin, 1993] (henceforth, M & R) describing the per-  formance of Hopfield-like attractor neural networks (ANN) [Hopfield, 1982] with  history-dependent dynamics.  *Currently in the Dept. of Computer science, University of Maryland  485  486 Meilijson and Ruppin  Building upon the findings presented in M & R, we now study a more general  framework:  We differentiate between 'input' neurons receiving the initial input signal  with high fidelity and 'background' neurons that receive it with low fidelity.  Dynamics now depend on the neuron's history of firing, in addition to its  history of input fields.  The dependence of ANN performance on the network architecture can be  explicitly expressed. In particular, this enables the investigation of cortical-  like architectures, where neurons are randomly connected to other neurons,  with higher probability of connections formed between spatially proximal  neurons [Braitenberg and Schuz, 1991].  Our goal is twofold: first, to search for the computationally most efficient history-  dependent neuronal signal (firing) function, and study its performance with relation  to memoryless dynamics. As we shall show, optimal history-dependent dynamics  are indeed much more efficient than memoryless ones. Second, to examine the  optimal signal function from a biological perspective. As will shall see, it shares  some basic properties with the firing of cortical neurons.  2 The model  Our framework is an ANN storing m + 1 memory patterns ,2,...,m+x, each  an N-dimensional vector. The network is composed of N neurons, each of which  is randomly connected to K other neurons. The (m + 1)N memory entries are  independent with equally likely :t:l values. The initial pattern X, signalled by  L(_< N) initially active neurons, is a vector of :t:l's, randomly generated from one  of the memory patterns (say  = m+l) such that P(Xi = i) = 1+ for each of  2  1+,  the L initially active neurons and P(Xi = i) = 2 for each initially quiescent  (non-active) neuron. Although e,/  [0, 1) are arbitrary, it is useful to think of e  as being 0.5 (corresponding to an initial similarity of 75%) and of/ as being zero  - a quiescent neuron has no prior preference for any given sign. Let ax = rn/nx  denote the initial memory load, where nx = LK/N is the average number of signals  received by each neuron.  The notion of 'iteration' is viewed as an abstraction of the overall dynamics for  some length of time, during which some continuous input/output signal function  (such as the conventional sigmoidal function) governs the firing rate of the neuron.  We follow a Bayesian approach under which the neuron's signalling and activation  decisions are based on the a-posteriori probabilities assigned to its two possible true  memory states, :t:l.  Initially, neuron i is assigned a prior probability hi  - P( = llXi Ii ()) = 14-e  -- ' 2  or 1___ which is conveniently expressed as hi  - x where, letting g(t) -  -- lq.e_2gi(O ) '  - log l+t  2 1-t'  #i(o) _ ( #(e)Xi if i is active  #()Xi if i is silent  Optimal Signalling in Attractor Neural Networks 487  The input field observed by neuron i as a result of the initial activity is  N  nl j=l  where Ij () = 0, 1 indicates whether neuron j has fired in the first iteration, Iij = 0, 1  indicates whether a connection exists from neuron j to neuron i and Wij denotes  its magnitude, given by the Hopfield prescription  rn+l  (2)  As a result of observing the input field fi(1), which is approximately normally  distributed (given i,Xi and Ii()), neuron i changes its opinion about {i - 1}  from hi  to  ( ) 1  Ai (1) = P i = llXi,//(1) fi(1) = (3)  ' 1 q- e -201() '  expressed in terms of the (additive) generalized field gi(D = gi  + fi ().  We now get to the second iteration, in which, as in the first iteration, some of the  neurons become active and signal to the network. We model the signal function  neuron i emits as h(gi(), Xi, Ii()). The field observed by neuron i (with n2 updating  neurons per neuron) is  N  i,(2) = 1  n2 j=l  (4)  on the basis of which neuron i computes its posterior belief A,  = P(i =  11Xi , Ii (), fi (), fi ) and expresses its final choice of sign as Xi (2) = sign(Ai  -  0.5). The two-iteration performance of the network is measured by the final simi-  larity  S! = 1 + ! = P(Xi(2 ) = i) = 1 +  E7=1Xj(2)j (5)  2 2  3 Analytical results  The goals of our analysis have been: A. To present an expression for the performance  under arbitrary architecture and activity parameters, for general signal functions  h0 and h. B. Use this expression to find the best choice of signal functions which  maximize performance. We show the following:  The neuron's final decision is given by  Xi  = Sign [(Ao + Boli(D)Xi + Alfi (D + A2fi (2)] (6)  for some constants A0, B0, A and A2.  488 Meilijson and Ruppin  The performance achieved is  where, for some Aa > 0  n* nl q- mA3 '  2 + 2  x x  d  is the standard normal cumulative distribution function.  (7)  (8)  (9)  The optimal analog signal function, illustrated in figure 1, is  ho = h(gi (x), q,1, 0)=  h = h(gi(X),+l,1)= R(gi(1),e)- 1  where, for some A4 > 0 and A5 > 0, R(s,t) = A4 tanh(s) - As(s - g(t)).  (10)  (a) (b)  4.0  -- Silent neurons  .... Acve neurons  ......  2.0  V  0.0  -V  ..0  Signal  ,5 i Input field  -4.0  -5.0 -3.0 -1.0 1.0 3.0 5.0  Input field  Figure 1: (a) A typical plot of the slanted sigmoid, Network parameters are N -  5000, K - 3000, n = 200 and m = 50. (b) A sketch of its discretized version.  The nonmonotone form of these functions, illustrated in figure 1, is clear. Neurons  that have already signalled +1 in the first iteration have a lesser tendency to send  positive signals than quiescent neurons. The signalling of quiescent neurons which  receive no prior information (5 = 0) has a symmetric form. The optimal signal is  shown to be essentially equal to the sigmoid modified by a correction term depending  only on the current input field. In the limit of low memory load (e/vfG' --. oo), the  best signal is simply a sigmoidal function of the generalized input field.  Optimal Signalling in Attractor Neural Networks 489  To obtain a discretized version of the slanted sigmoid, we let the signal be sign(h(y))  as long as Ih(y)l is big enough - where h is the slanted sigmoid. The resulting signal,  as a function of the generalized field, is (see figure la and lb)  +1 y < fix (j) or/?4 (i) < y < fi5 (j)  hj(y) = -1 y > fi6 (i) or fi2 (j) < y < fia (j) (11)  otherwise  where -oo < fil  < 2  <_ fi3  < 4  5 5  < 6  <  and - < 1 (1) <  2(1) g a()) < 4(1)  (1) < () <  define, respectively, the firing pattern of  the neurons that were silent or active in the first iteration. To find the best such  discretized version of the optimal signal, we search numerically for the activity level  v which mimizes performance. Every activity level v, used as a threshold on Ih(u)l,  defines the (at most) twelve parameters O) (which are identified numerically via  the Newton-aphson method)  illustrated in figure lb.  4 Numerical Results  (a) (b)  Ilty-baed tgnalling  ] ..... Discretlzed ;tnalllng  ..... Analog optimal tgnalllng  -- Discrete signalling  ......... Analog signalling  0.,5 , A , I ,  .  , 0.940 , . I , t , I . F ,  0.0 1000.0 2000.0 30(10.0 4000.0 5000.0 0.0 1000.0 2000.0 3000.0 4000.0 5000.0  K K  Figure 2: Two-iteration performance as a function of connectivity K. (a) Network  parameters are N = 5000, n = 200, and m = 50. All neurons receive their input  state with similar initial overlap e =/ = 0.5. (b) Network parameters are N = 5000,  m=50, nx=200, e=0.5andS=0.  Using the formulation presented in the previous section, we investigated numeri-  cally the two-iteration performance achieved in several network architectures with  optimal analog signalling and its discretization. Already in small scale networks of a  few hundred neurons our theoretical calculations correspond fairly accurately with  490 Meilijson and Ruppin  0.970  simulation results. First we repeat the example of a corticM-like network investi-  gated in M & R, but now with optimal analog and discretized signalling. The nearly  identical marked superiority of optimal analog and discretized dynamics over the  previous, posterior-probability-based signalling is evident, as shown in figure 2 (a).  While low activity is enforced in the first iteration, the number of neurons allowed  to become active in the second iteration is not restricted, and best performance is  typically achieved when about 70% of the neurons in the network are active (both  with optimal signalling and with the previous, heuristic signalling).  Figure 2 (b) displays the performance achieved in the same network, when the input  signal is applied only to the small fraction (4%) of neurons which are active in the  first iteration (expressing possible limited resources of input information). We see  that (for K > 1000) near perfect final similarity is achieved even when the 96%  initially quiescent neurons get no initial clue as to their true memory state, if no  restrictions are placed on the second iteration activity level.   1 and contrasted the case (nx =  Next we have fixed the value of o: = vr  - ,  200, e = 0.5) of figure 2 (b) with (nl = 50, e = 1). The overall initial similarity ander  (hi = 50, e = 1) is only half its value under (hi = 200, e = 0.5). In spite of this, we  have found that it achieves a slightly higher final similarity. This supports the idea  that the input pattern should not be applied as the conventional uniformly distorted  version of the correct memory, but rather as a less distorted pattern applied only  to a small subset of the neurons.  (a) (b)  -- I signaling  .... Analog signalllno  0.94 .0 2000.0 4000.0 2)00.0 8000.0 10000.0 O. 0 1000.0 2000.0  N K  I ! [ / .... Mul-Iayered nee,,vok  ,/,) / ........ Lower bound peremnce  Figure 3: (a) Two-iteration performance in a full-activity network as a function of  network size N. Network parameters are nl = K -- 200, m = 40 and e - 0.5.  (b) Two-iteration performance achieved with various network architectures, as a  function of the network connectivity K. Network parameters are N = 5000, nx =  200, m = 50, e = 0.5 and/ = 0.  Figure 3 (a) illustrates the performance when connectivity and the number of sig-  Optimal Signalling in Attractor Neural Networks 491  hals received by each neuron are held fixed, but the network size is increased.  A region of decreased performance is evident at mid-connectivity (K -, N/2)  values, due to the increased residual variance. Hence, for neurons capable of  forming K connections on the average, the network should either be fully con-  nected or have a size N much larger than K. Since (unavoidable eventually)  synaptic deletion would sharply worsen the performance of fully connected net-  works, cortical ANNs should indeed be sparsely connected. The final similar-  ity achieved in the fully connected network (with N = K = 200) should be  noted. In this case, the memory load (0.2) is significantly above the criti-  cal capacity of the Hopfield network, but optimal history-dependent dynamics  still manage to achieve a rather high two-iterations similarity (0.975) from ini-  tial similarity 0.75. This is in agreement with the findings of [Morita, 1993,  Yoshizawa et al., 1993], who show that nonmonotone dynamics increase capacity.  Figure 3 (b) illustrates the performance achieved with various network architec-  tures, all sharing the same network parameters N, K, m and input similarity pa-  rameters n, ,/, but differing in the spatial organization of the neurons' synapses.  As evident, even in low-activity sparse-connectivity conditions, the decrease in per-  formance with Gaussian connectivity (in relation, say, to the upper bound) does not  seem considerable. Hence, history-dependent ANNs can work well in a cortical-like  architecture.  5 Summary  The main results of this work are as follows:  The Bayesian framework gives rise to the slanted-sigmoid as the optimal  signal function, displaying the non monotone shape proposed by [Morita,  1993]. It also offers an intuitive explanation of its form.  Martingale arguments show that similarity Under Bayesian dynamics per-  sistently increases. This makes our two-iteration results a lower bound for  the final similarity achievable in ANNs.  The possibly asymmetric form of the function, where neurons that have  been silent in the previous iteration have an increased tendency to fire in  the next iteration versus previously active neurons, is reminiscent of the  hi-threshold phenomenon observed in biological neurons [Tam, 1992].   In the limit of low memory load the best signal is simply a sigmoidal func-  tion of the generalized input field.  In an efficient associative network, input patterns should be applied with  high fidelity on a small subset of neurons, rather than spreading a given  level of initial similarity as a low fidelity stimulus applied to a large subset  of neurons.  If neurons have some restriction on the number of connections they may  form, such that each neuron forms some K connections on the average, then  efficient ANNs, converging to high final similarity within few iterations,  should be sparsely connected.  492 Meilijson and Ruppin  With a properly tuned signal function, cortical-like Gaussian-connectivity  ANNs perform nearly as well as randomly-connected ones.  Investigating the 0, 1 (silent, firing) formulation, there seems to be an in-  terval such that only neurons whose field values are greater than some low  threshold and smaller than some high threshold should fire. This seemingly  bizarre behavior may correspond well to the behavior of biological neurons;  neurons with very high field values have most probably fired constantly in  the previous 'iteration', and due to the effect of neural adaptation are now  silenced.  References  [Braitenberg and Schuz, 1991] V. Braitenberg and A. Schuz. Anatomy of the Cor-  tex: Statistics and Geometry. Springer-Verlag, 1991.  [Connors and Gutnick, 1990] B.W. Connors and M.J. Gutnick. Intrinsic firing pat-  terns of diverse neocortical neurons. TINS, 13(3):99-104, 1990.  [Hopfield, 1982] J.J. Hopfield. Neural networks and physical systems with emergent  collective abilities. Proc. Nat. Acad. Sci. USA, 79:2554, 1982.  [Lytton, 1991] W. Lytton. Simulations of cortical pyramidal neurons synchronized  by inhibitory interneurons. J. Neurophysiol., 66(3):1059-1079, 1991.  [Meilijson and Ruppin, 1993] I. Meilijson and E. Ruppin. History-dependent at-  tractor neural networks. Network, 4:1-28, 1993.  [Morita, 1993] M. Morita. Associative memory with nonmonotone dynamics. Neu-  ral Networks, 6:115-126, 1993.  [Tam, 1992] David C. Tam. Signal processing in multi-threshold neurons. In  T. McKenna, J. Davis, and S.F. Zornetzer, editors, Single neuron computation,  pages 481-501. Academic Press, 1992.  [Yoshizawa et al., 1993] S. Yoshizawa, M. Morita, and S.-I. Amari. Capacity of as-  sociative memory using a nonmonotonic neuron model. Neural Networks, 6:167-  176, 1993.  
Dual Mechanisms for Neural Binding  and Segmentation  Paul Sajda and Leif H. Finkel  Department of Bioengineering and  Institute of Neurological Science  University of Pennsylvania  220 South 33rd Street  Philadelphia, PA. 19104-6392  Abstract  We propose that the binding and segmentation of visual features  is mediated by two complementary mechanisms; a low resolu-  tion, spatial-based, resource-free process and a high resolution,  temporal-based, resource-limited process. In the visual cortex, the  former depends upon the orderly topographic organization in stri-  ate and extrastriate areas while the latter may be related to ob-  served temporal relationships between neuronal activities. Com-  puter simulations illustrate the role the two mechanisms play in  figure/ground discrimination, depth-from-occlusion, and the vivid-  ness of perceptual completion.  1 COMPLEMENTARY BINDING MECHANISMS  The "binding problem" is a classic problem in computational neuroscience which  considers how neuronal activities are grouped to create mental representations. For  the case of visual processing, the binding of neuronal activities requires a mecha-  nism for selectively grouping fragmented visual features in order to construct the  coherent representations (i.e. objects) which we perceive. In this paper we argue for  the existence of two complementary mechanisms for neural binding, and we show  how such mechanisms may operate in the construction of intermediate-level visual  representations.  993  994 Sajda and Finkel  Ordered cortical topography has been found in both striate and extrastriate areas  and is believed to be a fundamental organizational principle of visual cortex. One  functional role for this topographic mapping may be to facilitate a spatial-based  binding system. For example, active neurons or neural populations within a cor-  tical area could be grouped together based on topographic proximity while those  in different areas could be grouped if they lie in rough topographic register. An  advantage of this scheme is that it can be carried out in parallel across the visual  field. However, a spatial-based mechanism will tend to bind overlapping or occluded  objects which should otherwise be segmented. An alternative binding mechanism  is therefore necessary for binding and segmenting overlapping objects and surfaces.  Temporal binding is a second type of neural binding. Temporal binding differs  from spatial binding in two essential ways; 1) it operates at high spatial resolutions  and 2) it binds and segments in the temporal domain, allowing for the coexistence  of multiple objects in the same topographic region. Others have proposed that  temporal events, such as phase-locked firing or '7 oscillations may play a role in  neural binding (yon der Malsburg, 1981; Gray and Singer, 1989, Crick and Koch,  1990). For purposes of this discussion, we do not consider the specific nature of the  temporal events underlying neural binding, only that the binding itself is temporally  dependent. The disadvantage of operating in the temporal domain is that the  biophysical properties of cortical neurons (e.g. membrane time constants) forces  this processing to be resource-limited-only a small number of objects or surfaces  can be bound and segmented simultaneously.  2  COMPUTING INTERMEDIATE-LEVEL VISUAL  REPRESENTATIONS: DIRECTION OF FIGURE  We consider how these two classes of binding can be used to compute context-  dependent (non-local) characteristics about the visual scene. An example of a  context-dependent scene characteristic is contour ownership or direction of figure.  Direction of figure is a useful intermediate-level visual representation since it can  be used to organize an image into a perceptual scene (e.g. infer relative depth and  link segregated features). Figure 1A illustrates the relationship between contours  and surfaces implied by direction of figure.  We describe a model which utilizes both spatial and temporal binding to compute  direction of figure (DOF). Prior to computing the DOF, the surface contours in  the image are extracted. These contours are then temporally bound by a process  we call "contour binding" (Finkel and Sajda, 1992). In the model, the temporal  properties of the units are represented by a temporal binding value. We will not  consider the details of this process except to say that units with similar temporal  binding values are bound together while those with different values are segmented.  In vivo, this temporal binding value may be represented by phase of neural firing,  oscillation frequency, or some other specific temporal property of neuronal activity.  The DOF is computed by circuitry which is organized in a columnar structure,  shown in figure 2A. There are two primary circuits which operate to compute the  direction of figure; one being a temporal-dependent/spatial-independent (TDSI)  circuit selective to "closure", the other a spatial-dependent/temporal-independent  Dual Mechanisms for Neural Binding and Segmentation 995  c  A  closure similarity &  proximity   !  concavities direction of  line endings  B  Figure 1: A Direction of figure as a surface representation. At point (1) the contour  belongs to the surface contour of region A and therefore A owns the contour. This  relationship is represented locally as a "direction of figure" vector pointing toward  region A. Additional ownership relationships are shown for points (2) and (3). B  Cues used in determining direction of figure.  (SDTI) circuit selective to "similarity and proximity". There are also two secondary  circuits which play a transient role in determining direction of figure. One is based  on the observation that concave segments bounded by discontinuities are a cue for  occlusion and ownership, while the other considers the direction of line endings  as a potential cue. Figure lB summarizes the cues used to determine direction  of figure. In this paper, we focus on the TDSI and SDTI circuits since they best  illustrate the nature of the dual linding mechanisms. The perceptual consequences  of attributing closure discrimination to temporal binding and similarity/proximity  to spatial binding is illustrated in figure 3.  2.1 TDSI CIRCUIT  Figure 2B(i) shows the neural architecture of the TDSI mechanism. The activity  of the TDSI circuit selective for a direction of figure a is computed by comparing  the amount of closure on either side of a contour. Closure is computed by summing  the temporal dependent inputs over all directions i;  TDSI = $(ti)-- . $i  The brackets ([ ]) indicate an implicit thresholding (if x < 0 then [x] = 0, otherwise  [x] = x) and s(ti) and s-8(t.  i  ) are the temporal dependent inputs, computed  as;  s'(ti)= i if and  ((ti - xt) < t: < (ti + xt)) (2)  0 otherwise  996 Sajda and Finkel  concavity   transformation .'_/  direction of 'fl ,""''.  line erichrigs -  to/from  contour binding  ' to/from  _ other  DOF columns  TDSI SDTI  c- 1 o- 180   (i) (ii)  A B  Figure 2: A Divisions, inputs, and outputs for a DOF column. B The two pri-  mary circuits operating to compute direction of figure. (i) Top view of temporal-  dependent/spatial-independent (TDSI) circuit architecture. Filled square repre-  sents position of a specific column in the network. Untilled squares represent other  DOF columns serving as input to this column. Bold curve corresponds to a surface  contour in the input. Shown is the pattern of long-range horizontal connections  converging on the right side of the column (side a). (ii) Top view of spatial-  dependent/temporal-independent (SDTI) circuit architecture. Shown is the pattern  of connections converging on the right side of the column (side a).  where a and a - 180  represent the regions on either side of the contour, sj is the  activation of a unit along the direction i (For simulations i varies between 0  and  315  by increments of 45), At determines the range of temporal binding values  over which the column will integrate input, and S2, is the activation threshold. The  temporal dependence of this circuit implies that only those DOF columns having  the same temporal binding value affect the closure computation.  2.2 SDTI CIRCUIT  Figure 2B(ii) illustrates the neural architecture of the SDTI mechanism. The SDTI  circuit organizes elements in the scene based on "proximity" and "similarity" of  orientation. Unlike the TDSI circuit which depends upon temporal binding, the  SDTI circuit uses spatial binding to access information across the network.  Activity is integrated from units with similar orientation tuning which lie in a  direction orthogonal to the contour (i.e. from parallel line segments). The activity  of the SDTI circuit selective for a direction of figure a is computed by comparing  input from similar orientations on either side of a contour;  l (. S(Oi)--ES-180(Oi) I (3)  SDTI = S.a--- i  where S.ax is a constant for normalizing the SDTI activity between 0 and 1 and  c- 180   8(0i) and s i (Oi) are spatial dependent inputs selective for an orientation O,  Dual Mechanisms for Neural Binding and Segmentation 997  A B  Figure 3: A The model predicts that a closed figure could not be discriminated in  parallel search since its detection depends on resource-limited temporal binding. B  Conversely, proximal parallel segments are predicted to be discriminated in parallel  search due to resource-free spatial binding.  computed as;  { (sj(x,y, Oj) > ST)  and  8(0i) -- ijSj(gg, y, Oj) if (Oi -- Oj)  0 otherwise  (4)  where a and a - 180  represent the regions on either side of the contour, 0 is the  orientation of the contour, i is the direction from which the unit receives input, Cij  is the connection strength (cij falls off as a gaussian with distance), and sj (x, y, Oj)  is the activation of a unit along the direction i which is mapped to retinotopic  location (x, y) and selective for an orientation Oj (For simulations i varies between  the following three angles; J_ 0i, J_ (Oi - 45), J_ (0i + 45)). Since the efficacy of the  connections, cij, decrease with distance, columns which are further apart are less  likely to be bound together. Neighboring parallel contours generate the greatest  activation and the circuit tends to discriminate the region between the two parallel  contours as the figure.  2.3 COMPUTED DOF  The activity of a direction of figure unit representing a direction c is given by the  sum of the four components;  DOF  = C(TDSI ) + Cu(SOTI ) + Ca(CON ) + C4(DLE ) (5)  where the constants define the contribution of each cue to the computed DOF. Note  that in this paper we have not considered the mechanisms for computing the DOF  given the two secondary cues (concavities (CON ') and direction of line endings  (DLE)). The DOF activation is computed for all directions c (For simulations c  varies between 0  and 315  by increments of 45 ) with the direction producing the  largest activation representing the direction of figure.  3 SIMULATION RESULTS  The following are simulations illustrating the role the dual binding mechanisms  play in perceptual organization. All simulations were carried out using the NEXUS  Neural Simulation Environment (Sajda and Finkel, 1992).  998 Sajda and Finkel  B C  Figure 4: A 128x128 pixel grayscale image. B Direction of figure computed by  the network. Direction of figure is shown as an oriented arrowhead, where the  orientation represents the preferred direction of the DOF unit which is most active.  C Depth of surfaces. Direction of figure relationships (such as those in the inset of  B) are used to infer relative depth. Plot shows % activity of units in the foreground  network-higher activity implies that the surface is closer to the viewer.  3.1 FIGURE/GROUND AND DEPTH-FROM-OCCLUSION  Figure 4A 18 a grayscale image used as input to the network. Figure 4B shows the  direction of figure computed by the model. Note that though the surface contours  are incomplete, the model is still able to characterize the direction of figure and  distinguish figure/ground over most of the contour. This is in contrast to mod-  els proposing diffusion-like mechanisms for determining figure/ground relationships  which tend to fail if complete contour closure is not realized.  The model utilizes direction of figure to determine occlusion relationships and strat-  ify objects in relative depth, results shown in figure 4C. This method of inferring  the relative depth of surfaces given occlusion is in contrast to traditional approaches  utilizing T-junctions. The obvious advantage of using direction of figure is that it  is a context-dependent feature directly linked to the representation of surfaces.  3.2 VIVIDNESS OF PERCEPTUAL COMPLETION  Our previous work (Finkel and Sajda, 1992) has shown that direction of figure is  important for completion phenomena, such as the construction of illusory contours  and surfaces. More interestingly, our model offers an explanation for differences in  perceived vividness between different inducing stimuli. For example, subjects tend  to rank the vividness of the illusory figures in figure 5 from left to right, with the  figure on the left being the most vivid and that on the right the least.  Our model accounts for this effect in terms of the magnitude of the direction of figure  along the illusory contour. Figure 6 shows the individual components contributing  to the direction of figure. For a typical inducer, such as the pacman in figure 6, the  TDSI and SDTI circuits tend to force the direction of figure of the L-shaped segment  to region 1 while the concavity/convexity transformation tries to force the direction  of figure of the segment to be toward region 2. This transformation transiently  overwhelms the TDSI and SDTI responses, so that the direction of figure of the  L-shaped segment is toward region 2. However, the TDSI and SDTI activation will  affect the magnitude of the direction of figure, as shown in figure 7. For example,  Dual Mechanisms for Neural Binding and Segmentation 999  l__J  A B C  Figure 5: Illusory contour vividness as a function of inducer shape. Three types of  inducers are arranged to generate an illusory square. A pacman inducer, B thick L  inducer and C thin L inducer. Subjects rank the vividness of the illusory squares  from left to right ((A) > (B) > (C)).  TDSI SDTI concavity/convexity  component component component  Figure 6: Processes contributing to the direction of figure of the L-shaped contour  segment. The TDSI and SDTI circuits assign the contour to region 1, while the  change of the concavity to a convexity assigns the segment to region 2.  i}  :& ...... .5  0.6  go4 i:.  oo  r  A B  Figure 7: A Activity of SDTI units for the upper left inducer of each stimulus,  where the area of each square is proportional to unit activity. The $DTI units try  to assign the L-shaped segment to the region of the pacman. Numerical values  indicates the magnitude of the SDTI effect. B Magnitude of direction of figure  along the L-shaped segment as a function of inducer shape. The direction of figure  in all cases is toward the region of the illusory square.  1000 Sajda and Finkel  the weaker the activation of the TDSI and SDTI circuits, the stronger the activation  of the DOF units assigning the L-shaped segment to region 2.  Referring back to the inducer types in figure 5, one can see that though the TDSI  component is the same for all three inducers (i.e. all three generate the same amount  of closure) the SDTI contribution differs, shown quantitatively in figure 7A. The  contribution of the SDTI circuit is greatest for the thin L inducers and least for the  pacmen inducers-the L-shaped segments for the pacman stimulus are more strongly  owned by the surface of the illusory square than those for the thin L inducer. This  is illustrated in figure 7B, a plot of the magnitude of the direction of figure for each  inducer configuration. This result can be interpreted as the model's ordering of  perceived vividness, which is consistent with that of human observers.  4 CONCLUSION  In this paper we have argued for the utility of binding neural activities in both the  spatial and temporal domains. We have shown that a scheme consisting of these  complementary mechanisms can be used to compute context-dependent scene char-  acteristics, such as direction of figure. Finally, we have illustrated with computer  simulations the role these dual binding mechanisms play in accounting for aspects  of figure/ground perception, depth-from-occlusion, and perceptual vividness of illu-  sory contours and surfaces. It is interesting to speculate on the relationship between  these complementary binding mechanisms and the traditional distinction between  preattentive and attentional perception.  Acknowledgements  This work is supported by grants from ONR (N00014-90-J-1864, N00014-93-1-0681),  The Whitaker Foundation, and The McDonnell-Pew Program in Cognitive Neuro-  science.  References  F. Crick and C. Koch. Towards a neurobiological theory of consciousness. Seminars  in Neuroscience, 2:263-275, 1990.  L.H. Finkel and P. Sajda. Object discrimination based on depth-from-occlusion.  Neural Computation, 4(6):901-921, 1992.  C. M. Gray and W. Singer. Neuronal oscillations in orientation columns of cat  visual cortex. Proceedings of the National Academy of Science USA, 86:1698-1702,  1989.  P. Sajda and L. Finkel. NEXUS: A simulation environment for large-scale neural  systems. Simulation, 59(6):358-364, 1992.  C. von der Malsburg. The correlation theory of brain function. Technical Report In-  ternal Rep. No. 81-2, Max-Plank-Institute for Biophysical Chemistry, Department  of Neurobiology, Gottingen, Germany, 1981.  
A  Connectionist Model of the Owls  Sound Localization System  Daniel J. Rosen*  Department of Psychology  Stanford University  Stanford, CA 94305  David E. Rumelhart  Department of Psychology  Stanford University  Stanford, CA 94305  Eric. I. Knudsen  Department of Neurobiology  Stanford University  Stanford, CA 94305  Abstract  We do not have a good understanding of how theoretical principles  of learning are realized in neural systems. To address this problem  we built a computational model of development in the owl's sound  localization system. The structure of the model is drawn from  known experimental data while the learning principles come from  recent work in the field of brain style computation. The model  accounts for numerous properties of the owl's sound localization  system, makes specific and testable predictions for future experi-  ments, and provides a theory of the developmental process.  I INTRODUCTION  The barn owl, Tyro Alba, has a remarkable ability to localize sounds in space. In  complete darkness it catches mice with nearly flawless precision. The owl depends  upon this skill for survival, for it is a nocturnal hunter who uses audition to guide  *Current address: Keck Center for Integrafive Neuroscience, UCSF, 513 Parnassus  Ave., San Francisco, CA 94143-0444.  606  A Connectionist Model of the Owl's Sound Localization System 607  its search for prey (Payne, 1970; Knudsen, Blasdel and Konishi, 1979). Central to  the owl's localization system are the precise auditory maps of space found in the  owl's optic tectum and in the external nucleus of the inferior colliculus (ICx).  The development of these sensory maps poses a difficult problem for the nervous  system, for their accuracy depends upon changing relationships between the animal  and its environment. The owl encodes information about the location of a sound  source by the phase and amplitude differences with which the sound reaches the  owl's two ears. Yet these differences change dramatically as the animal matures  and its head grows. The genome cannot "know" in advance precisely how the  animal's head will develop - many environmental factors affect this process - so it  cannot encode the precise development of the auditory system. Rather, the genome  must design the auditory system to adapt to its environment, letting it learn the  precise interpretation of auditory cues appropriate for its head and ears.  In order to understand the nature of this developmental process, we built a connec-  tionist model of the owl's sound localization system, using both theoretical principles  of learning and knowledge of owl neurophysiology and neuroanatomy.  2 THE ESSENTIAL SYSTEM TO BE MODELED  The owl calculates the horizontal component of a sound source location by mea-  suring the interaural time difference (ITD) of a sound as it reaches the two ears  (Knudsen and Konishi, 1979). It computes the vertical component of the signal by  determining the interaural level difference (ILD) of that same sound (Knudsen and  Konishi, 1979). The animal processes these signals through numerous sub-cortical  nuclei to form ordered auditory maps of space in both the ICx and the optic tectum.  Figure i shows a diagram of this neural circuit.  Neurons in both the ICx and the optic tectum are spatially tuned to auditory  stimuli. Cells in these nuclei respond to sound signals originating from a restricted  region of space in relation to the owl (Knudsen, 1984). Neurons in the ICx respond  exclusively to auditory signals. Cells in the optic tectum, on the other hand, encode  both audito?y and visual sensory maps, and drive the motor system to orient to the  location of an auditory or visual signal.  Researchers study the owl's development by systematically altering the animal's  sensory experience, usually in one of two ways. They may fit the animal with a  sound attenuating earplug, altering its auditory experience, or they may fit the owl  with displacing prisms, altering its visual experience.  Disturbance of either auditory or visual cues, during a period when the owl is de-  veloping to maturity, causes neural and behavioral changes that bring the auditory  map of space back into alignment with the visual nmp, and/or tune the auditory sys-  tem to be sensitive to the appropriate range of binaural sound signals. The earplug  induced changes take place at the level of the VLVp, where ILD is first computed  (Mogdans and Knudsen, 1992). The visually induced adjustment of the auditory  maps of space seems to take place at the level of the ICx (Brainard and Knudsen,  1993b). The ability of the owl to adjust to altered sensory signals diminishes over  time, and is greatly restricted in adulthood (Knudsen and Knudsen, 1990).  608 Rosen, Rumelhart, and Knudsen  OVERVIEW of the BARN OWL's  SOUND LOCALIZATION SYSTEM  I OPTIC TECTUM 1  MullI-Medal  SPACE MAP  SHELL ol IC ITD ITD SHELLoi IC  ITD & I..D ITD & I..D  Figure 1: A chart describing the flow of auditory information in the owl's sound  localization system. For simplicity, only the connections leading to the one of the  bilateral optic tecta are shown. Nuclei labeled with an asterisk (*) are included in  the model. Nuclei that process ILD and/or ITD information are so labeled.  3 THE NETWORK MODEL  The model has two major components: a network architecture based on the neuro-  biology of the owl's localization system, as shown in Figure 1, and a learning rule  derived from computational learning theory. The elements of the model are stan-  dard connectionist units whose output activations are sigmoidal functions of their  weighted inputs. The learning rule we use to train the model is not standard. In  the following section we describe how and why we derived this rule.  3.1 DEFINING THE GOAL OF THE NETWORK  The goal of the network, and presumably the owl, is to accurately map sound signals  to sound source locations. The network must discover a model of the world which  best captures the relationship between sound signals and sound source locations.  Recent work in connectionist learning theory has shown us ways to design networks  that search for the model that best fits the data at hand (Buntine and Weigend,  1991; MacKay, 1992; Rumelhart, Durbin, Golden and Chauvin, in press). In this  section we apply such an analysis to the localization network.  A Connectionist Model of the Owl's Sound Localization System 609  Table 1: A table showing the mathematical terms used in the analysis.  TERM MEANING  .h4 The Model  ) The Data  P(.AdlD) Probability of the Model given the Data  < , ff >i The set of i input/target training pairs   Th e input vector for training trial i   The target vector for training trial i  Yi The output vector for training trial i  ij The value of output unit j on training trial i  wi d The weight from unit j to unit i  /j The netinput to unit j  '(/j) The activation function of unit j evaluated at its netinput   The term to be maximized by the network  3.2 DERIVING THE FUNCTION TO BE MAXIMIZED  The network should maximize the probability of the model given the data. Using  Bayes' rule we write this probability as:  p(Mil ) =  P(V)  Ilere M represents the model (the units, weights and associated biases) and D  represents the data. We define the data as a set of ordered pairs, [< sound -  signal, location-signal >i], which represent the cues and targets normally used to  train a conncctionist network. In the owl's case the cues are the auditory signals,  and the target information is provided by the visual system. (Table I lists the  mathematical terms we use in this section.)  We simplify this equation by taking the natural logarithm of each side giving:  In P(MtV) - In P(VIM) + In P(M) - In  Since the natural logarithm is a monotonic transformation, if the network maximizes  the second equation it will also maximize the first.  The final term in the equation, In P(D), represents the probability of the ordered  pairs the network observes. Regardless of which model the network settles upon,  this term remains the same - the data are a constant during training. Therefore we  can ignore it when choosing a model.  The second term in the equation, In P(.A4), represents the probability of the model.  This is the prior term in Bayesian analysis and is our estimation of how likely it  is that a particular model is true, regardless of the data. We will discuss it below.  For now we will concentrate on maximizing In P(Dt./4 ).  610 Rosen, Rumelhart, and Knudsen  3.3 ASSUMPTIONS ABOUT THE NETWORK'S ENVIRONMENT  We assume that the training data - pairs of stylized auditory and visual signals -  are independent of one another and re-write the previous term as:  lnP(Z)tA4 ) = y]lnP(< 7, ff>i  i  The i subscript denotes the particular data, or training, pair. We further expand  this term to:  In P(DI.A4 ) =  In P(.li A .,M) + E In P(:r'i).  i i  We ignore the last term, since the sound signals are not dependent on the model.  Ve are left, then, with the task of maximizing ]iln P(li A .A4). It is important  to note that ffi represents a visual signal, not a localization decision. The network  attempts to predict its visual experience given its auditory experience. It does not  predict the probability of making an accurate localization decision. If we assume  that visual signals provide the target values for the network, then this analysis shows  that the auditory map will always follow the visual map, regardless of whether this  leads to accurate localization behavior or not. Our assumption is supported by  experiments showing that, in the owl, vision does guide the formation of auditory  spatial maps (Knudsen and Knudsen, 1985; Knudsen, 1988).  Next, we must clarify the relationship between the inputs, 'i and the targets, .  We know that the real world is probabilistic - that for a given input there exists  some distribution of possible target values. We need to estimate the shape of this  distribution. In this case we assume that the target values are binomially distributed  - that given a particular sound signal, the visual system did or did not detect a  sound source at each point in owl-centered space.  Having made this assumption, we can clarify our interpretation of the network  output array, . Each element, .Oij, of this vector represents the activity of output  unit j on training trial i. We assume that the output activation of each of these  units represents the expected value of its corresponding target, Yij. In this case  the expected value is the mean of a binomial distribution. So the value of each  output unit ij represents the probability that a sound signal originated from that  particular location. Ve now write the probability of the data given the model as:  P(ta AM) = H ',:' (1- .Oi.i) -u'5  Taking the natural log of the probability and summing over all data pairs we get:  C =  Eyij ln.0ij + (1 - yij)ln(1- Oij)  i j  where C is the term we want to maximize. This is the standard cross-entropy term.  3.4 DERIVING THE LEARNING RULE  Having defined our goal we derive a learning rule appropriate to achieving that goal.  oc where is the net input to a unit. (In these  To determine this rule we compute  r/  A Connectionist Model of the Owl's Sound Localization System 611  equations we have dropped the i subscript, which denotes the particular training  trial, since this analysis is identical for all trials.) We write this as:  oc  (y./- `0./)  `0i(1 - `0j )  where O.T'(?./) is the derivative of a unit's activation function evaluated at its net  input.  Next we choose an appropriate activation function for the output units. The logistic   is a good choice for two reasons. First, it is bounded by  function,-'O?j) = (+e-"J)'  zero and one. This makes sense since we assume that the probability that a sound  signal originated at any one point in space is bounded by zero and one. Second,  when we compute the derivative of the logistic function we get the following result:  O.T(r/./) = '(r/./)(1 - .,/'(L)) = ,0./(1 - `0.j ).  This term is the variance of a binomial distribution and when we return to the  derivative of our cost function, we see that this variance term is canceled by the  denominator. The final derivative we use to compute the weight changes at the  output units is therefore:  o  07./  The weights to other units in the network are updated according to the standard  backpropagation learning algorithm.  3.5 SPECIFYING MODEL PRIORS  There are two types of priors in this model. First is the architectural one. We design  a fixed network architecture, described in the previous section, based upon our  knowledge of the nuclei involved in the owl's localization system. This is equivalent  to setting the prior probability of this architecture to 1, and all others to 0.  We also use a weight elimination prior. This and similar priors may be interpreted  as ways to reduce the complexity of a network (Weigend, Huberman and Rumelhart,  1990). The network, therefore, maximizes an expression which is a function of both  its error and complexity.  3.6 TRAINING  We train the model by presenting it with input to the core of the inferior colliculus  (ICc), which encodes interaural phase and time differences (IPD/ITD), and the  angular nuclei, which encode sound level. The outputs of the network are then  compared to target values, presumed to come from the visual system. The weights  are adjusted in order to minimize this difference. We mimic plug training by varying  the average difference between the two angular input values. We mimic prism  training by systematically changing the target values associated with an input.  612 Rosen, Rumelhart, and Knudsen  Figure 2: The activity level of ICx units in response to a particular auditory in-  put immediately after simulated prism training was begun (left), midway through  training (middle) and after training was completed (right).  4 RESULTS and DISCUSSION  The trained network localizes accurately, shows appropriate auditory tuning curves  in each of the modeled nuclei, and responds appropriately to manipulations that  mimic experiments such as blocking inhibition at the level of the ICx. The network  also shows appropriate responses to changing average binaural intensity at the level  of the VLVp, the lateral shell and the ICx.  Furthermore, the network exhibits many properties found in the developing owl..  The model appropriately adjusts its auditory localization behavior in simulated  earplug experiments and this plasticity takes place at the level of the VLVp. As  earplug simulations are begun progressively later in training, the network's ability  to adapt to plug training gradually diminishes, following a time course of plasticity  qualitatively similar to the sensitive and critical periods described in the owl.  The network adapts appropriately in simulated prism studies and the changes in  response to these simulations primarily take place along the lateral shell to ICx  connections. As with the plug studies, the network's ability to adapt to prisms  diminishes over time. However, unlike the mature owl, a highly trained network  retains the ability to adapt in a simulated prism experiment.  We also discovered that the principally derived learning rule better models interme-  diate stages of prism adjustment than does a standard back-propagation network.  Brainard and Knudsen (1993a) report observing two peaks of activity across the rec-  tum in response to an auditory stimulus during prism training- one corresponding  to the pre-training response and one corresponding to the newly learned response.  Over time the pre-trained response diminishes while the newly learned one grows.  As shown in Figure 2, the network exhibits this same pattern of learning. Networks  we trained under a standard back-propagation learning algorithm do not. Such a  A Connectionist Model of the Owl's Sound Localization System 613  result lends support to the idea that the owl's localization system is computing a  function similar to the one the network was designed to learn.  In addition to accounting for known data, the network predicts results of experi-  ments it was not designed to mimic. Specifically, the network accurately predicted  that removal of the animal's facial ruff, which causes ILD to vary with azimuth  instead of elevation, would have no effect on the animal's response to varying ILD.  The network accomplishes the goals for which it was designed. It accounts for much,  though not all, of the developmental data, it makes testable predictions for future  experiments, and since we derived the learning rule in a principled fashion, the  network provides us with a specific theory of the owl's sound localization system.  References  Brainard, M. S., &: Knudsen, E. I. (1993a). Dynamics of the visual calibration of  the map of interaural time difference in the barn owl's optic tectum. Society  for Neuroscience Abstracts, 19, 369.8.  Brainard, M. S., &: Knudsen, E. I. (1993b). Experience-dependent plasticity in the  inferior colliculus: a site for visual calibration of the neural representation of  auditory space in the barn owl. The Journal of Neuroscience, 13, 4589-4608.  Buntine, W. L., &: Weigend, A. S. (1991). Bayesian back-propagation. Complezc  Systems, 5, 603-612.  Knudsen, E. (1984). Auditory properties of space-tuned units in owl's optic tectum.  Journal of Neurophysiology, 5(4), 709-723.  Knudsen, E. (1988). Early blindness results in a degraded auditory map of space  in the optic tectum of the barn owl. Proceedings of the National Academy of  Science, U.S.A., 85, 6211-6214.  Knudsen, E., Blasdel, G., &: Konishi, M. (1979). Sound localization by the barn  owl (tyto alba) measured with the search coil technique. The Journal of Com-  parative Physiology A, 133, 1-11.  Knudsen, E., & Knudsen, P. (1985). Vision guides the adjustment of auditory  localization in young barn owls. Science, 230, 545-548.  Knudsen, E., & Knudsen, P. (1990). Sensitive and critical periods for visual calibra-  tion of sound localization by barn owls. The Journal of Neuroscience, 10(1),  222-232.  MacKay, D. J. (1992). Bayesian Methods for Adaptive Models. Unpublished doctoral  dissertation, California Institute of Technology, Pasadena, California.  Mogdans, J., &: Knudsen, E. I. (1992). Adaptive adjustment of unit tuning to  sound localization cues in response to monaural occlusion in developing owl  optic tectum. The Journal of Neuroscience, 12, 3473-3484.  Payne, R. S. (1970). Acoustic location of prey by barn owls (tyto alba). The Journal  of Ezcperimental Biology, 53, 535-573.  Rumelhart, D. E., Durbin, R., Golden, R., &: Chauvin, Y. (in press). Backpropaga-  tion: The theory. In Y. Chauvin &: D. E. Rumelhart (Eds.), Backpropagation:  Theory, Architectures and Applications. Hillsdale, N.J.: Lawrence Earlbaum  Associates.  Weigend, A. S., Huberman, B. A., &: Rumelhart, D. E. (1990). Predicting the  future: A connectionist approach. International Journal of Neural Systems, 1,  193-209.  
Fast Non-Linear Dimension Reduction  Nanda Kambhatla and Todd K. Leen  Department of Computer Science and Engineering  Oregon Graduate Institute of Science & Technology  P.O. Box 91000 Portland, OR 97291-1000  Abstract  We present a fast algorithm for non-linear dimension reduction.  The algorithm builds a local linear model of the data by merging  PCA with clustering based on a new distortion measure. Exper-  iments with speech and image data indicate that the local linear  algorithm produces encodings with lower distortion than those built  by five layer auto-associative networks. The local linear algorithm  is also more than an order of magnitude faster to train.  I Introduction  Feature sets can be more compact than the data they represent. Dimension reduc-  tion provides compact representations for storage, transmission, and classification.  Dimension reduction algorithms operate by identifying and eliminating statistical  redundancies in the data.  The optimal linear technique for dimension reduction is principal component anal-  ysis (PCA). PCA performs dimension reduction by projecting the original n-  dimensional data onto the m < n dimensional linear subspace spanned by the  leading eigenvectors of the data's covariance matrix. Thus PCA builds a global  linear model of the data (an m dimensional hyperplane). Since PCA is sensitive  only to correlations, it fails to detect higher-order statistical redundancies. One  expects non-linear techniques to provide better performance; i.e. more compact  representations with lower distortion.  This paper introduces a local linear technique for non-linear dimension reduction.  We demonstrate its superiority to a recently proposed global non-linear technique,  152  Fast Non-Linear Dimension Reduction 153  and show that both non-linear algorithms provide better performance than PCA  for speech and image data.  2 Global Non-Linear Dimension Reduction  Several researchers (e.g. Cottrell and Metcalfe 1991) have used layered feedforward  auto-associative networks with a bottle-neck middle layer to perform dimension  reduction. It is well known that auto-associative nets with a single hidden layer  cannot provide lower distortion than PCA (Bourlard and Kamp, 1988). Recent  work (e.g. Oja 1991) shows that five layer auto-associative networks can improve  on PCA. These networks have three hidden layers (see Figure l(a)). The first and  third hidden layers have non-linear response, and are referred to as the mapping  layers. The rn < n nodes of the middle or representation layer provide the encoded  signal.  The first two layers of weights produce a projection from 7 n to 7 m. The last two  layers of weights produce an immersion from 7 ' into 7 n. If these two maps are  well chosen, then the complete mapping from input to output will approximate the  identity for the training data. If the data requires the projection and immersion  to be non-linear to achieve a good fit, then the network can in principal find such  functions.  rn =___> Low Dimensional  Encoding  Original High  Dimensional  Representation  X  1  -1 0  -1  (b)  Figure 1: (a) A five layer feedforward auto-associative network. This network can  perform a non-linear dimension reduction from n to rn dimensions. (b) Global  curvilinear coordinates built by a five layer network for data distributed on the  surface of a hemisphere. When the activations of the representation layer are swept,  the outputs trace out the curvilinear coordinates shown by the solid lines.  The activities of the nodes in the representation layer form global curvilinear co-  ordinates on a submanifold of the input space (see Figure l(b)). We thus refer  to five layer auto-associative networks as a global, nonlinear dimension reduction  technique.  154 Kambhatla and Leen  3 Locally Linear Dimension Reduction  Five layer networks have drawbacks; they can be very slow to train and they are  prone to becoming trapped in poor local optima. Furthermore, it may not be  possible to accurately fit global, low dimensional, curvilinear coordinates to the  data. We propose an alternative that does not suffer from these problems.  Our algorithm pieces together local linear coordinate patches. The local regions are  defined by the partition of the input space induced by a vector quantizer (VQ). The  orientation of the local coordinates is determined by PCA (see Figure 2). In this  section, we present two ways to obtain the partition. First we describe an approach  that uses Euclidean distance, then we describe a new distortion measure which is  optimal for our task (local PCA).  Figure 2: Local coordinates built by our algorithm (dubbed VQPCA) for data dis-  tributed on the surface of a hemisphere. The solid lines represent the two principal  eigen-directions in each Voronoi cell. The region covered by one Voronoi cell is  shown shaded.  3.1 Euclidean partitioning  Here, we do a clustering (with Euclidean distance) followed by PCA in each of the  local regions. The hybrid algorithm, dubbed VQPCA, proceeds in three steps:  1. Using competitive learning, train a VQ (with Euclidean distance) with Q  reference vectors (weights) (r, r2,..., rQ).  2. Perform a local PCA within each Voronoi cell of the VQ. For each cell,  compute the local covariance matrix for the data with respect to the cor-  responding reference vector (centroid) re. Next compute the eigenvectors  (e,..., e) of each covariance matrix.  3. Choose a target dimension m and project each data vector x onto  the leading m eigenvectors to obtain the local linear coordinates  z = (x - (x - ).  Fast Non-Linear Dimension Reduction 155  The encoding of x consists of the index c of the reference cell closest (Euclidean  distance) to x, together with the m < n component vector z. The decoding is given  by  m  = rc + ,  i=1  where rc is the reference vector (centroid) for the cell c, and e? are the leading  eigenvectors of the covariance matrix of the cell c. The mean squared reconstruction  error incurred by VQPCA is  ,co, = E[ Ilx - kll 2 ] = E[ [Ix - rc - E ziel[2] (2)  i:1  where El.] denotes an expectation with respect to x, and i is defined in (1).  Training the VQ and performing the local PCA are very fast relative to training a  five layer network. The training time is dominated by the distance computations  for the competitive learning. This computation can be speeded up significantly by  using a multi-stage architecture for the VQ (Gray 1984).  3.2 Projection partitioning  The VQPCA algorithm as described above is not optimal because the clustering is  done independently of the PCA projection. The goal is to minimize the expected  error in reconstruction (2). We can realize this by using the expected reconstruction  error as the distortion measure for the design of the VQ.  The reconstruction error for VQPCA (econ defined in (2)) can be written in matrix  form as  co = E[ (x - rc)T Pc Pc(x - rc)], (3)  where P is an rax n matrix whose rows are the orthonormal trailing eigenvectors  of the covariance matrix for the cell c. This is the mean squared Euclidean distance  between the data and the local hyperplane.  The expression for the VQPCA error in (2) suggests the distortion measure  d(x, rc) = (x- rc)TPcPc(x-rc) . (4)  We call this the reconstruction distance. The reconstruction distance is the error  incurred in approximating x using only ra local PCA coefficients. It is the squared  projection of the difference vector X-rc on the trailing eigenvectors of the covariance  matrix for the cell c. Clustering with respect to the reconstruction distance directly  minimizes the expected reconstruction error co.  The modified VQPCA algorithm is:  1. Partition the input space using a VQ with the reconstruction distance mea-  sure  in (4).  2. Perform a local PCA (same as in steps 2 and 3 of the algorithm as described  in section 3.1).  1The VQ is trained using the (batch mode) generalized Lloyd's algorithm (Gersho and  Gray, 1992) rather than an on-line competitive learning. This avoids recomputing the  matrix Pc (which depends on re) for each input vector.  156 Kambhatla and Leen  4 Experimental Results  We apply PCA, five layer networks (SLNs), and VQPCA to dimension reduction  of speech and images. We compare the algorithms using two performance criteria:  training time and the distortion in the reconstructed signal. The distortion measure  is the normalized reconstruction error:  4.1 Model Construction  The 5LNs were trained using three optimization techniques: conjugate gradient  descent (CGD), the BFGS algorithm (a quasi-Newton method (Press et al 1987)),  and stochastic gradient descent (SGD). In order to limit the space of architectures,  the 5LNs have the same number of nodes in both of the mapping (second and  fourth) layers.  For the VQPCA with Euclidean distance, clustering was implemented using stan-  dard VQ (VQPCA-Eucl) and multistage quantization (VQPCA-MS-E). The multi-  stage architecture reduces the number of distance calculations and hence the train-  ing time for VQPCA (Gray 1984).  4.2 Dimension Reduction of Speech  We used examples of the twelve monothongal vowels extracted from continuous  speech drawn from the TIMIT database (Fisher and Doddington 1986). Each input  vector consists of 32 DFT coefficients (spanning the frequency range 0-4kHz), time-  averaged over the central third of the utterance. We divided the data set into a  training set containing 1200 vectors, a validation set containing 408 vectors and  a test set containing 408 vectors. The validation set was used for architecture  selection (e.g the number of nodes in the mapping layers for the five layer nets).  The test set utterances are from speakers not represented in the training set or the  validation set. Motivated by the desire to capture formant structure in the vowel  encodings, we reduced the data from 32 to 2 dimensions. (Experiments on reduction  to 3 dimensions gave similar results to those reported here (Kambhatla and Leen  1993).)  Table I gives the test set reconstruction errors and the training times. The VQPCA  encodings have significantly lower reconstruction error than the global PCA or five  layer nets. The best 5LNs have slightly lower reconstruction error than PCA, but  are very slow to train. Using the multistage search, VQPCA trains more than  two orders of magnitude faster than the best 5LN, and achieves an error about 0.7  times as great. The modified VQPCA algorithm (with the reconstruction distance  measure used for clustering) provides the least reconstruction error among all the  architectures tried.  Fast Non-Linear Dimension Reduction 157  Table 1: Speech data test set reconstruction errors and training times. Architec-  tures represented here are from experiments with the lowest validation set error  over the parameter ranges explored. The numbers in the parentheses are the values  of the free parameters for the algorithm represented (e.g 5LN-CGD (5) indicates a  network with 5 nodes in both the mapping (2nd and 4th) layers, while VQPCA-Eucl  (50) indicates a clustering into 50 Voronoi cells).  ALGORITHM  norm  TRAINING TIME  (in seconds)  PCA 0.0060 11  5LN-CGD (5) 0.0069 956  5LN-BFGS (30) 0.0057 28,391  5LN-SGD (25) 0.0055 94,903  VQPCA-Eucl (50)  VQPCA-MS-E (9x9)  VQPCA-Recon (45)  0.0037 1,454  0.0036 142  0.0031 931  Table 2: Reconstruction errors and training times for a 50 to 5 dimension reduction  of images. Architectures represented here are from experiments with the lowest  validation set error over the parameter ranges explored.  ALGORITHM norm TRAINING TIME  (in seconds)  PCA O.458 5  5LN-CGD (40) 0.298 3,141  5LN-BFGS (20) 0.052 10,389  5LN-SGD (25) 0.350 15,486  VQPCA-Eucl (20)  VQPCA-MS-E (8x8)  VQPCA-Recon (25)  0.140 163  0.176 118  0.099 108  4.3 Dimension Reduction of Images  The data consists of 160 images of the faces of 20 people. Each is a 64x64, 8-bit/pixel  grayscale image. We extracted the first 50 principal components of each image and  use these as our experimental data. This is the same data and preparation that  DeMers and Cottrell used in their study of dimension reduction with five layer  auto-associative nets (DeMers and Cottrell 1993). They trained auto-associators to  reduce the 50 principal components to 5 dimensions.  We divided the data into a training set containing 120 images, a validation set (for  architecture selection) containing 20 images and a test set containing 20 images.  We reduced the images to 5 dimensions using PCA, 5LNs 2 and VQPCA. Table 2  2We used 5LNs with a configuration of 50-n-5-n-50, n varying from 10 to 40 in incre-  ments of 5. The BFGS algorithm posed prohibitive memory and time requirements for  n > 20 for this task.  158 Kambhatla and Leen  Table 3: Reconstruction errors and training times for a 50 to 5 dimension reduction  of images (training with all the data). Architectures represented here are from  experiments with the lowest error over the parameter ranges explored.  ALGORITHM  norrn  TRAINING TIME  (in seconds)  PCA 0.4054 7  5LN-SGD (30) 0.1034 25,306  5LN-SGD (40) 0.0729 31,980  VQPCA-Eucl (50) 0.0009 905  VQPCA-Recon (50) 0.0017 216  summarizes the results. We notice that a five layer net obtains the encoding with  the least error for this data, but it takes a long time to train. Presumably more  training data would improve the best VQPCA results.  --'?' . '...':5: .:  . .-.'"".':i:: . -i: .;':4: :.  v" E:::o'-.:.-' :'-.-:.  ' :-' "::f''-Z-.'.::--Z':'.  [. }-: -.'%::,..:.':'Z '  : ":;::.?..:.  ....... }  Figure 3: Two representative images: Left to right - Original 50-PC image, recon-  struction from 5-D encodings: PCA, 5LN-SGD(40), VQPCA(10), and VQPCA(50).  For comparison with DeMers and Cottrell's (DeMers and Cottrell 1993) work, we  also conducted experiments training with all the data. The results are summarized a  in Table 3 and Figure 3 shows two sample faces. Both non-linear techniques produce  encodings with lower error than PCA, indicating significant non-linear structure in  the data. With the same data, and with a 5LN with 30 nodes in each mapping layer,  DeMers (DeMers and Cottrell 1993) obtains a reconstruction error norm 0.13174.  We note that the VQPCA algorithms achieve an order of magnitude improvement  over five layer nets both in terms of speed of training and the accuracy of encodings.  aFor 5LNs, we only show results with SGD in order to compare with the experimental  results of DeMers. For this data, 5LN-CGD gave encodings with a higher error and 5LN-  BFGS posed prohibitive memory and computational requirements.  4DeMers reports half the MSE per output node, E ---- (1/2) * (1/50)  MSE -- 0.001.  This corresponds to ,or,,, = 0.1317  Fast Non-Linear Dimension Reduction 159  5 Summary  We have presented a local linear algorithm for dimension reduction. We propose  a new distance measure which is optimal for the task of local PCA. Our results  with speech and image data indicate that the nonlinear techniques provide more  accurate encodings than PCA. Our local linear algorithm produces more accurate  encodings (except for one simulation with image data), and trains much faster than  five layer auto-associative networks.  Acknowledgments  This work was supported by grants from the Air Force Office of Scientific Research  (F49620-93-1-0253) and Electric Power Research Institute (RP8015-2). The authors  are grateful to Gary Cottrell and David DeMers for providing their image database  and clarifying their experimental results. We also thank our colleagues in the Center  for Spoken Language Understanding at OGI for providing speech data.  References  H. Bourlard and Y. Kamp. (1988) Auto-association by multilayer perceptrons and  singular value decomposition. Biological Cybernetics, 59:291-294.  G. Cottrell and J. Metcalfe. (1991) EMPATH: Face, emotion, and gender recog-  nition using holons. In R. Lippmann, John Moody and D. Touretzky, editors,  Advances in Neural Information Processing Systems 3, pages 564-571. Morgan  Kauffmann.  D. DeMers and G. Cottrell. (1993) Non-linear dimensionality reduction. In Giles,  Hanson, and Cowan, editors, Advances in Neural Information Processing Systems  5. San Mateo, CA: Morgan Kaufmann.  W. M. Fisher and G. R. Doddington. (1986) The DARPA speech recognition re-  search database: specification and status. In Proceedings of the DARPA Speech  Recognition Workshop, pages 93-99, Palo Alto, CA.  A. Gersho and R. M. Gray. (1992) Vector Quantization and Signal Compression.  Kluwer academic publishers.  R. M. Gray. (1984) Vector quantization. IEEE ASSP Magazine, pages 4-29.  N. Kambhatla and T. K. Leen. (1993) Fast non-linear dimension reduction. In IEEE  International Conference on Neural Networks, Vol. 3, pages 1213-1218. IEEE.  E. Oja. (1991) Data compression, feature extraction, and autoassociation in feed-  forward neural networks. In Artificial Neural Networks, pages 737-745. Elsevier  Science Publishers B.V. (North-Holland).  W. H. Press, B. P. Flannery, S. A. Teukolsky, and W. T. Vetterling. (1987) Nu-  merical Recipes - the Art of Scientific Computing. Cambridge University Press,  Cambridge/New York.  
Foraging in an Uncertain Environment Using  Predictive Hebbian Learning  P. Read Montague ; Peter Dayan, and Terrence J. Sejnowski  Computational Neurobiology Lab, The Salk Institute,  10010 N. Torrey Pines Rd,  La Jolla, CA, 92037, USA  readbohr. bcm. tmc. edu  Abstract  Survival is enhanced by an ability to predict the availability of food,  the likelihood of predators, and the presence of mates. We present a  concrete model that uses diffuse neurotransmitter systems to implement  a predictive version of a Hebb learning rule embedded in a neural ar-  chitecture based on anatomical and physiological studies on bees. The  model captured the strategies seen in the behavior of bees and a number of  other animals when foraging in an uncertain environment. The predictive  model suggests a unified way in which neuromodulatory influences can  be used to bias actions and control synaptic plasticity.  Successful predictions enhance adaptive behavior by allowing organisms to prepare for fu-  ture actions, rewards, or punishments. Moreover, it is possible to improve upon behavioral  choices if the consequences of executing different actions can be reliably predicted. Al-  though classical and instrumental conditioning results from the psychological literature [ 1]  demonstrate that the vertebrate brain is capable of reliable prediction, how these predictions  are computed in brains is not yet known.  The brains of vertebrates and invertebrates possess small nuclei which project axons  throughout large expanses of target tissue and deliver various neurotransmitters such as  dopamine, norepinephrine, and acetylcholine [4]. The activity in these systems may report  on reinforcing stimuli in the world or may reflect an expectation of future reward [5, 6, 7, 8].  *Division of Neumscience, Baylor College of Medicine, Houston, TX 77030  598  Foraging in an Uncertain Environment Using Predictive Hebbian Learning 599  A particularly striking example is that of the honeybee. Honeybees can be conditioned to  a sensory stimulus such as a color, visual pattern, or an odorant when the sensory stimulus  is paired with application of sucrose to the antennae or proboscis. An identified neuron,  VUMmxl, projects widely throughout the entire bee brain, becomes active in response  to sucrose, and its firing can substitute for the unconditioned odor stimulus in classical  conditioning experiments [8]. Similar diffusely projecting neurons in the bee brain may  substitute for reward when paired with a visual stimulus.  In this paper, we suggest a role for diffuse neurotransmitter systems in learning and behavior  that is analogous to the function we previously postulated for them in developmental self-  organization[3, 2]. Specifically, we: (i) identify a neural substrate/architecture which is  known to exist in both vertebrates and invertebrates and which delivers information to  widespread regions of the brain; (ii) describe an algorithm that is both mathematically  sound and biologically feasible; and (iii) show that a version of this local algorithm, in the  context of the neural architecture, reproduces the foraging and decision behavior observed  in bumble bees and a number of other animals.  Our premise is that the predictive relationships between sensory stimuli and rewards are  constructed through these diffuse systems and are used to shape both ongoing behavior and  reward-dependent synaptic plasticity. We illustrate this using a simple example from the  ethological literature for which constraints are available at a number of different levels.  A Foraging Problem  Real and colleagues [9, 10] performed a series of experiments on bumble bees foraging on  artificial flowers whose colors, blue and yellow, predicted of the delivery of nectar. They  examined how bees respond to the mean and variability of this reward delivery in a foraging  version of a stochastic two-armed bandit problem [ 11 ]. All the blue flowers contained  of nectar,  of the yellow flowers contained 6gt, and the remaining - of the yellow flowers  contained no nectar at all. In practice, 85% of the bees' visits were to the constant yield  blue flowers despite the equivalent mean return from the more variable yellow flowers.  When the contingencies for reward were reversed, the bees switched their preference for  flower color within 1 to 3 visits to flowers. They further demonstrated that the bees could be  induced to visit the variable and constant flowers with equal frequency if the mean reward  from the variable flower type was made sufficiently high.  This experimental finding shows that bumble bees, like honeybees, can learn to associate  color with reward. Further, color and odor learning in honeybees has approximately the  same time course as the shift in preference described above for the bumble bees [ 12]. It also  indicates that under the conditions of a foraging task, bees prefer less variable rewards and  compute the reward availability in the short term. This is a behavioral strategy utilized by  a variety of animals under similar conditions for reward [9, 10, 13] suggesting a common  set of constraints in the underlying neural substrate.  The Model  Fig. 1 shows a diagram of the model architecture, which is based on the considerations  above about diffuse systems. Sensory input drives the units 'B' and 'Y' representing blue  and yellow flowers. These neurons (outputs x and xt y respectively at time t) project  600 Montague, Dayan, and Sejnowski  N ;ctar Sensory input  Action selection  inhibition  Motor  systems  Figure 1: Neural architecture showing how predictions about future expected rein-  forcement can be made in the brain using a diffuse neurotransmitter system [3, 2]. In  the context of bee foraging [9], sensory input drives the units B and Y representing blue and  yellow flowers. These units project to a reinforcement neuron P through a set of variable  weights (filled circles w B and w ) and to an action selection system. Unit $ provides input  to P, and fires while the bee sips the nectar. R projects its output rt through a fixed weight  to P. The variable weights onto P implement predictions about future reward rt (see text)  and P's output is sensitive to temporal changes in its input. The output projections of P, 6t  (lines with arrows), influence learning and also the selection of actions such as steering in  flight and landing, as in equation 5 (see text). Modulated lateral inhibition (dark circle) in  the action selection layer symbolizes this. Before encountering a flower and its nectar, the  output of P will reflect the temporal difference only between the sensory inputs B and Y.  During an encounter with a flower and nectar, the prediction error 6t is determined by the  output of B or Y and R, and learning occurs at connections w e and w . These strengths  are modified according to the correlation between presynaptic activity and the prediction  error 6t produced by neuron P as in equation 3 (see text). Learning is restricted to visits to  flowers [14].  through excitatory connection weights both to a diffusely projecting neuron P (weights  w e and w ) and to other processing stages which control the selection of actions such as  steering in flight and landing. P receives additional input rt through unchangeable weights.  In the absence of nectar (rt 0), the net input to P becomes B  '   - Vt  wt'gt -- wtx t --wtx t   The first assumption in the construction of this model is that learning (adjustment of  weights) is contingent upon approaching and landing on a flower. This assumption is  supported specifically by data from learning in the honeybee: color learning for flowers is  restricted to the final few seconds prior to landing on the flower and experiencing the nectar  [14].  This fact suggests a simple model in which the strengths of variable connections wt are  adjusted according to a presynaptic correlational rule:  (1)  where oc is the learning rate [ 15]. There are two problems with this formulation: (i) learning  would only occur about contingencies in the presence of a reinforcing stimulus (r t 5h 0);  Foraging in an Uncertain Environment Using Predictive Hebbian Learning 601  A  B  1.0  0.8  0.4  0.2  100.0  . 80.0   60.0  40.0  ;> 20.0  0.0 0.0  0.0 5.0 10.0  Nectar volume (gl)  0 5 0 5 20 25 30  Trial  Figure 2: Simulations of bee foraging behavior using predictive Hebbian learning. A)  Reinforcement neuron output as a function of nectar volume for a fixed concentration of  nectar[9, 10]. B) Proportion of visits to blue flowers. Each trial represents approximately  40 flower visits averaged over 5 real bees and exactly 40 flower visits for a single model  bee. Trials 1 - 15 for the real and model bees had blue flowers as the constant type, the  remaining trials had yellow flowers as constant. At the beginning of each trial, w  and w e  were set to 0.5 consistent with evidence that information from past foraging bouts is not  used[ 14]. The real bees were more variable than the model bees - sources of stochasticity  such as the two-dimensional feeding ground were not represented. The real bees also had a  slight preference for blue flowers [21]. Note the slower drop for 2 = 0.1 when the flowers  are switched.  and (ii) there is no provision for allowing a sensory event to predict the future delivery of  reinforcement. The latter problem makes equation 1 inconsistent with a substantial volume  of data on classical and instrumental conditioning [16]. Adding a postsynaptic factor to  equation 1 does not alter these conclusions [ 17].  This inadequacy suggests that another form of learning rule and a model in which P has a  direct input from ft. Assume that the firing rate of P is sensitive only to changes in its input  over time and habituates to constant or slowly varying input, like magnocellular ganglion  cells in the retina [18]. Under this assumption, the output of P, 6t, reflects a temporal  derivative of its net input, approximated by:  6t = v(rt + v0 - + (2)  where T is a factor that controls the weighting of near against distant rewards. We take  3/= 1 for the current discussion.  In the presence of the reinforcement, the weights w B and w  are adjusted according to the  simple correlational rule:  = -xt6t. (3)  This permits the weights onto P to act as predictions of the expected reward consequent  on landing on a flower and can also be derived in a more general way for the prediction of  future values of any scalar quantity [ 19].  602 Montague, Dayan, and Sejnowski  A  100.0  80.0 .  60.0 5'  40.0  20.0  0.0   0.0 2.0  --V=2  0 0=8   ' v= 30  I I  4.0 6.0  B  30.0  20.0  10.0  )= 0.1  ) = 0.9  bee  O.O  O.O 2.0 4.0 6.O  Mean Mean  Figure 3: Tradeoff between the mean and variance of nectar delivery. A) Method of  selecting indifference points. The indifference point is taken as the first mean for a given  variance (bold v in legend) for which a stochastic trial demonstrates the indifference. This  method of calculation tends to bias the indifference points to the left. B) Indifference plot  for model and real bees. Each point represents the (mean, variance) pair for which the  bee sampled each flower type equally. The circles are for 2 = 0.l and the pluses are for  , = 0.9.  When the bee actually lands on a flower and samples the nectar, R influences the output of  P through its fixed connection (Fig. 1). Suppose that just prior to sampling the nectar the  bee switched to viewing a blue flower, for example. Then, since rt-1 -- 0, 6t would be  rt - xB lwtB_i In this way, the term xt_ wt_ 1 is a prediction of the value ofrt and the  difference rt -- xt B_ 1 w't B- 1 is the error in that prediction. Adjusting the weight wt  according  to the correlational rule in equation 3 allows the weight wt , through P's outputs, to report  to the rest of the brain the amount of reinforcement rt expected from blue flowers when  they are sensed.  As the model bee flies between flowers, reinforcement from nectar is not present (rt -- 0)  and 6t is proportional to Vt- Vt- 1. w B and W Y can again be used as predictions but through  modulation of action choice. For example, suppose the learning process in equation 3 sets  w Y less than w 8. In flight, switching from viewing yellow flowers to viewing blue flowers  causes 6t to be positive and biases the activity in any acticOn selection units driven by  outgoing connections from B. This makes the bee more likely than chance to land on or  steer towards blue flowers. This discussion is not offered as an accurate model of action  choice, rather, it simply indicates how output from a diffuse system could also be used to  influence action choice.  The biological assumptions of this neural architecture are explicit: (i) the diffusely pro-  jecting neuron changes its firing according to the temporal difference in its inputs; (ii) the  output of P is used to adjust its weights upon landing; and (iiO the output otherwise biases  the selection of actions by modulating the activity of its target neurons.  For the particular case of the bee, both the learning rule described in equation 3 and the  biasing of action selection described above can be further simplified for the purposes of a  Foraging in an Uncertain Environment Using Predictive Hebbian Learning 603  simple demonstration. As mentioned above, significant learning about a particular flower  color may occur only in the 1 - 2 seconds just prior to an encounter [21, 14]. This  is tantamount to restricting weight changes to each encounter with the reinforcer which  allows only the sensory input just preceding the delivery or non-delivery of rt to drive  synaptic plasticity. We therefore make the learning rule punctate, updating the weights on  a flower by flower basis. During each encounter with the reinforcer in the environment, P  produces a prediction error 6t -- rt - Vt- 1 where rt is the actual reward at time t, and the  last flower color seen by the bee at time t, say blue, causes a prediction Vt-  -- wt _  xt _   of future reward rt to be made through the weight wt _ 1 and the input activity xt _ 1. The  weights are then updated using a form of the delta rule[20]:  last -- last- 1 q- tXt-1, (4)  where X is a time constant and controls the rate of forgetting. In this rule, the weights from  the sensory input onto P still mediate a prediction of r; however, the temporal component  for choosing how to steer and when to land has been removed.  We model the temporal biasing of actions such as steering and landing with a probabilistic  algorithm that uses the same weights onto P to choose which flower is actually visited  on each trial. At each flower visit, the predictions are used directly to choose an action,  according to:  e(,x )  To apply the model to the foraging experiment, it is necessary to specify how the amount of  nectar in a particular flower gets reported to P. We assume that the reinforcement neuron  l delivers its signal rt as a saturating function of nectar volume (Fig. 2A). Harder and  Real [10] suggest just this sort of decelerating function of nectar volume and justify it on  biomechanical grounds. Fig. 2B shows the behavior of model bees compared with that of  real bees [9] in the experiment testing the extent to which they prefer a constant reward to  a variable reward of the same long-term mean. Further details are presented in the figure  legend.  The behavior of the model matched the observed data for X = 0.9 suggesting that the real  bee utilizes information over a small time window for controlling its foraging [9]. At this  value of X, the average proportion of visits to blue was 85% for the real bees and 83%  for the model bees. The constant and variable flower types were switched at trial 15 and  both bees switched flower preference in 1 - 3 subsequent visits. The average proportion  of visits to blue changed to 23% and 20%, respectively, for the real and model bee. Part of  the reason for the real bees' apparent preference for blue may come from inherent biases.  Honey bees, for instance, are known to learn about shorter wavelengths more quickly than  others [21]. In our model, X is a measure of the length of time over which an observation  exerts an influence on flower selection rather than being a measure of the bee's time horizon  in terms of the mean rate of energy intake [9, 10].  where q(Y) is the probability of choosing a yellow flower. Values of It > 0 amplify the  difference between the two predictions so that larger values of  make it more likely that  the larger prediction will result in choice toward the associated flower color. In the limit as   -- c this approaches a winner-take-all rule. In the simulations,  was varied from 2.8 to  6.0 and comparable results obtained. Changing  alters the magnitude of the weights that  develop onto neuron P since different values of  enforce different degrees of competition  between the predictions.  604 Montague, Dayan, and Sejnowski  Real bees can be induced to forage equally on the constant and variable flower types if the  mean reward from the variable type is made sufficiently large, as in Fig. 3B. For a given  variance, the mean reward was increased until the bees appeared indifferent between the  flowers. In this experiment, the constant flower type contained 0.5t of nectar. The data  for the real bee is shown as points connected by a solid line in order to make clear the  envelope ofthereal data. The indifference points for X = 0.1 (circles)and X = 0.9 (pluses)  also demonstrate that a higher value of X is again better at reproducing the bee's behavior.  The model captured both the functional relationship and the spread of the real data.  The diffuse neurotransmitter system reports prediction errors to control learning and bias  the selection of actions. Distributing such a signal diffusely throughout a large set of  target structures permits this prediction error to influence learning generally as a factor in a  correlational or Hebbian rule. The same signal, in its second role, biases activity in an action  selection system to favor rewarding behavior. In the model, construction of the prediction  error only requires convergent input from sensory representations onto a neuron or neurons  whose output is a temporal derivative of its input. The output of this neuron can also be  used as a secondary reinforcer to associate other sensory stimuli with the predicted reward.  We have shown how this relatively simple predictive learning system closely simulates the  behavior of bumble bees in a foraging task.  Acknowledgements  This work was supported by the Howard Hughes Medical Institute, the National Institute  of Mental Health, the UK Science and Engineering Research Council, and computational  resources from the San Diego Supercomputer Center. We would like to thank Patricia  Churchland, Anthony Dayan, Alexandre Pouget, David Raizen, Steven Quartz and Richard  Zemel for their helpful comments and criticisms.  References  [1] Konorksi, J. Conditioned reflexes and neuron organization, (Cambridge, England,  Cambridge University Press, 1948).  [2] Quartz, SR, Dayan, P, Montague, PR, Sejnowski, TJ. (1992)SocietyforNeurosciences  Abstracts. 18, 210.  [3] Montague, PR, Dayan, P, Nowlan, SJ, Pouget, A, Sejnowski, TJ. (1993) In Advances  in Neural Information Processing Systems 5, SJ Hanson, JD Cowan, CL Giles, editors,  (San Mateo CA.' Morgan Kaufmann), pp. 969-976.  [4] Morrison, JH and Magistretti, PJ. Trends in Neurosciences, 6, 146 (1983).  [5] Wise, RA. Behavioral and Brain Sciences, 5, 39 (1982).  [6] Cole, BJ and Robbins, TW. Neuropsychopharmacology, 7, 129 (1992).  [7] Schultz, W. Seminars in the Neurosciences, 4, 129 (1992).  [8] Hammer, M, thesis, FU Berlin (1991).  [9] Real, LA. Science, 253, pp 980 (1991).  Foraging in an Uncertain Environment Using Predictive Hebbian Learning 605  [10]  [11]  [12]  [13]  [14]  [151  [16]  [17]  [18]  [19]  [20]  [21]  Real, LA. Ecology, 62, 20 (1981); Harder, LD and Real, LA. Ecology, 68(4), 1104  (1987); Real, LA, Ellner, S, Harder, LD. Ecology, 71(4), 1625 (1990).  Berry, DA and Fristedt, B. Bandit Problems: Sequential Allocation of Experiments.  (London, England: Chapman and Hall, 1985).  Gould, JL. In Foraging Behavior, AC Kamil, JR Krebs and HR Pulliam, editors, (New  York, NY: Plenum, 1987), p 479.  Krebs, JR, Kacelnik, A, Taylor, P. Nature,, 275, 27 (1978), Houston, A, Kacelnik,  A, McNamara, J. In Functional Ontogeny, D McFarland, editor, (London: Pitman,  1982).  Menzel, R and Erber, J. Scientific American, 239(1), 102.  Carew, TJ, Hawkins RD, Abrams TW and Kandel ER. Journal of Neuroscience, 4(5),  1217 (1984).  Mackintosh, NJ. Conditioning and Associative Learning. (Oxford, England: Oxford  University Press, 1983). Sutton, RS and Barto, AG. Psychological Review, 88 2, 135  (1981). Sutton, RS and Barto, AG. Proceedings of the Ninth Annual Conference of  the Cognitive Science Society. Seattle, WA (1987).  Reeke, GN, Jr and Sporns, O. Annual Review of Neuroscience. 16, 597 (1993).  Dowling, JE. The Retina. (Cambridge, MA: Harvard University Press, 1987).  The overall algorithm is a temporal difference (TD) learning rule and is related to  an algorithm Samuel devised for teaching a checker playing program, Samuel, AL.  IBM Journal of Research and Development, 3, 211 (1959). It was first suggested in  its present form in Sutton, RS, thesis, University of Massachusetts (1984); Sutton and  Barto [1] showed how it could be used for classical conditioning; Barto, AG, Sutton,  RS and Anderson, CW. IEEE Transactions on Systems, Man, and Cybernetics, 13,  834 (1983) used a variant of it in a form of instrumental conditioning task; Barto,  AG, Sutton, RS, Watkins, CJCH, Technical Report 89-95, (Computer and Information  Science, University of Massachusetts, Amherst, MA, 1989); Barto, AG, Bradtke, S J,  Singh, SP, Technical Report 91-57, (Computer and Information Science, University of  Massachusetts, Amherst, MA, 1991) showed its relationship to dynamic programming,  an engineering method of optimal control.  Rescorla, RA and Wagner, AR. In Classical Conditioning H: Current Research and  Theory, AH Black and WF Prokasy, editors, (New York, NY: Appleton-Century-  Crofts, 1972), p 64; Widrow, B and Stearns, SD. Adaptive Signal Processing, (Engle-  wood Cliffs, NJ: Prentice-Hall, 1985).  Menzel, R, Erber, J and Masuhr, J. In Experimental Analysis of Insect Behavior, LB  Browne, editor, (Berlin, Germany: Springer-Verlag, 1974), p 195.  
How to Describe Neuronal Activity:  Spikes, Rates, or Assemblies?  Wulfram Getstrier and J. Leo van Hemmen  Physik-Department der TU Miinchen  D-85748 Garching bei Miinchen, Germany  Abstract  What is the 'correct' theoretical description of neuronal activity?  The analysis of the dynamics of a globally connected network of  spiking neurons (the Spike Response Model) shows that a descrip-  tion by mean firing rates is possible only if active neurons fire in-  coherently. If firing occurs coherently or with spatio-temporal cor-  relations, the spike structure of the neural code becomes relevant.  Alternatively, neurons can be gathered into local or distributed en-  sembles or 'assemblies'. A description based on the mean ensemble  activity is, in principle, possible but the interaction between differ-  ent assemblies becomes highly nonlinear. A description with spikes  should therefore be preferred.  1 INTRODUCTION  Neurons communicate by sequences of short pulses, the so-called action potentials  or spikes. One of the most important problems in theoretical neuroscience concerns  the question of how information on the environment is encoded in such spike trains:  Is the exact timing of spikes with relation to earlier spikes relevant (spike or interval  code (MacKay and McCulloch 1952) or does the mean firing rate averaged over sev-  eral spikes contain all important information (rate code; see, e.g., Stein 1967)7 Are  spikes of single neurons important or do we have to consider ensembles of equivalent  neurons (ensemble code)7 If so, can we find local ensembles (e.g., columns; Hubel  and Wiesel 1962) or do neurons form 'assemblies' (Hebb 1949) distributed all over  the network7  463  464 Gerstner and van Hemmen  2 SPIKE RESPONSE MODEL  We consider a globally connected network of N neurons with 1 _< i < N. A neuron i  fires, if its membrane potential passes a threshold 0. A spike at time t[ is described  by a &pulse; thus Si(t) F  ---- Ef=i ((t -- t/f) is the spike train of neuron i. Spikes are  labelled such that t is the most recent spike and t/F is the Ftn spike going back in  time.  In the Spike Response Model, short SRM, (Gerstner 1990, Gerstner and van Hem-  men 1992) a neuron is characterized by two different response functions, e and l ref.  Spikes which neuron i receives from other neurons evoke a synaptic potential  N o  j--1  (1)  where the response kernel  : s-a' exp(  T s Ts  for s < Atr  for s > Atr (2)  describes a typical excitatory or inhibitory postsynaptic potential; see Fig. 1. The  weight Zij is the synaptic efficacy of a connection from j to i, Atr is the axonal (and  synaptic) transmission time, and rs is a time constant of the postsynaptic neuron.  The origin s = 0 in (2) denotes the firing time of a presynaptic spike. In simulations  we usually assume rs = 2 ms and for Atr a value between 1 and 4 ms  Similarly, spike emission induces refractoriness immediately after spiking. This is  modelled by a refractory potential  h? (t) = (s)S 1 (t - s)d  (3)  with a refractory function  {-cx for S < 7 re  rf(s) = r/o/(S-7 ) for s  7 r'.  (4)  For 0 _ s _< 7 ! the neuron is in the absolute refractory period and cannot spike at  all whereas for s > 7 ! spiking is possible but difficult (relative refractory period).  To put it differently, 0 - r/ (s) describes an increased threshold immediately after  spiking; cf. Fig. 1. In simulations, 7 ! is taken to be 4 ms. Note that, for the sake  of simplicity, we assume that only the most recent spike $/ induces refractoriness  whereas all past spikes $ contribute to the synaptic potential; cf., Eqs. (1) and (3).  How to Describe Neuronal Activity: Spikes, Rates, or Assemblies? 465  1,0  0.0  0.0 5.0  10.0 15.0 20.0  s[ms]  Fig I Response functions.  Immediately after firing at s =  0 the effective threshold is in-  creased to 0-r/f(s)(dashed).  The form of an excitatory post-  synaptic potential (EPSP) is  described by the response func-  tion e(s) (sohd). It is delayed by  a time A tr. The arrow denotes  the period Toc of coherent os-  cillations; cf. Section 5.  The total membrane potential is the sum of both parts, i.e.  h,(t) = ? (t) +  (5)  Noise is included by introduction of a firing probability  PF(h; St) = r-l (h ) St. (6)  where 5t is an infinitesimal time interval and r(h) is a time constant which depends  on the momentary value of the membrane potential in relation to the threshold 0.  In analogy to the chemical reaction constant we assume  r(h) = r0 exp[-/3(h - 0)], (7)  where r0 is the response time at threshold. The parameter/3 determines the amount  of noise in the system. For/3 - oo we recover the noise-free behavior, i.e., a neuron  fires immediately, if h > 0 (r - 0), but it cannot fire, if h < 0 (r - oo). Eqs. (1),  (3), (5), and (6) define the spiking dynamics in a network of SRM-neurons.  3 FIRING STATISTICS  We start our considerations with a large ensemble of identical neurons driven by the  same arbitrary synaptic potential hSU"(t). We assume that all neurons have fired a  first spike at t = tl . Thus the total membrane potential is h(t) = hY"(t) + rl rf (t-  tl). If h(t) slowly approaches 0, some of the neurons will fire again. We now ask  for the probability that a neuron which has fired at time t{ will fire again at a later  time t. The conditional probability P?)(tlt{) that the next spike of a given neuron  occurs at time t > t{ is  P(F2)(tltl ) = r-l[h(t)]exp - r-[h(s')]ds ' . (8)  The exponential factor is the portion of neurons that have survived from time t{ to  time t without firing again and the prefactor r-l[h(t)] is the instantaneous firing  probability (6) at time t. Since the refractory potential is reset after each spike,  the spiking statistics does not depend on earlier spikes, in other words, it is fully  described by P?)(tlt). This will be used below; cf. Eq. (14).  466 Gerstner and van Hemmen  As a special case, we may consider constant synaptic input h y" -- h . In this case,  (8) yields the distribution of inter-spike intervals in a spike train of a neuron driven  by constant input h . The mean firing rate at an input level h  is defined as the  inverse of the mean inter-spike interval. Integration by parts yields  f[h ] -- dt(t-t)P?)(tltl ) = ds exp { 7'-l[hOLvref(st)ld8 t }  (9)  Thus both firing rate and interval distribution can be calculated for arbitrary inputs.  4  ASSEMBLY FORMATION AND NETWORK  DYNAMICS  We now turn to a large, but structured network. Structure is induced by the  formation of different assemblies in the system. Each neuronal assembly cu (Hebb  1949) consists of neurons which have the tendency to be active at the same time.  Following the traditional interpretation, active means an elevated mean firing rate  during some reasonable period of time. Later, in Section 5.3, we will deal with a  different interpretation where active means a spike within a time window of a few  ms. In any case, the notion of simultaneous activity allows to define an activity  pattern {, 1 _< i <_ N} with  = i if i G cu and  = 0 otherwise. Each neuron  may belong to different assemblies 1 _< tt <_ q. The vector i = (/,...,[) is the  'identity card' of neuron i, e.g., i = (1, 0, 0, 1, 0) says that neuron i belongs to  assembly i and 4 but not to assembly 2,3, and 5.  Note that, in general, there are many neurons with the same identity card. This  can be used to define ensembles (or sublattices) L(x) of equivalent neurons, i.e.,  L(x) = {ili = x} (van Hemmen and Kiihn 1991). In general, the number of  neurons IL(x)l in an ensemble L(x) goes to infinity if N - cx:, and we write  IL(x)l = p(x)N. The mean activity of an ensemble L(x) can be defined by  et+At  A(x,t)= lim lim IL(x)l- x J, Sii(t)dt. (10)  At--*0  i )  In the following we assume that the synaptic efficacies have been adjusted according  to some Hebbian learning rule in a way that allows to stabilize the different activity  patterns or assemblies a u. To be specific, we assume  J0 q q  Jij -- - Y Y Qupost()pre() (11)  /=1 ':1  where post(x) and pre(x) are some arbitrary functions characterizing the pre- and  postsynaptic part of synaptic learning. Note that for Qu = u and post(x) and  pre(x) linear, Eq. (11) can be reduced to the usual Hebb rule.  With the above definitions we can write the synaptic potential of a neuron i  L(x)  in the following form  q q oo  h*Y"x,t) = Jo Y. Y] Qupost(x ) y.pre(z ) fo (s')p(z)A(z,t - s')ds'. (12)  /=1 ,=1  How to Describe Neuronal Activity: Spikes, Rates, or Assemblies? 467  We note that the index i and j has disappeared and there remains a dependence  upon x and z only. The activity of a typical ensemble is given by (Gerstner and  van Hemmen 1993, 1994)  A(x, t): P?)(tlt- s)A(x,t- s)ds (13)  where  p?)(tlt_s)=r-l[hY"(x,t)+rf*/(s)]exp- '  (14)  is the conditional probability (8) that a neuron i  L(x) which has fired at time  t-s fires again at time t. Equations (12) - (14) define the ensemble dynamics of the  network.  5 DISCUSSION  5.1 ENSEMBLE CODE  Equations. (12) - (14) show that in a large network a description by mean ensemble  activities is, in principle, possible. A couple of things, however, should be noted.  First, the interaction between the activity of different ensembles is highly nonlinear.  It involves three integrations over the past and one exponentiation; cf. (12) - (14).  If we had started theoretical modeling with an approach based on mean activities,  it would have been hard to find the correct interaction term.  Second, L(x) defines an ensemble of equivalent neurons which is a subset of a given  assembly cu. A reduction of (12) to pure assembly activities is, in general, not  possible. Finally, equivalent neurons that form an ensemble L(x) are not necessarily  situated next to each other. In fact, they may be distributed all over the network;  cf. Fig. 2. In this case a local ensemble average yields meaningless results. A  theoretical model based on local ensemble averaging is useful only if we know that  neighboring neurons have the same 'identity card'.  a) activity   20  1 oo  b) o) rate [Hz]  =1 2o   10  0  100  1 $0 200  time [ms]  '.'.'-'.';:: : .':'-: ,:;'.:: C C .  :.:.'.. :.',  '.'  '.'  :' :' ..:.? .. =..' ..-..-....: ..  1 $0 200  time [ms]  0 1 O0 200  rate [Hz]  Fig. 2  Stationary activity (incoherent  firing). In this case a descrip-  tion by firing rates is possible.  (a) Ensemble averaged activity  A(x,t). (b) Spike raster of 30  neurons out of a network of  4000. (c) Time-averaged mean  firing rate f. We have two dif-  ferent assemblies, one of them  active (A t = 2 ms, /3 = 5).  5.2 RATE CODE  Can the system of Eqs. (12) -(14) be transformed into a rate description? In general,  this is not the case but if we assume that the ensemble activities are constant in  468 Gerstner and van Hemmen  1.O  0.8  0.6  0.4  0.2  0.0  0 I O0  200 300  Zeit [ms]  A x 35  A 4  400 500 600  Fig. 3 Stability of stationary states.The postsynaptic potential h u" is plotted as a function  of time. Every 100 ms the delay A t has been increased by 0.5 ms. In the stationary state  (A t = 1.5 ms and A t = 3.5 ms), active neurons fire regularly with rate T - = 1/5.5 ms.  For a delay A t > 3.5 ms, oscillations with period Wl = 2r/Tp build up rapidly. For  intermediate delays 2 _ A t _ 2.5 small-amplitude oscillations with twice the frequency  occur. Higher harmonics are suppressed by noise (/3 = 20).  time, i.e., A(x,t) -- A(x), then an exact reduction is possible. The result  fixed-point equation (Gerstner and van Hemmen 1992)  q q  A(x) = f[Jo E E Oupst(xu) Epre(z)P(z)A(z)]  /=1 '-1 Z  where  {/o /o  f[h 'u'] = dsexp{- r-[h u' + rlref(st)]dst}  is a  (15)  is the mean firing rate (9) of a typical neuron stimulated by a synaptic input h u.  Constant activities correspond to incoherent, stationary firing and in this case a  rate code is sufficient; cf. Fig. 2.  Two points should, however, be kept in mind. First, a stationary state of incoherent  firing is not necessarily stable. In fact, in a noise-free system the stationary state  is always unstable and oscillations build up (Gerstner and van Hemmen 1993). In  a system with noise, the stability depends on the noise level fi and the delay Atr of  axonal and synaptic transmission (Gerstner and van Hemmen 1994). This is shown  in Fig. 3 where the delay A t has been increased every 100 ms. The frequency of  the smMl-amplitude oscillation around the stationary state is approximately equal  to the mean firing rate (16) in the stationary state or higher harmonics thereof.  A smMl-amplitude oscillation corresponds to partially synchronized activity. Note  that for A t = 4 ms a large-amplitude oscillation builds up. Here all neurons fire in  nearly perfect synchrony; cf. Fig. 4. In the noiseless case/? - oc, the oscillations  period of such a collective or 'locked' oscillation can be found from the threshold  condition  Toc=inf s[O=rf*f(s)+Jo c(ns) . (17)  n----1  In most cases the contribution with n = 1 is dominant which allows a simple graph-  ical solution. The first intersection of the effective threshold 0- r/J'(s) with the  (16)  How to Describe Neuronal Activity: Spikes, Rates, or Assemblies? 469  weighted EPSP Joe(s) yields the oscillation period; cf. Fig 1. An analytical argu-  d e  ment shows that locking is stable only if  12o,c > 0 (Gerstner and van Hemmen  1993).  a) activity  b)  o  1 oo 1,50 200  time [ms]  30   10  0  100  rate [Hz]  0 100 200  rate [Hz]  Fig. 4  Oscillatory activity (coherent  firing). In this case a descrip-  tion by firing rates must be com-  bined with a description by en-  semble activities. (a) Ensemble  averaged activity A(x,t). (b)  Spike raster of 30 neurons out  of a network of 4000. (c) Time-  averaged mean firing rate f. In  this simulation, we have used  A r=4ms and/?=8.  Second, even if the incoherent state is stable and attractive, there is always a transi-  tion time before the stationary state is assumed. During this time, a rate description  is insufficient and we have to go back to the full dynamic equations (12) - (14). Sim-  ilarly, if neurons are subject to a fast time-dependent external stimulus, a rate code  fails.  5.3 SPIKE CODE  A superficial inspection of Eqs. (12) - (14) gives the impression that all information  about neuronal spiking has disappeared. This is, however, false. The term A(x, t-s)  in (13) denotes all neurons with 'identity card' x that have fired at time t-s. The  integration kernel in (13) is the conditional probability that one of these neurons  fires again at time t. Keeping t-s fixed and varying t we get the distribution  of inter-spike intervals for neurons in L(x). Thus information on both spikes and  intervals is contained in (13) and (14).  We can make use of this fact, if we consider network states where in every time step a  different assembly is active. This leads to a spatio-temporal spike pattern as shown  in Fig. 5. To transform a specific spike pattern into a stable state of the network  we can use a Hebbian learning rule. However, in contrast to the standard rule, a  synapse is strenthened only if pre- and postsynaptic activity occurs simultaneously  within a time window of a few ms (Gerstner et al. 1993). Note that in this case,  averaging over time or space spoils the information contained in the spike pattern.  5.4 CONCLUSIONS  Equations. (12) - (14) show that in our large and fully connected network an  ensemble code with an appropriately chosen ensemble is sufficient. If, however, the  efficacies (11) and the connection scheme become more involved, the construction  of appropriate ensembles becomes more and more difficult. Also, in a finite network  we cannot make use of the law of large number in defining the activities (10). Thus,  in general, we should always start with a network model of spiking neurons.  470 Gerstner and van Hemmen  a) activity  b)  2O  100 150 200  time [ms]  30 '  'e ' ' "' ' ' '  .....  (oo 15o 200  time [ms]  rate [Hz]  100 200  rate [Hz]  Fig. 5  Spatio-temporal spike pattern.  In this case, neither firing rates  nor locally averaged activities  contain enough information to  describe the state of the net-  work. (a) Ensemble averaged  activity A(t). (b) Spike raster of  30 neurons out of a network of  4000. (c) Time-averaged mean  firing rate f.  Acknowledgements: This work has been supported by the Deutsche Forschungs-  gemeinschaft (DFG) under grant No. He 1729/2-1.  References  Gerstner W (1990) Associative memory in a network of 'biological' neurons. In:  Advances in Neural Information Processing Systems 3, edited by R.P. Lippmann,  J.E. Moody, and D.S. Touretzky (Morgan Kaufmann, San Mateo, CA) pp 84-90  Gerstner W and van Hemmen JL (1992a) Associative memory in a network of  'spiking' neurons. Network 3:139-164  Gerstner W, van Hemmen JL (1993) Coherence and incoherence in a globally cou-  pled ensemble of pulse-emitting units. Phys. Rev. Lett. 71:312-315  Gerstner W, Ritz R, van Hemmen JL (1993b) Why spikes? Hebbian learning and  retrieval of time-resolved excitation patterns. Biol. Cybern. 69:503-515  Gerstner W and van Hemmen JL (1994) Coding and Information processing in  neural systems. In: Models of neural networks, Vol. 2, edited by E. Domany, J.L.  van Hemmen and K. Schulten (Springer-Verlag, Berlin, Heidelberg, New York) pp  lff  Hebb DO (1949) The Organization of Behavior. Wiley, NewYork  van Hemmen JL and Kiihn R(1991) Collective phenomena in neural networks. In:  Models of neural networks, edited by E. Domany, J.L. van Hemmen and K. Schulten  (Springer-Verlag, Berlin, Heidelberg, New York) pp lff  Hubel DH, Wiesel TN (1962) Receptive fields, binocular interaction and functional  architecture in the cat's visual cortex. J. Neurophysiol. 28:215-243  MacKay DM, McCulloch WS (1952) The limiting information capacity of a neuronal  link. Bull. of Mathm. Biophysics 14:127-135  Stein RB (1967) The information capacity of nerve cells using a frequency code.  Biophys. J. 7:797-826  
Optimal Unsupervised Motor Learning  Predicts the Internal Representation of  Barn Owl Head Movements  Terence D. Sanger  Jet Propulsion Laboratory  MS 303-310  4800 Oak Grove Drive  Pasadena, CA 91109  Abstract  (Masino and Knudsen 1990) showed some remarkable results which  suggest that head motion in the barn owl is controlled by distinct  circuits coding for the horizontal and vertical components of move-  ment. This implies the existence of a set of orthogonal internal co-  ordinates that are related to meaningful coordinates of the external  world. No coherent computational theory has yet been proposed  to explain this finding. I have proposed a simple model which pro-  vides a framework for a theory of low-level motor learning. I show  that the theory predicts the observed microstimulation results in  the barn owl. The model rests on the concept of "Optimal Un-  supervised Motor Learning", which provides a set of criteria that  predict optimal internal representations. I describe two iterative  Neural Network algorithms which find the optimal solution and  demonstrate possible mechanisms for the development of internal  representations in animals.  I INTRODUCTION  In the sensory domain, many algorithms for unsupervised learning have been pro-  posed. These algorithms learn depending on statistical properties of the input  data, and often can be used to find useful "intermediate" sensory representations  614  Barn Owl Head Movements 615  tl  Figure 1: Structure of Optimal Unsupervised Motor Learning. z is a reduced-order  internal representation between sensory data y and motor commands u. P is the  plant and G and N are adaptive sensory and motor networks. A desired value  of z produces a motor command u -- Nz resulting in a new intermediate value  : = GPNz.  by extracting important features from the environment (Kohonen 1982, Sanger  1989, Linsker 1989, Becker 1992, for example). An extension of these ideas to the  domain of motor control has been proposed in (Sanger 1993). This work defined the  concept of "Optimal Unsupervised Motor Learning" as a method for determining  optimal internal representations for movement. These representations are intended  to model the important controllable components of the sensory environment, and  neural networks are capa.ble of learning the computations necessary to gain control  of these components.  In order to use this theory as a model for biological systems, we need methods to  infer the form of biological internal representations so that these representations  can be compared to those predicted by the theory. Discrepancies between the  predictions and results may be due either to incorrect assumptions in the model, or  to constraints on biological systems which prevent them from achieving optimality.  In either case, such discrepancies can lead to improvements in the model and are  thus important for our understanding of the computations involved. On the other  hand, if the model succeeds in making qualitative predictions of biological responses,  then we can claim that the biological system possesses the optimality properties of  the model, although it is unlikely to perform its computations in exactly the same  manner.  2 BARN OWL EXPERIMENTS  A relevant set of experiments was performed by (Masino and Knudsen 1990) in  the barn owl. These experiments involved microstimulation of sites in the optic  teeturn responsible for head movement. By studying the responses to stimulation  at different sites separated by short or long time intervals, it was possible to infer the  existence of distinct "channels" for head movement which could be made refractory  by prior stimulation. These channels were oriented in the horizontal and vertical  directions in external coordinates, despite the fact that the neck musculature of the  barn owl is sufficiently complex that such orientations appear unrelated to any set  616 Sanger  of natural motor coordinates. This result raises two related questions. First, why  are the two channels orthogonal with respect to external Cartesian coordinates, and  second, why are they oriented horizontally and vertically?  The theory of Optimal Unsupervised Motor Learning described below provides a  model which attempts to answer both questions. It automatically develops orthogo-  nal internal coordinates since such coordinates can be used to minimize redundancy  in the internal representation and simplify computation of motor commands. The  selection of the internal coordinates will be based on the statistics of the components  of the sensory data which are controllable, so that if horizontal and vertical move-  ments are distinguished in the environment then these components will determine  the orientation of intermediate channels. We can hypothesize that the horizontal  and vertical directions are distinguished in the owl by their relation to sensory in-  formation generated from physical properties of the environment such as gravity or  symmetry properties of the owl's head. In the simulation below, I show that reason-  able assumptions on such symmetry properties are sufficient to guarantee horizontal  and vertical orientations of the intermediate coordinate system.  3 OPTIMAL UNSUPERVISED MOTOR LEARNING  Optimal Unsupervised Motor Learning (OUML) attempts to invert the dynamics of  an unknown plant while maintaining control of the most important modes (Sanger  1993). Figure I shows the general structure of the control loop, where the plant P  maps motor commands u into sensory outputs y: Pu, the adaptive sensory trans-  formation G maps sensory data y into a reduced order intermediate representation  z -- Gy, and the adaptive motor transformation N maps desired values of z into the  motor commands u = Nz which achieve them. Let 2 = GPNz be the value of the  intermediate variables after movement, and  - PNGy be the resulting value of the  sensory variables. For any chosen value of z we want 2: z, so that we successfully  control the intermediate variables.  In (Sanger 1993) it was proposed that we want to choose z to have lower dimen-  sionality than y and to represent only the coordinates which are most important  for controlling the desired behavior. Thus, in general,   y and Ily- $11 is the  performance error. OUML can then be described as  1. Minimize the movement error 1t9-  2. Subject to accurate control 2 = z.  These criteria lead to a choice of internal representation that maximizes the loop  gain through the plant.  Theorem 1: (Sanger 1993) For any sensory mapping G there exists a motor  mapping N such that  - z, and  - E[lly- ll] is minimized when G is chosen to  minimize E[]ly- where is such that = I.  The function  is an arbitrary right inverse of G, and this function determines the  asymptotic values of the unobserved modes. In other words, since G in general is  dimensionality-reducing, z - Gy will not respond to all the modes in y so that  dissimilar states may project to identical intermediate control variables z. The  Barn Owl Head Movements 617  Plant-  I Motor Sensory  Linear Linear Eigenvectors of E[yy '']  RBF Linear Eigenvectors of basis function outputs  Polynomial Polynomial Eigenvectors of basis function outputs  Figure 2: Special cases of Theorem 1. If the plant inverse is linear or can be  approximated using a sum of radial basis functions or a polynomial, then simple  closed-form solutions exist for the optimal sensory network and the motor network  only needs to be linear or polynomial.  function -G is a projection operator that determines the resulting plant output   for any desired value of y. Unsu_pervised motor learning is "optimal" when the  projection surface determined by G-1G is the best approximation to the statistical  density of desired values of y.  Without detailed knowledge of the plant, it may be difficult to find the general  solution described by the theorem. Fortunately, there are several important special  cases in which simple closed-form solutions exist. These cases are summarized  in figure 2 and are determined by the class of functions to which the plant inverse  belongs. If the plant inverse can be approximated as a sum of radial basis functions,  then the motor network need only be linear and the optimal sensory network is given  by the eigenvectors of the autocorrelation matrix of the basis function outputs (as  in (Sanger 1991a)). If the plant inverse can be approximated as a polynomial over  a set of basis functions (as in (Sanger 1991b)), then the motor network needs to be  a polynomial, and again the optimal sensory network is given by the eigenvectors  of the autocorrelation matrix of the basis function outputs.  Since the model of the barn owl proposed below has a linear inverse we are interested  in the linear case, so we know that the mappings N and G need only be linear and  that the optimal value of G is given by the eigenvectors of the autocorrelation matrix  of the plant outputs y. In fact, it can be shown that the optimal N and G are given  by the matrices of left and right singular vectors of the plant inverse (Sanger 1993).  Although several algorithms for iterative computation of eigenvectors exist, until  recently there were no iterative algorithms for finding the left and right singular  vectors. I have developed two such algorithms, called the "Double Generalized  Hebbian Algorithm" (DGHA) and the "Orthogonal Asymmetric Encoder" (OAE).  (These algorithms are described in detail elsewhere in this volume.) DGHA is  described by:  AG  AN "  while OAE is described by:  AG  AN "  = ,(zy - LT[zz*]6)  : 7(zu - LT[zz*]N *)  : 7(y * -  : 7(GY-  where LT[ ] is an operator that sets the above diagonal elements of its matrix  argument to zero, y = Pu, z: Gy, ; = N:ru, and ? is a learning rate constant.  618 Sanger  Neck Muscles  Movement Sensors  tl  Motor Transform  Sensory Transform  Figure 3: Owl model, and simulation results. The "Sensory Transform" box shows  the orientation tuning of the learned internal representation.  4 SIMULATION  I use OUML to simulate the owl head movement experiments described in (Masino  and Knudsen 1990), and I predict the form of the internal motor representation. I  assume a simple model for the owl head using two sets of muscles which are not  aligned with either the horizontal or the vertical direction (see the upper left block  of figure 3). This model is an extreme oversimplification of the large number of  muscle groups present in the barn owl neck, but it will serve to illustrate the case  of muscles which do not distinguish the horizontal and vertical directions.  I assume that during learning the owl gives essentially random commands to the  muscles, but that the physics of head movement result in a slight predominance of  either vertical or horizontal motion. This assumption comes from the symmetry  properties of the owl head, for which it is reasonable to expect that the axes of  rotational symmetry lie in the coronal, sagittal, and transverse planes, and that  the moments of inertia about these axes are not equal. I model sensory receptors  using a set of 12 oriented directionally-tuned units, each with a half-bandwidth at  half-height of 15 degrees (see the upper right block of figure 3). Together, the Neck  Muscles and Movement Sensors (the two upper blocks of figure 3) form the model  of the plant which transforms motor commands u into sensory outputs y. Although  this plant is nonlinear, it can be shown to have an approximately linear inverse on  Barn Owl Head Movements 619  Desired Direction  Figure 4: Unsupervised Motor Learning successfully controls the owl head simula-  tion.  its range.  The sensory units are connected through an adaptive linear network G to three  intermediate units which will become the internal coordinate system z. The three  intermediate units are then connected back to the motor outputs through a motor  network N so that desired sensory states can be mapped onto the motor commands  necessary to produce them. The sensory to intermediate and intermediate to motor  mappings were allowed to adapt to 1000 random head movements, with learning  controlled by DGHA.  5 RESULTS  After learning, the first intermediate unit responded to the existence of a motion,  and did not indicate its direction. The second and third units became broadly  tuned to orthogonal directions. Over many repeated learning sessions starting from  random initial conditions, it was found that the intermediate units were always  aligned with the horizontal and vertical axes and never with the diagonal motor  axes. The resulting orientation tuning from a typical session is shown in the lower  right box of figure 3.  Note that these units are much more broadly tuned than the movement sensors  (the half-bandwidth at half-height is 45 degrees). The orientation of the internal  channels is determined by the assumed symmetry properties of the owl head. This  information is available to the owl as sensory data, and OUML allows it to determine  the motor representation. The system has successfully inverted the plant, as shown  in figure 4.  (Masino and Knudsen 1990) investigated the intermediate representations in the  owl by taking advantage of the refractory period of the internal channels. It was  found that if two electrical stimuli which at long latency tended to move the owl's  head in directions located in adjacent quadrants were instead presented at short  latency, the second head movement would be aligned with either the horizontal or  vertical axis. Figure 5 shows the general form of the experimental results, which  are consistent with the hypothesis that there are four independent channels coding  620 Sanger  Move 1  Move 2a  ii..?Move 2b  Long Interval  Move 2a  Move 1 Move 2b  Short Interval  Figure 5: Schematic description of the owl head movement experiment. At long  interstimulus intervals (ISI), moves 2a and 2b move up and to the right, but at  short ISI the rightward channel is refractory from move 1 and thus moves 2a and  2b only have an upward component.  as  2a' and  1.2  0.  b. o  Figure 6: Movements align with the vertical axis as the ISI shortens. a. Owl  data (reprinted with permission from (Masino and Knudsen 1990)). b. Simulation  results.  the direction of head movement, and that the first movement makes either the  left, right, up, or down channels refractory. As the interstimulus interval (ISI) is  shortened, the alignment of the second movement with the horizontal or vertical  axis becomes more pronounced. This is shown in figure 6a for the barn owl and 6b  for the simulation. If we stimulate sites that move in many different directions, we  find that at short latency the second movement always aligns with the horizontal  or vertical axis, as shown in figure 7a for the owl and figure 7b for the simulation.  6 CONCLUSION  Optimal Unsupervised Motor Learning provides a model for adaptation in low-level  motor systems. It predicts the development of orthogonal intermediate representa-  tions whose orientation is determined by the statistics of the controllable compo-  nents of the sensory environment. The existence of iterative neural algorithms for  both linear and nonlinear plants allows simulation of biological systems, and I have  Barn Owl Head Movements 621  a  I.ONG  '  INT:RVAL  SHORT  Figure 7: At long ISI, the second movement can occur in many directions, but  at short ISI will tend to align with the horizontal or vertical axis. a. Owl data  (reprinted with permission from (Masino and Knudsen 1990)). b. Simulation re-  sults.  shown that the optimal internal representation predicts the horizontal and vertical  alignment of the internal channels for barn owl head movement.  Acknowledgement s  Thanks are due to Tom Masino for helpful discussions as well as for allowing re-  production of the figures from (Masino and Knudsen 1990). This report describes  research done within the 'laboratory of Dr. Emilio Bizzi in the department of Brain  and Cognitive Sciences at MIT. The author was supported during this work by a  National Defense,Science and Engineering Graduate Fellowship, and by NIH grants  5R37AR26710 and 5R01NS09343 to Dr. Bizzi.  References  Becker S., 1992, An Information-Theoretic Unsupervised Learning Algorithm for  Neural Networks, PhD thesis, Univ. Toronto Dept. Computer Science.  Kohonen T., 1982, Self-organized formation of topologically correct feature maps,  Biological Cybernetics, 43:59-69.  Linsker R., 1989, How to generate ordered maps by maximizing the mutual infor-  mation between input and output signals, Neural Computation, 1:402-411.  Masino T., Knudsen E. I., 1990, Horizontal and vertical components of head move-  ment are controlled by distinct neural circuits in the barn owl, Nature, 345:434-437.  Sanger T. D., 1989, Optimal unsupervised learning in a single-layer linear feedfor-  ward neural network, Neural Networks, 2:459-473.  Sanger T. D., 1991a, Optimal hidden units for two-layer nonlinear feedforward  neural networks, International Journal of Pattern Recognition and Artificial Intel-  ligence, 5(4):545-561, Also appears in C. H. Chen, ed., Neural Networks in Pattern  Recognition and Their Applications, World Scientific, 1991, pp. 43-59.  Sanger T. D., 1991b, A tree-structured adaptive network for function approximation  in high dimensional spaces, IEEE Trans. Neural Networks, 2(2):285-293.  Sanger T. D., 1993, Optimal unsupervised motor learning, IEEE Trans. Neural  Networks, in press.  
ADAPTIVE KNOT PLACEMENT FOR  NONPARAMETRIC REGRESSION  Hossein L. Najafi*  Department of Computer Science  University of Wisconsin  River Falls, WI 54022  Vladimir Cherkassky  Department of Electrical Engineering  University of Minnesota  Minneapolis, Minnesota 55455  Abstract  Performance of many nonparametric methods critically depends  on the strategy for positioning knots along the regression surface.  Constrained Topological Mapping algorithm is a novel method that  achieves adaptive knot placement by using a neural network based  on Kohonen's self-organizing maps. We present a modification to  the original algorithm that provides knot placement according to  the estimated second derivative of the regression surface.  I INTRODUCTION  Here we consider regression problems. Using mathematical notation, we seek to find  a function f of N - 1 predictor variables (denoted by vector X) from a given set of  n data points, or measurements, Zi = (Xi , Y/) (i = 1, ..., n) in N - dimensional  sample space:  Y -- f(X) -t- error  where error is unknown (but zero mean) aud its distribution may depend on X. The  distribution of points in the training set can be arbitrary, but uniform distribution  in the domain of X is often used.  * Responsible for correspondence,  hossein.naj afi@uwrf. edu.  Telephone (715) 425-3769, e-mail  247  248 Najafi and Cherkassky  The goal of this paper is to show how statistical considerations can be used to  improve the performance of a novel neural network algorithm for regression [CN91],  in order to achieve adaptive positioning of knots along the regression surface. By  estimating and employing the second derivative of the underlying function, the  modified algorithm is made more flexible around the regions with large second  derivative. Through empirical investigation, we show that this modified algorithm  allocates more units around the regions where the second derivative is large. This  increase in the local knot density introduces more flexibility into the model (around  the regions with large second derivative) and makes the model less biased around  these regions. However, no over-fitting is observed around these regions.  2 THE PROBLEM OF KNOT LOCATION  One of the most challenging problems in practical implementations of adaptive  methods for regression is adaptive positioning of knots along the regression surface.  Typically, knot positions in the domain of X are chosen as a subset of the training  data set, or knots are uniformly distributed in X. Once X-locations are fixed,  commonly used data-driven methods can be applied to determine the number of  knots. However, de Boor [dB78] showed that a polynomial spline with unequally  spaced knots can approximate an arbitrary function much better than a spline  with equally spaced knots. Unfortunately, the minimization problem involved in  determination of the optimal placement of knots is highly nonlinear and the solution  space is not convex [FS89]. Hence, the performance of many recent algorithms that  include adaptive knot placement (e.g. MARS) is difficult to evaluate analytically. In  addition, it is well-known that when data points are uniform, more knots should be  located where the second derivative of the function is large. However, it is difficult to  extend these results for non-uniform data in conjunction with data-dependent noise.  Also, estimating the second derivative of a true function is necessary for optimal  knot placement. Yet, the function itself is uuknown and its estimation depends on  the good placement of knots. This suggests the need for some iterative procedure  that alternates between function estimation(smoothing) and knot positioning steps.  Many ANN methods effectively try to solve the problem of adaptive knot loca-  tion using ad hoc strategies that are not statistically optimal. For example, local  adaptive methods [Che92] are generalization of kernel smoothers where the ker-  nel functions and kernel centers are deterlnined from the data by some adaptive  algorithm. Examples of local adaptive methods include several recently proposed  ANN models known as radial basis function (RBF) networks, regularization net-  works, networks with locally tuned units etc [BL88, MD89, PG90]. When applied  to regression problems, all these methods seek to fiud regression estimate in the  (most general) form /k=l biI-li(X, Ci) where X is the vector of predictor variable,  Ci is the coordinates of the i-th 'center' or 'bump', Hi is the response function of  the kernel type (the kernel width may be different for each center i), bi are linear  coefficients to be determined, and k is the total number of knots or 'centers'.  Whereas the general formulation above assumes global optimization of an error mea-  sure for the training set with respect, to all parameters, i.e. center locations, kernel  width and linear coefficients, this is not practically feasible because the error surface  is generally non-convex and may have local minima [PG90, MD89]. Hence most  Adaptive Knot Placement for Nonparametric Regression 249  practical approaches first solve the problem of center(knot) location and assume  identical kernel functions. Then the remaining problem of finding linear coefficients  bi is solved by using familiar methods of Linear Algebra [PG90] or gradient-descent  techniques [MD89]. It appears that the problem of center locations is the most  critical one for the local neural network techniques. Unfortunately, heuristics used  for center location are not based on any statistical considerations, and empirical  results are too sketchy [PG90, MD89]. In statistical methods knot locations are  typically viewed as free parameters of the model, and hence the number of knots  directly controls the model complexity. Alternatively, one can impose local regu-  larization constraints on adjacent knot locations, so that neighboring knots cannot  move independently. Such an approach is effectively implemented in the model of  self-organization known as Kohonen's Self-Organizing Maps (SOM) [Koh84]. This  model uses a set of units ("knots") with neighborhood relations between units de-  fined according to a fixed topological structure (typically 1D or 2D grid). During  training or self-organization, data points are presented to the map iteratively, one  at a time, and the unit closest to the data moves towards it, also pulling along its  topological neighbors.  3  MODIFIED CTM ALGORITHM FOR ADAPTIVE  KNOT PLACEMENT  The SOM model has been applied to nonparametric regression by Cherkassky and  Najafi [CN91] in order to achieve adaptive positioning of knots along the regres-  sion surface. Their technique, called Constrained Topological Mapping (CTM), is a  modification of Kohonen's self-organization suitable for regression problems. CTM  interprets the units of the Kohonen map as movable knots of a regression surface.  Correspondingly, the problem of finding regression estimate can be stated as the  problem of forming an M - dimensional topological map using a set of samples  from N - dimensional sample space (where M _< N - 1) . Unfortunately, straight-  forward application of the Kohouen Algorithm to regression problem does not work  well [CN91]. Because, the presence of noise in the training data can fool the algo-  rithm to produce a map that is a multiple-valued function of independent variables  in the regression problem (1). This problem is overcome in the CTM algorithm,  where the nearest neighbor is found in the subspace of predictor variables, rather  than in the input(sample) space [CN91].  We present next a concise description of the CTM algorithm. Using standard for-  mulation (1) for regression, the training data are N - dimensional vectors Zi = (Xi  , Y}), where Yi is a noisy observation of an unknown function of N - 1 predictor  variables given by vector Xi. The CTM algorithm constructs an M - dimensional  topological map in N - dimensional sample space (M _< N - 1) as follows:  0. Initialize the M - dimensional topological map in N - dimensional sample  space.  1. Given an input vector Z in N - dimensional sample space, find the closest  (best matching) unit i in the subspace of independent variables:  II W i II-' Minj{l[Z*- *   Wj II} Vj [1,..., L]  250 Najafi and Cherkassky  where Z* is the projection of the input vector onto the subspace of inde-  pendent variables, Wj is the projection of the weight vector of unit j, and  k is the discrete time step.  2. Adjust the units' weights according to the following and return to 1:  W(k + 1) = Wj(k) +/9(k)Cj(k)(Z(k) - W'3(k)) j (2)  where/9(k) is the learning rate and Cj(k) is the neighborhood for unit j at  iteration k and are given by:  ( [[i-  exp 0'5 ,/9(k) x So )  where k,a is the final value of the time step (k,a is equal to the product of  the training set size by the number of times it was recycled),/90 is the initial  learning rate, and/9 I is the final learning rate (/90 = 1.0 and/9 I - 0.05 were  used in all of our experiments), IIi- Jll is the topological distance between  the unit j and the best matched unit i and So is the initial size of the map  (i.e., the number of units per dimeusion) .  Note that CTM method achieves placement of units (knots) in X-space according  to density of training data. This is due to the fact that X-coordinates of CTM units  during training follow the standard Kohonen self-organization algorithm [Koh84],  which is known to achieve faithful approximation of an unknown distribution. How-  ever, existing CTM method does not place more knots where the underlying function  changes rapidly. The improved strategy for CTM knot placement in X-space takes  into account estimated second derivative of a fimction as is described next.  The problem with estimating second derivative is that the function itself is unknown.  This suggests using an iterative strategy for building a model, i.e., start with a crude  model, estimate the second derivative based on this crude model, use the estimated  second derivative to refine the model, etc. This strategy can be easily incorporated  into the CTM algorithm due to its iterative nature. Specifically, in CTM method  the map of knots(i.e., the model) becomes closer and closer to the final regression  model as the training proceeds. Therefore, at each iteration, the modified algorithm  estimates the second derivative at the best matching unit (closest to the presented  data point in X-space), and allows additional movement of knots proportional to  this estimate. Estimating the second derivative from the map (instead of using the  training data) makes sense due to smoothing properties of CTM.  The modified CTM algorithm can be summarized as follows:  1. Present training sample Zi = (Xi , Y/) to the map and find the closest (best  matching) unit i in the subspace of independent variables to this data point.  (same as in the original CTM)  2. Move the the map (i.e., the best matching unit and all its neighbors) toward  the presented data point (same as in the original CTM)  Adaptive Knot Placement for Nonparametric Regression 251  3. Estimate average second derivative of the function at the best matching  unit based on the current positions of the map units.  4. Normalize this average second derivative to an interval of [0,1].  5. Move the map toward the presented data point at a rate proportional to  the estimated normalizes average second derivative and iterate.  For multivariate functions only gradients along directions given by the topological  structure of the map can be estimated in step 4. For example, given a 2-dimensional  mesh that approximates function f(xl, x2), every unit of the map (except the border  units for which there will be only one neighbor) has two neighboring units along  each topological dimension. These neighboring units can be used to approximate  the function's gradients along the corresponding topological dimension of the map.  These values along each dimension can then be averaged to provide a local gradient  estimate at a given knot.  In step 5, estimated average second derivative ftt is normalized to [0,1] range using  bi - 1 - exp(l'"l/tan(r)) This is done because the value of second derivative is used  as the learning rate.  In step 6, the map is modified according to the following equation:  Wj(k 2F 1) - Wj(]c) 2F (1 - (k))i(k)Cj(k)(X(k ) - Wj(k))  j (4)  It is this second movement of the map that allows for more flexibility around the  region of the map where the second derivative is large. The process described by  equation (4) is equivalent to pulling all units towards the data, with the learning  rate proportional to estimated second derivative at the best matched unit. Note  that the influence of the second derivative is gradually increased during the process  of self-organization by the factor (1-/(k)). This factor account for the fact that the  map becomes closer and closer to the underlying filnction during self-organization;  hence, providing a more reliable estimate of second derivative.  4 EMPIRICAL COMPARISON  Performance of the two algorithms (original and modified CTM) was compared for  several low-dimensional problems. In all experiments the two algorithms used the  same training set of 100 data points for the univariate problems and 400 data points  for the 2-variable problems.  The training samples (Xi, Y/) were generated according to (1), with X i randomly  drawn from a uniform distribution in the closed interval [-1,1], and the error drawn  from the normal distribution N(0, (0.1)2). Regression estimates produced by the  self-organized maps were tested on a different set of n - 200 samples (test set)  generated in the same manuer as the training set.  We used the Average Residual, AR- -.i=[Y/- f(Xi)] ,as the performance  measure on the test set. Here, f(X) is the piecewise linear estimate of the function  with knot locations provided by coordinates of the units of trained CTM. The Aver-  252 Najafi and Cherkassky  age Residual gives an indication of standard deviation of the overall generalization  error.  1.2  1  0.8  o.6  0.4  0.2  o  -0.2  -0.8  i I I [ I I I I  -  True function -  . iS Original CTM --.  - ,k Modified CTM -+-- -  I I I I I I I I  -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1  X  Figure 1: A 50 unit map formed by the original and modified algorithm for the  Gaussian function.  1 True function  Original CTM --.  0.8 Modified CTM -+- -  0.6  0.4  0.2  0  -0.2  0.1  I I I I I I I I i  0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1  X  Figure 2: A 50 unit map formed by the original and modified algorithm for the step  function.  We used a gaussian function (f(x) = exp -64") and a step function for our first set  of experiments. Figure 1 and 2 show the actual maps formed by the original and  modified algorithm for these functions. It is clear from these figures that the modi-  fied algorithm allocates more units around the regions where the second derivative  is large. This increase in the local knot density has introduced more flexibility into  the model around the regions with large second derivatives. As a result of this the  Adaptive Knot Placement for Nonparametric Regression 253  model is less biased around these regions. However, there is no over-fitting in the  regions where the second derivative is large.  0.29  0.28  0.27  0.26  0.25  0.24  0.23  0.22  0.21  0.2  0.19  0.18  I I I I I I  OriginalCTM  _  Modified CTM -t--. _  : _.,.......+-.--+ :  I  0 10 20 30 40 50 60  # of units per dimension  70  Figure 3' Average Residual error as a functiou of the size of the map for the 3-  dimensional Step function  0.55  0.5  0.45  0.4  0.35  0.3  0.25  0.2  Original CTM"  Modified CTM -t--.  I I I I I I  10 20 30 40 50 60  # of units per dimension  70  Figure 4' Average Residual error as a function of the size of the map for the 3-  dimensional Sine function  To compare the behavior of the two algorithms in their predictability of structureless  data, we trained them on a constant function f(x) = 0 with error= N(0, (0.1)).  This problem is known as smoothing pure noise in regression analysis. It has been  shown [CN91] that the original algorithm handles this problem well and quality of  CTM smoothing is independent of the number of units in the map. Our experiments  254 Najafi and Cherkassky  show that the modified algorithm performs as good as the original one in this  respect.  Finally, we used the following two-variable functions (step, and sine) to see how  well the modified algorithm performs in higher dimensional settings.  1 for ((;rl < 0.5) A (;r2 < 0.5)) V ((;rl _ 0.5) A (r2 _ 0.5))  Step: f(xl, x2) -- 0 otherwise  Sine: f(x,x.)= sin (2wX/(x) 2+ (x2 2  The results of these experiments are summarized in Figure 3 and 4. Again we see  that the modified algorithm outperforms the original algorithm. Note that the above  example of a two-variable step function can be easily handled by recursive partition-  ing techniques such as CART [BFOS84]. However, recursive methods are sensitive to  coordinate rotation. On the other hand, CTM is a coordinate-independent method,  i.e. its performance is independent of any affine transformation in X-space.  References  [BFOS84]  [L88]  [Che921  [c91]  [dB781  [FS89]  [Koh84]  [MD89]  [PG90]  L. Breiman, J.H. Friedman, R.A. Olshen, and C.J. Stone. Classification  and Regression Trees. Wadswordth, Belmont, CA, 1984.  D.S. Broomhead and D. Lowe. Multivariable functional interpolation  and adaptive networks. Complex Systems, 2:321-355, 1988.  V. Cherkassky. Neural networks and nonparametric regression. In S.Y.  Kung, F. Fallside, J.Aa. Sorenson, and C.A. Kamm, editors, Neural Net-  works for Signal Processing, volume II. IEEEE, Piscataway, N J, 1992.  V. Cherkassky and H.L. Najafi. Constrained topological mapping for  nonparametric regression analysis. Neural Networks, 4:27-40, 1991.  C. de Boor. A Practical Guide to Splines. Springer-Verlag, 1978.  J.H. Friedman and B.W. Silverman. Flexible parsimonious smoothing  and additive modeling. Technometrics, 31(1):3-21, 1989.  T. Kohonen. Self-Organization and Associative Memory. Springer-  Verlag, third edition, 1984.  J. Moody and C.J. Darken. Fast learning in networks of locally tuned  processing units. Neural Computation, 1:281, 1989.  T. Poggio and F. Girosi. Networks for approximation and learning. Pro-  ceedings of the IEEE, 78(9):1481-1497, 1990.  
An Optimization Method of Layered  Neural Networks based on the Modified  Information Criterion  Sumio Watanabe  Information and Communication R & D Center  Ricoh Co., Ltd.  3-2-3, Shin-Yokohama, Kohoku-ku, Yokohama, 222 Japan  su mio@ip e. rdc. ricoh. co.j p  Abstract  This paper proposes a practical optimization method for layered  neural networks, by which the optimal model and parameter can  be found simultaneously. We modify the conventional information  criterion into a differentiable function of parameters, and then, min-  imize it, while controlling it back to the ordinary form. Effective-  hess of this inethod is discussed theoretically and experimentally.  I INTRODUCTION  Learning in artificial neural networks has been studied based on a statistical frame-  work, because the statistical theory clarifies the quantitative relation between the  empirical error and the prediction error. Let us consider a function (w; x) from  the input space R Ic to the output space R  with a parameter v. We assnine that  N  training samples {(xi, Yi)}i=l are taken froin the true probability density Q(x,y).  Let, us define the empirical error by  293  294 Watanabe  and the prediction error by  E(v) = / / ,,,y- (v;x)ll2Q(x,y)dxdy. (2)  If we find a parameter v* which minimizes Eemp(W), then  2(F(w*) + 1)  < E(w') >= (1 + NL ) < Ee,p(V') > +o(---), (3)  where <  > is the average value for the training samples, o(1/N) is a sinall term  which satisfies No(i/N) --, 0 when N  x>, and F(w*), N, and L are respectively  the numbers of the effective parameters of w*, the training samples, and output  units.  Although the average <  > cannot be calculated in the actual application, the  optimal model for the mininmm prediction error can be found by choosing the  model that minimizes the Akaike information criterion (AIC) [1],  2(F0v* ) + 1))Eeop(V*). (4)  J(v*) = (1 + NL  This method was generalized for arbitrary distance [2]. The Bayes information  criterion (BIC) [3] and the mininmm description length (MDL) [4] were proposed  to overcome the inconsistency problem of AIC that the true model is not always  chosen even when N  x>.  The above information criteria have been applied to the neural network model selec-  tion problem, where the maxinmm likelihood estinator w* was calculated for each  model, and then information criteria were cronpared. Nevertheless, the practical  problem is caused by the fact that we can not always find the mximuln likelihood  estimator for each model, and even if we can, it. takes long calculation time.  In order to ilnprove such 1nodel selection procedures, this paper proposes a prac-  tical learning algorithm by which the optimal model and parameter can be found  simultaneously. Let us consider a modified information criterion,  2(F,(w) + 1))E,m0v) ' (5)  J,Ov) = (1 + NL  where a, > 0 is a parameter and Fo,(tv) is a C'-class fiinction which converges  to F(iv) when a, --} O. We nfinimize J,(w), while controlling a, as a, --} O, To  show effectiveness of this xnethod, we show experimental results, and discuss the  theoretical background.  2 A Modified Information Criterion  2.1 A Formal Information Criterion  Let us consider a conditional probability distribution,  1 exp( Ily - (w;x)112  vlx) -  ), (6)  An Optimization Method of Layered Neural Networks 295  xvhere a function V(,v;x)= {qi0v; x)} is given by the three-layered perceptton,  H K  = p(,v,o + p(wo + ) (7)  j=l k=l  and w = {w$o, w 0 } is a set of biases and weights and p(.) is a sigmoidM function.  Let M be the full-connected neural network model with K input units, H hidden  units, and L output units, and : be the family of all models made fi'om M, by  pruning weights or eliminating biases. When a set of training samples {(, y)}=  is given, we define an empirical loss and the 1)rediction loss by  N  'm'(w') -- N -'1gP(w'';Yilxi)' (8)  i=1  Minimizing Lemp(W, rr) is  L(w, ) is equivalent to lninimizing E(w  (w;, a;) which minimizes L,,,p(tv. )  AIC, we have the following formula,  = -//Q(x,y)logPOv, a;y[x)dxdy. (9)  equivalent to minimizing E,,p(w), and minimizing  ). Wc assumc that there exists a parameter  in each model AI  :4. By the theory of  F(w;,) + 1 1  < L(v;,o';,) >=< L,,,,(wS,. rr;,) > + N + o(). (10)  Based on this property, let us dcfine a forraM information criterion I(M) for a model  M by  I(M) = 2NL,,q,('tv, ;, ) + A(Fo(v;) + 1) (11)  where A is a constant and Fo(w) is the number of nonzero parameters in w,  L H H K  Fo(W) = Z E fo(Wi5) + E Z f(w5)' (1)  i= j=o 5= k=o  where fo(X) is 0 if z = 0, or 1 if otherwise. I(M) is formally equal to AIC if  A = 2, or MDL if A = log(N). Note that F(w) N Fo(w) for arbitrary w and that  F(,) = Fo(w;) if and only if the Pisher information matrix of the model M is  positive definite.  2.2 A Modified Information Criterion  In order to find the optimal inodcl and parameter simultaneously, we define a mod-  ified information criterion. For a, > O.  Io(w,a) = 2NL,p(w,a) + A(Fo(tv) + 1), (13)  L H H I  i= j=o j= .=o  where fo,(x) satisfies the following two conditions.  296 Watanabe  (1) f,.(x) --} fo(x)when c --} O.  (2) If 151  lYl then 0  f,(x)  f,(y) _< 1.  For example, 1- exp(-x2/a 2) and 1- 1/(1 + (x/a) 2) satisfy this condition. Based  on these definitions, we have the following theorem.  Theorem rain 1(214) = lira rain I (w, a).  AI a.0 tv,  This theorem shows that the optilnal modcl and parameter can be found by mini-  mizing I(w, ) while controlling a  a,  0 (The parameter a, plays the same role  s the temperature in tn simulated annealing). As F,(x)  Fo(x} is not uniform  convergence, this theorem needs the second condition on f,(x). (For proof of the  theorem, see [5]).  If we choose a differentiable function for f(w), then its local minimun can be  found by the steepest descent method,  dw _ 0 da a  dt -- -Ov I(w'a)' dt - I'Ov'a)' (15)  These equations result in a learning dynamics,   A& 20F.  i=1  whr a  = (l/X;)ET:, II - (';:,)11 . nd . is o,,,ly ontrond s ,  0.  This dynamics can be understood as the error backpropagation with the added  term.  3 Experimental Results  3.1 The true distribution is contained in the models  First, xve consider a case when tile true distribution is contained in the 1nodel  family .A4. Figure 1 (1) shows tile true model from which tile training samples were  taken. One thonsand input smnples were taken from tile uniform probability on  [-0.5, 0.5] x [-0.5,0.5] x [-0.5, 0.5]. Tile output samples were calculated by tile  network in Figure 1 (1), and noizes were added which were taken from a normal  distribution with tile expectation 0 and thc variance 3.33 x 10 -3. Ten thousands  testing samples were taken fi'om the same distribution. We used f(w) = 1 -  exp(-w2/20 '2) as a softener function, and tile "annealing schcdule" of ca xws sct  as c(n) = a'o(1 - n./n,m,) + e, where ,n is tile training cycle number, a'o = 3.0,  n, = 25000, and e = 0.01. Figure 1 (2) shows the full-connected model  with 10 hidden units, which is the initial model. In the training, the learning speed  /was set as 0.1.  We compared tile empirical errors and the prediction errors for several cases for A  (Figure 1 (5), (6}). If A = 2, tile criterion is AIC, and if A = log(N) = 6.907, it is  BIC or MDL. Fignre 1 (3) and (4) show the optimized models and parameters for  tile criteria with A = 2 and A = 5. When A = 5, tile true model could be found.  An Optimization Method of Layered Neural Networks 297  3.2 The true distribution is not contained  Second, let us coilsider a case that the true distribution is not contained in the  model family. For tile training samples and tile testing samples, we used tile same  probability density as the above case except that the fimction was  1 {sin(r(x + x2)) + tanh(x3) + 2}.  (17)  Figure 2 (1) and (2) show the training error and the prediction error, respectively.  In this case, the best generalized model was found by AIC, shown in Figure 3. In  the optimized network, x and x2 were ahnost separated fi'om x3, which means that  the network could find the structure of the true modcl in eq.(17.)  The practical application to ultrasonic image reconstruction is shown in Figure 3.  4 Discussion  4.1 An information criterion and pruning weights  If P(w, o'; y{x) sufficiently approximates Q(.y[x) and N is sufficiently large, xve have    L(tLM,o-^I  Lemp(VM,ryM) -- .  ) +  F( wt ) + 1 1  N + Z +o() (18)  where ZN = Letup(IbM ) - L(',b ) and ', is the parameter which minimizes L(w, a)  in the model M. Although < Z2v >= 0 resulting in equation (10), its standard  deviation has the same order a.s (1/x/). However, if/1{ C/I.I2 or/14 D M2, then  b and 5M2 expected to be ahnost common. and it doesn't essentially affect the  model selection problem [2].  The model fanfily made by pruning weights or by eliminating biases is not a totally  ordered set but a partially ordered set for the order "C". Therefore, if a model  M  .M is selected, it is the optimal model in a local inodel family ga  = {M    .Ad;_3g  C -3/ or _3(  D _3I}, but it may not be tile optimal inodel ill the global  family .A4. Artificial neural networks have the local nininmm 1)roblem not only in  the parameter space but also in the model family.  4.2 The degenerate Fisher information matrix.  If the true probability is contained in tile model and tile numl)er of hidden units is  larger than necessary one, then the Fisher information matrix is degenerated, and  consequently, the maxinmm likelihood estimator is not subject to the asymptotically  normal distribution [6]. Therefore, the prediction error is not given by eq.(3), or  AIC cannot be derived. However, by the proposed method, the selected model has  the non-degenerated Filler information matrix, because if it. is degenerate then the  modified information criterion is not lninimized.  298 Watanabe  unit   hidden  3 .  unlt$  ,  (3,ttml (. 2, (4, Optimid by A:5  (1)True model (2) hitial model for learning. m ) -3  3.31 10  E(w% = 3.39 X 10 w% = 3.37 X 10'3  3.35 '  3.3 '  3.25  E (w*)  amp  10'3 A/' initial 1   2 6.9 tial 2  I I I I I I I I  I  I I I I  I A  AIC MDL  (5) The emprical error  E(w*) l x10 -3   3.45 - / ' initial  3.4 --  3.380 X 10-3  3.35-- I I 2 I I I I 61'9 I  A  I ' I I I I  I  AIC MDL  (6) The prediction error  initial 2  initial  Figure 1' True distribution is contained in the models.  EemgW*) initial 2  X 10 '3 initial 1 3.7  3.6  ial 3 3.6--  3.5  3.5  3.4  3.4  3.3 ]/,  I I I I 6'1  AIC BIC  (1) The empirical error  -3  E(w*) The empirical error 3.31X 10  initial 2  The prediction error 3.41X 10 -3  X 10.3 initial 7 2  itial 3  (-3'.96 ' . _ .'  35 ,, -45   3.409 X 10 -3  i 12   t  6' A  AIC BIC xl x2 x3  (2) The prediction error  (3) Optimized by AI C (A=2).  Figure 2: True distribution is not contained in the models.  An Optimization Method of Layered Neural Networks 299  .. , ': -,  -_ . . ..  5'..? ...... :': '  i  - '-5 I  :. .-.....):.  (1) An Ultrasonic hnaging Systmn  (2) Sample Objects.  hnagcs for Training Images for Testing  Reconsuucted Image : ' jjj?'-?]j}; ?.?:'?j:  / I pixel  . ,i -: 5L ::' ::, '::::.  hidden   neighborhood  est llg .[IC ....  [  U!asonic Image 3 32 Restored using MDL  (3) NeurM Networks (d)Restored hnages  Figure 3: Practical Application to Image Restoration  The proposed method was applied to ultrasonic image restoration. Figure 3 (1). (2),  (3), (4) respectively show an ult. r,'tsonic imaging system, the sample objects, and a  neural network for image restoration, and the original restored images. The nmnber  of paramet. ers optimized by LSM, AIC. and MDL wcrc respectively 166. 138. and  57. Rather noizeless images were obtained using the nodified AIC or MDL. For  example, the "Tail of R" was clearly restored using AIC.  300 Watanabe  4.3 Relation to another generalization methods  In the neural infornlation processing field, nlany methods have been proposed for  preventing the over-fitting problcnl. One of the nlost famous nlcthods is the weight  decay nlethod, in xvhich we assunit a priori probability distribution on the parameter  space and nlininlize  El (w) - Zemp(tU) q- he(w), (19)  where A and C(w) are chosen by several heuristic nlethods [7]. Tile BIG is tile  infornlation criterion for such a nlcthod [3], and the proposed nlcthod nlay be  understood ,as a nlcthod hoxv to control A and C(w).  5 Conclusion  An optinlization nlethod for layered neural networks was proposed based on the  modified information criterion, and its effectiveness was discussed theoretically and  experimentally.  Acknowledgements  The author xvould like to thank Prof. S. Amari, Prof. S. Yoshizawa, Prof. K.  Aihara in University of Tokyo, and all nlembers of tile Alnari seminar for their  active discussions about statistical methods in neural networks.  References  [1] H.Akaike. (1974) A New Look at tile Statistical Model Identification. it IEEE  Trans. oil Automatic Control, Vol. AC-19, No.6, pp.716-723.  [2] N.Murata, S.Yoshiza;va, and S.Ainari.(1992) Leariling Curves, Model Selection  and Conlplexity of Neural Networks. Advances in Neural Inforrn, ation Procssing  Systems 5, San Mateo, Morgan Kaufman, pp.607-614.  [3] C.Schwarz (1978) Estimating the dimension of a model. Annals of Statistics  Vol.6, pp.461-464.  [4] J.Rissancn. (1984) Universal Coding, Information, Prediction, and Estimation.  IEEE Trans. on Information. Theory, Vol.30, pp.629-636.  [5] S.Vatanabc. (1993) An Optimization Method of Artificial Neural Networks  based on a Modified Information Criterion. IEICE technical Report Vol. NC93-52,  pp.71-78.  [6] H.Whitc. (1989) Learning in Artificial Neural Networks: A Statistical Perspec-  tive. Neural Computation, Vol.l, pp.425-464.  [7] A.S.Wcigcnd, D.E.Rumclhart, and B.A.Hubcrman. (1991) Generalization of  weight-elinlination with application to forecasting. Advances in Neural Inforrn, ation  Processing Systems, Vol.3, pp.875-882.  PART II  LEARNING THEORY,  GENERALIZATION,  AND COMPLEXITY  
Generalization Error and The Expected  Network Complexity  Chuanyi Ji  Dept. of Elec., Cornpt. and Syst Engr.  Rensselaer Polytechnic Inst, itule  Troy, NY 12180-3590  chuanyi@ecse.rpi.edu  Abstract  For two layer networks with n sigmoidal hidden units, the generalization error is  shown to be bounded by  0(: ) + O( (:C)d  N  where d and N are the input dimension and the number of training samples, re-  spectively. E represents the expectation on random number I( of hidden units  (1 _< X _< n). The proba,bility Pr(I( = k) (1 <_ k <_ n) is dctermined by a prior  distribution of weights, which corresponds to a Gibbs distribtttion of a regularizer.  This relationship makes it possible to characterize explicitly how a regularization  term affects bias/variance of networks. The bound can be obta.ined analytically  for a large c. lass of commonly used priors. It can also be applied to estimate the  expected network complexity E.r in practice. The result provides a quantitative  explanation on how large networks can generalize well.  1 Introduction  Pegularization (or weight-deca.y) methods are widely used in supervised learning by  adding a regularization term to an energy function. Although it is well known that  such a regularization term effectively reduces network complexity by introducing  more bias and less variance[4] to the networks, it is not clear whether and how the  information given by a regularization term can be used alone to characterize the  effective network complexity and how the estimated effective network complexity  relates to the generalization error. This research a. ttempts to provide answers to  these questions for two layer feedforward networks with sigmoidal hidden units.  367  368 Ji  Specifically, the effective network complexity is characterized by the expected  bet of hidden units determined by a Gibbs dist, ribution corresponding to a regulat'-  ization term. The generalization error can then be bounded by the expected network  complexity, and thus be tighter than the original bound given by Barron[2]. The  new bound shows explicitly, through a bigger approximation error and a smaller  estimation error, how a regularization term introduces more bias and less variance  to the networks It therefore provides a quantitative explanation on how a network  larger than necessary can also generalize well under certain conditions, which can  not, be explained by the existing learning theory[9].  For a class of commonly-used regularizers, the expec'ced netxvork complexity can  be obtained in a closed form. It is then used to estimate the expected network  complexity for Gaussion mixture model[6].  2 Background and Previous Results  A relationship has been developed bv Barron[2] between generalization error and  network complexity for two la,yer networks nsed for function approximation. We  will briefly describe this restlit in this section and give onr extension subsequently.  Consider a cla.ss of two layer networks of fixed architecture with n sigmoidal hidden  units and one (linear)output unit. Let .f,,(z; w)= w?)g(w)rz)be a net'work  /:1  fitnction, where w  O is the network weight vector comprising both w? and w?)  for 1 5 l 5 ". w ) and w? are the incoming weights to the/-th hidden unit  the weight from the /-th hidden unit to the output, respectively.   R  is  the weight space for n hidden nnits (and input dimension d). Each sigmoid unit  g(z) is assumed to be oftanh type: g(z)  1 as z   for 1 5 I 5   The input is z  D  Ra. Without loss of generality, D is assumed to be a unit  hypercube in R d, i.e., all the components of x are in [--1, 1].  Let j'(x) be a. target function defined in the same domain D and satisfy some  smoot. hness conditions [2]. Consider N [raining samples independently drawn from  some distribntion It(a:): (z,./(x)), ...,(x2v, f(:t:v)). Define au energy function e,  where e = c + h z'"*'(" L, (w) is a regularization term as a function of w  Ar ,,N  for a fixed ,. A is a constant. c  N  1  fimction such that 'b minimizes the  is a quadratic error function on N training  ))2. Let ./;,,(.v;'&) be the (optimal)network  energy fimction e: b = arg rain . The gert-  eralization error E.q is defined to be the squared L 2 --  norm = f- II --  f(f(x) -- fn,N(X;tb))2d/t(x), vhere  is the expectation over a]] training sets of  D  size Ar drawn from the same distribution. Thus, the generalization error measures  the mean squared distance between the unknoxvn function an,l the best network  function that can be obtained for training sets of size A; The ,,e  ' '   g me ahzgton error  In the previous work by Barron, the sigmoidal hidden units arc, ,,(:)+l  2  show tha, t his results axe applica,ble to the class of .qt(z)'s we consider here.  It is ea.sy to  Generalization Error and the Expected Network Complexity 369  Ea is shown[2] to be bounded as   _< o ( , ) ,  where ]?.,,, called the index of resolvability [2], can be expressed as  R,,N = rain {11/- I1-0+ (2)  wO, N  where .f, is the clipped fn(a:; w) (see [2]). The index of resolvability can be further  bounded as  bounded as  7.([ T'  where O(.3) and D("rtlo9N) are the bonnds for gpproximation error  es[iamtion error (variance), respec[ively.  In addRion, the bound br Eq can be minimized if an additiongl regulariza.tion [erm  L () is used in the energy hmcion to minimize the number of hidden units, i.e.,  50(toT).  R.,,,;v _< O(,)+ O( . Therefore, the generalization en'or is  '- l o 9 N )  (3)  (bias) and  3 Open Questions and Motivations  Two open questions, which can not be answered by the previous result, are of the  primary interest of this work.  1) How do large networks generalize?  Tle large networks refer to those with a ratio [ to be somewhat big, where W  and N are the total number of independently modifiable weights (W  cl, for   large) and the nnmber of training samples, respectively. Netvorks trained witIx  regularization terms ntay fall into this category. Such large networks are found  o }r. a10]r to generalize well sometimes. Ilowever, when ' is big, the bomd in  Equation (3) is too loose to loound the actual generalization error meaningfully.  Therefore. tbr the large networks, the total nnmber of hidden units l may no longer  be a. good estimate fbr netsyork complexity. Efforts have been made to develop  measures on effective network complexity both analytically and empirically[I][5][10].  These measures depend on training data as well as a regularization term in an  implicit way which make it dicult to see direct effects of a regularization term on  generalization error. This naturally leads to our second qnestion.  2) Is it possi101e to characterize network complexity for a el:,,, of networks using  only the information given by a regularization term2? Hmv to relate the estimated  network complexity rigorously with generalization error?  In practice, when a regularization term (L,,,a, (w)) is used to penalize the ma g,itude  of weights, it effectively minimizes the number of hidden units as well even tl,,,tlh an  additional regularization term Lsr0z ) is not used. This is due to the fact th:tt, some  of the hidden nnits may only operate in the linear region of a sigmoid when their  This was posed as an open problem by Solla et.al. [8]  370 Ji  incoming weights are small and inputs are bounded. Therefore, a L,,,v(w) term ca.n  effectively act like a L;v(n) term that reduces the effective number of hidden units,  and thus result in a degenerate parameter space whose degrees of freedom is fewer  than rid. This fact was not taken into consideration in the previous work, and as  shovn later in this work, will lead to a tighter bound on  In vhat follovs, ve will first define the expected network complexity, then use it to  bound the generalization error.  4 The Expected Network Complexity  For reasons that will become apparent, ve choose to define the effective complexity  of a feed forward two layer network as the expected mtm10er of hidden units Elf  (1 _< If <_ ,.) which are effectively nonlinear, i.e. operating outside the central  linear regions of their sigmoid response fnnction g(z). We define the linear region  as an interval [ z [< b with b a positive constant.  Consider the presynaptic input _" = w'ZPz to a hidden unit g(z), where w' is the  incoming weight vector for the unit. Then the unit is considered to be effectively  linear if[ z {< b for all z  D. This will happen if [ z' {< b, where z' = w'Ta ' with  x' being any vertex of the unit hypercube D. This is because {z {_< w'T:, where :  is the vertex of D whose elements are the sgn functions of the elements of w'.  Next, consider network veights as random variables with a distribution p(w) =  Aexp(-L,,,A,(w)), which corresponds to a Gibbs distribntion of a regularization  term with a normalizing consta. ut A. C, ousider the vector :' to 10e a random vector  also with equally probable l's and -l's. Then ] z' [< b will be a random event. The  probability for this hidden unit to be effectively nonlineal' equals to 1 - Pr(I z [< b),  which can be completely determined by the distributions of weights p(w) and z'  (equally pro10able). Let, If 10e the number of hidden units which are effectively  nonlinear. Then the probability, Pt(If = k) (1 _< /c' _< n), can be determined  through a joint probability of k hidden units that are operating beyond the central  linear region of sigmoid functions. The expected network complexity, EK, can then  be obtained through Pt(It = k), which is determined by the Gibbs distribution of  Lz,,, (w). The motivation on ntilizing such a Gibbs distri10ution comes from the fact  that/k,N is independent of training samples but dependent of a regularization term  vhich corresponds to a prior distril0ution of weights. Using such a formulation, as  will be shown later, the effect of a regularization term on bias and variance can be  characterized explicitly.  5 A New Bound for The Generalization Error  To develop a tighter bound For the generalization error, we consider snbspaces of  t. he weights indexed by different munber of effectively nonlinear hidden units: O g  02... g 0,,,. For each O j, there are j out of, hidden units which are effectively  nonlinea. r tbr 1 5 J 5 n. Ihere[oze, the index of resolvability R,,v can be expressed  as  R,r = rain R;,.,,V, (4)  Generalization Error and the Expected Network Complexity 371  where each Rk, rain {[[ f j ][2 ,N(w) ). Next let us consider the number  of effectively nonlinear units to be random. Since the minimum is no bigger than  the average, we hve  ,  E,, ()  where the expectation is taken over the random variable I4 utilizing the probability  Pt(I( = k). For cach I(, however, the two terms in K,N can be bounded as  by the triangle inequality, where f-c, is the actual network function with n -   hidden units operating in the region bounded by the constant b, a,nd fK is the  corresponding network function which treats the , - I units as linea.r nnits. In  ddition, we lmve  L,,O)  O([I .f,-,, - J II ) + O(Zoc. V), ()  where the first term also results from the triangle inequality, and the second term  is obtained by discretizing the degenerate parameter space $z using simHa.r tech-  niques as in ['2] a. Applying Taylor expansion on the term [l f-,,- fK 11 u, we  have  II f,-x, - fc [I u  O(Y=(n- Z)). (8)  Puting Equations (5) (6) (7) and (8) into Equa.ton (]), w, have  1 (EI4)dlogN) + O(b(, - EK)) + o(bC), (9)  z,  o() + o(   where EI is the expected mnber of hidden units which are effectively nonlinear.  If b5 0(%), wehave  6  A Closed Form Expression For a Class of Regularization  Te r 111 s  For commonly used regula. rization terms, how can we actually find the probability  distribution of the number of (nonlinear) hidden units Pt(If = k)? And how shall  we evaluate Elf and E?  As a simple example, we consider a special class of prior distrilmtions for iid weights,  i.e, p(w) = Hp(w), where zvi are the ,lcments of w  , This corresponds to  a. large class of regularization terms which minimize the magnitudes of individual  weights independently[7].  Consider each weight as a random variaMe with zero incan and a. common variance  or. Then for large input dimension d,  -' is a.pproximately normal with zero-mean  3Deta,ils will be given in a longer version of the pa,per in prepa,ra,tion.  372 Ji  and variance cr by the Central Limit Theorem[3]. Let q denote the probability that  a unit is effectively nonlinear. We have  b  q: (11)  where  hidden units are nonlinear.  a/, I( has a binomial distribution  where 1 < k < n. Then  where A =  v 2  '-' dy. Next consider the probability that N out of n  Based on our (independence) assumptions on w' and  (12)  = (1:3)  1 1  --=-+zX, (14)  E N n  + (1 - q)". Then the generalization error E satisfies  1 nqd,   < o(7 + zx) + )  7 Application  As an example for applications of the t.]]eoretical resnlts, the expected netxvork com-  plexity EIf is estimated [br Ga. ussian mixt, ure model used for time-series prediction  (details can be found in [6])4  In general, using only a prior distribntion of weights to estimate the network com-  plexity E/( may lead to a less accurate measure on the effective network complexi/,y  than incorporating information on training data also. However, if parameters of a  regularization term also get optimized during training, as shown in this example,  the resulting Gibbs prior distribution of weights may lead to a good estimate of the  effective number of hidden units.  Specifically, the corresponding Gibbs distribution p(w) of the weights from the  Gaussion mixture is lid, which consists of a linear combination of eight Gaussian  distributions. This function results in a skewed distribution with a sharp peak  around the zero (see [6]). The mean and variance of the presynaptic inputs z to  the hidden units can thus be estimated as 0.02 and 0.04, respectively. The other  parameters used are n = 8, d = 12. b = 0. is chosen. Then q  0.4 is obtained  through Equation (11). The effective netsyork complexity is EIV m 3 (or 4). The  empirical result[10], which estimates the effective number of hidden units using the  dominated eigenvalues at the hidden layer, results in about 3 effective hidden unit. s.  4Strictly spea.]dng, the theoretical results deal ;vith regularization terms with discrete  weights. It, can a,nd has been extended to continuous weights by D.F. McCaffrey and A.R.  Galla,nt. Details are beyond the content of titis paper.  Generalization Error and the Expected Network Complexity 373  4.5  4  3.5  3  2.5  2  1.5  1  0.5  0   0 0.2  variance  increase in bias  0.4 0.6 0.8 1  q  Fignre 1: Illustration of an increase A in bias and variance Bqn, as a function of q.  A scaling actor B = 0.25 is used for the convenience of the plot. n - 20 is chosen.  8 Discussions  Is this new bound for the generalization tighter than the old one which takes no  account of nwork-weight-dependent, information? If so. what does it, tell us?  Compared with the bomd in Equation (3), the new bound results in an increase /X  in approximatiou error (bias), and qn instea. d of r as estimat. ion error (varia,ce).  These two terms are plotted as bract. ions of q in Figure (1). Since q is a hmction of  cr which characterizes how strongly the magnitude of the weights is penalized, the  larger the rr, the less the weights get penalized, the larger the q, the more hidden  units are likely to be effectively nonlinear, thus the smaller the bia.s and larger the  variance. When q = 1, all the hidden units are effectively nonlinear and the new  bound reduces to the old one. This shows how a regularization term directly affects  bias/variance.  When the estimation error dominates, the bound for the generalization error will be  proportional to 'nq inst, ead of n,. The valne of nq, however, depends on the choice of  rr. For sinall or, the new bound can be nmch tighter than the old one, especially for  large mtworks with ,. large but nq small. This will provide a practical method to  eatingate generalization error for large networks as well as an explanation of when  and why large networks can generalize well.  l tow tight the bonnd really is depends on how well L,,,x (w) is chosen. Let. nc denote  the optimal number of (nonlinear) hidden units needed to approximate .f(.). If  L,,,,(w) is chosen so that the corresponding l>(w) is almost a delta function at n0,  then ERzc,x' m/il,.o,,V, xvhich gives a very tight bound. Otherwise, if, for instance,  374 Ji  L,,N(W) penalizes network complexity so little that Et{K,N  ln,N, the bound  will be as loose as the original one. It should also be noted that an exact value for  the bound cannot be obtained unless some information on the unknown function f  itself is available.  For commonly used regularization terms, the expected network complexity can be  estimated through a close form expression. Such expected network complexity is  shown to be a good estimate for the actual network complexity if a Gibbs prior  distribution of weights also gets optimized through training, and is also sharply  peaked. More research will be done to evaluate the applicability of the theoretical  results.  Acknowledgement  The support of National Science Foundation is gratefully acknowledged.  References  [1] S. Amari and N. Murata, "Statistical Theory of Learning Curves under En-  tropic Loss Criterion," Neural Computalion, 5, 140-153, 1993.  [2] A. Barron, "Approximation and Estimation Bounds for Artificial Neural Net-  works," Proc. of The 4th Workshop on Computalional Learning Theory, 243-  249, 1991.  [3] W. Feller, An Introduction to Probability Theory and Its Applications, New  york: John Wiley and Sons, 1968.  [4] S. Geman, E. Bienenstock, and R. Doursat, "Neural Networks and the  Bias/Variance Dilemma," Neural Compttation, 4, 1-58, 1992.  [5] J. Moody, "Generalization, Weight Decay, and Architecture Selection for Non-  linear Learning Systems," Proc. of Neural Information Processing Systems,  1991.  [6] S.J. Nowlan, and G.E. Hinton, "Simplifying Neural Networks by Soft Weight  Sha.ring," Neural computation, 4, 473-493(1992).  [7] R. Reed, "Pruning Algorithms-A Survey," I Trans. Neural Networks Vol.  4,740-747, (1993).  [8] S. Solla, "The Emergence of Generalization Ability in Learning," Presented at  NIPS02.  [9] V. Va. pnik, "Estimation of Dependences Based on Empirical Data," Springer-  Vcrlag, New york, 1982.  [10] A.S . Weigend and D.E . Rumelhart, "The Effective Dimension of the Space of  Hidden Units," Proc. of International oint Conference on Neural Networks,  1992.  
GDS: Gradient Descent Generation of  Symbolic Classification Rules  Reinhard Blasig  Kaiserslautern University, Germany  Present address: Siemens AG, ZFE ST SN 41  81730 M/inchen, Germany  Abstract  Imagine you have designed a neural network that successfully learns  a complex classification task. What are the relevant input features  the classifier relies on and how are these features combined to pro-  duce the classification decisions? There are applications where a  deeper insight into the structure of an adaptive system and thus  into the underlying classification problem may well be as important  as the system's performance characteristics, e.g. in economics or  medicine. GDS  is a backpropagation-based training scheme that  produces networks transformable into an equivalent and concise  set of IF-THEN rules. This is achieved by imposing penalty terms  on the network parameters that adapt the network to the expressive  power of this class of rules. Thus during training we simultaneously  minimize classification and transformation error. Some real-world  tasks demonstrate the viability of our approach.  I Introduction  This paper deals with backpropagation networks trained to perform a classification  task on Boolean or real-valued data. Given such a classification task in most cases  it is not too difficult to devise a network architecture that is capable of learning  the input-output relation as represented by a number of training examples. Once  training is finished one has a black box which often does a quite good job not   Gradient Descent Symbolic Rule Generation  1093  1094 Blasig  only on the training patterns but also on some previously unseen test patterns. A  good generalization performance indicates that the network has grasped part of the  structure inherent in the classification task. The net has figured out which input  features are relevant to make a classification decision and which are not. It has also  modelled the way the relevant features have to be combined in order to produce the  classifying output. In many applications it is important to get an understanding of  this information hidden inside the neural network. Not only does this help to create  or verify a domain theory, the analysis of this information may also serve human  experts to determine, when and in what way the classifier will fail.  In order to explicate the network's implicit information, we transform it into a  set of rules. This idea is not new, cf. (Saito and Nakano, 1988), (Bochereau and  Bourgine, 1990), (Y. Hayashi, 1991) and (Towell and Shavlik, 1992). In contrast to  these approaches, which extract rules after BP-training is finished, we apply penalty  terms during training to adapt the network's expressive power to that of the rules  we want to generate. Consequently the net will be transformable into an equivalent  set of rules.  Due to their good comprehensibility we restrict the rules to be of the form IF  < premise > THEN < conclusion >, where the premise as well as the conclusion  are Boolean expressions. To actually make the transformation two problems have  to be solved:   Neural nets are well known for their distributed representation of informa-  tion; so in order to transform a net into a concise and comprehensible rule  set one has to find a way of condensing this information without substan-  tially changing it.   In the case of backpropagation networks a continuous activation function  determines a node's output depending on its activation. However, the dy-  namic of this function has no counterpart in the context of rule-based de-  scriptions.  We address these problems by introducing a penalty function Ep, which we add to  the classification error E yielding the total backpropagation error  ET = ED + A  Ep. (1)  2 The Penalty Term  The term Ep is intended to have two effects on the network weights. First, by  a weight decay component it aims at reducing network complexity by pushing a  (hopefully large) fraction of the weights to 0. The smaller the net, the more concise  the rules describing its behavior will be. As a positive side effect, this component will  tend to act as a form of "Occam's razor": simple networks are more likely to exhibit  good generalization than complex ones. Secondly, the penalty term should minimize  the error caused by transforming the network into a set of rules. Adopting the  common approach that each non-input neuron represents one rule, there would be  no transformation error if the neurons' activation function were threshold functions;  the Boolean node output would then indicate, whether the conclusion is drawn or  not. But since backpropagation neurons use continuous activation functions like  GDS: Gradient Descent Generation of Symbolic Classification Rules 1095  y = tanh(x) to transform their activation value x into the output value y, we are  left with the difficulty of interpreting the continuous output of a neuron. Thus our  penalty term will be designed to produce a high penalty for those neurons of the  backpropagation net, whose behavior cannot be well approximated by threshold  neurons, because their activation values are likely to fall into the nonsaturated  region of the tanh-function .  Figure 1: We regard Ixl > a with lyl = I tanh(x)l > 0.9 as the regions, where a  sigmoidal neuron can be approximated by a threshold neuron. The nonsaturated  region is marked by the dashed box.  For a better understanding of our penalty term one has to be aware of the fact  that IF-THEN rules with a Boolean premise and conclusion are essentially Boolean  functions. It can easily be shown that any such function can be calculated by a  network of threshold neurons provided there is one (sufficiently large) hidden layer.  This is still true if we restrict connection weights to the values {-1, 0, 1} and node  thresholds to be integers (Hertz, Krogh and Palmer, 1991). In order to transfer this  scenario to nets with sigmoidal activation functions and having in mind that the  activation values of the sigmoidal neurons should always exceed :1:3 (see figure 1),  we require the nodes' biases to be odd multiples of :1:3 and the weights wii to obey  oj e {-6, 0, 6}. (2)  We shortly comment on the practical problem that sometimes bias values as large  as 4'6mi (mi being the fan-in of node i) may be necessary to implement certain  Boolean functions. This may slow down or even block the learning process. A simple  solution to this problem is to use some additional input units with a constant output  of +1. If the connections to these units are also subject to the penalty function Ep,  it is sufficient to restrict the bias values to  bl e {-3, 3}. (3)  2We have to point out that the conversion of sigmoidal neurons to threshold neurons  will reduce the net's computational power: there are Boolean functions which can be  computed by a net of sigmoidal neurons, but which exceed the capacity of a threshold  net of the same topology (Maass, Schnitger and Sontag, 1991). Note that the objective  to use threshold units is a consequence of the decision to search for rules of the type IF  < premise > THEN < conclusion >. A failure of the net to simultaneously minimize  both parts of the error measure may indicate that other rule types are more adequate to  handle the given classification task.  1096 Blasig  Now we can define penalty functions that push the biases and weights to the desired  values. Obviously E, (the bias penalty) and Ew (the weight penalty) have to be  different:  Eb(bi) = 13-[b,[I (4)  Ew(wji) = { 16-lwj11 for Iwil _> o  Iwagl for Iwagl < O  (5)  The parameter O determines whether a weight should be subject to decay or pushed  to attain the value 6 (or -6 respectively). Figure 2 displays the graphs of the penalty  functions.  -3-.0 I .0  -6.0 -0] 0 w 6.0"-  Figure 2: The penalty functions Eb and E.  The value of O is chosen with the objective that only those weights should exceed  this value, which almost certainly have to be nonzero to solve the given classification  task. Since we initialize the network with weights uniformly distributed in the  interval [-0.5, 0.5], O = 1.5 works well at the beginning of the training process. The  penalty term then has the effect of a pure weight decay. When learning proceeds  and the weights converge, we can slowly reduce the value of O, because superfluous  weights will already have decayed. So after each sequence of 100 training patterns,  say, we decrease O by a factor of 0.995.  Observation shows that weights which once exceeded the value of O quickly reach  6 or -6 and that there are relatively few cases where a large weight is reduced  again to a value smaller than O. Accordingly, the number of weights in {-6, 6}  successively grows in the course of learning, and the criterion to stop training thus  influences the number of nonzero weights.  The end of training is determined by means of cross validation. However, we do  not examine the cross validation performance of the trained net, but that of the  corresponding rule set. This is accomplished by calculating the performance of the  original net with all weights and biases replaced by their optimal values according  to (2) and (3).  The weighting factor A of the penalty term (see equation 1) is critical for good  learning performance. We pursued the strategy to start learning with A = 0, so  that the network parameters first move into a region where the classification error  is small. If this error falls below a prespecified tolerance level L, A is incremented  by 0.001. The factor A goes down by the same amount, when the error grows larger  GDS: Gradient Descent Generation of Symbolic Classification Rules 1097  than L a. By adjusting the weighting factor every 100 training patterns we keep the  classification error close to the tolerance level. The choice of L of course depends on  the learning task. As a heuristic, L should be slightly larger than the classification  error attainable by a non-penalized network.  3 Splice-Junction Recognition  The DNA, carrying the genetic information of biological cells, can be thought to  be composed of two types of subsequences: exons and introns. The task is to  classify each DNA position as either an exon-to-intron transition (EI), an intron-  to-exon transition (IE) or neither (N). The only information available is a sequence  of 30 nucleotides (A, C, G or T) before and 30 nucleotides after the position to be  classified. Splice-junction recognition is a classification task that has already been  investigated by a number of machine learning researchers using various adaptive  models.  The pattern reservoir contains about 3200 DNA samples, 30% of which were used for  training, 10% for cross-validation and 60% for testing. Since we used a grandmother-  cell coding for the input DNA sequence, the network has an input layer of 4*60  neurons. With a hidden layer of 20 neurons q and two output units for the classes EI  and IE, this amounts to about 5000 free parameters. The following table compares  the classification performance of our penalty term approach and other machine  learning algorithms, cf. (Murphy and Aha, 1992).  Table 1: Splice-junction recognition: error (in percent) of various machine learning  algorithms  algorithm N EI IE total  KBANN 4.62 7.56 8.47 6.32  GDS 6.71 4.43 9.24 6.75  Backprop 5.29 5.74 10.75 6.77  Perceptron 3.99 16.32 17.41 10.43  ID3 8.84 10.58 13.99 10.56  Nearest Neighbor 31.11 11.65 9.09 20.74  Surprisingly, the GDS network turned out to be very small. The weight decay  component of our penalty term managed to push all but 61 weights to zero, making  use of only three hidden neurons. Thus in addition to performing very well, the  network is transformable into a concise rule set, as followsS:  3Negative A-values are not allowed.  4A reasonable size, considering the experiments described in (Shavlik et al., 1991)  We adopt a notation commonly used in this domain: (n denotes the position of the  first nucleotide in the given sequence being left (negative n) or right (positive n) to the  point to be classified. Nucleotide 'Y' stands for ('C' or 'T'), 'X' is any of {A, C', G, T).  Consequently, e.g. neuron hidden(2) is active iff at least four of the five nucleotides of  the sequence 'GTAXG' are identical to the input pattern at positions I to 5 right of the  possible splice junction.  1098 Blasig  hidden(2): at least 4 nucleotides match sequence $1: 'GTAXG'  hidden(ll): at least $ nucleotides match sequence $-$:  hidden(17): at least 1 nucleotides matches sequence $-:  class El: hidden(2) AND hidden(11)  class IE: NOT(hidden(2)) AND hidden(17)  4 Prediction of Interest Rates  This is an application, where the network input is a vector of real numbers. Since  our approach can only handle binary input, we supplement the net with a dis-  cretization layer that provides a thermometer code representation (Hancock 1988)  of the continuous valued input. In contrast to pure Boolean learning algorithms  (Goodman, Miller and Smyth, 1989), (Mezard and Nadal, 1989), which can also be  endowed with discretization facilities, here the discretization process is fully inte-  grated into the learning scheme, as the discretization intervals will be adapted by  the backpropagation algorithm.  The data comprises a total of 226 patterns, which we distribute randomly on three  sets: training set (60%), cross-validation set (20%) and test set (20%). The input  represents the monthly development of 14 economic time series during the last 19  years. The Boolean target indicates, whether the interest rates will go up or down  during the six months succeeding the reference month 6. The time series include  among others month of the year, income of private households or the amount of  German foreign investments. For some time series it is useful not to take the raw  feature measurements as input, but the difference between two succeeding measure-  ments; this is advantageous if the underlying time series show only small changes  relative to their absolute values. All series were normalized to have values in the  range from - 1 to + 1.  We used a network containing a discretization layer of two neurons per input di-  mension. So there are 28 discretization neurons, which are fully connected to the  10 hidden nodes. The output layer consists of a single neuron. Since our data set  is relatively small, the intention to obtain simple rules is not only motivated by the  objective of comprehensibility, but also by the notion that we cannot expect a large  rule set to be justified by a small amount of training data. In fact, during training  90% of the weights were set to zero and three hidden units proved to be sufficient for  this task. Nevertheless the prediction error on the test set could be reduced to 25%.  This compares to an error rate of about 20% attainable by a standard backprop-  agation network with one hidden layer of ten neurons and no input discretization.  We thus sacrificed 5% of prediction performance to yield a very compact net, that  can be easily transformed into a set of rules. Some of the generated rules are shown  below. The first rule e.g. states that interest rates will rise if private income in-  creases AND foreign investments decrease by a certain amount during the reference  month.  If the rules produce contradicting predictions for a given input, the final decision  will be made according to a majority vote. A tie is broken by the bias value of the  I.e. the month where the input data has been measured.  GDS: Gradient Descent Generation of Symbolic Classification Rules 1099  output unit, which states that by default interest rates will rise.  IF (at least 2 of ( increase of private income < 0.75,  decrease of foreign investments < 64 MIO DM })  THEN (interest rates will rise)  ELSE (interest rates will fall).  IF (at least $ of {  THEN  ELSE  increase of business climate estimate < 1.75,  treasury bonds yields (1! month ago) >  treasury bonds yields (12 month ago) > 8.2,  increase of foreign investments < 60 MIO DM  (interest rates will fall)  (interest rates will rise).  5 Conclusion and Future Work  GDS is a learning algorithm that utilizes a penalty term in order to prepare a  backpropagation network for rule extraction. The term is designed to have two  effects on the network's weights:   By a weight decay component, the number of nonzero weights is reduced:  thus we get a net that can hopefully be transformed into a concise and  comprehensible rule set.   The penalty term encourages weight constellations that keep the node ac-  tivations out of the nonsaturated part of the activation function. This  is motivated by the fact that rules of the type IF < premise > THEN  < conclusion > can only mimic the behavior of threshold units.  The important point is that our penalty function adapts the net to the expressive  power of the type of rules we wish to obtain. Consequently, we are able to transform  the network into an equivalent rule set. The applicability of GDS was demonstrated  on two tasks: splice-junction recognition and the prediction of German interest  rates. In both cases the generated rules not only showed a generalization perfor-  mance close to or even superior to what can be attained by other machine learning  approaches such as MLPs or ID3. The rules also prove to be very concise and  comprehensible. This is even more remarkable, since both applications represent  real-world tasks with a large number of inputs.  Clearly the applied penalty terms impose severe restrictions on the network param-  eters: besides minimizing the number of nonzero weights, the weights are restricted  to a small set of distinct values. Last but not least, the simplification of sigmoidal to  threshold units also affects the net's computational power. There are applications,  where such a strong bias may negatively influence the net's learning capabilities.  Furthermore our current approach is only applicable to tasks with binary target  patterns. These limitations can be overcome by dealing with more general rules  than those of the Boolean IF-THEN type. Future work will go into this direction.  1100 Blasig  Acknowledgement s  I wish to thank Hans-Georg Zimmermann and Ferdinand Hergert for many useful  discussions and for providing the data on interest rates, and Patrick Murphy and  David Aha for providing the UCI Repository of ML databases. This work was  supported by a grant of the Siemens AG, Munich.  References  L. Bochereau, P. Bourgine. (1990) Extraction of Semantic Features and Logical  Rules from a Multilayer Neural Network. Proceedings of the 1990 IJCNN- Wash-  ington DC, Vol.II 579-582.  R.M. Goodman, J.W. Miller, P. Smyth. (1989) An Information Theoretic Approach  to Rule-Based Connectionist Expert Systems. Advances in Neural Information  Processing Systems 1, 256-263. San Mateo, CA: Morgan Kaufmann.  P.J.B. Hancock. (1988) Data Representation in Neural Nets: an Empirical Study.  Proc. Connectionist Summer School.  Y. Hayashi. (1991) A Neural Expert System with Automated Extraction of Fuzzy  If-Then Rules and its Application to Medical Diagnosis. Advances in Neural Infor-  mation Processing Systems 3, 578-584. San Mateo, CA: Morgan Kaufmann.  J. Hertz, A. Krogh, R.G. Palmer. (1991) Introduction to the Theory of Neural  Computation. Addison-Wesley.  C.M. Higgins, R.M. Goodman. (1991) Incremental Learning with Rule-Based Neu-  ral Networks. Proceedings of the 1991 IEEE INNS International Joint Conference  on Neural Networks- Seattle, Vol.I 875-880.  M. Mezard, J.-P. Nadal. (1989) Learning in Feedforward Layered Networks: The  Tiling Algorithm. J. Phys. A: Math. Gen. 22, 2191-2203.  W. Maass, G. Schnitger, E.D. Sontag. (1991) On the Computational Power of  Sigmoids versus Boolean Threshold Circuits. Proceedings of the 3nd Annual IEEE  Symposium on Foundations of Computer Science, 767-776.  P.M. Murphy, D.W. Aha. (1992). UCI Repository of machine learning databases  [ftp-site: ics.uci.edu: pub/machine-learning-databases]. Irvine, CA: University of  California, Department of Information and Computer Science.  J.R. Quinlan. (1986) Induction of Decision Trees. Machine Learning, 1: 81-106.  K. Siato, R. Nakano. (1988) Medical diagnostic expert systems based on PDP  model. Proc. IEEE International Conference on Neural Networks Vol. I 255-262.  V. Tresp, J. Hollatz, S. Ahmad. (1993) Network Structuring and Training Using  Rule-Based Knowledge. Advances in Neural Information Processing Systems 5,  871-878. San Mateo, CA: Morgan Kaufman.  G.G. Towell, J.W. Shavlik. (1991) Training Knowledge-Based Neural Networks  to Recognize Genes in DNA Sequences. In: Lippmann, Moody, Touretzky (eds.),  Advances in Neural Information Processing Systems 3, 530-536. San Mateo, CA:  Morgan Kaufmann.  
II  Correlation Functions in a Large  Stochastic Neural Network  Iris Ginzburg  School of Physics and Astronomy  Raymond and Beverly Sackler Faculty of Exact Sciences  Tel-Aviv University  Tel-Aviv 69978, Israel  Haim Sompolinsky  Racah Institute of Physics and Center for Neural Computation  Hebrew University  Jerusalem 91904, Israel  Abstract  Most theoretical investigations of large recurrent networks focus on  the properties of the macroscopic order parameters such as popu-  lation averaged activities or average overlaps with memories. How-  ever, the statistics of the fluctuations in the local activities may  be an important testing ground for comparison between models  and observed cortical dynamics. We evaluated the neuronal cor-  relation functions in a stochastic network comprising of excitatory  and inhibitory populations. We show that when the network is in  a stationary state, the cross-correlations are relatively weak, i.e.,  their amplitude relative to that of the auto-correlations are of or-  der of l/N, N being the size of the interacting population. This  holds except in the neighborhoods of bifurcations to nonstationary  states. As a bifurcation point is approached the amplitude of the  cross-correlations grows and becomes of order 1 and the decay time-  constant diverges. This behavior is analogous to the phenomenon  of critical slowing down in systems at thermal equilibrium near a  critical point. Near a Hopf bifurcation the cross-correlations ex-  hibit damped oscillations.  471  472 Ginzburg and Sompolinsky  1 INTRODUCTION  In recent years there has been a growing interest in the study of cross-correlations  between the activities of pairs of neurons in the cortex. In many cases the cross-  correlations between the activities of cortical neurons are approximately symmetric  about zero time delay. These have been taken as an indication of the presence of  "functional connectivity" between the correlated neurons (Fetz, Toyama and Smith  1991, Abeles 1991). However, a quantitative comparison between the observed  cross-correlations and those expected to exist between neurons that are part of a  large assembly of interacting population has been lacking.  Most of the theoretical studies of recurrent neural network models consider only time  averaged firing rates, which are usually given as solutions of mean-field equations.  They do not account for the fluctuations about these averages, the study of which  requires going beyond the mean-field approximations. In this work we perform a  theoretical study of the fluctuations in the neuronal activities and their correlations,  in a large stochastic network of excitatory and inhibitory neurons. Depending on the  model parameters, this system can exhibit coherent undamped oscillations. Here we  focus on parameter regimes where the system is in a statistically stationary state,  which is more appropriate for modeling non oscillatory neuronal activity in cortex.  Our results for the magnitudes and the time-dependence of the correlation functions  can provide a basis for comparison with physiological data on neuronal correlation  functions.  2 THE NEURAL NETWORK MODEL  We study the correlations in the activities of neurons in a fully connected recurrent  network consisting of excitatory and inhibitory populations. The excitatory con-  nections between all pairs of excitatory neurons are assumed to be equal to J/N  where N denotes the number of excitatory neurons in the network. The excitatory  connections from each of the excitatory neurons to each of the inhibitory neurons  are Jt/N. The inhibitory coupling of each of the inhibitory neurons onto each of  the excitatory neurons is K/M where M denotes the number of inhibitory neurons.  Finally, the inhibitory connections between pairs of inhibitory neurons are Kt/M.  The values of these parameters are in units of the amplitude of the local noise (see  below). Each neuron has two possible states, denoted by si = :kl and eri = :kl  for the i-th excitatory and inhibitory neurons, respectively. The value -1 denotes  a quiet state. The value +1 denotes an active state that corresponds to a state  with high firing rate. The neurons are assumed to be exposed to local noise result-  ing in stochastic dynamics of their states. This dynamics is specified by transition  probabilities between the -1 and +1 states that are sigmoidal functions of their  local fields. The local fields of the i-th excitatory neuron, El and the i-th inhibitory  neuron, Ii, at time t, are  E,(t) = Js(t) - - o (1)  = j' s(t) - ' - o (2)  Correlation Functions in a Large Stochastic Neural Network 473  where 0 represents the local threshold and s and cr are the population-averaged  activities s(t) = 1/N-]j sj(t), and o.(t) = 1/M-]d o.j(t) of the excitatory and  inhibitory neurons, respectively.  3 AVERAGE FIRING RATES  The macroscopic state of the network is characterized by the dynamics of s(t)  and o.(t). To leading order in 1IN and l/M, they obey the following well known  equations  ds  = -s + tanh (Js - Ker - 0) (3)  ro dt  do'  = -o. + tanh (Jt s- Ktcr-0) (4)  ro dt  where r0 is the microscopic time constant of the system. Equations of this form for  the two population dynamics have been studied extensively by Wilson and Cowan  (Wilson and Cowan 1972) and others (Schuster and Wagner 1990, Grannan, Kle-  infeld and Sompolinsky 1992)  Depending on the various parameters the stable solutions of these equations are  either fixed-points or limit cycles. The fixed-point solutions represent a stationary  state of the network in which the population-averaged activities are almost constant  in time. The limit-cycle solutions represent nonstationary states in which there  is a coherent oscillatory activity. Obviously in the latter case there are strong  oscillatory correlations among the neurons. Here we focus on the fixed-point case.  It is described by the following equations  so = tanh (Jso - Kero - O) (5)  o.0 = tanh (J'so - K' ero - O) (6)  where so and o.0 are the fixed-point values of s and er. Our aim is to estimate the  magnitude of the correlations between the temporal fluctuations in the activities of  neurons in this statistically stationary state.  4 CORRELATION FUNCTIONS  There are two types of auto-correlation functions, for the two different populations.  For the excitatory neurons we define the auto-correlations as:  (7)  where 5s(t) = s(t)-so and < ... >t means average over time t. A similar definition  holds for the auto-correlations of the inhibitory neurons. In our network there are  three different cross-correlations: excitatory-excitatory, inhibitory- inhibitory, and  inhibitory-excitatory. The excitatory-excitatory correlations are  = + . (8)  Similar definitions hold for the other functions.  474 Ginzburg and Sompolinsky  We have evaluated these correlation functions by solving the equations for the cor-  relations of 5si(t) in the limit of large N and M. We find the following forms for  the correlations:  1 3  Cii(r) , (1 - s)exp(-Alr) q-  Ealexp(--Als') (9)  =1  3  Cij(r)  E bl exp(-Alr) (10)  I=1  The coefficients at and bl are in general of order 1. The three Ai represent three  inverse time-constants in our system, where Re(A1) >_ Re(A2) ) Re(As). The first  inverse time constant equals simply to Ax = 1/to, and corresponds to a purely  local mode of fluctuations. The values of A and As depend on the parameters of  the system. They represent two collective modes of fluctuations that are coherent  across the populations. An important outcome of our analysis is that A and As  are exactly the eigenvalues of the stability matrix obtained by linearizing Eqs. (3)  and (4) about the fixed-point Eqs. (5) and (6)  The above equations imply two differences between the autocorrelations and the  cross-correlations. First, Cii are of order 1 where in general Cq is of O(1/N).  Secondly, the time-dependence of Cii is dominated by the local, ft time constant  r0, where Cq may be dominated by the slower, collective time-constants.  The conclusion that the cross-correlations are small relative to the autocorrelations  might break down if the coefficients btake anomalously large values. To check these  possibility we have studied in detail the behavior of the correlations near bifurcation  points, at which the fixed point solutions become unstable. For concreteness we will  discuss here the ce of Hopf bifurcations. (Similar results hold for other bifurcations   well). Near a Hopf bifurction A2 nd Aa can be written  A  e  iw,  where e > 0 and vanishes at the bifurcation point. In this parameter regime, the  I Similar results hold for a and as. Thus,  amplitudes bx << b, ba and b2  ba  7'  near the bifurcation, we have  C,,(v)  (1 - s)exp(-v/wo)cos(wv) (11)  B exp(-er)cos(wr) (12)  Note that near a bifurcation point e is linear in the difference between any of the  parameters and their value at the bifurcation. The above expressions hold for  e << 1 but large compared to 1/N.When e _ 1IN the cross-correlation becomes of  order 1, and remains so throughout the bifurcation.  Figures 1 and 2 summarize the results of Eqs. (9) and (10) near the Hopf  bifurcation point at J, J, K, K , 8 - 225, 65, 161, 422, 2.4. The population sizes  are N = 10000, M = 1000. We have chosen a parameter range so that the fixed  point values of so and r0 will represent a state with low firing rate resembling  the spontaneous activity levels in the cortex. For the above parameters the rates  relative to the saturation rates are 0.01 and 0.03 for the excitatory and inhibitory  populations respectively.  Correlation Functions in a Large Stochastic Neural Network 475  0.45 -   04  035 "-.  0 .:3  025 "-.  02  015  O  005  0  - -'1- I I 1, I I I  180 185 190 195 200 205 210 215 220 225  J  FIGURE 1. The equal-time cross-correlations between a pair of excitatory neu-  rons, and the real part of its inverse time-constant,e, vs. the excitatory coupling  parameter J.  The values of Cij(O) and of the real-part of the inverse-time constants of C'ij are  plotted (Fig. 1) as a function of the parameter J holding the rest of the parameters  fixed at their values at the bifurcation point. Thus in this case e a(225 - J). The  Figure shows the growth of  the bifurcation point is approached.  0,15 ,  0.1  0.05  0  -0.05  o0.1  -0.15  0 5 1 0 15 20 25 30 35 40 45 50  delay  The time-dependence of the cross-correlations near the bifurcation (J = 215) is  shown in Fig. 2. Time is plotted in units of r0. The pronounced damped oscillations  are, according to our theory, characteristic of the behavior of the correlations near  but below a Hopf bifurcation.  476 Ginzburg and Sompolinsky  5 CONCLUSION  Most theoretical investigations of large recurrent networks focus on the properties of  the macroscopic order parameters such as population averaged activity or average  overlap with memories. However, the statistics of the fluctuations in the activities  may be an important testing ground for comparison between models and observed  cortical dynamics. We have studied the properties of the correlation functions in a  stochastic network comprising of excitatory and inhibitory populations. We have  shown that the cross-correlations are relatively weak in stationary states, except in  the neighborhoods of bifurcations to nonstationary states. The growth of the am-  plitude of these correlations is coupled to a growth in the correlation time-constant.  This divergence of the correlation time is analogous to the phenomenon of critical  slowing down in systems at thermal equilibrium near a critical point. Our analysis  can be extended to stochastic networks consisting of a small number of interacting  homogeneous populations.  Detailed comparison between the model's results and experimental values of auto-  and cross- correlograms of extracellularly measured spike trains in the neocortex  have been carried out (Abeles, Ginzburg and Sompolinsky). The tentative con-  clusion of this study is that the magnitude of the observed correlations and their  time-dependence are inconsistent with the expected ones for a system in a sta-  tionary state. They therefore indicate that cortical neuronal assemblies are in a  nonstationary (but aperiodic) dynamic state.  Acknowledgements: We thank M. Abeles for most helpful discussions. This work  is partially supported by the USA-Israel Binational Science Foundation.  REFERENCES  Abeles M., 1991. Corticonics: Neural Circuits of the Cerebral Cortex. Cambridge  University Press.  Abeles M., Ginzburg I. & Sompolinsky H. Neuronal Cross-Correlations and Orga-  nized Dynamics in the Neocortex. to appear  Fetz E., Toyama K. & Smith W., 1991. Synaptic Interactions Between Cortical  Neurons. Cerebral Cortex, edited by A. Peters & G. Jones Plenum Press,NY. Vol  9. 1-43.  Grannan E., Kleinfeld D. & Sompolinsky H., 1992. Stimulus Dependent Synchro-  nization of Neuronal Assemblies. Neural Computation 4,550-559.  Schuster H. G.  Wagner P., 1990. Biol. Cybern. 64, 77.  Wilson H. R.  Cowan J. D., 1972. Excitatory and Inhibitory Interactions in  Localized Populations of Model Neurons. Biophy. J. 12, 1-23.  
A Unified Gradient-Descent/Clustering  Architecture for  Finite State Machine Induction  Sreerupa Das and Michael C. Mozer  Department of Computer Science  University of Colorado  Boulder, CO 80309-0430  Abstract  Although recurrent neural nets have been moderately successful  in learning to emulate finite-state machines (FSMs), the continu-  ous internal state dynamics of a neural net are not well matched  to the discrete behavior of an FSM. We describe an architecture,  called )oLc, that allows discrete states to evolve in a net as learn-  ing progresses. OOLCr, consists of a standard recurrent neural net  trained by gradient descent and an adaptive clustering technique  that quantizes the state space. DOLCE is based on the assumption  that a finite set of discrete internal states is required for the task,  and that the actual network state belongs to this set but has been  corrupted by noise due to inaccuracy in the weights. DOLCe, learns  to recover the discrete state with maximum a posterJori probabil-  ity from the noisy state. Simulations show that oo,c, leads to a  significant improvement in generalization performance over earlier  neural net approaches to FSM induction.  1 INTRODUCTION  Researchers often try to understand--post hoc--representations that emerge in the  hidden layers of a neural net following training. Interpretation is difficult because  these representations are typically highly distributed and continuous. By "contin-  uous," we mean that if one constructed a scatterplot over the hidden unit activity  space of patterns obtained in response to various inputs, examination at any scale  would reveal the patterns to be broadly distributed over the space.  Continuous representations aren't always appropriate. Many task domains seem to  require discrete representations--representations selected from a finite set of alter-  natives. If a neural net learned a discrete representation, the scatterplot over hidden  activity space would show points to be superimposed at fine scales of analysis. Some  19  20 Das and Mozer  examples of domains in which discrete representations might be desirable include:  finite-state machine emulation, data compression, language and higher cognition  (involving discrete symbol processing), and categorization in the context of decision  making. In such domains, standard neural net learning procedures, which have  a propensity to produce continuous representations, may not be appropriate. The  work we report here involves designing an inductive bias into the learning procedure  in order to encourage the formation of discrete internal representations.  In the recent years, various approaches have been explored for learning discrete  representations using neural networks (McMillan, Mozer, & Smolensky, 1992; Mozer  & Bachtach, 1990; Mozer & Das, 1993; Schiitze, 1993; Towell & Shavlik, 1992).  However, these approaches are domain specific, making strong assumptions about  the nature of the task. In our work, we describe a general methodology that makes  no assumption about the domain to which it is applied, beyond the fact that discrete  representations are desireable.  2 FINITE STATE MACHINE INDUCTION  We illustrate the methodology using the domain of finite-state machine (FSM)  induction. An FSM defines a class of symbol strings. For example, the class (10)*  consists of all strings with one or more repetitions of 10; 101010 is a positive example  of the class, 111 is a negative example. An FSM consists principally of a finite set  of states and a function that maps the current state and the current symbol of the  string into a new state. Gertain states of the FSM are designated "accept"states,  meaning that if the FSM ends up in these states, the string is a member of the  class. The induction problem is to infer an FSM that parsimoniously characterizes  the positive and negative exemplars, and hence characterizes the underlying class.  A generic recurrent net architecture that could be used for FSM emulation and  induction is shown on the left side of Figure 1. A string is presented to the input  layer of the net, one symbol at a time. Following the end of the string, the net  should output whether or not the string is a member of the class. The hidden unit  activity pattern at any point during presentation of a string corresponds to the  internal state of an FSM.  Such a net, trained by a gradient descent procedure, is able to learn to perform this  or related tasks (Elman, 1990; Giles et al., 1992; Pollack, 1991; Servan-Schreiber,  Cleeremans, & McClelland, 1991; Watrous & Kuhn, 1992). Although these models  have been relatively successful in learning to emulate FSMs, the continuous internal  state dynamics of a neural net are not well matched to the discrete behavior ofFSMs.  Roughly, regions of hidden unit activity space can be identified with states in an  FSM, but because the activities are continuous, one often observes the network  drifting from one state to another. This occurs especially with input strings longer  than those on which the network was trained.  To achieve more robust dynamics, one might consider quantizing the hidden state.  Two approaches to quantization have been explored previously. In the first, a net  is trained in the manner described above. After training, the hidden state space is  partitioned into disjoint regions and each hidden activity pattern is then discretized  by mapping it to the center of its corresponding region (Das & Das, 1991; Giles  A Unified Gradient-Descent/Clustering Architecture for Finite State Machine Induction 21  Figure 1: On the left is a generic recurrent architecture that could be used for FSM induc-  tion. Each box corresponds to a layer of units, and arrows depict complete connectivity  between layers. At each time step, a new symbol is presented on the input and the input  and hidden representations are integrated to form a new hidden representation. On the  right is the general architecture of x)o,cv,.  et al., 1992). In a second approach, quantization is enforced durin9 training by  mapping the the hidden state at each time step to the nearest corner of a [0, 1] '  hypercube (Zeng, Goodman, & Smyth, 1993).  Each of these approaches has its limitations. In the first approach, because learning  does not consider the latter quantization, the hidden activity patterns that result  from learning may not lie in natural clusters. Consequently, the quantization step  may not group together activity patterns that correspond to the same state. In the  second approach, the quantization process causes the error surface to have discon-  tinuities and to be flat in local neighborhoods of the weight space. Hence, gradient  descent learning algorithms cannot be used; instead, even more heuristic approaches  are required. To overcome the limitations of these approaches, we have pursued an  approach in which quantization is an integral part of the learning process.  3 DOLCE  Our approach incorporates a clustering module into the recurrent net architecture,  as shown on the right side of Figure 1. The hidden layer activities are processed by  the clustering module before being passed on to other layers. The clustering module  maps regions in hidden state space to a single point in the same space, effectively  partitioning or clustering the hidden state space. Each cluster corresponds to a  discrete internal state. The clusters are adaptive and dynamic, changing over the  course of learning. We call this architecture DOLCE, for _ynamic on-line clustering  and state extraction.  The DOLCE architecture may be explored along two dimensions: (1) the clustering  algorithm used (e.g., a Gaussian mixture model, ISODATA, the Forgy algorithm,  vector quantization schemes), and (2) whether supervised or unsupervised training  is used to identify the clusters. In unsupervised mode, the performance error on  the FSM induction task has no effect on the operation of the clustering algorithm;  instead, an internal criterion characterizes goodness of clusters. In supervised mode,  the primary measure that affects the goodness of a cluster is the performance error.  Regardless of the training mode, all clustering algorithms incorporate a pressure to  22 Das and Mozer  Figure 2: Two dimensions of a typical state space. The true states needed to perform  the task are cx, c2, and cs, while the observed hidden states, assumed to be corrupted by  noise, are distributed about the cl.  produce a small number of clusters. Additionally, as we elaborate more specifically  below, the algorithms must allow for a soft or continuous clustering during training,  in order to be integrated into a gradient-based learning procedure.  We have explored two possibilities for the clustering module. The first involves  the use of Forgy's algorithm in an unsupervised mode. Forgy's (1965) algorithm  determines both the number of clusters and the partitioning of the space. The  second uses a Gaussian mixture model in a supervised mode, where the mixture  model parameters are adjusted so as to minimize the performance error. Both  approaches were successful, but as the latter approach obtained better results, we  describe it in the next section.  4 CLUSTERING USING A MIXTURE MODEL  Here we motivate the incorporation of a Gaussian mixture model into DOLCE, us-  ing an argument that gives the approach a solid theoretical foundation. Several  assumptions underly the approach. First, we assume that the task faced by DOLCE  is such that it requires a finite set of internal or true states, C =  This is simply the premise that motivates this line of work. Second, we assume  that any observed hidden state--i.e., a hidden activity pattern that results from  presentation of a symbol sequence--belongs to C but has been corrupted by noise  due to inaccuracy in the network weights. Third, we assume that this noise is Gaus-  sian and decreases as learning progresses (i.e., as the weights are adjusted to better  perform the task). These assumptions are depicted in Figure 2.  Based on these assumptions, we construct a Gaussian mixture distribution that  models the observed hidden states:  where h denotes an observed hidden state, r/2 the variance of the noise that cor-  rupts state ci, qi is the prior probability that the true state is ci, and H is the  dimensionality of the hidden state space. For pedagogical purposes, assume for the  time being that the parameters of the mixture distributionroT, C, o', and qmare  all known; in a later section we discuss how these parameters are determined.  A Unified Gradient-Descent/Clustering Architecture for Finite State Machine Induction 23  before training  after successful training  Figure 3: A schematic depiction of the hidden state space before and after training. The  horizontal plane represents the state space. The bumps indicate the probability density  under the mixture model. Observed hidden states are represented by small open circles.  Given a noisy observed hidden state, h, DOLCE computes the maximum a posterJori  (MAP) estimator of h in C. This estimator then replaces the noisy state and is used  in all subsequent computation. The MAP estimator, a, is computed as follows. The  probability of an observed state h being generated by a given true state i is  p(hltrue state i)  Using Bayes' rule, one can compute the posterior probability of true state i, given  that h has been observed:  p(hltrue state i)qi  p(true state ilh)- Ej P(hltrue state j)qj  Finally, the MAP estimator is given by [t = argmaxap(true state ilh)' However,  because learning requires that DOLCE'S dynamics be differentiable, we use a soft  version of MAP which involves using fi = ]i cip(true state iih) instead of [, and  incorporating a "temperature" parameter into cri as described below.  An important parameter in the mixture model is T, the number of true states  (Gaussians bumps). Because T directly corresponds to the number of states in  the target FSM, if T is chosen too small, DOLCE could not emulate the FSM.  Consequently, we set T to a large value, and the training procedure includes a  technique for eliminating unnecessary true states. (If the initially selected T is not  large enough, the training procedure will not converge to zero error on the training  set, and the procedure can be restarred with a larger value of T.)  At the start of training, each Gaussian center, ci, is initialized to a random location  in the hidden state space. The standard deviations of each Gaussian, ri, are initially  set to a large value. The priors, qi, are set to 1IT. The weights are set to initial  values chosen from the uniform distribution in [-.25,.25]. All connection weights  feeding into the hidden layer are second order.  The network weights and mixture model parameters--C, er, and q--are adjusted by  gradient descent in a cost measure, C. This cost includes two components: (a) the  performance error, , which is a squared difference between the actual and target  network output following presentation of a training string, and (b) a complexity  24 Das and Mozer  ,- 800  o   $00    4o0    200  o   language  NC RQ LQ DF DG  2000  1500  1000  500  0  language 2  NC RQ LQ DF DG  30O  2OO  lOO  NC RQ LQ DF DG  600  4OO  2OO   language  NC RQ LQ DF DG  lOOO  50o   language 5  NC RQ LQ DF DG  1500  1000  50O  language 6  NC RQ LQ DF DG  Figure 4: Each graph depicts generalization performance on one of the Tornita languages  for 5 alternative neural net approaches: no clustering [NC], rigid quantization [RQ], learn  then quantize [LQ], DOLCV. in unsupervised mode using Forgy's algorithm [DF], DOLCE  in supervised mode using mixture model [DG]. The vertical axis shows the number of  misclassification of 3000 test strings. Each bar is the average result across 10 replications  with different initial weights.  cost, which is the entropy of the prior distribution, q:  = - A Z qi ln qi  where X is a regularization parameter. The complexity cost is minimal when only  one Gaussian has a nonzero prior, and maximal when all priors are equal. Hence,  the cost encourages unnecessary Gausslans to drop out of the mixture model.  The particular gradient descent procedure used is a generalization of back propa-  gation through time (Rumelhart, Hinton, & Williams, 1986) that incorporates the  mixture model. To better condition the search space and to avoid a constrained  search, optimization is performed not over  and q directly but rather over hyper-  parameters a and b, where r -- exp(ai)/15 and ql '- exp(-b)/5-.j exp(-b?).  The global parameter 5 scales the overall spread of the Gaussians, which corre-  sponds to the level of noise in the model. As performance on the training set  improves, we assume that the network weights are coming to better approximate  the target weights, hence that the level of noise is decreasing. Thus, we tie fi to  the performance error . We have used various annealing schedules and  appears robust to this variation; we currently use fi oc 1/. Note that as  -- 0,     and the probability density under one Gaussian at h will become infinitely  greater than the density under any other; consequently, the soft MAP estimator,  [, becomes equivalent to the MAP estimator f, and the transformed hidden state  becomes discrete. A schematic depiction of the probability landscape both before  and after training is depicted in Figure 3.  A Unified Gradient-Descent/Clustering Architecture for Finite State Machine Induction 25  5 SIMULATION STUDIES  The network was trained on a set of regular languages first studied by Tomira (1982).  The languages, which utilize only the symbols 0 and 1, are: (1) 1'; (2) (10)*; (3) no  odd number of consecutive l's is directly followed by an odd number of consecutive  O's; (4) any string not containing the substring "000"; (5). [(01110)(01110)]*; (6)  the difference between the number of ones and number of zeros in the string is a  multiple of three; and (7) 0* 1' 0* 1'.  A fixed training corpus of strings was generated for each language, with an equal  number of positive and negative examples. The maximum string length varied from  5 to 10 symbols and the total number of examples varied from 50 to 150, depending  on the difficulty of the induction task.  Each string was presented one symbol at a time, after which DOLCE was given a  target output that specified whether the string was a positive or negative example  of the language. Training continued until DOLCE converged on a set of weights  and mixture model parameters. Because we assume that the training examples are  correctly classified, the error  on the training set should go to zero when DOLCE has  learned. If this did not happen on a given training run, we restarred the simulation  with different initial random weights.  For each language, ten replications of DOLCE (with the supervised mixture model)  were trained, each with different random initial weights. The learning rate and  regularization parameter A were chosen for each language by quick experimentation  with the aim of maximizing the likelihood of convergence on the training set. We  also trained a version of DOLCE that clustered using the unsupervised Forgy algo-  rithm, as well as several alternative neural net approaches: a generic recurrent net,  as shown on the left side of Figure 1, which used no clustering [NC]; a version with  rigid quantization during training [RQ], comparable to the earlier work of Zeng,  Goodman, and Smyth (1993); and a version in which the unsupervised Forgy algo-  rithm was used to quantize the hidden state following training [LQ], comparable to  the earlier work of Das and Das (1991). In these alternative approaches, we used  the same architecture as DOLCE except for the clustering procedure. We selected  learning parameters to optimize performance on the training set, ran ten replica-  tions for each language, replaced runs which did not converge, and used the same  training sets.  6 RESULTS AND CONCLUSION  In Figure 4, we compare the generalization performance of DOLCE--both the unsu-  pervised Forgy [DF] and supervised mixture model [DG]--to the NC, RQ, and LQ  approaches. Generalization performance was tested using 3000 strings not in the  training set, half positive examples and half negative. The two versions of DOLCE  outperformed the alternative neural net approaches, and the DG version of DOLCE  consistently outperformed the DF version.  To summarize, we have described an approach that incorporates inductive bias into  a learning algorithm in order to encourage the evolution of discrete representations  during training. This approach is a quite general and can be applied to domains  26 Das and Mozer  other than grammaticality judgement where discrete representations might be de-  sirable. Also, this approach is not specific to recurrent networks and may be applied  to feedforward networks. We are now in the process of applying oOLCE to a much  larger, real-world problem that involves predicting the next symbol in a string. The  data base comes from a case study in software engineering, where each symbol  represents an operation in the software development process. This data is quite  noisy and it is unlikely that the data can be parsimoniously described by an FSM.  Nonetheless, our initial results are encouraging: oot,E produces predictions at  least three times more accurate than a standard recurrent net without clustering.  Acknowledgement s  This research was supported by NSF Presidential Young Investigator award IRI-  9058450 and grant 90-21 from the James S. McDonnell Foundation.  References  S. Das & R. Das. (1991) Induction of discrete state-machine by stabilizing a continuous re-  current network using clustering. Computer Science and Inforrnatics 21(2):35-40. Special  Issue on Neural Computing.  J.L. Elman. (1990) Finding structure in time. Cognitive Science 14:179-212.  E. Forgy. (1965) Cluster analysis of multivariate data: efficiency versus interpretabillty of  classifications. Biometrics 21:768-780.  M.C. Mozer & J.D Bachtach. (1990) Discovering the structure of a reactive environment  by exploration. Neural Computation 2(4):447-457.  C. McMillan, M.C. Mozer, & P. Singlerisky. (1992) Rule induction through integrated  symbolic and subsymbolic processing. In J.E. Moody, S.J. Hanson, & R.P. Lippmann  (eds.), Advances in Neural Information Processing Systems 4, 969-976. San Mateo, CA:  Morgan Kaufmann.  C.L. Giles, D. Chen, C.B. Miller, H.H. Chen, G.Z. Sun, & Y.C. Lee. (1992) Learning  and extracting finite state automata with second-order recurrent neural network. Neural  Computation 4(3):393-405.  H. Schfitze. (1993) Word space. In S.J. Hanson, J.D. Cowan, & C.L. Giles (eds.), Advances  in Neural Information Processing Systems 5, 895-902. San Mateo, CA: Morgan Kaufmann.  M. Tomita. (1982) Dynamic construction of finite-state automata from examples using hill-  climbing. Proceedings of the Fourth Annual Conference of the Cognitive Science Society,  105-108.  G. Towell & J. Shavlik. (1992) Interpretion of artificial neural networks: mapping  knowledge-based neural networks into rules. In J.E. Moody, S.J. Hanson, & R.P. Lipp-  mann (eds.), Advances in Neural Information Processing Systems 4, 977-984. San Mateo,  CA: Morgan Kaufmann.  R.L. Watrous & G.M. Kuhn. (1992) Induction of finite state languages using second-order  recurrent networks. In J.E. Moody, S.J. Hanson, & R.P. Lippmann (eds.), Advances in  Neural Information Processing Systems 4, 969-976. San Mateo, CA: Morgan Kaufmann.  Z. Zeng, R. Goodman, & P. Smyth. (1993) Learning finite state machines with self-  clustering recurrent networks. Neural Computation 5(6):976-990.  
Neural Network Exploration Using  Optimal Experiment Design  David A. Cohn  Dept. of Brain and Cognitive Sciences  Massachusetts Inst. of Technology  Cambridge, MA 02139  Abstract  Consider the problem of learning input/output mappings through  exploration, e.g. learning the kinematics or dynamics of a robotic  manipulator. If actions are expensive and computation is cheap,  then we should explore by selecting a trajectory through the in-  put space which gives us the most amount of information in the  fewest number of steps. I discuss how results from the field of opti-  mal experiment design may be used to guide such exploration, and  demonstrate its use on a simple kinematics problem.  I Introduction  Most machine learning research treats the learner as a passive receptacle for data  to be processed. This approach ignores the fact that, in many situations, a learner  is able, and sometimes required, to act on its environment to gather data.  Learning control inherently involves being active; the controller must act in order  to learn the result of its action. When training a neural network to control a  robotic arm, one may explore by allowing the controller to "flail" for a length of  time, moving the arm at random through coordinate space while it builds up data  from which to build a model [Kuperstein, 1988]. This is not feasible, however, if  actions are expensive and must be conserved. In these situations, we should choose  a training trajectory that will get the most information out of a limited number of  steps. Manually designing such trajectories is a slow process, and intuitively "good"  trajectories often fail to sufficiently explore the state space [Armstrong, 1989]. In  679  680 Cohn  this paper I discuss another alternative for exploration: automatic, incremental  generation of training trajectories using results from "optimal experiment design."  The study of optimal experiment design (OED) [Fedorov, 1972] is concerned with  the design of experiments that are expected to minimize variances of a parameter-  ized model. Viewing actions as experiments that move us through the state space,  we can use the techniques of OED to design training trajectories.  The intent of optimal experiment design is usually to maximize confidence in a  given model, minimize parameter variances for system identification, or minimize  the model's output variance. Armstrong [1989] used a form of OED to identify link  masses and inertial moments of a robot arm, and found that automatically gener-  ated training trajectories provided a significant improvement over human-designed  trajectories. Automatic exploration strategies have been tried for neural networks  (e.g. [Thrun and M611er, 1992], [Moore, 1994]), but use of OED in the neural net-  work community has been limited. Plutowski and White [1993] successfully used it  to filter a data set for maximally informative points, but its application to selecting  new data has only been proposed [MacKay, 1992], not demonstrated.  The following section gives a brief description of the relevant results from optimal  experiment design. Section 3 describes how these results may be adapted to guide  neural network exploration and Section 4 presents experimental results of imple-  menting this adaptation. Finally, Section 5 discusses implications of the results,  and logical extensions of the current experiments.  2 Optimal experiment design  Optimal experiment design draws heavily on the technique of Maximum Likelihood  Estimation (MLE) [Thisted, 1988]. Given a set of assumptions about the learner's  architecture and sources of noise in the output, MLE provides a statistical basis for  learning. Although the specific MLE techniques we use hold exactly only for linear  models, making certain computational approximations allows them to be used with  nonlinear systems such as neural networks.  We begin with a training set of input-output pairs (xi, Yi)i and a learner fw().  We define fw(x) to be the learner's output given input x and weight vector w.  Under an assumption of additive Gaussian noise, the maximum likelihood estimate  for the weight vector, , is that which minimizes the sum squared error Ess =  y4=(f(xi) - yi) '. The estimate zb gives us an estimate of the output at a novel  input: 0 = f(x) (see e.g. Figure la).  MLE allows us to compute the variances of our weight and output estimates. Writ-  ing the output sensitivity asg(x) = cgf(x)/cgw, the covariances of  are  where the last approximation assumes local linearity of g(x). (For brevity, the  output sensitivity will be abbreviated to g(x) in the rest of the paper.)  Neural Network Exploration Using Optimal Experiment Design 681  1  0.7  0,25  0  0  0.25  0,5  xl  0.75  8  Y  .75 200-75   5x 2 0 . . 0.5x2  0.25 0.25 -  xl 0.75  0 xl O. 7  1 1 3  Figure 1: a) A set of training examples for a classification problem, and the net-  work's best fit to the data. b) Maximum likelihood estimate of the network's output  variance for the same problem.  For a given reference input xr, the estimated output variance is  var(xr) = g(x,)T  (1)  Output variance corresponds to the model's estimate of the expected squared dis-  tance between its output f (x) and the unknown "true" output y. Output variance  then, corresponds to the model's estimate of its mean squared error (MSE) (see Fig-  ure lb). If the estimates are accurate then minimizing the output variance would  correspond to minimizing the network's MSE.  In optimal experiment design, we estimate how adding a new training example is  expected to change the computed variances. Given a novel x+, we can use OED  to predict the effect of adding x+ and its as-yet-unknown y+ to the training  set. We make the assumption that  A+ -  + 9(+)9(+)T -,  which corresponds to assuming that our current model is already fairly good. Based  on this assumption, the new parameter variances will be  -1  A+ = A  - A;9(+1)(1 + 9(+)A9(+))9(+)  ;  Combined with Equation 1, this predicts that if we take a new example at x+,  the change in output variance at reference input x will be  5a() = (9()A;9(+))(l+9(+)A9(+))  = cov(x,x+)(1 + vav(x+)) (2)  To minimize the expected value of vav(x), we should select x+ so as to maximize  the right side of Equation 2. For other interesting OED measures, see MacKay  [1992].  682 Cohn  3 Adapting OED to Exploration  When building a world model, the learner is trying to build a mapping, e.g. from  joint angles to cartesian coordinates (or from state-action pairs to next states). If  it is allowed to select arbitrary joint angles (inputs) in successive time steps, then  the problem is one of selecting the next "query" to make ([Cohn, 1990], [Baum and  Lang, 1991]). In exploration, however, one's choices for a next input are constrained  by the current input. We cannot instantaneously "teleport" to remote parts of the  state space, but must choose among inputs that are available in the next time step.  One approach to selecting a next input is to use selective sampling: evaluate a num-  ber of possible random inputs, choose the one with the highest expected gain. In a  high-dimensional action space, this is inefficient. The approach followed here is that  of gradient search, differentiating Equation 2 and hillclimbing on  Note that Equation 2 gives the expected change in variance only at a single point  xr, while we wish to minimize the average variance over the entire domain. Ex-  plicitly integrating over the domain is intractable, so we must make do with an  approximation. MacKay [1992] proposed using a fixed set of reference points and  measuring the expected change in variance over them. This produces spurious lo-  cal maxima at the reference points, and has the undesirable effect of arbitrarily  quantizing the input space. Our approach is to iteratively draw reference points at  random (either uniformly or according to a distribution of interest), and compute  a stochastic approximation of Avar.  By climbing the stochastically approximated gradient, either to convergence or to  the horizon of available next inputs, we will settle on an input/action with a (locally)  optimal decrease in expected variance.  4 Experimental Results  In this section, I describe two sets of experiments. The first attempts to confirm  that the gains predicted by optimal experiment design may actually be realized in  practice, and the second studies the application of OED to a simple learning task.  4.1 Expected versus actual gain  It must be emphasized that the gains predicted by OED are expected gains. These  expectations are based on the relatively strong assumptions of MLE, which may  not strictly hold. In order for the expected gains to materialize, two "bridges" must  be crossed. First, the expected decrease in model variance must be realized as an  actual decrease in variance. Second, the actual decrease in model variance must  translate into an actual decrease in model MSE.  4.1.1 Expected decreases in variance -- actual decreases in variance  The translation from expected to actual changes in variance requires coordination  between the exploration strategy and the learning algorithm: to predict how the  variance of a weight will change with a new piece of data, the predictor must know  how the weight itself (and its neighboring weights) will change. Using a black  Neural Network Exploration Using Optimal Experiment Design 683  0.012  0.01  0.008  0.006  0.004  0. 002  /> ..... actual=expected  0.002 0.004 0.006 0.008 0.01 0.012  expected delta var  2.8  2.4  2  1.6  1.2  0.8  0.4  0  -0.4  xx x  x  x x x x x  x  x  x Xx X  X  x x x  x  X XX X X  xj3 x x x x xx  x : x, x x x x  0.002 0.004 0.006 0.008 0.01 0.012  actual delta vat  x  x  Figure 2: a) Correlations between expected change in output variance and actual  change output variance b) Correlations between actual change in output variance  and change in meetn squetred error. Correlations are plotted for a network trained  on 50 examples from the etrm kinematics task.  box routine like backpropagation to update the weights virtually guarantees that  there will be some mismatch between expected and actual decreetses in variance.  Experiments indicate that, in spite of this, the correlation between predicted and  actual changes in variance are relatively good (Figure 2a).  4.1.2 Decreases in variance --+ decreases in MSE  A more troubling translation is the one from model variance to model correctness.  Given the highly nonlinear nature of a neural network, local minima may leave us  in situations where the model is very confident but entirely wrong. Due to high  confidence, the learner may reject actions that would reduce its mean squared error  and explore areas where the model is correct, but has low confidence. Evidence  of this behavior is seen in the lower right corner of Figure 2b, where some actions  which produce a large decrease in variance have little effect on the network's MSE.  While this decreases the utility of OED, it is not crippling. We discuss one possible  solution to this problem at the end of this paper.  4.2 Learning kinematics  We have used the the stochastic approximation of Avar to guide exploration on  several simple tasks involving classification and regression. Below, I detail the  experiments involving exploration of the kinematics of a simple two-dimensionM,  two-joint arm. The task was to learn a forward model {)1 X {)2 --+ Z X y through  exploration, which could then be used to build a controller following Jordan [1992].  684 Cohn  The model was to be learned by a feedforward network with a sigmoid transfer  function using a single hidden layer of 8 or 20 hidden units.  4  Figure 3: Learning 2D arm kinematics with 8 hidden units. a) Geometry of the 2D,  two-joint arm. b) Sample trajectory using OED-based greedy exploration.  On each time step, the learner was allowed to select inputs  and 02 and was then  given tip position x and y to incorporate into its training set. It then hillclimbed  to find the next  and 2 within its limits of movement that would maximize the  stochastic approximation of Avar. On each time step  and 2 were limited to  change by no more than +36  and +18  respectively. Simulations were performed on  the Xerion simulator (made available by the University of Toronto), approximating  the variance gradient on each step with 100 randomly drawn points. A sample tip  trajectory is illustrated in Figure 3b.  We compared the performance of this one-step optimal (greedy) learner, in terms  of mean squared error, with that of an identical learner which explored randomly  by "flailing." Not surprisingly, the improvement of greedy exploration over random  exploration is significant (Figure 4b). The asymptotic performance of the greedy  learner was better than that of the random learner, and it reached its asymptote in  much few steps.  5 Discussion  The experiments described in this paper indicate that optimal experiment design  is a promising tool for guiding neural network exploration. It requires no arbi-  trary discretization of state or action spaces, and is amenable to gradient search  techniques. It does, however, have high computational costs and, as discussed in  Section 4.1.2, may be led astray if the model settles in a local minimum.  5.1 Alternatives to greedy OED  The greedy approach is prone to "boxing itself into a corner" while leaving important  parts of the domain unexplored. One heuristic for avoiding local minima is to  Neural Network Exploration Using Optimal Experiment Design 685  O. 28J  0.24  o.2oq  0.16!f  0.12j  O. 08  0.04  o.oo ....... I  0 20 40 60 80 100 120  Number of steps  O. 27][ RandQm  00 2241!1   Greedy  0.15-.'     .9:  o.oi ..  0 20 0 60 80 10012010160180 200  Number o steps  Figure 4: Learning 2D arm kinematics. a) MSE for a single exploration trajectory  (20 hidden units). b) Plot of MSE for random and greedy exploration vs. number  of training examples, averaged over 12 runs (8 hidden units).  occasionally check the expected gain in other parts of the input space and move  towards them if they promise much greater gain than a greedy step.  The theoretically correct but computationally expensive approach is to optimize  over an entire trajectory. Trajectory optimization entails starting with an initial  trajectory, computing the expected gain over it, and iteratively perturbing points on  the trajectory towards towards optimal expected gain (subject to other points along  the trajectory being explored). Experiments are currently underway to determine  how much of an improvement may be realized with trajectory optimization; it is  unclear whether the improvement over the greedy approach will be worth the added  computational cost.  5.2 Computational Costs  The computational costs of even greedy OED are great. Selecting a next action  requires computation and inversion of the hessian c2Es,e/Ow 2. Each time an action  is selected and taken, the new data must be incorporated into the training set,  and the learner retrained. In comparison, when using a flailing strategy or a fixed  trajectory, the data may be gathered with little computation, and the learner trained  only once on the batch. In this light, the cost of data must be much greater than  the cost of computation for optimal experiment design to be a preferable strategy.  There are many approximations one can make which significantly bring down the  cost of OED. By only considering covariances of weights leading to the same neuron,  the hessian may be reduced to a block diagonal form, with each neuron computing  its own (simpler) covariances in parallel. As an extreme, one can do away with  covariances entirely and rely only on individual weight variances, whose computa-  tion is simple. By the same token, one can incorporate the new examples in small  batches, only retraining every 5 or so steps. While suboptimal from a data gather-  ing perspective, they appear to still outperform random exploration, and are much  cheaper than "full-blown" optimization.  686 Cohn  5.3 Alternative architectures  We may be able to bring down computational costs and improve performance by  using a different architecture for the learner. With a standard feedforward neural  network, not only is the repeated compution of variances expensive, it sometimes  fails to yield estimates suitable for use as confidence intervals (as we saw in Sec-  tion 4.1.2). A solution to both of these problems may lie in selection of a more  amenable architecture and learning algorithm. One such architecture, in which  output variances have a direct role in estimation, is a mixture of Gaussians, which  may be efficiently trained using an EM algorithm [Ghahramani and Jordan, 1994].  We expect that it is along these lines that our future research will be most fruitful.  Acknowledgements  I am indebted to Michael I. Jordan and David J.C. MacKay for their help in making  this research possible. This work was funded by ATR Human Information Process-  ing Laboratories, Siemens Corporate Research and NSF grant CDA-9309300.  Bibliography  B. Armstrong. (1989) On finding exciting trajectories for identification experiments.  Int. J. of Robotics Research, 8(6):28-48.  E. Baum and K. Lang. (1991) Constructing hidden units using examples and  queries. In R. Lippmann et al., eds., Advances in Neural Information Processing  Systems 3, Morgan Kaufmann, San Francisco, CA.  D. Cohn, L. Atlas and R. Ladner. (1990) Training connectionist networks with  queries and selective sampling. In D. Touretzky, editor, Advances in Neural Infor-  mation Processing Systems 2, Morgan Kaufmann, San Francisco.  V. Fedorov. (1972) Theory of Optimal Experiments. Academic Press, New York.  Z. Ghahramani and M. Jordan. (1994) Supervised learning from incomplete data  via an EM approach. In this volume.  M. Jordan and D. Rumelhart. (1992) Forward models: Supervised learning with a  distal teacher. Cognitive Science, 16(3):307-354.  D. MacKay. (1992) Information-based objective functions for active data selection,  Neural Computation 4(4): 590-604.  A. Moore. (1994) The parti-game algorithm for variable resolution reinforcement  learning in multidimensional state-spaces. In this volume.  M. Plutowski and H. White. (1993) Selecting concise training sets from clean data.  IEEE Trans. on Neural Networks, 4(2):305-318.  R. Thisted. (1988) Elements of Statistical Computing. Chapman and Hall, NY.  S. Thrun and K. MSller. (1992) Active Exploration in Dynamic Environments. In  J. Moody et al., editors, Advances in Neural Information Processing Systems .  Morgan Kaufmann, San Francisco, CA.  
Estimating analogical similarity by dot-products  of Holographic Reduced Representations.  Tony A. Plate  Department of Computer Science, University of Toronto  Toronto, Ontario, Canada M5S 1A4  email: tap @ ai.utoronto.ca  Abstract  Models of analog retrieval require a computationally cheap method of  estimating similarity between a probe and the candidates in a large pool  of memory items. The vector dot-product operation would be ideal for  this purpose if it were possible to encode complex structures as vector  representations in such a way that the superficial similarity of vector  representations reflected underlying structural similarity. This paper de-  scribes how such an encoding is provided by Holographic Reduced Rep-  resentations (HRRs), which are a method for encoding nested relational  structures as fixed-width distributed representations. The conditions un-  der which structural similarity is reflected in the dot-product rankings of  HRRs are discussed.  1 INTRODUCTION  Gentnet and Markman (1992) suggested that the ability to deal with analogy will be a  "Watershed or Waterloo" for connectionist models. They identified "structural alignment"  as the central aspect of analogy making. They noted the apparent ease with which people  can perform structural alignment in a wide variety of tasks and were pessimistic about the  prospects for the development of a distributed connectionist model that could be useful in  performing structural alignment.  In this paper I describe how Holographic Reduced Representations (HRRs) (Plate, 1991;  Plate, 1994), a fixed-width distributed representation for nested structures, can be used  to obtain fast estimates of analogical similarity. A HRR is a high dimensional vector,  1109  1110 Plate  and the vector dot-product of two HRRs is an efficiently computable estimate of the  overall similarity between the two structures represented. This estimate reflects both  surface similarity and some aspects of structural similarity,  even though alignments are  not explicitly calculated. I also describe contextualization, an enrichment of HRRs designed  to make dot-product comparisons of HRRs more sensitive to structural similarity.  2 STRUCTURAL ALIGNMENT & ANALOGICAL REMINDING  People appear to perform structural alignment in a wide variety of tasks, including per-  ception, problem solving, and memory recall (Gentner and Markman, 1992; Markman,  Gentner and Wisniewski, 1993). One task many researchers have investigated is analog  recall. A subject is shown a number of stories and later is shown a probe story. The task  is to recall stories that are similar to the probe story (and sometimes evaluate the degree of  similarity and perform analogical reasoning).  MAC/FAC, a computer models of this process, has two stages(Gentner and Forbus, 1991).  The first stage selects a few likely analogs from a large number of potential analogs. The  second stage searches for an optimal (or at least good) mapping between each selected story  and the probe story and outputs those with the best mappings. Two stages are necessary  because it is too computationally expensive to search for an optimal mapping between the  probe and all stories in memory. An important requirement for a first stage is that its  performance scale well with both the size and number of episodes in long-term memory.  This prevents the first stage of MAC/FAC from considering any structural features.  Large pool of items in memory Probp   () () / Potentia% Good  () 'J. () 'J() / analogies X analogies   __() _,  O Ex-ensive selection ro  ()  _.( ()- ,Chea,p filtenng process cesPs based on struclPurai  k_2 0 (10'0 () tasea on sunace eatures features  Figure 1: General architecture of a two-stage retrieval model.  While it is indisputable that people take structural correspondences into account when  evaluating and using analogies (Gentnet, Rattermann and Forbus, 1993), it is less certain  whether structural similarity influences access to long term memory (i.e., the first-stage  reminding process). Some studies have found little effect of analogical similarity on  reminding (Gentnet and Forbus, 1991; Gentnet, Rattermann and Forbus, 1993), while  others have found some effect (Wharton et al., 1994).  "Surface features" of stories are the features of the entities and relations involved, and "structural  features" are the relationships among the relations and entities.  Estimating Analogical Similarity by Dot-Products of Holographic Reduced Representations 1111  In any case, surface features appear to influence the likelihood of a reminding far more than  do structural features. Studies that have found an effect of structural similarity on reminding  seem to indicate the effect only exists, or is greater, in the presence of surface similarity  (Gentnet and Forbus, 1991; Gentnet, Rattermann and Forbus, 1993; Thagard et al., 1990).  2.1 EXAMPLES OF ANALOGY BETWEEN NESTED STRUCTURES.  To test how well the HRR dot-product works as an estimate of analogical similarity between  nested relational structures I used the following set of simple episodes (see Plate (1993)  for the full set). The memorized episodes are similar in different ways to the probe. These  examples are adapted from (Thagard et al., 1990).  Probe:  Episodes in long-term  E1 (LS)  E2 (AN )  E3 (AN)  E6 (SS)  E7 (FA)  Spot bit Jane, causing Jane to flee from Spot.  memory:  Fido bit John, causing John to flee from Fido.  Fred bit Rover, causing Rover to flee from Fred.  Felix bit Mort, causing Mort to flee from Felix.  John fled from Fido, causing Fido to bite John.  Mort bit Felix, causing Mort to flee from Felix.  In these episodes Jane, John, and Fred are people, Spot, Fido and Rover are dogs, Felix is  a cat, and Mort is a mouse. All of these are objects, represented by token vectors. Tokens  of the same type are considered to be similar to each other, but not to tokens of other types.  Bite, flee, and cause are relations. The argument structure of the cause relation, and the  patterns in which objects fill multiple roles constitutes the higher-order structure.  The second column classifies the relationship between each episode and the probe using  Gentner et al's types of similarity: LS (Literal Similarity) shares relations, object features,  and higher-order structure; AN (Analogy, also called True Analogy) shares relations and  higher-order structure, but not object features; SS (Surface Similarity, also called Mere  Appearance) shares relations and object features, but not higher-order structure; FA (False  Analogy) shares relations only. AN cm denotes a cross-mapped analogy - it involves the  same types of objects as the probe, but the types of corresponding objects are swapped.  2.2 MAC/FAC PERFORMANCE ON TEST EXAMPLES  The first stage of MAC/FAC (the "Many Are Called" stage) only inspects object features  and relations. It uses a vector representation of surface features. Each location in the vector  corresponds to a surface feature of an object, relation or function, and the value in the  location is the number of times the feature occurs in the structure. The first-stage estimate  of the similarity between two structures is the dot-product of their feature-count vectors. A  threshold is used to select likely analogies. It would give E1 (LS), E2 (ANtre), and E6  (SS) equal and highest scores, i.e., (LS, AN , SS) > (AN, FA)  The Structure Mapping Engine (SME) (Falkenhainer, Forbus and Gentner, 1989) is used  as the second stage of MAC/FAC (the "Few Are Chosen" stage). The rules of SME are  that mapped relations must match, all the arguments of mapped relations must be mapped  consistently, and mapping of objects must be one-to-one. SME would detect structural  correspondences between each episode and the probe and give the literally similar and  analogous episodes the highest rankings, i.e., LS > AN > (SS, FA).  1112 Plate  A simplified view of the overall similarity scores from MAC and the full MAC/FAC is  shown in Table 1. There are four conditions - the two structures being compared can be  similar in structure and/or in object attributes. In all four conditions, the structures are  assumed to involve similar relations - only structural and object attribute similarities are  varied. Ideally, the responses to the mixed conditions should be flexible, and controlled by  which aspects of similarity are currently considered important. Only the relative values of  the scores are important, the absolute values do not matter.  Structural Object Attribute Similarity  Similarity YES NO  YES (LS) High (AN) Low  NO (SS) High (FA) Low  (a) Scores from MAC.  Structural Object Attribute Similarity  Similarity YES NO  YES (LS) High (AN) $Med-High  NO (SS) $Med-Low (FA) Low  (b) Ideal similarity scores.  Table 1: (a) Scores from the fast (MAC) similarity estimator in MAC/FAC. (b) Scores from  an ideal structure-sensitive similarity estimator, e.g., SME as used in MAC/FAC.  In the remainder of this paper I describe how HRRs can be used to compute fast similarity  estimates that are more like ratings in Table lb, i.e., estimates that are flexible and sensitive  to structure.  3 HOLOGRAPHIC REDUCED REPRESENTATIONS  A distributed representation for nested relational structures requires a solution to the binding  problem. The representation of a relation such as bte (spot, jane) ("Spot bit Jane.")  must bind 'Spot' to the agent role and 'Jane' to the object role. In order to represent nested  structures it must also be possible to bind a relation to a role, e.g., bto (spot, j ano) and  the antecedent role of the cause relation.  z_=xy  n--1  Zi '-- E XkYj-k  k----O  (Subscript are modulo-n)  zo xy Y  (a) z2 (b)  z=x(by  zo = xoYo + x2yl q- xly2  z = xyo + xoy + x2y2  z2 = x2yo + xly q- xoy2  Figure 2: (a) Circular convolution. (b) Circular convolution illustrated as a compressed  outer product for n: 3. Each of the small circles represents an element of the outer product  of x and y, e.g., the middle bottom one is x2Yl. The elements of the circular convolution  of x and y are the sums of the outer product elements along the wrapped diagonal lines.  Holographic Reduced Representations (HRRs) (Plate, 1994) use circular convolution to  solve the binding problem. Circular convolution (Figure 2a) is an operation that maps two  n-dimensional vectors onto one n-dimensional vector. It can be viewed as a compressed  outer product, as shown in Figure 2b. Algebraically, circular convolution behaves like  multiplication - it is commutative, associative, and distributes over addition. Circular  Estimating Analogical Similarity by Dot-Products of Holographic Reduced Representations 1113  convolution is similarity preserving: if_a  _a' then __a  b_  _a'  b. Associations can be  decoded using a stable approximate inverse: _a*  (_a  b)  b (provided that the vector  elements are normally distributed with mean zero and variance l/n). The approximate  inverse is a permutation of vector elements: a' = an-i. The dot-product of two vectors,  a similarity measure, is: a. b = Y',=0 aibi. High dimensional vectors (n in the low  thousands) must be used to ensure reliable encoding and decoding.  The HRR forbite(spot ,jane) is: F =< bite + bitegt  spot + biteobj jane >,  where <  > is a normalization operation (< a >= a/). Multiple associations are  superimposed in one vector and the representations for the objects (spot and jane) can  also be added into the HRR in order to make it similar to other HRRs involving Spot and  Jane. The HRR for a relation is the same size as the representation for an object and can be  used as the filler for a role in another relation.  4 EXPT. 1: HRR DOT-PRODUCT SIMILARITY ESTIMATES  Experiment 1 illustrates the ways in which the dot-products of ordinary HRRs reflect, and  fail to reflect, the similarity of the underlying structure of the episodes.  Base vectors [ Token vectors  person, dog, cat, mouse jane =< person + idj,, > spot =< dog + idapot >  bite, flee, cause john =< person + idjonn > fido =< dog + idfido >  bite,g, fleeagt, cause,,,c fred =< person + idfr/ > rover =< dog + idro,r >  biteobj, flee from, causec,,q mort --< mouse + idmort > felix =< cat + idf,ti= >  The set of base and tokens vectors used in Experiments 1, 2 and 3 is shown above. All base  and id vectors had elements independently chosen from a zero-mean normal distribution  with variance 1In. The HRR for the probe is constructed as follows, and the HRRs for the  other episodes are constructed in the same manner.  Pbit =< bite + bitet ) spot + biteoj )jane >  Pfle =< flee + fieeagt jane + fleefrom ) spot >  Poj.ct. =< jane + spot >  P =< cause + Pobjects q- Pbite q- Pft** + causeantc  Pbit q- causec,q  Pf;** >  Experiment 1 was run 100 times, each time with a new choice of random base vectors. The  vector dimension was 2048. The means and standard deviations of the HRR dot-products  of the probe and each episode are shown in Table 2.  Dot-product with probe  Probe: Spot bit Jane, causing Jane to flee from Spot. Exptl Expt2 Expt3  Episodes in long-term memory: Avg Sd  E1 LS Fido bit John, causing John to flee from Fido. 0.70 0.016 0.63 0.81  E2 AN m Fred bit Rover, causing Rover to flee from Fred. 0.47 0.022 0.47 0.69  E3 AN Felix bit Mort, causing Mort to flee from Felix. 0.39 0.024 0.39 0.61  E6 SS John fled from Fido, causing Fido to bite John. 0.47 0.018 0.44 0.53  E7 FA Mort bit Felix, causing Mort to flee from Felix. 0.39 0.024 0.39 0.39  Table 2: Results of Experiments 1, 2 and 3.  In 94 out of 100 runs, the ranking of the HRR dot-products was consistent with  LS > (AN c, SS) > (FA, AN)  1114 Plate  (where the ordering within the parenthesis varies). The order violations are due to "random"  fluctuations of dot-products, whose variance decreases as the vector dimension increases.  When the experiment was rerun with vector dimension 4096 there was only one violation  of this order out of 100 runs.  These results represent an improvement over the first stage of MAC/FAC - the HRR dot-  product distinguishes between literal and surface similarity. However, when the episodes  do not share object attributes, the HRR dot-product is not affected by structural similarity  and the scores do not distinguish analogy from false analogy or superficial similarity.  5 EXPERIMENTS 2 AND 3: CONTEXTUALIZED HRRS  Dot-product comparisons of HRRs are not sensitive to structural similarity in the absence of  similar objects. This is because the way in which objects fill multiple roles is not expressed  as a surface feature in HRRs. Consequently, the analogous episodes E2 (AN cm) and E3  (AN) do not receive higher scores than the non analogous episodes E6 (SS) and E7 (FA).  We can force role structure to become a surface feature by "contextualizing" the represen-  tations of fillers. Contextualization involves incorporating information about what other  roles an object fills in the representation of a filler. This is like thinking of Spot (in the  probe) as an entity that bites (a biter) and an entity that is fled from (a "fled-from").  In ordinary HRRs the filler alone is convolved with the role. In contextualized HRRs a blend  of the filler and its context is convolved with the role. The representation for the context of  object in a role is the typical fillers of the other roles the object fills. The context for Spot in  the flee relation is represented by typb[ and the context in the bite relation is represented  t' _flee  ,  by ylJfrom (where - _bite __ bite  bt%g t and t- _free  typg t -- ylJfrom = flee  fiee*rom) The  f   degree of contextualization is governed by the mixing proportions no (object) and nc  (context). The contextualized HRR for the probe is constructed as follows:  Pbite =< bite + biteagt  (nospot + nctypff*m) ' ' fee  + blteobj  (no jane + nctypag t ) >   bite fleefrom  (noSpOt + ntyp/gt[) >  P flee --< flee + fleeagt  (nojane + ntypobj ) +  Pobjets --< jane + spot >  P =< cause + Pobjects q- Pbite q- P flee q- caUSeantc () Pbite q- causecnsq ) P flee >  A useful similarity estimator must be flexible and able to adjust salience of different aspects  of similarity according to context or command. The degree to which role-alignment affects  the HRR dot-product can be adjusted by changing the degree of contextualization in just  one episode of a pair. Hence, the items in memory can be encoded with a fixed n values  (no TM and n) and the salience of role alignment can be changed by altering the degree of  contextualization in the probe (no p and nP). This is fortunate as it would be impractical to  recode all items in memory in order to alter the salience of role alignment in a particular  comparison. The same technique can be used to adjust the importance of other features.  Two experiments were performed with contextualized HRRs, with the same episodes as  used in Experiment 1. In Experiment 2 the probe was non-contextualized (no p = 1, nP = 0),  and in Experiment 3 the probe was contextualized (no p = 1/x/, n p = 1/v). For both  Experiments 2 and 3 the episodes in memory were encoded with the same degree of  contextualization (n7 = 1/v, n2 = 1/x/). As before, each set of comparisons was run  100 times, and the vector dimension was 2048. The results are shown in Table 2.  Estimating Analogical Similarity by Dot-Products of Holographic Reduced Representations 1115  The scores in Experiment 2 (non-contextualized probe) were consistent (in 95 out of 100  runs) with the same order as given for Experiment 1:  LS > (ANCm, SS) > (FA, AN)  The scores in Experiment 3 (contextualized probe) were consistent (in all 100 runs) with an  ordering that ranks analogous episodes as strictly more similar than non-analogous ones:  LS > AN cr > AN > SS > FA  6 DISCUSSION  The dot-product of HRRs provides a fast estimate of the degree of analogical match and  is sensitive to various structural aspects of the match. It is not intended to be a model  of complex or creative analogy making, but it could be a useful first stage in a model of  analogical reminding.  Structural Object Attribute Similarity  Similarity YES NO  YES (LS) High (AN) Low  NO (SS) Med (FA) Low  (a) Ordinary-HRR dot-products.  Structural Object Attribute Similarity  Similarity YES NO  YES (LS) High (AN) Med-High  NO (SS) $Med-Low (FA) Low  (b) Contextualized-HRR dot-products.  Table 3: Similarity scores from ordinary and contextualized HRR dot-product comparisons.  The flexibility comes adjusting the weights of various components in the probe.  The dot-product of ordinary HRRs is sensitive to some aspects of structural similarity. It  improves on the existing fast similarity matchef in MAC/FAC in that it discriminates the  first column of Table 3 - it ranks literally similar (LS) episodes higher than superficially  similar (SS) episodes. However, it is insensitive to structural similarity when corresponding  objects are not similar. Consequently, it ranks both analogies (AN) and false analogies (FA)  lower than superficially similar (SS) episodes.  The dot-product of contextualized HRRs is sensitive to structural similarity even when  corresponding objects are not similar. It ranks the given examples in the same order as  would the full MAC/FAC or ARCS system.  Contextualization does not cause all relational structure to be expressed as surface features  in the HRR vector. It only suffices to distinguish analogous from non-analogous structures  when no two entities fill the same set of roles. Sometimes, the distinguishing context for  an object is more than the other roles that the object fills. Consider the situation where  two boys are bitten by two dogs, and each flees from the dog that did not bite him. With  contextualization as described above it is impossible to distinguish this from the situation  where each boy flees from the dog that did bite him.  HRR dot-products are flexible - the salience of various aspects of similarity can be adjusted  by changing the weights of various components in the probe. This is true for both ordinary  and contextualized HRRs.  HRRs retain many of the advantages of ordinary distributed representations: (a) There is a  simple and computationally efficient measure of similarity between two representations -  1116 Plate  the vector dot-product. Similar items can be represented by similar vectors. (b) Items are  represented in a continuous space. (c) Information is distributed and redundant.  Hummel and Biederman (1992) discussed the binding problem and identified two main  problems faced by conjunctive coding approaches such as Tensor Products (Smolensky,  1990). These are exponential growth of the size of the representation with the number  of associated objects (or attributes), and insensitivity to attribute structure. HRRs have  much in common with conjunctive coding approaches (they can be viewed as a compressed  conjunctive code), but do not suffer from these problems. The size of HRRs remains  constant with increasing numbers of associated objects, and sensitivity to attribute structure  has been demonstrated in this paper.  The HRR dot-product is not without its drawbacks. Firstly, examples for which it will  produce counter-intuitive rankings can be constructed. Secondly, the scaling with the size  of episodes could be a problem - the sum of structural-feature matches becomes a less  appropriate measure of similarity as the episodes get larger. A possible solution to this  problem is to construct a spreading activation network of HRRs in which each episode is  represented as a number of chunks, and each chunk is represented by a node in the network.  The software used for the HRR calculations is available from the author.  References  Falkenhainer, B., Forbus, K. D., and Gentnet, D. (1989). The Structure-Mapping Engine: Algorithm  and examples. Artificial Intelligence, 41:1-63.  Gentnet, D. and Forbus, K. D. (1991). MAC/FAC: A model of similarity-based retrieval. In Proceed-  ings of the Thirteenth Annual Cognitive Science Society Conference, pages 504-509, Hillsdale, NJ.  Erlbaum.  Gentnet, D. and Markman, A. B. (1992). Analogy- Watershed or Waterloo? Structural alignment  and the development of connectionist models of analogy. In Giles, C. L., Hanson, S. J., and Cowan,  J. D., editors, Advances in Neural Information Processing Systems 5 (NIPS*92), pages 855-862, San  Mateo, CA. Morgan Kaufmann.  Gentner, D., Rattermann, M. J., and Forbus, K. D. (1993). The roles of similarity in transfer:  Separating retrievability from inferential soundness. Cognitive Psychology, 25:431-467.  Hummel, J. E. and Biederman, I. (1992). Dynamic binding in a neural network for shape recognition.  Psychological Review, 99(3):480-517.  Markman, A. B., Gentner, D., and Wisniewski, E. J. (1993). Comparison and cognition: Implications  of structure-sensitive processing for connectionist models. Unpublished manuscript.  Plate, T. A. (1991). Holographic Reduced Representations: Convolution algebra for compositional  distributed representations. In Mylopoulos, J. and Reiter, R., editors, Proceedings of the 12th Interna-  tional Joint Conference on Artificial Intelligence, pages 30-35, San Mateo, CA. Morgan Kaufmann.  Plate, T. A. (1993). Estimating analogical similarity by vector dot-products of Holographic Reduced  Representations. Unpublished manuscript.  Plate, T. A. (1994). Holographic reduced representations. IEEE Transactions on Neural Networks.  To appear.  S molensky, P. (1990). Tensor product variable binding and the representation of symbolic structures  in connectionist systems. Artificial Intelligence, 46(1-2): 159-216.  Thagard, P., Holyoak, K. J., Nelson, G., and Gochfeld, D. (1990). Analog Retrieval by Constraint  Satisfaction. Artificial Intelligence, 46:259-310.  Wharton, C. M., Holyoak, K. J., Downing, P. E., Lange, T. E., Wickens, T D., and Melz, E. R.  (1994). Below the surface: Analogical similarity and retrieval competition in reminding. Cognitive  Psychology. To appear.  
Observability of Neural Network  Behavior  Max arzon Fernnnda Botelho  sarzonmhermes. nsc i. nemst. edu bore lhofeherme s. nsc i. menst. edu  Institute for Intelligent Systems Department of Mathematical Sciences  Memphis State University  Memphis, TN 38152 U.S.A.  Abstract  We prove that except possibly for small exceptional sets, discrete-  time analog neural nets are globally observable, i.e. all their cor-  rupted pseudo-orbits on computer simulations actually reflect the  true dynamical behavior of the network. Locally finite discrete  (boolean) neural networks are observable without exception.  INTRODUCTION  We address some aspects of the general problem of implementation and robustness  of (mainly recurrent) autonomous discrete-time neural networks with continuous  activation (herein referred to as analog networks) and discrete activation (herein,  boolean networks). There are three main sources of perturbations from ideal oper-  ation in a neural network. First, the network's parameters may have been contam-  inated with noie from external sources. Second, the network is being implemented  in optics or electronics (digital or analog) and inherent measurement limitations  preclude the use of perfect information on the network's parameters. Third, as has  been the most common practice so far, neural networks are simulated or imple-  mented on a digital device, with the consequent limitations on precision to which  net parameters can be represented. Finally, for these or other reasons, the activation  functions (e.g. sigmoids) of the network are not known precisely or cannot be evalu-  ated properly. Although perhaps negligible in a single iteration, these perturbations  are likely to accumulate under iteration, even in feedforward nets. Eventually, they  may, in fact, distort the results of the implementation to the point of making the  simulation useless, if not misleading.  455  456 Garzon and Botelho  There is, therefore, an important difference between the intended operation of an  idealized neural network and its observable behavior. This is a classical problem in  systems theory and it has been addressed in several ways. First, the classical no-  tions of distinguishability and observability in control theory (Sontag, 1990) which  roughly state that every pair of system's states are distinguishable by different out-  puts over evolution in finite time. This is thus a notion of local state observability.  More recently, several results have established more global notions of identit]abiI-  icy of discrete-time feedfoward (Sussmann, 1992; Chen, Lu, Hecht-Nelson, 1993)  and continuous-time recurrent neural nets (Albertini and Sontag, 1993a,b), which  roughly state that for given odd activation functions (such as tanh), the weights  of a neural network are essentially uniquely determined (up to permutation and  cell redundancies) by the input/output behavior of the network. These notions do  assume error-free inputs, weights, and activation functions.  In general, a computer simulation of an orbit of a given dynamical system in the  continuuum (known as a pseudo-orbit) is, in fact, far from the orbit in the ideal  system. Motivated by this problem, Birkhoff introduced the so-called shadowing  property. A system satisfies the shadowing property if all pseudo-orbits are uniformly  approximated by actual orbits so that the former capture the long-term behavior  of the system. Bowen showed that sufficiently hyperbolic systems in real euclidean  spaces do have the shadowing property (Bowen, 1978). However, it appears difficult  even to give a characterization of exactly which maps on the interval possess the  property -see e.g. (Coven, Kan, Yorke, 1988). Precise definitions of all terms can  be found in section 2.  By comparison to state observability and identifiability, the shadowing property is  a type of globaI obserYabiliy of a system through its dynamical behavior. Since  autonomous recurrent networks can be seen as dynamical systems, it is natural  to investigate this property. Thus, a neural net is observable in the sense that its  behavior (i.e. the sequence of its ideal actions on given initial conditions) can be  observed on computer simulations or discrete implementations, despite inevitable  concomitant approximations and errors.  The purpose of this paper is to explore this property as a deterministic model for  perturbations of neural network behavior in the presence of arbitrary small errors  from various sources. The model includes both discrete and analog networks. In  section 4 we sketch a proof that locally finite boolean neural networks (even with  an infinite number of neurons), are all observable in this sense. This is not true  in general for analog networks, and section 3 is devoted to sketching necessary and  sufficient conditions for the relatively few analog exceptions for the most common  transfer functions: hard-thresholds, a variety of sigmoids (hyperbolic tangent, lo-  gistic, etc.) and saturated linear maps. Finally, section 5 discusses the results and  poses some other problems worthy of further research.  2 DEFINITIONS AND MAIN RESULTS  This section contains notation and precise definitions in a general setting, so as to  include discrete-time networks both with discrete and continuous activations.  Let f: X --} X be a continuous map of a compact metric space with metric I*, * [.  Observability of Neural Network Behavior 457  The orbii of z 6 X is the sequence (z, f(z),..., re(z)...}, i.e. a sequence of points  {x}>o for which x '+ --- f(x), for all k _> 0. Given a number 5 > 0, a &pseudo-  orbi* is a sequence {x e} so that the distances I < for all k >_ 0.  Pseudo-orbits arise as trajectories of ideal dynamical processes contaminated by  errors and noise. In such cases, especially when errors propagate exponentially,  it is important to know when the numerical process is actually representing some  meaningful trajectory of the real process.  DerraiMon 2.1 The map f on a metric space X is (globally) obser'able (equiva-  lently, has he shadowing propery, or is raceable) if and only if for every e > 0  there exists a 6 > 0 so that any 6-pseudo-orbit {z e} is e-approximated by the orbit,  under f, of some point z  X, i.e. ]z e, fe(z)l < e for all k > O.  One might observe that computer simulations only run for finite time. On compact  spaces (as is the case below), observability can be shown to be equivalent to a similar  property of shadowing finite pseudo-orbits.  'Analog neural network' here means a finite number n of units (or cells), each of  which is characterized by an activation (sometimes called output) function cr i :  1 --* 1, and weigh* matrix W of synaptic strengths between the various units.  Units can assume real-valued activations i, which are updated synchronously and  simultaneously at discrete instants of time, according to the equation   ,(t + l) = (1)  The total activation of the network at any time is hence given by a vector  in  euclidean space R , and the entire network is characterized by a global dynamics  = (2)  where W denotes ordinary product and rr is the map acting as ai along the ith  component. This component in a vector  is denoted i (as opposed to z , the k th  term of a sequence). The unit hypercube in 1  is denoted I N. An analog network  is then defined as a dynamical system in a finite-dimensional euclidean space and  one may then call a neural network (globally) observable if its global dynamics is an  observable dynamical system. Likewise for boolean networks, which will be defined  precisely in section 4.  We end this section with some background facts about observability on the contin-  uum. It is perhaps surprising but a trivial remark that the identity map of the real  interval is not observable in this sense, since orbits remain fixed, but pseudo-orbits  may drift away from the original state and can, in fact, be dense in the interval.  Likewise, common activation functions of neural networks (such as hard thresholds  and logistic maps) are not observable. For linear maps, observability has long been  known to be equivalent to hyperboliciy (all eigenvalues A have 1). Composi-  tion of observable maps is usually not observable (take, for instance, a hyperbolic  homeomorphism and its inverse). In contrast, composition of linear and nonob-  servable activation functions in neural networks are, nevertheless, observable. The  main take-home message can be loosely summarized as follows.  Theorem 2.1 Except for a negligible fraction of exceptions, discrete-time analog  neural nets are observable. All discrete (boolean) neural networks are observable.  458 Garzon and Botelho  3 ANALOG NEURAL NETWORKS  This section contains (a sketch) of necessary and sufficient conditions for analog  networks to be observable for common types of activations functions.  3.1 HARD-THRESHOLD ACTIVATION FUNCTIONS  It is not hard to give necessary and sufficient conditions for observability of nets  with discrete activation functions of the type  1 ifu>_Oi  ai(u) := 0 else.  where Oi is a theshold characterizing cell i.  Lemma 3.1 A map f: R ' --} R ' with finite range is observable if and only if it  is continuous at each point of its range.  PROOF. The condition is clearly sufficient. If f is continuous at every point of its  range, small enough perturbations z + of an image fizz) have the same image  f(+) = f(f(a:)) and hence, for 5 small enough, every 5-pseudo-orbit is traced  by the first element of the pseudo-orbit. Conversely, assume f is not continuous at a  point of its range f(z). Let z , z2,... be a sequence constant under f whose image  does not converge to f(fo)) (s.ch a sequence can always be so chosen because  the range is discrete). Let  1  e:--- rain If(z),f(y)l.  For a given 5 > 0 the pseudo-orbit x,x,f(x),f2(x),... is not traceable for k  large enough. Indeed, for any z within e-distance of x , either f(z)  f(x), in which  case this distance is at least e, or else they coincide, in which case If2(z), f(x)] > e  anyway by the choice of x . []  Now we can apply Lemma 3.1 to obtain the following characterization.  Theorem 3.1 A discrete-time neural net T with weight matrix W := (wii) and  threshold vector 0 is observable if and only if for every y in the range ofT, (Wy)i   Oi for every i (1 < i < n).  3.2 SlGMOIDAL ACTIVATION FUNCTIONS  In this section, we establish the observability of arbitrary neural nets with a fairly  general type of sigmoidal activation functions, as defined next.  Definition 3.1 A map (7: It --} It is $igmoidM if it is strictly increasing, bounded  (above and below), and continuously differentiable.  Important examples are the logistic map  1  a(u) = 1 + exp(-pu) '  Observability of Neural Network Behavior 459  the arctan and the hyperbolic tangent maps  ai(u) --- arctan(pu) ai(u) = tanh(u) = exp(u) - exp(-u)  ' exp() + exp(-) '  Note that, in particular, the range of a sigmoidal map is an open and bounded  interval, which without loss of generality, can be assumed to be the unit interval  I. Indeed, if a neural net has weight matrix W and activation function a which is  conjugate to an activation function a  by a conjugacy , then  a o W ~  where ~ denotes conjugacy. One can, moreover, assume that the gain factors in the  sigmoid functions are all/ = 1 (multiply the rows of W).  Theorem 3.2 Every neural networks with a sigmoidal activation function has a  strong attractor, and in particular, it is observable.  PROOr'. Let a neural net with n cells have weight matrix W and sigmoidal  Consider a parametrlzed family {Tt}, of nets with sigmoidals given by a, :=/cr.  It is easy to see that each Tt (/ > 0) is conjugate to T. However, W needs to be  replaced by a suitable conjugation with a homeomorphism ,. By Brouwer's fixed  point theorem, Tt, has a fixed point p* in I n. The key idea in the proof is the fact  that the dynamics of the network admits a Lyapunov function given by the distance  from p*. Indeed,  II W(x) - W,,(p*) I1_< sup IJT. I II x - P* tl,  where J denotes jacobJan. Using the chain rule and the fact that the derivatives of  qb, and a, are bounded (say, below by b and above by B), the Jacobian satisfies  IJ,(y)l _< t,(bB)lWl,  where twI denotes the determinant of W. Therefore we can choose/ small enough  that the right-hand side of this expression is less than 1 for arbitrary y, so that T  is a contraction. Thus, the orbit of the first element in any -pseudo-orbit -traces  the orbit.  $.$ SATURATED-LINEAR ACTIVATION FUNCTIONS  The case of the nondifferentiable saturated linear sigmoid given by the piecewise  linear map  0, for u < 0  ri(u ) := u, forO_< u<_ 1 (3)  1, for u > 1  presents some difficulties. First, we establish a general necessary condition for  observability, which follows easily for linear maps since shadowing is then equivalent  to hyperbolicity.  Theorem $.$ If T leaves a segment of positive length pointwise fixed, then T is  not observable.  460 Garzon and Botelho  Although easy to see in the case of one-dimensional systems due to the fact that  the identity map is not observable, a proof in higher dimensions requires showing  that a dense pseudo-orbit in the fixed segment is not traceable by points outside  the segment. The proof makes use of an auxiliary result.  Lemma 3.2 A linear map L: R ' --* R ', acts along the orbit of a point z in the  unit hypercube either as an attractor to O, a repellop to infinity, or else as a rigid  rotation or reflection.  PROOF. By passing to the complexification L t: C" -. C" of L and then to a  conjugate, assume without loss of generality that L has a matrix in Jordan canonical  form with blocks either diagonal or diagonal with the first upper minor diagonal of  ls. It suffices to show the claim for each block, since the map is a cartesian product  of the restrictions to the subspaces corresponding to the blocks. First, consider the  diagonal case. If the eigenvalues I,kl < i (IAI > 1, respectively), clearly the orbit  L(z) -* 0 (1[ L(z) ]l-* oo). If ]l-- 1, L acts as a rotation. In the nondiagonal  case, it is easy to see that the iterates of a: -- (a:l, ..., Zm) are given by   --1  Lt(z) := E(})At-z+x +E(})At-+2+"'+At' (4)  k=0 k=0  The previous argument for the diagonal block still applies for ]A I  1. ff ]A I = 1  and if at least two components of z  I  are nonzero, then they are positive and  II L(4 II . In the remaining case, L acts as a rotation since it reduces to  multiplication of a single coordinate of z by A.   PROOF Or THEOREM 3.3. Assume that T = a o L and T leaves invariant a  segment  positive length. Suppose first that L leaves invariant the same segment  as well. By Lemma 3.2, a pseudo-orbit in the interior of the hypercube I  cannot  be traced by the orbit of a point in the hypercube. ff L moves the segment   invariant under T, we can assume without loss of generality it lies entirely on a  hyperplane face F of I  and the action of a on L() is just a projection over F.  But in that case, the action of T on the segment is a (composition of two) line  map(s) and the same argument applies.   We point out that, in particular, T may not be observable even ff W is hyperboSc.  The condition in Theorem 3.3 is, in fact, sucient. The proof is more involved and  is given in detail in (Gzon & Botelho, 1994). With Theorem 3.3 one can then  determine relatively simple necessary and sucient conditions for obseabflity (in  terms of the eigenvalues and determinants of a finite number of le maps). They  establish Theorem 2.1 for saturated-line activation functions.  4 BOOLEAN NETWORKS  This section contains precise definitions of discrete (boolean) neural networks and  a sketch of the proof that they are observable in general.  Discrete neural networks have a finite number of activations and their state sets are  endowed with an addition and multiplication. The activation function 51 (typically  Observability of Neural Network Behavior 461  a threshold function) can be given by an arbitrary boolean table, which may vary  from cell to cell. They can, moreover, have an infinite number of cells (the only case  of interest here, since finite booolean networks are trivially observable). However,  since the activation set if is finite, it only makes sense to consider locally  networks, for which every cell i only receives input from finltely many others.  A total state is now usually called a contlguration. A configuration is best thought  of as a hi-infinite sequence x := mx2a'" consisting of the activations of all cells  listed in some fixed order. The space of all configurations is a compact metric space  if endowed with any of a number of equivalent metrics, such as I , y[ :- b-, where  rn = inf{i: :i  Yi}. In this metric, a small perturbation of a configuration is  obtained by changing the values of  at pixels fax away from  The simplest question about observability in a general space concerns the shadowing  of the identity function. Observability of the identity happens to be a property  characteristic of configuration spaces. Recall that a totally disconnected topological  space is one in which the connected component of every element is itself.  Theorem 4.1 The identity map id of a compact metric space X is observable iff  X is totally disconnected.  The first step in the proof of Theorem 4.3 below is to characterize observability of  linear boolean networks (i.e. those obeying the superposltion principle).  Theorem 4.2 Every linear continuous map has the shadowing property.  For the other step we use a global decomposition T = F o L of the global dynamics  of a discrete network as a continuous transformation of configuration space due to  (Garzon & Franklin, 1990). The reader is referred to (Garzon & Botelho, 1992) for  a detailed proof of all the results in this section.  Theorem 4.3 Every discrete (boolean) neural network is observable.  5 CONCLUSION AND OPEN PROBLEMS  It has been shown that the particular combination of a linear map with an activa-  tion function is usually globally observable, despite the fact that neither of them  is observable and the fact that, ordinarily, composition destroys observabillty. In-  tuitively, this means that observing the input/output behavior of a neural network  will eventually give away the true nature of the network's behavior, even if the  network perturbs its behavior slighlty at each step of its evolution. In simple terms,  such a network cannot fool all the people all of the time.  The results are valid for virtually every type of autonomous first-order network  that evolves in discrete-time, whether the activations are boolean or continuous.  Several results follow from this characterization. For example, in all likelihood there  exist observable universal neural nets, despite the consequent undecidability of their  computational behavior. Also, neural nets are thus a very natural set of primitives  for approximation and implementation of more general dynamical systems. These  and other consequences will be explored elsewhere (Botelho g Garzon, 1994).  462 Garzon and Botelho  Natural questions arise from these results. First, whether observability is a general  property of most analog networks evolving in continuous time as well. Second, what  other type of combinations of nonobservable systems of more general types creates  observability, i.e. to what extent neural networks are peculiar in this regard. For  example, are higher-order neural networks observable? Those with sigma-pi units?  Finally, there is the broader question of robustness of neural network implementa-  tions, which bring about inevitable errors in input and/or weights. The results in  this paper give a deeper explanation for the touted robustness and fault-tolerance  of neural network solutions. But, further, they also seem to indicate that it may be  possible to require that neural net solutions have observable behavior as well, with-  out a tradeoff in the quality of the solution. An exact formulation of this question  is worthy of further research.  Acknowledgements  The work of the first author was partially done while on support from NSF grant  CCR-9010985 and CNRS-France.  References  F. Albertini and E.D. Sontag. (1993) Identifiability of discrete-time neural networks.  In Proc. European Control Conference, 460-465. Gronlngen, The Netherlands:  Springer-Verlag.  F. Albertini and E.D. Sontag. (1993) For neural networks, function determines  form. Neural Networks 6(7): 975-990.  F. Botelho and M. Garzon. (1992) Boolean Neural Nets are Observable, Memphis  State University: Technical Report 92-18.  F. Botelho and M. Garzon. (1994) Generalized Shadowing Properties. J. Random  and Computational Dynamics, in print.  R. Bowen. (1978) On Axiom A diffeomorphisms. In CBMS Regional Conference  Series in Math. 35. Providence, Rhode Island: American Math. Society.  A.M. Chen, H. Lu, and R. Hecht-Nielsen, (1993) On the Geometry of Feedforward  Neural Network Error Surfaces. Neural Computation 5(6): 910-927.  E. Coven, I. Kan, and J. Yorke. (1988) Pseudo-orbit shadowing in the family of  tent maps. Trans. AMS 1108: 227-241.  M. Garzon and S. P. Franklin. (1990) Global dynamics in neural networks II.  Complex Systems 4(5): 509-518.  M. Garzon and F. Botelho. (1994) Observability of Discrete-time Analog Networks,  preprint.  E.D. Sontag. (1990) Mathematical Control Theory: Deterministic Finite-  Dimensional Dynamical Systems. New York: Springer-Verlag.  H. Sussmann. (1992) Uniqueness of the Weights for Minimal Feedforward Nets with  a Given Input-Output Map. Neural Networks 5(4): 589-593.  
Robust Parameter Estimation And  Model Selection For Neural Network  Regression  Yong Liu  Department of Physics  Institute for Brain and Neural Systems  Box 1843, Brown University  Providence, RI 02912  yongcns. brown. edu  Abstract  In this paper, it is shown that the conventional back-propagation  (BPP) algorithm for neural network regression is robust to lever-  ages (data with = corrupted), but not to outliers (data with y  corrupted). A robust model is to model the error as a mixture of  normal distribution. The influence function for this mixture model  is calculated and the condition for the model to be robust to outliers  is given. EM algorithm [5] is used to estimate the parameter. The  usefulness of model selection criteria is also discussed. Illustrative  simulations are performed.  I Introduction  In neural network research, the back-propagation (BPP) algorithm is the most  popular algorithm. In the regression problem y : r(z, w) + e, in which r(m, 9)  denote a neural network with weight 9, the algorithm is equivalent to modeling  the error as identically independently normally distributed (i.i.d.), and using the  maximum likelihood method to estimate the parameter [13]. Howerer, the training  data set may contain surprising data points either due to errors in y space (outliers)  when the response vectors ys of these data points are far away from the underlying  function surface, or due to errors in a: space (leverages), when the the feature vectors  192  Robust Parameter Estimation and Model Selection for Neural Network Regression 193  ms of these data points are far away from the mass of the feature vectors of the rest  of the data points. These abnormal data points may be able to cause the parameter  estimation biased towards them. A robust algorithm or robust model is the one  that overcome the influence of the abnormal data points.  A lot of work has been done in linear robust regression [8, 6, 3]. In neural network.  it is generally believed that the role of sigmoidal function of the basic computing  unit in the neural net has some significance in the robustness of the neural net  to outliers and leverages. In this article, we investigate this more thoroughly. It  turns out the conventional normal model (BPP algorithm) is robust to leverages  due to sigmoidal property of the neurons, but not to outliers (section 2). From the  Bayesian point of view [2], modeling the error as a mixture of normal distributions  with different variances, with flat prior distribution on the variances, is more robust.  The influence function for this mixture model is calculated and condition for the  model to be robust to outliers is given (section 3.1). An efficient algorithm for  parameter estimation in this situation is the EM algorithm [5] (section 3.2). In  section 3.3, we discuss a choice of prior and its properties. In order to choose  among different probability models or different forms of priors, and neural nets  with different architecture, we discuss the model selection criteria in section 4.  Illustrative simulations on the choice of prior, or the t distribution model, and the  normal distribution model are given. Model selection statistics, is used to choose  the degree of freedom oft distribution, different neural network, and choose between  a t model and a normal model (section 4 and 5).  2  Issue Of Robustness In Normal Model For Neural Net  Regression  One way to think of the outliers and leverages is to regard them as a data per-  turbation on the data distribution of the good data. Remember that a estimated  parameter T = T(F) is an implicit function of the underlying data distribution F.  To evaluate the influence of T by this distribution perturbation, we use the influence  function [6] of estimator T at point z = (a, y) with data distribution F, which is  defined as  T((1 - t)F + tA) - T(F) (1)  IF(T, z, F) = lim_0+ t  in which A has mass 1 at z.1 This definition is equivalent to a definition of deriva-  tive with respect to F except what we are dealing now is the derivative of functional.  This definition gives the amount of change in the estimator T with respect to a dis-  tribution perturbation tAz at point z : (z, y). For a robust estimation of the  parameter, we expect the estimated parameter does not change significantly with  respect to a data perturbation. In another word, the influence function is bounded  for a robust estimation.  Denote the conditional probability model of y given z as i.i.d. f(yla, 8) with param-  eter 8. If the error function is the negative log-likelihood plus or not plus a penalty  term, then a general property of the influence function of the estimated parameter  , is IF(,(ai,yi),F) x Vlogf(yilai,) (for proof, see [11]). Denote the neural  The probability density of the distribution A is 5(y - a:).  194 Liu  net, with h hidden units and the dimension of the output being one (d  0): +  k=l  in which rr(z) is the sigmoidal function or 1/(1 + exp(z)) and O  = 1), as  (2)  -- {a, w,t/}.  For a normal model, f(ylz, O,a): c(y; v(m,O), cr ) in which A/'(y; c, rr) denotes  variate normal distribution with mean c and covariance matrix rr'I. Straightforward  calculation yield (dy = 1)  ir(,(,y),F)(y-v,)) aia(wiz+ti)z (3)  ale' (iz + ii) nx x  Since y with a large value makes the influence function unbounded, thus the normal  model or the back-propagation algorithm for regression is not robust to outliers.  Since a' (w +t) tends to be zero for z that is hr away from the projection z +i  O, the influence function is bounded for a abnormal , or the normal model for  regression is robust to leverages. This analysis can be easily extented to a neural  net with multiple hidden layers and multiple outputs. Since the neural net model  is robust to leverages, we shall concentrate on the discussion of robustness with  respect to outliers afterwards.  3 Robust Probability Model And Parameter Estimation  3.1 Mixture Model  One method for the robust estimation is by the Bayesian analysis [2]. Since our  goal is to overcome the influence of outliers in the data set, we model the error as  a mixture of normal distributions, or,  f(y[z,O, cr) = j f(yl,O,q, cr)r(q)dq (4)  with f(ylz, O,q, cr)= A/'(y;,(z,O),c?/q)and the prior distribution on q is denoted  as r(q). Intuitively, a mixture of different normal distributions with different qs, or  different variances, somehow conveys the idea that a data point is generated from  a normal distribution with large variance, which can be considered to be outliers,  or from that with small variance, which can be considered to be good data. This  requires r(q) to be fiat to accommodate the abnormal data points. A case of  extreme non-flat prior is to choose r(q) = 5(q - 1), which will make f(ylz, 0, cr) to  be a normal distribution model. This model has been discussed in previous section  and it is not robust to outliers.  Calculation yields (dy = 1) the influence function as  IF(, (,y),F)  (y- v,))   airy (iz + ii) hxl  (5)  Robust Parameter Estimation and Model Selection for Neural Network Regression 195  in which  b: E [qly, z, cr,] (6)  where expectation is taken with respect to the posterior distribution of q, or  ,r(qly , z, cr, 8) = f(yl,0,q,o)(q) For the influence function to be bounded for a y  with large value, (- r(m,g))b must be bounded. This is the condition on ,r(q)  when the distribution f(ylm, 8, or) is robust to outliers. It can be noticed that the  mixture model is robust to leverages for the same reason as in the case of the normal  distribution model.  3.2 Algorithm For Parameter Estimation  An efficient parameter estimation method for the model in equation 4 is the EM  algorithm [5]. In EM algorithm, a portion of the parameter is regarded as the miss-  ing observations. During the estimation, these missing observations are estimated  through previous estimated parameter of the model. Afterwards, these estimated  missing observations are combined with the real observations to estimate the pa-  rameter of the model. In our mixture model, we shall regard {qi, i: 1, ...n} as the  missing observations. Denote w: {mi, yi, i - 1, ...n} as the training data set.  It is a straight forward calculation for the EM algorithm (see Liu, 1993b) once one  write down the full probability f({yi,qi}l{:ri},cr,$). The algorithm is equivalent to  mmmmng  1 2  0))  i=1  _   (, _  and estimating cr at the s step by (5') (') -  Ei:i wi -1)(yi (i, ))2.  If we use/(ylm, 0, ) cr exp(-p(ly-(m, 0)l/)) and denote p(z) = p' (z), calculation  yield, w = E [qly, m c, 8] - () I=fy-.(,0)t/o. This has exact the same choice of  ' z  weight w? -1) as in the iterative reweighted regression algorithm [7]. What we have  here, different from the work of Holland et al., is that we link the EM algorithm  with the iterative reweighted algorithm, and also extend the algorithm to a much  more general situation. The weighting wi provides a measure of the goodness of a  data point. Equation 7 estimates the parameters based on the portion of the data  that are good. A penalization term on 9 can also be included in equation 7. 2  3.3 Choice Of Prior  There are a lot choices of prior distribution ,r(q) (for discussion, see [11]). We  only discuss the choice vq  X, i.e., a chi distribution with v degree of freedom.  By intergrating equation 4, f(ylm, 8, rr) - r((+a)/.) (1 + (Y-v('))) -(+a)/  r(/2)(a)/ a '  It is a dy variate t distribution with v degree of freedom, mean 0 and covariance  y+d  matrix aI. Calculation yields, E [qy, , a, $]: +(y-v(,0))/ The t distribution  2A prior on 0 can be '(0) cre -'(x')/(2), which yields a additional penalization term  a(A, 0) in equation 7, in which A denotes a tunning parameters of the penalization.  196 Liu  becomes a normal distribution as v goes to infinity. For finite v, it has heavier tail  than the normal distribution and thus is appropriate for regression with to outliers.  Actually the condition for robustness, (y- r/(z,t)) being bounded for a y with  large value, is satisfied. The weighting w o 1/{1 + y- r/(z,) /&2} balances the  influence of the ys with large values, achieving robustness with respect to outliers  for the t distribution.  4 Model Selection Criteria  The meaning of model is in a broad sense. It can be the degree of penalization, or  a probability model, or a neural net architecture, or the combination of the above.  A lot of work has been done in model selection [1, 17, 15, 4, 13, 14, 10, 12]. The  choice of a model is based on its prediction ability. A natural choice is the expected  negative log-likelihood. This is equivalent to using the Kullback-Leibler measure [9]  for model selection, or -E [logf(ylz , model)] + E [log f(ylz, true model)]. This has  problem if the model can not be normalized as in the case of a improper prior.  Equation 7 implies that we can use  1 (s)  as the cross-validation [16] assessment of model m, in which ,:  w;, w; is  the convergence limit of w? ), or equation 6, and _i is the estimator of $ with  th data deleted from the full data set. The successfulhess of the cross-validation  method depends on a robust parameter estimation. The cross-validation method is  to calculate the average prediction error on a data based on the rest of the data in  the training data set. In the presence of outliers, predicting an outlier based on the  rest of the data, is simply not meaningfil in the evaluation of the model. Equation  8 takes consideration of the outliers. Using result from [10], we can show [11] with  penalization term a(;, 0),  in which  models in comparison contains a improper prior, the above model selection statistics  can be used.  If the models in comparison have close forms of f(yl:, O, cr), the average negative  log-likelihood can be used as the model selection criteria. In Liu's work [10], an  approximation form for the unbiased estimation of expected negative log-likelihood  was provided. If we use the negative log-likelihood plus a penalty term a(;, 0) as  the parameter optimization criteria, the model selection statistics is  '* 1 '* 1  T(w) = 1 1ogf(yili,-i)  1ogf(Yilmi,) + -Tr(C-1D) (11)  Robust Parameter Estimation and Model Selection for Neural Network Regression 197  in which C = Y]ix Volgf(YilZi,)Vlgf(YitZi, tj)  and D = -Y]ix VoVlogf(yilzi,)+ VoX7c(Jk,0). The optimal model is the  one that minimizes this statistics. If the true underlying distribution is the normal  distribution model and there is no penalization terms, it is easy to prove C --, D as  n goes to infinite. Then the statistics becomes AIC [1].  2  1.5  1  0.5  0  -0.5  -1.5  0  ?P fit without leverages .....  _ data se with le[erage o I I I  1 2 3 4 5  o o   o  o  Figure 1: BPP fit to data set with leverages, and comparison with BPP fit to the  data set without the leverages. An one hidden layer neural net with 4 hidden units,  is fitted to a data set with 10 leverage, which are on the right side of m = 3.5, by  using the conventional BPP method. The main body of the data (90 data points)  was generated from y: sin(z) + e with e  ,V(e;O, rr = 0.2). It can be noticed  that the fit on the part of good data points was not dramatically influenced by the  leverages. This verified our theoretical result about the robustness of a neural net  with respect to leverages  5 Illustrative Simulations  For the results shown in figure 2 and 3, the training data set contains 93 data point  from y -- sin(z)+e and seven y values (outliers) randomly generated from region [1,  2), in which e -- A:(e; 0, rr: 0.2). The neural net we use is of the form in equation  2. Denote h as the number of hidden units in the neural net. The caption of each  figure (1, 2, 3) explains the usefulness of the parameter estimation algorithm and  the model selection.  Acknowledgement s  The author thanks Leon N Cooper, M.P. Perrone. The author also thanks his wife  Cong. This research was supported by grants from NSF, C)NR and ARC).  References  [1] H. Akaike.  Information theory and an extension of the maximum likelihood  198 Liu  1.1  1  0.9  0.8  0.7  0.6  0.5  0.4  - statisti  - MSE on the test set (x 10- -  (3,3)(2,3)(4,3)(2,4)(3,4)(5,3)(3,5)( 1,3)(3,7)(rh 3Xn,4Xrh5Xn,7)  models (v, h), n stands for normal distribution model (BPP fit)  Figure 2: Model selection statistics T, for fits to data set with outliers, tests on  a independent data set with 1000 data points from y = sin(z) + , where    A/'(e; 0, cr = 0.2). it can be seen that T, statistics is in consistent with the error on  the test data set. The T, statistics favors t model with small v than for the normal  distribution models.  Y  2  1.5  1  0.5  0  -0.5  -1  -1.5  3PP fit without outliers .....  - da.t_a sf. wi_t_h mf_.!i_ers o  0 1 2 3  o I I I I  I o I  _ 0  O0 CO 0 0  0 _0 '_ 0 O0 0  ......  . o o  o o  4 5 6 7  Figure 3: Fits to data set with outliers, and comparison with BPP fit to the data  set without the outliers. The best fit in the four BPP fits (h = 3), according to T,  statistics, was influenced by the outliers, tending to shift upwards. Although the  distribution is not a t distribution at all, the best fit by the EM algorithm under  the t model (v = 3, h = 3), also according to T, statistics, gives better result than  the BPP fit, actually is almost the same as the BPP fit (h = 3) to the training data  set without the outliers. This is due to the fact that a t distribution has a heavy  tail to accommodate the outliers  Robust Parameter Estimation and Model Selection for Neural Network Regression 199  [10]  [11]  [12]  [13]  [15]  [16]  principle. In Petroy and Czaki, editors, Proceedings of the 2nd International  Symposium on Information Theory, pages 267-281, 1973.  [2] J. O. Berger. Statistical Decision Theory and Bayesian Analysis. Springer-  Verlag, 1985.  [3] R. D. Cook and S. Weisberg. Characterization of an empirical influence func-  tion for detecting influential cases in regression. Technometrics, 22:495-508,  1980.  [4] P. Craven and G. Wahba. Smoothing noisy data with spline func-  tions:estimating the correct degree of smoothing by the method of generalized  cross-validation. Numer. Math., 31:377-403, 1979.  [5] A. P. Dempster, N.M. Laird, and D. B. Rubin. Maximum likelihood from  incomplete data via the EM algorithm. (with discussion). J. Roy. Star. Soc.  Set. B, 39:1-38, 1977.  [6] F.R. Hampel, E.M. Rouchetti, P.J. Rousseeuw, and W.A. Stahel. Robust Statis-  tics: The approach based on influence functions. Wiley, 1986.  [7] P.J. Holland and R.E. Welsch. Robust regression using iteratively reweighted  least-squares. Commun. star. A, 6:813-88, 1977.  [8] P.J. Huber. Robust Statistics. New York: Wiley, 1981.  [9] S. Kullback and R.A. Leibler. On information and sufficiency. Ann. Star.,  22:79-86, 1951.  Y. Liu. Neural Network Model Selection Using Asymptotic Jackknife Estima-  tor and Cross-Validation Method. In C.L. Giles, S.J.and Hanson, and J.D.  Cowan, editors, Advances in neural information processing system 5. Morgan  Kaufmann Publication, 1993.  Y. Liu. Robust neural network parameter estimation and model selection for  regression. Submitted., 1993.  Y. Liu. Unbiased estimate of generalization error and model selection criterion  in neural network. Submitted to Neural Network, 1993.  D. MacKay. Bayesian methods for adaptive models. PhD thesis, California  Institute of Technology, 1991.  [14] J. E. Moody. The effective number of parameters, an analysis of generalization  and regularization in nonlinear learning system. In J. E. Moody, S. J. Hanson,  and R. P. Lippmann, editors, Advances in neural information processing system  4, pages 847-854. Morgan Kaufmann Publication, 1992.  G. Schwartz. Estimating the dimension of a model. Ann. Star, 6:461-464, 1978.  M. Stone. Cross-validatory choice and assessment of statistical predictions  (with discussion). J. Roy. Star. Soc. Set. B, 36:111-147, 1974.  [17] M. Stone. An asymptotic equivalence of choice of model by cross-validation  and Akaike's criterion. J. Roy. Star. Soc., Set. B, 39(1):44-47, 1977.  
Bayesian Backprop in Action:  Pruning Committees Error Bars  and an Application to Spectroscopy  Hans Henrik Thodberg  Danish Meat Research Institute  Maglegaardsvej 2, DK-4000 Roskilde  hodbergnn. meare. dk  Abstract  MacKay's Bayesian framework for backpropagation is conceptually  appealing as well as practical. It automatically adjusts the weight  decay parameters during training, and computes the evidence for  each trained network. The evidence is proportional to our belief  in the model. The networks with highest evidence turn out to  generalise well. In this paper, the framework is extended to pruned  nets, leading to an Ockham Factor for "tuning the architecture  to the data". A committee of networks, selected by their high  evidence, is a natural Bayesian construction. The evidence of a  committee is computed. The framework is illustrated on real-world  data from a near infrared spectrometer used to determine the fat  content in minced meat. Error bars are computed, including the  contribution from the dissent of the committee members.  I THE OCKHAM FACTOR  William of Ockham's (1285-1349) principle of economy in explanations, can be  formulated as follows:  If several theories account for a phenomenon we should prefer the  simplest which describes the data su.fficiently well.  208  Bayesian Backprop in Action 209  The principle states that a model has two virtues: simplicity and goodness of fit.  But what is the meaning of "sufficiently well" - i.e. what is the optimal trade-off  between the two virtues? With Bayesian model comparison we can deduce this  trade-off.  We express our belief in a model as its probability given the data, and use Bayes'  formula:  P(7/ID ) = P(DI7/)P(7/) (1)  We assume that the prior belief P(7/) is the same for all models, so we can compare  models by comparing P(D 17/) which is called the evidence for 7/, and acts as a  quality measure in model comparison.  Assume that the model has a single tunable parameter w with a prior range Awprior  so that P(w17/) = 1/Awprior. The most probable (or maximum posterior) value  wxr of the parameter w is given by the maximum of  P(wlD, 7/)-- P(DIw,7/)P(w17/)  p(Di7/) (2)  The width of this distribution is denoted Awpoaterio r. The evidence P(D I 7/) is  obtained by integrating over the posterior w distribution and approximating the  integral:  P(D[7/) = / P(Dlw,7/)P(w17/)dw (3)  = P(DlwuP,7/)A? (4)  /x Wprior  Evidence = Likelihood x OckhamFactor (5)  The evidence for the model is the product of two factors:  The best fit likelihood, i.e. the probability of the data given the model and  the tuned parameters. It measures how well the tuned model fits the data.  The integrated probability of the tuned model parameters with their un-  certainties, i.e. the collapse of the available parameter space when the data  is taken into account. This factor is small when the model has many pa-  rameters or when some parameters must be tuned very accurately to fit  the data. It is called the Ockham Factor since it is large when the model is  simple.  By optimizing the modelling through the evidence framework we can avoid the  overfitting problem as well as the equally important "underfitting" problem.  2 THE FOUR LEVELS OF INFERENCE  In 1991-92 MacKay presented a comprehensive and detailed framework for combi-  ning backpropagation neural networks with Bayesian statistics (MacKay, 1992). He  outlined four levels of inference which applies for instance to a regression problem  where we have a training set and want to make predictions for new data:  210 Thodberg  Level 1 Make predictions including error bars for new input data.  Level 2 Estimate the weight parameters and their uncertainties.  Level 3 Estimate the scale parameters (the weight decay parameters and the noise  scale parameter) and their uncertainties.  Level 4 Select the network architecture and for that architecture select one of the  w-minima. Optionally select a committee to reflect the uncertainty on this  level.  Level i is the typical goal in an application. But to make predictions we have to  do some modelling, so at level 2 we pick a net and some weight decay parameters  and train the net for a while. But the weight decay parameters were picked rather  arbitrarily, so on level 3 we set them to their inferred maximum posterior (MP)  value. We alternate between level 2 and 3 until the network has converged. This is  still not the end, because also the network architecture was picked rather arbitrarily.  Hence level 2 and 3 are repeated for other architectures and the evidences of these  are computed on level 4. (Pruning makes level 4 more complicated, see section 6).  When we make inference on each of these levels, there are uncertainties which are  described by the posterior distributions of the parameters which are inferred. The  uncertainty on level 2 is described by the Hessian (the second derivative of the net  cost function with respect to the weights). The uncertainty on level 3 is negligible  if the number of weight decays parameters is small compared to the number of  weights. The uncertainty on level 4 is described by the committee of networks with  highest evidence within some margin (discussed below).  The uncertainties are used for two purposes. Firstly they give rise to error bars on  the predictions on level 1. And secondly the posterior uncertainty divided by the  prior uncertainty (the Ockham Factor) enters the evidence.  MacKay's approach differs in two respects from other Bayesian approaches to neural  nets:  It assumes the Gaussian approximation to the posterior weight distribution.  In contrast, the Monte Carlo approach of (Neal, 1992) does not suffer from  this limitation.  It determines maximum posterior values of the weight decay parameters,  rather than integrating them out as done in (Buntine and Weigend, 1991).  It is difficult to justify these choices in general. The Gaussian approximation is  believed to be good when there are at least 3 training examples per weight (MacKay,  1992). The use of MP weight decay parameters is the superior method when there  are ill-defined parameters, as there usually is in neural networks, where some weights  are typically poorly defined by the data (MacKay, 1993).  3 BAYESIAN NEURAL NETWORKS  The training set D consists of N cases of the form (x, t). We model t as a function  of x, t = y(x) + t,, where t, is Gaussian noise and y(x) is computed by a neural  Bayesian Backprop in Action 211  network 7/ with weights w. The noise scale is a free parameter/ = 1/a. The  probability of the data (the likelihood) is  P(DIw,/,7/) cr exp(-/Eo) (6)  - ! (7)  ED = 2  where the sum extends over the N cases.  In Bayesian modelling we must specify the prior distribution of the model param-  eters. The model contains k adjustable parameters w, called weights, which are  in general split into several groups, for instance one per layer of the net. Here we  consider the case with all weights in one group. The general case is described in  (MacKay, 1992) and in more details in (Thodberg, 1993). The prior of the weights  w is  P(wl,,7/)  exp(-/Ew) (8)  -- !Ew2  Ew = 2 (9)  / and  are called the scales of the model and are free parameters determined by  the data.  The most probable values of the weights given the data, some values of the scales  (to be determined later) and the model, is given by the maximum of  P(wlD,/,,7/) = P(DIw,/,,7/)P(wI/,,7/)  P(DI,,7/) cr exp(-/C) (10)  C -- ED+6Ew (11)  So the maximum posterior weights according to the probabilistic interpretation are  identical to the weights obtained by minimising the familiar cost function C with  weight decay parameter 6. This is the well-known Bayesian account for weight  decay.  4 MACKAY'S FORMULAE  The single most useful result of MacKay's analysis is a simple formula for the MP  value of the weight decay parameter  ED 7  Mr = -- (12)  Err N-7  where 7 is the number of well-determined parameters which can be approximated  by the actual number of parameters k, or computed more accurately from the  eigenvalues hi of the Hessian VVED:  k  (13)  i--1  The MP value of the noise scale is/Mr = N/(2C).  212 Thodberg  The evidence for a neural network 7/ is, as in section 1, obtained by integration  over the posterior distribution of the inferred parameters, which gives raise to the  Ockham Factors:  Ev(7) = log P(DI)  log Ock(w)  Ock(/)  _ N - 7 N log 47r__C  2 2 N  + log Ock(w) + log Ock(/) + log Ock({) (14)  k  __ 1 MP 7 h! 15)  - Zlg +log +hlog2 (  = V"4'/( N - 7) Ock({) = V/-/7 (16)  log 2 log 2  The first line in (14) is the log likelihood. The Ockham Factor for the weights  Ock(w) is small when the eigenvalues Ai of the Hessian are large, corresponding to  well-determined weights.  is the prior range of the scales and is set (subjectively)  to 10 a.  The expression (15) (valid for a network with a single hidden layer) contains a  symmetry factor h!2 a. This is because the posterior volume must include all w  configurations which are equivalent to the particular one. The hidden units can be  permuted, giving a factor h! more posterior volume. And the sign of the weights to  and from every hidden unit can be changed giving 2 a times more posterior volume.  5 COMMITTEES  For a given data set we usually train several networks with different numbers of  hidden units and different initial weights. Several of these networks have evidence  near or at the maximal value, but the networks differ in their predictions. The  different solutions are interpreted as components of the posterior distribution and  the correct Bayesian answer is obtained by averaging the predictions over the solu-  tions, weighted by their posterior probabilities, i.e. their evidences. However, the  evidence is not accurately determined, primarily due to the Gaussian approxima-  tion. This means that instead of weighting with Ev(7/) we should use the weight  exp(log Ev/A(log Ev)), where A(log Ev) is the total uncertainty in the evaluation  of log Ev. As an approximation to this, we define the committee as the models  with evidence larger than log Evmax- A log Ev, where Evmax is the largest evidence  obtained, and all members enter with the same weight.  To compute the evidence Ev() of the committee, we assume for simplicity that all  networks in the committee C share the same architecture. Let Nc be the number  of truly different solutions in the committee. Of course, we count symmetric reali-  sations only once. The posterior volume i.e. the Ockham Factor for the weights is  now Nc times larger. This renders the committee more probable - it has a larger  evidence:  log Ev(C) = log Nc + log Ev(7/) (17)  where log Ev(7) denotes the average log evidence of the members. Since the evi-  dence is correlated with the generalisation error, we expect the committee to gene-  ralise better than the committee members.  Bayesian Backprop in Action 213  6 PRUNING  We now extend the Bayesian framework to networks which are pruned to adjust the  architecture to the particular problem. This extends the fourth level of inference.  At first sight, the factor h! in the Ockham Factor for the weights in a sparsely con-  nected network appears to be lost, since the network is (in general) not symmetric  with respect to permutations of the hidden units. However, the symmetry reappears  because for every sparsely connected network with tuned weights there are h! other  equivalent network architectures obtained by permuting the hidden units. So the  factor h! remains. If this argument is not found compelling, it can be viewed as an  assumption.  If the data are used to select the architecture, which is the case in pruning designed  to minimise the cost function, an additional Ockham Factor must be included.  With one output unit, only the input-to-hidden layer is sparsely connected, so  consider only these connections. Attach a binary pruning parameter to each of  the m potential connections. A sparsely connected architecture is described by  the values of the pruning parameters. The prior probability of a connection to be  present is described by a hyperparameter qb which is determined from the data i.e.  it is set to the fraction of connections remaining after pruning (notice the analogy  between qb and a weight decay parameter). A non-pruned connection gives an  Ockham Factor qb and a pruned 1 - qb, assuming the data to be certain about the  architecture. The Ockham Factors for the pruning parameters is therefore  log Ock(pruning) = m(qbMr log qbr + (1 - qbr) log(1 - qbr)) (18)  The tuning of the meta-parameter to the data gives an Ockham factor Ock(qb) m  V//rn, which is rather negligible.  From a minimum description length perspective (18) reflects the extra information  needed to describe the topology of a pruned net relative to a fully connected net. It  acts like a barrier towards pruning. Pruning is favoured only if the negative contri-  bution log Ock(pruning) is compensated by an increase in for instance log Ock(w).  7 APPLICATION TO SPECTROSCOPY  Bayesian Backprop is used in a real-life application from the meat industry. The  data were recorded by a Tecator near-infrared spectrometer which measures the  spectrum of light transmitted through samples of minced pork meat. The ab-  sorbance spectrum has 100 channels in the region 850-1050 nm. We want to calibrate  the spectrometer to determine the fat content. The first 10 principal components  of the spectra are used as input to a neural network.  Three weight decay parameters are used: one for the weights and biases of the  hidden layer, one for the connections from the hidden to the output layer, and one  for the direct connections from the inputs to the output as well as the output bias.  The relation between test error and log evidence is shown in figure 1. The test error  is given as standard error of prediction (SEP), i.e. the root mean square error. The  12 networks with 3 hidden units and evidence larger than -270 are selected for a  214 Thodberg  ! I I I   1 hidden unit   2 hidden units   3 hidden units  X 4 hidden units   6 hidden units  [] 8 hidden units  X  X   0   X   x  o    x  [] [] x  x  [] J'x x  -320 -300 -280 -260  log Evidence  Figure 1' The test error as a function of the log evidence for networks trained on  the spectroscopic data. High evidence implies low test error.  committee. The committee average gives 6% lower SEP than the members do on  average, and 21% lower SEP than a non-Bayesian analysis using early stopping (see  Thodberg, 1993).  Pruning is applied to the networks with 6 hidden units. The evidence decreases  slightly, i.e. Ock(pruning) dominates. Also the SEP is slightly worse. So the  evidence correctly suggests that pruning is not useful for this problem.   The Bayesian error bars are illustrated for the spectroscopic data in figure 2. We  study the model predictions on the line through input space defined by the second  principal component axis, i.e. the second input is varied while all other inputs are  zero. The total prediction variance for a new datum x is  2  = ,,-,, + +  (19)  where O'wu comes from the weight uncertainties (level 2) and acu from the com-  mittee dissent (level 4).  1For artificial data generated by a sparsely connected network the evidence correctly  points to pruned nets as better models (see Thodberg, 1993).  Bayesian Backprop in Action 215  i I  " '" Commitlee Predion  '.   '. Tot.J Uncertainty   '.   '.  \ ..   '. 10 ' Total Uncertainty  10 ' Random Noise  I i i  \ '.10 ' Committee Uncertainly  \.  \.  \.  "'.... 10 * Weight Unceaeinty  \,  x'x. -. ................................................. "  //  //  .: //  :' //  :" //  ",/  /  -4 -2 0 2 4  econd Principal Component  Figure 2: Prediction of the fat content as a function of the second principal com-  ponent P2 of the NIR spectrum. 95% of the training data have Ip21 < 2. The total  error bars are indicated by a "1 sigma" band with the dotted lines. The total stan-  dard errors (rtotal(X) and the standard errors of its contributions ((rv, (rwu(x) and  acu(x)) are shown separately, multiplied by a factor of 10.  References  W.L.Buntine and A.S.Weigend, "Bayesian Back-Propagation", Complex Systems 5,  (1991) 603-643.  R.M.Neal, "Bayesian Learning via Stochastic Dynamics", Neural Information Pro-  cessing Systems, Vol. 5, ed. C.L.Giles, S.J.Hanson and J.D.Cowan (Morgan Kauf-  mann, San Mateo, 1993)  D.J.C.MacKay, "A Practical Bayesian Framework for Backpropagation Networks"  Neural Comp. 4 (1992) 448-472.  D.J.C.MacKay, paper on Bayesian hyperparameters, in preparation 1993.  H.H.Thodberg, "A Review of Bayesian Backprop with an Application to  Near Infrared Spectroscopy" and "A Bayesian Approach to Pruning of Neu-  ral Networks", submitted to IEEE Transactions of Neural Networks 1993 (in  /pub/neuroprose/thodberg.ace-of-bayes*.ps.Z on archive.cis.ohio-state.edu).  
Figure of Merit Training for Detection and  Spotting  Eric I. Chang and Richard P. Lippmann  MIT Lincoln Laboratory  Lexington, MA 02173-0073, USA  Abstract  Spotting tasks require detection of target patterns from a background of  richly varied non-target inputs. The performance measure of interest for  these tasks, called the figure of merit (FOM), is the detection rate for  target patterns when the false alarm rate is in an acceptable range. A  new approach to training spotters is presented which computes the FOM  gradient for each input pattern and then directly maximizes the FOM  using backpropagation. This eliminates the need for thresholds during  training. It also uses network resources to model Bayesian a posteriori  probability functions accurately only for patterns which have a  significant effect on the detection accuracy over the false alarm rate of  interest. FOM training increased detection accuracy by 5 percentage  points for a hybrid radial basis function (RBF) - hidden Markov model  (HMM) wordspotter on the credit-card speech corpus.  1 INTRODUCTION  Spotting tasks require accurate detection of target patterns from a background of richly var-  ied non-target inputs. Examples include keyword spotting from continuous acoustic input,  spotting cars in satellite images, detecting faults in complex systems over a wide range of  operating conditions, detecting earthquakes from continuous seismic signals, and finding  printed text on images which contain complex graphics. These problems share three com-  mon characteristics. First, the number of instances of target patterns is unknown. Second,  patterns from background, non-target, classes are varied and often difficult to model accu-  rately. Third, the performance measure of interest, called the figure of merit (FOM), is the  detection rate for target patterns when the false alarm rate is over a specified range.  Neural network classifiers are often used for detection problems by training on target and  background classes, optionally normalizing target outputs using the background output,  1019  1020 Chang and Lippmann  PUTATIVE HITS  l A l"  I NORMALIZATION AND THRESHOLDING I  t" t  CLASSIFIER  INPUT PATTERN  Figure 1. Block diagram of a spotting system.  and thresholding the resulting score to generate putative hits, as shown in Figure 1. Putative  hits in this figure are input patterns which generate normalized scores above a threshold.  We have developed a hybrid radial basis function (RBF) - hidden Markov model (HMM)  keyword spotten This wordspotter was evaluated using the NIST credit card speech data-  base as in (Rohlicek, 1993, Zeppenfeld, 1993) using the same train/evaluation split of the  training conversations as was used in (Zeppenfeld, 1993). The system spots 20 target key-  words, includes one general filler class, and uses a Viterbi decoding backtrace as described  in (Lippmann, 1993) to backpropagate errors over a sequence of input speech frames. The  performance of this spotting system and its improved versions is analyzed by plotting de-  tection versus false alarm rate curves as shown in Figure 2. These curves are generated by  adjusting the classifier output threshold to allow few or many putative hits. Wordspotter pu-  tative hits used to generate Figure 2 correspond to speech frames when the difference be-  tween the cumulative log Viterbi scores in output HMM nodes of word and filler models is  above a threshold. The FOM for this wordspotter is defined as the average keyword detec-  tion rate when the false alarm rate ranges from 1 to 10 false alarms per keyword per hour.  The 69.7% figure of merit for this system means that 69.7% of keyword occurrences are  detected on the average while generating from 20 to 200 false alarms per hour of input  speech.  2 PROBLEMS WITH BACKPROPAGATION TRAINING  Neural network classifiers used for spotting tasks can be trained using conventional back-  propagation procedures with 1 of N desired outputs and a squared error cost function. This  approach to training does not maximize the FOM because it attempts to estimate Bayesian  a posteriori probability functions accurately for all inputs even if a particular input has little  effect on detection accuracy at false alarm rates of interest. Excessive network resources  may be allocated to modeling the distribution of common background inputs dissimilar  from targets and of high-scoring target inputs which are easily detected. This problem can  be addressed by training only when network outputs are above thresholds. This approach is  problematic because it is difficult to set the threshold for different keywords, because using  fixed target values of 1.0 and 0.0 requires careful normalization of network output scores to  prevent saturation and maintain backpropagation effectiveness, and because the gradient  calculated from a fixed target value does not reflect the actual impact on the FOM.  Figure of Merit Training for Detection and Spotting 1021  Figure 2.  z  o  lOO  90  80  70  60  50  40  30  20  lO  o  o  A SPLIT OF CREDIT-CARD  TRAINING DATA  .  FOM BACK-PROP (FOM: 69.7%)  /  ........ EMBEDDED REESTIMATION (FOM: 64.5%)  ISOLATED WORD TRAIN (FOM: 62.5%)  2 4 6 8 10  FALSE ALARMS PER KW PER HR  Detection vs. false alarm rate curve for a 20-word hybrid wordspotter.  Figure 3 shows the gradient of true hits and false alarms when target values are set to be 1.0  for true hits and 0.0 for false alarms, the output unit is sigmoidal, and the threshold for a  putative hit is set to roughly 0.6. The gradient is the derivative of the squared error cost with  respect to the input of the sigmodal output unit. As can be seen, low-scoring hits or false  alarms that may affect the FOM are ignored, the gradient is discontinuous at the threshold,  the gradient does not fall to zero fast enough at high values, and the relative sizes of the hit  and false alarm gradients do not reflect the true effect of a hit or false alarm on the FOM.  3 FIGURE OF MERIT TRAINING  A new approach to training a spotter system called "figure of merit training" is to directly  compute the FOM and its derivative. This derivative is the change in FOM over the change  in the output score of a putative hit and can be used instead of the derivative of a squared-  error or other cost function during training. Since the FOM is calculated by sorting true hits  and false alarms separately for each target class and forming detection versus false alarm  curves, these measures and their derivatives can not be computed analytically. Instead, the  FOM and its derivative are computed using fast sort routines. These routines insert a new  THRESHOLD  il GRADIENT  0.2  z  0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9  OUTPUT VALUE  Figure 3. The gradient for a sigmoid output unit when the target value for true hits is set to  1.0 and the target value for false alarms is set to 0.0.  1022 Chang and Lippmann  putative hit into an already sorted list and calculate the change in the FOM caused by that  insertion. The running putative hit list used to compute the FOM is updated after every new  putative hit is observed and it must contain all putative hits observed during the most recent  past training cycle through all training patterns. The gradient estimate is smoothed over  nearby putative hit scores to account for the quantized nature of detection versus false alarm  rate curves.  Figure 4 shows plots of linearly scaled gradients for the 20-word hybrid wordspotter. Each  value on the curve represents the smoothed change in the FOM that occurs when a single  hit or false alarm with the specified normalized log output score is inserted into the current  putative hit list. Gradients are positive for putative hits corresponding to true hits and neg-  ative for false alarms. They also fall off to zero for putative hits with extremely high or low  scores. Shapes of these curves vary across words. The relative importance of a hit or false  alarm, the normalized output score which results in high gradient values, and the shape of  the gradient curve varies. Use of a squared error or other cost function with sigmoid output  nodes would not generate this variety of gradients or automatically identify the range of pu-  tative hit scores where gradients should be high. Application of FOM training requires only  the gradients shown in these curves with no supplementary thresholds. Patterns with low  and high inputs will have a minimal effect during training without using thresholds because  they produce gradients near zero.  Different keywords have dramatically different gradients. For example, credit-card is long  and the detection rate is high. The overall FOM thus doesn't change much if more true hits  are found. A high scoring false alarm, however, decreases the FOM drastically. There is thus  a large negative gradient for false alarms for credit-card. The keywords account and check  are usually short in duration and thus more difficult to detect, thus any increase in number  of true hits strongly increases the overall FOM. On the other hand, since in this database,  the words account and check occur much less frequently than credit-card, a high scoring  false alarm for the words account and check has less impact on the overall FOM. The gra-  dient for false alarms for these words is thus correspondingly smaller. Comparing the  curves in Figure 3 with the fixed prototypical curve in Figure 4 demonstrates the dramatic  differences in gradients that occur when the gradient is calculated to maximize the FOM  directly instead of using a threshold with sigmoid output nodes.  "ACCOUNT"  0.8  z   -o a  Figure 4.  FA  "CHECK"  -0.6  -0.9 i I i I i I i , I ,  , I ,  -100 0 100 200 300-100 0 100 200 300-100  PUTATI VE HIT SCORE  "CREDIT-CARD"  HIT  0 100 00 300  Figure of merit gradients computed for true hits (HIT) and false alarms (FA)  with scores ranging from -100 to 300 for the keywords account, check, and  credit-card.  Figure of Merit Training for Detection and Spotting 1023  FOM training is a general technique that can applied to any "spotting" task where a set of  targets must be discriminated from background inputs. FOM training was successfully test-  ed using the hybrid radial basis function (RBF) - hidden Markov model (HMM) keyword  spotter described in (Lippmann, 1993).  4 IMPLEMENTATION OF FOM TRAINING  FOM training is applied to our high-performance HMM wordspotter after forward-back-  ward training is complete. Word models in the HMM wordspotter are first used to spot on  training conversations. The FOM gradient of each putative hit is calculated when this hit is  inserted into the putative hit list. The speech segment corresponding to a putative hit is ex-  cised from the conversation speech file and the corresponding keyword model is used to  match each frame with a particular state in the model using a Viterbi backtrace (shown in  Figure 5.) The gradient is then used to adjust the location of each Gaussian component in a  node as in RBF classifiers (Lippmann, 1993) and also the state weight of each state. The  state weight is a penalty added for each frame assigned to a state. The weight for each in-  dividual state is adjusted according to how important each state is to the detection of the  keyword. For example, many false alarms for the word card are words that sound like part  of the keyword such as hard or far. The first few states of the card model represent the sound  /k/and false alarms stay in these front states only a short time. If the state weight of the first  few states of the card model is large, then a true hit has a larger score than false alarms.  The putative hit score which is used to detect peaks representing putative hits is generated  according to  In this equation,  RADIAL  BASIS  Stota I = Skeyword- Stiller. (EQ 1)  Stota I is the putative hit score, Skeyword is the log Viterbi score in the  RAW KEYWORD 1  STA EWEIG TS  V!TERBI ALIGNMENT  NODES  SPEECH...  INPUT  Figure 5. State weights and center updates are applied to the state that is matched to each  frame in a Viterbi backtrace.  1024 Chang and Lippmann  last node of a specific keyword model computed using the Viterbi algorithm from the be-  ginning of the conversation to the frame where the putative hit ended, and Sfiller is the log  Viterbi score in the last node of the filler model. The filler score is used to normalize the  keyword score and approximate a posterior probability. The keyword score is calculated us-  ing a modified form of the Viterbi algorithm  i(t + 1) = max(i(t ) + ai, i, i-1 (t) + ai_l,i) + di(t,x ) + w i.  (EQ 2)  This equation is identical to the normal Viterbi recursion for left-to-right linear word mod-  els after initialization, except the extra state score w i is added. In this equation, 0 i (t) is  the log Viterbi score in node i at time t, a i . is the log of the transition probability from  node i tO node j, and d i (t, x) is the log likelihood distance score for node i for the nput  feature vector x at time t.  Word scores are computed and a peak-picking algorithm looks for maxima above a low  threshold. After a peak representing a putative hit is detected, frames of a putative hit are  aligned with the states in the keyword model using the Viterbi backtrace and both the means  of Gaussians in each state and state weights of the keyword model are modified. State  weights are modified according to  W i (t + 1) = W i (t) + gradient x rlstate X duration  (EQ 3)  In this equation, W i(t) is the state weight in node i at time t, gradient is the FOM  gradient for the putative hit, fistate is the stepsize for state weight adaptation, and  duration is the number of frames aligned to node i. If a true hit occurs, and the gradient  is positive, the state weight is increased in proportion to the number of frames assigned to  a state. If a false alarm occurs, the state weight is reduced in proportion to the number of  frames assigned to a state. The state weight will thus be strongly positive if there are many  more frames for a true hit that for a false alarm. It will be strongly negative if there are more  frames for a false alarm than for a true hit. High state weight values should thus improve  discrimination between true hits and false alarms.  The center of the Gaussian components within each node, which are similar to Gaussians  in radial basis function networks, are modified according to  xj(t) -mij (t)  rnij (t + 1) = mij (t) + gradient x qcenter x (EQ 4)  (sij  In this equation, m.. (t) is the j th component of the mean vector for a Gaussian hidden  node in HMM state at time t, gradient is the FOM gradient, rlcente r is the stepsize  for moving Gaussian centers, xj (t) is the value of the j th component of the input feature  vector at time t, and .. is the standard deviation of the j th component of the Gaussian  hidden node in HMM state i.  For each true hit, the centers of Gaussian hidden nodes in a state move toward the observa-  tion vectors of frames assigned to a particular state. For a false alarm, the centers move  away from the observation vectors that are assigned to a particular state. Over time, the cen-  ters move closer to the true hit observation vectors and further away from false alarm ob-  servation vectors.  Figure of Merit Training for Detection and Spotting 1025  Figure 6.  FOM  0.95  0.9  0.85  0.8  0.75  0.7  0.65  0.6  0.55  0.5  - MALETRAI -  - FEMALE TRAIN  MALE TEST  0 20 40 60 80 100 120 140 160  NUMBER OF CONVERSATIONS  Change in FOM vs. the number of conversations that the models have been  trained with. There were 25 male training conversations and 23 female training  conversations.  5 EXPERIMENTAL RESULTS  Experiments were performed using a HMM wordspotter that was trained using maximum  likelihood algorithm. More complicated models were created for words which occur fre-  quently in the training set. The word models for card and credit-card were increased to four  mixtures per state. The models for cash, charge, check, credit, dollar, interest, money,  month, and visa were increased to two mixtures per state. All other word models had one  mixture per state. The number of states per keyword is roughly 1.5 times the number ofpho-  nemes in each keyword. Covariance matrices were diagonal and variances were estimated  separately for all states. All systems were trained on the first 50 talkers in the credit card  training corpus and evaluated using the last 20 talkers.  An initial set of models was trained during 16 passes through the training data using whole-  word training and Viterbi alignment on only the excised words from the training conversa-  tions. This training provided a FOM of 62.5% on the 20 evaluation talkers. Embedded for-  ward-backward reestimation training was then performed where models of keywords and  fillers are linked together and trained jointly on conversations which were split up into sen-  tence-length fragments. This second stage of HMM training increased the FOM by two per-  centage points to 64.5%. The detection rate curves of these systems are shown in Figure 2.  FOM training was then performed for six passes through the training data. On each pass,  conversations were presented in a new random order. The change in FOM for the training  set and the evaluation set is shown in Figure 6. The FOM on the training data for both male  and female talkers increased by more than 10 percentage points after roughly 50 conversa-  tions had been presented. The FOM on the evaluation data increased by 5.2 percentage  points to 69.7% after three passes through the training data, but then decreased with further  training. This result suggests that the extra structure learned during the final three training  passes is overfitting the training data and providing poor performance on the evaluation set.  Figure 7 shows the spectrograms of high scoring true hits and false alarms for the word card  generated by our wordspotter. All false alarms shown are actually the occurrences of the  word car. The spectrograms of the true hits and the false alarms are very similar and the  actual excised speech segments are difficult even for humans to distinguish.  1026 Chang and Lippmann  A) True hits for card B) False alarms car mistaken for card  Figure 7. Spectrograms of high scoring true hit and false alarm for the word card.  6 SUMMARY  Detection of target signals embedded in a noisy background is a common and difficult prob-  lem distinct from the task of classification. The evaluation metric of a spotting system,  called Figure of Merit (FOM), is also different from the classification accuracy used to eval-  uate classification systems. FOM training uses a gradient which directly reflects a putative  hit's impact on the FOM to modify the parameters of the spotting system. FOM training  does not require careful adjustment of thresholds and target values and has been applied to  improve a wordspotter's FOM from 64.5% to 69.7% on the credit card database. FOM  training can also be applied to other spotting tasks such as arrhythmia detection and address  block location.  ACKNOWLEDGEMENT  This work was sponsored by the Advanced Research Projects Agency. The views expressed are those  of the authors and do not reflect the official policy or position of the U.S. Government. Portions of  this work used the HTK Toolkit developed by Dr. Steve Young of Cambridge University.  BIBLIOGRAPHY  R. Lippmann & E. Singer. (1993) Hybrid HMM/Neural-Network Approaches to Wordspotting. In IC-  ASSP '93, volume I, pages 565-568.  J. Rohlicek et. al. (1993) Phonetic and Language Modeling for Wordspotting. In ICASSP '93, volume  II, pages 459-462.  T. Zeppenfeld, R. Houghton & A. Waibel. (1993) Improving the MS-TDNN for Word Spotting. In  ICASSP '93, volume II, pages 475-478.  
Learning Curves: Asymptotic Values and  Rate of Convergence  Corinna Cortes, L. D. Jackel, Sara A. Solla, Vladimir Vapnik,  and John S. Denker  AT&T Bell Laboratories  Holmdel, NJ 07733  Abstract  Training classifiers on large databases is computationally demand-  ing. It is desirable to develop efficient procedures for a reliable  prediction of a classifier's suitability for implementing a given task,  so that resources can be assigned to the most promising candidates  or freed for exploring new classifier candidates. We propose such  a practical and principled predictive method. Practical because it  avoids the costly procedure of training poor classifiers on the whole  training set, and principled because of its theoretical foundation.  The effectiveness of the proposed procedure is demonstrated for  both single- and multi-layer networks.  I Introduction  Training classifiers on large databases is computationally demanding. It is desirable  to develop efficient procedures for a reliable prediction of a classifier's suitability  for implementing a given task. Here we describe such a practical and principled  predictive method.  The procedure applies to real-life situations with huge databases and limited re-  sources. Classifier selection poses a problem because training requires resources --  especially CPU-cycles, and because there is a combinatorical explosion of classifier  candidates. Training just a few of the many possible classifiers on the full database  might take up all the available resources, and finding a classifier particular suitable  for the task requires a search strategy.  327  328 Cortes, Jackel, Solla, Vapnik, and Denker  test  error  I I I I I I training  10,000 30,000 50,000  set size  Figure 1' Test errors as a function of the size of the training set for three different  classifiers. A classifier choice based on best test error at trailting set size l0 = 10,000  will result in an inferior classifier choice if the full database contains more than  15,000 patterils.  The naive solution to tile resource dilemma is to reduce tile size of the database to  l = 10, so that it is feasible to train all classifier candidates. The performance of the  classifiers is estimated froln an independently chosen test set after training. This  makes up one point for each classifier in a plot of the test error as a function of the  size I of the training set. The naive search strategy is to keep the best classifier at  10, under the assumption that the relative ordering of the classifiers is unchanged  when the test error is extrapolated from the reduced size l0 to the full database size.  Such an assumption is questionable and could easily result in an inferior classifier  choice as illustrated in Fig. 1.  Our predictive method also utilizes extrapolation from medium sizes to large sizes  of the training set, but it is based oll several data points obtained at various sizes  of the training set in the intermediate size regime where the computational cost of  training is low. A change in the representation of the measured data points is used  to gain confidence in the extrapolation.  2 A Predictive Method  Our predictive method is based on a simple modeling of tile learning curves of a  classifier. By learning curves we mean the expectation value of the test and training  errors as a function of the training set size I. The expectation value is taken over  all the possible ways of choosing a training set of a given size.  A typical example of learning curves is shown ill Fig. 2. Tile test error is always  larger than the training error, but asymptotically they reach a common value, a.  We model the errors for large sizes of the training set as power-law decays to the  Learning Curves: Asymptotic Values and Rate of Convergence 329  error  test error  ........... ..... training error  training set size,  Figure 2: Learning curves for a typical classifier. For all finite values of the  training set size I the test error is larger than the training error. Asymptotically  they converge to the same value a.  asymptotic error value, a:  b c  Stest = a + l- and Strai n = a - l-  where I is the size of the training set, and ( and  are positive exponents. From  these two expressions the sum and difference is formed:  b c  Stest+Strai n - 2a+ l- --- (1)  b c  Stest -Strain = l - + l- (2)  If we make the assumption  =  and b - c the equation (1) and (2) reduce to  Stest + train = 2a  2b  test - train =  (3)  These expressions suggest a log-log representation of the sum and difference of the  test and training errors as a filnction of the the training set size l, resulting in  two straight lines for large sizes of the training set: a constant ,- log(2a) for the  sum, and a straight line with slope -o and intersection log(b + c) ,. log(2b) for the  difference, as shown in Fig. 3.  The assumption of equal alnplitudes b = c of the two convergent terms is a conve-  nient but not crucial simplification of the model. YVe find experimentally that for  classifiers where this approximation does not hold, the difference Stest -Strai n  still forms a straight line in a log-log-plot. Froin this line the sum s = b + c  can be extracted as the intersection, as indicated on Fig. 3. The weighted sum  330 Cortes, Jackel, Solla, Vapnik, and Denker  log(error)  Iog(b+c) ~  Iog(2b)  ", 1og(test +train ) ~ log(2a)  1 lt % - Etrain ) .  log(training set size, /)  Figure 3: Within the validity of the power-law modeling of the test and training  errors, the sum and difference between the two errors as a function of training set  size give two straight lines in a log-log-plot: a constant ~ log(2a) for the sum, and a  straight line with slope -r and intersection log(b + c) ~ log(2b) for the difference.  c  Stest + b  Strai n will give a constant for an appropriate choice of b and c, with  b+c-s.  The validity of the above model was tested on numerous boolean classifiers with  linear decision surfaces. In all experiments we found good agreement with the model  and we were able to extract reliable estimates of the three parameters needed to  model the learning curves: the asymptotic value a, and the power c, and amplitude  b of the power-law decay. An example is shown in Fig. 4, (left). The considered  task is separation of handwritten digits 0-4 from the digits 5-9. This problem is  unrealizable with the given database and classifier.  The simple modeling of the test and training errors of equation (3) is only assumed  to hold for large sizes of the training set, but it appears to be valid already at  intermediate sizes, as seen in Fig. 4, (left). The predictive model suggested here is  based on this observation, and it can be illustrated from Fig. 4, (left): with test  and training errors measured for l _< 2560 it is possible to estimate the two straight  lines, extract approximate values for the three parameters which characterize the  learning curves, and use the resulting power-laws to extrapolate the learning curves  to the full size of the database.  The algorithm for the predictive method is therefore as follows:  1. Measure Stest and Strai n for intermediate sizes of the training set.  2. Plot log(Stest + Strain) and log(Stest - Strain) versus logl.  3. Estimate the two straight lines and extract the asymptotic value a  the amplitude b, and the exponent a.  4. Extrapolate the learning curves to the full size of the database.  Learning Curves: Asymptotic Values and Rate of Convergence 331  -1  -2  -3  irr)  -test + qxain 0.25  0.2  0.15  st- ffr--tra/n  0.1,  0.05  0 1 2 log ( l/256) 0  ! ! I  256 2560 25600 l   error  st error  training error  training set size, I  ---. points used for prediction  .... predicted learning curves  Figure 4:  Left: Test of the model for a 256 dimensional boolean classifier trained by minimiz-  ing a mean squared error. The sum and difference of the test and training errors  are shown as a function of the normalized training set size in a log-log-plot (base  10). Each point is the mean with standard deviation for ten different choices of a  training set of the given size. The straight line with a = 1, corresponding to a 1/l  decay, is shown as a reference.  Right: Prediction of learning curves for a 256 dimensional boolean classifier trained  by minimizing a mean squared error. Measured errors for training set size of  I _< 2560 are used to fit the two proposed straight lines in a log-log plot. The  three parameters which characterize the learning curves are extracted and used for  extrapolation.  A prediction for a boolean classifier with linear decision surface is illustrated in  Fig. 4, (right). The prediction is excellent for this type of classifiers because the  sum and difference of the test and training errors converge quickly to two straight  lines in a log-log-plot. Unfortunately, linear decision surfaces are in general not  adequate for many real-life applications.  The usefulness of the predictive method proposed here can be judged from its per-  formance on real-life sophisticated multi-layer networks. Fig. 5 demonstrates the  validity of the model even for a fully-connected multi-layer network operating in its  non-linear regime to implement an unrealizable digit recognition task. Already for  intermediate sizes of the training set the sum and difference between the test and  training errors are again observed to follow straight lines.  The predictive method was finally tested on sparsely connected multi-layer net-  works. Fig. 6, (left), shows the test and training errors for two networks trained  for the recognition of handwritten digits. The network termed "old" is commonly  referred to as LeNet [LCBD+90]. The network termed "new" is a modification of  LeNet with additional feature maps. The full size of the database is 60,000 patterns,  332 Cortes, Jackel, Solla, Vapnik, and Denker  log(error) a  -1  -2  -3 test,-- Etrain  log (l/100)  1000 10000 100000 I  Figure 5: Test of the model for a fully-connected 100-10-10 network. The sum  and the difference of the test and training error are shown as a function of the  normalized training set size in a log-log-plot. Each point is the mean with standard  deviation for 20 different choices of a training set of the given size.  a 50-50 % mixture of the NIST 1 training and test sets.  After training on 12,000 patterns it becomes obvious that the new network will out-  perform the old network when trained on the full database, but we wish to quantify  the expected improvement. If our predictive method gives a good quantitative  estimate of the new network's test error at 60,000 patterns, we can decide whether  three weeks of training should be devoted to the new architecture.  A log-log-plot based on the three datapoints from the new network result in values  for the three parameters that determine the power-laws used to extrapolate the  learning curves of the new network to the full size of the database, as illustrated in  Fig. 6, (right). The predicted test error at the full size of the database I - 60,000  is less than half of the test error for the old architecture, which strongly suggest  performing the training on the full database. The result of the full training is also  indicated in Fig. 6, (right). The good agreement between predicted and measured  values illustrates the power and applicability of the predictive method proposed  here to real-life applications.  3 Theoretical Foundation  The proposed predictive method based on power-law modeling of the learning curves  is not just heuristic. A fair amount of theoretical work has been done within the  framework of statistical mechanics [SST92] to compute learning curves for simple  classifiers implementing unrealizable rules with non-zero asymptotic error value. A  key assumption of this theoretical approach is that the number of weights in the  network is large.  National Institute for Standards and Technology, Special Database 3.  Learning Curves: Asymptotic Values and Rate of Convergence 333  error  0.03  0.02  '0. .... old network    new network  0.01 '  o  lO 20  Figure 6:  error  0.03,  0.02,  0.01,  30 40 50 60  training set size, 1HOO0   new network  - - -  new network predicted  10 20 3'0 40 0 dO  training set size, I11000  Left: Test (circles) and training (triangles) errors for two networks. The "old" net-  work is what commonly is referred to as LeNet. The network termed "new" is a  modification of LeNet with additional feature maps. The full size of the database  is 60,000 patterns, and it is a 50-50 % mixture of the NIST training and test set.  Right: Test (circles) and training (triangles) errors for the new network. The figure  shows the predicted values of the learning curves in the range 20,000 - 60,000 train-  ing patterns for the "new" network, and the actually measured values at 60,000  patterns.  The statistical mechanical calculations support a symmetric power-law decay of the  expected test and training errors to their common asymptotic value. The power-  laws describe the behavior in the large I regime, with an exponent ( which falls in  the interval 1/2 _< ( _< 1. Our numerical observations and modeling of the test and  training errors are in agreement with these theoretical predictions.  We have, moreover, observed a correlation between the exponent c and the asymp-  totic error value a not accounted for by any of the theoretical models considered so  far. Fig. 7 shows a plot of the exponent ( versus the asymptotic error a evaluated  for three different tasks. It appears from this data that the more difficult the target  rule, the smaller the exponent, or the slower the learning. A larger generalization  error for intermediate training set sizes is in such cases due to the combined effect  of a larger asymptotic error and a slower convergence. Numerical results for classi-  tiers of both smaller and larger input dimension support the explanation that this  correlation might be due to the finite size of the input dimension of the classifier  (here 256).  4 Summary  In this paper we propose a practical and principled method for predicting the suit-  ability of classifiers trained on large databases. Such a procedure may eliminate  334 Cortes, Jackel, Solla, Vapnik, and Denker  exponent,  0.9  0.8  0.7  o    0.6  0 01 0J2  asymptotic  error, a  Figure ?: Exponent of extracted power-law decay as a function of asymptotic  error for three different tasks. The un-realizability of the tasks, as characterized by  the asymptotic error a, can be changed by tuning the strength of a weight-decay  constraint on the norm of the weights of the classifier.  poor classifiers at an early stage of the training procedure and allow for a more  intelligent use of computational resources.  The method is based on a simple modeling of the expected training and test errors,  expected to be valid for large sizes of the training set. In this model both er-  ror measures are assumed to follow power-law decays to their common asymptotic  error value, with the same exponent and amplitude characterizing the power-law  convergence.  The validity of the model has been tested on classifiers with linear as well as non-  linear decision surfaces. The free parameters of the model are extracted from data  points obtained at medium sizes of the training set, and an extrapolation gives good  estimates of the test error at large size of the training set.  Our numerical studies of learning curves have revealed a correlation between the  exponent of the power-law decay and the asymptotic error rate. This correlation is  not accounted for by any existing theoretical models, and is the subject of continuing  research.  References  [LCBD+90]  [SST92]  Y. Le Cun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard,  W. Hubbard, and L. D. Jackel. Handwritten digit recognition with a  back-propagation network. In Advances in Neural Information Pro-  cessing Systems, volume 2, pages $96-404. Morgan Kaufman, 1990.  H. S. Seung, H. Sompolinsky, and N. Tishby. Statistical mechanics of  learning from examples. Physical Review A, 45:60156-6091, 1992.  
Packet Routing in Dynamically  Changing Networks:  A Reinforcement Learning Approach  Justin A. Boyan  School of Computer Science  Carnegie Mellon University  Pittsburgh, PA 15213  Michael L. Littman*  Cognitive Science Research Group  Bellcore  Morristown, NJ 07962  Abstract  This paper describes the Q-routing algorithm for packet routing,  in which a reinforcement learning module is embedded into each  node of a switching network. Only local communication is used  by each node to keep accurate statistics on which routing decisions  lead to minimal delivery times. In simple experiments involving  a 36-node, irregularly connected network, Q-routing proves supe-  rior to a nonadaptive algorithm based on precomputed shortest  paths and is able to route efficiently even when critical aspects of  the simulation, such as the network load, are allowed to vary dy-  namically. The paper concludes with a discussion of the tradeoff  between discovering shortcuts and maintaining stable policies.  1 INTRODUCTION  The field of reinforcement learning has grown dramatically over the past several  years, but with the exception of backgammon [8, 2], has had few successful appli-  cations to large-scale, practical tasks. This paper demonstrates that the practical  task of routing packets through a communication network is a natural application  for reinforcement learning algorithms.  *Now at Brown University, Department of Computer Science  671  672 Boyan and Littman  Our "Q-routing" algorithm, related to certain distributed packet routing algorithms  [6, 7], learns a routing policy which balances minimizing the number of "hops" a  packet will take with the possibility of congestion along popular routes. It does  this by experimenting with different routing policies and gathering statistics about  which decisions minimize total delivery time. The learning is continual and online,  uses only local information, and is robust in the face of irregular and dynamically  changing network connection patterns and load.  The experiments in this paper were carried out using a discrete event simulator to  model the transmission of packets through a local area network and are described  in detail in [5].  2  ROUTING AS A REINFORCEMENT LEARNING  TASK  A packet routing policy answers the question: to which adjacent node should the  current node send its packet to get it as quickly as possible to its eventual destina-  tion? Since the policy's performance is measured by the total time taken to deliver  a packet, there is no "training signal" for directly evaluating or improving the policy  until a packet finally reaches its destination. However, using reinforcement learning,  the policy can be updated more quickly and using only local information.  Let Qx(d, y) be the time that a node x estimates it takes to deliver a packet P  bound for node d by way of x's neighbor node y, including any time that P would  have to spend in node x's queue.  Upon sending P to y, x immediately gets back  y's estimate for the time remaining in the trip, namely  t -- min Qy(d,z)  zEneighbors of y  If the packet spent q units of time in x's queue and s units of time in transmission  between nodes x and y, then x can revise its estimate as follows:  AQx(d,y) - rl (  new estimate old estimate  q+s+' - Q(d,y) )  where r/ is a "learning rate" parameter (usually 0.5 in our experiments). The re-  sulting algorithm can bc characterized as a version of the Bellman-Ford shortest  paths algorithm [1, 3] that (1) performs its path relaxation steps asynchronously  and online; and (2) measures path length not merely by number of hops but rather  by total delivery time.  We call our algorithm "Q-routing" and represent the Q-function Q(d, y) by a large  table. We also tried approximating Q with a neural network (as in e.g. [8, 4]), which  allowed the learner to incorporate diverse parameters of the system, including local  queue size and time of day, into its distance estimates. However, the results of these  experiments were inconclusive.  We denote the function by Q because it corresponds to the Q function used in the  reinforcement learning technique of Q-learning [10].  Packet Routing in Dynamically Changing Networks: A Reinforcement Learning Approach 673  Figure 1: The irregular 6 x 6 grid topology  3 RESULTS  We tested the Q-routing algorithm on a variety of network topologies, including  the 7-hypercube, a 116-node LATA telephone network, and an irregular 6 x 6 grid.  Varying the network load, we measured the average delivery time for packets in the  system after learning had settled on a routing policy, and compared these delivery  times with those given by a static routing scheme based on shortest paths. The  result was that in all cases, Q-routing is able to sustain a higher level of network  load than could shortest paths.  This section presents detailed results for the irregular grid network pictured in  Figure 1. Under conditions of low load, the network learns fairly quickly to route  packets along shortest paths to their destinations. The performance vs. time curve  plotted in the left part of Figure 2 demonstrates that the Q-routing algorithm,  after an initial period of inefficiency during which it learns the network topology,  performs about as well as the shortest path router, which is optimal under low load.  As network load increases, however, the shortest path routing scheme ceases to be  optimal: it ignores the rising levels of congestion and soon floods the network with  packets. The right part of Figure 2 plots performance vs. time for the two routing  schemes under high load conditions: while shortest path is unable to tolerate the  packet load, Q-routing learns an efficient routing policy. The reason for the learning  algorithm's success is apparent in the "policy summary diagrams" in Figure 3.  These diagrams indicate, for each node under a given policy, how many of the  36 x 35 point-to-point routes go through that node. In the left part of Figure 3,  which summarizes the shortest path routing policy, two nodes in the center of  the network (labeled 570 and 573) are on many shortest paths and thus become  congested when network load is high. By contrast, the diagram on the right shows  that Q-routing, under conditions of high load, has learned a policy which routes  674 Boyan and Littman  5OO  400  3OO  200  1 O0  Q-routing  Shortest paths .....  0 2000 4000 6000 8000 10000  Simulator Time  5OO  4OO  300  200  lOO  0 2000  Q-routing --  Shortestpaths .....  4000 6000 8000 10000  SimulatorTime  Figure 2: Performance under low load and high load  164---1-3-1---1-t-7 ........... -1-t6---1-25---155  207 45 .... 5,4  ,,  ',  ,, ,,  286-34-4--50 ........... 573-3  ,,  140496-249  5,4 .... -,k3, 199  K)--278  255---185--140  95--t4-3--t46  45 .... 75 .... ,58  384-392396 ........... -396393387  375 le2---5,9 54 .... 62 377  394-292-258 ........... -2{ ;9--246-383  227-201-2!7  154---14-9--108 160---14--1---162  ,,  262-24-8---144  ,,  170--14-8---3  Figure 3: Policy summaries: shortest path and Q-routing under high load  Packet Routing in Dynamically Changing Networks: A Reinforcement Learning Approach 675  some traffic over a longer than necessary path (across the top of the network) so as  to avoid congestion in the center of the network.  The basic result is captured in Figure 4, which compares the performances of the  shortest path policy and Q-routing learned policy at various levels of network load.  Each point represents the median (over 19 trials) of the mean packet delivery time  after learning has settled. When the load is very low, the Q-routing algorithm routes  nearly as efficiently as the shortest path policy. As load increases, the shortest  path policy leads to exploding levels of network congestion, whereas the learning  algorithm continues to route efficiently. Only after a further significant increase in  load does the Q-routing algorithm, too, succumb to congestion.  18  16  14  12  10  4  0.5  Q-routing --  Shortest paths .....  /  I I [ I I I I  1 1.5 2 2.5 3 3.5 4  Network Load Level  4.5  Figure 4: Delivery time at various loads for Q-routing and shortest paths  3.1 DYNAMICALLY CHANGING NETWORKS  One advantage a learning algorithm has over a static routing policy is the potential  for adapting to changes in crucial system parameters during network operation. We  tested the Q-routing algorithm, unmodified, on networks whose topology, traffic  patterns, and load level were changing dynamically:  Topology We manually disconnected links from the network during simulation.  Qualitatively, Q-routing reacted quickly to such changes and was able to  continue routing traffic efficiently.  Traffic patterns We caused the simulation to oscillate periodically between two  very different request patterns in the irregular grid: one in which all traffic  was directed between the upper and lower halves of the network, and one  in which all traffic was directed between the left and right halves. Again,  676 Boyan and Littman  after only a brief period of inefficient routing each time the request pattern  switched, the Q-routing algorithm adapted successfully.  Load level When the overall level of network traffic was raised during simula-  tion, Q-routing quickly adapted its policy to route packets around new  bottlenecks. However, when network traffic levels were then lowered again,  adaptation was much slower, and never converged on the optimal shortest  paths. This effect is discussed in the next section.  3.2 EXPLORATION  Given the similarity between the Q-routing update equation and the Bellman-Ford  recurrence for shortest paths, it seems surprising that there is any difference what-  soever between the performance of Q-routing and shortest paths routing at low  load, as is visible in Figure 4. However, a close look at the algorithm reveals that  Q-routing cannot fine-tune a policy to discover shortcuts, since only the best neigh-  bor's estimate is ever updated. For instance, if a node learns an overestimate of the  delivery time for an optimal route, then it will select a suboptimal route as long as  that route's delivery time is less than the erroneous estimate of the optimal route's  delivery time.  This drawback of greedy Q-learning is widely recognized in the reinforcement learn-  ing community, and several exploration techniques have been suggested to overcome  it [9]. A common one is to have the algorithm select actions with some amount of  randomness during the initial learning period[10]. But this approach has two seri-  ous drawbacks in the context of distributed routing: (1) the network is continuously  changing, thus the initial period of exploration never ends; and more significantly,  (2) random traffic has an extremely negative effect on congestion. Packets sent in  a suboptimal direction tend to add to queue delays, slowing down all the packets  passing through those queues, which adds further to queue delays, etc. Because the  nodes make their policy decisions based on only local information, this increased  congestion actually changes the problem the learners are trying to solve.  Instead of sending actual packets in a random direction, a node using the "full  echo" modification of Q-routing sends requests for information to its immediate  neighbors every time it needs to make a decision. Each neighbor returns a single  number--using a separate channel so as to not contribute to network congestion  in our model--giving that node's current estimate of the total time to the destina-  tion. These estimates are used to adjust the Q(d, y) values for each neighbor y.  When shortcuts appear, or if there are inefficiencies in the policy, this information  propagates very quickly through the network and the policy adjusts accordingly.  Figure 5 compares the performance of Q-routing and shortest paths routing with  "full echo" Q-routing. At low loads the performance of "full echo" Q-routing is in-  distinguishable from that of the shortest path policy, as all inefficiencies are purged.  Under high load conditions, "full echo" Q-routing outperforms shortest paths but  the basic Q-routing algorithm does better still. Our analysis indicates that "full  echo" Q-routing constantly changes policy under high load, oscillating between us-  ing the upper bottleneck and using the central bottleneck for the majority of cross-  network traffic. This behavior is unstable and generally leads to worse routing times  under high load.  Packet Routing in Dynamically Changing Networks: A Reinforcement Learning Approach 677  18  16  14  12  10  4  0.5  Q-routing --  Shortest paths .....  Full Echo .....  /  I I I I I I I  1 1.5 2 2.5 3 3.5 4 4.5  Network Load Level  Figure 5: Delivery time at various loads for Q-routing, shortest paths and "full  echo" Q-routing  Ironically, the "drawback" of the basic Q-routing algorithm--that it does no ex-  ploration and no fine-tuning after initially learning a viable policy--actually leads  to improved performance under high load conditions. We still know of no single  algorithm which performs best under all load conditions.  4 CONCLUSION  This work considers a straightforward application of Q-learning to packet rout-  ing. The "Q-routing" algorithm, without having to know in advance the network  topology and traffic patterns, and without the need for any centralized routing con-  trol system, is able to discover efficient routing policies in a dynamically changing  network. Although the simulations described here are not fully realistic from the  standpoint of actual telecommunication networks, we believe this paper has shown  that adaptive routing is a natural domain for reinforcement learning. Algorithms  based on Q-routing but specifically tailored to the packet routing domain will likely  perform even better.  One of the most interesting directions for future work is to replace the table-based  representation of the routing policy with a function approximator. This could allow  the algorithm to integrate more system variables into each routing decision and  to generalize over network destinations. Potentially, much less routing information  would need to be stored at each node, thereby extending the scale at which the  algorithm is useful. We plan to explore some of these issues in the context of packet  routing or related applications such as auto traffic control and elevator control.  678 Boyan and Littman  Acknowledgements  The authors would like to thank for their support the Bellcore Cognitive Science  Research Group, the National Defense Science and Engineering Graduate fellowship  program, and National Science Foundation Grant IRI-9214873.  References  [lO]  [1] R. Bellman. On a routing problem. Quarterly of Applied Mathematics,  16(1):87-90, 1958.  [2] J. Boyan. Modular neural networks for learning context-dependent game strate-  gies. Master's thesis, Computer Speech and Language Processing, Cambridge  University, 1992.  [3] L. R. Ford, Jr. Flows in Networks. Princeton University Press, 1962.  [4] L.-J. Lin. Reinforcement Learning for Robots Using Neural Networks. PhD  thesis, School of Computer Science, Carnegie Mellon University, 1993.  [5] M. Littman and J. Boyan. A distributed reinforcement learning scheme for net-  work routing. Technical Report CMU-CS-93-165, School of Computer Science,  Carnegie Mellon University, 1993.  [6] H. Rudin. On routing and delta routing: A taxonomy and performance com-  parison of techniques for packet-switched networks. IEEE Transactions on  Communications, COM-24(1):43-59, January 1976.  [7] A. Tanenbaum. Computer Networks. Prentice-Hall, second edition edition,  1989.  [8] G. Tesauro. Practial issues in temporal difference learning. Machine Learning,  8(3/4), May 1992.  [9] Sebastian B. Thrun. The role of exploration in learning control. In David A.  White and Donald A. Sofge, editors, Handbook of Intelligent Control: Neural,  Fuzzy, and Adaptive Approaches. Van Nostrand Reinhold, New York, 1992.  C. Watkins. Learning from Delayed Rewards. PhD thesis, King's College,  Cambridge, 1989.  
An Analog VLSI Saccadic Eye Movement  System  Timothy K. Horiuchi Brooks Bishofberger and  Computation and Neural Systems Program  California Institute of Technology  MS 139-74  Pasadena, CA 91125  Christof Koch  Abstract  In an effort to understand saccadic eye movements and their rela-  tion to visual attention and other forms of eye movements, we --  in collaboration with a number of other laboratories -- are carry-  ing out a large-scale effort to design and build a complete primate  oculomotor system using analog CMOS VLSI technology. Using  this technology, a low power, compact, multi-chip system has been  built which works in real-time using real-world visual inputs. We  describe in this paper the performance of an early version of such  a system including a 1-D array of photoreceptors mimicking the  retina, a circuit computing the mean location of activity represent-  ing the superior colliculus, a saccadic burst generator, and a one  degree-of-freedom rotational platform which models the dynamic  properties of the primate oculomotor plant.  I Introduction  When we look around our environment, we move our eyes to center and stabilize  objects of interest onto our fovea. In order to achieve this, our eyes move in quick  jumps with short pauses in between. These quick jumps (up to 750 deg/sec in hu-  mans) are known as saccades and are seen in both exploratory eye movements and  as reflexive eye movements in response to sudden visual, auditory, or somatosen-  sory stimuli. Since the intent of the saccade is to bring new objects of interest onto  the fovea, it can be considered a primitive attentional mechanism. Our interest  582  An Analog VLSI Saccadic Eye Movement System 583  lies in understanding how saccades are directed and how they might interact with  higher attentional processes. To pursue this goal, we are designing and building a  closed-loop hardware system based on current models of the saccadic system.  Using traditional software methods to model neural systems is difficult because  neural systems are composed of large numbers of elements with non-linear char-  acteristics and a wide range of time-constants. Their mathematical behavior can  rarely be solved analytically and simulations slow dramatically as the number and  coupling of elements increases. Thus, real-time behavior, a critical issue for any  system evolved for survival in a rapidly changing world, becomes impossible. Our  approach to these problems has been to fabricate special purpose hardware that  reflects the organization of real neural systems (Mead, 1989; Mahowald and Dou-  glas, 1991; Horiuchi e! al., 1992.) Neuromorphic analog VLSI technology has many  features in common with nervous tissue such as: processing strategies that are fast  and reliable, circuits that are robust against noise and component variability, local  parameter storage for the construction of adaptive systems and low-power consump-  tion. Our analog chips and the nervous system both use low-accuracy components  and are significantly constrained by wiring.  The design of the analog VLSI saccadic system discussed here is part of a long-term  effort of a number of laboratories ( Douglas and Mahowald at Oxford University,  Clark at Harvard University, Sejnowski at UCSD and the Salk Institute, Mead and  Koch at Caltech) to design and build a complete replica of the early mammalian  visual system in analog CMOS VLSI.  The design and fabrication of all circuits is carried out via the US-government  sponsored silicon service MOSIS, using their 2 ttrn line process.  2 An Analog VLSI Saccadic System  Photoreceptor  & Centroid Chip  Lens  Burst  Generator  Chip  Motors  Figure 1: Diagram of the current system.  The system obtains visual inputs from a photoreceptor array, computes the target  location within a model of the superior colliculus and outputs the saccadic burst  command to drive the eyeball. While not discussed here, an auditory localization  584 Horiuchi, Bishofberger, and Koch  input is being developed to trigger saccades to acoustical stimuli.  2.1 The Oculomotor Plant  The oculomotor plant is a one degree-of-freedom turntable which is driven by a pair  of antagonistic-pulling motors. In the biological system where the agonist muscle  pulls against a passive viscoelastic force, the fixation position is set by balancing  these two forces. In our system, the viscoelastic properties of the oculomotor plant  are simulated electronically and the fixation point is set by the shifting equilibrium  point of these forces. In order maintain fixation off-center, like the biological system,  a tonic signal to the motor controller must be maintained.  2.2 Photoreceptors  The front-end of the system is an adaptive photoreceptor array (Delbriick, 1992)  which amplifies small changes in light intensity yet adapts quickly to gross changes in  lighting level. The current system uses a 1-D array of 32 photoreceptors 40 microns  apart. This array provides the visual input to the superior colliculus circuitry.  The gain control occurs locally at each pixel of the image and thus the maximum  sensitivity is maintained everywhere in the image in contrast to traditional imaging  arrays which may provide washed out or blacked-out areas of an image when the  contrast within an image is too large. In order to trigger reflexive, visually-guided  saccades, the output of the photoreceptor array is coupled to the superior colliculus  model by a luminance change detection circuit. A change in luminance somewhere  in the image sends a pulse of current to the colliculus circuit which computes the  center of this activity. This coupling passes a current signal which is proportional to  the absolute-value of the temporal derivative of a photoreceptor's voltage output,  (i.e. I]dI(x,t)/dt]l where I(x,t) is the output of the photreceptor array). While we  are initially building a 1-D system, 2-D photoreceptor arrays have been built in  anticipation of a two degree-of-freedom system. While these photoreceptor circuits  have been successfully constructed, we do not discuss the results here since the  performance of these circuits are described in the literature (Delbriick 1992).  2.3 Superior Colllculus Model  The superior colliculus, located on the dorsal surface of the midbrain, is a key area  in the behavioral orientation system of mammals. The superficial layers have a  topographic map of visual space and the deeper layers contain a motor map of  saccadic vectors. Microstimulation in this area initiates saccades whose metrics  are related to the location stimulated. This type of representation is known as a  population coding. Many neurons in the deeper layers of superior colliculus are  multisensory and will generate saccades to auditory and somatosensory targets as  well as visual targets.  While it is clear that the superior colliculus performs a multitude of integrative  functions between sensory modalities and attentional processes, our initial model  of superior colliculus simply computes the center of activity from the population  code seen in the superficial layers (i.e. the photoreceptor array) using the weighted  average techniques developed by DeWeerth (1991) for computing the centroid of  An Analog VLSI Saccadic Eye Movement System 585  2.6  2.2  2.O  Centtold Circuit Output vs. Target Error  1.8  -40 -0 - -10 0 10 20 30  T   Figure 2: Output of the centroid circuit for a flashed red LED target at different  angles away from the center position. Note that the output of the circuit was  sampled 1 msec. after stimulus onset to account for capacitive delays.  brightness. The results of the photoreceptor/centroid circuits are shown in Fig. 2.  In the case of visually-guided saccades, retinal error translates directly into motor  error and thus we can use the photoreceptors directly as our inputs. This simplified  retina/superior colliculus model provides the motor error which is then passed on  to the burst generator.  2.4 Saccadic Burst Generator  The burst generator model (Fig. 3) driving the oculomotor plant receives as its  input, desired change in eye position from the superior colliculus model and creates  a two-component signal, a pulse and a step (Fig. 4). A pair of these pulse/step  signals drive the two muscles of the eye which in turn moves the retinal array, thus  closing the loop. The burst generator model is a double integrator model based on  the work by Jiirgens, et al (1981) and Lisberger et al (1987) which uses initial motor  error as the input to the system. This motor error is injected into the "integrating"  burst neuron which has negative feedback onto itself. This arrangement has the  effect of firing a number of spikes proportional to the initial value of motor error. In  the circuit, this integrator is implemented by a 1.9 pF capacitor. This burst of spikes  serves to drive the eye rapidly against the viscosity. The burst is also integrated by  the "neural integrator" (another 1.9 pF capacitor) which holds the local estimate of  the current eye position from which the tonic, or holding signal is generated. Figs.  4 and 5 show output data from the burst generator chip and the response of the  physical mechanism to this output. The inputs to the burst generator chip are 1) a  voltage indicating desired eye position and 2) a digital trigger signal. The outputs  are a pair of asynchronous digital pulse trains which carry the pulse/step signals  which drive the left and right motors.  586 Horiuchi, Bishofberger, and Koch  3 Discussion  As we are still in the formative stages of our project, our first goal has been to  demonstrate a closed-loop system which can fixate a particular stimulus whose im-  age is falling onto its photoreceptor array. The first set of chips represent dramat-  ically simplified circuits in order to capture the first-order behavior of the system  while using known representations. Owing to the large number of parameters that  must be set, and their sensitivity to variations, we have begun a study to investigate  biologically plausible approaches to automatic parameter-setting.  In the short term we intend to dramatically refine the models used at each stage,  most notably the superior colliculus which is involved in the integration of non-visual  sources of saccade targets (e.g. memory or audition), and in the mechanisms used  for target selection or fixation. In the longer run, we plan to model the interaction  of this system with other oculomotor processes such as smooth pursuit, VOR, OKR,  AND vergence eye movements.  While the biological microcircuits of the superior colliculus and brainstem burst  generator are not well known, more is understood about the representations found  in these areas. By exploring the advantages and disadvantages of various compu-  tational models in a working system, it is hoped that a truly robust system will  emerge as well as better models to explain the biological data. The construction of  a compact hardware system which operates in real-time can often provide a more  intuitive understanding of the closed-loop system. In addition, a visually-attentive  hardware system which is physically small and low-power has numerous applications  in the real world such as in mobile robotics or remote surveillance.  4 Acknowledgements  Many thanks go to Prof. Carver Mead and his group for developing the foundations  of this research. Our laboratory is partially supported by grants from the Office  of Naval Research and the Rockwell International Science Center. Tim Horiuchi is  supported by a grant from the Office of Naval Research.  5 References  T. Delbriick and C. Mead, (1993) Ph.D. Thesis, California Institute of Technology.  S. P. DeWeerth, (1991) Ph.D. Thesis, California Institute of Technology.  T. Horiuchi, W. Bair, B. Bishofberger, A. Moore, J. Lazzaro, C. Koch, (1992) Int.  J. Computer Vision 8:3,203-216.  R. Jiirgens, W. Becker, and H. H. Kornhuber, (1981) Biol. Cybern. 39:87-96.  10:97-  S. G. Lisberger, E. J. Morris, and L. Tychsen, (1987) Ann. Rev. Neurosci.  129.  M. Mahowald, and R. Douglas, (1991) Nature 354:515-518.  C. Mead, (1989) Analog VLSI and Neural Systems, Addison-Wesley.  An Analog VLSI Saccadic Eye Movement System 587  Left Burst Neuron  Left Motor Neuron  Motor Error < 0  Motor Error > 0  Other inputs:  VOR/OKR  Smooth Pursuit  Neural  Integrator  Right Burst Neuron  Ill  Right Motor Neuron  Figure 3: Schematic diagram of the burst generator. The burst neuron "samples"  the motor error when it receives a trigger signal (not shown) and begins firing  as a sigmoidal function of the motor error. The spikes feedback and discharge the  integrator and the burst is shut down. This "pulse" signal drives the eye against the  viscosity. This signal is also integrated by the neural integrator which contributes  the "step" portion of the motor command to hold the eye in its final position. The  neural integrator has additional velocity inputs for other oculomotor behavior such  as smooth pursuit, VOR and OKR. Note that the burst neuron for the other muscle  is silent in this direction.  588 Horiuchi, Bishofberger, and Koch  I  -0.25 2.25  0.00 0.25  0.50  I I I I I I  0.75 1.00 1.25 1.50 1.75 2.00  re (sonds) (10 '2 )  Figure 4: Spike signals in the circuit during a small saccade. (7.5 degrees to the  right, starting from 4.8 degrees to the right.) Top: Burst neuron, Middle: Neural  Integrator, Bottom: Motor neuron. (one of the outputs of the chip) Note that  the "neuron" circuit currently used increases both its pulse frequency and pulse  duration for large input currents, causing the voltage saturation seen in the bottom  trace.  80'  6O  4O   2O  .-6O  Eye Position vs.  -8o I I I I I I I I I I  -0.025 0,000 0.025 0.050 0.075 0.100 0.125 0.150 0.175 0.200 0.225  ame (sonds)  Figure 5: Horizontal position vs. time for 21 different saccades. Peak angular  velocity achieved for the 60 degree saccade to the right was approximately 870  degrees per second. The input command was changed uniformly from -60 to +60  degrees.  An Analog VLSI Saccadic Eye Movement System 589  S0'  60'  40'  -60 '  -80  1  Final Eye Position vs. Burst Command Voltage  I I I I I I I I I  1.50 1.75 2.00 2.25 2.50 2.75 3.00 3.25 3.50  Input Voltage to Burst Ocnerator (center = 2.5v)  Figure 6: Linearity of the system for the position data given in the previous figure.  Final eye position was computed as the average eye position during the last 20 msec.  of each trace.  30'  25   2O   1o  Average of 10 Saccades from "center" to 30 deg. R  -5 I I I I I I I I I I  -0.025 0.000 0.025 0.050 0.075 0.100 0,125 0.150 0.175 0.200 0.225  time (seconds)  Figure 7: Repeatability: The solid line shows averaged eye position (relative to cen-  ter) vs. time for 10 identical saccades. The dashed lines show a standard deviation  on each side of the mean. Most of the variability is attributed to problems with  friction.  
Mixtures of Controllers for  Jump Linear and Non-linear Plants  Timothy W. Cacciatore  Department of Neurosciences  University of California at San Diego  La Jolla, CA 92093  Steven J. Nowlan  Synaptics, Inc.  2698 Orchard Parkway  San Jose, CA 95134  Abstract  We describe an extension to the Mixture of Experts architecture for  modelling and controlling dynamical systems which exhibit nulti-  ple modes of behavior. This extension is based on a Markov process  model, and suggests a recurrent network for gating a set of linear  or non-linear controllers. The new architecture is demonstrated to  be capable of learning effective control strategies for jump linear  and non-linear plants with multiple modes of behavior.  I Introduction  Many stationary dynamic systems exhibit significantly different behaviors under  different operating conditions. To control such complex systems it is computation-  ally more efficient to decompose the problem into smaller subtasks, with different  control strategies for different operating points. When detailed information about  the plant is available, gain scheduling has proven a successful method for designing a  global control (Shamma and Athans, 1992). The system is partitioned by choosing  several operating points and a linear model for each operating point. A controller  is designed for each linear model and a method for interpolating or 'scheduling' the  gains of the controllers is chosen.  The control problem becomes even more challenging when the system to be con-  trolled is non-stationary, and the mode of the system is not explicitly observable.  One important, and well studied, class of non-stationary systems are jump linear  systems of the form: 8--- - A(i)x + B(i)u. where x represents the system state,  dt --  719  720 Cacciatore and Nowlan  u the input, and i, the stochastic parameter that determines the mode of the sys-  tem, is not explicitly observable. To control such a system, one must estimate the  mode of the system from the input-output behavior of the plant and then choose  an appropriate control strategy.  For many complex plants, an appropriate decomposition is not known a priori. One  approach is to learn the decomposition and the piecewise solutions in parallel. The  Mixture of Experts architecture (Nowlan 1990, Jacobs et al 1991) was proposed as  one approach to simultaneously learning a task decomposition and the piecewise  solutions in a neural network context. This architecture has been applied to con-  trol simple stationary plants, when the operating mode of the plant was explicitly  available as an input to the gating network (Jacobs and Jordan 1991).  There is a problem with extending this architecture to deal with non-stationary  systems such as jump linear systems. The original formulation of this architecture  was based on an assumption of statistical independence of training pairs appropriate  for classification tasks. However, this assumption is inappropriate for modelling the  causal dependencies in control tasks. We derive an extension to the original Mixture  of Experts architecture which we call the Mixture of Controllers. This extension  is based on an nth order Markov model and can be implemented to control non-  stationary plants. The new derivation suggests the importance of using recurrence  in the gating network, which then learns to estimate the conditional state occupancy  for sequences of outputs. The power of the architecture is illustrated by learning  control and switching strategies for simple jmnp linear and non-stationary non-  linear plants. The modified recurrent architecture is capable of learning both the  control and switching for these plants, while a non-recurrent architecture fails to  learn an adequate control.  2 Mixtures of Controllers  The architecture of the system is shown in figure 1. xt denotes the vector of inputs  to the controller at time t and yt is the corresponding overall control output. The  architecture is identical to the Mixture of Experts architecture, except that the  gating network has become recurrent, receiving its outputs from the previous time  step as part of its input. The underlying statistical model, and corresponding train-  ing procedure for the Mixture of Controllers, is quite different from that originally  proposed for the Mixture of Experts.  We assume that the system we are interested in controlling has N different modes  or states I and we will have a distinct control 3Ik for each mode. In general we are  interested in the likelihood of producing a sequence of control outputs F1,..., .W  given a sequence of inputs xl,  ., zr. This likelihood can be computed as:  L(yx . . .yTIx . . .XT)  = 1-I P(ytl& = k,xt)P(st-- klm...yt_,x...xt)  t k  = (1)  t k  1This is an idealization and if N is unknown it is safest to overestimate it.  Mixtures of Controllers for Jump Linear and Non-Linear Plants 721  xt  l  Sel----i1t 2  t 3  Yt  Figure 1: The Mixture of Controllers architecture. M1, M2 and M3  are feedforward networks implementing controls appropriate for different  modes of the system to be controlled. The gating network (Sel.) is  recurrent and uses a softmax non-linearity to compute the weight to  be assigned to each of the control outputs. The weighted sum of the  controls is then used as the overall control for the plant.  where bt  represents the probability of producing the desired control yt given the  input xt and that the system is in state k. 7t  represents the conditional probability  of being in state k given the sequence of inputs and outputs seen so far. In order  to make the problem tractable, we assume that this conditional probability is com-  pletely determined by the current input to the system and the previous state of the  system:  Thus we are assuming that our control can be approximated by a Markov process,  and since we are assuming thai the mode of [he sys[em is no[ explici[ly available, [his  becomes a hidden Markov model. This Markov assumption leads to [he particular  recurten[ gating archi[ec[ure used in the Mixlure of Controllers.  If we make [he same gaussian assumptions used in [he original Mixture of ExperIs  model, we can define a gradien[ descent procedure for maximizing the log of [he  likelihood given in Equation 1. Assume  k 2 2  -(Yt-Yt ) /2a  deane  : ', ', , = fit % and  Lt  Then the derivative of the likelihood with respect to the output of one of the con-  trollers becomes:  = (2)  ay2  722 Cacciatore and Nowlan  The derivative of the likelihood with respect to a weight in one of the control net-  works is computed by accumulating partial derivatives over the sequence of control  outputs:  0 log L 0 log L Oyt  (3)  OWq -- Z Oytk 'OOWq'  t  For the gating network, we once again use a softmax non-linearity so:  Then  exp gt   Zj exp g  O lo___g L = -(Rt _ ?})7L. (4)  The derivatives for the weights in the gating network are again computed by accu-  mulating partial derivatives over output sequences'  0 log L 0 log L Og  o,o (5)  t  Equations (2) and (4) turn out to be quite similar to those derived for the original  Mixture of Experts architecture. The primary difference is the appearance of/3}  rather than b in the expression for/i}. The appearance of/3 is a direct. result of  the recurrence introduced into the gating network. /3 can be computed as part of  a modified back propagation through time algorithm for the gating network using  the recurrence:  /3 = b +  wkj/3+, (6)  where  Equation (6) is the analog of the backward pass in the forward-backward algorithm  for standard hidden Markov models.  In the simulations reported in the next section, we used an online gradient descent.  procedure which employs an approximation for/} which uses only one step of back  propagation through time. This approximation did not appear to significantly affect  the final performance of the recurrent architecture.  3 Results  The performances of the recurrent Mixture of Controllers and non-recurrent Mixture  of Experts were compared on three control tasks: a first order jump linear system,  a second order jump linear system, and a tracking task that required two non-  linear controllers. The object of the first two jump-linear tasks was to control a  plant which switched randomly between two linear systems. The resulting overall  systems were highly non-linear. In both the first and second order cases it was  Mixtures of Controllers for Jump Linear and Non-Linear Plants 723  First Order Model Traning Error  First Order Model Trajectory  20000 200  Non-recurrenl 15 0  15000  lo0   10(20 0  5 0 '',,  uJ  ideal  Recurrsnt  0%0 100 20 0 0 50 -10 0 10 10  Figure 2: Left: Training convergence of Mixtures of Experts and Mix-  tures of Controllers on first order jump linear system. The vertical axis  is average squared error over training sequences and horizontal axis is  the number of training sequences seen. Right: Sample test trajectory  of first order jump linear system under control of Mixture of Controllers.  The system switches states at times 50 and 100.  desired to drive all plant outputs to zero (zero-forcing control). Neither the first or  second order systems could be successfully controlled by a single linear controller.  For both jump-linear tasks, the architecture of the Mixture of Controllers and Mix-  ture of Experts consisted of two linear experts, and a one layer gating network.  The input to the experts was the plant output at the previous time step, while the  input to the gating network was the ratio of the plant outputs at the two preceding  time steps. An ideal linear controller was designed for each mode of the system.  Training targets were derived from outputs of the appropriate ideal controller, us-  ing the known mode of the system for the training trajectories. The parameters  of the gating and control networks were updated after each pass through sample  trajectories which contained several state transitions.  The recurrent Mixture of Controllers could be trained to successfully control the  first order jump linear system (figure 2), and once trained generalized successfully  to novel test trajectories. The non-recurrent Mixture of Experts failed to learn even  the training data for the first order jump linear system (note the high asymptote  for the training error without recurrence in figure 2). The recurrent Mixture of  Controllers was also able to learn to control the second order jump linear system  (figure 3), however, it was necessary to teacher force the system during the first  5000 epochs of training by providing the true mode of the system as an extra  input to the gating network. This extra input was removed at epoch 5000 and the  error initially increases dramatically but the system is able to eventually learn to  control the second order jump linear system autonomously. Note that the Mixture of  Experts system is actually able to learn a successful control even more rapidly than  the Mixture of Controllers when the additional teacher input is provided, however  learning again completely fails once this input is removed at epoch 5000 (figure 3).  724 Cacciatore and Nowlan  Second Order Model Training Error  500O  4O0 0  3O0 0  2OO o  1  iC  oo 2ooo0 4O000  Second Order Model Trajectory  800 t  o o  O0 500 1000 1500  E,ochs 13me  Figure 3: Left: Training convergence of Mixtures of Experts and Mix-  tures of Controllers on second order jump linear system. Right: Sample  test trajectory of second order jump linear system under control of Mix-  ture of Controllers. The system again switches states at times 50 and  100.  In both first and second order cases, the trained Mixture of Controllers is able to  control the system in both modes of system behavior, and to detect mode changes  automatically. The difficulty in designing a control for a jump linear system usually  lies in identifying the state of the system. No explicit law describing how t.o identify  and switch between control modes is necessary to train the Mixture of Controllers,  as this is learned automatically as a byproduct of learning to successfully control  the system.  Performance of the Mixture of Controllers and the Mixture of Experts was also  compared on a lnore complex task requiring a non-linear control law in each mode.  The task was to control the trajectory of a ship to track an object traveling in a  straight line, or flee from an object having a random walk trajectory (figure 4).  There is a high degree of task interference between the controls appropriate during  the two modes of object behaviors. The ship dynamics were taken from Miller and  Sutton (1990).  For both the Mixture of Controllers and the Mixture of Experts two experts were  used. The experts received past and present ]neasurements of the object bearing,  distance, velocity, and the ship heading and turn rate. The controllers specified the  desired turn rate of the ship. A one layer gating network was used which received  the velocity of the object as input.  Training targets were produced from ideal controllers designed for each object be-  havior. The ideal controller for the random walk behavior produced a turn rate that  headed directly away from the object. The ideal controller for intercepting the ob-  ject used future information about object position to determine the turn rate which  would lead to the closest possible intercept point. Both ideal controllers made use  of information not available to the Mixture of Experts or Mixture of Controllers.  The Mixture of Controllers and the Mixture of Experts were trained on sequences of  Mixtures of Controllers for Jump Linear and Non-Linear Plants 725  a)   desiredJ  :: //// target  x poMtion  b)  o?  correc  inco/'ect  Figure 4: (a) Actual and desired trajectories of ship under control of  Mixture of Experts while attempting to intercept target. (b) Gating  unit activities as a function of time for trajectory in (a).  trajectories where the object changed behaviors multiple times. The weights of the  networks were updated after each pass through the trajectories. The input to the  gatlug net in this task provided more instantaneous information about the mode of  object behavior than was provided in the jump linear tasks. As a result, the non-  recurrent Mixture of Experts was able to achieve a minimum level of performance  on the overall task. The recurrent Mixture of Controllers performed much better.  The differences between two architectures are revealed by examining the gating net-  work outputs. Without recurrence, the Mixture of Experts gating network could  not determine the state of the object with certainty, and compromised by select-  ing a combination of the correct and incorrect. control (figure 4b). Since the two  controls are incompatible, this uncertainty degrades the performance of the overall  controller. With recurrence in the ga.ting network, the Mixture of Controllers is able  to determine the target state with greater certainty by integrating information from  many observations of object behavior. The sharper decisions about which control  to use greatly improve tracking performance (figure 5).  We explored the ability of the Mixture of Controllers to learn the dynamics of  switching by training on trajectories where the object switched behavior with vary-  ing frequency. The gatlug network trained on an object that switched behaviors  infrequently was sluggish to respond to transitions, but more noise tolerant than the  gatlug network trained on a frequently switching object. Thus, the gating network  is able to incorporate the frequency of transition into its state model.  4 Discussion  We have described an extension to the Mixture of Experts architecture for modelling  and controlling dynamical systems which exhibit multiple modes of behavior. The  algorithm we have presented for updating the parameters of the model is a simple  gradient descent procedure. Application of the technique to large scale problems  726 Cacciatore and Nowlan  a)  Figure 5: (a) Actual and desired trajectories of ship under control of  Mixture of Controllers while attempting to intercept target. (b) Gating  unit activities as a function of time for trajectory in (a). Note that these  are much less noisy than the activities seen in figure 4(b).  may require the development of faster converging update algorithms, perhaps based  on the generalized EM (GEM) family of algorithms, or a variant of the iterative  reweighted least squares procedure proposed by Jordan and Jacobs (1993) for hier-  archies of expert networks. Additional work is also required to establish the stability  and convergence rate of the algorithm for use in adaptive control applications.  References  Jacobs, R.A. and Jordan, M.I. A competitive modular connectionist architecture.  Neural Information Processing Systems 3 (1991).  Jacobs, R.A., Jordan, M.I., Nowlan, S.J. and Hinton, G.E. Adaptive Mixtures of  Local Experts. Neural Computation, 3, 79-87, (1991).  Jordan, M.I. and Jacobs, R.A. Hierarchical Mixtures of Experts and the EM algo-  rithm. Neural Computation, (1994).  Miller, W.T., Sutton, R.S. and Werbos, P.J. Neural Networks for Control, MIT  Press (1993).  Nowlan, S.J. Competing Experts: An Experimental Investigation of Associative  Mixture Models. Technical Report CRG-TR-90-5, Department of Computer Sci-  ence, University of Toronto (1990).  Shamma, J.S., and Athans, M. Gain scheduling: potential hazards and possible  remedies. IEEE Control Systems Magazine, 12:(3), 101-107 (1992).  
The Power of Amnesia  Dana Ron Yoram Singer Naftali Tishby  Institute of Computer Science and  Center for Neural Computation  Hebrew University, Jerusalem 91904, Israel  Abstract  We propose a learning algorithm for a variable memory length  Markov process. Human communication, whether given as text,  handwriting, or speech, has multi characteristic time scales. On  short scales it is characterized mostly by the dynamics that gen-  erate the process, whereas on large scales, more syntactic and se-  mantic information is carried. For that reason the conventionally  used fixed memory Markov models cannot capture effectively the  complexity of such structures. On the other hand using long mem-  ory models uniformly is not practical even for as short memory as  four. The algorithm we propose is based on minimizing the sta-  tistical prediction error by extending the memory, or state length,  adaptively, until the total prediction error is sufficiently small. We  demonstrate the algorithm by learning the structure of natural En-  glish text and applying the learned model to the correction of cor-  rupted text. Using less than 3000 states the model's performance  is far superior to that of fixed memory models with similar num-  ber of states. We also show how the algorithm can be applied to  intergenic E. coli DNA base prediction with results comparable to  HMM based methods.  I Introduction  Methods for automatically acquiring the structure of the human language are at-  tracting increasing attention. One of the main difficulties in modeling the natural  language is its multiple temporal scales. As has been known for many years the  language is far more complex than any finite memory Markov source. -Yet Markov  176  The Power of Amnesia 177  models are powerful tools that capture the short scale statistical behavior of lan-  guage, whereas long memory models are generally impossible to estimate. The  obvious desired solution is a Markov source with a 'deep' memory just where it is  really needed. Variable memory length Markov models have been in use for language  modeling in speech recognition for some time [3, 4], yet no systematic derivation,  nor rigorous analysis of such learning mechanism has been proposed.  Markov models are a natural candidate for language modeling and temporal pattern  recognition, mostly due to their mathematical simplicity. It is nevertheless obvious  that finite memory Markov models can not in any way capture the recurslye nature  of the language, nor can they be trained effectively with long enough memory. The  notion of a variable length mentory seems to appear naturally also in the context of  universal coding [6]. This information theoretic notion is now known to be closely  related to efficient modeling [7]. The natural measure that appears in information  theory is the description length, as measured by the statistical predictability via  the Kullback- Liebler (KL) divergence.  The algorithm we propose here is based on optimizing the statistical prediction  of a Markov lnodel, measured by the instantaneous KL divergence of the following  symbols, or by the current statistical surprise of the model. The memory is extended  precisely when such a surprise is significant, until the overall statistical prediction  of the stochastic model is sufficiently good. We apply this algorithm successfully for  statistical language modeling. Here we demonstrate its ability for spelling correction  of corrupted English text. We also show how the algorithm can be applied to  intergenie E. coli DNA base prediction with results comparable to HMM based  methods.  2 Prediction Suffix Trees and Finite State Automata  Definitions and Notations  Let E be a finite alphabet. Denote by E* the set of all strings over E. A string  s, over E* of length n, is denoted by s = ss2...s. We denote by e the empty  string. The length of a string s is denoted by Is[ and the size of an alphabet E is  denoted by [E I. Let, Prefix(s): ss2... s_, denote the longest prefix of a string  s, and let Prefix*(s) denote the set of all prefixes of s, including the empty string.  Similarly, Suffix(s) = s2s3...s and Suffix*(s) is the set of all suffixes of s. A  set of strings is called a prefix free set if, V s  , s 2  S' { s  } N Prefix* (s ) - 0. We  call a probability measure P, over the strings in E* proper if P(e) = 1, and for every  string s, oes P(srr) - P(s). Hence, for every prefix free set S, 2e$ P(s) _< l,  and specifically for every integer n >_ 0, 2eEn P(s) = 1.  Prediction Suffix Trees  A prediction suffix tree T over E, is a tree of degree [E I. The edges of the tree  are labeled by symbols from E, such that from every internal node there is at most  one outgoing edge labeled by each symbol. The nodes of the tree are labeled by  pairs (s, 72) where s is the string associated with the walk starting from that node  and ending in the root of the tree, and % : E -- [0, 1] is the output probability  function related with s satisfying oez %((r): 1. A prediction suffix tree induces  178 Ron, Singer, and Tishby  probabilities on arbitrary long strings in the following manner. The probability that  T generates a string w = ww2...w in E ', denoted by PT(W), is  where s o = e, and for 1 _< i _< n - 1, sJ is the string labeling the deepest node  reached by taking the walk corresponding to wx ... wi starting at the root of T. By  definition, a prediction suffix tree induces a proper measure over E , and hence for  every prefix free set of strings {wl,..., w"}, i:1 PT(wi) -- 1, and specifically for  n _> 1, then -e: PT(S) = 1. An example of a prediction suffix tree is depicted  in Fig. 1 on theleft, where the nodes of the tree are labeled by the corresponding  suffix they present.  0.4  Figure 1: Right: A prediction suffix tree over E - {0, 1}. The strings written in  the nodes are the suffixes the nodes present. For each node there is a probability  vector over the next possible symbols. For example, the probability of observing  '1' after observing the string '010' is 0.3. Left: The equivalent probabilistic finite  automaton. Bold edges denote transitions with the symbol '1' and dashed edges  denote transitions with '0'. The states of the automaton are the leaves of the tree  except for the leaf denoted by the string 1, which was replaced by the prefixes of  the strings 010 and 110' 01 and  Finite State Automata and Markov Processes  A Probabilistic Finite Automaton (PFA) A is a 5-tuple (Q,E, r, 7, r), where Q is  a finite set of n states, E is an alphabet of size k, r  Q x E --+ Q is the transition  function, 3/  Q x E  [0, 1] is the output probability function, and r  Q  [0, 1]  is the probability distribution over the starting states. The functions 3/ and r  must satisfy the following requirements: for every q  Q, -oes'Y(q, rr) -- 1, and  qeQ r(q) -- 1. The probability that A generates a string s = sxs2... s,  E  is  PA(S) : qOeQ r(q ) I-Iix 3/(qi-l, si), where qi+l : T(qi, si).  We are interested in learning a sub-class of finite state machines which have the  following property. Each state in a machine M belonging to this sub-class is labeled  by a string of length at most L over E, for some L >_ 0. The set of strings labeling  the states is suffix free. We require that for every two states ql, q2 G Q and for every  symbol rr  E, if r(q , rr) = q and qX is labeled by a string s , then q is labeled  The Power of Amnesia 179  by a string s 2 which is a suffix of S 1 .O'. Since the set of strings labeling the states  is suffix free, if there exists a string having this property then it is unique. Thus,  in order that r be well defined on a given set of string S, not only must the set be  suffix free, but it must also have the property, that for every string s in the set and  every symbol rr, there exists a string which is a suffix of set. For our convenience,  from this point on, if q is a state in Q then q will also denote the string labeling  that state.  A special case of these automata is she case in which Q includes all 2  strings of  length L. These automata are known as Markov processes of order L. We are  interested in learning automata for which the lmmber of states, n, is actually much  smaller than 2 , which means that few states have "long memory" and most states  have a short one. We refer to these autolnata as Markov processes with bounded  memory L. In the case of Markov processes of order L, the "identity" of the states  (i.e. the strings labeling the states) is known and learning such a process reduces to  approximating the output probability function. When learning Markov processes  with bounded memory, the task of a learning algorithm is much more involved since  it must reveal the identity of the states as well.  It can be shown that under a slightly more complicated definition of prediction  suffix trees, and assuming that the initial distribution on the states is the stationary  distribution, these two models are equivalent up to a grow up in size which is at  most linear in L. The proof of this equivalence is beyond the scope of this paper, yet  the transformation from a prediction suffix tree to a finite state automaton is rather  simple. Roughly speaking, in order to implement a prediction suffix tree by a finite  state automaton we define the leaves of the tree to be the states of the automaton.  If the transition function of the automaton, r(., .), can not be well defined on this  set of strings, we might, need to slightly expand the tree and use the leaves of the  expanded tree. The output probability function of the automaton, '/(., .), is defined  based on the prediction values of the leaves of the tree. i.e., for every state (leaf)  s, and every symbol rr, '/(s, rr): %(rr). The outgoing edges from the states are  defined as follows: r(q , rr) - q2 where q2 E Suffix(qrr). An example of a finite  state automaton which corresponds to the prediction tree depicted in Fig. 1 on the  left, is depicted on the right part of the figure.  3 Learning Prediction Suffix Trees  Given a sample consisting of one sequence of length I or m sequences of lengths  l, l,..., l, we would like to find a prediction suffix tree that will have the same  statistical properties of the sample and thus can be used to predict the next outcome  for sequences generated by the same source. At each stage we can transform the  tree into a Markov process with bounded memory. Hence, if the sequence was  created by a Markov process, the algorithm will find the structure and estimate  the probabilities of the process. The key idea is to iteratively build a prediction  tree whose probability measure equals the empirical probability measure calculated  from the sample.  We start with a tree consisting of a single node (labeled by the empty string e) and  add nodes which we have reason to believe should be in the tree. A node rrs, must be  added to the tree if it statistically differs from its parent node s. A natural measure  180 Ron, Singer, and Tishby  to check the statistical difference is the relative enropy (also known as the Kullback-  Liebler (KL) divergence) [5], between the conditional probabilities P(.]s) and  P(.lrrs). Let X be an observation space and Px, P2 be probability measures over X  then the KL divergence between Px and P., is, Dz(Px]IP2 ) - xex Px(x) lg P(:)  P2(x)'  Note that this distance is not symmetric and Px should be absolutely continuous  with respect to P:. In our problem, the KL divergence measures how much addi-  tional information is gained by using the suffix as for prediction instead of predicting  using the shorter suffix s. There are cases where the statistical difference is large  yet the probability of observing the suffix as itself is so small that we can neglect  those cases. Hence we weigh the the statistical error by the prior probability of  observing as. The statistical error measure in our case is,  = P(s), P(tls)log p(a,ls )  P(')   , P(8')log p(,ls)p(s)  Therefore, a node s is added Lo the tree if the statistical difference (defined by  Err(s, s)) between the node and its parrent s is larger than a predetermined  accuracy e. The tree is grown level by level, adding a son of a given leaf in the  tree whenever the statistical surprise is large. The problem is that the requirement  that a node statistically differs from it's parent node is a necessary condition for  belonging to the tree, buL is not sucient. The leaves of a prediction sux tree must  differ from their parents (or they are redundant) but internal nodes might not have  this property. Therefore, we must continue testing further potential descendants  of the leaves in the tree up to depth L. In order to avoid exponential grow in the  number of strings tested, we do not test sLrings which belong to branches which are  reached with small probabiliW. The set of sLrings, tested t each step, is denoted  by S, and can be viewed as a kind of potential ffontier' of the growing tree T. At  each stage or when the construction is completed we can produce the equivalent  Markov process with bounded memory. The learning lgorithm of the prediction  sux tree is depicted in Fig. . The lgorihm ges two parameters: an accuracy  parameter e and the mximal order of Lhe process (which is also the maximal depth  of the tree) L.  The Lrue source probabilities are not known, hence they should be estimated from  the empirical counts of their appearances in Lhe observation sequences. Denote by  s the number of time Lhe string s appeared in the observation sequences and by  [s the number of time the symbol  appeared after the string s. Then, using  Laplace's rule of succession, the empirical estimation of the probabilities is,  P(s) + 1 P(ls) = + 1  + : +  4 A Toy Learning Example  The algorithm was applied to a 1000 symbols long sequence produced by the au-  tomaton depicted top left, in Fig. 3. The alphabet was binary. Bold lines in the  figure represent transition with the symbol '0' and dashed lines represent the sym-  bol '1' The prediction suffix tree is plotted at each stage of the algorithm. At the  The Power of Amnesia 181   Initialize the tree T and the candidate strings S:  T consists of a single root node, and S   While S  0, do the following:  1. Pick any s  S and remove it from S.  2. If Err(s, Suffix(s)) >_  then add to T the node corresponding to s  and all the nodes on the path from the deepest node in T (the deepest  ancestor of s) until s.  3. If Isl < L then for every rr  Z if/5(rrs) >_ e add rrs to S.  Figure 2: The algorithm for learning a prediction suffix tree.  end of the run the correponding automaton is plotted as well (bottom right). Note  that the original automaton and the learned automaton are the same except for  small diffrences in the transition probabilities.  0.9 i0.1'-,,,,,,,,,,p. 6  0.4 ',,  0.7 0.3  (1.3211  0.6j  0.14 0.69  0.86 l 0.31   0.4(I   0.6(I  0.32 O  0.68  0.321 0.321  0.6 0.14  0.31 0.86  I).32[  0.6  0.14   0.867 0.31  0.40A A 0.10  o.6o  0.9o  0.90 '%  0.10  ".0,   .60  0.40  0.69 0.31  Figure 3: The original automaton (top left), the instantaneous automata built along  the run of the algorithm (left to right. and top to bottom), and the final automaton  (bottom left).  5 Applications  We applied the algorithm to the Bible with L - 30 and e - 0.001 which resulted in  an automaton having less than 3000 states. The alphabet was the english letters and  the blank character. The final automaton constitutes of states that are of length  2, like 'qu' and 'xe', and on the other hand 8 and 9 symbols long states, like  'shall be' and 'there was' This indicates that the algorithm really captures  182 Ron, Singer, and Tishby  the notion of variable context length prediction which resulted in a compact yet  accurate model. Building a full Markov model in this case is impossible since  it requires [El L -- 279 states. Here we demonstrate our algorithm for cleaning  corrupted text. A test text (which was taken out of the training sequence) was  modified in two different ways. First by a stationary noise that altered each letter  with probability 0.2, and then the text was further modified by changing each  blank to a random letter. The most probable state sequence was found via dynamic  programming. The 'cleaned' observation sequence is the most probable outcome  given the knowledge of the error rate. An example of such decoding for these two  types of noise is shown in Fig. 4. We also applied the algorithm to intergenic  Original Text:  and god called the dry land earth and the gathering together of the waters called  he seas and god saw that it was good and god said let the earth bring forth grass  the herb yielding seed and the fruit tree yielding fruit after his kind  Noisy text (1):  and god cavsed the drxjland earth ibd shg gathervng together oj the waters cfiled  re seas aed god saw thctpit was good ann god said let tae earth bring forth gjasb  tse hemb yielpinl peed and thesfruit tree sielxing fzuitnafter his kind  Decoded text (1):  and god caused the dry land earth and she gathering together of the waters called  he sees and god saw that it was good and god said let the earth bring forth grass  the memb yielding peed and the fruit tree fielding fruit after his kind  Noisy text (2):  andhgodpcilledjthesdryjlandbeasthcandmthelgat ceringhlogetherjfytrezaatersoczlled  xherseasaknddgodbsawwthathit qwasoqoohanwzgodcsaidhlet dtheuejrthriringmforth  bgrasstthexherbyieldingzseedmazdctcybfruitttreeayieldinglfruztbafierihiskind  Decoded text (2):  and god called the dry land earth and the gathering together of the altars called he  seasaked god saw that it was took and god said let the earthfiring forth grass the  herb yielding seed and thy fruit treescielding fruit after his kind  Figure 4: Cleaning corrupted text using a Markov process with bounded memory.  regions of E. coli DNA, with L = 20 and e - 0.0001. The alphabet is:  The result of the algorithm is an automaton having 80 states. The names of the  states of the final automaton are depicted in Fig. 5. The performance of the model  can be compared to other models, such as the HMM based model [8], by calculating  the normalized log-likelihood (NLL) over unseen data. The NLL is an empirical  measure of the the entropy of the source as induced by the model. The NLL of  bounded memory Markov model is about the same as the one obtained by the  HMM based model. -Yet, the Markov model does not contain length distribution  of the intergenic segments hence the overall performace of the HMM based model  is slightly better. On the other hand, the HMM based model is more complicated  and requires manual tuning of its architecture.  The Power of Amnesia 183  ACTGAAACATCACCCTCGTATCTTTGGAGCGTGGAACAATAAG  ACA ATT CAA CAC CAT CAG CCA CCT CCG CTA CTC CTT CGA CGC CGT TAT  TAG TCA TCT TTA TTG TGC GAA GAC GAT GAG GCA GTA GTC GTT GTG  GGA GGC GGT AACT CAGC CCAG CCTG CTCA TCAG TCTC TTAA TTGC  TTGG TGCC GACC GATA GAGC GGAC GGCA GGCG GGTA GGTT GGTG  CAGCC TTGCA GGCGC GGTTA  Figure 5: The states that constitute the automaton for predicting the next base of  intergenie regions in E. coli DNA.  6 Conclusions and Future Research  In this paper we present a new efficient algorithm for estimating the structure and  the transition probabilities of a Markov processes with bounded yet variable mem-  ory. The algorithm when applied to natural language modeling result in a compact  and accurate model which captures the short term correlations. The theoretical  properties of the algorithm will be described elsewhere. In fact, we can prove that  a slightly different algorithm constructs a bounded memory markov process, which  with arbitrary high probability, induces distributions (over E  for n > 0) which  are very close to those induced by the 'true' Markovian source, in the sense of the  KL divergence. This algorithm uses a polynomial size sample and runs in poly-  nomial time in the relevent parameters of the problem. We are also investigating  hierarchical models based on these automata which are able to capture multi-scale  correlations, thus can be used to model more of the large scale structure of the  natural language.  Acknowledgment  We would like to thank Lee Giles for providing us with the software for plotting finite state  machines, and Anders Krogh and David Haussler for letting us use their E. coli DNA data  and for many helpful discussions. Y.S. would like to thank the Clore foundation for its  support.  References  [1] J.G Kemeny and J.L. Snell, Finite Markov Chains, Springer-Verlag 1982.  [2] Y. Freund, M. Kearns, D. Ron, R. Rubinfeld, R.E. Schapire, and L. Sellie,  Efficient Learning of Typical Finite Automata from Random Walks, STOC-93.  [3] F. Jelinek, Self-Organized Language Modeling for Speech Recognition, 1985.  [4] A. Nadas, Estimation of Probabilities in the Language Model of the IBM Speech  Recognition System, IEEE Trans. on ASSP Vol. 32 No. 4, pp. 859-861, 1984.  [5] S. Kullback, Information Theory and Statistics, New -York: Wiley, 1959.  [6] J. Rissanen and G. G. Langdon, Universal modeling and coding, IEEE Trans.  on Info. Theory, IT-27 (3), pp. 12-23, 1981.  [7] J. Rissanen, Stochastic complexity and modeling, The Ann. of Star., 14(3), 1986.  [8] A. Krogh, S.I. Mian, and D. Haussler, A Hidden Markov Model that finds genes  in E. coli DNA, UCSC Tech. Rep. UCSC-CRL-93-16.  
Dopaminergic Neuromodulation Brings a  Dynamical Plasticity to the Retina  Eric Boussard  Jean-Fran;ois Vibert  B3E, INSERM U263  Facult6 de m6decine Saint-Antoine  27 rue Chaligny  75571 Paris cedex 12  Abstract  The fovea of a mammal retina was simulated with its detailed bio-  logical properties to study the local preprocessing of images. The  direct visual pathway (photoreceptors, bipolar and ganglion cells)  and the horizontal units, as well as the D-amacrine cells were sim-  ulated. The computer program simulated the analog non-spiking  transmission between photoreceptor and bipolar cells, and between  bipolar and ganglion cells, as well as the gap-junctions between hor-  izontal cells, and the release of dopamine by D-amacrine cells and  its diffusion in the extra-cellular space. A 64x64 photoreceptors  retina, containing 16,448 units, was carried out. This retina dis-  played contour extraction with a Mach effect, and adaptation to  brightness. The simulation showed that the dopaminergic amacrine  cells were necessary to ensure adaptation to local brightness.  1 INTRODUCTION  The retina is the first stage in visual information processing. One of its functions  is to compress the information received from the environment by removing spatial  and temporal redundancies that occur in the light input signal. Modelling and  computer simulations present an efficient means to investigate and characterize the  physiological mechanisms that underlie such a complex process. In fact, filtering  depends on the quality of the input image (van Hateren, 1992):  559  560 Boussard and Vibert  1.High mean light intensity (high signal to noise ratio). A high-pass filter en-  hances the edges (contour extraction) and the temporal changes of the input.  2.Low mean light intensity (low signal to noise ratio). The sensitivity of high-  pass filters to noise makes them inefficient in this case. A low-pass filter, averaging  the signal over several receptors, is required to extract the relevant information.  There are three aspects in the filtering adaptivity displayed by the retina: adaptivity  to i) the global spatial changes in the image, ii) the local spatial changes in the  image, iii) the temporal changes in the image. We will focus on the second feature.  A biologically plausible mammalian retina was modelled and simulated to explore  the local preprocessing of the images. A first model (Bedfer &: Vibert, 1992), that  did not take into account the dopamine neuromodulation, reproduced some of the  behaviors found in the living retina, like a progressive decrease of ganglion cells'  firing rate in response to a constant image presented to photoreceptors, reversed  post-image, and optic illusion (Hermann grid). The model, however, displayed a  poor local adaptivity. It could not give both a good contrast rendering and a Mach  effect. The Mach effect is a psychophysical law that is characterized by an edge  enhancement (Ratlift, 1965). The retina network produces a double lighter and  darker contour from the frontier line between two areas of different brightness in  the stimulus. This phenomenon is indispensable for contour extraction. This paper  will first present the conditions in which high-pass filtering and low-pass filtering  occur exclusively in the retina model. These results are then compared to those  obtained with a model that includes dopamine neuromodulation, thus illustrating  the role played by dopamine in local adaptivity (Besharse &: Iuvone, 1992).  2 METHODS  The retina is an unusual neural structure: i) the photoreceptors respond to light by  an hyperpolarization, ii) signal transmission from photoreceptors to bipolar units  does not involve spikes, neurotransmitter release at these synapses is a continu-  ous function of the membrane potential (Buser & Imbert, 1987). Only ganglion  cells generate spikes. Furthermore, horizontal cells are connected by dopamine  dependent gap-junctions. Dopamine is an ubiquitous neurotransmitter and neuro-  modulator in the central nervous system. In the visual pathway, dopamine affects  several types of retinal neurons (Witkovsky &: Dearry, 1992). Dopamine is released  by stimulated D-amacrine and interplexiform cells. It diffuses in the extra-cellular  space, and produces: cone shortening and rod elongation, reduced permeability of  gap-junctions, increased conductance of glutamate-induced current among horizon-  tal cells, increased conductance of the cone-to- horizontal cell synapse, and retro-  inhibition on D-amacrine cells (Djamgoz &: Wagner, 1992). Our model focused on  the adaptive filtering mechanism in the fovea that enables the retina to simultane-  ously perform both high-pass and low-pass filtering. Therefore, dopamine action  on gap-junction between horizontal cells and the retro-inhibition on D-amacrine  cells was the only dopamine effect implemented (fig. 1). Our model included the  three neuron types of the direct pathway - photoreceptors, bipolar and ganglion  units - as well as two types of the indirect pathway - the horizontal and dopamin-  ergic amacrine cells. Only the On pathway of a mammal fovea was studied here.  Each neuron type has been modelled with its own anatomical and electrophysiolog-  Dopaminergic Neuromodulation Brings a Dynamical Plasticity to the Retina 561  release  Figure 1: The dopaminergic amacrine units in the modelled retina.  The connections of an On center pathway in the simulated retina. Photo: Pho-  toreceptors. Horiz: Horizontal units. Bip: Bipolar units. Gang: Ganglion Units.  DA: Dopaminergic Amacrine unit. DA units are stimulated by many bipolar units.  With an enough ezcitation, they can release dopamine in the extracellular space.  This released dopamine goes to modulates the conductance value of horizontal gap-  junctions.  562 Boussard and Vibert  ical properties (W'issle & Boycott, 1991)(Lewick & Dvorak, 1986). The temporal  evolution of the membrane potential of each unit can be recorded.  3 RESULTS  A 64x64 photoreceptors retina was constructed as a noisy hexagonal frame where  photoreceptors, bipolar and ganglion units were connected to their nearest neigh-  bours. Horizontal units were connected to their 18 nearest photoreceptors and bipo-  lar units, with a number of synaptic boutons decreasing as a function of distance.  They did not retroact on the nearest photoreceptor. This horizontal layer architec-  ture produces lateral inhibition. Each modelled D-amacrine unit was connected to  about fifty bipolar units. The diffusion of released dopamine in the extra-cellular  space was simulated. The modelled retina consisted of 16,448 units and 862,720  synapses.  At each simulation, the photoreceptors layer was stimulated by an input image.  Stimulations were given as a 256 x 256 pixel image presented to the simulated 64 x 64  photoreceptor retina. Since the localization of photoreceptors was not regular, each  receptor received the input from 16 pixels on the average. The output image was  reconstructed using the ganglion units response. For each of the 4096 ganglion units  the spike frequency was measured during a given time (according to the experiment)  and coded in a grey level for the given unit retinotopic position. Thus, each simu-  lation produced an image of the retina output. This output image was compared  to the input image.  The input image (stimulus) consisted here of one white disk on a dark background.  The results presented, in fig. 2, were obtained after 750 ms of stationary stimula-  tions. The stimuli were here a white disk on a black background. The inputs were  stationary to avoid temporal effects owing to evolving inputs. Output images of  stationary inputs, however, vanished after 1000 ms. The time was limited to 750  ms to optimize the quality of the output image.  Biological datas available on the conductance value suggest that in the mammalian  retina the conductance does not remain constant and undergoes a dynamical tuning  depending on the local brightness [?]. This provides a range of possible values for the  conductance. The behavior of the model was tested for values within this range.  Different values lead to different network behaviors. Three types of results were  obtained from the simulations:  1.Without dopamine action, the conductance values were fixed for all gap-junctions  to 10-% (fig. 2-A). The output image rendered well the contrast in the input image,  but did not display the Mach effect (low-pass filtering).  2.Without dopamine action, the conductance values were fixed for all gap-junctions  to 10-S (fig. 2-B). The low conductance value allowed a pronounced Mach effect,  but the contrast in the output image was strongly diminished (high-pass filtering).  This contrast appears like an average of the two brightness. Only the contour  delimited by Mach effect allows the disk to be distinguished.  3.With dopamine, the conductance values were initially set to 10-7S (fig. 2-C). The  output image displayed both the contrast rendering and the Mach effect (locally  Dopaminergic Neuromodulation Brings a Dynamical Plasticity to the Retina 563  15  :: 38  40  22  34  40-  32-  24-  16-  8-  0-  0  40-  32-  24-  16-  8-  0  A  11 22 34 45 56  B  12 2 36 48 60  25  35  65  52  39  26  13  o i  c  12 24 37 49 61  Figure 2: ontour extraction (Mach effect) according to gap-junctions conduc-  tances.  On the left, results obtained after 750 ms of stimulation for an mage of a white  disk on a black background. On the right, sections through the corresponding image.  Abscissa: spike count; Ordinates: geographic position of the unit, from the left side  to the middle of the left panel. A: without dopamine (fixed Ggap -- 10-6S). B:  without dopamine (fixed Ggap -- 10-1S). C: with dopamine release (starting Ggap  ---- 10-7S). A gives a good contrast rendering, but no Mach effect. B gives a Mach  effect, but there is an averaging between darker and lighter areas. C, with dopamin-  ergic neuromodulation, gives both a Mach effect and a good contrast rendering.  564 Boussard and Vibert  adaptive filtering).  4 DISCUSSION  These results show that the conductance cannot be fixed at a single value for all  the gap-junctions. If the conductance value is high (fig. 2-A), the model acts like  a low-pass filter. A good contrast rendering was obtained, but there was no Mach  effect. If the conductance value is low (fig. 2-B), the model becomes a high-pass  filter. A Mach effect was obtained, but the contrast in the post-retinal image was  dramatically deteriorated: an undesirable averaging of the brightness between the  darker and the more illuminated areas appeared. Therefore in this model the Mach  effect was only obtained at the expense of the contrast. A mammalian retina is able  to perform both contrast rendering and contour extraction functions together. It  works like an adaptive filter. To obtain a similar result, it is necessary to have a  variable communication between horizontal units. The simulated retina needs low  gap-junctions conductance in the high light intensity areas and high conductance  in the low light intensity areas. The conductance of each gap-junction must be  tuned according to the local stimulation. The model used to obtain the fig. 2-C  takes into account the dopamine release by the D-amacrine cells. Here, the network  performs the two antagonist functions of filtering. Dopamine provides our model  with the capacity to have a biological behaviour. What is the action of dopamine  on network? Dopamine is released by D-amacrine units. Then, it diffuses from its  release point into the extra-cellular space among the neurons, reaches gap-junctions  and decreases their conductance value. Thus the conductance modulation depends  in time and in intensity on the distance between gap-junction and D-amacrine unit.  In addition, this action is transient.  5 CONCLUSION  Thanks to dopamine neuromodulation, the network is able to subdivise itself  into several subnetworks, each having the appropriate gap-junction conductance.  Each subnetwork is thus adapted for a better processing of the external stimulus.  Dopamine neuromodulation is a chemically addressed system, it acts more diffusely  and more slowly than transmission through the axo-synaptic connection system.  Therefore neuromodulation adds a dynamical plasticity to the network.  References  G. Bedfer &; J.-F. Vibert. (1992) Image preprocessing in simulated biological retina.  Proc. lJth Ann. Conf. IEEE EMBS 1570-1571.  J. Besharse &; P. Iuvone. (1992). Is dopamine a light-adaptive or a dark-adaptive  modulator in retina? NeuroChemistry International 20:193-199.  P. Buser &; M. Imbert. (1987) Vision. Paris: Hermann.  M. Djamgoz &; H.-J. Wagner. (1992) Localization and function of dopamine in the  adult vertebrate retina. NeuroC'hemistry International 20:139-191.  L. Dowling. (1986) Dopamine: a retinal neuromodulator? Trends In NeuroSciences  Dopaminergic Neuromodulation Brings a Dynamical Plasticity to the Retina 565  9:236-240.  W. Levick & D. Dvorak. (1986) The retina - from molecule to network. Trends In  Neuro$ciences 9:181-185.  F. Ratlift. (1965) Mach bands: quantitative studies on neural network in the retina.  Holden-Day.  J. H. van Hateren. (1992) Real and optimal images in early vision. Nature 360:68-  70.  H. Wssle & B. B. Boycott. (1991) Functional architecture of the mammalian  retina. Physiological Reviews 71(2):447-479.  P. Witkovsky & A. Dearry. (1992) Functional roles of dopamine in the vertebrate  retina. Retinal Research 11:247-292.  
I  A Comparative Study Of A Modified  Bumptree Neural Network With Radial Basis  Function Networks and the Standard Multi-  Layer Perceptron.  Richard T.J. Bostock and Alan J. Harget  Department of Computer Science & Applied Mathematics  Aston University  Birmingham  England  Abstract  Bumptrees are geometric data structures introduced by Omohundro  (1991) to provide efficient access to a collection of functions on a  Euclidean space of interest. We describe a modified bumptree structure  that has been employed as a neural network classifier, and compare its  performance on several classification tasks against that of radial basis  function networks and the standard mutli-layer perceptron.  1 INTRODUCTION  A number of neural network studies have demonstrated the utility of the multi-layer  perceptron (MLP) and shown it to be a highly effective paradigm. Studies have also  shown, however, that the MLP is not without its problems, in particular it requires an  extensive training time, is susceptible to local minima problems and its performance is  dependent upon its internal network architecture. In an attempt to improve upon the  generalisation performance and computational efficiency a number of studies have been  undertaken principally concerned with investigating the parametrisation of the MLP. It is  well known, for example, that the generalisation performance of the MLP is affected by  the number of hidden units in the network, which have to be determined empirically since  theory provides no guidance. A number of investigations have been conducted into the  possibility of automatically determining the number of hidden units during the training  phase (Bostock, 1992). The results show that architectures can be attained which give  satisfactory, although generally sub-optimal, performance.  Alternative network architectures such as the Radial Basis Function (RBF) network have  also been studied in an attempt to improve upon the performance of the MLP network.  The RBF network uses basis functions in which the weights are effective over only a  small portion of the input space. This is in contrast to the MLP network where the  weights are used in a more global fashion, thereby encoding the characteristics of the  training set in a more compact form. RBF networks can be rapidly trained thus making  240  Modified Bumptree Neural Network and Standard Multi-Layer Perceptron 241  them particularly suitable for situations where on-line incremental learning is required.  The RBF network has been successfully applied in a number of areas such as speech  recognition (Renals, 1992) and financial forecasting (Lowe, 1991). Studies indicate that  the RBF network provides a viable alternative to the MLP approach and thus offers  encouragement that networks employing local solutions are worthy of further  investigation.  In the past few years there has been an increasing interest in neural network architectures  based on tree structures. Important work in this area has been carded out by Omohundro  (1991) and Gentric and Withagen (1993). These studies seem to suggest that neural  networks employing a tree based structure should offer the same benefits of reduced  training time as that offered by the RBF network. The particular tree based architecture  examined in this study is the bumptree which provides efficient access to collections of  functions on a Euclidean space of interest. A bumptree can be viewed as a natural  generalisation of several other geometric data structures including oct-trees, k-d trees,  balltrees (Omohundro, 1987) and boxtrees (Omohundro, 1989).  In this paper we present the results of a comparative study of the performance of the three  types of neural networks described above over a wide range of classification problems.  The performance of the networks was assessed in terms of the percentage of correct  classifications on a test, or generalisation data set, and the time taken to train the  network. Before discussing the results obtained we shall give an outline of the  implementation of our bumptree neural network since this is more novel than the other  two networks.  2 THE BUMPTREE NEURAL NETWORK  Bumptree neural networks share many of the underlying principles of decision trees but  differ from them in the manner in which patterns are classified. Decision trees partition  the problem space into increasingly small areas. Classification is then achieved by  determining the lowest branch of the tree which contains a reference to the specified point.  The bumptree neural network described in this paper also employs a tree based structure to  partition the problem space, with each branch of the tree being based on multiple  dimensions. Once the problem space has been partitioned then each branch can be viewed  as an individual neural network modelling its own local area of the problem space, and  being able to deal with patterns from multiple output classes.  Bumptrees model the problem space by subdividing the space allowing each division to  be described by a separate function. Initial partitioning of the problem space is achieved  by randomly assigning values to the root level functions. A learning algorithm is applied  to determine the area of influence of each function and an associated error calculated. If  the error exceeds some threshold of acceptability then the area in question is further  subdivided by the addition of two functions; this process continues until satisfactory  performance is achieved. The bumptree employed in this study is essentially a binary tree  in which each leaf of the tree corresponds to a function of interest although the possibility  exists that one of the functions could effectively be redundant if it fails to attract any of  the patterns from its parent function.  A number of problems had to be resolved in the design and implementation of the  bumptree. Firstly, an appropriate procedure had to be adopted for partitioning the  242 Bostock and Harget  problem space. Secondly, consideration had to be given to the type of learning algorithm  to be employed. And finally, the mechanism for calculating the output of the network  had to be determined. A detailed discussion of these issues and the solutions adopted now  follows.  2.1 PARTITIONING THE PROBLEM SPACE  The bumptree used in this study employed gaussian functions to partition the problem  space, with two functions being added each time the space was partitioned. Patterns were  assigned to whichever of the functions had the higher activation level with the restfiction  that the functions below the root level could only be active on patterns that activated their  parents. To calculate the activation of the gaussian function the following expression  was used:  A = exp -( '5'((In?C>/a)2* 1 / (a t * /3.14159 * 2) (1)  where Afp is the activation of function f on pattern p over all the input dimensions, aft is  the radius of function f in input dimension i, Cfi is the centre of function f in input  dimension i, and Inpi is the ith dimension of the pth input vector.  It was found that the locations and radii of the functions had an important impact on the  performance of the network. In the original bumptree introduced by Omohundro every  function below the root level was required to be wholly enclosed by its parent function.  This restriction was found to degrade the performance of the bumptree particularly if a  function had a very small radius since this would produce very low levels of activation for  most patterns. In our studies we relaxed this constraint by assigning the radius of each  function to one, since the data presented to the bumptree was always normalised between  zero and one. This modification led to an improved performance.  A number of different techniques were examined in order to effectively position the  functions in the problem space. The first approach considered, and the simplest, involved  selecting two initial sets of centres for the root function with the centre in each dimension  being allocated a value between zero and one. The functions at the lower levels of the tree  were assigned in a similar manner with the requirement that their centres fell within the  area of the problem space for which their parent function was active. The use of non-  hierarchical clustering techniques such as the Forgy method or the K-means clustering  technique developed by MacQueen provided other alternatives for positioning the  functions. The approach finally adopted for this study was the multiple-initial function  (MIF) technique.  In the MIF procedure ten sets of functions centres were initially defined by random  assignment and each pattern in the training set assigned to the function with the highest  activation level. A "goodness" measure was then determined for each function over all  patterns for which the function was active. The goodness measure was defined as the  square of the error between the calculated and observed values divided by the number of  active patterns. The function with the best value was retained and the remaining  functions that were active on one or more patterns had their centres averaged in each  dimension to provide a second function. The functions were then added to the network  structure and the patrems assigned to the function which gave the greater activation.  Modified Bumptree Neural Network and Standard Multi-Layer Perceptron 243  2.2 THE LEARNING ALGORITHM  A bumptree neural network comprises a number of functions each function having its  own individual weight and bias parameters and each function being responsive to different  characteristics in the training set. The bumptree employed a weighted value for every  input to output connection and a single bias value for each output unit. Several different  learning algorithms for determining the weight and bias values were considered together  with a genetic algorithm approach (Williams, 1993). A one-shot learning algorithm was  finally adopted since this gave good results and was computationally efficient. The  algorithm used a pseudo-matrix inversion technique to determine the weight and bias  parameters of each function after a single presentation of the relevant patterns in the  training set had been made. The output of any function for a given pattern p was  determined from  + (2)  j=l  where aoip z is the output of the zth output unit of the ith function on the pth pattern, j is  the input unit, jmax is the total number of input units, oij z is the weight that connects  the jth input unit to the zth output unit for the ith function, xj(P) is the element of the  pth pattern concerned with the jth input dimension, and 13iz is the bias value for the zth  output unit.  The weight and bias parameters were determined by minimising the squared error given in  (3), where E i is the error of the ith function across all output dimensions (zmax), for all  patterns upon which the function is active (pmax). The desired output for the zth output  dimension is tVpz,, and Ooipz is the actual output of the ith function on the zth  dimension of the pth pattern. The weight values are again represented by jz and the bias  by iz.  1 prnaxzmax jmax  E, = 7 Z Z { Z aiizX ') + }2 (3)  p=l z=l j=l  After the derivatives of Cqjz and iz were determined it was a simple task to arrive at the  three matrices used to calculate the weight and bias values for the individual functions.  Problems were encountered in the matrix inversion when dealing with functions which  were only active on a few patterns and which were far removed from the root level of the  tree; this led to difficulties with singular matrices. It was found that the problem could be  overcome by using the Gauss-Jordan singular decomposition technique for the pseudo-  inversion of the matrices.  2.3 CALCULATION OF THE NETWORK OUTPUT  The difficulty in determining the output of the bumptree was that there were usually  functions at different levels of the tree that gave slightly different outputs for each active  pattern. Several different approaches were studied in order to resolve the difficulty  including using the normalised output of all the active functions in the tree irrespective of  their level in the structure. A technique which gave good results and was used in this  244 Bostock and Harget  study calculated the output for a pattern solely on the output of the lowest level active  function in the tree. The final output class of a pattern being given by the output unit  with the highest level of activation.  3 NETWORK PERFORMANCES  The performance of the bumptree neural network was compared against that of the  standard MLP and RBF networks on a number of different problems. The bumptree used  the MIF placing technique in which the radius of each function was set to one. This  particular implementation of the bumptree will now be referred to as the MIF bumptree.  The MLP used the standard backpropagation algorithm (Rumelhart, 1986) with a  learning rate of 0.25 and a momentum value of 0.9. The initial weights and bias values  of the network were set to random values between -2 and +2. The number of hidden units  assigned to the network was determined empirically over several runs by varying the  number of hidden units until the best generalisation performance was attained. The RBF  network used four different types of function, they were gaussian, multi-quadratic,  inverse multi-quadratic and thin plate splines. The RBF network placed the functions  using sample points within the problem space covered by the training set.  3.1 INITIAL STUDIES  In the initial studies, a set of classical non-linear problems was used to compare the  performance of the three types of networks. The set consisted of the XOR, Parity(6) and  Encoder(8) problems. The average results obtained over 10 runs for each of the data sets  are shown in Table 1 - the figures presented are the percentage of patterns correctly  classified in the training set together with the standard deviation.  Table 1. Percentage of Patterns Correctly Classified for the three Data Sets for each  Network type.  DATA SET MLP RBF MIF  XOR 100 100 100  Parity(6) 100 92.1 + 4.7 98.3 + 4.2  Encoder(8) 100 82.5 + 16.8 100  For the XOR problem the MLP network required an average of 222 iterations with an  architecture of 4 hidden units, for the parity problem an architecture of 10 hidden units and  an average of 1133 iterations, and finally for the encoder problem the network required an  average of 1900 iterations for an architecture consisting of three hidden units.  The RBF network correctly classified all the patterns of the XOR data set when four  multi-quadratic, inverse multi-quadratic or gaussian functions were used. For the parity(6)  problem the best result was achieved with a network employing between 60 and 64  inverse multi-quadratic functions. In the case of the encoder problem the best performance  was obtained using a network of 8 multi-quadratic functions.  The MIF bumptree required two functions to achieve perfect classification for the XOR  and encoder problems and an average of 40 functions in order to achieve the best  performance on the parity problem. Thus in the case of the XOR and encoder problems  no further functions were required additional to the root functions.  Modified Bumptree Neural Network and Standard Multi-Layer Perceptron 245  A comparison of the training times taken by each of the networks revealed considerable  differences. The MLP required the most extensive training time since it used the  backpropagation training algorithm which is an iterative procedure. The RBF network  required less training time than the MLP, but suffered from the fact that for all the  patterns in the training set the activity of all the functions had to be calculated in order to  arrive at the optimal weights. The bumptree proved to have the quickest training time for  the parity and encoder problems and a training time comparable to that taken by the RBF  network for the XOR problem. This superiority arose because the bumptree used a non-  iterative training procedure, and a function was only trained on those members of the  training set for which the function was active.  In considering the sensitivity of the different networks to the parameters chosen some  interesting results emerge. The performance of the MLP was found to be dependent on  the number of hidden units assigned to the network. When insufficient hidden units were  allocated the performance of the MLP degraded. The performance of the RBF network  was also found to be highly influenced by the values taken for various parameters, in  particular the number and type of functions employed by the network. The bumptree on  the other hand was assigned the same set of parameters for all the problems studied and  was found to be less sensitive than the other two networks to the parameter settings.  3.2 COMPARISON OF GENERALISATION PERFORMANCE  The performance of the three different networks was also measured for a set of four 'real-  world' problems which allowed the generalisation performance of each network to be  determined. A summary of the results taken over 10 runs is given in Table 2.  Table 2 Performance of the Networks on the Training and Generalisation Data Sets of the  Test Problems.  DATA NETWORK FUNCTIONS TRAINING TEST  HIDDEN UNITS  Iris  Skin  Cancer  Vowel  Data  Diabetes  MLP 4 100 95.7 _+ 0.6  RBF 75 gaussians 100 96.0 _+ 0.0  MIF 8 100 97.5 + 0.4  MLP 6 88.7 -+ 4.3 79.2 + 1.7  RBF 10 multi-quad 84.4 _+ 3.2 80.3 _+ 4.4  MIF 4 79.8 _+ 5.2 80.8 + 1.9  MLP 20 82.4 _+ 5.3 77.1 + 6.6  RBF 50 Thin plate spl. 82.1 _+ 1.5 77.8 + 1.4  MIF 104 86.5 _+ 5.6 73.6 + 4.6  MLP 16 82.5 _+ 2.7 78.9 _+ 1.2  RBF 25 Thin plate spl. 76.0 _+ 0.8 78.9 _+ 0.9  MIF 3 76.5 +_ 1.2 80.0 + 1.1  All three networks produce a comparable performance on the test problems, but in the  case of the bumptree this was achieved with a training time substantially less than that  required by the other networks. Inspection of the results also shows that the bumptree  required fewer functions in general than the RBF network.  246 Bostock and Harget  The results shown above for the bumptree were obtained with the same set of parameters  used in the initial study which further confirms its lack of sensitivity to parameter  settings.  4. CONCLUSION  A comparative study of the performance of three different types of networks, one of which  is novel, has been conducted on a wide range of problems. The results show that the  performance of the bumptree compared very favourably, both in terms of generalisation  and training times, with the more traditional MLP and RBF networks. In addition, the  performance of the bumptree proved to be less sensitive to the parameters settings than  the other networks. These results encourage us to continue further investigation of the  bumptree neural network and lead us to conclude that it has a valid place in the list of  current neural networks.  Acknowledgement  We gratefully acknowledge the assistance given by Richard Rohwer.  References  Bostock R.T.J. & Harget A.J. (1992) Towards a Neural Network Based System for Skin  Cancer Diagnosis: IEE Third International Conference on Artificial Neural Networks:  P215-220.  Broomhead D.S. & Lowe D. (1988) Radial Basis Functions, Multi-Variable Functional  Interpolation and Adaptive Networks: RSRE Memorandum No. 4148, Royal Signals and  Radar Establishment, Malvern, England.  Gentric P. & Withagen H.C.A.M. (1993) Constructive Methods for a New Classifier  Based on a Radial Basis Function Network Accelerated by a Tree: Report, Eindhoven  Technical University, Eindhoven, Holland.  Lowe D. & Webb A.R. (1991) Time Series Prediction by Adaptive Networks: A  Dynamical Systems Perspective: IEE Proceedings-F, vol. 128(1), Feb.,, P17-24.  Moody J. & Darken C. (1988) Learning With Localized Receptive Fields: Research  Report YALEU/DCS/RR-649.  Omohundro S.M. (1987) Efficient Algorithms With Neural Network Behaviour; in  Complex Systems 1 (1987): P273-347.  Omohundro S.M. (1989) Five Balltree Construction Algorithms: International  Computer Science Institute Technical Report TR-89-063.  Omohundro S.M. (1991) Bumptrees for Efficient Function, Constraint, and  Classification Learning: Advances in Neural Information Processing Systems 3, P693-  699.  Renals S. & Rohwer R.J. (1989) Phoneme Classification Experiments Using Radial  Basis Functions: Proceedings of the HCNN, P461-467.  Rumelhart D.E., Hinton G.E. & Williams R.J. (1986) Learning Internal Representations  by Error Propagation: in Parallel Distributed Processing, vol. 1 P318-362. Cambridge,  MA: MIT Press.  Williams B.V., Bostock R.T.J., Bounds D.G. & Harget A.J. (1993) The Genetic  Bumptree Classifier: Proceedings of the BNSS Symposium on Artifi'cial Neural  Networks: to be published.  
Asynchronous Dynamics of Continuous  Time Neural Networks  Xin Wang  Computer Science Department  University of California at Los Angeles  Los Angeles, CA 90024  Qingnan Li  Department of Mathematics  University of Southern California  Los Angeles, CA 90089-1113  Edward K. Blum  Department of Mathematics  University of Southern California  Los Angeles, CA 90089-1113  ABSTRACT  Motivated by mathematical modeling, analog implementation and  distributed simulation of neural networks, we present a definition of  asynchronous dynamics of general CT dynamical systems defined  by ordinary differential equations, based on notions of local times  and communication times. We provide some preliminary results  on globally asymptotical convergence of asynchronous dynamics  for contractive and monotone CT dynamical systems. When ap-  plying the results to neural networks, we obtain some conditions  that ensure additive-type neural networks to be asynchronizable.  1 INTRODUCTION  Neural networks are massively distributed computing systems. A major issue in par-  allel and distributed computation is synchronization versus asynchronization (Bert-  sekas and Tsitsiklis, 1989). To fix our idea, we consider a much studied additive-type  model (Cohen and Grossberg, 1983; Hopfield, 1984; Hirsch, 1989) of a continuous-  time (CT) neural network of n neurons, whose dynamics is governed by  ki(t) = -aixi(t) + E wiJ'J(ljxj  + Ii, i = 1,2, ...,n, (1)  j=l  493  494 Wang, Li, and Blum  with neuron states zi(t) at time t, constant decay rates ai, external inputs h, gains  /d, neuron activation functions a i and synaptic connection weights wij. Simu-  lation and implementation of idealized models of neural networks such as (1) on  centralized computers not only limit the size of networks, but more importantly  preclude exploiting the inherent massive parallelism in network computations. A  truly faithful analog implementation or simulation of neural networks defined by  (1) over a distributed network requires that neurons follow a global clock t, com-  municate timed states xj(t ) to all others instantaneously and synchronize global  dynamics precisely all the time (e.g., the same x I (t) should be used in evolution of  all xi(t) at time t). Clearly, hardware and software realities make it very hard and  sometimes impossible to fulfill these requirements; any mechanism used to enforce  such synchronization may have an important effect on performance of the net-  work. Moreover, absolutely insisting on synchronization contradicts the biological  manifestation of inherent asynchrony caused by delays in nerve signal propagation,  variability of neuron parameters such as refractory periods and adaptive neuron  gains. On the other hand, introduction of asynchrony may change network dynam-  ics, for example, from convergent to oscillatory. Therefore, validity of asynchronous  dynamics of neural networks must be assessed in order to ensure desirable dynamics  in a distributed environment.  Motivated by the above issues, we study asynchronous dynamics of general CT dy-  namical systems with neural networks in particular. Asynchronous dynamics has  been thoroughly studied in the context of iterative maps or discrete-time (DT) dy-  namical systems; see, e.g., (Bertsekas and Tsitsiklis, 1989) and references therein.  Among other results are that P-contractive maps on It n (Baudet, 1978) and contin-  uous maps on partially ordered sets (Wang and Parker, 1992) are asynchronizable,  i.e., any asynchronous iterations of these maps will converge to the fixed points  under synchronous (or parallel) iterations. The synchronization issue has also been  addressed in the context of neural networks. In fact, the celebrated DT Hopfield  model (Hopfield, 1982) adopts a special kind of asynchronous dynamics: only one  randomly chosen neuron is allowed to update its state at each iterative step. The  issue is also studied in (Barhen and Gulati, 1989) for CT neural networks. The  approach there is, however, to convert the additive model (1) into a DT version  through the Euler discretization and then to apply the existing result for contrac-  tive mappings in (Baudet, 1978) to ensure the discretized system to be asynchro-  nizable. Overall, studies for asynchronous dynamics of CT dynamical systems are  still lacking; there are even no reasonable definitions for what it means, at least to  our knowledge.  In this paper, we continue our studies on relationships between CT and DT dy-  namical systems and neural networks (Wang and Blum, 1992; Wang, Blum and Li,  1993) and concentrate on their asynchronous dynamics. We first extend a concept  of asynchronous dynamics of DT systems to CT systems, by identifying the distinc-  tion between synchronous and asynchronous dynamics as (i) presence or absence of  a common global clock used to synchronize the dynamics of the different neurons  and (ii) exclusion or inclusion of delay times in communication between neurons,  and present some preliminary results for asynchronous dynamics of contractive and  monotone CT systems.  Asynchronous Dynamics of Continuous Time Neural Networks 495  2 MATHEMATICAL FORMULATION  To be general, we consider a CT dynamical system defined by an n-dimensional  system of ordinary differential equations,  ki(t) = fi(ah(t),...,xn(t)), i= 1,2,...,n,  (2)  where fi: R n '--* R are continuously differentiable and x(t)  R n for all t in R+ (the  set of all nonnegative real numbers). In contrast to the asynchronous dynamics  given below, dynamics of this system will be called synchronous. An asynchronous  scheme consists of two families of functions ci : R+  R+ and rj : R+  R+,  i, j = 1, ..., n, satisfying the following constraints: for any t _> 0,  (i) Initiation: ci(t) >_ 0 and rj(t) _> 0;  (ii) Non-starvation: ci's are differentiable and ki(t) > 0;  (iii) Liveness: limt_.. ci(t) = cx> and limt_... rj(t)  (iv) Accessibility: rj(t) _< cj(t).  Given an asynchronous scheme ({ci}, {rj}), the associated asynchronous dynamics  of the system (2) is the solution of the following parametrized system:  i(Ci()) --'  We shall call this system an asynchronized system of the original one (2).  The functions ci(t) should be viewed as respective "local" times (or clocks) of com-  ponents i, as compared to the "global" time (or clock) t. As each component i  evolves its state according to its local time ci(t), no shared global time t is needed  explicitly; t only occurs implicitly. The functions rj(t) should be considered as time  instants at which corresponding values xj of components j are used by component  i; hence the differences (cj(t)- rj(t)) _> 0 can be interprated as delay times in  communication between the components j and i. Constraint (i) reflects the fact  that we are interested in the system dynamics after some global time instance, say  0; constraint (ii) states that the functions ci are monotone increasing and hence the  local times evolve only forward; constraint (iii) characterizes the liveness property  of the components and communication channels between components; and, finally,  constraint (iv) precludes the possibility that component i accesses states xj ahead  of the local times cj(t) of components j which have not yet been generated.  Notice that, under the assumption on monotonicity of ci (t), the inverses c- x (t) exist  and the asynchronized system (3) can be transformed into  9,(t) = a,(t) 9}(t), ...,  (4)  by letting yi(t) = xi(ci(t)) and .0j (t) = xj(rj(t)) = yj(cj-X(rj(t)) for i,j = 1,2,..., n.  The vector form of (4) can be given by  t = C'F[] (5)  496 Wang, Li, and Blum  w_here y(t) = [yx (t), ..., yn(t)] T, C' = diag(dc (t)/dt, ..., dcn(t)/dt), F = [f, ..., fn] T  Y = [] and  .,  r[?] = f2((t),(t),...,}(t)) .  Notice that the complication in the way F applies to  simply means that every  component i will use possibly different "global" states [.O(t),.O}(t), ...,-i  y.(t)]. This  peculiarity makes the equation (5) fit into none of the categories of general functional  differential equations (Hale, 1977). However, if rj(t) for i : 1,...,n are equal,  all the components will use a same global state .0 = [.O(t),.O(t),...,(t)l and  the asynchronized system (5) assumes a form of retarded functional differential  equations,  = (6)  We shall call this case uniformly-delayed, which will be a main consideration in the  next section where we discuss asynchronizable systems.  The system (5) includes some special cases. In a no communication delay situation,  rj(t) = cj(t) for alii and the system (5) reduces to : C'F(y). This includes the  simplest case where the local times ci(t) are taken as constant-time scalings cit of  the global time t; specially, when all ci(t) = t the system goes back to the original  one (2). If, on the other hand, all the local times are identical to the global time t  and the communication times take the form of vj(t) = t - 0(t) one obtains a most  general delayed system  i(t) -- fi(Yl(t --O(t)),y2(t -- 0}(/)), ...,yn(t- O/n(/))),  (7)  where the state yj(t) of component j may have different delay times O(t) for dif-  ferent other components i.  Finally, we should point out that the above definitions of asynchronous schemes and  dynamics are analogues of their counterparts for DT dynamical systems (Bertsekas  and Tsitsiklis, 1989; Blum, 1990). Usually, an asynchronous scheme for a DT  system defined by a map f  X -4 X, where X = Xx x X2 x  .. x X,, consists of a  family {T i C_Nli: 1, ...,n} of subsets of discrete times (N) at which components  i update their states and a family {vj  N -4 Nli = 1, 2, ..., n} of communication  times. Asynchronous dynamics (or chaotic iteration, relaxation) is then given by  ;ri(l q- 1) -- { fi(Xl(T())'''"X"(Tni())) if t  T   xi(t) otherwise.  Notice that the sets T i can be interpreted as local times of components i. In fact,  one can define local time functions ci ' N -4 N as ci(O) = 0 and ci(t + 1) = ci(t) + 1  if t  T/ and ci(t) otherwise. The asynchronous dynamics can then be defined by  xi(t + 1)- xi(t) = (ci(t + 1)-ci(t))(f(x(r(t)),...,,(r(t)))- x(t)),  which is analogous to the definition given in (4).  Asynchronous Dynamics of Continuous Time Neural Networks 497  3 ASYNCHRONIZABLE SYSTEMS  In general, we consider a CT dynamical system as asynchronizable if its synchronous  dynamics (limit sets and their asymptotic stability) persists for some set of asyn-  chronous schemes. In many cases, asynchronous dynamics of an arbitrary CT sys-  tem will be different from its synchronous dynamics, especially when delay times  in communication are present. An example can be given for the network (1) with  symmetric matrix W. It is well-known that (synchronous) dynamics of such net-  works is quasi-convergent, namely, all trajectories approach a set of fixed points  (Hirsch, 1989). But when delay times are taken into consideration, the networks  may have sustained oscillation when the delays exceed some threshold (Marcus and  Westervelt, 1989). A more careful analysis on oscillation induced by delays is given  in (Wu, 1993) for the networks with symmetric circulant weight matrices.  Here, we focus on asynchronizable systems. We consider CT dynamical systems on  R n of the following general form  A(t) = -x(t) + F(t)) (8)  where x(t)  R , A = diag(ax,a2,...,a, 0 with ai > 0 and F = [fi]  Cx(R). It  is easy to see that a point x  R'* is a fixed point of (8) if and only if x is a fixed  point of the map F. Without loss of generality, we assume that 0 is a fixed point  of the map F. According to (5), the asynchronized version of (8) for an arbitrary  asynchronous scheme ({ci), {rj}) is  = + F[?]), (9)  where 0 [9}(t),-2 t ...,  = y2(), 92(t)].  3.1 Contractlye Systems  Our first effort attempts to obtain a result similar to the one for P-contractive  maps in (Baudet, 1978). We call the system (8) strongly P-contractive if there is  symmetric and invertible matrix $ such that I$ -xF($x)[ <_ Ix I for all x G R n and  [$-XF($x)[ = Ix[ only for x = 0; here Ix[ denotes the vector with components  and <_ is component-wise.  Theorem 1 If the system (8) is strongly P-contractive, then it is asynchronizable  for any asynchronous schemes without self time delays (i.e., rj(t) - ci(t) for all  i= 1,2,...,n).  Proof. It is not hard to see that synchronous dynamics of a strongly P-contractive  system is globally convergent to the fixed point 0. Now, consider the transformation  z = A-y and the system for z  A = C'(-z + $- F[S2]) = C'(-z + a[2]),  where G[,] = S-XFS[,]. This system has the same type of dynamics as (9).  Define a function E: R+ x R'*  R+ by E(t) = z ' (t)Az(t)/2, whose derivative  with respect to t is  k = z T C' (-z q- S()) < I1'11 (-z T z q-Izl T IS()l) < 11'11( -zT z + Izl T Izl) < 0.  498 Wang, Li, and Blum  Hence E is an energy function and the asynchronous dynamics converges to the  fixed point 0. cl  Our second result is for asynchronous dynamics of contractive systems with no  communication delay. The system (8) is called contractive if there is a real constant  0 < c < 1 such that  II?(x)- r(y)11 < allz- yll  for all x, y 6 Rn; here II. II denotes the usual Euclidean norm on R n.  Theorem 2 If the system (8) is contractive, then it is asynchronizable for asyn-  chronous schemes with no communication delay.  Proof. The synchronous dynamics of contractive systems is known to be globally  convergent to a unique fixed point (Kelly, 1990). For an asynchronous scheme with  no communication delay, the system (8) is simplified to Ay: C (-y + F(y)). W  consider again the function E = yq'Ay/2, which is an energy function as shown  below.  :k = y'C' (-y + r(y)) 5 IIC'11(-IlYll 2 q- [lYll [Ir(y)[I) < 0.  Therefore, the asynchronous dynamics converges to the fixed point 0. r  For the additive-type neural networks (1), we have  Corollary 1  type with 0 < a(z) _< supzcR a(z) = 1. If it satisfies the condition  IM-xWMII < 1,  where M -- diag(yx,...,yn), then it is asynchronizable for any  schemes with no communication delay.  Let the network (1) have neuron activation functions ri of sigmoidal  (10)  asynchronous  Proof. The condition (10) ensures the map F(x) = A-1Wo'(Mx)q- A-I to be  contractive. r  Notice that the condition (10) is equivalent to many existing ones on globally asymp-  totteal stability based on various norms of matrix W, especially the contraction con-  dition given in (Kelly, 1990) and some very recent ones in (Matsuoka, 1992). The  condition (10) is also related very closely to the condition in (Barhen and Gulati,  1989) for asynchronous dynamics of a discretized version of (1) and the condition  in (Marcus and Westervelt, 1989) for the networks with delay.  We should emphasize that the results in Theorem 2 and Corollary 1 do not directly  follow from the result in (Kelly, 1990); this is because local times ci(t) are allowed  to be much more general functions than linear ones ci t.  3.2 Monotone Systems  A binary relation -g on R n is called a partial order if it satisfies that, for all x, y, z 6  [i n , (i) x -g x; (ii) x -g y and y -g x imply x = y; and (iii) x -g y and y -g z  imply x -g z. For a partial order -g on [i n , define << on [i n by x << y iffx < y  and xi : yi for all i - 1,..-, n. A map F  [i n --+ [i n is monotone if x _ y implies  Asynchronous Dynamics of Continuous Time Neural Networks 499  F(x)  F(y). ACT dynamical system ofthe form (2) is monotone if xx -4_ x2 implies  the trajectories x(t),x2(t) with z(0) = z and x2(0) = x2 satisfy x(t) _ x2(t)  for all t _ 0 (Hirsch, 1988).  Theorem 3 If the map F in (8) is monotone, then the system (8) is asynchroniz-  able for uniformly-delayed asynchronous schemes, provided that all orbits x(t) have  compact orbit closure and there is a to > 0 with x(to) >> x(O) or x(to) << x(O).  Proof. This is an application of a Henry's theorem (see Hirsch, 1988) that im-  plies that the asynchronized system (9) in the no communication delay situation  is monotone and Hirsch's theorem (Hirsch, 1988) that guarantees the asymptotic  convergence of monotone systems to fixed points. r  Corollary 2 If the additive-type neural network (I) with sigmoidal activation func-  tions is cooperative (/.e., wij _> 0 for i k j (Hirsch, 1988 and I989)), then it is  asynchronizable for uniformly-delayed asynchronous schemes, provided that there is  a to > 0 with x(to) >> x(O) or x(to) << x(O).  Proof. According to (Hirsch, 1988), cooperative systems are monotone. As the  network has only bounded dynamics, the result follows from the above theorem. []  4 CONCLUSION  By incorporating the concepts of local times and communication times, we have  provided a mathematical formulation of asynchronous dynamics of continuous-time  dynamical systems. Asynchronized systems in the most general form haven't been  studied in theories of dynamical systems and functional differential equations. For  contractive and monotone systems, we have shown that for some asynchronous  schemes, the systems are asynchronizable, namely, their asynchronizations preserve  convergent dynamics of the original (synchronous) systems. When applying these  results to the additive-type neural networks, we have obtained some special condi-  tions for the networks to be asynchronizable.  We are currently investigating more general results for asynchronizable dynamical  systems, with a main interest in oscillatory dynamics.  References  G. M. Baudet (1978). Asynchronous iterative methods for multiprocessors. Journal  of the Association for Computing Machinery, 25:226-244.  J. Barhen and S. Gulati (1989). "Chaotic relaxation" in concurrently asynchronous  neurodynamics. In Proceedings of International Conference on Neural Networks,  volume I, pages 619-626, San Diego, California.  Bertsekas and Tsitsiklis (1989). Parallel and Distributed Computation: Numerical  Methods. Englewood Cliffs, N J: Prentice Hall.  E. K. Blum (1990). Mathematical aspects of outer-product asynchronous content-  addressable memories. Biological Cybernetics, 62:337-348, 1990.  500 Wang, Li, and Blum  E. K. Blum and X. Wang (1992). Stability of fixed-points and periodic orbits, and  bifurcations in analog neural networks. Neural Networks, 5:577-587.  J. Hale (1977). Theory of Functional Differential Equations. New York: Springer-  Verlag.  M. W. Hirsch (1988). Stability and convergence in strongly monotone dynamical  systems. J. reine angew. Math., 383:1-53.  M. W. Hirsch (1989). Convergent activation dynamics in continuous time networks.  Neural Networks, 2:331-349.  J. Hopfield (1982). Neural networks and physical systems with emergent computa-  tional abilities. Proc. Nat. Acad. Sci. USA, 79:2554-2558.  J. Hopfield (1984). Neurons with graded response have collective computational  properties like those of two-state neurons. Proc. Nat. Acad. Sci. USA, 81:3088-  3092.  D. G. Kelly (1990). Stability in contractive nonlinear neural networks. IEEE Trans.  Biomedi. Eng., 37:231-242.  Q. Li (1993). Mathematical and Numerical Analysis of Biological Neural Networks.  Unpublished Ph.D. Thesis, Mathematics Department, University of Southern Cali-  fornia.  C. M. Marcus and R. M. Westervelt (1989). Stability of analog neural networks  with delay. Physical Review A, 39(1):347-359.  K. Matsuoka (1992). Stability conditions for nonlinear continuous neural networks  with asymmetric connection weights. Neural Networks, 5:495-500.  J. M. Ortega and W. C. Rheinboldt (1970). Iterative solution of nonlinear equations  in several variables. New York: Academic Press.  X. Wang, E. K. Blum, and Q. Li (1993). Consistency on Local Dynamics and  Bifurcation of Continuous-Time Dynamical Systems and Their Discretizations. To  appear in the AMS proceedings of Symposia in Applied Mathematics, Mathematics  of Computation 19d3 - 1993, Vancouver, BC, August, 1993, edited by W. Gautschi.  X. Wang and E. K. Blum (1992). Discrete-time versus continuous-time neural  networks. Computer and System Sciences, 49:1-17.  X. Wang and D. S. Parker (1992). Computing least fixed points by asynchronous  iterations and random iterations. Technical Report CSD-920025, Computer Science  Department, UCLA.  J.-H. Wu (1993). Delay-Induced Discrete Waves of Large Amplitudes in Neural Net-  works with Circulant Connection Matrices. Preprint, Department of Mathematics  and Statistics, York University.  
VLSI Phase Locking Architectures for  Feature Linking in Multiple Target  Tracking Systems  Andreas G. Andreou  andreou@j hunix.hcf.jhu.edu  Department of Electrical and  Computer Engineering  The Johns Hopkins University  Baltimore, MD 21218  Thomas G. Edwards  tedwards@src.umd.edu  Department of Electrical Engineering  The University of Maryland  College Park, MD 20722  Abstract  Recent physiological research has shown that synchronization of  oscillatory responses in striate cortex may code for relationships  between visual features of objects. A VLSI circuit has been de-  signed to provide rapid phase-locking synchronization of multiple  oscillators to allow for further exploration of this neural mechanism.  By exploiting the intrinsic random transistor mismatch of devices  operated in subthreshold, large groups of phase-locked oscillators  can be readily partitioned into smaller phase-locked groups. A  multiple target tracker for binary images is described utilizing this  phase-locking architecture. A VLSI chip has been fabricated and  tested to verify the architecture. The chip employs Pulse Ampli-  tude Modulation (PAM) to encode the output at the periphery of  the system.  I Introduction  In striate cortex, visual information coming from the retina (via the lateral genic-  ulate nuclei) is processed to extract retinotopic maps of visual features. Some cells  in cortex are receptive to lines of particular orientation, length, and/or movement  direction (Hubel, 1988). A fundamental problem of visual processing is how to  866  VLSI Phase Locking Architectures for Feature Linking in Multiple Target Tracking Systems 867  associate certain groups of features together to form coherent representations of  objects. Since there is an almost infinite number of possible feature combinations,  it seems unlikely that there are dedicated "grandmother" cells which code for ev-  ery possible feature combination. There probably exists a type of adaptive and  transitory method to "bind" these features together. The Binding Problem (Crick,  1990) is the problem of making neural elements which are receptive to these visual  features temporarily become active as a group that codes for a particular object,  yet maintaining the group's specificity towards that object, even when there are  several different interleaved objects in the visual field.  Temporal correlation of neural response is one solution to the binding problem  (von der Malsburg, 1986). Response from neurons (or neural oscillating circuits)  which are receptive to a particular visual feature are required to have high temporal  correlation with responses to other visual features that correspond to the same  object. This would require that there is stimulus-driven oscillation in visual cortex,  and that there is also a degree of oscillation synchronization between neural circuits  receptive to the same object. Both of these requirements have been found in visual  cortex (Gray, 1987; Gray, 1989). Furthermore, there have been several computer  simulations of the synchronization phenomena and related visual processing tasks  (Baldi, 1990; Eckhorn, 1990).  This paper describes a phase-locking architecture for a circuit which performs a  multiple-target tracking problem. It will accomplish this task by establishing a zero  valued phase difference between oscillators that are receptive to those features to be  "bound" together to form an object. Each object will then be recognized as a group  of synchronous oscillators, and oscillators that correspond to different objects will  be identified due to their lack of synchronization. We assume these oscillators have  low duty-cycle pulsed outputs, and the oscillators which correspond to the same  object will all pulse high at the same time. Target location will be communicated  to the periphery by Pulse Amplitude Modulation (PAM).  2 The Neural Oscillator  The oscillator for the target tracker must have two qualities. It needs to be capable  of producing a fairly smooth phase representation so that it is easy to compare  the difference between oscillator phases to allow for robust phase-locking. It is also  useful to have a pulsed output present so that one group of oscillators can be easily  discerned from another group of oscillators when their outputs are examined over  time. The self-resetting neuron circuit (Mead, 1989) provides both of these outputs  (Figure 1). Current Ii, provided by FET Q1 charges capacitor C1 until positive  feedback though the non-inverting CMOS amplifier and capacitor C2 brings Vphas,  all the way to Vaa. This causes the output voltage to go high, which turns on Q2  thus draining charge from C1 by I,.,s,t through Q3 and lowering Vpha,. When  the Vh,,,, is brought low enough, positive feedback brings both Vpn,, and the  output voltage down to V,,. This turns transistor Q2 off, and the cycle repeats.  The duration of output pulses is inversely proportional to I,.,, - Ii,, and the time  between output pulses is inversely proportional to Ii,. Figure 2 is a plot of the  pulse output voltage and Vph,, vs. time.  868 Andreou and Edwards  Vfreq  Vphase .'  Q1  -- C1 C2  Q2  Q3  .T,.  Vreset  Pulse  Output  Figure 1' Self-Resetting Neural Oscillator  Oscillator Output  -1  /  f  f  1  /  t  1  t  /  /  /  /  /  /  f  I/  t t  t /  / / /  / / /  / / /  / / /  / / t  I I I I I I I I I I  0 1 2 3 4 5 6 7 8 9  (10 -6sec)  Figure 2: Plot of Pulse Output (line) and Phase Voltage (dashed) vs.  Neural Oscillator  Time for  VLSI Phase Locking Architectures for Feature Linking in Multiple Target Tracking Systems 869  3 Phase Locking  To achieve stable and reliable performance, the Comparator Model (Kammen, 1990)  of phase-locking was used. Oscillator phase is adjusted according to  Where O(x,t) is the phase of oscillator x at time t, w(x) is the intrinsic phase  advance of oscillator x, n is the total number of oscillators, and f is a sigmoid  squashing-function.  Each object in the visual field requires one averaging circuit to achieve phase-locking  of its receptive oscillators. But at any time we do not know the number of objects  which will be in the visual field. Therefore, instead of having a pool of monolithic  averaging circuits, it is preferable to distribute the averaging function over all the  oscillator cells in a way which allows partitioning of the visual field into multiple  phase-locked groups of oscillators. The follower-aggregator circuit (Mead, 1989) can  be used to develop the average phase information using current-mode computation.  It consists of transconductance amplifiers connected as voltage-followers with all  outputs tied together to form the average of all input voltages.  The phase averaging circuitry can be distributed among the oscillators by placing  one transconductance amplifier in each oscillator cell, and linking those oscillators  to be phase-locked by a common line. The visual field can be partitioned into  multiple phase-locked groups with separate average phases by using FETs to gate  whether or not the averaging information can pass through an oscillator cell to its  neighbors.  To lock an oscillator in phase with the rest of the oscillators which are attached to  the averaging line, extra current is provided to the oscillator by a transconductance  amplifier to slightly speed up or slow down the oscillator to match its phase to  the average phase of the oscillators in the group. Figure 3 shows the circuit for a  complete phase-locking oscillator cell.  Computer simulations of this phase-locking system were carried out using the Ana-  log circuit simulator. Figure 4 shows the result of a simulation of two oscillators.  Vgate is the voltage controlling the NFET of the transmission gate which links the  phase averaging lines of the two oscillators together (the PFETs are controlled  complementary). As soon as the Vgat, is brought high, the oscillators rapidly phase  lock.  4 Target Location  We will assume that the input to a visual tracking chip is a binary image projected  onto the die. Phototransistors detect the brightness of each pixel, and if it is above  a threshold level, the pixel control circuitry will turn the pixel's oscillator on. If a  pixel oscillator is turned on, gating circuitry will allow the propagation of the phase  averaging line through the pixel's oscillator cell to its nearest-neighbors. Illuminated  870 Andreou and Edwards  To top neighbor  vgt   To left  neighbor  VgateP  x  VgateP  VgateN  To right  neighbor  Vol  VO2  To bottom neighbor  Figure 3: Phase-Locking Oscillator Cell  Vr  Pulse  Output  VLSI Phase Locking Architectures for Feature Linking in Multiple Target Tracking Systems 871  -5  10 sec  Figure 4: Phase-Locking Simulation  5V  nearest-neighbor connected pixels will thus have their oscillators turned on and will  become phase-locked.  The follower-aggregator circuit can be modified to determine linear position (Maher,  1989) by using voltage taps off of a resistive line as inputs to the transconductance  amplifiers, and biasing the amplifiers by currents that correspond to the pulsing  outputs of the oscillators (see Figure 5).  During the time that a group of oscillators are spiking, the output of the tracking  circuitry will yield a location corresponding to the average position of the distribu-  tion of those oscillators. There can be many different nearest-neighbor connected  objects projected onto the die, and the position of the center of each object is com-  municated to the periphery via PAM. Thus, we can use multiplexing in time to  simplify connectivity of communication with the periphery of the chip.  5 Test Chip  A chip to test the Comparator Model phase-locking method and multiple-target  tracking system was fabricated by the MOSIS service in 2.0 pm feature size CMOS.  To keep this test chip simple, the oscillators were arranged in a one-dimensional  chain, and voltage inputs to the chip were used to control whether or not a pixel  was considered "illuminated." A polysilicon resistive line was used to provide linear  position information to the tracking system. All transistors used were minimum  size (6 pm wide and 4 pm long).  The test chip was able to rapidly and robustly phase-lock groups of nearest-neighbor  872 Andreou and Edwards  vpu.e(n-O  '  Postional Voltage Line  vp,ase(n) .   Vpulse(a+l)   Average Position Line  Figure 5: Circuit to determine object location  connected oscillators. This phase-locking could occur with oscillator frequencies set  from 10 Hz to 4 KHz. A phase-locked group of oscillators would almost instantly  split into two separate phase-locked groups with little temporal correlation between  them when a connected chain of on oscillators was severed by turning off an oscillator  in the middle of the original group. Mismatch in the transconductances of the  oscillator transistors provided easy desynchronization.  Position tracking was measured by examining the resistive-line aggregator output  during the time a certain phase-locked group of oscillators was pulsing. When  multiple phase-locked groups of oscillators were active, it was still quite easy to  make out the positional PAM voltage associated with each group by triggering an  oscilloscope off of the pulsing output of an oscillator in that group. While there are  occasional instances of two or more groups pulsing at the same time, if the duty  cycle of the spiking oscillator is kept relatively small, there is little interference on  average.  6 Discussion  It is becoming obvious that oscillation and synchronization phenomena in cortex  may play an important role in neural information processing. In addition to striate  cortex, the olfactory bulb also has oscillatory neural circuits which may be impor-  tant in neural information processing (Freeman, 1988). It has been suggested that  temporal correlation may be used for pattern segmentation in associative memories  (Wang, 1990), and correlations between multiple oscillators may be used for storing  time intervals (Miall, 1989).  We have described a circuit which performs Comparator Model phase-locking. The  distributed and partitionable qualities of this circuit make it attractive as a possible  physiological model. The PAM representation of object position shows one way that  connectivity requirements can be minimized for communication in a neuromorphic  VLSI Phase Locking Architectures for Feature Linking in Multiple Target Tracking Systems 873  system. The chip has been fabricated using subthreshold CMOS technology, and  thus uses little power.  Acknowledgements  The authors are pleased to acknowledge helpful discussion with C. Koch and J.  Lazzaro. Chip fabrication was provided by the MOSIS service.  References P. Baldi & R. Meir. (1990) Computing with arrays of coupled oscillators: an appli-  cation to preattentive texture discrimination. Neural Computation 2,458-471.  F. Crick &: C. Koch. (1990) Towards a neurobiological theory of consciousness.  Seminars in the Neurosciences 2,263-275.  R. Eckhorn, H. J. Reitboek, M. Arndt &: P. Dicke. (1990) Feature linking via  synchronization among distributed assemblies: simulations of results from cat visual  cortex. Neural Computation 2,293-307.  W. J. Freeman, Y. Yao, &: B. Burke. (1988) Central pattern generating and recog-  nizing in olfactory bulb: a correlation learning rule. Neural Networks 1, 277-288.  Gray &: W. Singer. (1987) Stimulus-specific neuronal oscillations in the cat  cortex: A cortical functional unit. Soc. Neurosci. Abstr. 13(404.3)  Gray, P. KSnig, A. K. Engel &: W. Singer. (1989) Oscillatory responses in cat  cortex exhibit inter-columnar synchronization which reflects global stimulus  C. M.  visual  C.M.  visual  properties. Nature (London) 338, 334-337.  D. H. Hubel. (1988) Eye, Brain, and Vision.  Library.  D. M. Kammen, C. Koch &: P. J. Holmes.  New York, NY: Scientific American  (1990) Collective oscillations in the  visual cortex. In D. S. Touretzky (ed.) Advances in Neural Information Processing  Systems 2. San Mateo, CA: Morgan Kaufman Publishers.  C. A. Mead. (1989)Analog VLSI and Neural Systems. Reading, MA: Addison-  Wesley.  M. A. Maher, S. P. Deweerth, M. A. Mahowald & C. A. Mead. (1989) Implementing  neural architectures using analog VLSI circuits. IEEE Trans. Cite. Sys. 36,643-  652.  C. Miall. (1989) The storage of time intervals using oscillating neurons. Neural  Computation 1,359-371.  C. von der Malsburg. (1986) A neural cocktail-party processor. Biological Cyber-  netics. 54,29-40.  D. Wang, J. Buhmann &: C. von der Malsburg. (1990) Pattern segmentation in  associative memory. Neural Computation 2, 95-106.  
Discontinuous Generalization in Large  Committee Machines  H. Schwarze  Dept. of Theoretical Physics  Lund University  SSlvegatan 14A  223 62 Lund  Sweden  J. Hertz  Nordita  Blegdamsvej 17  2100 Copenhagen  Denmark  Abstract  The problem of learning from examples in multilayer networks is  studied within the framework of statistical mechanics. Using the  replica formalism we calculate the average generalization error of a  fully connected committee machine in the limit of a large number  of hidden units. If the number of training examples is proportional  to the number of inputs in the network, the generalization error  as a function of the training set size approaches a finite value. If  the number of training examples is proportional to the number of  weights in the network we find first-order phase transitions with a  discontinuous drop in the generalization error for both binary and  continuous weights.  1 INTRODUCTION  Feedforward neural networks are widely used as nonlinear, parametric models for the  solution of classification tasks and function approximation. Trained from examples  of a given task, they are able to generalize, i.e. to compute the correct output for  new, unknown inputs. Since the seminal work of Gardner (Gardner, 1988) much  effort has been made to study the properties of feedforward networks within the  framework of statistical mechanics; for reviews see e.g. (Hertz et al., 1989; Warkin et  al., 1993). Most of this work has concentrated on the simplest feedforward network,  the simple perceptton with only one layer of weights connecting the inputs with a  399  400 Schwarze and Hertz  single output. However, most applications have to utilize architectures with hidden  layers, for which only a few general theoretical results are known, e.g. (Levin et al.,  1989; Krogh and Hertz, 1992; Seung et al., 1992).  As an example of a two-layer network we study the committee mackine (Nilsson,  1965). This architecture has only one layer of adjustable weights, while the hidden-  to-output weights are fixed to +1 so as to implement a majority decision of the  hidden units. For binary weights this may already be regarded as the most general  two-layer architecture, because any other combination of hidden-output weights can  be gauged to +1 by flipping the signs of the corresponding input-hidden weights.  Previous work has been concerned with some restricted versions of this model, such  as learning geometrical tasks in machines with local input-to-hidden connectivity  (Sompolinsky and Tishby, 1990) and learning in committee machines with nonover-  lapping receptive fields (Schwarze and Hertz, 1992; Mato and Parga, 1992). In  this tree-like architecture there are no correlations between hidden units and its  behavior was found to be qualitatively similar to the simple perceptron.  Recently, learning in fully connected committee machines has been studied within  the annealed approximation (Schwarze and Hertz, 1993a,b; Kant et al, 1993), re-  vealing properties which are qualitatively different from the tree model. However,  the annealed approximation (AA) is only valid at high temperatures, and a correct  description of learning at low temperatures requires the solution of the quenched  theory. The purpose of this paper is to extend previous work towards a better  understanding of the learning properties of multilayer networks. We present results  for the average generalization error of a fully connected committee machine within  the replica formalism and compare them to results obtained within the AA. In par-  ticular we consider a large-net limit in which both the number of inputs N and  the number of hidden units K go to infinity but with K << N. The target rule is  defined by another fully connected committee machine and is therefore realizable  by the learning network.  2 THE MODEL  We consider a network with N inputs, K hidden units and a single output unit  Each hidden unit (rl, 1 G {1,..., K}, is connected to the inputs _S - (S,..., Sjv)  through the weight vector W l and performs the mapping  The hidden units may be regarded as outputs of simple perceptrons and will be  referred to as students. The factor N -/2 in (1) is included for convenience; it  ensures that in the limit N -- oo and for lid inputs the argument of the sign  function is of order 1. The overall network output is defined as the majority vote  of the student committee, given by  ,/_g . (2)  Discontinuous Generalization in Large Committee Machines 401  This network is trained from P = aKN input-output examples (", r(")),  {1,..., P}, of the desired mapping r, where the components ' of the training inputs  are independently drawn from a distribution with zero mean and unit variance. We  study a realizable task defined by another committee machine with weight vectors  V_ (the teachers), hidden units n and an overall output r(_S) of the form (2). We  will discuss both the binary version of this model with W..t, V__  {1}  and the  continuous version in which the t's and 's are normalized to .  The goal of learning is to find a nelwork that performs well on unknown examples,  which are not included in the training set. The network quality can be menured  by the generalization error  ((W,)) = (O[-((W,),)()]), (3)  the probability that a randomly chosen input is misclassified.  Following the statistical mechanics approach we consider a stochastic learning al-  gorithm that for long training times yields a Gibbs distribution of networks with  the corresponding partition function  Z: f dpo((Wt})e-'((,}), (4)  where  is the training error,  = 1/T is a formal temperature parameter, and  includes a priori constraints on the weights. The average generalization and train-  ing errors at thermal equilibrium, averaged over all representations of the training  examples, are given by  : << < f({__w,}) >T >>  = (( < E,({__W,}) >T  (6)  where <<...)) denotes a quenched average over the training examples and (...), a  thermal average. These quantities may be obtained from the average free energy  F = -T{{ln Z)), which can be calculated within the standard replica formalism  (Gardner, 1988; GySrgyi and Tishby, 1990).  Following this approach, we introduce order parameters and make symmetry as-  sumptions for their values at the saddle point of the free energy; for details of the  calculation see (Schwarze, 1993). We assume replica symmetry (RS) and a par-  tial committee symmetry allowing for a specialization of the hidden units on their  respective teachers. Furthermore, a self-consistent solution of the saddle-point  equations requires scaling assumptions for the order parameters. Hence, we are left  with the ansatz  1  1  = ---P +  K  d  =  + q  _  _ +(1- ,  402 Schwarze and Hertz  where p, A, d, q and care of order 1. For A = q = 0 this solution is symmetric  under permutations of hidden units in the student network, while nonvanishing A  and q indicate a specialization of hidden units that breaks this symmetry. The  values of the order parameters at the saddle point of the replica free energy finally  allow the calculation of the average generalization and training errors.  3 THEORETICAL RESULTS  In the limit of small training set sizes, a  O(1/K), we find a committee-symmetric  solution where each student weight vector has the same overlap to all the teacher  vectors, corresponding to A = q = 0. For both binary and continuous weights  the generalization error of this solution approaches a nonvanishing residual value as  shown in figure 1. Note that the asymptotic generalization ability of the committee-  symmetric solution improves with increasing noise level.  0.50  0.40  0.30  0.20  0.30  0.25  0.20  0.15  0.10  0.05  0.10  0.00 0.00 ............  a) 0 0 20 30 40 50 b) 0.0 0.5 .0 .5 2.0  a: P/N T  Figure 1: a) Generalization (upper curve) and training (lower curve) error as func-  tions of & = Ka. The results of Monte Carlo simulations for the generalization  (open symbols) and training (closed symbols) errors are shown for K = 5 (circles)  and K = 15 (triangles) with T = 0.5 and N = 99. The vertical lines indicate the  predictions of the large-K theory for the location of the phase transition &c = Kac  in the binary model for K = 5 and K = 15, respectively.  b) Temperature dependence of the asymptotic generalization and training errors for  the committee-symmetric solution.  Only if the number of training examples is sufficiently large, a --. O(1), can the  committee symmetry be broken in favor of a specialization of hidden units. We find  first-order phase transitions to solutions with A, q > 0 in both the continuous and  the binary model. While in the binary model the transition is accompanied by a  perfect alignment of the hidden-unit weight vectors with their respective teachers  (A = 1), this is not possible in a continuous model. Instead, we find a close approach  of each student vector to one of the teachers in the continuous model: At a critical  value as(T) of the load parameter a second minimum of the free energy appears,  corresponding to the specialized solution with A, q > 0. This solution becomes the  Discontinuous Generalization in Large Committee Machines 403  global minimum at at(T) > a,(T), and its generalization error decays algebraically.  In both models the symmetric, poorly generalizing state remains metastable for  arbitrarily large a. For increasing system sizes it will take exponentially long times  for a stochastic training algorithm to escape from this local minimum (see figure  la). Figure 2 shows the qualitative behavior of the generalization error for the  continuous model, and the phase diagrams in figure 3 show the location of the  transitions for both models.  o(T)  It  II  a = 'r  Figure 2: Schematic behavior of the generalization error in the large-K committee  machine with continuous weights.  In the binary model a region of negative thermodynamic entropy (below the dashed  line in figure 3a) suggests that replica symmetry has to be broken to correctly  describe the metastable, symmetric solution at large a.  A comparison of the RS solution with the results previously obtained within the  AA (Schwarze and Hertz, 1993a,b) shows that the AA gives a qualitatively correct  description of the main features of the learning curve. However, it fails to predict the  temperature dependence of the residual generalization error (figure lb) and gives an  incorrect description of the approach to this value. Furthermore, the quantitative  predictions for the locations of the phase transitions differ considerably (figure 3).  4 SIMULATIONS  We have performed Monte Carlo simulations to check our analytical findings for the  binary model (see figure la). The influence of the metastable, poorly generalizing  state is reflected by the fact that at low temperatures the simulations do not follow  the predicted phase transition but get trapped in the metastable state. Only at  higher temperatures do the simulations follow the first order transition (Schwarze,  1993). Furthermore, the deviation of the training error from the theoretical result  indicates the existence of replica symmetry breaking for finite a. However, the gen-  eralization error of the symmetric state is in good quantitative agreement with the  404 Schwarze and Hertz  0.8  0.6  0.4  0.2  0.0  1.0  0.8  0.6  0.4  0.2  0.0  a) 5 10 15 20 25 30 b) 1.0 1.5 2.0 2.5 5.0 5.5 4.0  a = P/KN a = P/KN  Figure 3: Phase diagrams of the large-K committee machine.  a) continuous weights: The two left lines show the RS results for the spinodal  line (--), where the specialized solution appears, and the location of the phase  transition (--). These results are compared to the predictions of the AA for the  spinodal line (-.-) and the phase transition (...).  b) binary weights: The RS result for the location of the phase transition (J) and  its zero-entropy line (--) are compared to the prediction of the AA for the phase  transition (-..) and its zero-entropy line (-.-).  theoretical results.  In order to investigate whether our analytical results for a Gibbs ensemble of com-  mittee machines carries over to other learning scenarios we have studied a variation  of this model allowing the use of backpropagation. We have considered a 'soft-  committee' whose output is given by  a({Wt}, S) = tanh (-] tanh ( ..Wt  S)). (8)  1--1  The first-layer weights Wt of this network were trained on examples (', r(')),  p 6 {1,..., P}, defined by another soft-committee with weight vectors __V t using  on-line backpropagation with the error function  4_s) = s)- r(_s)] (9)  In general this procedure is not guaranteed to yield a Gibbs distribution of weights  (Hansen et al., 1993) and therefore the above analysis does not apply to this case.  However, the generalization error for a network with N = 45 inputs and K =  3 hidden units, averaged over 50 independent runs, shows the same qualitative  behavior as predicted for the Gibbs ensemble of committee machines (see figure 4).  After an initial approach to a nonvanishing value, the average generalization error  decreases rather smoothly to zero. This smooth decrease of the average error is  due to the fact that some runs got trapped in a poorly-generalizing, committee-  symmetric solution while others found a specialized solution with a close approach  to the teacher.  Discontinuous Generalization in Large Committee Machines 405  , t  0.18  0.16  0.14  0.12  0.1  0.08  0.06  0.04  0.02  0  0  generalization error  tmlnlng error  200 400 600 800 1000 1200  P  Figure 4: Generalization error and training error of the 'soft-committee' with N =  45 and K - 3. We have used standard on-line backpropagation for the first-layer  weights with a learning rate r/= 0.01 for 1000 epochs. the results are averaged over  50 runs with different teacher networks and different training sets.  5 CONCLUSION  We have presented the results of a calculation of the generalization error of a multi-  layer network within the statistical mechanics approach. We have found nontrivial  behavior for networks with both continuous and binary weights. In both mod-  els, phase transitions from a symmetric, poorly-generalizing solution to one with  specialized hidden units occur, accompanied by a discontinuous drop of the gener-  alization error. However, the existence of a metastable, poorly generalizing solution  beyond the phase transition implies the possibility of getting trapped in a local  minimum during the training process. Although these results were obtained for a  Gibbs distribution of networks, numerical experiments indicate that some of the  general results carry over to other learning scenarios.  Acknowledgements  The authors would like to thank M. Biehl and S. Solla for fruitful discussions. HS  acknowledges support from the EC under the SCIENCE programme (under grant  number B/SC1'/915125) and by the Danish Natural Science Council and the Danish  Technical Research Council through CONNECT.  References  E. Gardner (1988), J. Phys. A 21,257.  G. GySrgyi and N. Tishby (1990), in Neural Networks and Spin Glasses, edited by  K. Thuemann and R. KSberle, (World scientific, Singapore).  L.K. Hansen, R. Pathria, and P. Salamon (1993), J. Phys. A 26, 63.  J. Hertz, A. Krogh, and R.G. Palmer (1989), Introduction to the Theory of Neural  406 Schwarze and Hertz  Computation, (Addison-Wesley, Redwood City, CA).  K. Kang, J.-H. Oh, C. Kwon, and Y. Park (1993), preprint Pohang Institute of  Science and Technology, Korea.  A. Krogh and J. Hertz (1992), in Advances in Neural Information Processing Sys-  tems IV, eds. J.E. Moody, S.J. Hanson, and R.P. Lippmann, (Morgan Kaufmann,  San Mateo).  E. Levin, N. Tishby, and S.A. Solla (1989), in Proc. 2rid Workshop on C'omputa-  riohal Learning Theory, (Morgan Kaufmann, San Mateo).  G. Mato and N. Parga (1992), J. Phys. A 25, 5047.  N.J. Nilsson (1965), Learning Machines, (mcCraw-Hin, New York).  H. Schwarze (1993), J. Phys. A 21t, 5781.  H. Schwarze and J. Hertz (1992), Europhys. Left. 20, :i75.  H. Schwarze and J. Hertz (1993a), J. Phys. A 211, 4919.  H. Schwarze and J. Hertz (1993b), in Advances in Neura/Information Processing  Systems V, (Morgan Kaufmann, San Mateo).  H.S. Seung, H. Sompolinsky, and N. Tishby (1992), Phys. Rev. A 45, 6056.  H. Sompolinsky and N. Tishby (1990), Europhys. Left. 13, 567.  T. Watkin, A. Rau, and M. Biehl (199:i), Rev. Mod. Phys. 115,499.  
Efficient Computation of Complex  Distance Metrics Using Hierarchical  Filtering  Patrice Y. Simard  AT&T Bell Laboratories  Holmdel, NJ 07733  Abstract  By their very nature, memory based algorithms such as KNN or  Parzen windows require a computationally expensive search of a  large database of prototypes. In this paper we optimize the search-  ing process for tangent distance (Simard, LeCun and Denker, 1993)  to improve speed performance. The closest prototypes are found  by recursively searching included subsets of the database using dis-  tances of increasing complexity. This is done by using a hierarchy  of tangent distances (increasing the number of tangent vectors from  0 to its maximum) and multiresolution (using wavelets). At each  stage, a confidence level of the classification is computed. If the  confidence is high enough, the computation of more complex dis-  tances is avoided. The resulting algorithm applied to character  recognition is close to three orders of magnitude faster than com-  puting the full tangent distance on every prototypes.  I INTRODUCTION  Memory based algorithms such as KNN or Parzen xvindoxvs have been extensively  used in pattern recognition. (See (Dasarathy, 1991) for a survey.) Unfortunately,  these algorithms often rely on simple distances (such as Euclidean distance, Ham-  ming distance, etc.). As a result, they suffer from high sensitivity to simple trans-  formations of the input patterns that should leave the classification unchanged (e.g.  translation or scaling for 2D images). To make the problem worse, these algorithms  168  Efficient Computation of Complex Distance Metrics Using Hierarchical Filtering 169  are further limited by extensive computational requirements due to the large number  of distance computations. (If no optimization technique is used, the computational  cost is given in equation 1.)  computational cost  number of distance  x (1)  prototypes complexity  Recently, the problem of transformation sensitivity has been addressed by the intro-  duction of a locally transformation-invariant metric, the tangent distance (Simard,  LeCun and Denker, 1993). The basic idea is that instead of measuring the distance  d(A, B) between two patterns A and B, their respective sets of transformations TA  and TB are approximated to the first order, and the distance between these two  approximated sets is computed. Unfortunately, the tangent distance becomes com-  putationally more expensive as more transformations are taken into consideration,  which results in even stronger speed requirements.  The good news is that memory based algorithms are well suited for optimization  using hierarchies of prototypes, and that this is even more true when the distance  complexity is high. In this paper, we applied these ideas to tangent distance in two  ways: 1) Finding the closest prototype can be done by recursively searching included  subsets of the database using distances of increasing complexity. This is done by  using a hierarchy of tangent distances (increasing the number of tangent vectors  from 0 to its maximum) and nmltiresolution (using wavelets). 2) A confidence level  can be computed for each distance. If the confideuce in the classification is above a  threshold early on, there is no need to compute the more expensive distances. The  two methods are described in the next section. Their application on a real world  problem will be shown in the result section.  2 FILTERING USING A HIERARCHY OF DISTANCES  Our goal is to compute the distance fi'om one unknown pattern to every prototype  in a large database in order to determine which one is the closest. It is fairly obvious  that some patterns are so different from each other that a very crude approximation  of our distance can tell us so. There is a wide range of variation in computation time  (and performance) depending on the choice of the distance. For instance, computing  the Euclidean distance on -pixel images is a factor n/k of the computation of  computing it on k-pixels images.  Similarly, at a given resolution, computing the tangent distance with m tangent  vectors is (m + 1)"- times as expensive as computing the Euclidean distance (m = 0  tangent vectors).  This observations provided us with a hierarchy of about a dozen different distances  ranging in computation time from 4 multiply/adds (Euclidean distance on a 2 x 2  averaged image) to 20,000 lnultiply/adds (tangent distance, 7 tangent vectors, 16 x  16 pixel images). The resulting filtering algorithm is very straightforward and is  exemplified in Figure 1.  The general idea is to store the database of prototypes several times at different  resolutions and with different tangent vectors. Each of these resolutions and groups  of tangent vectors defines a distance di. These distances are ordered in increasing  170 Simard  Prototypes  10,00  Euc. Dist I  2x2  Cost: 4  Confidence  3,500  Unknown Pattern  Euc. Dist  4x4  Cost: 16  Confidence  I [Tang. Dist  ..._.14 vectors  I_ '  Cost:2000C  Confidence  Category  Figure 1: Pattern recognition using a hierarchy of distance. The filter proceed  from left (starting with the whole database) to right (where only a few prototypes  remain). At each stage distances between prototypes and the unknown pattern are  computed, sorted and the best candidate prototypes are selected for the next stage.  As the complexity of the distance increases, the number of prototypes decreases,  making computation feasible. At each stage a classification is attempted and a  confidence score is computed. If the confidence score is high enough, the remaining  stages are skipped.  accuracy and complexity. The first distance dl is computed on all (K0) prototypes  of the database. The closest K patterns are then selected and identified to the  next stage. This process is repeated for each of the distances; i.e. at each stage i,  the distance di is computed on each I,;i-1 patterns selected by the previous stage.  Of course, the idea is that. as the complexity of the distance increases, the number  of patterns on which this distance must be computed decreases. At the last stage,  the most complex and accurate distance is computed on all remaining patterns to  determine the classification.  The only difficult part is to det. ermine the ninimum /i patterns selected at each  stage for which the filtering does not decrease the overall performance. Note that  if the last distance used is the most accurate distance, setting all ICi to the number  of patterns in the database will give optimal performance (at the most expensive  cost). Increasing Ki always improves the performance in the sense that it allows to  find patterns that are closer for the next distance measure di+. The simplest way  to determine Ki is by selecting a validation set and plotting the performance on this  validation set as a function of Ki. The optimal Ix'i is then determined graphically.  An automatic way of computing each I,;,: is currently being developed.  This method is very useful when the performance is not degraded by choosing small  ICi. In this case, the distance evaluation is done using distance metrics which are  relatively inexpensive to compute. The computation cost becomes:  Efficient Computation of Complex Distance Metrics Using Hierarchical Filtering 171  number of distance  computational cost  y, prototypes x complexity (2)  i at stage i at stage i  Curves showing the performance as a fimction of the value of Ni will be shown in  the result section.  3  PRUNING THE SEARCH USING CONFIDENCE  SCORES  If a confidence score is computed at each stage of the distance evaluation, it is  possible for certain patterns to avoid completely computing the most expensive  distances. In the extreme case, if the Euclidean distance between two patterns is 0,  there is really no need to compute the tangent distance. A simple (aud crude) way  to compute a confidence score at a given stage i, is to find the closest prototype  (for distance di) in each of the possible classes. The distance difference between the  closest class and the next closest class gives an approximation of a confidence of  this classification. A simple algorithm is then to compare at stage i the confidence  score cip of the current unknown pattern p to a threshold Oi, and to stop the  classification process for this pattern as soon as cip> Oi. The classification will  then be determined by the closest prototype at. this stage. The computation time  will therefore be different depending on the pattern to be classified. Easy patterns  will be recognized very quickly while difficult patterns will need to be compared to  some of the prototypes using the most complex distance. The total computation  cost is therefore:  computational cost  nulnber of distance probability  prototypes x complexity x t,o reach (3)  at stage i at stage i stage i  Note that if all Oi are high, the performance is maximized but so is the cost. We  therefore wish to find the smallest value of Oi which does not degrade the perfor-  mance (increasing Oi always improves the performance). As in the previous section,  the simplest way to determine the optimal Oi is graphically with a validation set..  Example of curves representing the performance as a function of Oi will be given in  the result section.  4 CHOSING A GOOD HIERARCHY OPTIMIZATION  4.1 k-d tree  Several hierarchies of distance are possible for optimizing the search process. An  incremental nearest neighbor search algorithm based on k-d tree (Broder, 1990)  was implemented. The k-d tree structure was interesting because it can potentially  be used with tangent distance. Indeed, since the separating hyperplanes have n-1  dimension, they can be made parallel to many tangent vectors at the same time.  As much as 36 images of 256 pixels with each 7 t. angent vectors can be separated  into two group of 18 images by one hypcrplane which is parallel to all tangent  172 Simard  vectors. The searching algorithm is taking some knowledge of the transformation  invariance into account when it computes on which side of each hyperplane the  unknown pattern is. Of course, when a leaf is reached, the full tangent distance  must be computed.  The problem with the k-d tree algorithm however is that in high dimensional space,  the distance froin a point to a hyperplane is almost always smaller than the distance  between any pair of points. As a result, the unknown pattern must be coinpared to  many prototypes to have a reasonable accuracy. The speed up factor was compa-  rable to our multiresolution approach in the case of Euclidean distance (about 10),  but we have not been able to obtain both good performance and high speedup with  the k-d tree algorithm applied to tangent distance. This algorithm was not used in  our final experiments.  4.2 Wavelets  One of the main advantages of the multiresolution approach is that it is easily  implemented with wavelet transforms (Mallat, 1989), and that in the wavelet space,  the tangent distance is conserved (with orthonormal wavelet bases). Furthermore,  the multiresolution decomposition is completely orthogonal to the tangent distance  decomposition. In our experiments, the ttaar transform was used.  4.3 Hierarchy of tangent distance  Many increasingly accurate approximations can be made for the tangent distance  at a given resolution. For instance, the tangent distance can be computed by an  iterative process of alternative projections onto the tangent hyperplanes. A hierar-  chy of distances results, derived from the number of projections performed. This  hierarchy is not very good because the initial projection is already fairly expensive.  It is more desirable to have a better efficiency in the first stages since only few  patterns will be left for the latter stages.  Our most successful hierarchy consisted in adding tangent vectors one by one, on  both sides. Even though this implies solving a new linear system at each stage,  the computational cost is mainly dominated by computing dot products between  tangent vectors. These dot-products are then reused in the subsequent stages to  create larger linear systems (involving more tangent vectors). This hierarchy has the  advantage that the first stage is only twice as expensive, yet much more accurate,  than the Euclidean distance. Each subsequent stage brings a lot of accuracy at. a  reasonable cost. (The cost increases quicker toward the later stages since solving the  linear system grows with the cube of the number of tangent vector.) In addition,  the last stage is exactly the full tangent distance. As xvc will see in section 5 the  cost in the final stages is negligible.  Obviously, the tangent vectors can be added in different order. We did not try to  find the optimal order. For character recognition application adding translations  first, followed by hyperbolic deformations, the scaliugs, the thickness deformations  and the rotations yielded good performance.  Efficient Computation of Complex Distance Metrics Using Hierarchical Filtering 173  i # of T.V. Reso # of proto (Ki) # of prod Probab # of mul/add  0 0 4 9709 1 1.00 40,000  1 0 16 3500 1 1.00 56,000  2 0 64 500 1 1.00 32,000  3 1 64 125 2 0.90 14,000  4 2 256 50 5 0.60 40,000  5 4 256 45 7 0.40 32,000  6 6 256 25 9 0.20 11,000  7 8 256 15 11 0.10 4,000  8 10 256 10 13 0.10 3,000  9 12 256 5 15 0.05 1,000  10 14 256 5 17 0.05 1,000  Table 1: Summary computation for the classification of 1 pattern: The first column  is the distance index, the second column indicates the number of tangent vector  (0 for the Euclidean distance), and the third colunto indicates the resolution in  pixels, the fourth is /i or the number of prototypes on which the distance di must  be computed, the fifth colunto indicates the number of additional dot products  which must be computed to evaluate distance di, the sixth colunto indicates the  probability to not skip that stage after the confidence score has been used, and  the last column indicates the total average number of multiply-adds which must be  performed (product of column 3 to 6) at each stage.  4.4 Selecting the k closests out of N prototypes in O(N)  In the multiresolution filter, at the early stages we must select the k closest proto-  types from a large number of prototypes. This is problematic because the prototypes  cannot be sorted since O(NlogN) is expensive compared to computing N distances  at very low resolution (like 4 pixels). A simple solution consists in using a variation  of "quicksort" or "finding the k-th element" (Aho, Hopcroft and Ulh-nan, 1983),  which can select the k closests out of N prototypes in O(N). The generic idea is  to compute the mean of the distances (an approximation is actually sufficient) and  then to split the distances  into two halves (of different sizes) according to xvhether they are smaller or larger  than the mean distance. If they are more distances smaller than the mean than k,  the process is reiterated on the upper half, otherwise it is reiterated on the lower  half. The process is recursively executed until there is only one distance in each  half. (k is then reached and all the k prototypes in the lower halves are closer to  the unknown pattern than all the N -k prototypes in the upper halves.) Note that  the elements are not sorted and that only the expected time is O(N), but this is  sufficient for our problem.  5 RESULTS  A simple task of pattern classification was used to test the filtering. The prototype  set and the test set consisted respectively of 9709 and 2007 labeled images (16  by 16 pixels) of handwritten digits. The prototypes were also averaged to lower  174 Simard  Error in %  j Resolution 4 pixels   t / Resolution 16 pixels  0 1 2 3 4 5 6 7 8 9 10  K (in 1000)  Error in % 7  2  j Resolution 16 pixels  /// Resolution 64 pxels  10 20 30 40 50 60 ?0 80 90  % of pat. kept.  Figure 2: Left: Raw error performance as a fimction of /'1 and K2. The final  chosen values were K1 = 3500 and K2 = 500. Riglit,: Raw error as a finction of  the percentage of pattern which have not exceeded the confidence threshold 0i. A  100% means all the pattern were passed to the next stage.  resolutions (2 by 2, 4 by 4 and 8 by 8) and copied to separate databases. The 1  by 1 resolution was not useful for anything. Therefore the fastest distance was the  Euclidean distance on 2 by 2 images, while the slowest distance was the full tangent  distance with 7 tangent vectors for both the prototype and the unknown pattern  (Simard, LeCun and Denker, 1993). Table 1 sumnm.rizes the reslilts.  Several observations can be made. First, simple distance metrics are very useful to  eliminate large proportions of prototypes at no cost in performances. Indeed the  Euclidean distance computed on 2 by 2 images can remove 2 third of the prototypes.  Figure 2, left, shows the performance as a function of Ki and K2 (2.5 % raw error  was considered optimal performance). It can be noticed that for Ki above a certain  value, the performance is optimal and constant. The most complex distances (6 and  7 tangent vectors on each side) need only be computed for 5% of the prototypes.  The second observation is that tile use of a confidence score can greatly reduce the  number of distance evaluations in later stages. For instance the dominant phases of  the computation would be with 2, 4 and 6 tangent vectors at resolution 256 if there  were not reduced to 60%, 40% and 20% respectively using the confidence scores.  Figure 2, riglit, shows the raw error performance as a function of the percentage  of rejection (confidence lower than 0i) at stage i. It can be noticed that above a  certain threshold, the performance are optimal and constant. Less than 10% of the  unknown patterns need the most complex distances (5, 6 and 7 tangent vectors on  each side), to be computed.  Efficient Computation of Complex Distance Metrics Using Hierarchical Filtering 175  6 DISCUSSION  Even though our method is by no way optimal (the order of the tangent vector  can be changed, intermediate resolution can be used, etc...), the overall speed up  we achieved was about 3 orders of magnitude (compared with computing the full  tangent distance on all the patterns). There was no significant decrease in perfor-  mances. This classification speed is comparable with neural network method, but  the performance are better with tangent distance (2.5% versus 3%). Furthermore  the above methods require no learning period which makes them very attractive for  application were the distribution of the patterns to be classified is changing rapidly.  The hierarchical filtering can also be combined with learning the prototypes using  algorithms such as learning vector quantization (LVQ).  References  Aho, A. V., Hopcroft, J. E., and Ulhnan, J. D. (1983). Data Structure and Algo-  rithms. Addison-Wesley.  Broder, A. J. (1990). Strategies for Efficient Incremental Nearest Neighbor Search.  Pattern Recognition, 23:171-178.  Dasarathy, B. V. (1991). Neares! Nei9bbor (NN) Norms: NN Pattern classification  Techniques. IEEE Computer Society Press, Los Alamitos, California.  Mallat, S. G. (1989). A Theory for Multiresolution Signal Decomposition: The  Wavelet Representation. IEEE Transactions on Paiiern Analysis and Machine  Intelligence, 11, No. 7:674-693.  Simard, P. Y., LeCun, Y., and Denker, J. (1993). Efficient Pattern Recognition  Using a New Transformation Distance. In Neural Information Processin9  terns, volume 4, pages 50-58, San Mateo, CA.  
I I  Analysis of Short Term Memories for Neural  Networks  Jose C. Principe, Hui-H. Hsu and Jyh-Ming Kuo  Computational NeuroEngineering Laboratory  Department of Electrical Engineering  University of Florida, CSE 447  Gainesville, FL 32611  principe @ synapse.ee.ufl .edu  Abstract  Short term memory is indispensable for the processing of time  varying information with artificial neural networks. In this paper a  model for linear memories is presented, and ways to include  memories in connectionist topologies are discussed. A comparison  is drawn among different memory types, with indication of what is  the salient characteristic of each memory model.  INTRODUCTION  An adaptive system that has to interact with the external world is faced with the  problem of coping with the time varying nature of real world signals. Time varying  signals, natural or man made, carry information in their time structure. The problem  is then one of devising methods and topologies (in the case of interest here, neural  topologies) that explore information along time.This problem can be appropriately  called temporal pattern recognition, as opposed to the more traditional case of static  pattern recognition. In static pattern recognition an input is represented by a point in  a space with dimensionality given by the number of signal features, while in temporal  pattern recognition the inputs are sequence of features. These sequence of features  can also be thought as a point but in a vector space of increasing dimensionality.  Fortunately the recent history of the input signal is the one that bears more  information to the decision making, so the effective dimensionality is finite but very  large and unspecified a priori. How to find the appropriate window of input data  1011  1012 Principe, Hsu, and Kuo  (memory depth) for a given application is a difficult problem. Likewise, how to  combine the information in this time window to better meet the processing goal is  also nontrivial. Since we are interested in adaptive systems, the goal is to let the  system find these quantities adaptively using the output error information.  These abstract ideas can be framed more quantitatively in a geometric setting (vector  space). Assume that the input is a vector [u(1),... u(n), .... ] of growing size. The  adaptive processor (a neural network in our case) has a fixed size to represent this  information, which we assign to its state vector [xl(n), .... xN(n)] of size N. The  usefulness of xl(n) depends on how well it spans the growing input space (defined by  the vector u(n)), and how well it spans the decision space which is normally  associated with the minimization of the mean square error (Figure 1). Therefore, in  principle, the procedure can be divided into a representational and a mapping  problem.  The most general solution to this problem is to consider a nonlinear projection  manifold which can be modified to meet both requirements. In terms of neural  topologies, this translates to a full recurrent system, where the weights are adapted  such that the error criterion is minimized. Experience has shown that this is a rather  difficult proposition. Instead, neural network researchers have worked with a wealth  of methods that in some way constrain the neural topology.  '" Projection space  Nonlinear mapping  x(n) error  Decision space  Figure 1. Projection of u(n) and the error for the task. (for simplicity we  are representing only linear manifolds)  The solution that we have been studying is also constrained. We consider a linear  manifold as the projection space, which we call the memory space. The projection of  u(n) in this space is subsequently mapped by means of a feedforward neural network  (multilayer perceptton) to a vector in decision space that minimizes the error  criterion. This model gives rise to the focused topologies. The advantage of this  constrained model is that it allows an analytical study of the memory structures, since  they become linear filters. It is important to stress that the choice of the projection  space is crucial for the ultimate performance of the system, because if the projected  version of u(n) in the memory space discards valuable information about u(n), then  Analysis of Short Term Memories for Neural Networks 1013  the nonlinear mapping will always produce sub-optimal results.  2 Projection in the memory space  If the projection space is linear, then the representational problem can be studied with  linear system concepts. The projected vector u(n) becomes Yn  N  Yn = E widen- I  k=l  (1)  where x n are the memory traces. Notice that in this equation the coefficients w k are  independent of time, and their number fixed to N. What is the most general linear  structure that implements this projection operation? It is the generalizedfeedforward  structure [Principe et al, 1992] (Figure 2), which in connectionist circles has been  called the time lagged recursire network [Back and Tsoi, 1992]. One can show that  the defining relation for generalized feedforward structures is  g(n) = g(n) g_l(n) k>l  where  represents the convolution operation, and go(n) = 5(n). This relation  means that the next state vector is constructed from the previous state vector by  convolution with the same function g(n), yet unspecified. Different choices of g(n)  will provide different choices for the projection space axes. When we apply the input  u(n) to this structure, the axes of the projection space become xk(n), the convolution  of u(n) with the tap signals. The projection is obtained by linearly weighting the tap  signals according to equation (1).  [ g(n) f/ g(n)  gl ((n22(n)  gk(n)  Figure 2. The generalized feedforward structure  We define a memory structure as a linear system whose generating kernel g(n) is  causal g (n) - 0 for n < 0 and normalized, i.e.   Ig(n)l - 1  n=0  We define memory depth D as the modified center of mass (first moment in time) of  the last memory tap.  D = Englc(n)  n=0  And we define the memory resolution R as the number of taps by unit time, which  1014 Principe, Hsu, and Kuo  becomes 1/D. The purpose of the memory structure is to transform the search for an  unconstrained number of coefficients (as necessary if we worked directly with u(n))  into one of seeking a fixed number of coefficients in a space with time varying axis.  3 Review of connectionist memory structures  The gamma memory [deVries and Principe, 1992] contains as special cases the  context unit [Jordan, 1986] and the tap delay line as used in TDNN [Waibel et al,  1989]. However, the gamma memory is also a special case of the generalized  feedforward filters where g (n) =  (1 - [) n which leads to the gamma functions as  the tap signals. Figure 3, adapted from [deVries and Principe, 1993], shows the most  common connectionist memory structures and its characteristics.  As can be seen when k=l, the gamma memory defaults to the context unit, and when  =1 the gamma memory becomes the tap delay line. In vector spaces the context unit  represents a line, and by changing [ we are finding the best projection of u(n) on this  line. This representation is appropriate when one wants long memories but low  resolution.  Likewise, in the tap delay line, we are projecting u(n) in a memory space that is  uniquely determined by the input signal, i.e. once the input signal u(n) is set, the axes  become u(n-k) and the only degree of freedom is the memory order K. This memory  structure has the highest resolution but lacks versatility, since one can only improve  the input signal representation by increasing the order of the memory. In this respect,  the simple context unit is better (or any memory with a recursive parameter), since  the neural system can adapt the parameter  to project the input signal for better  performance.  We recently proved that the gamma memory structure in continuous time represents  a memory space that is rigid [Principe et al, 1994]. When minimizing the output mean  square error, the distance between the input signal and the projection space  decreases. The recursive parameter in the feedforward structures changes the span of  the memory space with respect to the input signal u(n) (which can be visualized as  some type of complex rotation). In terms of time domain analysis, the recursive  parameter is finding the length of the time window (the memory depth) containing  the relevant information to decrease the output mean square error. The recursive  parameter  can be adapted by gradient descent learning [deVries and Principe,  1992], but the adaptation becomes nonlinear and multiple minima exists.Notice that  the memory structure is stable for 0<<2.  The gamma memory when utilized as a linear adaptive filter extends Widrow's  ADALINE [deVries et al, 1992], and results in a more parsimonious filter for echo  cancellation [Palkar and Principe, 1994]. Preliminary results with the gamma  memory in speech also showed that the performance of word spotters improve when   is different from one (i.e. when it is not the tap delay line). In a signal such as  speech where time warping is a problem, there is no need to use the full resolution  provided by the tap delay line. It is more important to trade depth by resolution.  Analysis of Short Term Memories for Neural Networks 1015  4  Other Memory Structures  There are other memory structures that fit our definition. Back and Tsoi proposed a  lattice structure that fits our definition of generalized feedforward structure.  Essentially this system orthogonalizes the input, uncorrelating the axis of the vector  space (or the signals at the taps of the memory). This method is known to provide the  best speed of adaptation because gradient descent becomes Newton's method (after  the lattice parameters converge). The problem is that it becomes more computational  demanding (more parameters to adapt, and more calculations to perform).  u(t)  Tape delay line  Delay operator: Z 'l  memory depth: K  z domain  Memory resolution: 1.  Context Unit  u(t)  y(t)  Delay operator: it  z- (1-Ix)  Memory depth: 1/g  Memory resolution:  Gamma memory  Delay operator:  z- (  z domain  Memory depth: k/g Memory resolution:  Figure 3. Connectionist memory structures  Laguerre memories  A set of basis intimately related to the gamma functions is the Laguerre bases. The  1016 Principe, Hsu, and Kuo  Laguerre bases is an orthogonal span of the gamma space [Silva, 1994], which means  that the information provided by both memories is the same. The advantage of the  Laguerre is that the signals at the taps (the basis) are less correlated and so the  adaptation speed becomes faster for values of g close to 0 or 2 [Silva, 1994] (the  condition number of the matrix created by the tap signals is bounded). Notice that the  Laguerre memory is still very easy to compute (a lowpass filter followed by a cascade  of first order all pass filters).  Laguerre memory  1-g l-Ix L(z)  1 r g2(t ) il(I  g z- (1-g)-  Delay operator: ;  z- (1-it) z- (1  z domain  0  Gamma II memories.  The Gamma memory has a multiple pole that can be adaptively moved along the real  Z domain axis, i.e. the Gamma memory can only implement lowpass (0< g <1) or  highpass (1 < <2) transfer functions. We experimentally observed that in nonlinear  prediction of chaotic time series the recursive parameter sometimes adapts to values  less than one. The highpass creates an extra ability to match the prediction by  alternating the signs of the samples in the gamma memory (the impulse response for  1< g <2 is alternating in sign). But with a single real parameter the adaptation is  unable to move the poles to complex values. Two conditions come to mind that  require a memory structure with complex poles. First, the information relevant for  the signal processing task appears in periodic bursts, and second, the input signal is  corrupted by periodic noise. A memory structure with adaptive complex poles can  successfully cope with these two conditions by selecting in time the intervals where  the information is concentrated (or the windows that do not provide any information  for the task). Figure 3 shows one possible implementation for the Gamma II kernel.  Notice that for stability, the parameter v must obey the condition  (1 + O) < 2 and  0 < <2. Complex poles are obtained for v> 0. These parameters can be adapted by  gradient descent [Silva et al, 1992]. In terms of versatility, the Gamma II has a pair  of free complex poles, the Gamma I has a pole restricted to the real line in the Z  domain, and the tap delay line has the pole set at the origin of the Z domain (z=0). A  multilayer perceptron equipped with an input memory layer with the Gamma II  memory structure implements a nonlinear mapping on an ARMA model of the input  signal.  5  How to use Memory structures in Connectionist networks.  Although we have presented this theory with the focused architectures (which  Analysis of Short Term Memories for Neural Networks 1017  corresponds to a nonlinear moving average model (NMAX)), the memory structures  can be placed anywhere in the neural topology. Any nonlinear processing element can  feed one of these memory kernels as an extension of [Wan, 1990]. If the memory  structures are used to store traces of the output of the net, we obtain a nonlinear  autoregressive model (NARX). If they are used both at the input and output, they  represent a nonlinear ARMAX model shown very powerful for system identification  tasks. When the memory layer is placed in the hidden layers, there is no  corresponding linear model.  Gamma II  !r gl(t) gK(t) ,  Delay operator:  It[z- (1  [z- (l-[t)]2+o[t 2  z domain  One must realize that these types of memory structures are recursive (except the tap  delay line), so their training will involve gradients that depend on time. In the focused  topologies the network weights can still be trained with static backpropagation, but  the recursive parameter must be trained with real time recurrent learning (RTRL) or  backpropagation through time (BPTT). When memory structures are scattered  through out the topology, training can be easily accomplished with backpropagation  through time, provided a systematic way is utilized to decompose the global  dynamics in local dynamics as suggested in [Lefebvre and Principe, 1993].  6 Conclusions  The goal of this paper is to present a set of memory structures and show their  relationship. The newly introduced Gamma II is the most general of the memories  reviewed. By adaptively changing the two parameters v,g the memory can create  complex poles at any location in the unit circle. This is probably the most general  memory mechanism that needs to be considered. With it one can model poles and  zeros of the system that created the signal (if it accepts the linear model).  In this paper we addressed the general problem of extracting patterns in time. We  have been studying this problem by pre-wiring the additive neural model, and  decomposing it in a linear part -the memory space- that is dedicated to the storage of  past values of the input (output or internal states), and in a nonlinear part which is  static. The memory space accepts local recursion, which creates a powerful  representational structure and where stability can be easily enforced (test in a single  parameter). Recursive memories have the tremendous advantage of being able to  trade memory depth by resolution. In vector spaces this means changing the relative  1018 Principe, Hsu, and Kuo  position between the projection space and the input signal. However, the problem of  finding the best resolution is still open (this means adaptively finding k, the memory  order). Likewise ways to adaptively find the optimal value of the memory depth need  improvements since the gradient procedures used up to now may be trapped in local  minima. It is still necessary to modify the definition of memory depth such that it  applies to both of these new memory structures. The method is to define it as the  center of mass of the envelope of the last kernel.  Acknowledgments:This work was partially supported by NSF grant ECS #920878.  7 References  Back, A.D. and A. C. Tsoi, An Adaptive Lattice Architecture for Dynamic Multilayer  Perceptrons, Neural Computation, vol. 4, no. 6, pp. 922-931, November, 1992.  de Vries, B. and J. C. Principe, "The gamma model - a new neural model for temporal  processing," Neural Networks, vol. 5, no. 4, pp. 565-576, 1992.  de Vries, B., J.C. Principe, and P.G. De Oliveira, "Adaline with adaptive recursive  memory," Proc. IEEE Workshop Neural Networks on Signal Processing, Princeton,  NJ, 1991.  Jordan, M., "Attractor dynamics and parallelism in a connectionist sequential  machine," Proc. 8th annual Conf. on Cognitive Science Society, pp. 531-546, 1986.  Lefebvre, C., and J.C. Principe, "Object-oriented artificial neural network  implementations", Proc. World Cong on Neural Nets, vol IV, pp436-439, 1993.  Principe, J. deVries B., Oliveira P., "Generalized feedforward structures: a new class  of adaptive fitlets", ICASSP92, vol IV, 244-248, San Francisco.  Principe, J.C., and B o de Vries, "Short term neural memories for time varying signal  classification," in Proc. 26th ASILOMAR Conf., pp. 766-770, 1992.  Principe J. C., J.M. Kuo, and S. Celebi," An Analysis of Short Term Memory  Structures in Dynamic Neural Networks", accepted in the special issue of recurrent  networks of IEEE Trans. on Neural Networks.  Palkar M., and J.C. Principe, "Echo cancellation with the gamma filter," to be  presented at ICASSP, 1994.  Silva, T.O., "On the equivalence between gamma and Laguerre filters," to be presented  at ICASSP, 1994.  Silva, T.O., J.C. Principe, and B. de Vries, "Generalized feedforward filters with  complex poles," Proc. Second IEEE Conf. Neural Networks for Signal Processing,  pp.503-510, 1992.  Waiber, A., "Modular Construction of Time-Delay Neural Networks for Speech  Recognition," Neural Computation 1, pp39-46, 1989.  Wan, A. E., "Temporal backpropagation: an efficient algorithm for finite impulse  response neural networks," Connectionist Models, Proc. of the 1990 Summer School,  pp.131-137, 1990.  
Using Local Trajectory Optimizers To  Speed Up Global Optimization In  Dynamic Programming  Christopher G. Atkeson  Department of Brain and Cognitive Sciences and  the Artificial Intelligence Laboratory  Massachusetts Institute of Technology, NE43-771  545 Technology Square, Cambridge, MA 02139  617-253-0788, cga@ai.mit.edu  Abstract  Dynamic programming provides a methodology to develop planners  and controllers for nonlinear systems. However, general dynamic  programming is computationally intractable. We have developed  procedures that allow more complex planning and control problems  to be solved. We use second order local trajectory optimization  to generate locally optimal plans and local models of the value  function and its derivatives. We maintain global consistency of the  local models of the value function, guaranteeing that our locally  optimal plans are actually globally optimal, up to the resolution of  our search procedures.  Learning to do the right thing at each instant in situations that evolve over time is  difficult, as the future cost of actions chosen now may not be obvious immediately,  and may only become clear with time. Value functions are a representational tool  that makes the consequences of actions explicit. Value functions are difficult to  learn directly, but they can be built up from learned models of the dynamics of the  world and the cost function. This paper focuses on how fast optimizers that only  produce locally optimal answers can play a useful role in speeding up the process  of computing or learning a globally optimal value function.  Consider a system with dynamics Xkq-1 -- f(xk, uk) and a cost function L(xk, uk),  663  664 Atkeson  where x is the state of the system and u is a vector of actions or controls. The sub-  script k serves as a time index, but will be dropped in the equations that follow. A  goal of reinforcement learning and optimal control is to find a policy that minimizes  the total cost, which is the sum of the costs for each time step. One approach to  doing this is to construct an optimal value function, V(x). The value of this value  function at a state x is the sum of all future costs, given that the system started in  state x and followed the optimal policy P(x) (chose optimal actions at each time  step as a function of the state). A local planner or controller can choose globally  optimal actions if it knew the future cost of each action. This cost is simply the  sum of the cost of taking the action right now and the future cost of the state that  the action leads to, which is given by the value function.  u* -- arg nn (L(x, u) + V(f(x, u))) (1)  Value functions are difficult to learn. The environment does not provide training  examples that pair states with their optimal cost (x, V(x)). In fact, it seems that the  optimal policy depends on the optimal value function, which in turn depends on the  optimal policy. Algorithms to compute value functions typically iteratively refine  a candidate value function and/or a corresponding policy (dynamic programming).  These algorithms are usually expensive. We use local optimization to generate  locally optimal plans and local models of the value function and its derivatives. We  maintain global consistency of the local models of the value function, guaranteeing  that our locally optimal plans are actually globally optimal, up to the resolution of  our search procedures.  I A SIMPLE EXAMPLE: A PENDULUM  In this paper we will present a simple example to make our ideas clear. Figure 1  shows a simulated set of locally optimal trajectories in phase space for a pendulum  being driven by a motor at the joint from the stable to the unstable equilibrium  position. S marks the start point, where the pendulum is hanging straight down,  and G marks the goal point, where the pendulum is inverted (pointing straight up).  The optimization criteria quadratically penalizes deviations from the goal point  and the magnitude of the torques applied. In the three locally optimal trajectories  shown the pendulum either swings directly up to the goal (1), moves initially away  from the goal and then swings up to the goal (2), or oscillates to pump itself and  then swing to the goal (3). In what follows we describe how to find these locally  optimal trajectories and also how to find the globally optimal trajectory.  2 LOCAL TRAJECTORY OPTIMIZATION  We base our local optimization process on dynamic programming within a tube  surrounding our current best estimate of a locally optimal trajectory (Dyer and  McReynolds 1970, Jacobson and Mayne 1970). We have a local quadratic model  of the cost to get to the goal (V) at each time step along the optimal trajectory  (assume a time step index k in everything below unless otherwise indicated):  lxTVxxx (2)  V(x) Vo + +  Using Local Trajectory Optimizers to Speed Up Global Optimization 665  Figure 1: Locally optimal trajectories for the pendulum swing up task.  A locally optimal policy can be computed using local models of the plant (in this  case local linear models) at each time step along the trajectory:  Xk+l = f(x, u)  Ax + Bu + c (3)  and local quadratic models of the one step cost at each time step along the trajec-  tory:  L(x, u)  -x T qx + -1 urRu + xTSu + tTu (4)  2  At each point along the trajectory the optimal policy is given by:  u pt = -(R + BZVxxB) -1 x  (BzVxAx + SZx + BrV,,c + V,B + t)  One can integrate the plant dynamics forward in time based on the above policy,  and then integrate the value functions and its first and second spatial derivatives  backwards in time to compute an improved value function, policy, and trajectory.  For a one step cost of the form:  1  L(x, .)  (x - xd)  q(x - xd)+  1 (. _ .)r l(. _ .) + (x _ x)"s (. - .)  the backward sweep takes the following form (in discrete time)'  Zx = V A + q(x- xa)  Zu = VB + R(u - ua)  Z = A r VxA + Q  Z,, = BrV,A + S  Z = BTVxxB + R  K = ZS 1 Z  Vk_l ' Z -- Z,K  V, , k _ l = Z , , - Z , , K  (5)  (6)  (7)  (8)  (9)  (lO)  (11)  (12)  666 Atkeson  3 STANDARD DYNAMIC PROGRAMMING  A typical implementation of dynamic programming in continuous state spaces dis-  cretizes the state space into cells, and assigns a fixed control action to each cell.  Larson's state increment dynamic programming (Larson 1968) is a good example  of this type of approach. In Figure 2A we see the trajectory segments produced by  applying the constant action in each cell, plotted on a phase space for the example  problem of swinging up a pendulum.  4  USING LOCAL TRAJECTORY OPTIMIZATION  WITH DP  We want to minimize the number of cells used in dynamic programming by making  the cells as large as possible. Combining local trajectory optimization with dynamic  programming allows us to greatly reduce the resolution of the grid on which we do  dynamic programming and still correctly estimate the cost to get to the goal from  different parts of the space. Figure 2A shows a dynamic programming approach  in which each cell contains a trajectory segment applied to the pendulum problem.  Figure 2B shows our approach, which creates a set of locally optimal trajectories  to the goal. By performing the local trajectory optimizations on a grid and forcing  adjacent trajectories to be consistent, this local optimization process becomes a  global optimization process. Forcing adjacent trajectories to be consistent means  requiring that all trajectories can be generated from a single underlying policy.  A trajectory can be made consistent with a neighbor by using the neighboring  trajectory as an initial trajectory in the local optimization process, or by using the  value function from the neighboring trajectory to generate the initial trajectory in  the local optimization process. Each grid element stores the trajectory that starts  at that point and achieves the lowest cost.  The trajectory segments in figure 2A match the trajectories in 2B. Figures 2C and  2D are low resolution versions of the same problem. Figure 2C shows that some  of the trajectory segments are no longer correct. In Figure 2D we see the locally  optimal trajectories to the goal are still consistent with the trajectories in Figure 2B.  Using locally optimal trajectories which go all the way to the goal as building blocks  for our dynamic programming algorithm allows us to avoid the problem of correctly  interpolating the cost to get to the goal function on a sparse grid. Instead, the cost  to get to the goal is measured directly on the optimal trajectory from each node to  the goal. We can use a much sparser grid and still converge.  5  ADAPTIVE GRIDS BASED ON CONSTANT COST  CONTOURS  We can limit the search by "growing" the volumes searched around the initial and  goal states by gradually increasing a cost threshold C a. We will only consider states  around the goal that have a cost less than C a to get to the goal and states around  the initial state that have a cost less than C a to get from the initial state to that  state (Figure 3B). These two regions will increase in size as C a is increased. We stop  Using Local Trajectory Optimizers to Speed Up Global Optimization 667  A  c  B  D  Figure 2: Different dynamic programming techniques (see text).  668 Atkeson  Figure 3: Volumes defined by a cost threshold.  increasing C a as soon as the two regions come into contact. The optimal trajectory  has to be entirely within the union of these two regions, and has a cost of 2Ca.  Instead of having the initial conditions of the trajectories laid out on a grid over the  whole space, the initial conditions are laid out on a grid over the surface separating  the inside and the outside surfaces of the volumes described above. The resolution  of this grid is adaptively determined by checking whether the value function of one  trajectory correctly predicts the cost of a neighboring trajectory. If it does not,  additional grid points are added between the inconsistent trajectories.  During this global optimization we separate the state space into a volume around  the goal which has been completely solved and the rest of the state space, in which  no exploration or computation has been done. Each iteration of the algorithm  enlarges the completely solved volume by performing dynamic programming from  a surface of slightly increased cost to the current constant cost surface. When the  solved volume includes a known starting point or contacts a similar solved volume  with constant cost to get to the boundary from the starting point, a globally optimal  trajectory from the start to the goal has been found.  6  DP BASED ON APPROXIMATING CONSTANT  COST CONTOURS  Unfortunately, adaptive grids based on constant cost contours still suffer from the  curse of dimensionality, having only reduced the dimensionality of the problem by  1. We are currently exploring methods to approximate constant cost contours. For  example, constant cost contours can be approximated by growing "key" trajectories.  Using Local Trajectory Optimizers to Speed Up Global Optimization 669  Figure 4: Approximate constant cost contours based on key trajectories  A version of this is illustrated in Figure 4. Here, trajectories were grown along the  "bottoms" of the value function "valleys". The location of a constant cost contour  can be estimated by using local quadratic models of the value function produced  by the process which optimizes the trajectory. These approximate representations  do not suffer from the curse of dimensionality. They require on the order of TD 2,  where T is the length of time the trajectory requires to get to the goal, and D is  the dimensionality of the state space.  7 SUMMARY  Dynamic programming provides a methodology to plan trajectories and design con-  trollers and estimators for nonlinear systems. However, general dynamic program-  ming is computationally intractable. We have developed procedures that allow more  complex planning problems to be solved. We have modified the State Increment  Dynamic Programming approach of Larson (1968) in several ways:  In State Increment DP, a constant action is integrated to form a trajectory  segment from the center of a cell to its boundary. We use second order local  trajectory optimization (Differential Dynamic Programming) to generate an  optimal trajectory and form an optimal policy in a tube surrounding the  optimal trajectory within a cell. The trajectory segment and local policy  are globally optimal, up to the resolution of the representation of the value  function on the boundary of the cell.  We use the optimal policy within each cell to guide the local trajectory  optimization to form a globally optimal trajectory from the center of each  670 Atkeson  cell all the way to the goal. This helps us avoid the accumulation of inter-  polation errors as one moves from cell to cell in the state space, and avoid  limitations caused by limited resolution of the representation of the value  function over the state space.  3. The second order trajectory optimization provides us with estimates of  the value function and its first and second spatial derivatives along each  trajectory. This provides a natural guide for adaptive grid approaches.  4. During the global optimization we separate the state space into a voltime  around the goal which has been completely solved and the rest of the state  space, in which no exploration or computation has been done. The sur-  face separating these volumes is a surface of constant cost, with respect to  achieving the goal.  5. Each iteration of the algorithm enlarges the completely solved volume by  performing dynamic programming from a surface of slightly increased cost  to the current constant cost surface.  6. When the solved volume includes a known starting point or contacts a  similar solved volume with constant cost to get to the boundary from the  starting point, a globally optimal trajectory from the start to the goal has  been found. No optimal trajectory will ever leave the solved volumes. This  would require the trajectory to increase rather than decrease its cost to get  to the goal as it progressed.  7. The surfaces of constant cost can be approximated by a representation that  avoids the curse of dimensionality.  8. The true test of this approach lies ahead: Can it produce reasonable solu-  tions to complex problems?  Acknowledgement s  Support was provided under Air Force Office of Scientific Research grant AFOSR-  89-0500, by the Siemens Corporation, and by the ATR Human Information Process-  ing Research Laboratories. Support for CGA was provided by a National Science  Foundation Presidential Young Investigator Award.  References  Bellman, R., (1957) Dynamic Programming, Princeton University Press, Princeton,  NJ.  Bertsekas, D.P., (1987) Dynamic Programming: Deterministic and Stochastic Mod-  els, Prentice-Hall, Englewood Cliffs, NJ.  Dyer, P. and S.R. McReynolds, (1970) The Computation and Theory of Optimal  Control, Academic Press, New York, NY.  Jacobson, D.H. and D.Q. Mayne, (1970) Differential Dynamic Programming, Else-  vier, New York, NY.  Larson, R.E., (1968) State Increment Dynamic Programming, Elsevier, New York,  NY.  
Recovering a Feed-Forward Net  From Its Output  Charles 'efferman* and Scott Markel  David Sarnoff Research Center  CN5300  Princeton, NJ 08543-5300  e-mail: cfmath.princeton.edu  smarkel@sarnoff. com  ABSTRACT  We study feed-forward nets with arbitrarily many layers, using the stan-  dard sigmoid, tanh x. Aside from technicalities, our theorems are:  1. Complete knowledge of the output of a neural net for arbitrary inputs  uniquely specifies the architecture, weights and thresholds; and 2. There  are only finitely many critical points on the error surface for a generic  training problem.  Neural nets were originally introduced as highly simplified models of the nervous  system. Today they are widely used in technology and studied theoretically by  scientists from several disciplines. However, they remain little understood.  Mathematically, a (feed-forward) neural net consists of:  (i)  (2)  A finite sequence of positive integers (Do, D,..., DL);  A family of real numbers (w) defined for 1 < t? < L, 1 < j < D,  and  l <k<De_;  (3) A family of real numbers (0) defined for I<<L, I<j<D.  The sequence (Do, D,..., D) is called the architecture of the neural net, while the  wk are called weights and the 0 thresholds.  Neural nets are used to compute non-linear maps from I 2 to m by the following  construction. Ve begin by fixing a nonlinear function (x) of one variable. Analogy  with the nervous system suggests that we take (x) asymptotic to constants as x  tends to q-(x>; a standard choice, which we adopt throughout this paper, is r(:) =  *Alternate address: Dept. of Mathematics, Princeton University, Princeton, NJ 085,t4-1000.  335  336 Fefferman and Markel  e for  tanh(x). Given an "input" (t1,...,too)  Oo, we define real numbers x 3  0 <  < L, 1 <_ j < Dt by the following induction on .  t--tj  (4) If g = 0 then xj .  (5) If the x - are known vith g fixed (1 < g < L), then we set  xj =  wjix + Oj for 15jsD.  l<k<Dt_  Here x .. z t  '' , Dt are interpreted as the outputs of Dt "neurons" in the th "layer"  of the net. The output map of the net is defined as the map  (6) (I): (tl, . . .,tDo ) , , (Zf,   .,Z//.).  In practical applications, one tries to pick the neural net [(D0,Dt,...,D), (w,),  (0)] so that the output map (I) approximates a given map about which we have  only imperfect information. The main result of this paper is that under generic  conditions, perfect knowledge of the output map (I) uniquely specifies the architec-  tre, the weights and the thresholds of a neural net, up to obvious symmetries.  More precisely, the obvious symmetries are as follows. Let (70,7,.-., 7) be per-  mutations, with 7e' {1,...,De} ---, {1, ...,De}; and let {e' 0<  < L, 1 <j < Dr} be  a collection of -t-l's. Assume that -yt = (identity) and ej = +1 whenever  = 0 or   -- L. Then one checks easily that the neural nets  (7) [(Do,D,...,D,), (wJ), (OJ)] and  (s)  h ave  [(Do, DL), -t  the same output map if we set  (9) -e t t t- 7t t  3jk -- zjW[ytj][vt_lk]ek and Oj -- ejO[7tj I.  This reflects the facts that the neurons in layer  are interchangeable (1 <  < L- 1),  and that the function or(x) is odd. The nets (7) and (8) will be called isornorph,c  if they are related by (9). Note in particular that isomorphic neural nets have the  same architecture. Our main theorem asserts that, under generic conditions, any  two neural nets with the same output map are isomorphic.  We discuss the generic conditions which we impose on neural nets. We have to  avoid obvious counterexamples such as:  Suppose all the weights w are zero. Then the output map (I) is constant.  The architecture and thresholds of the neural net are clearly not uniquely  determined by (I).  (11) Fix 0, j, j2 with 1 _< e0 <_ L - 1 and 1 <j < j2 < Dto. Suppose we have  0 e- = 0: and to to to = x..o Therefore, the  3 Wj k -- Wj k for all k. Then (5) gives xj .  Recovering a Feed-Forward Net from Its Output 337   .t0+l . fo+l So   ,/o+1 ,lo+1 only through the sum jj  output depends on '"3J and Wjj . .  the output map does not uniquely determine the weights.  Our hypotheses are more than adequate to exclude these counterexamples. Specif-  ically, we assume that  (12) /  0 and 10ffl # 10ff,I for j -7 ! j'.  (13) wk 0: and forjj',theratio l   Wjk/Wj, k is not equal to any fraction of the  form p/q with p, q integers and 1 < q < 100 D].  Evidently, these conditions hold for generic neural nets. The precise statement of  our main theorem is as follows. If two neural nets satisfy (12), (13) and have the  same output, then the nets are isomorphic. It would be interesting to replace (12),  (13) by minimal hypotheses. and to study functions rr(;c) other than tanh (x).  We now sketch the proof of our main result. sacrificing accuracy for simplicity.  After a trivial reduction. we may assume Do = D/. = 1. Thus, the outputs of the  nodes :c5 (t) are functions of one variable, and the output map of the neural net is  t -, a:(t). The key idea is to continue the xS(t) analytically to complex values oft,   Note  and to read off the structure of the net from the set of singularities of the xj.  that rr(;c) = tanh (x) is meromorphic, with poles at the points of an arithmetic  progression {(2m + 1),-ri: m  Z}. This leads to two crucial observations.  e  and  (14) When  = 1, the poles of zj(t) form an arithmetic progression  (15) When g > 1, every pole of any z-(t) is an accumulation point of poles of  any xj (t).  :(t) = rr(w)t + 0J), which is merely  In fact, (14) is immediate from the formula z  the special case Do = 1 of (5). We obtain   { (2rn + 1)"'ri - ). rn G Z }  (16) IIj= cv)  To see (15), fix e, j, /, and assume for simpliciw that .r.-(t) has a simple pole at  to, while x-(t) (k  ) is analytic in a neighborhood of to. Then  (17) .-l(t) -  t --to  From  + f(t),  (17) and (5), we obtain  with f analytic in a neighborhood of to.  (lS)  xj(t) to)- + g(t)), with  g(t) wjf(t) + Z' e  = wjxk (t) + Oj analytic in a neighborhood of to.  Thus, in a neighborhood of to, the poles of x5 (t ] are the solutions f, of the equation  + = + e -  (20) - to  338 Fefferman and Markel  There are infinitely many solutions of (20), accumulating at to. Hence. to is an  accumulation point of poles of a:(t), which completes the proof of (15).  hi view of (14), (15), it is natural to make the following definitions. The natural  domain of a neural net is the largest open subset of the complex plane to which the  output map t  z(t) can be analytically continued. For t> 0 we define the th  singular set Sing(C) by setting  Sing(O) = complement of the natural domain in C, and  Sing(e + 1) = the set of all accumulation points of Sing(t).  These definitions are made entirely in terms of the output map, without reference  to the structure of the given neural net. On the other hand, the sets Sing(r.) contain  nearly complete information on the architecture, weights and thresholds of the net.  This will allow us to read off the structure of a neural net from the analytic contin-  uation of its output map. To see how the sets Sing(f) reflect the structure of the  net, we reason as follows.  From (14) and (15) we expect that  (21) For 1 <  < L, Sing(L - t) is the union over j = 1,..., Dt of the set of poles of  zJ(t), together with their accumulation points (which we ignore here), and  (22) For t?> L, Sing(t) is empty.  Immediately, then, we can read off the "depth" L of the neural net; it is simply the  smallest e for which Sing(t) is empty.  z  We proceed bv induction on .  Ve need to solve for D,  When  = 1, (14) and (21) show that Sing(L- 1) is the union of arithmetic pro-  gressions nJ, j = 1,..., Therefore, from Sing( - 1) we can read off D and  the II}. (We will return to this point later in the introduction.) In view of (1),  IIJ determines the weights and thresholds at layer 1, modulo signs. Thus. we have  found D, wJk, 0J.  When t > 1, we may assume that  z' 0' are already known, for 1 < ' < .  (23) The D,, wit, _  Our task is to find D, wi, 0. In view of (23), we can find a pole to of z.-(t) for  our favorite k. Assume for simplicity that to is a simple pole of x.-(t), and that  the x.-(t) (k :/e) are analytic in a neighborhood of t0. Then   (t) is given by  (17) in a neighborhood of to, with ,X already known by virtue of (23). Let U be a  small neighborhood of to.  We will look at the image Y of U Cl Sing(L - t) under the map t - t-to ,  to and Sing(L - f) are already known, so is Y. On the other hand, we can relate Y  to D, , t?J as follows. From (21) we see that Y is the union over j = 1,..., De  of  (24) Yj =imageofUCl{ Poles ofxj(t)) undert, ,-  It-to)'  Recovering a Feed-Forward Net from Its Output 339  For fixed j, the poles of zJ(t) in a neighborhood of to are the , given by (20). We  write  = 3 +  (25) im- to -  + [g(t0)-  Equation (20) shows that the first expression in brackets in (25) is equal to (2m +  1)ri. Also, since ',n -- to as Irnl --. oc and g is analytic in a neighborhood of to,  the second expression in brackets in (25) tends to zero. Hence,  cA _. (2m+l)ri-g(t0)+o(1) for large m.  ,n --to  Comparing this with the definition (24), we see that 5 is asymptotic to the arith-  metic progression  e {(2m+l)i-g(to)  (26) II= we .mZ .  j  Thus, the known set Y is the union over j = 1 .... , De of sets Yj, with Yj asymptotic  to the arithmetic progression IIJ. From Y, we can therefore read off De and the IIJ.  (We will return to this point in a moment.) We see at once from (26) that w{ is  determined up to sign by IIJ. Thus, we have found De and w e With more work,  we can also find the 0J, completing the induction on .  The above induction shows that the structure of a neural net may be read off  from the analytic continuation of its output map. We believe that the analytic  continuation of the output map will lead to further consequences in the study of  neural nets.  Let us touch briefly on a few points which we glossed over above. First of all, suppose  we are given a set Y C C, and we know that ' is the union of sets Y,..., YD, with  Yj asymptotic to an arithmetic progression IIj. Ve assumed above that II,..., IID  are uniquely determined by Y. In fact, without some further hypothesis on the  IIj, this need not be true. For instance, we cannot distinguish II U II2 from IIa  if II -- {odd integers}, II2 - {even integers}. II3 - {all integers}. On the other  hand, we can clearly recognize II = {all integers} and II2 = {m,/:- m an integer}  from their union II U II2. Thus, irrational numbers enter the picture. The r61e of  our generic hypothesis (13) is to control the arithmetic progressions that arise in  our proof.  Secondly, suppose xi(t) has a pole at to. We assumed for simplicity that xi.(t ) is an-  alytic in a neighborhood of to for k : . However, one of the x(t) (k :/) may also  have a pole at to. In that case, the x + (t) may all be analytic in a neighborhood of  to, because the contributions of the singularities of the x. in  ,j  + Oj  may cancel. Thus, the singularity at to may disappear from the output map. While  this circumstance is hardly generic, it is not ruled out by our hypotheses (12), (13).  340 Fefferman and Markel  Because singularities can disappear, we have to make technical changes in our de-  scription of Sing(). For example, in the discussion following (23), Y need not be  the union of the sets l) .. Rather, Y is their "approximate union". (See IF]).  Next, we should point out that the signs of the weights and thresholds require  some attention, even though we have some freedom to change signs by applying  isomorphisms. (See (9).)  Finally, in the definition of the natural domain, we have assumed that there is a  unique maximal open set to which the output map continues analytically. This  need not be true of a general real-analytic function on the line - for instance. take  f(t) = (1 +/2)1/2 Fortunately, the natural domain is well-defined for any function  that continues analytically to the complement of a countable set. The defining  formula (5) lets us check easily that the output map continues to the complement  of a countable set, so the natural domain makes sense. This concludes our overview  of the proof of our main theorem. The full proof of our results will appear in [Fl.  Both the uniqueness problem and the use of analytic continuation have already  appeared in the neural net literature. In particular, it was R. Hecht-Nielson who  pointed out the r61e of isomorphisms and posed the uniqueness problem. His pa-  per with Chen and Lu [CLH] on "equioutput transformations" on the space of all  neural nets influenced our work. E. Sontag [So] and H. Sussman [Su] proved sharp  uniqueness theorems for one hidden layer. The proof in [So] uses complex variables.  Acknowledgements  Fefferman is grateful to R. Crane, S. Markel, J. Pearson, E. Sontag, R. Sverdlove,  and N. Winarsky for introducing him to the study of neural nets.  This research was supported by the Advanced Research Projects Agency of the De-  partment of Defense and was monitored by the Air Force Office of Scientific Research  under Contract F49620-92-C-0072. The United States Government is authorized to  reproduce and distribute reprints for governmental purposes notwithstanding any  copyright notation hereon. This work was also supported by the National Science  Foundation.  The following posters, presented at NIPS 93, may clarify our uniqueness theorem.  References  [CLH]  [F]  [So]  [Su]  R. Hecht-Nielson, et al., On the geometry of feedforward neural network error  surfaces. (to appear).  C. Fefferman, Reconstructing a neural network from its output, Revista  Mathemgztica Iberoamericana. (to appear).  F. Albertini and E. Sontag, Uniqueness of weights for neural networks. (to  appear).  H. Sussman, Uniqueness of the weights for minimal feedforward nets tcith a  given input-output map, Neural Netxvorks 5 (1992), pp. 589-593.  Recovering a Feed-Forward Net from Its Output 341  Recovering a Feed-Forward  Net from Its Output  Charles Feferman  David Samoff Research Center and PrinceIon Universily  Princeton, New Jersey  Scott A. Markel  David Samoff Research Center  Princeton, New Jersey  Tske-Home Mecaage  Suppose an unknown neural network is placed in a  You a/en't altowed to look in the box, but you are  allowed to observe the outputs produced by the  network for arbitral/inputs.  Then, in principle, you have enough information to  determine the network architecture (number o layers  and number of nodes in each layer) and the unique  values for all the weights.  The Output Map of a Neural Network  Fix a feed-forward neural network with the standard  sigrnoid o(x) =tanh x.  The map that carries input vectors (x ..... Xn)  to output vectors t,y ..... Ym)  is called the OUTPUT MAP of the neural network.  The Key Question  When can two neural networks  have the same output map?  Obvloug Example of Two Neural  Networks with the Same Output Map  Start with a neural network N.  Then either  1. permute the nodes in a hidden layer, or  2. fix a hidden node, and change the sign oi  every weight (including the bLas weight) that  involves that node  This yields a new neural necwork with the same  output map as N.  Unlquene Theorem  Let N and N' be neural networks that satisfy generic  conditions described below.  If N and N' have the same output map, then they differ  only by sign changes and permutations of hidden nodes.  342 Fefferman and Markel  Generic Conditions  We assume that   all weights are non-zero   bias weights within each layer have distinct  absolute values   the ratio of weights from node i in layer I to nodes j  and k in layer (1+1) is not equal to any fraction of the  form p/q with p, q integers and 1<q<100*(numbe of  nodes in layer I)  Some such assumptions are needed to avoid obvious  counterexamples.  Outline of the Proof   it's enough to consider networks with one input node  and one output node (see below)   all node outputs are now functions of a single, real  variable t (the nelwork input)   analytically continue the network output to a function f  of a single, complex variable t   the qualitative geornet/of the poles of the function f  determines the network architecture (see below)   the asymptotics of the function f near its singularities  determine the weights  Reduction to a Network with  Single Input and Output Nodes   focus attention on a single output node. ignoring the  others   study only input data with a single non-zero entry  Geometric Description of the Poles     poles (small dots) accumulate at essential singularities  (small squares)   essential singularities (small squares) accumulate at  more complicated esse; sinulr;les (large dos)  Determining the Network Architecture tram the Picture   three kinds of singularities (small dots, sinaft squares,  large dcts)   three layers of sigmoids. i.e. two hidden  layers and an output layer   three 'spiral arms" of small squares accumulate at  each large dot   three nodes in the second hidden iaye   two 'spiral arms' of small dcts accumulate at each  small square  := two nodes in the first hidden layer  Determining the Network Architecture from the Picture  (cont'd)   from the network reduction we know that there is  one input node and one output node   therefore, the network architecture is as pictured  
A Local Algorithm to Learn Trajectories  with Stochastic Neural Networks  Javier R. Movellan*  Department of Cognitive Science  University of California San Diego  La Jolla, CA 92093-0515  Abstract  This paper presents a simple algorithm to learn trajectories with a  continuous time, continuous activation version of the Boltzmann  machine. The algorithm takes advantage of intrinsic Brownian  noise in the network to easily compute gradients using entirely local  computations. The algorithm may be ideal for parallel hardware  implementations.  This paper presents a learning algorithm to train continuous stochastic networks  to respond with desired trajectories in the output units to environmental input  trajectories. This is a task, with potential applications to a variety of problems such  as stochastic modeling of neural processes, artificial motor control, and continuous  speech recognition. For example, in a continuous speech recognition problem, the  input trajectory may be a sequence of fast Fourier transform coefficients, and the  output a likely trajectory of phonemic patterns corresponding to the input. This  paper was based on recent work on diffusion networks by Movellan and McClelland  (in press) and by recent papers by Apolloni and de Falco (1991) and Neal (1992)  on asymmetric Boltzmann machines. The learning algorithm can be seen as a  generalization of their work to the stochastic diffusion case and to the problem of  learning continuous stochastic trajectories.  Diffusion networks are governed by the standard connectionist differential equations  plus an independent additive noise component. The resulting process is governed  * Part of this work was done while at Carnegie Mellon University.  83  84 Movellan  by a set of Langevin stochastic differential equations  dai(t) = hi drifti(t) dt +adBi(t) ; i  {1,...,n} (1)  where hi is the processing rate of the ita unit, a is the diffusion constant, which con-  trols the flow of entropy throughout the network, and dBi(t) is a Brownian motion  differential (Soon, 1973). The drift function is the deterministic part of the process.  For consistency I use the same drift function as in Movellan and McClelland, 1992  but many other options are possible: drifti(t) = -?=l wiiaJ(t) - f-lai(t), where  wij is the weight from the jth to the i th unit, and f- is the inverse of a logistic  function scaled in the (rnin - max) interval:f-(a) = log a-,i  mtso- t$ '  In practice DNs are simulated in digital computers with a system of stochastic  difference equations  ai(t + At) = ai(t) + hi drifti(t) At +  zi(t) At ; i  {1, ...,n} (2)  where zi(t) is a standard Gaussian random variable. I start the derivations of  the learning algorithm for the trajectory learning task using the discrete time pro-  cess (equation 2) and then I take limits to obtain the continuous diffusion ex-  pression. To simplify the derivations I adopt the following notation: a trajectory  of states -input, hidden and output units- is represented as a = [a(1)...a(tm)] =  The trajectory vector can be partitioned into 3  consecutive row vectors representing the trajectories of the input, hidden and out-  put units a = [xhy].  The key to the learning algorithm is obtaining the gradient of the probability of  specific trajectories. Once we know this gradient we have all the information needed  to increase the probability of desired trajectories and decrease the probability of  unwanted trajectories. To obtain this gradient we first need to do some derivations  on the transition probability densities. Using the discrete time approximation to  the diffusion process, it follows that the conditional transition probability density  functions are multivariate Gaussian  p(a(t + At)la(t)) =  From equation 2 and 3 it follows that  Since the  computed from the product of the transition probabilities  era--!  r(a) = r(a(t0)) 17I p((t +  =o  The derivative of the probability of a specific trajectory follows  Or()   =  =to  0  Owi-log p(a(t + At)l a(t)) = hizi(t) aj(t) (4)  network is Markovian, the probability of an entire trajectory can be  (5)  (6)  A Local Algorithm to Learn Trajectories with Stochastic Neural Networks 85  In practice, the above rule is all is needed for discrete time computer simulations.  We can obtain the continuous time form by taking limits as At -. O, in which case  the sum becomes Ito's stochastic integral of aj(t) with respect to the Brownian  motion differential over a {to, T} interval.  A similar equation may be obtained for the )q parameters  For notational convenience I define the following random variables and refer to them  as the delia signals  Svii(a) c31og p(a) hl T  = = -- a(i)dBi(t) (9)  and  5x,(a) = Olog p(a) = 1_. a' drifti(i)dBi(t) (10)  A 1 B  1  I I  0 100 200 900 0 100 200 300  Time Steps Time Steps  Figure 1: A) A sample Trajectory. B) The Average Trajectory. As Time Progresses  Sample Trajectories Become Statistically Independent Dampening the Average.  86 Movellan  The approach taken in this paper is to minimize the expected value of the error  assigned to spontaneously generated trajectories 0 = E(p(a)) where p(a) is a signal  indicating the overall error of a particular trajectory and usually depends only on  the output unit trajectory. The necessary gradients follow  0o  = E(6,,jp) (11)  Owij  o  =  Since the above learning rule does not require calculating derivatives of the p func-  tion, it provides great flexibility making it applicable to a wide variety of situations.  For example p(a) can be the TSS between the desired and obtained output unit  trajectories or it could be a reinforcement signal indicating whether the trajectory is  or is not desirable. Figure 1.a shows a typical output of a network trained with TSS  as the p signal to follow a sinusoidal trajectory. The network consisted of 1 input  unit, 3 hidden units, and 1 output unit. The input was constant through time and  the network was trained only with the first period of the sinusoid. The expected  values in equations 11 and 12 were estimated using 400 spontaneously generated  trajectories at each learning epoch. It is interesting to note that although the net-  work was trained for a single period, it continued oscillating without dampening.  However, the expected value of the activations dampened, as Figure 1.b shows.  The dampening of the average activation is due to the fact that as time progresses,  the effects of noise accumulate and the initially phase locked trajectories become  independent oscillators.  2O  p transition: 0.2  Hidden state: 0  Hidden state = 1  p(response 1): 0.1 p(response 1): 0.8  18-  14-  0  0 6-  4-  0  .0  best possible performance  I I I  900 1900 2900  Learning Epoch  Figure 2: A) The Hidden Markov Emitter. B) Average Error Throughout Training.  The Bayesian Limit is Achieved at About 2000 Epochs.  A Local Algorithm to Learn Trajectories with Stochastic Neural Networks 87  The learning rule is also applicable in reinforcement situations where we just have  an overall measure of fitness of the obtained trajectories, but we do not know  what the desired trajectory looks like. For example, in a motor control problem  we could use as fitness signal (-p) the distance walked by a robot controlled by  a DN network. Equations 11 and 12 could then be used to gradually improve the  average distance walked by the robot. In trajectory recognition problems we could  use an overall judgment of the likelihood of the obtained trajectories. I tried this  last approach with a toy version of a continuous speech recognition problem. The  "emitter" was a hidden Markov model (see Figure 2) that produced sequences of  outputs - the equivalent of fast Fourier transform loads - fed as input to the receiver.  The receiver was a DN network which received as input, sequences of 10 outputs  from the emitter Markov model. The network's task was to guess the sequence  of hidden states of the emitter given the sequence of outputs from the emitter.  The DN outputs were interpreted as the inferred state of the emitter. Output unit  activations greater than 0.5 were evaluated as indicating that the emitter was in  state I at that particular time. Outputs smaller than 0.5 were evaluated as state  0. To achieve optimal performance in this task the network had to combine two  sources of information: top-down information about typical state transitions of the  emitter, and bottom up information about the likelihood of the hidden states of the  emitter given its responses.  The network was trained with rules 11 and 12 using the negative log joint prob-  ability of the DN input trajectory and the DN output trajectory as error signal.  This signal was calculated using the transition probabilities of the emitter hidden  Markov model and did not require knowledge of its actual state trajectories. The  necessary gradients for equations 11 and 12 were estimated using 1000 spontaneous  trajectories at each learning epoch. As Figure 3 shows the network started pro-  ducing unlikely trajectories but continuously improved. The figure also shows the  performance expected from an optimal classifier. As training progressed the network  approached optimal performance.  Acknowledgements  This work was funded through the NIMH grant MH47566 and a grant from the  Pittsburgh Supercomputer Center.  References  B. Apolloni, k D. de Falco. (1991) Learning by asymmetric parallel Boltzmann  machines. Neural Computation, 3,402-408.  R. Neal. (1992) Asymmetric Parallel Boltzmann Machines are Belief Networks,  Neural Computation, 4, 832-834.  J. Movellan k J. McClellan& (1992a) Learning continuous probability distributions  with symmetric diffusion networks. To appear in Cognitive Science.  T. Soon. (1973) Random Differential Equations in Science and Engineering, Aca-  demic Press, New York.  
Convergence of Stochastic Iterative  Dynamic Programming Algorithms  Tommi Jaakkola* Michael I. Jordan  Satinder P. Singh  Department of Brain and Cognitive Sciences  Massachusetts Institute of Technology  Cambridge, MA 92139  Abstract  Increasing attention has recently been paid to algorithms based on  dynamic programming (DP) due to the suitability of DP for learn-  ing problems involving control. In stochastic environments where  the system being controlled is only incompletely known, however,  a unifying theoretical account of these methods has been missing.  In this paper we relate DP-based learning algorithms to the pow-  erful techniques of stochastic approximation via a new convergence  theorem, enabling us to establish a class of convergent algorithms  to which both TD(A) and Q-learning belong.  I INTRODUCTION  Learning to predict the future and to find an optimal way of controlling it are the  basic goals of learning systems that interact with their environment. A variety of  algorithms are currently being studied for the purposes of prediction and control  in incompletely specified, stochastic environments. Here we consider learning algo-  rithms defined in Markov environments. There are actions or controls (u) available  for the learner that affect both the state transition probabilities, and the proba-  bility distribution for the immediate, state dependent costs (ci(u)) incurred by the  learner. Let pij(u) denote the probability of a transition to state j when control  u is executed in state i. The learning problem is to predict the expected cost of a  *E-mMI: tommi@psyche.mit.edu  703  704 Jaakkola, Jordan, and Singh  fixed policy tt (a function from states to actions), or to obtain the optimal policy  (it*) that minimizes the expected cost of interacting with the environment.  If the learner were allowed to know the transition probabilities as well as the imme-  diate costs the control problem could be solved directly by Dynamic Programming  (see e.g., Bertsekas, 1987). However, when the underlying system is only incom-  pletely known, algorithms such as Q-learning (Watkins, 1989) for prediction and  control, and TD(,) (Sutton, 1988) for prediction, are needed.  One of the central problems in developing a theoretical understanding of these  algorithms is to characterize their convergence; that is, to establish under what  conditions they are ultimately able to obtain correct predictions or optimal control  policies. The stochastic nature of these algorithms immediately suggests the use  of stochastic approximation theory to obtain the convergence results. However,  there exists no directly available stochastic approximation techniques for problems  involving the maximum norm that plays a crucial role in learning algorithms based  on DP.  In this paper, we extend Dvoretzky's (1956) formulation of the classical Robbins-  Munro (1951) stochastic approximation theory to obtain a class of converging pro-  cesses involving the maximum norm. In addition, we show that Q-learning and  both the on-line and batch versions of TD(A) are realizations of this new class.  This approach keeps the convergence proofs simple and does not rely on construc-  tions specific to particular algorithms. Several other authors have recently presented  results that are similar to those presented here: Dayan and Sejnowski (1993) for  TD(,), Peng and Williams (1993) for TD(,), and Tsitsiklis (1993) for Q-learning.  Our results appear to be closest to those of Tsitsiklis (1993).  2 Q-LEARNING  The Q-learning algorithm produces values--"Q-values"--by which an optimal ac-  tion can be determined at any state. The algorithm is based on DP by rewriting  Bellman's equation such that there is a value assigned to every state-action pair  instead of only to a state. Thus the Q-values satisfy  Q(s, u) = g,(u) + '7 y]p**,(u)mfixQ(s', u') (1)  where  denotes the mean of c. The solution to this equation can be obtained  by updating the Q-values iteratively; an approach known as the value iteration  method. In the learning problem the values for the mean of c and for the transition  probabilities are unknown. However, the observable quantity  c,(ut) + 7maxQ(st+l, u) (2)  where st and ut are the state of the system and the action taken at time t, respec-  tively, is an unbiased estimate of the update used in value iteration. The Q-learning  algorithm is a relaxation inethod that uses this estimate iteratively to update the  current Q-values (see below).  The Q-learning algorithm converges mainly due to the contraction property of the  value iteration operator.  Convergence of Stochastic Iterative Dynamic Programming Algorithms 705  2.1 CONVERGENCE OF Q-LEARNING  Our proof is based on the observation that the Q-learning algorithm can be viewed as  a stochastic process to which techniques of stochastic approximation are generally  applicable. Due to the lack of a formulation of stochastic approximation for the  maximum norm, however, we need to slightly extend the standard results. This is  accomplished by the following theorem the proof of which can be found in Jaakkola  et al. (1993).  Theorem I A random iterative process An+l(z) -- (1-an(X))An(X)-kfin(X)Fn(X)  converges to zero w.p.1 under the following assumptions:  1) The state space is finite.  2  2) Z an(X) -- , Zn an(X) <  E{/3n(x)lPn) _ E{an(x)lPn ) uniformly m.p.1.  3) II ;{Fn(x)lPn)IIw<_ - II/x, Ilw, where ' e (o, 1).  4) Var{Fn(x)lPn) _< C(I+ II an II) , where C is some constant.  Here Pn = {An, An-I,..., Fn-1,..., an-1,-.., fin-i,..-} stands for the past at step  n. Fn(x), an(X) and fin(X) are allowed to depend on the past insofar as the above  conditions remain valid. The notation I1' I1 refers to some weighted maximum  norm.  In applying the theorem, the An process will generally represent the difference  between a stochastic process of interest and some optimal value (e.g., the optimal  value function). The formulation of the theorem therefore requires knowledge to be  available about the optimal solution to the learning problem before it can be applied  to any algorithm whose convergence is to be verified. In the case of Q-learning the  required knowledge is available through the theory of DP and Bellman's equation  in particular.  The convergence of the Q-learning algorithm now follows easily by relating the  algorithm to the converging stochastic process defined by Theorem 1. I  Theorem 2 The Q-learning algorithm given by  qt+l(St, ut) = (1 -- at(st, ut))Qt(st, ut) + at(st, ut)[cs,(ut) + "Vt (St+l)]  converges to the optimal Q*(s, u) values if  I) The state and action spaces are finite.  2) Zt at(s, u): oc and Zt a2( s, u) < oo uniformly w.p.1.  3) Var{c,(u))is bounded.  We note that the theorem is more powerful than is needed to prove the convergence  of Q-learning. Its generality, however, allows it to be applied to other algorithms as well  (see the following section on TD()).  706 Jaakkola, Jordan, and Singh  3) If'/= 1, all policies lead to a cost free terminal state w.p.1.  Proof. By subtracting Q*(s, u) from both sides of the learning rule and by defining  At(s, u) - Qt(s, u) - Q*(s, u) together with  F(s, u) = c(u) + - q*(8, u) (3)  the Q-learning algorithm can be seen to have the form of the process in Theorem 1  with (s, u) = st(s, u).  To verify that Ft(s, u) has the required properties we begin by showing that it is a  contraction mapping with respect to some maximum norm. This is done by relating  Ft to the DP value iteration operator for the same Markov chain. More specifically,  maxlE{F(i,u)}l = '/maxlEpij(u)[(j) -- v*(j)]l  J  < '/axy.pi(u)maxIQt(j,v)- Q*(j,v)l  J  - '/maxZpij(u)V/x(j): T(V/X)(i)  where we have used the notation l/a(j) = maxv Iq(J, v)-q*(j, v)l and T is the DP  value iteration operator for the case where the costs associated with each state are  zero. If'/< 1 the contraction property of E{Ft(i, u)} can be obtained by bounding  j Pij(u)Va(j) by maxj Va(j) and then including the '/factor. When the future  costs are not discounted ('/= 1) but the chain is absorbing and all policies lead to  the terminal state w.p.1 there still exists a weighted maximum norm with respect  to which T is a contraction mapping (see e.g. Bertsekas & Tsitsiklis, 1989) thereby  forcing the contraction of E{Ft(i,u)}. The variance of Ft(s,u) given the past is  within the bounds of Theorem I as it depends on Or(s, u) at most linearly and the  variance of c(u) is bounded.  Note that the proof covers both the on-line and batch versions. []  3 THE TD(A) ALGORITHM  The TD(A) (Sutton, 1988) is also a DP-based learning algorithm that is naturally  defined in a Markov environment. Unlike Q-learning, however, TD does not involve  decision-making tasks but rather predictions about the future costs of an evolving  system. TD(A) converges to the same predictions as a version of Q-learning in which  there is only one action available at each state, but the algorithms are derived from  slightly different grounds and their behavioral differences are not well understood.  The algorithm is based on the estimates  x(i) = (1 - A) E/n-lvt(n)(i) (4)  n'--I  where ('*)(i) are n step look-ahead predictions. The expected values of the VX(i)  are strictly better estimates of the correct predictions than the (i)s are (see  Convergence of Stochastic Iterative Dynamic Programming Algorithms  707  Jaakkola et al., 1993) and the update equation of the algorithm  +i(it) = (it) + oq[)'(it) - (it)] (5)  can be written in a practical recursive form as is seen below. The convergence of  the algorithm is mainly due to the statistical properties of the x(i) estimates.  3.1 CONVERGENCE OF TD(A)  As we are interested in strong forms of convergence we need to impose some new  constraints, but due to the generality of the approach we can dispense with some  others. Specifically, the learning rate parameters ct,, are replaced by (x,,(i) which  satisfy ,, c,,(i) - oc and Yn c2(i) < cx> uniformly w.p.1. These parameters  allow asynchronous updating and they can, in general, be random variables. The  convergence of the algorithm is guaranteed by the following theorem which is an  application of Theorem 1.  Theorem 3 For any finite absorbing Markov chain, for any distribution of starting  states with no inaccessible states, and for any distributions of the costs with finite  variances the TD(A) algorithm given by  1)  m t  Vn+l (i) : V,(i) + c,(i) Z[ci, + 7Vn(it+l ) - Vn(it)] Z(TA)t-k Xi(k)  t:l k:l  E. = E. < uniformly  t  +z(i) = Vt(i) + (x(i)[ci, + 7(it+l) - (it)] E(TA) xi(k)  k--1  -.tct(i) : oc and -.,c(i) < oc uniformly w.p.1 and within sequences  ct(i)/rnaxtesct(i)--> 1 uniformly w.p.1.  converges to the optimal predictions w.p.1 provided 7, A 6 [0, 1] with 7A < 1.  Proof for (1):  previous section).  We use here a slightly different form for the learning rule (cf. the  ,(i)  1  E{rn(i)) Z V(i;k)  k=l  where V(i; k) is an estimate calculated at  sequence and for mathematical convenience  c,,(i) - E{m(i)}c,,(i), where re(i) is the  during the sequence.  re(i)  E{m(i)) V,, (i)]  the k tt occurrence of state i in a  we have made the transformation  number of times state i was visited  Vn+l(i) = Vn(i) q- (x,(i)[G,(i)  708 Jaakkola, Jordan, and Singh  To apply Theorem 1 we subtract V*(i), the optimal predictions, from both sides of  the learning equation. By identifying a,,(i) := o,,(i)m(i)/E{m(i)}, /3,,(i) := a,,(i),  and F,(i) :: G,(i)- V*(i)m(i)/E{m(i)} we need to show that these satisfy the  conditions of Theorem 1. For a,(i) and/3,,(i) this is obvious. We begin here by  showing that F,(i) indeed is a contraction mapping. To this end,  max IE{F.(i)I X/.)l-  i  1  mx I E{m(i)} E{(I/,x(i; 1)- l/*(i)) + (l/,x (i; 2)- l/*(i)) +... l l/,, }1  which can be bounded above by using the relation  IE{v(i; k)- v*(i) I  _< E{ IE{V(i;k)- V*(i)Ira(i) >_ k,V)10(m(i)-k) I  _< e{m(i) >_ k}lE{V(i)- v*(i) l v.}l  < 7P{m(i) >_ k} m.axlV,,(i)- V*(i)[  where 0(x) = 0 if x < 0 and 1 otherwise. Here we have also used the fact that V(i)  is a contraction mapping independent of possible discounting. As 5-}k P{m(i) >_  k} = E{m(i)} we finally get  maxlE{F,(i) l V.}l < 7 m.ax [V,,(i)- V*(i)l  i z  The variance of F,(i) can be seen to be bounded by  E{m4} m.xlV.(i)l 2 $  For any absorbing Markov chain the convergence to the terminal state is geometric  and thus for every finite k, E{rn k} <_ C(k), implying that the variance of F,(i) is  within the bounds of Theorem 1. As Theorem I is now applicable we can conclude  that the batch version of TD() converges to the optimal predictions w.p.1. []  Proof for (2) The proof for the on-line version is achieved by showing that the  effect of the on-line updating vanishes in the limit thereby forcing the two versions  to be equal asymptotically. We view the on-line version as a batch algorithm in  which the updates are made after each complete sequence but are made in such a  manner so as to be equal to those made on-line.  Define G',(i) = G,(i) + G(i) to be a new batch estimate taking into account the  on-line updating within sequences. Here G, (i) is the batch estimate with the desired  properties (see the proof for (1)) and G(i) is the difference between the two. We  take the new batch learning parameters to be the maxima over a sequence, that  is a,,(i) = maxres cq(i). As all the ct(i) satisfy the required conditions uniformly  w.p.1 these new learning parameters satisfy them as well.  To analyze the new batch algorithm we divide it into three parallel processes: the  batch TD(X) with a,(i) as learning rate parameters, the difference between this and  the new batch estimate, and the change in the value function due to the updates  made on-line. Under the conditions of the TD(X) convergence theorem rigorous  Convergence of Stochastic Iterative Dynamic Programming Algorithms 709  upper bounds can be derived for the latter two processes (see Jaakkola, et al.,  1993). These results enable us to write  II g{c'- V*)11   II E{G - V*)II + II C II   (-( + CZ)II V - V* II +C  where C' and C' go to zero with w.p.1. This implies that for any e > 0 and  I[ v - v* II>> e there exists 7 < 1 such that  for n large enough. This is the required contraction property of Theorem 1. In  addition, it can readily be checked that the variance of the new estimate falls under  the conditions of Theorem 1.  Theorem 1 now guarantees that for any e the value function in the on-line algorithm  converges w.p.1 into some e-bounded region of V* and therefore the algorithm itself  converges to V* w.p.1. []  4 CONCLUSIONS  In this paper we have extended results from stochastic approximation theory to  cover asynchronous relaxation processes which have a contraction property with  respect to some maximum norm (Theorem 1). This new class of converging iterative  processes is shown to include both the Q-learning and TD(A) algorithms in either  their on-line or batch versions. We note that the convergence of the on-line version  of TD(A) has not been shown previously. We also wish to emphasize the simplicity  of our results. The convergence proofs for Q-learning and TD(A) utilize only high-  level statistical properties of the estimates used in these algorithms and do not rely  on constructions specific to the algorithms. Our approach also sheds additional  light on the similarities between Q-learning and TD(A).  Although Theorem 1 is readily applicable to DP-based learning schemes, the theory  of Dynamic Programming is important only for its characterization of the optimal  solution and for a contraction property needed in applying the theorem. The theo-  rem can be applied to iterative algorithms of different types as well.  Finally we note that Theorem I can be extended to cover processes that do not show  the usual contraction property thereby increasing its applicability to algorithms of  possibly more practical importance.  References  Bertsekas, D. P. (1987). Dynamic Programming: Deterministic and Stochastic Mod-  els. Englewood Cliffs, NJ' Prentice-Hall.  Bertsekas, D. P., & Tsitsiklis, J. N. (1989). Parallel and Distributed Computation:  Numerical Methods. Englewood Cliffs, N J: Prentice-Hall.  Dayan, P. (1992). The convergence of TD(A) for general h. Machine Learning, 8,  341-362.  710 Jaakkola, Jordan, and Singh  Dayan, P., & Sejnowski, T. J. (1993). TD(A) converges with probability 1. CNL,  The Salk Institute, San Diego, CA.  Dvoretzky, A. (1956). On stochastic approximation. Proceedings of the Third Berke-  ley Symposium on Mathematical Statistics and Probability. University of California  Press.  Jaakkola, T., Jordan, M. I., & Singh, S. P. (1993). On the convergence of stochastic  iterative dynamic programming algorithms. Submitted to Neural Computation.  Peng J., & Williams R. J. (1993). TD(A) converges with probability 1. Department  of Computer Science preprint, Northeastern University.  Robbins, H., & Monro, S. (1951). A stochastic approximation model. Annals of  Mathematical Statistics, 22, 400-407.  Sutton, R. S. (1988). Learning to predict by the methods of temporal differences.  Machine Learning, 3, 9-44.  Tsitsiklis J. N. (1993). Asynchronous stochastic approximation and Q-learning.  Submitted to: Machine Learning.  Watkins, C.J.C.H. (1989). Learning from delayed rewards. PhD Thesis, University  of Cambridge, England.  Watkins, C.J.C.H, & Dayan, P. (1992). Q-learning. Machine Learning, 8, 279-292.  
Non-Intrusive Gaze Tracking Using Artificial  Neural Networks  Shumeet Baluja  baluja@cs.cmu.edu  School of Computer Science  Carnegie Mellon University  Pittsburgh, PA 15213  Dean Pomerleau  pomerleau@cs.cmu.edu  School of Computer Science  Carnegie Mellon University  Pittsburgh, PA 15213  Abstract  We have developed an artificial neural network based gaze tracking system  which can be customized to individual users. Unlike other gaze trackers,  which normally require the user to wear cumbersome headgear, or to use a  chin rest to ensure head immobility, our system is entirely non-intrusive.  Currently, the best intrusive gaze tracking systems are accurate to approxi-  mately 0.75 degrees. In our experiments, we have been able to achieve an  accuracy of 1.5 degrees, while allowing head mobility. In this paper we  present an empirical analysis of the performance of a large number of artifi-  cial neural network architectures for this task.  1 INTRODUCTION  The goal of gaze tracking is to determine where a subject is looking from the appearance  of the subject's eye. The interest in gaze tracking exists because of the large number of  potential applications. Three of the most common uses of a gaze tracker are as an alterna-  tive to the mouse as an input modality [Ware & Mikaelian, 1987], as an analysis tool for  human-computer interaction (HCI) studies [Nodine et. al, 1992], and as an aid for the  handicapped [Ware & Mikaelian, 1987].  Viewed in the context of machine vision, successful gaze tracking requires techniques to  handle imprecise data, noisy images, and a potentially infinitely large image set. The most  accurate gaze tracking has come from intrusive systems. These systems either use devices  such as chin rests to restrict head motion, or require the user to wear cumbersome equip-  ment, ranging from special contact lenses to a camera placed on the user's head. The sys-  tem described here attempts to perform non-intrusive gaze tracking, in which the user is  neither required to wear any special equipment, nor required to keep his/her head still.  753  754 Baluja and Pomerleau  2 GAZE TRACKING  2.1 TRADITIONAL GAZE TRACKING  In standard gaze trackers, an image of the eye is processed in three basic steps. First, the  specular reflection of a stationary light source is found in the eye's image. Second, the  pupil's center is found. Finally, the relative position of the light's reflection to the pupil's  center is calculated. The gaze direction is determined from information about the relative  positions, as shown in Figure 1. In many of the current gaze tracker systems, the user is  required to remain motionless, or wear special headgear to maintain a constant offset  between the position of the camera and the eye.  Specular  Reflection  Looking at Looking Above Looking Below Looking Left of  Light Light Light Light  Figure 1: Relative position of specular reflection and pupil. This diagram assumes that  the light is placed in the same location as the observer (or camera).  2.2 ARTIFICIAL NEURAL NETWORK BASED GAZE TRACKING  One of the primary benefits of an artificial neural network based gaze tracker is that it is  non-intrusive; the user is allowed to move his head freely. In order to account for the shifts  in the relative positions of the camera and the eye, the eye must be located in each image  frame. In the current system, the right eye is located by searching for the specular reflec-  tion of a stationary light in the image of the user's face. This can usually be distinguished  by a small bright region surrounded by a very dark region. The reflection's location is used  to limit the search for the eye in the next frame. A window surrounding the reflection is  extracted; the image of the eye is located within this window.  To determine the coordinates of the point the user is looking at, the pixels of the extracted  window are used as the inputs to the artificial neural network. The forward pass is simu-  lated in the ANN, and the coordinates of the gaze are determined by reading the output  units. The output units are organized with 50 output units for specifying the X coordinate,  and 50 units for the Y coordinate. A gaussian output representation, similar to that used in  the ALVINN autonomous road following system [Pomerleau, 1993], is used for the X and  Y axis output units. Gaussian encoding represents the network's response by a Gaussian  shaped activation peak in a vector of output units. The position of the peak within the vec-  tor represents the gaze location along either the X or Y axis. The number of hidden units  and the structure of the hidden layer necessary for this task are explored in section 3.  The training data is collected by instructing the user to visually track a moving cursor. The  cursor moves in a predefined path. The image of the eye is digitized, and paired with the  (X,Y) coordinates of the cursor. A total of 2000 image/position pairs are gathered. All of  the networks described in this paper are trained with the same parameters for 260 epochs,  using standard error back propagation. The training procedure is described in greater  Non-Intrusive Gaze Tracking Using Artificial Neural Networks 755  detail in the next section.  3 THE ARTIFICIAL NEURAL NETWORK IMPLEMENTATION  In designing a gaze tracker, the most important attributes are accuracy and speed. The  need for balancing these attributes arises in deciding the number of connections in the  ANN, the number of hidden units needed, and the resolution of the input image. This sec-  tion describes several architectures tested, and their respective performances.  3.1 EXAMINING ONLY THE PUPIL AND CORNEA  Many of the traditional gaze trackers look only at a high resolution picture of the subject's  pupil and cornea. Although we use low resolution images, our first attempt also only used  an image of the pupil and cornea as the input to the ANN. Some typical input images are  shown below, in Figure 2(a). The size of the images is 15xl 5 pixels. The ANN architec-  ture used is shown in Figure 2(b). This architecture was used with varying numbers of hid-  den units in the single, divided, hidden layer; experiments with 10, 16 and 20 hidden units  were performed.  As mentioned before, 2000 image/position pairs were gathered for training. The cursor  automatically moved in a zig-zag motion horizontally across the screen, while the user  visually tracked the cursor. In addition, 2000 image/position pairs were also gathered for  testing. These pairs were gathered while the user tracked the cursor as it followed a verti-  cal zig-zag path across the screen. The results reported in this paper, unless noted other-  wise, were all measured on the 2000 testing points. The results for training the ANN on  the three architectures mentioned above as a function of epochs is shown in Figure 3. Each  line in Figure 3 represents the average of three ANN training trials (with random initial  weights) for each of the two users tested.  Using this system, we were able to reduce the average error to approximately 2.1 degrees,  which corresponds to 0.6 inches at a comfortable sitting distance of approximately 17  inches. In addition to these initial attempts, we have also attempted to use the position of  the cornea within the eye socket to aid in making finer discriminations. These experiments  are described in the next section.  50 X Output Units  50 Y Output Units  15 x 15  Input  Retina  Figure 2: (a-left) 15 x 15 Input to the ANN. Target outputs also shown. (b-right)  the ANN architecture used. A single divided hidden layer is used.  756 Baluja and Pomerleau  Figure 3: Error vs. Epochs for the 15x15  images. Errors shown for the 2000  image test set. Each line represents  three ANN trainings per user; two  users are tested.  15xl5 Images  i i i p  3.2 USING THE EYE SOCKET FOR ADDITIONAL INFORMATION  20 Hidden  Epochs  In addition to using the information present from the pupil and cornea, it is possible to  gain information about the subject's gaze by analyzing the position of the pupil and cornea  within the eye socket. Two sets of experiments were performed using the expanded eye  image. The first set used the network described in the next section. The second set of  experiments used the same architecture shown in Figure 2(b), with a larger input image  size. A sample image used for training is shown below, in Figure 4.  Figure 4: Image of the pupil and the  eye socket, and the corresponding  target outputs. 15 x 40 input image  shown.  3.2.1. Using a Single Continuous Hidden Layer  One of the remaining issues in creating the ANN to be used for analyzing the position of  the gaze is the structure of the hidden unit layer. In this study, we have limited our explo-  ration of ANN architectures to simple 3 layer feed-forward networks. In the previous  architecture (using 15 x 15 images) the hidden layer was divided into 2 separate parts, one  for predicting the x-axis, and the other for the y-axis. Selecting this architecture over a  fully connected hidden layer makes the assumption that the features needed for accurate  prediction of the x-axis are not related to the features needed for predicting the y-axis. In  this section, this assumption is tested. This section explores a network architecture in  which the hidden layer is fully connected to the inputs and the outputs.  In addition to deciding the architecture of the ANN, ii is necessary to decide on the size of  the input images. Several input sizes were attempted, 15x30, 15x40 and 20x40. Surpris-  ingly, the 20x40 input image did not provide the most accuracy. Rather, it was the 15x40  image which gave the best results. Figure 5 provides two charts showing the performance  of the 15x40 and 20x40 image sizes as a function of the number of hidden units and  epochs. The 15x30 graph is not shown due to space restrictions, it can be found in [Baluja  & Pomerleau, 1994]. The accuracy achieved by using the eye socket information, for the  15x40 input images, is better than using only the pupil and cornea; in particular, the 15x40  input retina worked better than both the 15x30 and 20x40.  Non-Intrusive Gaze Tracking Using Artificial Neural Networks 757  Error-Do  360_  34o  3 20  3o11  2 80  26o  240  22o  2o11  15 x401mage  I I  10 Hidden  16 Hidden  20 Hidden  Erro-Cre 20 x 40 Image  I I I  37O  360  3 50  340  3 30  320  300  290  2 80  I 10 Hidden  16 Hidden  i 20 Hidden  Figure 5: Performance of 15x40, and 20x40 input image sizes as a function of  epochs and number of hidden units. Each line is the average of 3 runs. Data  points taken every 20 epochs, between 20 and 260 epochs.  3.2.2. Using a Divided Hidden Layer  The final set of experiments which were performed were with 15x40 input images and 3  different hidden unit architectures: 5x2, 8x2 and 10x2. The hidden unit layer was divided  in the manner described in the first network, shown in Figure 2(b). Two experiments were  performed, with the only difference between experiments being the selection of training  and testing images. The first experiment was similar to the experiments described previ-  ously. The training and testing images were collected in two different sessions, one in  which the user visually tracked the cursor as it moved horizontally across the screen and  the other in which the cursor moved vertically across the screen. The training of the ANN  was on the "horizontally" collected images, and the testing of the network was on the  "vertically" collected images. In the second experiment, a random sample of 1000 images  from the horizontally collected images and a random sample of 1000 vertically collected  images were used as the training set. The remaining 2000 images from both sets were used  as the testing set. The second method yielded reduced tracking errors. If the images from  only one session were used, the network was not trained to accurately predict gaze posi-  tion independently of head position. As the two sets of data were collected in two separate  sessions, the head positions from one session to the other would have changed slightly.  Therefore, using both sets should have helped the network in two ways. First, the presen-  tation of different head positions and different head movements should have improved the  ability of the network to generalize. Secondly, the network was tested on images which  were gathered from the same sessions as it was trained. The use of mixed training and test-  ing sets will be explored in more detail in section 3.2.3.  The results of the first and second experiments are presented here, see Figure 6. In order to  compare this architecture with the previous architectures mentioned, it should be noted  that the performance of this architecture, with 10 hidden units, more accurately predicted  gaze location than the architecture mentioned in section 3.2.1, in which a single continu-  ous hidden layer was used. In comparing the performance of the architectures with 16 and  20 hidden units, the performances were very similar. Another valuable feature of using the  758 Baluja and Pomerleau  divided hidden layer is the reduced number of connections decreases the training and sim-  ulation times. This architecture operates at approximately 15hz. with 10 and 16 hidden  units, and slightly slower with 20 hidden units.  Error-Djm Separate Hidden Layer & 15x40 Image - Test Set 1 Error-Drs Seperate Hidden Layer & 15x40 Images - Test Set 2  3 io I I I i I : sc I [ I I t 10 Hidden  I!t 10 Hidden ', ..........  30o ..........  16 Hidden  16 Hidden ................  2o .................... 0 20 Hidden  20 Hidden  280 220  Z70 210  I.o Epochs ! o I I I  [ I Epochs  Figure 6: (Left) The average of 2 users with the 15x40 images, and a divided hidden  layer architecture, using test setup #1. (Right) The average performance tested on  5 users, with test setup #2. Each line represents the average of three ANN  trainings per user per hidden unit architecture.  3.2.3. Mixed Training and Testing Sets  It was hypothesized, above, that there are two reasons for the improved performance of a  mixed training and testing set. First, the network ability to generalize is improved, as it is  trained with more than a single head position. Second, the network is tested on images  which are similar, with respect to head position, as those on which it was trained. In this  section, the first hypothesized benefit is examined in greater detail using the experiments  described below.  Four sets of 2000 images were collected. In each set, the user had a different head position  with respect to the camera. The first two sets were collected as previously described. The  first set of 2000 images (horizontal train set 1) was collected by visually tracking the cur-  sor as it made a horizontal path across the screen. The second set (vertical test set 1) was  collected by visually tracking the cursor as it moved in a vertical path across the screen.  For the third and fourth image sets, the camera was moved, and the user was seated in a  different location with respect to the screen than during the collection of the first training  and testing sets. The third set (horizontal train set 2) was again gathered from tracking the  cursor's horizontal path, while the fourth (vertical test set 2) was from the vertical path of  the cursor.  Three tests were performed. In the first test, the ANN was trained using only the 2000  images in horizontal training set 1. In the second test, the network was trained using the  2000 images in horizontal training set 2. In the third test, the network was trained with a  random selection of 1000 images from horizontal training set 1, and a random selection of  1000 images of horizontal training set 2. The performance of these networks was tested on  both of the vertical test sets. The results are reported below, in Figure 7. The last experi-  ment, in which samples were taken from both training sets, provides more accurate results  Non-Intrusive Gaze Tracking Using Artificial Neural Networks 759  when testing on vertical test set 1, than the network trained alone on horizontal training set  1. When testing on vertical test set 2, the combined network performs almost as well as the  network trained only on horizontal training set 2.  These three experiments provide evidence for the network's increased ability to generalize  if sets of images which contain multiple head positions are used for training. These exper-  iments also show the sensitivity of the gaze tracker to movements in the camera; if the  camera is moved between training and testing, the errors in simulation will be large.  3O0  28O  2611  24O  2 20  2O0  Vertical Test Set I Vertical Test Set 2  combined  train set 1  train set 2  I I I I  32O  3OO  2O0  I combin  trn tl  trn set2  Figure 7: Comparing the performance between networks trained with only one head  position (horizontal train set 1 & 2), and a network trained with both.  4 USING THE GAZE TRACKER  The experiments described to this point have used static test sets which are gathered over  a period of several minutes, and then stored for repeated use. Using the same test set has  been valuable in gauging the performance of different ANN architectures. However, a  useful gaze tracker must produce accurate on-line estimates of gaze location. The use of  an "offset table" can increase the accuracy of on-line gaze prediction. The offset table is a  table of corrections to the output made by a gaze tracker. The network's gaze predictions  for each image are hashed into the 2D offset-table, which performs an additive correction  to the network's prediction. The offset table is filled after the network is fully trained. The  user manually moves and visually tracks the cursor to regions in which the ANN is not  performing accurately. The offset table is updated by subtracting the predicted position of  the cursor from the actual position. This procedure can also be automated, with the cursor  moving in a similar manner to the procedure used for gathering testing and training  images. However, manually moving the cursor can help to concentrate effort on areas  where the ANN is not performing well; thereby reducing the time required for offset table  creation.  With the use of the offset table, the current system works at approximately 15 hz. The best  on-line accuracy we have achieved is 1.5 degrees. Although we have not yet matched the  best gaze tracking systems, which have achieved approximately 0.75 degree accuracy, our  system is non-intrusive, and does not require the expensive hardware which many other  systems require. We have used the gaze tracker in several forms; we have used it as an  760 Baluja and Pomerleau  input modality to replace the mouse, as a method of selecting windows in an X-Window  environment, and as a tool to report gaze direction, for human-computer interaction stud-  ies.  The gaze tracker is currently trained for 260 epochs, using standard back propagation.  Training the 8x2 hidden layer network using the 15x40 input retina, with 2000 images,  takes approximately 30-40 minutes on a Sun SPARC 10 machine.  5 CONCLUSIONS  We have created a non-intrusive gaze tracking system which is based upon a simple ANN.  Unlike other gaze-tracking systems which employ more traditional vision techniques,  such as a edge detection and circle fitting, this system develops its own features for suc-  cessfully completing the task. The system's average on-line accuracy is 1.7 degrees. It has  successfully been used in HCI studies and as an input device. Potential extensions to the  system, to achieve head-position and user independence, are presented in [Baluja &  Pomerleau, 1994].  Acknowledgments  The authors would like to gratefully acknowledge the help of Kaari Flagstad, Tammy  Carter, Greg Nelson, and Ulrike Harke for letting us scrutinize their eyes, and being "will-  ing" subjects. Profuse thanks are also due to Henry Rowley for aid in revising this paper.  Shumeet Baluja is supported by a National Science Foundation Graduate Fellowship. This  research was supported by the Department of the Navy, Office of Naval Research under  Grant No. N00014-93-1-0806. The views and conclusions contained in this document are  those of the authors and should not be interpreted as representing the official policies,  either expressed or implied, of the National Science Foundation, ONR, or the U.S. gov-  ernment.  References  Baluja, S. Pomerleau, D.A. (1994) "Non-Intrusive Gaze Tracking Using Artificial Neural Networks"  CMU-CS-94.  Jochem, T.M., D.A. Pomerleau, C.E. Thorpe (1993), "MANIAC: A Next Generation Neurally  Based Autonomous Road Follower". In Proceedings of the International Conference on Intelligent  Autonomous Systems (IAS-3).  Nodine, C.F., H.L. Kundel, L.C. Toto & E.A. Krupinksi (1992) "Recording and analyzing eye-posi-  tion data using a microcomputer workstation", Behavior Research Methods, Instruments & Comput-  ers 24 (3) 475-584.  Pomerleau, D.A. (1991) "Efficient Training of Artificial Neural Networks for Autonomous Naviga-  tion," Neural Computation 3:1, Terrence Sejnowski (Ed).  Pomerleau, D.A. (1993) Neural Network Perception for Mobile Robot Guidance. Kluwer Academic  Publishing.  Pomerleau, D.A. (1993) "Input Reconstruction Reliability Estimation", Neural Information Pro-  cessing Systems 5. Hanson, Cowan, Giles (eds.) Morgan Kaufmann, pp. 270-286.  Starker, I. & R. Bolt (1990) "A Gaze-Responsive Self Disclosing Display", In CHI-90. Addison  Wesley, Seattle, Washington.  Waibel, A., Sawai, H. & Shikano, K. (1990) "Consonant Recognition by Modular Construction of  Large Phonemic Time-Delay Neural Networks". Readings in Speech Recognition. Waibel and Lee.  Ware, C. & Mikaelian, H. (1987) "An Evaluation of an Eye Tracker as a Device for Computer  Input", In J. Carrol and P. Tanner (ed.) Human Factors in Computing Systems - IV. Elsevier.  
Recognition-based Segmentation of  On-line Cursive Handwriting  Nicholas S. Flann  Department of Computer Science  Utah State University  Logan, UT 84322-4205  ilamnil. as. usu. edu  Abstract  This paper introduces a new recognition-based segmentation ap-  proach to recognizing on-line cursive handwriting from a database  of 10,000 English words. The original input stream of a:, y pen coor-  dinates is encoded as a sequence of uniform stroke descriptions that  are processed by six feed-forward neural-networks, each designed  to recognize letters of different sizes. Words are then recognized by  performing best-first search over the space of all possible segmen-  tations. Results demonstrate that the method is effective at both  writer dependent recognition (1.7% to 115.15% error rate) and writer  independent recognition (15.2% to 31.1% error rate).  I Introduction  With the advent of pen-based computers, the problem of automatically recognizing  handwriting from the motions of a pen has gained much significance. Progress has  been made in reading disjoint block letters [Weissman el. al, 93]. However, cursive  writing is much quicker and natural for humans, but poses a significant challenge to  pattern recognition systems because of its variability, ambiguity and need to both  segment and recognize the individual letters. Recent techniques employing self-  organizing networks are described in [Morasso el. al, 93] and [Schomaker, 1993].  This paper presents an alternative approach based on feed-forward networks.  On-line handwriting consists of writing with a pen on a touch-terminal or digitizing  777  778 Flann  /  (a)  (,)  Figure 1: The five principal stages of preprocessing: (a) The original data, z,//  values sampled every 10rnS. (b) The slant is normalized through a shear transfor-  mation; (c) Stroke boundaries are determined at points where t velocity equals 0 or  pen-up or pen-down events occur; (d) Delayed strokes are reordered and associated  with corresponding strokes of the same letters; (e) Each stroke is resampled in space  to correspond to exactly 8 points. Note pen-down strokes are shown as thick lines,  pen-up strokes as thin lines.  Recognition-Based Segmentation of On-Line Cursive Handwriting 779  tablet. The device produces a regular stream of x, y coordinates, describing the  positions of the pen while writing. Hence the problem of recognizing on-line cur-  sively written words is one of mapping a variable length sequence of x, y coordinates  to a variable length sequence of letters. Developing a system that can accurately  perform this mapping faces two principal problems: First, because handwriting is  done with little regularity in speed, there is unavoidable variability in input size.  Second, because no pen-up events or spatial gaps signal the end of one letter and the  beginning of the next, the system must perform both segmentation and recognition.  This second problem necessitates the development of a recognition-based segmenta-  tion approach. In [Schenkel et al., 93] one such approach is described for connected  block letter recognition where the system learns to recognize segmentation points.  In this paper an alternative method is presented that first performs exhaustive  recognition then searches the space of possible segmentations. The remainder of  the paper describes the method in more detail and presents results that demon-  strate its effectiveness at recognizing a variety of cursive handwriting styles.  2 Methodology  The recognition system consists of three subsystems: (a) the preprocessor that maps  the initial stream of x, y coordinates to a stream of stroke descriptions; (b) the letter  classifier that learns to recognize individual letters of different size; and (c) the word  finder that performs recognition-based segmentation over the output of the letter  classifier to identify the most likely word written.  2.1 Preprocesslng  The preprocessing stage follows steps outlined in [Guerfali &; Plamondon, 93] and  is illustrated in Figure 1. First the original data is smoothed by passing it through  a low-pass filter, then reslanted to make the major stroke directions vertical. This  is achieved by computing the mean angle of all the individual lines then applying  a shear transformation to remove it. Second, the strokes boundaries are identified  as points when 9 = 0 or when the pen is picked up or put down. Zero y velocity  was chosen rather than minimum absolute velocity [Morasso et. al, 93] since it was  found to be more robust. Third, delayed strokes such as those that dot an i or cross  a t are reordered to be associated with their corresponding letter. Here the delayed  stroke is placed to immediately follow the closest down stroke and linked into the  stroke sequence by straight line pen-up strokes. Fourth, each stroke is resampled in  the space domain (using linear interpolation) so as to represent it as exactly eight  x, y coordinates. Finally the new stream of x, y coordinates is converted to a stream  of 14 feature values.  Eight of these features are similar to those used in [Weissman et. al, 93], and repre-  sent the angular acceleration (as the sin and cos of the angle), the angular velocity  of the line (as the sin and cos of the angle), the x, y coordinates (x has a linear  ramp removed), and first differential x, y. One feature denotes whether the pen  was down or up when the line was drawn. The remaining features encode more  abstract information about the stroke.  780 Flann  32  14  32  Figure 2: The pyramid-style architecture of the network used to recognize 2 stroke  letters. The input size is 32 x 14; 32 is from the 4 input strokes (each represented by  8 resampled points), two central strokes from the letter and the 2 context strokes,  one each side; 14 is from the number of features employed to represent each point.  Not all the receptive fields are shown. The first hidden layer consists of 7 fields,  4 over each stroke and 3 more spanning the stroke boundaries. The next hidden  layer consists of 5 fields, each spanning 3 x 20 inputs. The output is a 32 bit  error-correcting code.  Figure 3' Examples of the class "other" for stroke sizes i though 6. Each letter is  a random fragment of a word, such that it is not an alphabetic letter.  Recognition-Based Segmentation of On-Line Cursive Handwriting 781  2.2 Letter Recognition  The letter classifier consists of six separate pyramid-style neural-networks, each  with an architecture suitable for recognizing a letter of one through six strokes.  A neural network designed to recognize letters of size j strokes encodes a map-  ping from a sequence of j + 9. stroke descriptions to a 32 bit error-correcting code  [Dietterich & Bakiri, 91]. Experiments have shown this use of a context window  improves performance, since the allograph of the current letter is dependent on the  allographs of the previous and following letters. The network architecture for stroke  size two is illustrated in Figure 2. The architecture is similar to a time-delayed  neural-network [Lang & Waibel, 90] in that the hierarchical structure enables dif-  ferent levels of abstract features to be learned. However, the individual receptive  fields are not shared as in a TDNN, since translational variance is not problem and  the sequence of data is important.  The networks are trained using 80% of the raw data collected. This set is further  divided into a training and a verification set. All training and verification data is  preprocessed and hand segmented, via a graphical interface, into letter samples.  These are then sorted according to size and assembled into distinct training and  verification sets. It is often the case that the same letter will appear in multiple  size files due to variability in writing and different contexts (such as when an o is  followed by a 9 it is at least a 3 stroke allograph, while an o followed by an l is  usually only a two stroke allograph). Included in these letter samples are samples  of a new letter class "other," illustrated in Figure 3. Experiments demonstrated  that use of an "other" class tightens decision boundaries and thus prevents spurious  fragments--of which there are many during performance from being recognized as  real letters. Each network is trained using back-propagation until correctness on  the verification set is maximized, usually requiring less than 100 epochs.  2.3 Word Interpreter  To identify the correct word, the word interpreter explores the space of all possible  segmentations of the input stroke sequence. First, the input sequence is partitioned  into all possible fragments of size one through six, then the appropriately sized  network is used to classify each fragment. An example of this process is illustrated  as a matrix in Figure 4(a).  The word interpreter then performs a search of this matrix to identify candidate  words. Figure 4(b) and Figure 4(c) illustrates two sets of candidate words found  for the example in Figure 4(a). Candidates in this search process are generated  according to the following constraints:  A legal segmentation point of the input stream is one where no two adja-  cent fragments overlap or leave a gap. To impose this constraint the i'th  fragment of size j may be extended by all of the i + j fragments, if they  exist.  A legal candidate letter sequence must be a subsequence of a word in the  provided lexicon of expected English words.  782 Flann  (a)  4CVRE  Figure 4: (a) The matrix of fragments and their classifications that is generated by  applying the letter recognizers to a sample of the word are. The original handwriting  sample, following preprocessing, is given at the top of the matrix. The bottom row  of the matrix corresponds to all fragments of size one (with zero overlap), the second  row to all fragments of size two (with an overlap of one stroke) etc. The column  of letters in each fragment box represents the letter classifications generated by  the neural network of appropriate size. The higher the letter in the column, the  more confident the classification. Those fragments with no high scoring letter were  recognized as examples of the class "other." (b) The first five candidates found by  the word recognizer employing no lexicon. The first column is the word recognized,  the second column is the score for that word, the third is the sequence of fragments  and their classifications. (c) The first five candidates found by the word recognizer  employing a lexicon of 10748 words.  Recognition-Based Segmentation of On-Line Cursive Handwriting 783  In a forward search, a candidate of size n consists of: (a) a legal sequence of frag-  ments fl, f2,..., fn that form a prefix of the input stroke sequence, (b) a sequence  of letters Ii, 12,..., In that form a prefix of an English word from the given dictio-  nary and (c) a score s for this candidate, defined as the average letter recognition  error:  I,)  s--  where $(fi, li) is the hamming distance between letter li's code and the actual code  produced by the neural network when given fi as input. This scoring function is  the same as employed in [Edelman el. al, 90].  The best word candidate is one that conforms with the constraints and has the  lowest score. Although this is a reasonable scoring function, it is easy to show  that it is not admissible when used as an evaluation function in forward search.  With a forward search, problems arise when the prefix of the correct word is poorly  recognized. To help combat this problem without greatly increasing the size of the  search space, both forward and backward search is performed.  Search is initiated by first generating all one letter and one fragment prefix and suffix  candidates. Then at each step in the search, the candidate with the lowest score is  expanded by considering the cross product of all legal letter extensions (according to  the lexicon) with all legal fragment extensions (according to the fragment-sequence  constraints). The list of candidates is maintained as a heap for efficiency. The search  process terminates when the best candidate satisfies: (1) the letter sequence is a  complete word in the lexicon and (2) the fragment sequence uses all the available  input strokes.  The result of this bi-directional search process is illustrated in Figure 4(a)(b), where  the five best candidates found are given for no lexicon and a large lexicon. The use  of a 10,748 word lexicon eliminates meaningless fragment sequences, such as cvre,  which is a reasonable segmentation, but not in the lexicon. The first two candidates  are the same fragment sequence, found by the two search directions. The third  candidate with a 10,748 word dictionary illustrates an alternative segmentation of  the correct word. This candidate was identified by a backward search, but not a  forward search, due to the poor recognition of the first fragment.  3 Evaluation  To evaluate the system, 10 writers have provided samples of approximately 100  words picked by a random process, biased to better represent uncommon letters.  Two kinds of experiments were performed. First, to test the ability of the system to  learn a variety of writing styles, the system was tested and trained on distinct sets  of samples from the same writer. This experiment was repeated 10 times, once for  each writer. The error rate varied between 1.7% and 15.5%, with a mean of 6.2%,  when employing a database of 10,748 English words. The second experiments tested  the ability of the system to recognize handwriting of a writer not represented in the  training set. Here the set of 10 samples were split into two sets, the training set  of 9 writers with the remaining i writer being the test set. The error rate was  understandably higher, varying between 5.2% and 31.1%, with a mean of 10.8%,  when employing a database of 10,748 English words.  784 Flann  4 Summary  This paper has presented a recognition-based segmentation approach for on-line  cursive handwriting. The method is very flexible because segmentation is performed  following exhaustive recognition. Hence, we expect the method to be successful  with more natural unconstrained writing, which can include mixed block, curslye  and disjoint letters, diverse orderings of delayed strokes, overwrites and erasures.  Acknowledgement s  This work was supported by a Utah State University Faculty Grant. Thanks to  Balaji Allamapatti, Rebecca Rude and Prashanth G Bilagi for code development.  References  [Dietterich & Bakiri, 91] Dietterich, T., G. & Bakiri, G. (1991). Error correcting  output codes: A general method for improving multiclass inductive learning  programs, in Proceedings of the Ninth National Conference on Artificial  Intelligence, Vol 2, pp 572-577.  [Edelman et. al, 90] Edelman S., Tamar F., and Ullman S. (1990). Reading cursive  handwriting by alignment of letter prototypes. International Journal o*f  Computer Vision, 5:3,303-331.  [Guerfali & rlamondon, 93] Guerfali W. & rlamondon R. (1993). Normalizing and  restoring on-line handwriting. Pattern Recognition, Vol. 26, No. 3, pp. 419-  431.  [Guyon et. al, 90] Guyon I., Albrecht P., Le Cun Y., Denker J. & Hubbard W.  (1991). Design of a neural network character recognizer for a touch terminal.  Pattern Recognition, Vol. 24, No. 2. pp. 105-119.  [Lang & Waibel, 90] Lang K., J. & Waibel A., H. (1990). A time-delayed neural  network architecture for isolated word recognition, Neural Networks, Vol 3,  pp 33-43.  [Morasso et. al, 93] Morasso P., Barberis, S. Pagliano S. & VetganG, D. (1993).  Recognition experiments of cursive dynamic handwriting with self-  organizing networks. Pattern Recognition, Vol. 26, No. 3, pp. 451-460.  [Schenkel et al., 93] Schenkel M., Weissman H., Guyon I., Nohl C., & Henderson D.  (1993). Recognition-based segmentation of on-line hand-printed words. In  S. J. Hanson, J. D. Cowan & C. L. Giles (Eds), Advances in Neural In.forma-  tion Processing Systems, 5, 723-730. San Mateo, CA: Morgan Kaufmann.  [Schomaker, 1993] Schomaker L. (1993). Using stroke or character based self-  organizing maps in the recognition of on-line connected cursive script. Pat-  tern Recognition, Vol. 26. No. 3., pp. 442-450.  [Srihari & Bozinovic, 87] Srihari S. N. & Bozinovic R. M. (1987). A multi-level  perception approach to reading cursive script. Artificial Intelligence, 33  217-255.  [Weissman et. al, 93] Weissman H., Schenkel M., Guyon I., Nohl C. & Henderson  D. (1993). Recognition-based segmentation of on-line run-on hand printed  words: input vs. output segmentation. Pattern Recognition.  
Address Block Location with a Neural Net System  Hans Peter Graf  AT&T Bell Laboratories  Crawfords Comer Road  Holmdel, NJ 07733, USA  Eric Cosatto  Abstract  We developed a system for finding address blocks on mail pieces that can  process four images per second. Besides locating the address block, our  system also determines the writing style, handwritten or machine printed, and  moreover, it measures the skew angle of the text lines and cleans noisy  images. A layout analysis of all the elements present in the image is  performed in order to distinguish drawings and dirt from text and to separate  text of advertisement from that of the destination address.  A speed of more than four images per second is obtained on a modular  hardware platform, containing a board with two of the NET32K neural net  chips, a SPARC2 processor board, and a board with 2 digital signal  processors. The system has been tested with more than 100,000 images. Its  performance depends on the quality of the images, and lies between 85%  correct location in vej noisy images to over 98% in cleaner images.  1 INTRODUCTION  The system described here has been integrated into an address reading machine developed for  the 'Remote Computer Reader' project of the United States Postal Service. While the actual  reading of the text is done by other modules, this system solves one of the major problems,  namely, finding reliably the location of the destination address. There are only a few constraints  on how and where an address has to be written, hence they may appear in a wide variety of  styles and layouts. Often an envelope contains advertising that includes images as well as text.  785  786 Graf and Cosatto  Sometimes, dirt covers part of the envelope image, including the destination address. Moreover,  the image captured by the camera is thresholded and the reader is given a binary image. This  binahzation process introduces additional distortions; in particular, often the destination address  is surrounded by a heavy texture. The high complexity of the images and their poor quality make  it difficult to find the location of the destination address, requiring an analysis of all the elements  present in the image. Such an analysis is compute-intensive and in our system it turned out to  be the major bottleneck for a fast throughput. In fact, finding the address requires much more  computation than reading it. Special-purpose hardware in the form of the NET32K neural net  chips (Graf, Henderson, 90) is used to solve the address location problem.  Finding address blocks has been the focus of intensive research recently, as several companies  are developing address reading machines (United States Postal Service 92). The wide variety  of images that have to be handled has led other researchers to apply several different analysis  techniques to each image and then t. to combine the results at the end, see e.g. (Palumbo et al.  92). In order to achieve the throughput required in an industrial application, special purpose  processors for finding connected components and/or for executing Hough transforms have been  applied.  In our system we use the NET32K processor to extract geometrical features from an image. The  high compute power of this chip allows the extraction of a large number of features  simultaneously. From this feature representation, an interpretation of the image's content can  then be achieved with a standard processor. Compared to an analysis of the original image, the  analysis of the feature maps requires several orders of magnitude less computation. Moreover,  the feature representation introduces a high level of robustness against noise. This paper gives  a brief overview of the hardware platform in section 2 and then describes the algorithms to find  the address blocks in section 3.  2 THE HARDWARE  The NET32K system has been designed to serve as a high-speed image processing platform,  where neural nets as well as conventional algorithms can be executed. Three boards form the  whole system. Two NET32K neural net chips are integrated with a sequencer and data  formatting circuits on one board. The second board contains two digital signal processors  (DSPs), together with 6 Mbytes ofmemolj. Control of the whole system is provided by a board  containing a SPARC2 processor plus 64 Mbytes of memory. A schematic of this system is  shown in Figm'e 1.  Image buffering and communication with other modules in the address reader are handled by  the board with the SPARC2 processor. When an image is received, it is sent to the DSP board  and from there over to the NET32K processor. The feature maps produced by the NET32K  processor are stored on the DSP board, while the SPARC2 starts with the analysis of the feature  maps. The DSP's main task is formatting of the data, while the NET32K processor extracts all  the features. Its speed of computation is more than 100 billion multiply-accumulates per second  with operands that have one or two bits of resolution. Images with a size of 512x512 pixels are  processed at a rate of more than 10 fi'ames per second, and 64 convolution kernels, each with  a size of 16x16 pixels, can be scanned simultaneously over the image. Each such kemel is  tuned to detect the presence of a feature, such as a line, an edge or a comer.  Address Block Location with a Neural Net System 787  NET32K MODULE  i NE NET32  .................................. ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::: ...........................  I ..... i""""' ........  VME BUS  Figure 1: Schematic of the xvhole NET32K system. Each of the dashed  boxes represents one 6U VME board. The arrows show the  communication paths.  3. SEQUENCE OF ALGORITHMS  The final result of the adch'ess block location system is a box describing a tight bound around  the destination address, if the address is machine printed. Of handwritten addresses, only the  zip code is read, and hence, one has to find a tight boundary around the zip code. This  information is then passed along to reader modules of the address reading machine. There is no  a priori knowledge about the writing style. Therefore the system first has to discriminate  between handwritten and machine printed text. At the end of the address block location process,  additional algorithms are executed to improve the accuracy of the reader. An overview of the  sequence of algorithms used to solve these tasks is shown in Figure 2. The whole process is  divided into three major steps: Preprocessing, featm'e extraction, and high-level analysis based  on the feature information.  3.1. Preprocessing  To quickly get an idea about the complexity of the image, a coarse evaluation of its layout is  done. By sampling the density of the black pixels in various places of the image, one can see  already whether the image is clean or noisy and whether the text is lightly printed or is dark.  788 Graf and Cosatto  The images are divided into four categories, depending on their darkness and the level of noise.  This information is used in the subsequent processing to guide the choice of the features. Only  about one percent of the pixels are taken into account for this analysis, therefore, it can be  executed quickly on the SPARC2 processor.  Preprocessing  Extract features  NET32K  clean, light clean, dark noisy, light  16 Feature  maps  MACHINE PRINT  Analyse group of lines  Determine level of noise  Clean with NET32K;  Extract text lines  Cluster lines into groups  ( "Classifygroups of lines   HAND WRI'I-rEN  Cluster text segments into lines  Analyse group of lines  Segment lines to find ZIP  Determine slant/skew angle;  Figure 2: Schematic of the sequence of algorithms for finding the  position of the address blocks.  3.2. Feature Extraction  After the preprocessing, the image is sent to the NET32K board where simple geometrical  features, such as edges, corners and lines are extracted. Up to 16 different feature maps are  generated, where a pixel in one of the maps indicates the presence of a feature in this location.  Some of these feature maps are used by the host processor, for example, to decide whether text  is handwritten or machine printed. Other feature maps are combined and sent once more  through the NET32K processor in order to search for combinations of features representing  more complex features. Typically, the feature maps are thresholded, so that only one bit per  pixel is kept. More resolution of the computation results is available from the neural net chips,  but in this way the amount of data that has to be analyzed is minimal, and one bit of resolution  turned out to be sufficient.  Examples of kernels used for the detection of stxokes and text lines are shown in Figure 3. In  the chip, usually four line detectors of increasing height plus eight stroke detectors of different  orientations are stored. Other detectors are tuned to edges and su'okes of machine printed text.  The line detectors respond to any black line of the proper height. Due to the large width of 16  Address Block Location with a Neural Net System 789  pixels, a kemel stretches over one or even several characters. Hence a text line gives a response  similar to that produced by a continuous black line. When the threshold is set properly, a text  line in the original image produces a continuous line in the feature map, even across the gaps  between characters and across small empty spaces between words. For an interpretation of a  line feature map only the lea and right end points of each connected component are stored. In  this way one obtains a compact representation of the lines' positions that are well suited for the  high-level analysis of the layout.  Kernel: Line detector  Image  the NET32K syste  Feature map  Kernel: Stroke detector  Image  Feature map  Figure 3 :Examples of convolution kemels and their results. The kernels' sizes  are 16xl 6 pixels, and their pixels' values are +1,0, -1. The upper part illustrates  the response of a line detector on a machine printed text line. The lower kernel  extracts sla'okes of a certain orientation from handuritten text.  Handwritten lines are detected by a second technique, because they are more irregular in height  and the characters may be spaced apm widely. Detectors for strokes, of the type shown in the  lower half of Figure 3, are well suited for sensing the presence of handwritten text. The feature  maps resulting form handwritten text tend to exhibit blobs of pixels along the text line. By  smearing such feature maps in horizontal direction the responses of individual strokes are  merged into lines that can then be used in the same way as described for the machine printed  lines.  Horizontal smearing of text lines, combined with connected component analysis is a well-known  790 Graf and Cosatto  technique, often applied in layout analysis, to find words and whole lines of text. But when  applied to the pixels of an image, such an approach works well only in clean images. As soon  as there is noise present, this technique produces irregular responses. The key to success in a  real world environment is robustness against noise. By extracting features first and then  analyzing the feature maps, we drastically reduce the influence of noise. Each of the convolution  kernels covers a range of 256 pixels and its response depends on several dozens of pixels inside  this area. If pixels in the image are corrupted by noise, this has only a minor effect on the result  of the convolution and, hence, the appearance of the feature map.  When the analysis is started, it is unknown, whether the address is machine printed or hand  written. In order to distinguish between the two writing styles, a simple one-layer classifier  looks at the results of four stroke detectors and of four line detectors. It can determine reliably  whether text is handwritten or machine printed. Additional useful information that can be  extracted easily from the feature maps, is the skew angle of handwritten text. People tend to  write with a skew anywhere from -45 degrees to almost +90 degrees. In order to improve the  accuracy of a reader, the text is first deskewed. The most time consuming part of this operation  is to determine the skew angle of the writing. The stroke detector with the maximum response  over a line is a good indicator of the skew angle of the text. We compared this simple technique  with several alternatives and found it to be as reliable as the best other algorithm and much  faster to compute.  3.3. High-level Analysis  The results of the feature ex'action process are line segments, each one marked as handwritten  or machine printed. Only the left and right end points of such lines are stored. At this point,  there may still be line segments in this group that do not correspond to text, but rather to solid  black lines or to line drawings. Therefore each line segment is checked, to determine whether  the ratio of black and white pixels is that found typically in text.  Blocks of lines are identified by clustering the line segments into groups. Then each block is  analyzed, to see whether it can represent the destination address. For this purpose such features  as the number of lines in the block, its size, position, etc. are used. These features are entered  into a classifier that ranks each of the blocks. Certain conditions, such as a size that is too large,  or fithere are too many text lines in the block, will lead to an attempt to split blocks. If no good  result is obtained, clustering is tried again with a changed distance metric, where the horizontal  and the vertical distances between lines are weighted differently.  If an address is machine printed, the whole address block is passed on to the reader, since not  only the zip code, but the whole adch'ess, including the city name, the street name and the name  of the recipient have to be read. A big problem for the reader present images of poor quality,  particularly those with background noise and texture. State-of-the-art readers handle machine  printed text reliably if the image quality is good, but they may fail totally if the text is buried in  noise. For that reason, an address block is cleaned before sending it to the reader. Feature  extraction with the NET32K board is used once more for this task, this time with detectors tuned  to find all the strokes of the machine printed text. Applying stroke detectors with the proper  width allows a good discrimination between the text and any noise. Even texture that consists  of lines can be rejected reliably, if the line thickness of the texture is not the same as that of the  text.  Address Block Location with a Neural Net System 791  Figure 4: Example of an envelope image at various stages of the processing. Top: The  result of the clustering process to find the bounding box of the address. Bottom right: The  text lines within the address block are marked. Bottom left: Cuts in the text line with the  zip code and below that the result of the reader. (The zip code is actually the second  segment sent to the reader; the ilrst one is the string 'USA').  If the address is handwritten, only the zip code is sent to the reader. In order to find the zip code,  an analysis of the internal sta'uctm-e of the address block has to be done, which starts with finding  the true text lines. Handwritten lines are often not straight, may be heavily skewed, and may  contain large gaps. Hence simple techniques, such as connected component analysis, do not  provide proper results. Clustering of the line segments obtained from the feature maps, provides  a reliable solution of this problem. Once the lines are found, each one is segmented into words  and some of them are selected as candidates for the zip code and are sent to the reader. Figure  4 shows an example of an envelope image as it progresses tlu'ough the various processing steps.  The system has been tested extensively on overall more than 100,000 images. Most of these  tests were done in the assembled address reader, but during development of the system, large  792 Graf and Cosatto  tests were also done with the address location module alone. One of the problems for evaluating  the performance is the lack of an objective quality measure. When has an address been located  correctly? Cutting off a small part of the address may not be detrimental to the final  interpretation, while a bounding box that includes some additional text may slow the reader  down too much, or it may throw off the interpretation. Therefore, it is not always clear when a  bounding box, describing the address' location, is tight enough. Another important factor  affecting the accuracy numbers is, how many candidate blocks one actually considers. For all  these reasons, accuracy numbers given for address block location have to be taken with some  caution. The results mentioned here were obtained by judging the images by eye. If images are  clean and the address is surrounded by a white space larger than two line heights, the location  is found correctly in more than 98% of the cases. Often more than one text block is found and  of these the destination address is the first choice in 90% of the images, for a typical layout. If  the image is very noisy, which actually happens surprisingly often, a tight bound around the  address is found in 85% of the cases. These results were obtained with 5,000 images, chosen  from more than 100,000 images to represent as much variety as possible. Of these 5,000 images  more than 1,200 have a texture around the address, and often this texture is so dark that a  human has difficulties to make out each character.  4. CONCLUSION  Most of our algorithms described here consist of two parts: feature extraction implemented with  a convolution and interpretation, typically implemented with a small classifier. Surprisingly  many algorithms can be cast into such a format. This common framework for algorithms has  the advantage of facilitating the implementation, in particular when algorithms are mapped into  hardware. Moreover, the featm'e extraction with large convolution kernels makes the system  robust against noise. This robustness is probably the biggest advantage of our approach. Most  existing automatic reading systems are very good as long as the images are clean, but they  deteriorate rapidly with decreasing image quality.  The biggest drawback of convolutions is that they require a lot of computation. In fact, without  special purpose hardware, convolutions are often too slow. Our system relies on the NET32K  neural net chips to obtain the necessary throughput. The NET32K system is, we believe, at the  moment the fastest board system for this type of computation. This speed is obtained by  systematically exploiting the fact that only a low resolution of the computation is required. This  allows to use analog computation inside the chip and hence much smaller circuits than would  be the case in an all-digital circuit.  References  United States Postal Service, (1992), Proc. Advanced Technology Conf., Vol. 3, Section on  address block location: pp. 1221 - 1310.  P.W. Palumbo, S.N. Srihari, J. Soh, R. Sridhar, V. Demjanenko, (1992), :'Postal Address Block  Location in Real Time", IEEE COMPUTER, Vol. 25/7, pp. 34 - 42.  H.P. Graf and D. Henderson, (1990), "A Reconfigurable CMOS Neural Network", Digest  IEEE Int. Solid State Circuits Conf. p. 144.  
Directional Hearing by the Mauthner  System  Audrey L. Gusik  Department of Psychology  University of Colorado  Boulder, Co. 80309  Robert C. Eaton  E. P.O. Biology  University of Colorado  Boulder, Co. 80309  Abstract  We provide a computational description of the function of the Mau-  thner system. This is the bralnstem circuit which initiates fast-  start escapes in telcost fish in response to sounds. Our simula-  tions, using backpropagation in a realistically constrained feedfor-  ward network, have generated hypotheses which are directly inter-  pretable in terms of the activity of the auditory nerve fibers, the  principle cells of the system and their associated inhibitory neu-  rons.  1 INTRODUCTION  1.1 THE MAUTHNER SYSTEM  Much is known about the brainstem system that controls fast-start escapes in teleost  fish. The most prominent feature of this network is the pair of large Mauthner  cells whose axons cross the midline and descend down the spinal cord to synapse  on primary motoneurons. The Mauthner system also includes inhibitory neurons,  the PHP cells, which have a unique and intense field effect inhibition at the spike-  initiating zone of the Mauthner cells (Faber and Korn, 1978). The Mauthner system  is part of the full brainstem escape network which also includes two pairs of cells  homologous to the Mauthner cell and other populations of reticulospinal neurons.  With this network fish initiate escapes only from appropriate stimuli, turn away  from the offending stimulus, and do so very rapidly with a latency around 15 msec  in goldfish. The Mauthner cells play an important role in these functions. Only one  574  Directional Hearing by the Mauthner System 575  fires thus controlling the direction of the initial turn, and it fires very quickly (4-5  msec). They also have high thresholds due to instrinsic membrane properties and  the inhibitory influence of the PHP cells. (For reviews, see Eaton, et al, 1991 and  Faber and Korn, 1978.)  Acoustic stimuli are thought to be sufficient to trigger the response (Blaxter, 1981),  both Manthner cells and PHP cells receive innerration from primary auditory fibers  (Faber and Korn, 1978). In addition, the Manthner cells have been shown physio-  logically to be very sensitive to acoustic pressure (Canfield and Eaton, 1990).  1.2 LOCALIZIN( SOUNDS UNDERWATER  In contrast to terrestrial vertebrates, there are several reasons for supposing that  fish do not use time of arrival or intensity differences between the two ears to localize  sounds: underwater sound travels over four times as fast as in air; the fish body  provides no acoustic shadow; and fish use a single transducer to sense pressure which  is conveyed equally to the two ears. Sound pressure is transduced into vibrations  by the swim bladder which, in goldfish, is mechanically linked to the inner ear.  Fish are sensitive to an additional component of the acoustic wave, the particle  motion. Any particle of the medium taking part in the propagation of a longitudehal  wave will oscillate about an equilibrium point along the axis of propagation. Fish  have roughly the same density as water, and will experience these oscillations. The  motion is detected by the bending of sensory hairs on auditory receptor cells by  the otolith, an inertial mass suspended above the hair cells. This component of the  sound will provide the axis of propagation, but there is a 180 degree ambiguity.  Both pressure and particle motion are sensed by hair cells of the inner ear. In  goldfish these signals may be nearly segregated. The linkage with the swim bladder  impinges primarily on a honey chamber containing two of the endorgans of the inner  ear: the saccule and the lagena. The utricle is a third endorgan also thought to  mediate some acoustic function, without such direct input from the swimbladder.  Using both of these components fish can localize sounds. According to the phase  model (Schuijf, 1981) fish analyze the phase difference between the pressure com-  ponent of the sound and the particle displacement component to calculate distance  and direction. When pressure is increasing, particles will be pushed in the direc-  tion of sound propagation, and when pressure is decreasing particles will be pulled  back. There will be a phase lag between pressure and particle motion which varies  with frequency and distance from the sound source. This, and the separation of the  pressure from the displacement signals in the ear of some species pose the greatest  problems for theories of sound localization in fish.  The acoustically triggered escape in goldfish is a uniquely tractable problem in  underwater sound localization. First, there is the fairly good segregation of pressure  from particle motion at the sensory level. Second, the escape is very rapid. The  decision to turn left or right is equivalent to the firing of one or the other Manthner  cell, and this happens within about 4 msec. With transmission delay, this decision  relies only on the initial 2 msec or so of the stimulus. For most salient frequencies,  the phase lag will not introduce uncertainty: both the first and second derivatives  of particle position and acoustic pressure will be either positive or negative.  576 Guzik and Eaton  1.3 THE XNOR MODEL  A  Active  pressure  inpui  Active  displacemeni  input  Leh  Mauthner  outpu  Right  Mauthner  output  B  P+  P+  p-  p-  DL  DR  DL  DR  On  off  off  On  Off  On  On  Off  Left sound  SOLlICe  DR  P+  p-  DL  Mauthner  Mauthncr  cell X cell   0 = excitntory  No response  DL  P+  P-  DR  Figure 1 Truth table and minimal network for the XNOR model.  Given the above simplification of the problem, we can see that each Mauthner  cell must perform a logical operation (Guzik and Eaton, 1993; Eaton et al, 1994).  The left Mauthner cell should fire when sounds are located on the left, and this  occurs when either pressure is increasing and particle motion is from the left or  when pressure is decreasing and particle motion is from the right. We can call  displacement from the left positive for the left Mauthner cell, and immediately we  Directional Hearing by the Mauthner System 577  have the logical operator exclusive-nor (or XNOR). The right Mauthner cell must  solve the same problem with a redefinition of right displacement as positive. The  conditions for this logic gate are shown in figure 1A for both Mauthner cells. This  analysis simplifies our task of understanding the computational role of individual  elements in the system. For example, a minimal network could appear as in figure  lB.  In this model PHP units perform a logical sub-task of the XNOR as AND gates.  This model requires at least two functional classes of PHP units on each side of  the brain. These PHP units will be activated for the combinations of pressure  and displacement that indicate a sound coming from the wrong direction for the  Mauthner cell on that side. Both Mauthner cells are activated by sufficient changes  in pressure in either direction, high or low, and will be gated by the PHP cells. This  minimal model emerged from explorations of the system using the connectionist  paradigm, and inspired us to extend our efforts to a more realistic context.  2 THE NETWORK  We used a connectionist model to explore candidate solutions to the left/right dis-  crimination problem that include the populations of neurons known to exist and  include a distributed input resembling the sort available from the hair cells of the  inner ear. We were interested in generating a number of alternative solutions to be  better prepared to interpret physiological recordings from live goldfish, and to look  for variations of, or alternatives to, the XNOR model.  2.1 THE ARCHITECTURE  As shown in figure 2, there are four layers in the connectionist model. The input  layer consists of four pools of hair cell units. These represent the sensory neurons  of the inner ear. There are two pools on each side: the saccule and the utricle.  Treating only the hori.ontal plane, we have ignored the lagena in this model. The  saccule is the organ of pressure sensation and the utricle is treated as the organ  of particle motion. Each pool contains 16 hair cell units maximally responsive for  displacements of their sensory hairs in one particular direction. They are activated  as the cosine of the difference between their preferred direction and the stimulus  deflection. All other units use sigmoidal activation functions.  The next layer consists of units representing the auditory fibers of the VIIIth nerve.  Each pool receives inputs from only one pool of hair cell units, as nerve fibers have  not been seen to innervate more than one endorgan. There are 10 units per fiber  pool.  The fiber units provide input to both the inhibitory PHP units, and to the Mauthner  units. There are four pools of PHP units, two on each side of the fish. One set  on each side represents the collateral PHP cells, and the other set represents the  commissural PHP cells (Faber and Korn, 1978). Both types receive inputs from the  auditory fibers. The collaterals project only to the Mauthner cell on the same side.  The commissurals project to both Mauthner cells. There are five units per PHP  pool.  578 Guzik and Eaton  The Mauthner cell units receive inputs from saccular and utricular fibers on their  same side only, as well as inputs from a single collateral PHP population and both  commissural PHP populations.  Left Saccule Left Utricle  Hair Cells I ] I I  ! l  Auditory Nerve i  Fiber Pools  Right Utricle Right Saccule  PHPs c--  Left Mauthner Right Mauthner  Figure 2 The architecture.  Weights from the PHP units are all constrained to be negative, while all others are  constrained to be positive. The weights are implemented using the function below,  positive or negative depending on the polarity of the weight.  f(w) = 1/2 (w + In cosh(w) + In 2)  The function asymptotes to zero for negative values, and to the identity function for  values above 2. This function vastly improved learning compared with the simpler,  but highly nonlinear exponential function used in earlier versions of the model.  2.2 TRAINING  We used a total of 240 trnlnlng examples. We began with a set of 24 directions for  particle motion, evenly distributed around 360 degrees. These each appeared twice,  once with increasing pressure and once with decreasing pressure, making a base set  of 48 examples. Pressure was introduced as a deflection across saccular hair ceils of  either 0 degrees for low pressure, or 180 degrees for high pressure. These should be  thought of as reflecting the expansion or compression of the swim bladder. Targets  for the Mauthner cells were either 0 or I depending upon the conditions as described  in the XNOR model, in figure 1A.  Directional Hearing by the Mauthner System 579  Next by randomly perturbing the activations of the hair cells for these 48 patterns,  we generated 144 noisy examples. These were randomly increased or decreased up  to 10%. An additional 48 examples were generated by dividing the hair cell activity  by two to represent sub-threshold stimuli. These last 48 targets were set to zero.  The network was trained in batch mode with backpropagation to minimize a cross-  entropy error measure, using conjugate gradient search. Unassisted backpropaga-  tion was unsuccessful at finding solutions.  For the eight solutions discussed here, two parameters were varied at the inputs. In  some solutions the utricle was stimulated with a vector sum of the displacement and  the pressure components, or a "mixed" input. In some solutions the hair cells in the  utricle are not distributed uniformly, but in a gaussian manner with the mean tuning  of 45 degrees to the right or left, in the two ears respectively. This approximates  the actual distribution of hair cells in the goldfish utricle (Platt, 1977).  RESULTS  Analyzing the activation of the hidden units as a function of input pattern we found  activity consistent with known physiology, nothing inconsistent with our knowledge  of the system, and some predictions to be evaluated during intracellular recordings  from PHP cells and auditory afferents.  First, many PHP cells were found exhibiting a logical function, which is consistent  with our minimal model described above. These tended to project only to one  Mauthner cell unit, which suggests that primarily the collateral PHP cells will  demonstrate logical properties. Most logical PHP units were NAND gates with  very large weights to one Mauthner cell. An example is a unit which is on for all  stimuli except those having displacements anywhere on the left when pressure is  high.  Second, saccular fibers tended to be either sensitive to high or low pressure, consis-  tent with recordings of Furukawa and Ishii (1967). In addition there were a class  which looked like threshold fibers, highly active for all supra-threshold stimuli, and  inactive for all sub-threshold stimuli. There were some fibers with no obvious se-  lectivity, as well.  Third, utricular fibers often demonstrate sensitivity for displacements exclusively  from one side of the fish, consistent with our minimal model. Right and left utricular  fibers have not yet been demonstrated in the real system.  Utricular fibers also demonstrated more coarsely tuned, less interpretable receptive  fields. All solutions that included a mixed input to the utricle, for example, pro-  duced fibers that seemed to be "not 180 degree",or "not 0 degree", countering the  pressure vectors. We interpret these fibers as doing clean-up given the absence of  negative weights at that layer.  Fourth, sub-threshold behavior of units is not always consistent with their supra-  threshold behavior. At sub-threshold levels of stimulation the activity of units may  not reflect their computational role in the behavior. Thus, intracellular recordings  should explore stimulus ranges known to elicit the behavior.  580 Guzik and Eaton  Fifth, Mauthner units usually receive very strong inputs from pressure fibers. This  is consistent with physiological recordings which suggest that the Mauthner cells  in goldfish are more sensitive to sound pressure than displacement (Canfield and  Eaton, 1990).  Sixth, Mauthner cells always acquired relatively equal high negative biases. This is  consistent with the known low input resistance of the real Mauthner cells, giving  them a high threshold (Faber and Korn, 1978).  Seventh, PHP cells that maintain substantial bilateral connections tend to be ton-  ically active. These contribute additional negative bias to the Manthner cells. The  relative sies of the connections are often assymetric. This suggests that the commis-  sural PHP cells serve primarily to regulate Mauthner threshold, ensure behavioral  response only to intense stimuli, consistent with Faber and Korn (1978). These cells  could only contribute to a partial solution of the XNOR problem.  Eighth, all solutions consistently used logic gate PHP units for only 50% to 75%  of the training examples. Probably distributed solutions relying on the direct con-  nections of auditory nerve fibers to Mauthner cells were more easily learned, and  logic gate units only developed to handle the unsolved cases. Cases solved without  logic gate units were solved by assymetric projections to the Mauthner cells of one  polarity of pressure and one class of direction fibers, left or right.  Curiously, most of these cases involved a preferential projection from high pressure  fibers to the Mauthner units, along with directional fibers encoding displacements  from each Mauthner unit's positive direction. This means the logic gate units  tended to handle the low pressure cases. This may be a result of the presence of  the assymetric distributions of utricular hair cells in 6 out of the 8 solutions.  4 CONCLUSIONS  We have generated predictions for the behavior of neurons in the Mauthner system  under different conditions of acoustic stimulation. The predictions generated with  our connectionist model are consistent with our interpretation of the phase model  for underwater sound localization in fishes as a logical operator. The results are also  consistent with previously described properties of the Mauthner system. Though  perhaps based on the characteristics more of the training procedure, our solutions  suggest that we may find a mixed solution in the fish. Direct projections to the  Manthner cells from the auditory nerve perhaps handle many of the commonly  encountered acoustic threats. The results of Blaxter (1981) support the idea that  fish do escape from stimuli regardless of the polarity of the initial pressure change.  Without significant nonlinear processing at the Mauthner cell itself, or more com-  plex processing in the auditory fibers, direct connections could not handle all of  these cases. These possibilities deserve exploration.  We propose different computational roles for the two classes of inhibitory PHP  neurons. We expect only unilaterally-projecting PHP cells to demonstrate some  logical function of pressure and particle motion. We believe that some elements of  the Mauthner system must be found to demonstrate such minimal logical functions  if the phase model is an explanation for left-right discrimination by the Mauthner  system.  Directional Hearing by the Mauthner System 581  We are currently preparing to deliver controlled acoustic stimuli to goldfish during  acute intracellular recording procedures from the PHP neurons, the afferent fibers  and the Manthner cells. Our insights from this model will greatly assist us in  designing the stimulus regimen, and in interpreting our experimental results. Plans  for future computational work are of a dynamic model that will include the results of  these physiological investigations, as well as a more realistic version of the Manthner  cell.  Acknowledgements  We are grateful for the technical assistance of members of the Boulder Connectionist  Research Group, especially Don Mathis for help in debugging and optilniing the  original code. We thank P.L. Edds-Walton for crucial discussions. This work was  supported by a grant to RCE from the National Institutes of Health (RO 1 N S22621).  References  Blaxter, J.H.S., J.A.B. Gray, and E.J. Denton (1981) Sound and startle responses  in herring shoals. J. Mar. Biol. Assoc. UK, 61:851-869  Canfield, J.G. and R.C. Eaton (1990) Swimbladder acoustic pressure transduction  intiates Mauthner-mediated escape. Nature, 347:760-762  Eaton, R.C., J.G. Canfield and A.L. Ouzik (1994) Left-fight discrimination of sound  onset by the Manthner system. Brain Behar. Evol., in prezz  Eaton, R.C., R. DiDomenico and J. Nissanov (1991) Role of the Mauthner cell in  sensorimotor integration by the brain stem escape network. Brain Behar. Evol.,  37:272-285  Faber, D.S. and H. Korn (1978) Electrophysiology of the Mauthner cell: Basic  properties, synaptic mechanisms and associated networks. In Neurobiology of the  Mauthner Cell, D.S. Faber and H. Korn (eds), Raven Press, NY, pp. 47-131  Fay, R.R.(1984) The goldfish ear codes the axis of acoustic particle motion in three  dimensions. Science, I5:951-954  Furukawa, T. and Y. Ishii (1967) Effects of static bending of sensory hairs on sound  reception in the goldfish. Japanese J. Physiol., 17:572-588  Guzik, A.L. and R.C. Eaton (1993) The XNOR model for directional hearing by  the Mauthner system. Soc. Neurosci. Abstr.  Platt, C. (1977) Hair cell distribution and orientation in goldfish otolith organs. J.  Comp. Neurol., 17: 283-298  Schuijf, A. (1981) Models of acoustic localization. In Hearing and Sound Commu-  nication in Fishes, W.N. Tavolga, A.N. Popper and R.R. Fay (eds.), Springer, New  York,. pp. 267-310  
