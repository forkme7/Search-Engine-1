Reinforcement Learning for  Spoken Dialogue Systems  Satinder Singh  AT&T Labs  Michael Kearns  AT&T Labs  Diane Litman  AT&T Labs  Marilyn Walker  AT&T Labs  { baveja,mkearns,diane,walker} @ research.att.com  Abstract  Recently, a number of authors have proposed treating dialogue systems as Markov  decision processes (MDPs). However, the practical application of MDP algorithms  to dialogue systems faces a number of severe technical challenges. We have built a  general software tool (RLDS, for Reinforcement Learning for Dialogue Systems)  based on the MDP framework, and have applied it to dialogue corpora gathered  from two dialogue systems built at AT&T Labs. Our experiments demonstrate that  RLDS holds promise as a tool for "browsing" and understanding correlations in  complex, temporally dependent dialogue corpora.  1 Introduction  Systems in which human users speak to a computer in order to achieve a goal are called  spoken dialogue systems. Such systems are some of the few realized examples of open-  ended, real-time, goal-oriented interaction between humans and computers, and are therefore  an important and exciting testbed for AI and machine learning research. Spoken dialogue  systems typically integrate many components, such as a speech recognizer, a database back-  end (since often the goal of the user is to retrieve information), and a dialogue strategy. In  this paper we are interested in the challenging problem of automatically inferring a good  dialogue strategy from dialogue corpora.  Research in dialogue strategy has been perhaps necessarily ad-hoc due to the open-ended  nature of dialogue system design. For example, a common and critical design choice is be-  tween a system that always prompts the user to select an utterance from fixed menus (system  initiative), and one that attempts to determine user intentions from unrestricted utterances  (mixed initiative). Typically a system is built that explores a few alternative strategies, this  system is tested, and conclusions are drawn regarding which of the tested strategies is best  for that domain [4, 7, 2]. This is a time-consuming process, and it is difficult to rigorously  compare and evaluate alternative systems in this fashion, much less design improved ones.  Recently, a number of authors have proposed treating dialogue design in the formalism of  Markov decision processes (MDPs)[ 1, 3, 7]. In this view, the population of users defines the  stochastic environment, a dialogue system's actions are its (speech-synthesized) utterances  and database queries, and the state is represented by the entire dialogue so far. The goal is  to design a dialogue system that takes actions so as to maximize some measure of reward.  Viewed in this manner, it becomes possible, at least in principle, to apply the framework and  algorithms of reinforcement learning (RL) to find a good dialogue strategy.  However, the practical application of RL algorithms to dialogue systems faces a number of  severe technical challenges. First, representing the dialogue state by the entire dialogue so  Reinforcement Learning for Spoken Dialogte Systems 957  far is often neither feasible nor conceptually useful, and the so-called belief state approach  is not possible, since we do not even know what features are required to represent the belief  state. Second, there are many different choices for the reward function, even among systems  providing very similar services to users. Previous work [7] has largely dealt with these issues  by imposing a priori limitations on the features used to represent approximate state, and then  exploring just one of the potential reward measures.  In this paper, we further develop the MDP formalism for dialogue systems, in a way that does  not solve the difficulties above (indeed, there is no simple "solution" to them), but allows us  to attenuate and quantify them by permitting the investigation of different notions of approx-  imate state and reward. Using our expanded formalism, we give one of the first applications  of RL algorithms to real data collected from multiple dialogue systems. We have built a gen-  eral software tool (RLDS, for Reinforcement Learning for Dialogue Systems) based on our  framework, and applied it to dialogue corpora gathered from two dialogue systems built at  AT&T Labs, the TOOT system for voice retrieval of train schedule information [4] and the  ELVIS system for voice retrieval of electronic mail [7].  Our experiments demonstrate that RLDS holds promise not just as a tool for the end-to-  end automated synthesis of complicated dialogue systems from passive corpora -- a "holy  grail" that we fall far short of here  -- but more immediately, as a tool for "browsing"  and understanding correlations in complex, temporally dependent dialogue corpora. Such  correlations may lead to incremental but important improvements in existing systems.  2 The TOOT and ELVIS Spoken Dialogue Systems  The TOOT and ELVIS systems were implemented using a general-purpose platform devel-  oped at AT&T, combining a speaker-independent hidden Markov model speech recognizer,  a text-to-speech synthesizer, a telephone interface, and modules for specifying data-access  functions and dialogue strategies. In TOOT, the data source is the Amtrak train schedule web  site, while in ELVIS, it is the electronic mail spool of the user.  In a series of controlled experiments with human users, dialogue data was collected from  both systems, resulting in 146 dialogues from TOOT and 227 dialogues from ELVIS. The  TOOT experiments varied strategies for information presentation, confirmation (whether and  how to confirm user utterances) and initiative (system vs. mixed), while the ELVIS experi-  ments varied strategies for information presentation, for summarizing email folders, and ini-  tiative. Each resulting dialogue consists of a series of system and user utterances augmented  by observations derived from the user utterances and the internal state of the system. The  system's utterances (actions) give requested information, ask for clarification, provide greet-  ings or instructions, and so on. The observations derived from the user's utterance include  the speech-recognizer output, the corresponding log-likelihood score, the semantic labels as-  signed to the recognized utterances (such as the desired train departure and arrival cities in  TOOT, or whether the user prefers to hear their email ordered by date or sender in ELVIS);  indications of user barge-ins on system prompts; and many more. The observations derived  from the internal state include the grammar used by the speech recognizer during the turn,  and the results obtained from a query to the data source. In addition, each dialogue has an  associated survey completed by the user that asks a variety of questions relating to the user's  experience. See [4, 7] for details.  3 Spoken Dialogue Systems and MDPs  Given the preceding discussion, it is natural to formally view a dialogue as a sequence d   .,   However, in recent work we have applied the methodology described here to significantly improve  the performance of a new dialogue system [5].  958 S. Singh, M. Kearns, D. Litman and M. Walker  Here ai is the action taken by the system (typically a speech-synthesized utterance, and less  frequently, a database query) to start the ith exchange (or turn, as we shall call it), r7i consists  of all the observations logged by the system on this turn, as discussed in the last section,  and ri is the reward received on this turn. As an example, in TOOT a typical turn might  indicate that the action ai was a system utterance requesting the departure city, and the 7i  might indicate several observations: that the recognized utterance was "New York", that the  log-likelihood of this recognition was -2.7, that there was another unrecognized utterance as  well, and so on. We will use d[i] to denote the prefix of d that ends following the ith turn, and  d. (a, r7, r) to denote the one-turn extension of dialogue d by the turn (a, r7, r). The scope of  the actions ai and observations r7i is determined by the implementation of the systems (e.g.  if some quantity was not logged by the system, we will not have access to it in the  in the  data). Our experimental results will use rewards derived from the user satisfaction surveys  gathered for the TOOT and ELVIS data.  We may view any dialogue d as a trajectory in a well-defined true MDP M. The states 2  of M are all possible dialogues, and the actions are all the possible actions available to the  spoken dialogue system (utterances and database queries). Now from any state (dialogue) d  and action a, the only possible next states (dialogues) are the one-turn extensions d- (a, G, r).  The probability of transition from d to d. (a, r7, r) is exactly the probability, over the stochastic  ensemble of users, that rYand r would be generated following action a in dialogue d.  It is in general impractical to work directly on M due to the unlimited size of the state (di-  alogue) space. Furthermore, M is not known in advance and would have to be estimated  from dialogue corpora. We would thus like to permit a flexible notion of approximate states.  We define state estimator sE to be a mapping from any dialogue d into some space $. For  example, a simple state estimator for TOOT might represent the dialogue state with boolean  variables indicating whether certain pieces of information had yet been obtained from the  user (departure and arrival cities, and so on), and a continuous variable tracking the average  log-likelihood of the recognized utterances so far. Then sE(d) would be a vector represent-  ing these quantities for the dialogue d. Once we have chosen a state estimator SE, we can  transform the dialogue d into an S-trajectory, starting from the initial empty state so  $:  so -a sE(d[1]) -a sE(d[2]) -+3 '" -'+, sE(d[t])  where the notation -+, sv..(d[i]) indicates a transition to sv,(d[q) $ following action  ai. Given a set of dialogues dx,..., d,, we can construct the empirical MDP fi;/sr. The state  space of fi;/sr is $, the actions are the same as in M, and the probability of transition from s to  s t under action a is exactly the empirical probability of such a transition in the S-trajectories  ^  obtained from dl,..., d,. Note that we can build Msr from dialogue corpora, solve for its  optimal policy, and analyze the resulting value function.  The point is that by choosing sv. carefully, we hope that the empirical MDP 3;/st will be a  good approximation of M. By this we mean that/13/st renders dialogues (approximately)  Markovian: the probability in M of transition from any dialogue d to any one-turn extension  d. (a, (7, r) is (approximately) the probability of transition from sv.(d) to sv.(d. (a, (7, r)) in  fi;/sr. We hope to find state estimators sv. which render dialogues approximately Markovian,  but for which the amount of data and computation required to find good policies in 35/sr will  be greatly reduced compared to working directly in dialogue space.  While conceptually appealing, this approach is subject to at least three important caveats:  First, the approach is theoretically justified only to the extent that the chosen state estima-  tor renders dialogues Markovian. In practice, we hope that the approach is robust, in that  "small" violations of the Markov property will still produce useful results. Second, while  2These are not to be confused with the intemal states of the spoken dialogue system(s) during the  dialogue, which in our view merely contribute observations.  Reinforcement Learning for Spoken Dialogue Systems 959  state estimators violating the Markov property may lead to meaningful insights, they can-  not be directly compared. For instance, if the optimal value function derived from one state  estimator is larger than the optimal value function for another state estimator, we cannot nec-  essarily conclude that the first is better than the second. (This can be demonstrated formally.)  Third, even with a Markovian state estimator sv., data that is sparse with respect to SE limits  the conclusions we can draw; in a large space $, certain states may be so infrequently visited  in the dialogue corpora that we can say nothing about the optimal policy or value function  there.  4 The RLDS System  We have implemented a software tool (written in C) called RLDS that realizes the above  formalism. RLDS users specify an input file of sample dialogues; the dialogues include the  rewards received at each turn. Users also specify input files defining $ and a state estimator  sg. The system has command-line options that specify the discount factor to be used, and  a lower bound on the number of times a state s 6 $ must be visited in order for it to be  ^  included in the empirical MDP Mss (to control overfitting to sparse data). Given these inputs  and options, RLDS converts the dialogues into S-trajectories, as discussed above. It then  uses these trajectories to compute the empirical MDP Mss specified by the data -- that is,  the data is used to compute next-state distributions and average reward in the obvious way.  States with too few visits are pruned from 3/s,. RLDS then uses the standard value iteration  algorithm to compute the optimal policy and value function [6] for 3/s,, all using the chosen  discount factor.  5 Experimental Results  The goal of the experiments reported below is twofold: first, to confirm that our RLDS  methodology and software produce intuitively sensible policies; and second, to use the value  functions computed by the RLDS software to discover and understand correlations between  dialogue properties and performance. We have space to present only a few of our many  experiments on TOOT and ELVIS data.  Each experiment reported below involves choosing a state estimator, running RLDS using  either the TOOT or ELVIS data, and then analyzing the resulting policy and value function.  For the TOOT experiments, the reward function was obtained from a question in the user  satisfaction survey: the last turn in a dialogue receives a reward of +1 if the user indicated  that they would use the system again, a reward of 0 if the user answered "maybe", and a  reward of -1 if the user indicated that they would not use the system again. All turns other  than the last receive reward 0 (i.e., a reward is received only at the end of a dialogue). For  the ELVIS experiments, we used a summed (over several questions) user-satisfaction score  to reward the last turn in each dialogue (this score ranges between 8 and 40).  Experiment 1 (A Sensible Policy): In this initial "sanity check" experiment, we created a  state estimator for TOOT whose boolean state variables track whether the system knows the  value for the following five informational attributes: arrival city (denoted AC), departure city  (DC), departure date (DD), departure hour (DH), and whether the hour is AM or PM (AP) 3  Thus, if the dialogue so far includes a turn in which TOOT prompts the user for their depar-  ture city, and the speech recognizer matches the user utterance with "New York", the boolean  state variable GotDC? would be assigned a value of 1. Note that this ignores the actual values  of the attributes. In addition, there is another boolean variable called ConfirmedAll? that is  set to 1 if and only if the system took action ConfirmAll (which prompts the user to explicitly  verify the attribute values perceived by TOOT) and perceived a "yes" utterance in response.  Thus, the state vector is simply the binary vector  aRemember that TOOT can only track its perceptions of these attributes, since errors may have  occurred in speech recognition.  960 S. Singh, M. Kearns, D. Litman and M. Walker  [ GotAC? , GotAP? , GotDC? , GotDD? , GotDH? , ConfirmedAll? ]  Among the actions (the system utterances) available to TOOT are prompts to the user to  specify values for these informational attributes; we shall denote these actions with labels  AskDC, AskAC, AskDD, AskDH, and AskAP. The system takes several other actions that  we shall mention as they arise in our results.  The result of running RLDS was the  be taken from each state:  [0,0,0,0,0,0]: SayGreeting  [1,0,1,1,0,0]: AskDH  [0,1,0,1,1,0]: AskAll  [1,1,1,1,1,0]: ConfirmAll  following policy, where we have indicated the action to  [1,0,0,0,0,0]:  [0,0,0,1,1,0]:  [1,1,0,1,1,0]:  [1,1,1,1,1,1]:  AskDC [1,0,1,0,0,0]: AskAP  AskAP [1,0,0,1,1,0]: AskAP  AskAll [1,0,1,1,1,0]: ASkAP  Close  Thus, RLDS finds a sensible policy, always asking the user for information which it has not  already received, confirming the user's choices when it has all the necessary information, and  then presenting the closest matching train schedule and closing the dialogue (action Close).  Note that in some cases it chooses to ask the user for values for all the informational attributes  even though it has values for some of them. It is important to emphasize that this policy was  derived purely through the application of RLDS to the dialogue data, without any knowledge  of the "goal" of the system. Furthermore, the TOOT data is such that the empirical MDP  built by RLDS for this state estimator does include actions considerably less reasonable than  those chosen above from many states. Examples include confirming the values of .specific  informational attributes such as DC (since we do not represent whether such confirmations  were successful, this action would lead to infinite loops of confirmation), and requesting  values for informational attributes for which we already have values (such actions appear  in the empirical MDP due to speech recognition errors). The mere fact that RLDS was  driven to a sensible policy that avoided these available pitfalls indicates a correlation between  the chosen reward measure (whether the user would use the system again) and the intuitive  system goal of obtaining a completely specified train trip. It is interesting to note that RLDS  finds it better to confirm values for all 5 attributes when it has them, as opposed to simply  closing the dialogue without confirmation.  In a similar experiment on ELVIS, RLDS again found a sensible policy that summarizes the  user's inbox at the beginning of the dialogue, goes on to read the relevant e-mail messages  until done, and then closes.  (a)  0.2  0.24  0.2  0.2  0.18  0.16  0,14  0,12  0.08  0,08  (b)  I = Number of Information Attributes  I--5  I=l  0.05  0   1 2 3 4 5 0 1 2 3 4 5  Number of Attributes Confirmed Number of Information Attributes  Figure l: a) Role of Confirmation. b) Role of Distress Features (indicators that the dialogue is in  trouble). See description of Experiments 2 and 3 respectively in the text for details.  Experiment 2 (Role of Confirmation): Here we explore the effect of confirming with the  user the values that TOOT perceives for the informational attributes -- that is, whether the  Reinforcement Learning for Spoken Dialogue Systems 961  trade-off between the increased confidence in the utterance and the potential annoyance to  the user balances out in favor of confirmation or not (for the particular reward function we  are using). To do so, we created a simple state estimator with just two state variables. The  first variable counts the number of the informational attributes (DC, AC, etc.) that TOOT  believes it has obtained, while the second variable counts the number of these that have been  confirmed with the user. Figure 1 (a) presents the optimal value as a function of the number of  attributes confirmed. Each curve in the plot corresponds to a different setting of the first state  variable. For instance, the curve labeled with "I=3" corresponds to the states where the sys-  tem has obtained 3 informational attributes. We can make two interesting observations from  this figure. First, the value function grows roughly linearly with the number of confirmed  attributes. Second, and perhaps more startlingly, the value function has only a weak depen-  dence on the first feature -- the value for states when some number of attributes have been  confirmed seems independent of how many attributes (the system believes) have been ob-  tained. This is evident from the lack of separation between the plots for varying values of the  state variable I. In other words, our simple (and preliminary) analysis suggests that for our  reward measure, confirmed information influences the value function much more strongly  than unconfirmed information. We also repeated this experiment replacing attribute confir-  mation with thresholded speech recognition log-likelihood scores, and obtained qualitatively  similar results.  Experiment 3 (Role of Distress Features): Dialogues often contain timeouts (user silence  when system expected response), resets (user asks for current context of dialogue to be aban-  doned and the system is reinitialized), user requests for help, and other indicators that the  dialogue is potentially in trouble. Do such events correlate with low value? We created a  state estimator for TOOT that, in addition to our variable I counting informational attributes,  counted the number of such distress events in the dialogue. Figure l(b) presents the optimal  value as a function of the number of attributes obtained. Each curve corresponds to a differ-  ent number of distress features. This figure confirms that the value of the dialogue is lower  for states with a higher number of distress features.  (a)  0.?  0.6  0.S  0.4  0.3  0.2  0.1  T = Number of Tums  T<4   4<= T < 8   8 <= T< 12   1 2 3 4  Number of Information Attributes  (b)  P = Task Progress  P=3  1 2 3 4 S 6 7 8  Number of Turns divided by 4  Figure 2: a) Role of Dialogue Length in TOOT. b) Role of Dialogue Length in ELVIS. See description  of Experiment 4 in the text for details.  Experiment 4 (Role of the Dialogue Length): All other things being equal (e.g. extent of  task completion), do users prefer shorter dialogues? To examine this question, we created a  state estimator for TOOT that counts the number of informational attributes obtained (vari-  able I as in Experiment 2), and a state estimator for ELVIS that measures "task progress"  (a measure analogous to the variable I for TOOT; details omitted). In both cases, a second  variable tracks the length of the dialogue.  962 S. Singh, M. Kearns, D. Litman and M. Walker  Figure 2(a) presents the results for TOOT. It plots the optimal value as a function of the  number I of informational values; each curve corresponds to a different range of dialogue  lengths. It is immediately apparent that the longer the dialogue, the lower the value, and  that within the same length of dialogue it is better to have obtained more attributes ,t. Of  course, the effect of obtaining more attributes is weak for the longest dialogue length; these  are dialogues in which the user is struggling with the system, usually due to multiple speech  recognition errors.  Figure 2(b) presents the results for ELVIS from a different perspective. The dialogue length  is now the x-axis, while each curve corresponds to a different value of P (task progress). It is  immediately apparent that the value increases with task progress. More interestingly, unlike  TOOT, there seems to be an "optimal" or appropriate dialogue length for each level of task  progress, as seen in the inverse U-shaped curves.  Experiment 5 (Role of Initiative): One of the important questions in dialogue theory is how  to choose between system and mixed initiative strategies (cf. Section 1). Using our approach  on both TOOT and ELVIS data, we were able to confirm previous results [4, 7] showing that  system initiative has a higher value than mixed initiative.  Experiment 6 (Role of Reward Functions): To test the robustness of our framework, we  repeated Experiments 1-4 for TOOT using a new reward function based on the user's per-  ceived task completion. We found that except for a weaker correlation between number of  tums and value function, the results were basically the same across the two reward functions.  6 Conclusion  This paper presents a new RL-based framework for spoken dialogue systems. Using our  framework, we developed RLDS, a general-purpose software tool, and used it for empirical  studies on two sets of real dialogues gathered from the TOOT and ELVIS systems. Our  results showed that RLDS was able to find sensible policies, that in ELVIS there was an  "optimal" length of dialogue, that in TOOT confirmation of attributes was highly correlated  with value, that system initiative led to greater user satisfaction than mixed initiative, and  that the results were robust to changes in the reward function.  Acknowledgements: We give warm thanks to Esther Levin, David McAllester, Roberto  Pieraccini, and Rich Sutton for their many contributions to this work.  References  [ 1 ] A.W. Biermann and P.M. Long. The composition of messages in speech-graphics interactive sys-  tems. In Proceedings of the 1996 International Symposium on Spoken Dialogue. 97-100, 1996.  [2] A. L. Gorin, B. A. Parker, R. M. Sachs and J. G. Wilpon. How May I Help You. In Proceedings  of lnternational Symposium on Spoken Dialogue. 57-60, 1996.  [3] E. Levin, R. Pieraccini and W. Eckert. Learning dialogue strategies within the Markov decision  process framework. In Proc. 1EEE Workshop on Automatic Speech Recognition and Understand-  ing 1997.  [4] D. J. Litman and S. Pan. Empirically Evaluating an Adaptable Spoken Dialogue System. In Pro-  ceedings of the 7th International Conference on User Modeling 1999.  [5] S. Singh, M. Kearns, D. Litman, and M. Walker. In preparation.  [6] R. S. Sutton and A. G. Barto. ReinforcementLearning: An Introduction MIT Press, 1998.  [7] M. A. Walker, J. C. Fromet and S. Narayanan. Learning Optimal Dialogue Strategies: A Case  Study of a Spoken Dialogue Agent for Email. In Proceedings of the 36th Annual Meeting of the  Association of Computational Linguistics, COLING/ACL 98 1345-1352, 1998.  4There is no contradiction with Experiment 2 in this statement, since here we are not separating  confirmed and unconfirmed attributes.  
Policy Gradient Methods for  Reinforcement Learning with Function  Approximation  Richard S. Sutton, David McAllester, Satinder Singh, Yishay Mansour  AT&T Labs - Research, 180 Park Avenue, Florham Park, NJ 07932  Abstract  Function approximation is essential to reinforcement learning, but  the standard approach of approximating a value function and deter-  mining a policy from it has so far proven theoretically intractable.  In this paper we explore an alternative approach in which the policy  is explicitly represented by its own function approximator, indepen-  dent of the value function, and is updated according to the gradient  of expected reward with respect to the policy parameters. Williams's  REINFORCE method and actor-critic methods are examples of this  approach. Our main new result is to show that the gradient can  be written in a form suitable for estimation from experience aided  by an approximate action-value or advantage function. Using this  result, we prove for the first time that a version of policy iteration  with arbitrary differentiable function approximation is convergent to  a locally optimal policy.  Large applications of reinforcement learning (RL) require the use of generalizing func-  tion approximators such neural networks, decision-trees, or instance-based methods.  The dominant approach for the last decade has been the value-function approach, in  which all function approximation effort goes into estimating a value function, with  the action-selection policy represented implicitly as the "greedy" policy with respect  to the estimated values (e.g., as the policy that selects in each state the action with  highest estimated value). The value-function approach has worked well in many appli-  cations, but has several limitations. First, it is oriented toward finding deterministic  policies, whereas the optimal policy is often stochastic, selecting different actions with  specific probabilities (e.g., see Singh, Jaakkola, and Jordan, 1994). Second, an arbi-  trarily small change in the estimated value of an action can cause it to be, or not be,  selected. Such discontinuous changes have been identified as a key obstacle to estab-  lishing convergence assurances for algorithms following the value-function approach  (Bertsekas and Tsitsiklis, 1996). For example, Q-learning, Sarsa, and dynamic pro-  gramming methods have all been shown unable to converge to any policy for simple  MDPs and simple function approximators (Gordon, 1995, 1996; Baird, 1995; Tsit-  siklis and van Roy, 1996; Bertsekas and Tsitsiklis, 1996). This can occur even if the  best approximation is found at each step before changing the policy, and whether the  notion of "best" is in the mean-squared-error sense or the slightly different senses of  residual-gradient, temporal-difference, and dynamic-programming methods.  In this paper we explore an alternative approach to function approximation in RL.  1058 R. S. Sutton, D. McAllester, S. Singh and Y. Mansour  Rather than approximating a value function and using that to compute a determinis-  tic policy, we approximate a stochastic policy directly using an independent function  approximator with its own parameters. For example, the policy might be represented  by a neural network whose input is a representation of the state, whose output is  action selection probabilities, and whose weights are the policy parameters. Let 0  denote the vector of policy parameters and p the performance of the corresponding  policy (e.g., the average reward per step). Then, in the policy gradient approach, the  policy parameters are updated approximately proportional to the gradient:  Op  A  a, (1)  where a is a positive-definite step size. If the above can be achieved, then 0 can  usually be assured to converge to a locally optimal policy in the performance measure  p. Unlike the value-function approach, here small changes in 0 can cause only small  changes in the policy and in the state-visitation distribution.  In this paper we prove that an unbiased estimate of the gradient (1) can be obtained  from experience using an approximate value function satisfying certain properties.  Williams's (1988, 1992) REINFORCE algorithm also finds an unbiased estimate of  the gradient, but without the assistance of a learned value function. REINFORCE  learns much more slowly than RL methods using value functions and has received  relatively little attention. Learning a value function and using it to reduce'the variance  of the gradient estimate appears to be essential for rapid learning. Jaakkola, Singh  and Jordan (1995) proved a result very similar to ours for the special case of function  approximation corresponding to tabular POMDPs. Our result strengthens theirs and  generalizes it to arbitrary differentiable function approximators. Konda and Tsitsiklis  (in prep.) independently developed a very simialr result to ours. See also Baxter and  Bartlett (in prep.) and Marbach and Tsitsiklis (1998).  Our result also suggests a way of proving the convergence of a wide variety of algo-  rithms based on "actor-critic" or policy-iteration architectures (e.g., Barto, Sutton,  and Anderson, 1983; Sutton, 1984; Kimura and Kobayashi, 1998). In this paper we  take the first step in this direction by proving for the first time that a version of  policy iteration with general differentiable function approximation is convergent to  a locally optimal policy. Baird and Moore (1999) obtained a weaker but superfi-  cially similar result for their VAPS family of methods. Like policy-gradient methods,  VAPS includes separately parameterized policy and value functions updated by gra-  dient methods. However, VAPS methods do not climb the gradient of performance  (expected long-term reward), but of a measure combining performance and value-  function accuracy. As a result, VAPS does not converge to a locally optimal policy,  except in the case that no weight is put upon value-function accuracy, in which case  VAPS degenerates to REINFORCE. Similarly, Gordon's (1995) fitted value iteration  is also convergent and value-based, but does not find a locally optimal policy.  I Policy Gradient Theorem  We consider the standard reinforcement learning framework (see, e.g., Sutton and  Barto, 1998), in which a learning agent interacts with a Markov decision process  (MDP). The state, action, and reward at each time t 6 {0, 1, 2,...} are denoted st   $, at  .A, and rt   respectively. The environment's dynamics are characterized by  state transition probabilities, Pa s, = Pr {St+l = s' I st = s, at = a}, and expected re-  wards 7Z] = E {rt+l I st = s, at = a}, Vs, s   $, a  .A. The agent's decision making  procedure at each time is characterized by a policy, r(s, a, 0) = Pr {at = alst = s, 0},  Vs  $, a  jI, where 0  t, for I << I$1, is a parameter vector. We assume that r  is diffentiable with respect to its parameter, i.e., that o exists. We also usually  write just r(s,a) for r(s,a,O).  Policy Gradient Methods for RL with Function Approximation 1059  With function approximation, two ways of formulating the agent's objective are use-  ful. One is the average reward formulation, in which policies are ranked according to  their long-term expected reward per step, p(r):  p(r) = lim 1-E{rl + r2 +... + rn [r} = Z d(s) Z r(s,a)7,  where d  (s) = limt_ Pr (st = s[so, r) is the stationary distribution of states under  r, which we assume exists and is independent of so for all policies. In the average  reward formulation, the value of a state-action pair given a policy is defined as  ao=a,r ), Vse$,aeA.  t----1  The second formulation we cover is that in which there is a designated start state  so, and we care only about the long-term reward obtained from it. We will give our  results only once, but they will apply to this formulation as well under the definitions  p(r) = E 7t-lrt SO,r and Qr(s,a) = E 7k-lrt+k st = s, at = a,r .  t=l Xk=l  where 7 6 [0, 1] is a discount rate (7 = I is allowed only in episodic tasks). In this  formulation, we define dr(s) as a discounted weighting of states encountered starting  at so and then following r: dr(s) = --o 7 tPr (st- S[So,').  Our first result concerns the gradient of the performance metric with respect to the  policy parameter:  Theorem I (Policy Gradient). For any MDP, in either the average-reward or  start-state formulations,  Op Or( s, a)  oo = a's) oo (2)  Proof: See the appendix.  This way of expressing the gradient was first discussed for the average-reward formu-  lation by Marbach and Tsitsiklis (1998), based on a related expression in terms of the  state-value function due to Jaakkola, Singh, and Jordan (1995) and Cao and Chen  (1997). We extend their results to the start-state formulation and provide simpler  and more direct proofs. Williams's (1988, 1992) theory of REINFORCE algorithms  can also be viewed as implying (2). In any event, the key aspect of both expressions  for the gradient is that their are no terms of the form oa._s: the effect of policy  changes on the distribution of states does not appear. This is convenient for approxi-  mating the gradient by sampling. For example, if s was sampled from the distribution  (8'a)O(s,a) would be an unbiased estimate of  obtained by following r, then -]-a oe '  -e Of course, Q (s, a) is also not normally known and must be estimated. One ap-  00'  proach is to use the actual returns, Rt oo oo  -- Ek=i l't+k -- p(w) (or Re = Ek=l vk-iwt+k  in the start-state formulation) as an approximation for each Qr(st, at). This leads to  Williams's episodic REINFORCE algorithm, A0t oc o(,,)Re  (the 1  corrects for the oversampling of actions preferred by r), which is known to follow   in expected value (Williams, 1988, 1992).  2 Policy Gradient with Approximation  Now consider the case in which Q is approximated by a learned function approxima-  tor. If the approximation is sufficiently good, we might hope to use it in place of Q  1060 R. S. Sutton, D. Mc,41lester, S. Singh and Y. Mansour  in (2) and still point roughly in the direction of the gradient. For example, Jaakkola,  Singh, and Jordan (1995) proved that for the special case of function approximation  arising in a tabular POMDP one could assure positive inner product with the gra-  dient, which is sufficient to ensure improvement for moving in that direction. Here  we extend their result to general function approximation and prove equality with the  gradient.  Let fw: $ x j[ - R be our approximation to Qx, with parameter w. It is natural  a rtr / s  to learn fw by following rr and updating w by a rule such as Awt o<  [  t, at) -   'fw(8"aO where Or(st, at) is some unbiased  f(st,at)]   [O(st,at)- f(st,-tj o ,  estimator of Qr(st, at), perhaps Pu. When such a process has converged to a local  optimum, then   d'r(s) E '(s'a)[ Q'r(s'a) - fu,(s,a)] Ofu,(s,a)  Ow - O.  (s)  Theorem 2 (Policy Gradient with Function Approximation). If fw satisfies  (3) and is compatible with the policy parameterization in the sense that I  Ofu,(s,a) O'(s,a) I  Ow O0 r(s,a) '  then  Op &r( s, a)  00 = d(s)  S(s,a).  (4)  (5)  Proof: Combining (3) and (4) gives  &r(s,a)  oo - = 0  (6)  which tells us that the error in fu(s, a) is orthogonal to the gradient of the policy  parameterization. Because the expression above is zero, we can subtract it from the  policy gradient theorem (2) to yield  Op &r( s, a)  O0 = y" dr(s) y" O0 qr(s'a) - E dr(s) Y O'(s,a)  00 [(s,)- w(s,)]  &r(s,a) [Q=(s,a) - Q=(s,a) + f,,(s,a)]  &r( s, a)  = d(s) oo /(s,a). q..V.  3 Application to Deriving Algorithms and Advantages  Given a policy parameterization, Theorem 2 can be used to derive an appropriate  form for the value-function parameterization. For example, consider a policy that is  a Gibbs distribution in a linear combination of features:  Eb eOTq '  Vs 6 $, s  A,  Tsitsiklis (personal communication) points out that f being linear in the features given  on the righthand side may be the only way to satisfy this condition.  Policy Gradient Methods for RL with Function Approximation 1061  where each qbsa is an/-dimensional feature vector characterizing state-action pair s, a.  Meeting the compatibility condition (4) requires that  Ofw(s,a) Or(s,a) 1 y.r(s,b)qbsb,  0w = b  so that the natural parameterization of fw is  In other words, fw must be linear in the same features as the policy, except normalized  to be mean zero for each state. Other algorithms can easily be derived for a variety  of nonlinear policy parameterizations, such as multi-layer backpropagation networks.  The careful reader will have noticed that the form given above for f requires  that it have zero mean for each state: -.ar(s,a)fw(s,a) = O, s  $. In this  sense it is better to think of fw as an approximation of the advantage function,  Ar(s,a) = Qr(s,a) - Vr(S) (much as in Baird, 1993), rather than of Qx. Our  convergence requirement (3) is really that fw get the relative value of the ac-  tions correct in each state, not the absolute value, nor the variation from state to  state. Our results can be viewed as a justification for the special status of advan-  tages as the target for value function approximation in RL. In fact, our (2), (3),  and (5), can all be generalized to include an arbitrary function of state added to  the value function or its approximation. For example, (5) can be generalized to  o_ _ sdr(s) --a O [fw(s,a) + v(s)] ,where v' $ - R is an arbitrary function.  o0--  (This follows immediately because a -0'a -- 0, s  $.) The choice of v does not  affect any of our theorems, but can substantially affect the variance of the gradient  estimators. The issues here are entirely analogous to those in the use of reinforce-  ment baselines in earlier work (e.g., Williams, 1992; Dayan, 1991; Sutton, 1984). In  practice, v should presumably be set to the best available approximation of V . Our  results establish that that approximation process can proceed without affecting the  expected evolution of f and r.  4 Convergence of Policy Iteration with Function Approximation  Given Theorem 2, we can prove for the first time that a form of policy iteration with  function approximation is convergent to a locally optimal policy.  Theorem 3 (Policy Iteration with Function Approximation). Let rr  and fw be any differentiable function approximators for the policy and value  function respectively that satisfy the compatibility condition (4) and for which  max0,,.,i,j I ooaoj I < B < oo. Let {rk}=o be any step-size sequence such that  limkm ak = 0 and }-'--k ak = oo. Then, for any MDP with bounded rewards, the  sequence {p(rrk)}k=o , defined by any 0o, rk = r(.,., Ok), and  wk = w such that y.d(s) y.rk(s,a)[Q'*(s,a) - f(s,a)]Of- 'a) = 0  Ok+ 1 ---- 0 k q-Otk ydW($) y o7rk($'a)  OO  converges such that limk_m o__ _- 0.  Proof: Our Theorem 2 assures that the 0k update is in the direction of the gradient.  (8'a) and on the MDP's rewards together assure us that 0____  The bounds on oo, ooj ao,ao  1062 R. S. Sutton, D. McAllester, S. Singh and Y. Mansour  is also bounded. These, together with the step-size requirements, are the necessary  conditions to apply Proposition 3.5 from page 96 of Bertsekas and Tsitsildis (1996),  which assures convergence to a local optimum. Q.E.D.  Acknowledgements  The authors wish to thank Martha Steenstrup and Doina Precup for comments, and Michael  Kearns for insights into the notion of optimal policy under function approximation.  References  Baird, L. C. (1993). Advantage Updating. Wright Lab. Technical Report WL-TR-93-1146.  Baird, L. C. (1995). Residual algorithms: Reinforcement learning with function approxima-  tion. Proc. of the Twelfth Int. Conf. on Machine Learning, pp. 30-37. Morgan Kaufmann.  Baird, L. C., Moore, A. W. (1999). Gradient descent for general reinforcement learning.  NIPS 11. MIT Press.  Barto, A. G., Sutton, R. S., Anderson, C. W. (1983). Neuronlike elements that can solve  difficult learning control problems. IEEE Trans. on Systems, Man, and Cybernetics 3:835.  Baxter, J., Bartlett, P. (in prep.) Direct gradient-based reinforcement learning: I. Gradient  estimation algorithms.  Bertsekas, D. P., Tsitsiklis, J. N. (1996). Neuro-Dynamic Programming. Athena Scientific.  Cao, X.-R., Chen, H.-F. (1997). Perturbation realization, potentials, and sensitivity analysis  of Markov Processes, IEEE Trans. on Automatic Control J(10):1382-1393.  Dayan, P. (1991). Reinforcement comparison. In D. S. Touretzky, J. L. Elman, T. J. Se-  jnowski, and G. E. Hinton (eds.), Connectionist Models: Proceedings of the 1990 Summer  School, pp. 45-51. Morgan Kaufmann.  Gordon, G. J. (1995). Stable function approximation in dynamic programming. Proceedings  of the Twelfth Int. Conf. on Machine Learning, pp. 261-268. Morgan Kaufmann.  Gordon, G. J. (1996). Chattering in SARSA(A). CMU Learning Lab Technical Report.  Jaakkola, T., Singh, S. P., Jordan, M. I. (1995) Reinforcement learning algorithms for par-  tially observable Markov decision problems, NIPS 7, pp. 345-352. Morgan Kaufman.  Kimura, H., Kobayashi, S. (1998). An analysis of actor/critic algorithms using eligibility  traces: Reinforcement learning with imperfect value functions. Proc. ICML-98, pp. 278-286.  Konda, V. R., Tsitsiklis, J. N. (in prep.) Actor-critic algorithms.  Marbach, P., Tsitsiklis, J. N. (1998) Simulation-based optimization of Markov reward pro-  cesses, technical report LIDS-P-2411, Massachusetts Institute of Technology.  Singh, S. P., Jaakkola, T., Jordan, M. I. (1994). Learning without state-estimation in  partially observable Markovian decision problems. Proc. ICML-9J, pp. 284-292.  Sutton, R. S. (1984). Temporal Credit Assignment in Reinforcement Learning. Ph.D. thesis,  University of Massachusetts, Amherst.  Sutton, R. S., Barto, A. G. (1998). Reinforcement Learning: An Introduction. MIT Press.  Tsitsiklis, J. N. Van Roy, B. (1996). Feature-based methods for large scale dynamic pro-  gramming. Machine Learning 22:59-94.  Williams, R. J. (1988). Toward a theory of reinforcement-learning connectionist systems.  Technical Report NU-CCS-88-3, Northeastern University, College of Computer Science.  Williams, R. J. (1992). Simple statistical gradient-following algorithms for connectionist  reinforcement learning. Machine Learning 8:229-256.  Appendix: Proof of Theorem 1  We prove the theorem first for the average-reward formulation and then for the start-  state formulation.  OV' (s) 0  O0 -- OOZW(s,a)Q'(s,a) Vse$  a  : 'Ols, a'ls,  O0  Policy Gradient Methods for RL with Function Approximation 1063  Therefore,  0-7 = [ oo q'(,,a) + (,,a)   S t  Summing both sides over the stationary distribution d ,  but since d  is stationary,  Op  $  O(s,a)  ,, ou(s ')  = d(s) oo q(s,a) + d (s    $ a S t  $  0__ 0(s,  o0 = d(s) o0 )(s,). q.s..  For the sta-state formulation:  OVa(s) df 0  oo = oo(s,)(s,)  =  'O(s,)(s,) + (s,)  o0 (s,) + (s,) ,p2,, U(s') (7)   O(,a)  = (3,,) oo  x k=0 a  aer several steps of unrolling (7), where Pr(s  x, k, ) is the probability of going  kom state s to state x in k steps under policy . It is then immiate that  t=l   O(s,a) -(s,a)  s k=O a  0(s, a)  = :(s) oo q(s,a).  Q.E.D.  
State Abstraction in MAXQ Hierarchical  Reinforcement Learning  Thomas G. Dietterich  Department of Computer Science  Oregon State University  Corvallis, Oregon 97331-3202  tgd@cs. orst. edu  Abstract  Many researchers have explored methods for hierarchical reinforce-  ment learning (RL) with temporal abstractions, in which abstract  actions are defined that can perform many primitive actions before  terminating. However, little is known about learning with state ab-  stractions, in which aspects of the state space are ignored. In previ-  ous work, we developed the MAXQ method for hierarchical RL. In  this paper, we define five conditions under which state abstraction  can be combined with the MAXQ value function decomposition.  We prove that the MAXQ-Q learning algorithm converges under  these conditions and show experimentally that state abstraction is  important for the successful application of MAXQ-Q learning.  1 Introduction  Most work on hierarchical reinforcement learning has focused on temporal abstrac-  tion. For example, in the Options framework [1, 2], the programmer defines a set of  macro actions ("options") and provides a policy for each. Learning algorithms (such  as semi-Markov Q learning) can then treat these temporally abstract actions as if  they were primitives and learn a policy for selecting among them. Closely related  is the HAM framework, in which the programmer constructs a hierarchy of finite-  state controllers [3]. Each controller can include non-deterministic states (where the  programmer was not sure what action to perform). The HAMQ learning algorithm  can then be applied to learn a policy for making choices in the non-deterministic  states. In both of these approaches--and in other studies of hierarchical RL (e.g.,  [4, 5, 6])--each option or finite state controller must have access to the entire state  space. The one exception to this--the Feudal-Q method of Dayan and Hinton [7]--  introduced state abstractions in an unsafe way, such that the resulting learning  problem was only partially observable. Hence, they could not provide any formal  results for the convergence or performance of their method.  Even a brief consideration of human-level intelligence shows that such methods can-  not scale. When deciding how to walk from the bedroom to the kitchen, we do not  need to think about the location of our car. Without state abstractions, any RL  method that learns value functions must learn a separate value for each state of the  State Abstraction in MAXQ Hierarchical Reinforcement Learning 995  world. Some argue that this can be solved by clever value function approximation  methods--and there is some merit in this view. In this paper, however, we explore  a different approach in which we identify aspects of the MDP that permit state ab-  stractions to be safely incorporated in a hierarchical reinforcement learning method  without introducing function approximations. This permits us to obtain the first  proof of the convergence of hierarchical RL to an optimal policy in the presence of  state abstraction.  We introduce these state abstractions within the MAXQ flamework [8], but the  basic ideas are general. In our previous work with MAXQ, we briefly discussed state  abstractions, and we employed them in our experiments. However, we could not  prove that our algorithm (MAXQ-Q) converged with state abstractions, and we did  not have a usable characterization of the situations in which state abstraction could  be safely employed. This paper solves these problems and in addition compares the  effectiveness of MAXQ-Q learning with and without state abstractions. The results  show that state abstraction is very important, and in most cases essential, to the  effective application of MAXQ-Q learning.  2 The MAXQ Framework  Let M be a Markov decision problem with states S, actions A, reward function  R(sl s, a) and probability transition function P(sls, a). Our results apply in both  the finite-horizon undiscounted case and the infinite-horizon discounted case. Let  {M0,...,Mn} be a set of subtasks of M, where each subtask Mi is defined by a  termination predicate Ti and a set of actions Ai (which may be other subtasks or  primitive actions from A). The "goal" of subtask Mi is to move the environment into  a state such that Ti is satisfied. (This can be refined using a local reward function  to express preferences among the different states satisfying Ti [8], but we omit this  refinement in this paper.) The subtasks of M must form a DAG with a single "root"  node--no subtask may invoke itself directly or indirectly. A hierarchical policy is  a set of policies r - {r0,...,rn}, one for each subtask. A hierarchical policy  is executed using standard procedure-call-and-return semantics, starting with the  root task M0 and unfolding recursively until primitive actions are executed. When  the policy for Mi is invoked in state s, let P(s,NIs,i) be the probability that it  terminates in state s  after executing N primitive actions. A hierarchical policy is  recursively optimal if each policy 7ri is optimal given the policies of its descendants  in the DAG.  Let V(i, s) be the value function for subtask i in state s (i.e., the value of following  some policy starting in s until we reach a state s  satisfying Ti(s)). Similarly, let  Q(i,s,j) be the Q value for subtask i of executing child action j in state s and  then executing the current policy until termination. The MAXQ value function  decomposition is based on the observation that each subtask Mi can be viewed as a  Semi-Markov Decision problem in which the reward for performing action j in state  s is equal to V(j, s), the value function for subtask j in state s. To see this, consider  the sequence of rewards rt that will be received when we execute child action j and  then continue with subsequent actions according to hierarchical policy r:  Q(i,s,j) = E{rt + "/rt+l + "/2rt+2 +...[st -- s,7r}  The macro action j will execute for some number of steps N and then return. Hence,  we can partition this sum into two terms:  Q(i,s,j) = E 3' t+ + 3'rt+ st = s,7r  u----0 u----N  996 T. G. Dietterich  The first term is the discounted sum of rewards until subtask j terminates--V(j, s).  The second term is the cost of finishing subtask i after j is executed (discounted  to the time when j is initiated). We call this second term the completion function,  and denote it C(i, s, j). We can then write the Bellman equation as  Q(i,s,j) = y P(s',N[s,j) . [V(j,s) +'V maxQ(i,s',j')]  j,  s',N  = V(j,s) +C(i,s,j)  To terminate this recursion, define V(a, s) for a primitive action a to be the expected  reward of performing action a in state s.  The MAXQ-Q learning algorithm is a simple variation of Q learning in which at  subtask Mi, state s, we choose a child action j and invoke its (current) policy. When  it returns, we observe the resulting state s  and the number of elapsed time steps  N and update C(i, s, j) according to  C(i, s,j) := (1- (t)C(i, s,j) + (t ' 'N[ma,ax V(a', s') + C(i, s',a')].  To prove convergence, we require that the exploration policy executed during learn-  ing be an ordered GLIE policy. An ordered policy is a policy that breaks Q-value  ties among actions by preferring the action that comes first in some fixed ordering.  A GLIE policy [9] is a policy that (a) executes each action infinitely often in every  state that is visited infinitely often and (b) converges with probability 1 to a greedy  policy. The ordering condition is required to ensure that the recursively optimal  policy is unique. Without this condition, there are potentially many different re-  cursively optimal policies with different values, depending on how ties are broken  within subtasks, subsubtasks, and so on.  Theorem I Let M = (S, A, P, R) be either an episodic MDP for which all de-  terministic policies are proper or a discounted infinite horizon MDP with discount  factor '. Let H be a DAG defined over subtasks (Mo,... ,Mk). Let (t(i)  0 be a  sequence of constants for each subtask Mi such that  T T  lim y (t(i) = o and lim y (t2(i) ( oe (1)  T- T-  t=l  Let rx (i, s) be an ordered GLIE policy at each subtask Mi and state s and assume  that [Vt(i, s)[ and ICt(i,s,a)[ are bounded for all t, i, s, and a. Then with probability  1, algorithm MAXQ-Q converges to the unique recursively optimal policy for M  consistent with H and rx.  Proof.' (sketch) The proof is based on Proposition 4.5 from Bertsekas and Tsit-  siklis [10] and follows the standard stochastic approximation argument due to [11]  generalized to the case of non-stationary noise. There are two key points in the  proof. Define Pt (d, Nls, j) to be the probability transition function that describes  the behavior of executing the current policy for subtask j at time t. By an inductive  argument, we show that this probability transition function converges (w.p. 1) to  the probability transition function of the recursively optimal policy for j. Second,  we show how to convert the usual weighted max norm contraction for Q into a  weighted max norm contraction for C. This is straightforward, and completes the  proof.  What is notable about MAXQ-Q is that it can learn the value functions of all  subtasks simultaneously--it does not need to wait for the value function for subtask  j to converge before beginning to learn the value function for its parent task i. This  gives a completely online learning algorithm with wide applicability.  State Abstraction in MAXQ Hierarchical Reinforcement Learning 997  4  3  2  1  0  R G  0 1 2 3 4  own J  Figure 1: Left: The Taxi Domain (taxi at row 3 column 0). Right: Task Graph.  3 Conditions for Safe State Abstraction  To motivate state abstraction, consider the simple Taxi Task shown in Figure 1.  There are four special locations in this world, marked as R(ed), B(lue), G(reen),  and Y(ellow). In each episode, the taxi starts in a randomly-chosen square. There  is a passenger at one of the four locations (chosen randomly), and that passenger  wishes to be transported to one of the four locations (also chosen randomly). The  taxi must go to the passenger's location (the "source"), pick up the passenger, go  to the destination location (the "destination"), and put down the passenger there.  The episode ends when the passenger is deposited at the destination location.  There are six primitive actions in this domain: (a) four navigation actions that  move the taxi one square North, South, East, or West, (b) a Pickup action, and (c)  a Putdown action. Each action is deterministic. There is a reward of -1 for each  action and an additional reward of +20 for successfully delivering the passenger.  There is a reward of -10 if the taxi attempts to execute the Putdown or Pickup  actions illegally. If a navigation action would cause the taxi to hit a wall, the action  is a no-op, and there is only the usual reward of -1.  This task has a hierarchical structure (see Fig. 1) in which there are two main  sub-tasks: Get the passenger (Get) and Deliver the passenger (Put). Each of these  subtasks in turn involves the subtask of navigating to one of the four locations  (Navigate(t); where t is bound to the desired target location) and then performing  a Pickup or Putdown action. This task illustrates the need to support both tem-  poral abstraction and state abstraction. The temporal abstraction is obvious--for  example, Get is a temporally extended action that can take different numbers of  steps to complete depending on the distance to the target. The top level policy (get  passenger; deliver passenger) can be expressed very simply with these abstractions.  The need for state abstraction is perhaps less obvious. Consider the Get subtask.  While this subtask is being solved, the destination of the passenger is completely  irrelevant--it cannot affect any of the nagivation or pickup decisions. Perhaps more  importantly, when navigating to a target location (either the source or destination  location of the passenger), only the taxi's location and identity of the target location  are important. The fact that in some cases the taxi is carrying the passenger and  in other cases it is not is irrelevant.  We now introduce the five conditions for state abstraction. We will assume that the  state s of the MDP is represented as a vector of state variables. A state abstraction  can be defined for each combination of subtask Mi and child action j by identifying  a subset X of the state variables that are relevant and defining the value function  and the policy using only these relevant variables. Such value functions and policies  998 T. G. Dietterich  are said to be abstract.  The first two conditions involve eliminating irrelevant variables within a subtask of  the MAXQ decomposition.  Condition 1: Subtask Irrelevance. Let Mi be a subtask of MDP M. A set  of state variables Y is irrelevant to subtask i if the state variables of M can be  partitioned into two sets X and Y such that for any stationary abstract hierarchical  policy r executed by the descendants of Mi, the following two properties hold: (a)  the state transition probability distribution P(s , NIs , j) for each child action j of  Mi can be factored into the product of two distributions:  P(x',y',Nlx, y,j ) = P(x',NIx,j )  P(y'lx,y,j), (2)  where x and x  give values for the variables in X, and y and y give values for the  variables in Y; and (b) for any pair of states Sl = (x, yl) and s2 = (x, y2) and any  child action j, V(j,s) - V(j,s).  In the Taxi problem, the source and destination of the passenger are irrelevant to  the Navigate(t) subtask--only the target t and the current taxi position are relevant.  The advantages of this form of abstraction are similar to those obtained by Boutilier,  Dearden and Goldszmidt [12] in which belief network models of actions are exploited  to simplify value iteration in stochastic planning.  Condition :2: Leaf Irrelevance. A set of state variables Y is irrelevant for a  primitive action a if for any pair of states s and s that differ only in their values  for the variables in Y,   P($l$1,a)R($l$1, a) =  P($]$2,a)R(sl$2,a).  81 $  This condition is satisfied by the primitive actions North, South, East, and West in  the taxi task, where all state variables are irrelevant because R is constant.  The next two conditions involve "funnel" actions--macro actions that move the  environment from some large number of possible states to a small number of re-  sulting states. The completion function of such subtasks can be represented using  a number of values proportional to the number of resulting states.  Condition 3: Result Distribution Irrelevance (Undiscounted case.) A set  of state variables Yj is irrelevant for the result distribution of action j if, for all  abstract policies r executed by Mj and its descendants in the MAXQ hierarchy, the  following holds: for all pairs of states s and s2 that differ only in their values for  the state variables in Yj,  V s' P'(s'lsl,j ) = P'(s'ls2,j ).  Consider, for example, the Get subroutine under an optimal policy for the taxi  task. Regardless of the taxi's position in state s, the taxi will be at the passenger's  starting location when Get finishes executing (i.e., because the taxi will have just  completed picking up the passenger). Hence, the taxi's initial position is irrelevant  to its resulting position. (Note that this is only true in the undiscounted setting--  with discounting, the result distributions are not the same because the number of  steps N required for Get to finish depends very much on the starting location of the  taxi. Hence this form of state abstraction is rarely useful for cumulative discounted  reward.)  Condition 4: Termination. Let Mj be a child task of Mi with the property  that whenever Mj terminates, it causes Mi to terminate too. Then the completion  State Abstraction in MAXQ Hierarchical Reinforcement Learning 999  cost C(i, s, j) - 0 and does not need to be represented. This is a particular kind of  funnel action--it funnels all states into terminal states for Mi.  For example, in the Taxi task, in all states where the taxi is holding the passenger,  the Put subroutine will succeed and result in a terminal state for Root. This is  because the termination predicate for Put (i.e., that the passenger is at his or her  destination location) implies the termination condition for Root (which is the same).  This means that C(Root, s, Put) is uniformly zero, for all states s where Put is not  terminated.  Condition 5: Shielding. Consider subtask Mi and let s be a state such that  for all paths from the root of the DAG down to Mi, there exists a subtask that is  terminated. Then no C values need to be represented for subtask Mi in state s,  because it can never be executed in s.  In the Taxi task, a simple example of this arises in the Put task, which is terminated  in all states where the passenger is not in the taxi. This means that we do not need  to represent C(Root, s, Put) in these states. The result is that, when combined  with the Termination condition above, we do not need to explicitly represent the  completion function for Put at all!  By applying these abstraction conditions to the Taxi task, the value function can  be represented using 632 values, which is much less than the 3,000 values required  by fiat Q learning. Without state abstractions, MAXQ requires 14,000 values!  Theorem 2 (Convergence with State Abstraction) Let H be a MAXQ task  graph that incorporates the five kinds of state abstractions defined above. Let rx be  an ordered GLIE exploration policy that is abstract. Then under the same condi-  tions as Theorem 1, MAXQ-Q converges with probability i to the unique recursively  optimal policy 7r defined by rx and H.  Proof: (sketch) Consider a subtask Mi with relevant variables X and two ar-  bitrary states (x, y) and (x, y2). We first show that under the five abstraction  conditions, the value function of r can be represented using C(i, x, j) (i.e., ignor-  ing the y values). To learn the values of C(i,x,j) = Yx,,v P(x',N[x,j)V(i,x'), a  Q-learning algorithm needs samples of x  and N drawn according to P(x , Nix, j).  The second part of the proof involves showing that regardless of whether we execute  j in state (x,y) or in (x, y2), the resulting x  and N will have the same distribu-  tion, and hence, give the correct expectations. Analogous arguments apply for leaf  irrelevance and V(a, x). The termination and shielding cases are easy.  4 Experimental Results  We implemented MAXQ-Q for a noisy version of the Taxi domain and for Kael-  bling's HDG navigation task [5] using Boltzmann exploration. Figure 2 shows the  performance of flat Q and MAXQ-Q with and without state abstractions on these  tasks. Learning rates and Boltzmann cooling rates were separately tuned to opti-  mize the performance of each method. The results show that without state abstrac-  tions, MAXQ-Q learning is slower to converge than flat Q learning, but that with  state abstraction, it is much faster.  5 Conclusion  This paper has shown that by understanding the reasons that state variables are  irrelevant, we can obtain a simple proof of the convergence of MAXQ-Q learning  1000 T. G. Dietterich  200  MAXQ-t-Absmction  -200  -400 'Flat Q  -600  0 20000 40000 60000 8000 100000 120000 140000 160000  Primitive Actions  -2O  -I00  -120  -140  MAXQ+Abtracfion '  200000 400000 600000 gOOOO0 le+06 1.2e+06 I .'-06  Primitive Actions  Figure 2: Comparison of MAXQ-Q with and without state abstraction to fiat Q learning  on a noisy taxi domain (left) and Kaelbling's HDG task (right). The horizontal axis gives  the number of primitive actions executed by each method. The vertical axis plots the  average of 100 separate runs.  under state abstraction. This is much more fruitful than previous efforts based  only on weak notions of state aggregation [10], and it suggests that future research  should focus on identifying other conditions that permit safe state abstraction.  References  [1] D. Precup and R. S. Sutton, "Multi-time models for temporally abstract planning,"  in NIPS10, The MIT Press, 1998.  [2] R. S. Sutton, D. Precup, and S. Singh, "Between MDPs and Semi-MDPs: Learn-  ing, planning, and representing knowledge at multiple temporal scales," tech. rep.,  Univ. Mass., Dept. Comp. Inf. Sci., Amherst, MA, 1998.  [3] R. Parr and S. Russell, "Reinforcement learning with hierarchies of machines," in  NIPS-10, The MIT Press, 1998.  [4] S. P. Singh, "Transfer of learning by composing solutions of elemental sequential  tasks," Machine Learning, vol. 8, p. 323, 1992.  [5] L. P. Kaelbling, "Hierarchical reinforcement learning: Preliminary results," in Pro-  ceedings ICML-10, pp. 167-173, Morgan Kaufmann, 1993.  [6] M. Hauskrecht, N. Meuleau, C. Boutilier, L. Kaelbling, and T..Dean, "Hierarchical  solution of Markov decision processes using macro-actions," tech. rep., Brown Univ.,  Dept. Comp. Sci., Providence, RI, 1998.  [7] P. Dayan and G. Hinton, "Feudal reinforcement learning," in NIPS-5, pp. 271-278,  San Francisco, CA: Morgan Kaufmann, 1993.  [8] T. G. Dietterich, "The MAXQ method for hierarchical reinforcement learning," in  ICML-15, Morgan Kaufmann, 1998.  [9] S. Singh, T. Jaakkola, M. L. Littman, and C. Szpesvari, "Convergence results  for single-step on-policy reinforcement-learning algorithms," tech. rep., Univ. Col.,  Dept. Comp. Sci., Boulder, CO, 1998.  [10] D. P. Bertsekas and J. N. Tsitsiklis, Neuro-Dynamic Programming. Belmont, MA:  Athena Scientific, 1996.  [11] T. Jaakkola, M. I. Jordan, and S. P. Singh, "On the convergence of stochastic iterative  dynamic programming algorithms," Neut. Comp., vol. 6, no. 6, pp. 1185-1201, 1994.  [12] C. Boutilier, R. Dearden, and M. Goldszmidt, "Exploiting structure in policy con-  struction," in Proceedings IJCAI-95, pp. 1104-1111, 1995.  
Robust Neural Network Regression for Offiine  and Online Learning  Thomas Briege!*  Siemens AG, Corporate Technology  D-81730 Munich, Germany  thomas.briegel@mchp.siemens.de  Volker Tresp  Siemens AG, Corporate Technology  D-81730 Munich, Germany  volker. tresp@mchp.siemens.de  Abstract  We replace the commonly used Gaussian noise model in nonlinear  regression by a more flexible noise model based on the Student-t-  distribution. The degrees of freedom of the t-distribution can be chosen  such that as special cases either the Gaussian distribution or the Cauchy  distribution are realized. The latter is commonly used in robust regres-  sion. Since the t-distribution can be interpreted as being an infinite mix-  ture of Gaussians, parameters and hyperparameters such as the degrees  of freedom of the t-distribution can be learned from the data based on an  EM-leaming algorithm. We show that modeling using the t-distribution  leads to improved predictors on real world data sets. In particular, if  outliers are present, the t-distribution is superior to the Gaussian noise  model. In effect, by adapting the degrees of freedom, the system can  "learn" to distinguish between outliers and non-outliers. Especially for  online learning tasks, one is interested in avoiding inappropriate weight  changes due to measurement outliers to maintain stable online learn-  ing capability. We show experimentally that using the t-distribution as  a noise model leads to stable online learning algorithms and outperforms  state-of-the art online learning methods like the extended Kalman filter  algorithm.  1 INTRODUCTION  A commonly used assumption in nonlinear regression is that targets are disturbed by inde-  pendent additive Gaussian noise. Although one can derive the Gaussian noise assumption  based on a maximum entropy approach, the main reason for this assumption is practica-  bility: under the Gaussian noise assumption the maximum likelihood parameter estimate  can simply be found by minimization of the squared error. Despite its common use it is far  from clear that the Gaussian noise assumption is a good choice for many practical prob-  lems. A reasonable approach therefore would be a noise distribution which contains the  Gaussian as a special case but which has a tunable parameter that allows for more flexible  distributions. In this paper we use the Student-t-distribution as a noise model which con-  tains two free parameters - the degrees of freedom , and a width parameter o -2. A nice  feature of the t-distribution is that if the degrees of freedom , approach infinity, we recover  the Gaussian noise model. If , < c we obtain distributions which are more heavy-tailed  than the Gaussian distribution including the Cauchy noise model with , - 1. The latter  * Now with McKinsey & Company, Inc.  408 T. Briegel and V. Tresp  is commonly used for robust regression. The first goal of this paper is to investigate if the  additional free parameters, e.g. v, lead to better generalization performance for real world  data sets if compared to the Gaussian noise assumption with v = oc. The most common  reason why researchers depart from the Gaussian noise assumption is the presence of out-  liers. Outliers are errors which occur with low probability and which are not generated by  the data-generation process that is subject to identification. The general problem is that a  few (maybe even one) outliers of high leverage are sufficient to throw the standard Gaus-  sian error estimators completely off-track (Rousseeuw & Leroy, 1987). In the second set of  experiments we therefore compare how the generalization performance is affected by out-  liers, both for the Gaussian noise assumption and for the t-distribution assumption. Dealing  with outliers is often of critical importance for online learning tasks. Online learning is of  great interest in many applications exhibiting non-stationary behavior like tracking, sig-  nal and image processing, or navigation and fault detection (see, for instance the NIPS*98  Sequential Learning Workshop). Here one is interested in avoiding inappropriate weight  chances due to measurement outliers to maintain stable online learning capability. Outliers  might result in highly fluctuating weights and possible even instability when estimating the  neural network weight vector online using a Gaussian error assumption. State-of-the art  online algorithms like the extended Kalman filter, for instance, are known to be nonrobust  against such outliers (Meinhold & Singpurwalla, 1989) since they are based on a Gaussian  output error assumption.  The paper is organized as follows. In Section 2 we adopt a probabilistic view to outlier  detection by taking as a heavy-tailed observation error density the Student-t-distribution  which can be derived from an infinite mixture of Gaussians approach. In our work we use  the multi-layer perceptron (MLP) as nonlinear model. In Section 3 we derive an EM algo-  rithm for estimating the MLP weight vector and the hyperparameters offiine. Employing  a state-space representation to model the MLP's weight evolution in time we extend the  batch algorithm of Section 3 to the online learning case (Section 4). The application of the  computationally efficient Fisher scoring algorithm leads to posterior mode weight updates  and an online EM-type algorithm for approximate maximum likelihood (ML) estimation  of the hyperparameters. In in the last two sections (Section 5 and Section 6) we present  experiments and conclusions, respectively.  2 THE t-DENSITY AS A ROBUST ERROR DENSITY  We assume a nonlinear regression model where for the t-th data point the noisy target  Yt  I is generated as  Yt = g(xt;w) + v (1)  and xt  k is a k-dimensional known input vector. g(.; wt) denotes a neural network  model characterized by weight vector wt  n, in our case a multi-layer perceptron  (MLP). In the offiine case the weight vector wt is assumed to be a fixed unknown constant  vector, i.e. wt -- w. Furthermore, we assume that vt is uncorrelated noise with density  Pvt (.). In the offiine case, we assume Pvt (.) to be independent of t, i.e. pv (.) _= p (.). In  the following we assume that p (.) is a Student-t-density with , degrees of freedom with  F(+i ( z2) ,+  \ 2   p(z) = T(zlae, v) = aF(-) 1 + cr-5- , v,a > 0. (2)  It is immediately appent at for v = 1 we recover e heavy-tailed Cauchy density. at  is not so obvious is at for v   we obtain a Gaussi density. For e derivation of   e EM-leing rules in e next section it is important to note at the t-denstiy can be  ought of as being  infinite mixture of Gaussians of the fore  Robust Neural Network Regression for Offiine and Online Learning 409  3  2  1  o  -1  -2  -3  Boaton Housing data with additive outliers  t.2  0.71-    .,,,,,,," .I, l  o -   ;o ; 2'0 2  number of outller [%]  Figure 1: Left: p(.)-functions for the Gaussian density (dashed) and t-densities with v =  1, 4,15 degrees of freedom. Right: MSE on Boston Housing data test set for additive  oufliers. The dashed line shows results using a Gaussian error measure and the continuous  line shows the results using the Student-t-distribution as error measure.  where T(zl a2, v) is the Student-t-density with v degrees of freedom and width parameter  a 2, A/'(zl0, a/u) is a Gaussian density with center 0 and variance a/u and   where X is a Chi-square distribution with v degrees of freedom evaluated at  > 0.  To compare different noise models it is useful to evaluate the "/,-function" defined as (Hu-  ber, 1964)  l,(z) = -0 logp,(z)/Oz (4)  i.e. the negative score-function of the noise density. In the case of i.i.d. samples the p-  function reflects the influence of a single measurement on the resulting estimator. Assum-  ing Gaussian measurement errors pv(z) = A/'(zlO, a ) we derive p(z) = z/a  which  means that for Izl - o a single outlier z can have an infinite leverage on the estimator. In  contrast, for constructing robust estimators West (1981) states that large outliers should not  have any influence on the estimator, i.e. p(z) -+ 0 for Izl -* o. Figure 1 (left) shows p(z)  for different v for the Student-t-distribution. It can be seen that the degrees of freedom v  determine how much weight outliers obtain in influencing the regression. In particular, for  finite v, the influence of outliers with Izl -. o approaches zero.  3 ROBUST OFFLINE REGRESSION  As stated in Equation (3), the t-density can be thought of as being generated as an infinite  mixture of Gaussians. Maximum likelihood adaptation of parameters and hyperparameters  can therefore be performed using an EM algorithm (Lange et al., 1989). For the t-th sample,  a complete data point would consist of the triple (xt, Yt, ut) of which only the first two are  known and at is missing.  In the E-step we estimate for every data point indexed by t  Ot: (t/ld q- 1)/(Y ld q- (it)  (5)  where at = E[utlYt, xt] is the expected value of the unknown ut given the available data  (xt, Yt ) and where 6t = (Yt - g(xt ; wld)) 2/0.2,old '  In the M-step the weights w and the hyperparameters a 2 and v are optimized using  T  w new = argm2n{Eat(Ut-a(xt;w)) (6)  t=l  410 T. Briegel and I. Tresp  T  t=l  /]new .__ argmax{ Tv v  log - Tlog{r()}  T T  t=l t=l  where  t = DG ( vld + 1  2 ) - og( + (9)  wi e Digamma function DG(z) = OF(z)/Oz. Note at e M-step for v is a one-  dimensional nonlinem optimization problem. Also note at e M-steps for e weights in   e MLP reduce to a weighted least squmes regression problem in which outliers tend to  be weighted down. e exception of course is e Gaussi case wi v   in which all  tes obtain equal weight.  4 ROBUST ONLINE REGRESSION  For robust online regression, we assume that the model Equation (1) is still valid but that  w can change over time, i.e. w _= wt. In particular we assume that wt follows a first order  random walk with normally distributed increments, i.e.  wtlWt_l ,,, ./V(wt_, Qt) (10)  and where wo is normally distributed with center ao and covariance Q0. Clearly, due to  the nonlinear nature of g and due to the fact that the noise process is non-Gaussian, a fully  Bayesian online algorithm which for the linear case with Gaussian noise can be realized  using the Kalman filter -- is clearly infeasible.  T ,  On the other hand, if we consider data D = {xt,yt}t= 1 the negative log-posterior  - logp(WrID) of the parameter sequence Wr = (Wo-r,..., WT) -r is up tO a normaliz-  ing constant  T  1  -logp(Wr[T)) o - Elogpv(yt - g(xt;wt)) + (wo - ao)V2-l(wo - ao)  T  I  Wt--1) Qt (W t -- Wt-1) (11)  __  E ( ?l)t -- -[- --1  t=l  and can be used as the appropriate cost function to derive the posterior mode estimate  W ^P for the weight sequence. The two differences to the presentation in the last section  are that first, wt is allowed to change over time and that second, penalty terms, stemming  from the prior and the transition density, are included. The penalty terms are penalizing  roughness of the weight sequence leading to smooth weight estimates.  A suitable way to determine a stationary point of - logp(WTIT)), the posterior mode es-  timate of WT, is to apply Fisher scoring. With the current estimate WJ d we get a better  estimate W ew = W, ld + rF7 for the unknown weight sequence WT where 7 is the solution  of  S(wpld)') ' = $(Wp ld) (12)  with the negative score function s(WT) = --010gp(WTJT))/OWT and the expected infor-  mation matrix $(W2r = E[0 2 logp(WTIT))/OWTOW]. By applying the ideas given in  Fahrmeir & Kaufmann (1991) to robust neural network regression it tums out that solving  (12), i.e. to compute the inverse of the expected information matrix, can be performed by  Robust Neural Network Regression for Offiine and Online Learning 411  Cholesky decomposition in one forward and backward pass through the set of data D. Note  that the expected information matrix is a positive definite block-tridiagonal matrix. The  forward-backward steps have to be iterated to obtain the posterior mode estimate W ^P  for WT.  For online posterior mode smoothing, it is of interest to smooth backwards after each filter  step t. If Fisher scoring steps are applied sequentially for t = 1, 2,..., then the posterior  mode smoother at time-step t - 1 W MAP T  , t-1 = (/;0[t-l''''' /;t-1--llt-1) T together with the  step-one predictor writ-1 = Wt-llt-1 is a reasonable starting value for obtaining the pos-  terior mode smoother Wt MAP at time t. One can reduce the computational load by limiting  the backward pass to a sliding time window, e.g. the last rt time steps, which is reasonable  in non-stationary environments for online purposes. Furthermore, if we use the underly-  ing assumption that in most cases a new measurement Yt should not change estimates too  drastically then a single Fisher scoring step often suffices to obtain the new posterior mode  estimate at time t. The resulting single Fisher scoring step algorithm with lookback param-  eter rt has in fact just one additional line of code involving simple matrix manipulations  compared to online Kalman smoothing and is given here in pseudo-code. Details about the  algorithm and a full description can be found in Briegel & Tresp (1999).  Online single Fisher scoring step algorithm (pseudo-code)  for t = 1, 2,... repeat the following four steps:   Evaluate the step-onepredictor writ_ .   Perform the forward recursions for s -- t - rt,..., t.   New data point (a:t, Yt) arrives: evaluate the corrector step totl t.   Perform the backward smoothing recursions Ws-lit for s = t,..., t - ft.  For the adaptation of the parameters in the t-distribution, we apply results from Fahrmeir  & Kilnstier (1999) to our nonlinear assumptions and use an online EM-type algorithm for  approximate maximum likelihood estimation of the hyperparameters vt and at e . We assume  the scale factors cr and the degrees of freedom vt being fixed quantities in a certain time  window of length h, e.g. at e = a 2 , vt = v, t  {t - t, t}. For deriving online EM update  equations we treat the weight sequence wt together with the mixing variables ut as missing.  By linear Taylor series expansion oft7(.; ws) about the Fisher scoring solutions walt and by  approximating posterior expectations E[w,]/)] with posterior modes wdt, s  {t - t, t}  and posterior covariances cov [w, [D] with curvatures Edt = E[ (w, - w dt)(w, - w,lt) -r [/)]  in the E-step, a somewhat lengthy derivation results in approximate maximum likelihood  update rules for a 2 and v similar to those given in Section 3. Details about the online  EM-type algorithm can be found in Briegel & Tresp (1999).  5 EXPERIMENTS  1. Experiment: Real World Data Sets. In the first experiment we tested if the Student-  t-distribution is a useful error measure for real-world data sets. In training, the Student-  t-distribution was used and both, the degrees of freedom v and the width parameter cr 2  were adapted using the EM update rules from Section 3. Each experiment was repeated  50 times with different divisions into training and test data. As a comparison we trained  the neural networks to minimize the squared error cost function (including an optimized  weight decay term). On the test data set we evaluated the performance using a squared  error cost function. Table 1 provides some experimental parameters and gives the test  set performance based on the 50 repetitions of the experiments. The additional explained  variance is defined as [in percent] 100 x (1 - MSPE7-/MSPE) where MSPE7- is the  mean squared prediction error using the t-distribution and MSPE is the mean squared  prediction error using the Gaussian error measure. Furthermore we supply the standard  412 T. Briegel and V. Tresp  Table 1: Experimental parameters and test set performance on real world data sets.  Data Set # Inputs/Hidden Training I Test Add.Exp.Var. [%] Std. [%] I  Boston Housing (13/6) 400 106 4.2 0.93  Sunspot ( 12/7) 221 47 5.3 0.67  Fraser River (12/7) 600 334 5.4 0.75  error based on the 50 experiments. In all three experiments the networks optimized with  the t-distribution as noise model were 4-5% better than the networks optimized using the  Gaussian as noise model and in all experiments the improvements were significant based on  the paired t-test with a significance level of 1%. The results show clearly that the additional  free parameter in the Student-t-distribution does not lead to overfitting but is used in a  sensible way by the system to value down the influence of extreme target values. Figure 2  shows the normal probability plots. Clearly visible is the derivation from the Gaussian  distribution for extreme target values. We also like to remark that we did not apply any  preselection process in choosing the particular data sets which indicates that non-Gaussian  noise seems to be the rule rather than the exception for real world data sets.  0'999 I  0 08 ,/  -0751  0.98 : ,t  090  0 75  025  -08-o9-0'4-0'2  0:2 0'4 0'9 0'8  madeels afro. b'atmfxj v,h Gausam-e denaty  Nomd obablty p4 FmMf Fvor am  0.98  095  090  075  050  025  010  005  oo2  ooo  Figure 2: Normal probability plots of the three training data sets after learning with the  Gaussian error measure. The dashed line show the expected normal probabilities. The  plots show clearly that the residuals follow a more heavy-tailed distribution than the normal  distribution.  2. Experiment: Outliers. In the second experiment we wanted to test how our approach  deals with outliers which are artificially added to the data set. We started with the Boston  housing data set and divided it into training and test data. We then randomly selected a  subset of the training data set (between 0.5% and 25%) and added to the targets a uniformly  generated real number in the interval [-5, 5]. Figure 1 (right) shows the mean squared error  on the test set for different percentages of added outliers. The error bars are derived from  20 repetitions of the experiment with different divisions into training and test set. It is  apparent that the approach using the t-distribution is consistently better than the network  which was trained based on a Gaussian noise assumption.  3. Experiment: Online Learning. In the third experiment we examined the use of the  t-distribution in online learning. Data were generated from a nonlinear map t = 0.6a: 2 +  bsin(6a:) - I where b = -0.75,-0.4,-0.1,0.25 for the first, second, third and fourth  set of 150 data points, respectively. Gaussian noise with variance 0.2 was added and for  training, a MLP with 4 hidden units was used. In the first experiment we compare the  performance of the EKF algorithm with our single Fisher scoring step algorithm. Figure 3  (left) shows that our algorithm converges faster to the correct map and also handles the  transition in the model (parameter b) much better than the EKE In the second experiment  with a probability of 10% outliers uniformly drawn from the interval [-5, 5] were added to  the targets. Figure 3 (middle) shows that the single Fisher scoring step algorithm using the  Robust Neural Network Regression for Offline and Online Learning 413  t-distribution is consistently better than the same algorithm using a Gaussian noise model  and the EKF. The two plots on the right in Figure 3 compare the nonlinear maps learned  after 150 and 600 time steps, respectively.  EK v-' GFS-tO  10'  EIO: . GFS.-10 rs. 'iFS-I 0 Mal,ing afte T-150 Mal,ing afte T-6OO  100 200 3 4 5 IXX3 I 200 3 4 500 IXX3 -1 0 -1 0  Te me x x  Figure 3: Left & Middle: Online MSE over each of the 4 sets of training data. On the  left we compare extended Kalman filtering (EKF) (dashed) with the single Fisher scoring  step algorithm with rt = 10 (GFS-10) (continuous) for additive Gaussian noise. The  second figure shows EKF (dashed-dotted), Fisher scoring with Gaussian error noise (GFS-  10) (dashed)and t-distributed error noise (TFS-10) (continuous), respectively for data with  additive outliers. Right: True map (continuous), EKF learned map (dashed-dotted) and  TFS-10 map (dashed) after T - 150 and T = 600 (data sets with additive outliers).  6 CONCLUSIONS  We have introduced the Student-t-distribution to replace the standard Gaussian noise as-  sumption in nonlinear regression. Learning is based on an EM algorithm which estimates  both the scaling parameters and the degrees of freedom of the t-distribution. Our results  show that using the Student-t-distribution as noise model leads to 4-5% better test errors  than using the Gaussian noise assumption on real world data set. This result seems to in-  dicate that non-Gaussian noise is the rule rather than the exception and that extreme target  values should in general be weighted down. Dealing with outliers is particularly important  for online tasks in which outliers can lead to instability in the adaptation process. We in-  troduced a new online learning algorithm using the t-distribution which leads to better and  more stable results if compared to the extended Kalman filter.  References  Briegel, T. and Tresp, V. (1999) Dynamic Neural Regression Models, Discussion Paper, Seminar far  Statistik, Ludwig Maximilians Universit3t MQnchen.  de Freitas, N., Doucet, A. and Niranjan, M. (1998) Sequential Inference and Learning, NIPS*98  Workshop, Breckenridge, CO.  Fahrmeir, L. and Kaufmann, H. (199 l) On Kalman Filtering, Posterior Mode Estimation and Fisher  Scoring in Dynamic Exponential Family Regression, Metrika 38, pp. 37-60.  Fahrmeir, L. and KQnstler, R. (1999) Penalized likelihood smoothing in robust state space models,  Metrika 49, pp. 173-191.  Huber, P.J. (1964) Robust Estimation of Location Parameter, Annals of Mathematical Statistics 35,  pp. 73-101.  Lange, K., Little, L., Taylor, J. (1989) Robust Statistical Modeling Using the t-Distribution, JASA  84, pp. 881-896.  Meinhold, R. and Singpurwalla, N. (1989) Robustification of Kalman Filter Models, JASA 84, pp.  470-496.  Rousseeuw, P. and Leroy, A. (1987) Robust Regression and Outlier Detection, John Wiley & Sons.  West, M. (1981) Robust Sequential Approximate Bayesian Estimation, JRSS B 43, pp. 157-166.  
Agglomerative Information Bottleneck  Noam $1onim Naftali Tishby*  Institute of Computer Science and  Center for Neural Computation  The Hebrew University  Jerusalem, 91904 Israel  emaih {nomm,tishby}cs .huj i. ac. il  Abstract  We introduce a novel distributional clustering algorithm that max-  imizes the mutual information per cluster between data and giv-  en categories. This algorithm can be considered as a bottom up  hard version of the recently introduced "Information Bottleneck  Method". The algorithm is compared with the top-down soft ver-  sion of the information bottleneck method and a relationship be-  tween the hard and soft results is established. We demonstrate the  algorithm on the 0 Newsgroups data set. For a subset of two news-  groups we achieve compression by 3 orders of magnitudes loosing  only 10% of the original mutual information.  1 Introduction  The problem of self-organization of the members of a set X based on the similarity  of the conditional distributions of the members of another set, Y, {p(y]x)}, was first  introduced in [8] and was termed "distributional clustering".  This question was recently shown in [9] to be a special case of a much more fun-  damental problem: What are the features of the variable X that are relevant for  the prediction of another, relevance, variable Y ? This general problem was shown  to have a natural information theoretic formulation: Find a compressed represen-  tation of the variable X, denoted , such that the mutual information between   and Y, I(;Y), is as high as possible, under a constraint on the mutual infor-  mation between X and , I(X; ). Surprisingly, this variational problem yields  an exact self-consistent equations for the conditional distributions p(y]), p(x]),  and p(). This constrained information optimization problem was called in [9] The  Information Bottleneck Method.  The original approach to the solution of the resulting equations, used already in [8],  was based on an analogy with the "deterministic annealing" approach to clustering  (see [7]). This is a top-down hierarchical algorithm that starts from a single cluster  and undergoes a cascade of cluster splits which are determined stochastically (as  phase transitions) into a "soft" (fuzzy) tree of clusters.  In this paper we propose an alternative approach to the information bottleneck  618 N. Slonim and N. Tishby  problem, based on a greedy bottom-up merging. It has several advantages over the  top-down method. It is fully deterministic, yielding (initially) "hard clusters", for  any desired number of clusters. It gives higher mutual information per-cluster than  the deterministic annealing algorithm and it can be considered as the hard (zero  temperature) limit of deterministic annealing, for any prescribed number of clusters.  Furthermore, using the bottleneck self-consistent equations one can "soften" the  resulting hard clusters and recover the deterministic annealing solutions without  the need to identify the cluster splits, which is rather tricky. The main disadvantage  of this method is computational, since it starts from the limit of a cluster per each  member of the set X.  1.1 The information bottleneck method  The mutual information between the random variables X and Y is the symmetric  functional of their joint distribution,  I(X;Y)= E p(x,y)log( p(x'Y) )=  xex,e k,p(x)p(y)  f P(Yl x)  E P(x)p(ylx)log k, p-- )  xX,yY  (1)  The objective of the information bottleneck method is to extract a compact rep-  resentation of the variable X, denoted here by 5:, with minimal loss of mutual  information to another, relevance, variable Y. More specifically, we want to find a  (possibly stochastic) map, p(lx), that minimizes the (lossy) coding length of X via  5:, I(X; 5:), under a constraint on the mutual information to the relevance variable  1(5:; ). In other words, we want to find an efficient representation of the variable  X, 5:, such that the predictions of Y from X through  will be as close as possible  to the direct prediction of Y from X.  As shown in [9], by introducing a positive Lagrange multiplier  to enforce the  mutual information constraint, the problem amounts to minimization of the La~  grangian:  C[plx)] = t(X; 5:) - t(5:; Y) , (2)  with respect to p(lx), subject to the Markov condition 5: -> X -> Y and normal-  ization.  This minimization yields directly the following self-consistent equations for the map  p(5:lx), as well as for p(ylS:) and p(5:):  { p(lx) = P() exp(-D::[p(ylx)l]p(yl)])  z(,x)  p(yl) = p(Ylx)P(lx)  P() -- E P(lx)P(x)  (3)  The variational principle, Eq. (2), determines also the shape of the annealing process,  since by changing  the mutual informations Ix - I(X; 5:) and I _-- I(Y; 5:) vary  such that  6I _ _  5Ix (4)  where Z(,x) is a normalization function. The functional DcL[Pllq] -=  y.yp(y) log qP(--) is the Kulback-Liebler divergence [3], which emerges here from the  variational principle. These equations can be solved by iterations that are proved to  converge for any finite value of  (see [9]). The Lagrange multiplier ] has the nat-  ural interpretation of inverse temperature, which suggests deterministic annealing  [7] to explore the hierarchy of solutions in 5:, an approach taken already in [8].  Agglomerative Information Bottleneck 619  Thus the optimal curve, which is analogous to the rate distortion function in in-  formation theory [3], follows a strictly concave curve in the (Ix, I) plane, called  the information plane. Deterministic annealing, at fixed number of clusters, follows  such a concave curve as well, but this curve is suboptimal beyond a certain critical  value of .  Another interpretation of the bottleneck principle comes from the relation between  the mutual information and Bayes classification error. This error is bounded above  and below (see [6]) by an important information theoretic measure of the class con-  ditional distributions p(xlYi), called the Jensen-Shannon divergence. This measure  plays an important role in our context.  The Jensen-Shannon divergence of M class distributions, pi(x), each with a prior  ri, 1 _< i _< M, is defined as, [6, 4].  M M  JSv,p2, ...,PM] ------ H[y riPi(X)] - y riHvi(x)] , (5)  i=1 i=1  where Hip(x)] is Shannon's entropy, Hip(x)] = -Exp(x)logp(x). The convexi-  ty of the entropy and Jensen inequality guarantees the non-negativity of the JS-  divergence.  1.2 The hard clustering limit  For any finite cardinality of the representation ll -- m the limit  -> o of the  Eqs.(3) induces a hard partition of X into m disjoint subsets. In this limit each  member x 6 X belongs only to the subset  6 . for which p(ylS:) has the smallest  DKLV(ylx)llp(yl)] and the probabilistic map p(l x) obtains the limit values 0 and  i only.  In this paper we focus on a bottom up agglomerative algorithm for generating  "good" hard partitions of X. We denote an m-partition of X, i.e.  with cardinality  m, also by Zm = {Zl,Z2, ..., Zm}, in which case p() = p(zi). We say that Zm is an  optimal m-partition (not necessarily unique) of X if for every other m-partition of  X, Ztm, I(Zm; Y) _> I(Zm; Y). Starting from the trivial N-partition, with N = IX],  we seek a sequence of merges into coarser and coarser partitions that are as close  as possible to optimal.  It is easy to verify that in the  - o limit Eqs.(3) for the m-partition distributions  are simplified as follows. Let  -- z = {x,x2,...,xlzl} , xi  X denote a specific  component (i.e. cluster) of the partition Zm, then  1 if x  z Vx  X  p(z[x) = { 0 otherwise  P(Y] ) = p--(Ty Yi= p(xi, y) Yy e Y  p(z) ---- Yi= p(xi)  (6)  Using these distributions one can easily evaluate the mutual information between  Zm and Y, I(Zm; Y), and between Zm and X, I(Zm; X), using Eq.(1).  Once any hard partition, or hard clustering, is obtained one can apply "reverse  annealing" and "soften" the clusters by decreasing  in the self-consistent equations,  Eqs.(3). Using this procedure we in fact recover the stochastic map, p(lx), from  the hard partition without the need to identify the cluster splits. We demonstrate  this reverse deterministic annealing procedure in the last section.  620 N. Sionin and N. Tishby  1.3 Relation to other work  A similar agglomerative procedure, without the information theoretic framework  and analysis, was recently used in [1] for text categorization on the 20 newsgroup  corpus. Another approach that stems from the distributional clustering algorith-  m was given in [5] for clustering dyadic data. An earlier application of mutual  information for semantic clustering of words was used in [2].  2 The agglomerative information bottleneck algorithm  The algorithm starts with the trivial partition into N = ]X I clusters or components,  with each component contains exactly one element of X. At each step we merge  several components of the current partition into a single new component in a way  that locally minimizes the loss of mutual information I(.; Y) = I(Zm; Y).  Let Z, be the current m-partition of X and Zm denote the new -partition  of X after the merge of several components of Zm. Obviously,  < m. Let  {zi,z2,...,zk} C_ Zm denote the set of components to be merged, and 2k  Zm  the new component that is generated by the merge, so h = m - k + 1.  To evaluate the reduction in the mutual information I(Zm; Y) due to this merge one  needs the distributions that define the new h-partition, which are determined as  follows. For every z e Zm,z  , its probability distributions (p(z),p(y[z),p(zlx))  remains equal to its distributions in Zm. For the new component, 2  Zm, we  define,  = 5-.i= p(zi)  () -i= p(zi,y) Vy e Y (7)  1 ifxziforsomel<i<k VxX  p(2lx)- 0 otherwise  It is easy to verify that Zm is indeed a valid h-partition with proper probability  distributions.  Using the same notations, for every merge we define the additional quantities:   The merge prior distribution: defined by IIk -- (r, r2, ..., r), where ri  is the prior probability of zi in the merged subset, i.e. ri -= p(,).   The Y-information decrease: the decrease in the mutual information  I(.; Y) due to a single merge, 5I(zl, ...,z) -- I(Zm; Y) - I(Zm; Y)   The X-information decrease: the decrease in the mutual information  I(., X) due to a single merge, 5Ix(z,z2,...,zl) -- I(Zm;X) - I(Zm;X)  Our algorithm is a greedy procedure, where in each step we perform "the best possi-  ble merge", i.e. merge the components {z,..., z} of the current m-partition which  minimize 5I (z,..., zk). Since 5I(z, ..., z) can only increase with k (corollary 2),  for a greedy procedure it is enough to check only the possible merging of pairs of  components of the current m-partition. Another advantage of merging only pairs is  that in this way we go through all the possible cardinalities of Z = X, from N to 1.  re(m--l) possible pairs  For a given m-partition Zm -- {21,22,...,2m} there are 2  to merge. To find "the best possible merge" one must evaluate the reduction of  information 5I(zi, zj) = I(Zm;Y) - I(Zm-1;Y) for every pair in Zm, which is  O(m  IYI) operations for every pair. However, using proposition I we know that  5I(zi,zj) = (p(zi) +p(zj)). JSn2(p(Y[zi),p(YIzj)), so the reduction in the mutual  Agglomerat've Information Bottleneck 621  information due to the merge of zi and zj can be evaluated directly (looking only  at this pair) in O(]Y]) operations, a reduction of a factor of m in time complexity  (for every merge).  Input: Empirical probability matrix p(x,y), N = IXI, M = IYI  Output: zm: m-partition of X into m clusters, for every 1 <_ m _< N  Initialization:   Construct Z --- X  - For i = 1...N   z- {x}   p(zi) =p(xi)   P(YlZi) ---- P(YlXi) for every y e Y   p(zlxj) = I if j = i and 0 otherwise  - Z= {z,...,zN}   for every i, j = 1...N, i < j, calculate  d,j - (P(Xd + P(XJ)) ' JSn2(ylxd,p(ylxy)]  (every di,j points to the corresponding couple in Z)  Loop:   For t = 1...(N- 1)  - Find {a,} = argmini,j{di,j}  (if there e severM minima choose bitrily between them)  - Merge {z,zz}  2:   p() = p(,) + p(,)   p(yl)= 1  p(p(z,y) +p(zz,y)) for every y  Y   p(2[x) = 1 if x  z U ze and 0 otherwise, for every x  X  - Update Z= {Z-{z,ze}}U{ }  (Z is now a new (N - t)-partition of X with N - t clusters)  - Update di,j COSTS and pointers w.r.t.   (only for couples contained z or zz).   End For  Figure 1: Pseudo-code of the algorithm.  3 Discussion  The algorithm is non-parametric, it is a simple greedy procedure, that depends only  on the input empirical joint distribution of X and Y. The output of the algorithm  is the hierarchy of all m-partitions Zm of X for m = N, (N - 1), ..., 2, 1. Moreover,  unlike most other clustering heuristics, it has a built in measure of efficiency even  for sub-optimal solutions, namely, the mutual information I(Zm; Y) which bounds  the Bayes classification error. The quality measure of the obtained Z, partition is  the fraction of the mutual information between X and Y that Zm captures. This  I(Z,;Y) ]Zm[. We found that empirically this  is given by the curve (x;r) vs. m =  curve was concave. If this is always true the decrease in the mutual information at  (Zm;r)-(Zm-;r)  every step, given by 5(m) -- (x;) can only increase with decreasing  m. Therefore, if at some point 5(m) becomes relatively high it is an indication  that we have reached a value of m with "meaningful" partition or clusters. Further  622 N. Slonim and N. Tishby  merging results in substantial loss of information and thus significant reduction in  the performance of the clusters as features. However, since the computational cost  of the final (low m) part of the procedure is very low we can just as well complete  the merging to a single cluster.  Figure 2: On the left figure the results of the agglomerative algorithm are shown in the  "information plane", normalized I(Z; Y) vs. normalized I(Z; X) for the NG1000 dataset.  It is compared to the soft version of the information bottleneck via "reverse annealing"  for [Z[ - 2, 5, 10, 20,100 (the smooth curves on the left). For ]Z I -- 20,100 the annealing  curve is connected to the starting point by a dotted line. In this plane the hard algorithm  is clearly inferior to the soft one.  On the right-hand side: I(Z,, Y) of the agglomerative algorithm is plotted vs. the car-  dinality of the partition m for three subsets of the newsgroup dataset. To compare the  performance over the different data cardinalities we normalize I(Z,; Y) by the value of  I(Z50; Y), thus forcing all three curves to start (and end) at the same points. The predic-  tive information on the newsgroup for NG1000 and NG100 is very similar, while for the  dichotomy dataset, 2ng, a much better prediction is possible at the same IZI, as can be  expected for dichotomies. The inset presents the full curve of the normalized I(Z; Y) vs.  IZ[ for NG100 data for comparison. In this plane the hard partitions are superior to the  soft ones.  4 Application  To evaluate the ideas and the algorithm we apply it to several subsets of the  20Newsgroups dataset, collected by Ken Lang using 20,000 articles evenly distribut-  ed among 20 UseNet discussion groups (see [1]). We replaced every digit by a single  character and by another to mark non-alphanumeric characters. Following this pre-  processing, the first dataset contained the 530 strings that appeared more then 1000  times in the data. This dataset is referred as NG1000. Similarly, all the strings that  appeared more then 100 times constitutes the NG100 dataset and it contains 5148  different strings. To evaluate also a dichotomy data we used a corpus consisting of  only two discussion groups out of the 20Newsgroups with similar topics: alt. atheism  and talk. religion. misc. Using the same pre-processing, and removing strings that  occur less then 10 times, the resulting "lexicon" contained 5765 different strings.  We refer to this dataset as ng.  We plot the results of our algorithm on these three data sets in two different planes.  First, the normalized information t(x;) vs. the size of partition of X (number  of clusters), [Z I. The greedy procedure directly tries to maximize I(Z; Y) for a  given [Z], as can be seen by the strong concavity of these curves (figure 2, right).  Indeed the procedure is able to maintain a high percentage of the relevant mutual  information of the original data, while reducing the dimensionality of the "features",  Agglomerative Information Bottleneck 623  [Z[, by several orders of magnitude.  On the right hand-side of figure 2 we present a comparison between the efficiency  of the procedure for the three datasets. The two-class data, consisting of 5765  different strings, is compressed by two orders of magnitude, into 50 clusters, almost  without loosing any of the mutual information about the news groups (the decrease  in I(;Y) is about 0.1%). Compression by three orders of magnitude, into 6  clusters, maintains about 90% of the original mutual information.  Similar results, even though less striking, are obtained when Y contain all 20 news-  groups. The NG100 dataset was compressed from 5148 strings to 515 clusters,  keeping 86% of the mutual information, and into 50 clusters keeping about 70%  of the information. About the same compression efficiency was obtained for the  NG1000 dataset.  The relationship between the soft and hard clustering is demonstrated in the Infor-  mation plane, i.e., the normalized mutual information values, // vs. /H--' In  this plane, the soft procedure is optimal since it is a direct maximization of I(Z; Y)  while constraining I(Z; X). While the hard partition is suboptimal in this plane,  as confirmed empirically, it provides an excellent starting point for reverse anneal-  ing. In figure 2 we present the results of the agglomerative procedure for NG1000  in the information plane, together with the reverse annealing for different values  of I Z]. As predicted by the theory, the annealing curves merge at various critical  values of  into the globally optimal curve, which correspond to the "rate distor-  tion function" for the information bottleneck problem. With the reverse annealing  ("heating") procedure there is no need to identify the cluster splits as required in the  original annealing ("cooling") procedure. As can be seen, the "phase diagram" is  much better recovered by this procedure, suggesting a combination of agglomerative  clustering and reverse annealing as the ultimate algorithm for this problem.  References  [1] L. D. Baker and A. K. McCallum. Distributional Clustering of Words for Text Clas-  siftcation In A CM SIGIR 98, 1998.  [2] P. F. Brown, P.V. deSouza, R.L. Mercer, V.J. DellaPietra, and J.C. Lai. Class-based  n-gram models of natural language. In Computational Linguistics, 18(4):467-479,  1992.  [3] T. M. Cover and J. A. Thomas. Elements of Information Theory. John Wiley &:  Sons, New York, 1991.  [4] R. E1-Yaniv, S. Fine, and N. Tishby. Agnostic classification of Maxkovian sequences.  In Advances in Neural Information Processing (NIPS'97) , 1998.  [5] T. Holmann, J. Puzicha, and M. Jordan. Learning from dyadic data. In Advances in  Neural Information Processing (NIPS'98), 1999.  [6] J. Lin. Divergence Measures Based on the Shannon Entropy. IEEE Transactions on  Information theory, 37(1):145-151, 1991.  [7] K. Rose. Deterministic Annealing for Clustering, Compression, Classification, Regres-  sion, and Related Optimization Problems. Proceedings of the IEEE, 86(11):2210-2239,  1998.  [8] F.C. Perefta, N. Tishby, and L. Lee. Distributional clustering of English words. In  30th Annual Meeting of the Association for Computational Linguistics, Columbus,  Ohio, pages 183-190, 1993.  [9] N. Tishby, W. Bialek, and F. C. Peteira. The information bottleneck method: Ex-  tracting relevant information from concurrent data. Yet unpublished manuscript,  NEC Research Institute TR, 1998.  
Bayesian model selection for Support  Vector machines, Gaussian processes and  other kernel classifiers  Matthias Seeger  Institute for Adaptive and Neural Computation  University of Edinburgh  5 Forrest Hill, Edinburgh EH1 2QL  seeger@dai. ed. ac. uk  Abstract  We present a variational Bayesian method for model selection over  families of kernels classifiers like Support Vector machines or Gaus-  sian processes. The algorithm needs no user interaction and is able  to adapt a large number of kernel parameters to given data without  having to sacrifice training cases for validation. This opens the pos-  sibility to use sophisticated families of kernels in situations where  the small "standard kernel" classes are clearly inappropriate. We  relate the method to other work done on Gaussian processes and  clarify the relation between Support Vector machines and certain  Gaussian process models.  I Introduction  Bayesian techniques have been widely and successfully used in the neural networks  and statistics community and are appealing because of their conceptual simplicity,  generality and consistency with which they solve learning problems. In this paper  we present a new method for applying the Bayesian methodology to Support Vector  machines. We will briefly review Gaussian Process and Support Vector classification  in this section and clarify their relationship by pointing out the common roots.  Although we focus on classification here, it is straightforward to apply the methods  to regression problems as well. In section 2 we introduce our algorithm and show  relations to existing methods. Finally, we present experimental results in section 3  and close with a discussion in section 4.  Let X be a measure space (e.g. X = IR a ) and D = (J,t) - ((x,t),...,  (xn, tn)), xi  X, ti 6 (-1,+1) a noisy i.i.d. sample from a latent function  y: X -  where P(tly ) denotes the noise distribution. Given further points x. we  wish to predict t. so as to minimize the error probability P(t]x., D), or (more diffi-  cult) to estimate this probability. Generarive Bayesian methods attack this problem  by placing a stochastic process prior P(y(.)) over the space of latent functions and  604 M. Seeger  then compute posterior and predictive distributions P(y[D), P(y,[x,, D) as  P(y[D)- P(D[y)P(y),  P(O) (1)  P(y,]D,x,) - f P(y, ly)P(y]D)dy  where y - (y(xi))i, y, - y(x,), the likelihood P(DlY) - rli P(ti]y) and P(D) is  a normalization constant. P(tlx,,D ) can then be obtained by averaging P(t[y,)  over P(y, lx,, D). Gaussian process (GP) or spline smoothing models use a Gaus-  sian process prior on y(.) which can be seen as function of X into a set of random  variables such that for each finite X C X the corresponding variables are jointly  Gaussian (see [15] for an introduction). A GP is determined by a mean function   x  E [y(x)] and a positive definite covariance kernel K(x,). Gaussian process  classification (GPC) amounts to specifying available prior knowledge by choosing  a class of kernels K(x, x]),   O, where  is a vector of hyperparameters, and a  hyperprior P(). Usually, these choices are guided by simple attributes of y(-) such  as smoothness, trends, differentiability, but more general approaches to kernel de-  sign have also been considered [5]. For 2-class classification the most common noise  distribution is the binomial one where P(t]y) - a(ty), a(u) - (1 + exp(-u)) - the  logistic function, and y is the logit log(P(+ l[x)/P(-l[x)) of the target distribution.  For this noise model the integral in (1) is not analytically tractable, but a range  of approximative techniques based on Laplace approximations [16], Markov chain  Monte Carlo [7], variational methods [2] or mean field algorithms [8] are known.  We follow [16]. The Laplace approach to GPC is to approximate the posterior  P(ylD,) by the Gaussian distribution N(,7/-) where  - argmaxP(ylD, )  is the posterior mode and 7/ - VVy(-logP(y[D, 8)), evaluated at . Then it  is easy to show that the predictive distribution is Gaussian with mean  and variance k. -k(x.)lg-k(x.) where/C is the covariance matrix (K(xi, xj))ij,  k(.) = (K(xi,-))i, k. = K(x., x.) and the prime denotes transposition. The final  discriminant is therefore a linear combination of the K(xi, .).  The discriminative approach to the prediction problem is to choose a loss function  g(t,y), being an approximation to the misclassification loss 2 I(ty_o} and then to  search for a discriminant y(.) which minimizes E[g(t,y(x.))] for the points x. of  interest (see [14]). Support Vector classification (SVC) uses the e-insensitive loss  (SVC loss) g(t,y) - [1- ty]+, [u]+ - uI(_o} which is an upper bound on the  misclassification loss, and a reproducing kernel Hilbert space (RKHS) with kernel  K(x,[) as hypothesis space for y(-). Indeed, Support Vector models and the  Laplace method for Gaussian processes are special cases of spline smoothing models  in RKHS where the aim is to minimize the functional  g(ti,Yi) + A[[Y(')[[C (2)  i1  where I1' [[K denotes the norm of the RKHS. It can be shown that the minimizer of  (2) can be written as k(-)]C- where/t maximizes  - g(ti,yi) - AyIC-y. (3)  i----1  All these facts can be found in [13]. Now (3) is, up to terms not depending on y,  the log posterior in the above GP framework if we choose g(t, y) - - log P(t[y) and  W.l.o.g. we only consider GPs with mean function 0 in what follows.  2I denotes the indicator function of the set A C  Bayesian Model Selection for Support Vector Machines 605  absorb A into 0. For the SVC loss, (3) can be transformed into a dual problem via  y = ICcp, where c is a vector of dual variables, which can be efficiently solved using  quadratic programming techniques. [12] is an excellent reference.  Note that the SVC loss cannot be written as the negative log of a noise distribution,  so we cannot reduce SVC to a special case of a Gaussian process classification  model. Although a generarive model for SVC is given in [11], it is easier and less  problematic to regard SVC as efficient approximation to a proper Gaussian process  model. Various such models have been proposed (see [8],[4]). In this work, we  simply normalize the SVC loss pointwise, i.e. use a Gaussian process model with  the normalized $VC loss g(t,y) = [1 - ty]+ + log Z(y), Z(y) = exp(-[1 - y]+) +  exp(-[1 + y]+). Note that g(t, y) is a close approximation of the (unnormalized)  SVC loss. The reader might miss the SVM bias parameter which we dropped here  for clarity, but it is straightforward to apply this semiparametric extension to GP  models too 3.  2 A variational method for kernel classification  The real Bayesian way to deal with the hyperparameters 0 is to average  P(y. lx.,D,O) over the posterior P(OID ) in order to obtain the predictive dis-  tribution P(y. lx.,D). This can be approximated by Markov chain Monte Carlo  methods [7], [16] or simply by P(y.[x.,D,),  = argmaxP(OlD). The latter  approach, called maximum a-posteriori (MAP), can be justified in the limit of large  n and often works well in practice. The basic challenge of MAP is to calculate the  evidence  P(D[O)=fP(D,y[O)dy=/exp(-g(ti,Yi))N(y[O,]g(O))dy. (4)  i=1  Our plan is to attack (4) by a variational approach. Let P be a density from a  model class F chosen to approximate the posterior P(ylD, 0). Then:  -logP(D,O) = - f P(y)log (P(D'yIO)P_ (y)) dy  k P(ult), O)P(U)  (S)  where we call F(P, 0) = Ep[- log U10)] +EpOog P(U)] the variational free en-  ergy. The second term in (5) is the well-known Kullback-Leibler divergence between  /5 and the posterior which is nonnegative and equals zero iff P(y) = P(ylD, O)  almost everywhere with respect to the distribution P. Thus, F is an upper bound  on -logP(D[O), and changing (P, 0) to decrease F enlarges the evidence or de-  creases the divergence between the posterior and its approximation, both being  favourable. This idea has been introduced in [3] as ensemble learning 4 and has  been successfully applied to MLPs [1]. The latter work also introduced the model  class r we use here, namely the class of Gaussians with mean t and factor-analyzed  covariance Z = 7)+ Y--1 c5c}, 7) diagonal with positive elements*. Hinton and  aThis is the "random effects model with improper prior" of [13], p.19, and works by  placing a flat improper prior on the bias parameter.  4We average different discriminants (given by y) over the ensemble/5.  *Although there is no danger of overfitting, the use of full covariances would render the  optimization more difficult, time and memory consuming.  606 M. Seeger  van Camp [3] used diagonal covariances which would be M = 0 in our setting. By  choosing a small M, we are able to track the most important correlations between  the components in the posterior using O(Mn) parameters to represent/5.  Having agreed on F, the criterion F and its gradients with respect to 0 and the  parameters of/5 can easily and efficiently be computed except for the generic term  (6)  a sum of one-dimensional Gaussian expectations which are, depending on the ac-  tual g, either analytically tractable or can be approximated using a quadrature  algorithm. For example, the expectation for the normalized SVC loss can be de-  composed into expectations over the (unnormalized) SVC loss and over log Z(y) (see  end of section 1). While the former can be computed analytically, the latter expec-  tation can be handled by replacing log Z(y) by a piecewise defined tight bound such  that the integral can be solved analytically. For the GPC loss (6) cannot be solved  analytically and was in our experiments approximated by Gaussian quadrature.  We can optimize F using a nested loop algorithm as follows. In the inner loop we  run an optimizer to minimize F w.r.t. /5 for fixed 0. We used a conjugate gradients  optimizer since the number of parameters of/5 is rather large. The outer loop is  an optimizer minimizing F w.r.t. 0, and we chose a Quasi-Newton method here  since the dimension of 0 is usually rather small and gradients w.r.t. 0 are costly  to evaluate.  We can use the resulting minimizer (P, 0) of F in two different ways. The most  natural is to discard 5, plug & into the original architecture and predict using the  mode of P(y ]D, &) as an approximation to the true posterior mode, benefitting from  a kernel now adapted to the given data. This is particularly interesting for Support  Vector machines due to the sparseness of the final kernel expansion (typically only  a small fraction of the components in the weight vector /C-xp is non-zero, the  corresponding datapoints are termed Support Vectors) which allows very efficient  predictions for a large number of test points. However, we can also retain/5 and use  it as a Gaussian approximation of the posterior P(y[D, ). Doing so, we can use  the variance of the approximative predictive distribution P(y. Ix., D) to derive error  bars for our predictions, although the interpretation of these figures is somewhat  complicated in the case of kernel discriminants like SVM whose loss function does  not correspond to a noise distribution.  2.1 Relations to other methods  Let us have a look at alternative ways to maximize (4). If the loss g(t, y) is twice  differentiable everywhere, progress can be made by replacing g by its second order  Taylor expansion around the mode of the integrand. This is known as Laplace  approximation and is used in [16] to maximize (4) approximately. However, this  technique cannot be used for nondifferentiable losses of the e-insensitive type 6.  Nevertheless, for the SVC loss the evidence (4) can be approximated in a Laplace-  like fashion [11], and it will be interesting to compare the results of this work with  ours. This approximation can be evaluated very efficiently, but is not continuous   6The nondifferentiabilities cannot be ignored since with probability one a nonzero num-  ber of the i sit exactly at these maxgin locations.  7Although continuity can be accomplished by a further modification, see [11].  Bayesian Model Selection for Support Vector Machines 607  w.r.t. 0 and difficult to optimize if the dimension of O is not small. Opper and  Winther [8] use mean field ideas to derive an approximate leave-one-out test error  estimator which can be quickly evaluated, but suffers from the typical noisiness  of cross-validation scores. Kwok [6] applies the evidence framework to Support  Vector machines, but the technique seems to be restricted to kernels with a finite  eigenfunction expansion (see [13] for details).  It is interesting to compare our variational method to the Laplace method of [16]  and the variational technique of [2]. Let g(t, y) be differentiable and suppose that  for given 0 we restrict ourselves to approximate (6) by replacing g(ti,Yi) by the  expansion  Og (ti,Ii)(Yi - [i) q- 1 02g (ti i)(Yi [i) 2 (7)  g(ti,Pi) + yy  Oy---  , - ,  where fi is the posterior mean. This will change the criterion F to Fappro, say. Then  it is easy to show that the Gaussian approximation to the posterior employed by  the Laplace method, namely N(fi, (]C - + W)-), 142 = diag(a(i)(1-a(i))), min-  imizes Fappro w.r.t. /5 if full covariances E are used, and plugging this minimizer  into F, ppo we end up with the evidence approximation which is maximized by the  Laplace method. The latter is not a variational technique since the approximation  (7) to the loss function is not an upper bound, and works only for differentiable  loss functions. If we upper bound the loss function g(t, y) by a quadratic polyno-  mial and add the variational parameters of this bound to the parameters of/5, our  method becomes broadly similar to the lower bound algorithm of [2]. Indeed, since  for fixed variational parameters of the polynomials we can easily solve for the mean  and covariance of P, the former parameters are the only essential ones. However,  the quadratic upper bound is poor for functions like the SVC loss, and in these  cases our bound is expected to be tighter.  3 Experiments  We tested our variational algorithm on a number of datasets from the UCI ma-  chine learning repository and the DELVE archive of the University of TorontoS:  Leptograpsus crabs, Pima Indian diabetes, Wisconsin Breast Cancer, Ringnorm,  Twonorm and Waveform (class I against 2). Descriptions may be found on the  web. In each case we normalized the whole set to zero mean, unit variance in all  input columns, picked a training set at random and used the rest for testing. We  chose (for X = IR d) the well-known squared-exponential kernel (see [15]):  , 0 = ((wd'i, C, v)'. (8)  All parameters are constrained to be positive, so we chose the representation Oi =  We did not use a prior on 0 (see comment at end of this section). For comparison  we trained a Gaussian Process classifier with the Laplace method (also without  hyperprior) and a Support Vector machine using 10-fold cross-validation to select  the free parameters. In the latter case we constrained the scale parameters wi to  be equal (it is infeasible to adapt d + 2 hyperparameters to the data using cross-  validation) and dropped the v parameter while allowing for a bias parameter. As  mentioned above, within the variational method we can use the posterior mode  SSee http://m,.cs.utoronto.ca/delve and  http://m,.ics.uci.edu/~mlearn/LRepository.html.  608 M. Seeger  Name train test Var. GP GP Var. SVM SVM Lin.  size size  / Lapl.  / 10-CV discr.  crabs 80 120 3 4 4 4 4 4 3  pima 200 332 66 66 68 64 66 67 67  wdbc 300 269 11 11 8 10 10 9 19  twonorm 300 7100 233 224 297 260 223 163 207  ringnorm 400 7000 119 124 184 129 126 160 1763  waveform 800 2504 206 204 221 211 206 197 220  Table 1: Number of test errors for various methods.  as well as the mean / of/5 for prediction, and we tested both methods. Error  bars were not computed. The baseline method was a linear discriminant trained  to minimize the squared error. Table I shows the test errors the different methods  attained.  These results show that the new algorithm performs equally well as the other meth-  ods we considered. They have of course to be regarded in combination with how  much effort was necessary to produce them. It took us almost a whole day and a lot  of user interactions to do the cross-validation model selection. The rule-of-thumb  that a lot of Support Vectors at the upper bound indicate too large a parameter C  in (8) failed for at least two of these sets, so we had to start with very coarse grids  and sweep through several stages of refinement.  An effect known as automatic relevance determination (ARD) (see [7]) can be nicely  observed on some of the datasets, by monitoring the length scale parameters wi in  (8). Indeed, our variational SVC algorithm almost completely ignored (by driving  their length scales to very small values) 3 of the 5 dimensions in "crabs", 2 of  7 in "pima" and 3 of 21 in "waveform". On "wdbc", it detected dimension 24  as particularly important with regard to separation, all this in harmony with the  GP Laplace method. Thus, a sensible parameterized kernel family together with  a method of the Bayesian kind allows us to gain additional important information  from a dataset which might be used to improve the experimental design.  Results of experiments with the methods tested above and hyperpriors as well as a  more detailed analysis of the experiments can be found in [9].  4 Discussion  We have shown how to perform model selection for Support Vector machines using  approximative Bayesian variational techniques. Our method is applicable to a wide  range of loss functions and is able to adapt a large number of hyperparameters to  given data. This allows for the use of sophisticated kernels and Bayesian techniques  like automatic relevance determination (see [7]) which is not possible using other  common model selection criteria like cross-validation. Since our method is fully  automatic, it is easy for non-experts to use 9, and as the evidence is computed on  the training set, no training data has to be sacrificed for validation. We refer to [9]  where the topics of this paper are investigated in much greater detail.  A pressing issue is the unfortunate scaling of the method with the training set  9As an aside, this opens the possibility of comparing SVMs against other fully-  automatic methods within the DELVE project (see section 3).  Bayesian Model Selection for Support Vector Machines 609  size n which is currently O(n3) 10. We are currently explorin. g the applicability of  the powerful approximations of [10] which might bring us very much closer to the  desired O(n 2) scaling (see also [2]). Another interesting issue would be to connect  our method with the work of [5] who use generarive models to derive kernels in  situations where the "standard kernels" are not applicable or not reasonable.  Acknowledgments  We thank Chris Williams, Amos Storkey, Peter Sollich and Carl Rasmussen for  helpful and inspiring discussions. This work was partially funded by a scholarship  of the Dr. Erich Milllet foundation. We are grateful to the Division of Informatics  for supporting our visit in Edinburgh, and to Chris Williams for making it possible.  References  [1] David Barber and Christopher Bishop. Ensemble learning for multi-layer networks.  In Advances in NIPS, number 10, pages 395-401. MIT Press, 1997.  [2] Mark N. Gibbs. Bayesian Gaussian Processes for Regression and Classification. PhD  thesis, University of Cambridge, 1997.  [3] Geoffrey E. Hinton and D. Van Camp. Keeping neural networks simple by minimizing  the description length of the weights. In Proceedings of the 6th annual conference on  computational learning theory, pages 5-13, 1993.  [4] Tommi Jaakkola, Marina Meila, and Tony Jebara. Maximum entropy discrimination.  In Advances in NIPS, number 13. MIT Press, 1999.  [5] Tommi S. Jaakkola and David Haussler. Exploiting generarive models in discrimina-  rive classifiers. In Advances in NIPS, number 11, 1998.  [6] James Tin-Tau Kwok. Integrating the evidence framework and the Support Vector  machine. Submitted to ESANN 99, 1999.  [7] Radford M. Neal. Monte Carlo implementation of Gaussian process models for  Bayesian classification and regression. Technical Report 9702, Department of Statis-  tics, University of Toronto, January 1997.  [8] Manfred Opper and Ole Winther. GP classification and SVM: Mean field results and  leave-one-out estimator. In Advances in Large Margin Classifiers. MIT Press, 1999.  [9] Matthias Seeger. Bayesian methods for Support Vector machines and Gaussian  processes. Master's thesis, University of Karlsruhe, Germany, 1999. Available at  http://www. dai. ed. ac. uk/~ seeger.  [10] John Skilling. Maximum entropy and Bayesian methods. Cambridge University Press,  1988.  [11] Peter Sollich. Probabilistic methods for Support Vector machines. In Advances in  NIPS, number 13. MIT Press, 1999.  [12] Vladimir N. Vapnik. Statistical Learning Theory. Wiley, 1998.  [13] Grace Wahba. Spline Models for Observational Data. CBMS-NSF Regional Confer-  ence Series. SIAM, 1990.  [14] Grace Wahba. Support Vector machines, reproducing kernel Hilbert spaces and the  randomized GACV. Technical Report 984, University of Wisconsin, 1997.  [15] Christopher K. I. Williams. Prediction with Gaussian processes: From linear regres-  sion to linear prediction and beyond. In M. I. Jordan, editor, Learning in Graphical  Models. Kluwer, 1997.  [16] Christopher K.I. Williams and David Barber. Bayesian classification with Gaussian  processes. IEEE Trans. PAMI, 20(12):1342-1351, 1998.  rathe running time is essentially the same as that of the Laplace method, thus being  comparable to the fastest known Bayesian GP algorithm.  
On input selection with reversible jump  Markov chain Monte Carlo sampling  Peter Sykacek  Austrian Research Institute for Artificial Intelligence ()FAI)  Schottengasse 3, A-1010 Vienna, Austria  peterai. univie. ac. at  Abstract  In this paper we will treat input selection for a radial basis function  (RBF) like classifier within a Bayesian framework. We approximate  the a-posteriori distribution over both model coefficients and input  subsets by samples drawn with Gibbs updates and reversible jump  moves. Using some public datasets, we compare the classification  accuracy of the method with a conventional ARD scheme. These  datasets are also used to infer the a-posteriori probabilities of dif-  ferent input subsets.  1 Introduction  Methods that aim to determine relevance of inputs have always interested re-  searchers in various communities. Classical feature subset selection techniques, as  reviewed in [1], use search algorithms and evaluation criteria to determine one opti-  mal subset. Although these approaches can improve classification accuracy, they do  not explore different equally probable subsets. Automatic relevance determination  (ARD) is another approach which determines relevance of inputs. ARD is due to [6]  who uses Bayesian techniques, where hierarchical priors penalize irrelevant inputs.  Our approach is also "Bayesian": Relevance of inputs is measured by a probability  distribution over all possible feature subsets. This probability measure is determined  by the Bayesian evidence of the corresponding models. The general idea was already  used in [7] for variable selection in linear regression models. Though our interest  is different as we select inputs for a nonlinear classification model. We want an  approximation of the true distribution over all different subsets. As the number of  subsets grows exponentially with the total number of inputs, we can not calculate  Bayesian model evidence directly. We need a method that samples efficiently across  different dimensional parameter spaces. The most general method that can do this  is the reversible jump Markov chain Monte Carlo sampler (reversible jump MC)  recently proposed in [4]. The approach was successfully applied by [8] to determine  a probability distribution in a mixture density model with variable number of kernels  and in [5] to sample from the posterior of RBF regression networks with variable  number of kernels. A Markov chain that switches between different input subsets is  useful for two tasks: Counting how often a particular subset was visited gives us a  relevance measure of the corresponding inputs; For classification, we approximate  On Input Selection with Reversible Jump MCMC 639  the integral over input sets and coefficients by summation over samples from the  Markov chain.  The next sections will show how to implement such a reversible jump MC and apply  the proposed algorithm to classification and input evaluation using some public  datasets. Though the approach could not improve the MLP-ARD scheme from  [6] in terms of classification accuracy, we still think that it is interesting: We can  assess the importance of different feature subsets which is different than importance  of single features as estimated by ARD.  2 Methods  The classifier used in this paper is a RBF like model. Inference is performed within  a Bayesian framework. When conditioning on one set of inputs, the posterior over  model parameters is already multimodal. Therefore we resort to Markov chain  Monte Carlo (MCMC) sampling tchniques to approximate the desired posterior  over both model coefficients and feature subsets. In the next subsections we will  propose an appropriate architecture for the classifier and a hybrid sampler for model  inference. This hybrid sampler consists of two parts: We use Gibbs updates ([2]) to  sample when conditioning on a particular set of inputs and reversible jump moves  that carry out dimension switching updates.  2.1 The classifier  In order to allow input relevance determination by Bayesian model selection, the  classifier needs at least one coefficient that is associated with each input' Roughly  speaking, the probability of each model is proportional to the likelihood of the most  probable coefficients, weighted by their posterior width divided by their prior width.  The first factor always increases when using more coefficients (or input features).  The second will decrease the more inputs we use and together this gives a peak  for the most probable model. A classifier that satisfies these constraints is the so  called classification in the sampling paradigm. We model class conditional densities  and together with class priors express posterior probabilities for classes. In neural  network literature this approach was first proposed in [10]. We use a model that  allows for overlapping class conditional densities:  D K  p(x_lk ) =  wkap(x_l_) , p(x_) =  Pkp(x_lk ) (1)  d--1 k=l  Using P for the K class priors and p(xlk) for the class conditional densities, (1)  expresses posterior probabilities for classes as P(klx ) = Pp(xlk)/p(x ). We choose  the component densities, p(xld) , to be Gaussian with restricted parametrisation:  Each kernel is a multivariate normal distribution with a mean and a diagonal co-  variance matrix. For all Gaussian kernels together, we get 2, D, I parameters, with  I denoting the current input dimension and D denoting the number of kernels.  Apart from kernel coefficients, (I)d, (1) has D coefficients per class, wd, indicat-  ing the prior kernel allocation probabilities and K class priors. Model (1) allows to  treat labels of patterns as missing data and use labeled as well as unlabeled data for  model inference. In this case training is carried out using the likelihood of observing  inputs and targets:  p(T, XlO__ ) rl= v ppk(x_lO_O)ii=p(x_,10__), (2)  where T denotes labeled and X unlabeled training data. In (2) O__ are all coefficients  the k-th class conditional density depends on. We further use _O for all model  640 P Sykacek  coefficients together, nk as number of samples belonging to class k and m as index  for unlabeled samples. To make Gibbs updates possible, we further introduce two  latent allocation variables. The first one, d, indicates the kernel number each sample  was generated from, the second one is the unobserved class label c, introduced for  unlabeled data. Typical approaches for training models like (1), e.g. [3] and [9],  use the EM algorithm, which is closely related to the Gibbs sampler introduce in  the next subsection.  2.2 Fixed dimension sampling  In this subsection we will formulate Gibbs updates for sampling from the posterior  when conditioning on a fixed set of inputs. In order to allow sampling from the full  conditional, we have to choose priors over coefficients from their conjugate family:  Each component mean, m__a, is given a Gaussian prior: rn d --, Afd(_, _).  The inverse variance of input i and kernel d gets a Gamma prior:  2  All d variances of input i have a common hyperparameter, /?i, that has  itself a Gamma hyperprior: /?i "' F(g, hi).  The mixing coefficients, __, get a Dirichlet prior: __W& '" D(5o, ..., 5w).  Class priors, P, also get a Dirichlet prior: P--, D(Sp, ...,Sp).  The quantitative settings are similar to those used in [8]: Values for a are between  1 and 2, g is usually between 0.2 and 1 and hi is typically between 1/R and lOIRe,  with Ri denoting the i'th input range. The mean gets a Gaussian prior centered  at the midpoint, , with diagonal inverse covariance matrix _, with ii = 1/R.  The prior counts w and 5, are set to 1 to give the corresponding probabilities  non-informative proper Dirichlet priors.  The Gibbs sampler uses updates from the full conditional distributions in (3). For  notational convenience we use  for the parameters that determine class condi-  tional densities. We use m as index over unlabeled data and c, as latent class label.  The index for all data is n, d, are the latent kernel allocations and n the number  of samples allocated by the d-th component. One distribution does not occur in  the prior specification. That is AAn(1, ...) which is a multinomial-one distribution.  Finally we need some counters: m ... m: are the counts per class and rnk ..  count kernel allocations of class-k-patterns. The full conditional of the d-th kernel  variances and the hyper parameter/?i contain i as index of the input dimension.  There we express each r -2 separately. In the expression of the d-th kernel mean,  i,d  On Input Selection with Reversible Jump MCMC 641  m__, we use V_ to denote the entire covariance matrix.  p(cl...)  -2  v(, I...)  = F gq-Da, hiq-E  d  =  ((w q- talk,..., (w q- mDk)  -' 'D ( d p q- rn l , . . . , d p q- rn K )  = v((ndv 1 +_)-(nd_v:+_,(_v:  +_)-)  = r .+-,/,+  (x.,-_,,)   ,nd,-d  (3)  2.3 Moving between different input subsets  The core part of this sampler are reversible jump updates, where we move between  different feature subsets. The probability of a feature subset will be determined by  the corresponding Bayesian model evidence and by an additional prior over number  of inputs. In accordance with [7], we use the truncated Poisson prior:  p(I) = l/ ( Im. ) 'k  I c., where c is a constant and Im, the total nr. of inputs.  Reversible jump updates are generalizations of conventional Metropolis-Hastings  updates, where moves are bijections (x, u) 6-> (x', u'). For a thorough treatment we  refer to [4]. In order to switch subsets efficiently, we will use two different types of  moves. The first consist of a step where we add one input chosen at random and a  matching step that removes one randomly chosen input. A second move exchanges  two inputs which allows "tunneling" through low likelihood areas.  Adding an input, we have to increase the dimension of all kernel means and diagonal  covariances. These coefficients are drawn from their priors. In addition the move  proposes new allocation probabilities in a semi deterministic way. Assuming the  ordering, wk,a _< wk,a+l:  Vd _ D/2  Beta(b,, b0 + I)  w,v+_  = w,v+- +  w' . - w (1  (4)  The matching step proposes removing a randomly chosen input. Removing corre-  sponding kernel coefficients is again combined with a semi deterministic proposal  of new allocation probabilities, which is exactly symmetric to the proposal in (4).  642 P Sykacek  Table 1: Summary of experiments  Data avg() max(X) RBF (%,ha) MLP (%,n0)  Ionosphere 4.3 9 (91.5,11) (95.5,4)  Pima 4 7 (78.9,11) (79.8,8)  Wine 4.4 8 (100, 0) (96.8,2)  We accept births with probability:  ao = min(1,1h. rt. x () 7x/- exp -0.5 (//_)2  D  , ,-2  r'(a) ] 1-Jr) exp(-/ rd )  The first line in (5) are the likelihood and prior ratio. The prior ratio results from  the difference in input dimension, which affects the kernel means and the prior over  number of inputs. The first term of the proposal ratio is from proposing to add  or remove one input. The second term is the proposal density of the additional  kernel components which cancels with the corresponding term in the prior ratio.  Due to symmetry of the proposal (4) and its reverse in a death move, there is no  contribution from changing allocation probabilities. Death moves are accepted with  probability ad = 1/ao.  The second type of move is an exchange move. We select a new input and one  from the model inputs and propose new mean coefficients. This gives the following  acceptance probability:  rain( 1,1h. ratio (-r' xD ID exp (--0.5--(/;- )2) (6)  x  (r, x/ D lid exp (-0.5 -!r (/a - a) 2)  l-Iv  X X ).  -  The first line of (6) are again likelihood and prior ratio. For exchange moves, the  prior ratio is just the ratio from different values in the kernel means. The first term  in the proposal ratio is from proposing to exchange an input. The second term is the  proposal density of new kernel mean components. The last part is from proposing  new allocation probabilities.  3 Experiments  Although the method can be used with labeled and unlabeled data, the following  experiments were performed using only labeled data. For all experiments we set  c = 2 and g = 0.2. The first two data sets are from the UCI repository . We use  Available at http://www.ics.uci.edu/mlearn/MLRepository.html.  On Input Selection with Reversible Jump MCMC 643  the Ionosphere data which has 33 inputs, 175 training and 176 test samples. For  this experiment we use 6 kernels and set h = 0.5. The second data is the wine  recognition data which provides 13 inputs, 62 training and 63 test samples. For this  data, we use 3 kernels and set h - 0.28. The third experiment is performed with  the Pima data provided by B. D. Ripley 2. For this one we use 3 kernels and set  h = 0.16.  For all experiments we draw 15000 samples from the posterior over coefficients and  input subsets. We discard the first 5000 samples as burn in and use the rest for  predictions. Classification accuracy, is compared with an MLP classifier using R.  Neals hybrid Monte Carlo sampling with ARD priors on inputs. These experiments  use 25 hidden units. Table 1 contains further details: avg() is the average and  max() the maximal number of inputs used by the hybrid sampler; RBF (%, ha) is  the classification accuracy of the hybrid sampler and the number of errors it made  that were not made by the ARD-MLP; MLP(%, no) is the same for the ARD-MLP.  We compare classifiers by testing (ha, no) against the null hypothesis that this is an  observation from a Binomial Bn(na + no, 0.5) distribution. This reveals that neither  difference is significant. Although we could not improve classification accuracy on  these data, this does not really matter because ARD methods usually lead to high  generalization accuracy and we can compete.  The real benefit from using the hybrid sampler is that we can infer probabilities  telling us how much different subsets contribute to an explanation of the target  variables. Figure 3 shows the occurrence probabilities of feature subsets and fea-  tures. Note that table 1 has also details about how many features were used in  these problems. Especially the results from Ionosphere data are interesting as on  average we use only 4.3 out of 33 input features. For ionosphere and wine data the  Markov chain visits about 500 different input subsets within 10000 samples. For  the Pima data the number is about 60 and an order of magnitude smaller.  4 Discussion  In this paper we have discussed a hybrid sampler that uses Gibbs updates and  reversible jump moves to approximate the a-posteriori distribution over parameters  and input subsets in nonlinear classification problems. The classification accuracy  of the method could compete with R. Neals MLP-ARD implementation. However  the real advantage of the method is that it provides us with a relevance measure of  feature subsets. This allows to infer the optimal number of inputs and how many  different explanations the data provides.  Acknowledgement s  I want to thank several people for having used resources they provide: I have used  R.Neals hybrid Markov chain sampler for the MLP experiments; The data used for  the experiments were obtained form the University at Irvine repository and from  B. D. Ripley. Furthermore I want to express gratitude to the anonymous reviewers  for their comments and to J.F.G. de Freitas for useful discussions during the confer-  ence. This work was done in the framework of the research project GZ 607.519/2-  V/B/9/98 "Verbesserung der Biosignalverarbeitung durch Beruecksichtigung von  Unsicherheit und Konfidenz", funded by the Austrian federal ministry of science  and transport (BMWV).  Available at http://www.stats.ox.ac.uk  644 P. Sykacek  Probabilities of input subsets-Ionosphere  10  I , 9.1% :[7, 14]  I I I 8.6% :[3, 4]  II I 7.9% :[4, 7]  0 100 200 300 400  Probabilities of input subsets-Pima  20[-- 17.3% :[2, 7]  ..  16.1%: [2, ]  ,o[ 9.8%:[2, 5]  10 11 8.6%: [2, 5, 7]  5OO  3O  Probabilities of inputs - Ionosphere  0  0 10 20 30  Probabilities of inputs - Pima  4O  -I  30  20  10  0  0 40 60 2 4 6  Probabilities of inputs - Wine  10  2O 0  Probabilities of input subsets - Wine  5.2% :[7, 13]  3,4% :[ 13] 20  3.1% :[1,12,13]  ,Lll, .jib j_. .......... 0  1 O0 200 300 400 500 0  o L  5 10  Figure 1' Probabilities of inputs and input subsets measuring their relevance.  References  [1] P.A. Devijver and J. V. Kittler. Pattern Recognition. A Statistical Approach. Prentice-  Hall, Englewood Cliffs, N J, 1982.  [2] S. Geman and D. Geman. Stochastic relaxation, gibbs distributions and the bayesian  restoration of images. IEEE Trans. Pattn. Anal. Mach. Intel., 6:721-741, 1984.  [3] Z. Ghahramaxfi, M.I. Jordan Supervised Learning from Incomplete Data via an EM  Approach In Cowan J.D., et al.(eds.), Advances in Neural Information Processing  Systems 6, Morgan Kaufmann, Los Altos/Palo Alto/San Francisco, pp.120-127, 1994.  [4] P. J. Green. Reversible jump markov chain monte carlo computation and bayesian  model determination. Biometrika, 82:711-732, 1995.  [5] C. C. Holmes and B. K. Mallick. Bayesian radial basis functions of variable dimension.  Neural Computation, 10:1217-1234, 1998.  [6] R. M. Neal. Bayesian Learning for Neural Networks. Springer, New York, 1986.  [7] D. B. Phillips and A. F. M. Smith. Bayesian model comparison via jump diffusioons.  In W.R. Gilks, S. Richardson, and D.J. Spiegelhalter, editors, Markov Chain Monte  Carlo in Practice, pages 215-239, London, 1996. Chapman &: Hall.  [8] S. Richardson and P.J. Green On Bayesian Analysis of Mixtures with an tmknown  number of components Journal Royal Stat. Soc. B, 59:731-792, 1997.  [9] M. Stensmo, T.J. Sejnowski A Mixture Model System for Medical and Machine Diag-  nosis In Tesauro G., et al.(eds.), Advances in Neural Information Processing System  7, MIT Press, Cambridge/Boston/London, pp.1077-1084, 1995.  [10] H. G. C. Tr&vn A neural network approach to statistical pattern classification by  "semiparametric" estimation of probability density functions IEEE Trans. Neur. Net.,  2:366-377, 1991.  
Graded grammaticality in Prediction  Fractal Machines  Shan Parfitt, Peter Tifio and Georg Dorffner  Austrian Research Institute for Artificial Intelligence,  Schottengasse 3, A-1010 Vienna, Austria.  { shan, petert, georg) @ai. univie. ac. at  Abstract  We introduce a novel method of constructing language models,  which avoids some of the problems associated with recurrent neu-  ral networks. The method of creating a Prediction Fractal Machine  (PFM) [1] is briefly described and some experiments are presented  which demonstrate the suitability of PFMs for language modeling.  PFMs distinguish reliably between minimal pairs, and their be-  havior is consistent with the hypothesis [4] that wellformedness is  'graded' not absolute. A discussion of their potential to offer fresh  insights into language acquisition and processing follows.  1 Introduction  Cognitive linguistics has seen the development in recent years of two important,  related trends. Firstly, a widespread renewal of interest in the statistical, 'graded'  nature of language (e.g. [2]-[4]) is showing that the traditional all-or-nothing no-  tion of well-formedness may not present an accurate picture of how the congruity  of utterances is represented internally. Secondly, the analysis of state space tra-  jectories in artificial neural networks (ANNs) has provided new insights into the  types of processes which may account for the ability of learning devices to acquire  and represent language, without appealing to traditional linguistic concepts [5]-[7].  Despite the remarkable advances which have come out of connectionist research  (e.g. [8]), and the now common use of recurrent networks, and Simple Recurrent  Networks (SRNs) [9] especially, in the study of language (e.g. [10]), recurrent neu-  ral networks suffer from particular problems which make them imperfectly suited  to language tasks. The vast majority of work in this field employs small networks  and datasets (usually artificial), and although many interesting linguistic issues  may be thus tackled, real progress in evaluating the potentials of state trajecto-  ries and graded 'grammaticality' to uncover the underlying processes responsible  for overt linguistic phenomena must inevitably be limited whilst the experimental  tasks remain so small. Nevertheless, there are certain obstacles to the scaling-up  of networks trained by back-propagation (BP). Such networks tend towards ever  Graded GrammaticaliF in Prediction Fractal Machines 53  longer training times as the sizes of the input set and of the network increase, and  although Real-Time Recurrent Learning (RTRL) and Back-propagation Through  Time are potentially better at modeling temporal dependencies, training times are  longer still [11]. Scaling-up is also difficult due to the potential for catastrophic  interference and lack of adaptivity and stability [12]-[14]. Other problems include  the rapid loss of information about past events as the distance from the present in-  creases [15] and the dependence of learned state trajectories not only on the training  data, but also upon such vagaries as initial weight vectors, making their analysis  difficult [16]. Other types of learning device also suffer problems. Standard Markov  models require the allocation of memory for every n-gram, such that large values  of n are impractical; variable-length Markov models are more memory-efficient, but  become unmanageable when trained on large data sets [17]. Two important, related  concerns in cognitive linguistics are thus (a) to find a method which allows language  models to be scaled up, which is similar in spirit to recurrent neural networks, but  which does not encounter the same problems of scale, and (b) to use such a method  to evince new insights into graded grammaticality from the state trajectories which  arise given genuinely large, naturally-occurring data sets.  Accordingly, we present a new method of generating state trajectories which avoids  most of these problems. Previously studied in a financial prediction task, the  method creates a fractal map of the training data, from which state machines are  built. The resulting models are known as Prediction Fractal Machines (PFMs)  [18] and have some useful properties. The state trajectories in the fractal repre-  sentation are fast and computationally efficient to generate, and are accurate and  well-understood; it may be inferred that, even for very large vocabularies and train-  ing sets, catastrophic interference and lack of adaptivity and stability will not be a  problem, given the way in which representations are built (demonstrating this is a  topic for future work); training times are significantly less than for recurrent net-  works (in the experiments described below, the smallest models took a few minutes  to build, while the largest ones took only around three hours; in comparison, all  of the ANNs took longer - up to a day - to train); and there is little or no loss of  information over the course of an input sequence (allowing for the finite precision  of the computer). The scalability of the PFM was taken advantage of by training  on a large corpus of naturally-occurring text. This enabled an assessment of what  potential new insights might arise from the use of this method in truly large-scale  language tasks.  2 Prediction Fractal Machines (PFMs)  A brief description of the method of creating a PFM will now be given. Interested  readers should consult [1], since space constraints preclude a detailed examination  here. The key idea behind our predictive model is a transformation F of symbol  sequences from an alphabet (here, tagset) {1, 2, ..., N) into points in a hypercube  H = [0, 1] t). The dimensionality D of the hypercube H should be large enough for  each symbol 1, 2, ..., N to be identified with a umque vertex of H. The particular  assignment of symbols to vertices is arbitrary. The transformation F has the crucial  property that symbol sequences sharing the same suffix (context) are mapped close  to each other. Specifically,-the longer the common suffix shared by two sequences,  the smaller the (Euclidean) distance between their point representations. The trans-  formation F used in this study corresponds to an Iterative Function System [19]  54 S. Parfitt, P. 7o and G. Dorffner  consisting of N affine maps i: H -- H, i - 1, 2, ..., N,  1  i(x)-(x+ti), tie,O, 1) , ti7 tjfori:j.  (1)  Given a sequence $182...$L of L symbols from the alphabet 1, 2, ..., N, we construct  its point representation as  (x*)))...))= (sL o sL_ o ... o s2 o  x }z) of the hypercube H. (Note that as is common in the  where x* is the center {  Iterative Function Systems literature, i refers either to a symbol or to a map, de-  pending upon the context.) PFMs are constructed on point representations of sub-  sequences appearing in the training sequence. First, we slide the window of length  L > i over the training sequence. At each position we transform the sequence  of length L appearing in the window into a point. The set of points obtained by  sliding through the whole training sequence is then partitioned into several classes  by k-means vector quamization (in the Euclidean space), each class represented by  a particular codebook vector. The number of codebook vectors required is cho-  sen experimentally. Since quantization classes group points lying close together,  sequences having point representations in the same class (potentially) share long  suffixes. The quantization classes may then be treated as prediction contexts, and  the corresponding predictive symbol probabilities computed by sliding the window  over the training sequence again and counting, for each quantization class, how of-  ten a sequence mapped to that class was followed by a particular symbol. In test  mode, upon seeing a new sequence of L symbols, the transformation F is again  performed, the closest quantization center found, and the corresponding predictive  probabilities used to predict the next symbol.  3  An experimental comparison of PFMs and recurrent  networks  The performance of the PFM was compared against that of a RTRL-trained re-  current network on a next-tag prediction task. Sixteen grammatical tags and a  'sentence start' character were used. The models were trained on a concatenated  sequence (22781 tags) of the top three-quarters of each of the 14 sub-corpora of  the University of Pennsylvania 'Brown' corpus 1. The remainder was used to create  test data, as follows. Because in a large training corpus of naturally-occurring data,  contexts in most cases have more than one possible correct continuation, simply  counting correctly predicted symbols is insufficient to assess performance, since this  fails to count correct responses which are not targets. The extent to which the mod-  els distinguished between grammatical axed ungrammatical utterances was therefore  additionally measured by generating minimal pairs and comparing their negative log  likelihoods (NLLs) per symbol with respect to the model. Likelihood is computed  by sliding through the test sequence and for each window position, determining the  probability of the symbol that appears immediately beyond it. As processing pro-  gresses, these probabilities are multiplied. The negative of the natural logarithm is  then taken and divided by the number of symbols. Significant differences in NLLs  http://www.ldc.upenn.edu/  Graded Grammaticality in Prediction Fractal Machines 55  are much harder to achieve between members of minimal pairs than between gram-  matical and random sequences, and are therefore a good measure of model validity.  Minimal pairs generated by theoretically-motivated manipulations tend to be no  longer ungrammatical given a small tagset, because the removal of grammatical  sub-classes necessarily also removes a large amount of information. Manipulations  were therefore performed by switching the positions of two symbols in each sentence  in the test sets. Symbols switched could be any distance apart within the sentence,  as long as the resulting sentence was ungrammatical under all surface instantiations.  By changing as little as possible to make the sentence ungrammatical, the goal was  retained that the task of distinguishing between grammatical and ungrammatical  sequences be as difficult as possible. The test data then consisted of 28 paired  grammatical/ungrammatical test sets (around 570 tags each), plus an ungrammat-  ical, 'meaningless' test set containing all 17 codes listed several times over, used  to measure baseline performance. Ten 1st-order randomly-initialised networks were  trained for 100 epochs using RTRL. The networks consisted of I input and I output  layer, each with 17 units corresponding to the 17 tags, 2 hidden layers, each with 10  units, and 1 context layer of 10 units connected to the first hidden layer. The second  hidden layer was used to increase the flexibility of the maps between the hidden  representations in the recurrent portion and the tag activations at the output layer.  A logistic sigmoid activation function was used, the learning rate and momentum  were set to 0.05, and the training sequence was presented at the rate of one tag  per clock tick. The PFMs were derived by clustering the fractal representation of  the training data ten times for various numbers of codebook vectors between 5 and  200. More experiments were performed using PFMs than neural networks because  in the former case, experience in choosing appropriate numbers of codebook vectors  was initially lacking for this type of data.  The results which follow are given as averages, either over all neural networks, or  else over all PFMs derived from a given number of codebook vectors. The net-  works correctly predicted 36.789% and 32.667% of next tags in the grammatical  and ungrammatical test sets, respectively. The PFMs matched this performance  at around 30 codebook vectors (37.134% and 32.814% respectively), and exceeded  it for higher numbers of vectors (39.515% and 34.388% respectively at 200 vec-  tors). The networks generated mean NLLs per symbol of 1.966 and 2.182 for the  grammatical and ungrammatical test sets, respectively (a difference of 0.216) and  4.157 for the 'meaningless' test set (the difference between NLLs for grammatical  and 'meaningless' data = 2.191). The PFMs matched this difference in NLLs at  40 codebook vectors (NLL grammatical = 1.999, NLL ungrammatical = 2.217; dif-  ference = 0.218). The NLL for the 'meaningless' data at 40 codebook vectors was  6.075 (difference between NLLs for grammatical and .'meaningless' data = 4.076).  The difference between NLLs for grammatical and ungrammatical, and for gram-  matical and 'meaningless' data sets, became even larger with increased numbers  of codebook vectors. The difference in performance between grammatical and un-  grammatical test sets was thus highly significant in all cases (p < .0005): all the  models distinguished what was grammatical from what was not. This conclusion  is supported by the fact that the mean, NLLs for the 'meaningless' test set were  always noticeably higher than those for the minimal pair sets.  56 S. Parfitt, P Tiho and G. Dorffner  4 Discussion  The PFMs exceeded the performance of the networks for larger numbers of code-  book vectors, but it is possible that networks with more hidden nodes would also  do better. In terms of ease of use, however, as well as in their scaling-up potential,  PFMs are certainly superior. Their other great advantage is that the representations  created are dependable (see section 1), making hypothesis creation and testing not  just more rapid, but also more straightforward: the speed with which PFMs may  be trained made it possible to make statistically significant observations for a large  number of clustering runs. In the introduction, 'graded' wellformedness was spoken  of as being productive of new hypotheses about the nature of language. Our use of  minimal pairs, designed to make a clear-cut distinction between grammatical and  ungrammatical utterances, appears to leave this issue to one side. But in reality, our  results were rather pertinent to it, as the use of the likelihood measure might indeed  imply. The Brown corpus consists of subcorpora representative of 14 different dis-  course types, from fiction to government documents. Whereas traditional notions  of gramrnaticality would lead us to treat all of the 'ungramrnatical' sentences in  the minimal pair test sets as equally ungrammatical, the NLLs in our experiments  tell a different story. The grammatical versions consistently had a lower associated  NLL (higher probability) than the ungrammatical versions, but the difference be-  tween these was much smaller than that between the 'meaningless' data and either  the grammatical or the ungramrnatical data. This supports the concept of 'graded  grammaticality', and NLLs for 'meaningless' data such as ours might be seen as a  sort of benchmark by which to measure all lesser degrees of ungrammaticality. (Note  incidentally that the PFMs appear to associate with the 'meaningless' data a signif-  icantly higher NLL than did the networks, even though the difference between the  NLLs of the grammatical and ungrammatical data was the same. This is suggestive  of PFMs having greater powers of discrimination between grades of wellformedness  than the recurrent networks used, but further research will be needed to ascertain  the validity of this.) Moreover, the NLL varied not just between grammatical and  ungrammatical test sets, but also from sentence to sentence, from word to word  and from discourse style to discourse style. While it increased, often dramatically,  when the manipulated portion of an ungrammatical sentence was encountered, some  words in grammatical sentences exhibited a similar effect: thus, if a subsequence  in a well-formed utterance occurs only rarely - or never - in a training set, it will  have a high associated NLL in the same way as an ungrammatical one does. This is  likely to happen even for very large corpora, since some grammatical structures are  very rare. This is consistent with recent findings that, during human sentence pro-  cessing, well-formedness is linked to conformity with expectation [20] as measured  by GLOZE scores. Interesting also was the remarkable variation in NLL between  discourse styles. Although the mean NLL across all discourse styles (test sets) is  lower for the grammatical than for the ungrammatical versions, it cannot be guar-  anteed that the grammatical version of one test set will have a lower NLL than the  ungrammatical version of another. Indeed, the grammatical and ungrammatical  NLLs interleave, as may be observed in figure 1, which shows the NLLs for the  three discourse styles which lie at the bottom, middle and top of the range. Even  more interestingly, if the NLLs for the grammatical versions of all discourse styles  are ordered according to where they lie within this range, it becomes clear that NLL  is a predictor of discourse style. Styles which linguists class as 'formal', e.g. those of  Graded GrammaticaliF in Prediction Fractal Machines 57  2.8  2.6  2.4  2.2  1.8  0  NILs aociated h grammatical and ungrammatical versions of 3 discourse types  Learned text:. grammatical  Learned text: ungrammatical -n,---  Romanfie liction: grammacal ........  Romantic fiction: ungrammatical -+-- . ....................  Science liction: grammatical .....  Science fiction: ungrammatical ---- . .....  ..:._._._.:.-+- .......  I I I I  50 100 150 200  No. of codebook vectors  Figure 1' NLLs of minimal pair test sets containing different discourse styles suggest  grades of wellformedness based upon prototypicality.  the Learned and Government Document test sets, have the lowest NLLs, with the  three Press test sets clustering just above, and the Fiction test sets, exemplifying  creative language use, clustering at the high end. Similarly, that the Learned and  Government test sets have the lowest NLLs conforms with the intuition that their  usage lies closest to what is grammatically 'prototypical'- even though in the train-  ing set, 6 out of the 14 test sets are fiction and thus might be expected to contribute  more to the prototype. That they do not, suggests that usage varies significantly  across fiction test sets.  5 Conclusion  Work on the use of PFMs in language modeling is at an early stage, but as results  to date show, they have a lot to offer. A much larger project is planned, which will  examine further Allen ad Seidenberg's hypothesis that 'graded grammaticality' (or  wellformedness) applies not only to syntax, but also to other language subdomains  such as semantics, an integral part of this being the use of larger corpora and  tagsets, and the identification of vertices with semantic/syntactic features rather  than atomic symbols. Identifying the possibilities of combining PFMs with ANNs,  for example as a means of bypassing the normal method of creating state-space  trajectories, is the subject of current study.  Acknowledgments  This work was supported by the Austrian Science Fund (FWF) within the research  project "Adaptive Information Systems and Modeling in Economics and Manage-  ment Science" (SFB 010). The Austrian Research Institute for Artificial Intelligence  is supported by the Austrian Federal Ministry of Science and Transport.  58 S. Parfitt, P. Tito and G. Dorffner  References  [1] P. Tifio & G. Dorffner (1998). Constructing finite-context sources from fractal repre-  sentations of symbolic sequences. Technical Report TR-98-18, Austrian Research Institute  for AI, Vienna.  [2] J. R. Taylor (1995). Linguistic categorisation: Prototypes in linguistic theory. Claren-  don, Oxford.  [3] J. R. Salfran, R. N. Aslin & E. L. Newport (1996). Statistical cues in language acquisi-  tion: Word segmentation by infants. In Proc. of the Cognitive Science Society Conference,  376-380, La Jolla, CA.  [4] J. Allen & M. S. Seidenberg (in press). The emergence of grammaticality in connec-  tionist networks. In B. Macwhinney (ed.), Emergentist approaches to language: Proc. of  the 28th Carnegie Symposium on cognition. Erlbaum.  [5] S. Parfitt (1997). Aspects of anaphora resolution in artificial neural networks: Impli-  cations for nativism. PhD thesis, Imperial College, London.  [6] D. Servan-Schreiber et al (1989). Graded state machines: The representation of tem-  poral contingencies in Simple Recurrent Networks. In Advances in Neural Information  Processing Systems, 643-652.  [7] W. Tabor & M. Tanenhaus (to appear). Dynamical models of sentence processing.  Cognitive Science.  [8] J. L. Elman et al (1996). Rethinking innatehess: A connectionist perspective on devel-  opment. Bradford.  [9] J. L. Elman (1990). Finding structure in time. In: Cognitive Science, 14: 179-211.  [10] S. Lawrence, C. Lee Giles & S. Fong (in press). Natural language grammatical infer-  ence with recurrent neural networks. IEEE Trans. on knowledge and data engineering.  [11] J. Hertz, A. Krogh & R. G. Palmer (1991). Introduction to the theory of neural  computation. Addison Wesley.  [12] M. McCloskey & N.J. Cohen (1989). Catastrophic interference in connectionist  networks: The sequential learning problem. In G. Bower (ed.), The psychology of learning  and motivation, vol 2J. Academic, NY.  [13] J. K. Kruschke (1991). ALCOVE: A connectionist model of human category learning.  In R. P. Lippman et al (eds.), Advances in Neural Information Processing 3, 649-655.  Kaufmarm, San Mateo, CA.  [14] S. Grossberg (ed.) (1988). Neural networks and natural intelligence. Bradford, MiT,  Cambs, MA.  [15] Y. Bengio, P. Simard & P. Frasconi (1994). Learning long-term dependencies with  gradient descent is difficult. IEEE Trans. on neural networks, 5(2).  [16] M.P. Casey (1996). The dynamics of discrete-time computation, with application  to recurrent neural networks and finite-state machine extraction. Neural Computation,  8(6):1135-1178.  [17] D. Ron, Y. Singer & N. Tishby (1996). The power of amnesia. Machine Learning, 25.  [18] P. Tifio, B. G. Home, C. Lee Giles & P. C. Coilingwood (1998). Finite state ma-  chines and recurrent neural networks - automata and dynamical systems approaches. In  J. E. Dayhoff & O. Omidvar (eds.), Neural Networks and Pattern Recognition, 171-220.  Academic.  [19] M. F. Barnsley (1988). Fractals everywhere. Academic, NY.  [20] S. Coulson, J. W. King & M. Kutas (1998). Expect the unexpected:  morphosyntactic violations. Language and Cognitive Processes, 13(1).  Responses to  
Differentiating Functions of the Jacobian  with Respect to the Weights  Gary William Flake  NEC Research Institute  4 Independence Way  Princeton, NJ 08540  flake @ research. nj. ne c. corn  Barak A. Pearlmutter  Dept of Computer Science, FEC 313  University of New Mexico  Albuquerque, NM 87131  bap @ cs. unm. edu  Abstract  For many problems, the correct behavior of a model depends not only on  its input-output mapping but also on properties of its Jacobian matrix, the  matrix of partial derivatives of the model's outputs with respect to its in-  puts. We introduce the J-prop algorithm, an efficient general method for  computing the exact partial derivatives of a variety of simple functions of  the Jacobian of a model with respect to its free parameters. The algorithm  applies to any parametrized feedforward model, including nonlinear re-  gression, multilayer perceptrons, and radial basis function networks.  1 Introduction  Let f (a:, w) be an n input, m output, twice differentiable feedforward model parameterized  by an input vector, a:, and a weight vector w. Its Jacobian matrix is defined as  ... Of  O/m ...  df(a:, w)  dx  The algorithm we introduce can be used to optimize functions of the form  or  I 2  1  Ev(w) =  IIJv - bll 2 (2)  where u, v, a, and b are user-defined constants. Our algorithm, which we call J-prop,  can be used to calculate the exact value of both OEu/Ow or OEv/OW in O(1) times the  time required to calculate the normal gradient. Thus, J-prop is suitable for training models  to have specific first derivatives, or for implementing several other well-known algorithms  such as Double Backpropagation [1] and Tangent Prop [2].  Clearly, being able to optimize Equations 1 and 2 is useful; however, we suspect that the  formalism which we use to derive our algorithm is actually more interesting because it  allows us to modify J-prop to easily be applicable to a wide-variety of model types and  436 G. W. Flake and B. A. Pearlmutter  objective functions. As such, we spend a fair portion of this paper describing the mathe-  matical framework from which we later build J-prop.  This paper is divided into four more sections. Section 2 contains background information  and motivation for why optimizing the properties of the Jacobian is an important problem.  Section 3 introduces our formalism and contains the derivation of the J-prop algorithm.  Section 4 contains a brief numerical example of J-prop. And, finally, Section 5 describes  further work and gives our conclusions.  2 Background and motivation  Previous work concerning the modeling of an unknown function and its derivatives can be  divided into works that are descriptive or prescriptive. Perhaps the best known descriptive  result is due to White et al. [3, 4], who show that given noise-free data, a multilayer percep-  tron (MLP) can approximate the higher derivatives of an unknown function in the limit as  the number of training points goes to infinity. The difficulty with applying this result is the  strong requirements on the amount and integrity of the training data; requirements which  are rarely met in practice. This problem was specifically demonstrated by Principe, Rathie  and Kuo [5] and Deco and Schfirmann [6], who showed that using noisy training data from  chaotic systems can lead to models that are accurate in the input-output sense, but inaccu-  rate in their estimates of quantifies related to the Jacobian of the unknown system, such as  the largest Lyapunov exponent and the correlation dimension.  MLPs are particularly problematic because large weights can lead to saturation at a particu-  lar sigmoidal neuron which, in turn, results in extremely large first derivatives at the neuron  when evaluated near the center of the sigmoid transition. Several methods to combat this  type of over-fitting have been proposed. One of the earliest methods, weight decay [7],  uses a penalty term on the magnitude of the weights. Weight decay is arguably optimal  for models in which the output is linear in the weights because minimizing the magnitude  of the weights is equivalent to minimizing the magnitude of the model's first derivatives.  However, in the nonlinear case, weight decay can have suboptimal performance [1] be-  cause large (or small) weights do not always correspond to having large (or small) first  derivatives.  The Double Backpropagation algorithm [1] adds an additional penalty term to the error  function equal to I lcgE/cga:ll 2. Training on this function results in a form of regularization  that is in many ways an elegant combination of weight decay and training with noise: it is  strictly analytic (unlike training with noise) but it explicitly penalizes large first derivatives  of the model (unlike weight decay). Double Backpropagation can be seen as a special case  of J-prop, the algorithm derived in this paper.  As to the general problem of coercing the first derivatives of a model to specific values,  Simard, et al., [2] introduced the Tangent Prop algorithm, which was used to train MLPs  for optical character recognition to be insensitive to small affine transformations in the  character space. Tangent Prop can also be considered a special case of J-prop.  3 Derivation  We now define a formalism under which J-prop can be easily derived. The method is  very similar to a technique introduced by Pearlmutter [8] for calculating the product of the  Hessian of an MLP and an arbitrary vector. However, where Pearlmutter used differential  operators applied to a model's weight space, we use differential operators defined with  respect to a model's input space.  Our entire derivation is presented in five steps. First, we will define an auxiliary error  Differentiating Funca'ons of the Jacobian 43 7  function that has a few useful mathematical properties that simplify the derivation. Next,  we will define a special differential operator that can be applied to both the auxiliary error  function, and its gradient with respect to the weights. We will then see that the result of  applying the differential operator to the gradient of the auxiliary error function is equivalent  to analytically calculating the derivatives required to optimize Equations 1 and 2. We then  show an example of the technique applied to an MLP. Finally, in the last step, the complete  algorithm is presented.  To avoid confusion, when referring to generic data-driven models, the model will always be  expressed as a vector function y = f(x, w), where x refers to the model input and to refers  to a vector of all of the tunable parameters of the model. In this way, we can talk about  models while ignoring the mechanics of how the models work internally. Complementary  to the generic vector notation, the notation for an MLP uses only scalar symbols; however,  these symbols must refer to internal variable of the model (e.g., neuron thresholds, net  inputs, weights, etc.), which can lead to some ambiguity. To be clear, when using vector  notation, the input and output of an MLP will always be denoted by x and y, respectively,  and the collection of all of the weights (including biases) map to the vector to. However,  when using scalar arithmetic, the scalar notation for MLPs will apply.  3.1 Auxiliary error function  Our auxiliary error function,/, is defined as  to)= urf(, to).  (3)  Note that we never actually optimize with respect ; we define it only because it has the  property that 0//0x = uTJ, which will be useful to the derivation shortly. Note that  0//0x appears in the Taylor expansion of/ about a point in input space:  (4)  Thus, while holding the weights, to, fixed and letting Ax be a perturbation of the input, x,  Equation 4 characterizes how small changes in the input of the model change the value of  the auxiliary error function.  Be setting Ax = rv, with v being an arbitrary vector and r being a small value, we can  rearrange Equation 4 into the form:  ~T  = lim 1 [/(x+rv, w) l(x,w)l  r---0 T  u T Jr = 31(x+rv'w),.=o' (5)  This final expression will allow us to define the differential operator in the next subsection.  3.2 Differential operator  Let h(x, to) be an arbitrary twice differentiable function.  operator  0 t(x + rv, to)  We define the differentiable  (6)  438 G. W. Flake and B. .4. Pearlmutter  which has the property that R{/(x, w)} = urJv. Being a differential operator, R{.}  obeys all of the standard rules for differentiation:  The operator also yields the identity _{x} - v.  3.3 Equivalence  We will now see that the result of calculating/{0//0to} can be used to calculate both  OE,/Oto and OEv/Oto. Note that Equations 3-5 all assume that both u and v are in-  dependent of :r and to. To calculate OEu/Oto and OEv/Oto, we will actually set u or  v to a value that depends on both :r and to; however, the derivation still works because  our choices are explicitly made in such a way that the chain rule of differentiation is not  supposed to be applied to these terms. Hence, the correct analytical solution is obtained  despite the dependence.  To optimize with respect to Equation 1, we use:  (7)  I l  i -- y l--1 l  Yj Wij -- 0 i.  J  In these equations, superscripts denote the layer number (starting at 0), subscripts index  over terms in a particular layer, and Nt is the number of input nodes in layer I. Thus, y is  t is the net input coming into the same neuron.  the output of neuron i at node layer l, and x i  Moreover, yp is an output of the entire MLP while y/o is an input going into the MLP.  The feedback equations calculated with respect to/ are:  = ui (11)  (10)  with v = (JTtt - a). To optimize with respect to Equation 2, we use:  0 1,,Jv_b[,2=(Jv_b)TlOJv   ow2 k-w-wj = ' (8)  with u - (Jv - b).  3.4 Method applied to MLPs  We are now ready to see how this technique can be applied to a specific type of model.  Consider an MLP with L + 1 layers of nodes defined by the equations:  y-- g(3fi) (9)  Nt  Differentiating Functions of the JacobJan 439  01 v,+ 01  -- wij ,' l-q- 1  j IJXj  =  Ox; Oy  o o  l--1  = yj  Ow  (for/< L) (12)  (13)  (14)  (15)  where the u i term is a component in the vector u from Equation 1. Applying the R,,{-}  operator to the feedforward equations yields:  4y?) =  =  ?)i (16)  I l  g'(xi)Rv{xi} (for/ > 0) (17)  N,  E Rv{Y} -1 ) wlij ' (18)  J  where the vi term is a component in the vector v from Equation 2. As the final step, we  apply the R,,{.} operator to the feedback equations, which yields:  Rv Oy i  n-Z--_/. 5 = 0 (19)  O  t+ ,_, f 0/ (for/ < L) (20)  OYi j  = + 9"(x'i) 44) (21)  R Owti j = R  yj + {yj- } (22)  ox i  o  }. (23)  3.5 Complete algorithm  Implementing this algorithm is nearly as simple as implementing normal gradient descent.  For each type of variable that is used in an MLP (net input, neuron output, weights, thresh-  olds, partial derivatives, etc.), we require that an extra variable be allocated to hold the  result of applying the R,,{.} operator to the original variable. With this change in place, the  complete algorithm to compute OE,/Ow is as follows:   Set u and a to the user specified vectors from Equation 1.   Set the MLP inputs to the value of x that J is to be evaluated at.   Perform a normal feedforward pass using Equations 9 and 10.   Set 0//0yi 6 to ui.  440 G. W. Flake and B. A. Pearlmutter  (a) Co)  Figure 1: Learning only the derivative: showing (a) poor approximation of the function  with (b) excellent approximation of the derivative.   Perform the feedback pass with Equations 11-15. Note that values in the  terms are now equal to JTu.   Set v to (JTu -- a)   Perform a Rv{.} forward pass with Equations 16-18.   Set the P,{0//0gt/L} terms to0.   Perform a Rv{.} backward pass with Equations 19-23.  After the last step, the values in the R,,,{Ot/Owtij} and R,,,{01/00(} terms contain the  required result. It is important to note that the time complexity of thd"J-forward" and "J-  backward" calculations are nearly identical to the typical output and gradient evaluations  (i.e., the "forward" and "backward" passes) of the models used.  A similar technique can be used for calculating OEv/OW. The main difference is that the  Rv{-} forward pass is performed between the normal forward and backward passes because  u can only be determined after the P{f(a:, w)) has been calculated.  4 Experimental results  To demonstrate the effectiveness and generality of the J-prop algorithm, we have imple-  mented it on top of an existing neural network library [9] in such a way that the algorithm  can be used on a large number of architectures, including MLPs, radial basis function net-  works, and higher order networks.  We trained an MLP with ten hidden tanh nodes on 100 points with conjugate gradient. The  training exemplars consisted of inputs in [-1, 1] and a target derivative from 3 cos(3a:) +  5 cos(10a:). Our unknown function (which the MLP never sees data from) is sin(3a:) +  - sin(10a:). The model quickly converges to a solution in approximately 100 iterations.  Figure 1 shows the performance of the MLP. Having never seen data from the unknown  function, the MLP yields a poor approximation of the function, but a very accurate approx-  imation of the function's derivative. We could have trained on both outputs and derivatives,  but our goal was to illustrate that J-prop can target derivatives alone.  Differentiating Functions of the Jacobian 441  5 Conclusions  We have introduced a general method for calculating the weight gradient of functions of  the Jacobian matrix of feedforward nonlinear systems. The method can be easily applied to  most nonlinear models in common use today. The resulting algorithm, J-prop, can be easily  modified to minimize functionals from several application domains [10]. Some possible  uses include: targeting known first derivatives, implementing Tangent Prop and Double  Backpropagation, enforcing identical I/O sensitivities in auto-encoders, deflating the largest  eigenvalue and minimizing all eigenvalue bounds, optimizing the determinant for blind  source separation, and building nonlinear controllers.  While some special cases of the J-prop algorithm have already been studied, a great deal  is unknown about how optimization of the Jacobian changes the overall optimization prob-  lem. Some anecdotal evidence seems to imply that optimization of the Jacobian can lead to  better generalization and faster training. It remains to be seen if J-prop used on a nonlinear  extension of linear methods will lead to superior solutions.  Acknowledgements  We thank Frans Coetzee, Yannis Kevrekidis, Joe O'Ruanaidh, Lucas Parra, Scott Rickard,  Justinian Rosca, and Patrice Simard for helpful discussions. GWF would also like to thank  Eric Baum and the NEC Research Institute for funding the time to write up these results.  References  [ 1 ] H. Drucker and Y. Le Cun. Improving generalization performance using double back-  propagation. IEEE Transactions on Neural Networks, 3(6), November 1992.  [2] P. Simard, B. Victorri, Y. Le Cun, and J. Denker. Tangent prop--A formalism for  specifying selected invariances in an adaptive network. In John E. Moody, Steve J.  Hanson, and Richard P. Lippmann, editors, Advances in Neural Information Process-  ing Systems, volume 4, pages 895-903. Morgan Kaufmann Publishers, Inc., 1992.  [3] H. White and A. R. Gallant. On learning the derivatives of an unknown mapping  with multilayer feedforward networks. In Halbert White, editor, Artificial Neural  Networks, chapter 12, pages 206-223. Blackwell, Cambridge, Mass., 1992.  [4] H. White, K. Hornik, and M. Stinchcombe. Universal approximation of an unknown  mapping and its derivative. In Halbert White, editor, Artificial Neural Networks,  chapter 6, pages 55-77. Blackwell, Cambridge, Mass., 1992.  [5] J. Principe, A. Rathie, and J. Kuo. Prediction of chaotic time series with neural  networks and the issues of dynamic modeling. Bifurcations and Chaos, 2(4), 1992.  [6] G. Deco and B. Schfirmann. Dynamic modeling of chaotic time series. In Russell  Greiner, Thomas Petsche, and Stephen Jos6 Hanson, editors, Computational Learn-  ing Theory and Natural Learning Systems, volume IV of Making Learning Systems  Practical, chapter 9, pages 137-153. The MIT Press, Cambridge, Mass., 1997.  [7] G. E. Hinton. Learning distributed representations of concepts. In Proc. Eigth Annual  Conf Cognitive Science Society, pages 1-12, Hillsdale, NJ, 1986. Erlbaum.  [8] Barak A. Pearlmutter. Fast exact multiplication by the Hessian. Neural Computation,  6(1):147-160, 1994.  [9] G. W. Flake. Industrial strength modeling tools. Submitted to NIPS 99, 1999.  [ 10] G.W. Flake and B. A. Pearlmutter. Optimizing properties of the Jacobian of nonlinear  feedforward systems. In preperation, 1999.  
Image Recognition in Context: Application to  Microscopic Urinalysis  Xubo Song*  Department of Electrical and Computer Engineering  Oregon Graduate Institute of Science and Technology  Beaverton, OR 97006  xubosong @ ece. ogi. edu  Joseph Sill  Department of Computation and Neural Systems  California Institute of Technology  Pasadena, CA 91125  joe @ busy. work. caltech. edu  Yaser Abu-Mostafa  Department of Electrical Engineering  California Institute of Technology  Pasadena, CA 91125  yase r@ work. caltech. edu  Harvey Kasdan  International Remote Imaging Systems, Inc.  Chatsworth, CA 91311  Abstract  We propose a new and efficient technique for incorporating contextual  information into object classification. Most of the current techniques face  the problem of exponential computation cost. In this paper, we propose a  new general framework that incorporates partial context at a linear cost.  This technique is applied to microscopic urinalysis image recognition,  resulting in a significant improvement of recognition rate over the context  free approach. This gain would have been impossible using conventional  context incorporation techniques.  1 BACKGROUND: RECOGNITION IN CONTEXT  There are a number of pattern recognition problem domains where the classification of an  object should be based on more than simply the appearance of the object itself. In remote  sensing image classification, where each pixel is part of ground cover, a pixel is more like-  ly to be a glacier if it is in a mountainous area, than if surrounded by pixels of residential  areas. In text analysis, one can expect to find certain letters occurring regularly in particu-  lar arrangement with other letters(qu, ee,est, tion, etc.). The information conveyed by the  accompanying entities is referred to as contextual information. Human experts apply con-  textual information in their decision making [2][6]. It makes sense to design techniques and  algorithms to make computers aggregate and utilize a more complete set of information in  their decision making the way human experts do. In pattern recognition systems, however,  *Author for correspondence  964 X. B. Song, J. Sill, Y. Abu-Mostafa and H. Kasdan  the primary (and often only) source of information used to identify an object is the set of  measurements, or features, associated with the object itself. Augmenting this information  by incorporating context into the classification process can yield significant benefits.  Consider a set of N objects Ti, i = 1,...N. With each object we associate a  class label ci that is a member of a label set ft =  is characterized by a set of measurements xi E R P,  tor. Many techniques [1][2][4][6] incorporate context  {1,...,D}. Each object Ti  which we call a feature vec-  by conditioning the posterior  probability of objects' identities on the joint features of all accompanying objects, e.,  p(Cl, C2,..., CN IX1,    , XN), and then maximizing it with respect to c1, c2,..., CN. It can  be shown that p(ci c2 ... civlxl,.. xiv) cr p(cllXl) ..p(civlxiv) v(c,...,cN) given  ' ' ' '' ' p(Cl)...p(CN)  certain reasonable assumptions.  Once the context-free posterior probabilities p(cilxi) are known, e.g. through the  use of a standard machine learning model such as a neural network, computing  p(cl,..., civlx,..., xiv) for all possible Cl,..., c/v would entail (2N + 1)D/v multi-  plications, and finding the maximum has complexity of D iv, which is intractable for large  N and D. [2]  Another problem with this formulation is the estimation of the high dimensional joint dis-  tribution p(cl,   , CN), which is ill-posed and data hungry.  One way of dealing with these problems is to limit context to local regions. With this  approach, only the pixels in a close neighborhood, or letters immediately adjacent are con-  sidered [4][6][7]. Such techniques may be ignoring useful information, and will not apply  to situations where context doesn't have such locality, as in the case of microscopic uri-  nalysis image recognition. Another way is to simplify the problem using specific domain  knowledge [1 ], but this is only possible in certain domains.  These difficulties motivate the efficient incorporation of partial context as a general frame-  work, formulated in section 2. In section 3, we discuss microscopic urinalysis image recog-  nition, and address the importance of using context for this application. Also in section 3,  techniques are proposed to identify relevant context. Empirical results are shown in section  4, followed by discussions in section 5.  2  FORMULATION FOR INCORPORATION OF PARTIAL  CONTEXT  To avoid the exponential computational cost of using the identities of all accompanying  objects directly as context, we use "partial context", denoted by A. It is called "partial" be-  cause it is derived from the class labels, as opposed to consisting of an explicit labelling of  all objects. The physical definition of A depends on the problem at hand. In our application,  A represents the presence or absence of certain classes. Then the posterior probability of  an object Ti having class label ci conditioned on its feature vector and the relevant context  A is  p(cilxi, A) =  p(ci,xi;A)  p(xilci, A)P(ci; A)  p(xi;A) p(xi;A)  We assume that the feature distribution of an object depends only on its own class, i.e.,  p(xilc, A) - p(xlci). This assumption is roughly true for most real world problems.  Then,  Image Recognition in Context: Application to Microscopic Urinalysis 965  p(xilci)p(ci; A) p(ci[A) p(A)p(xi)  p(ci[xi, A) - - p(cilxi)  J; p(xi; A)  oc p(ci 11A) - p(ci[xi)p(PcliA ))  p(ci)  where p(ci A) - p(ci[A) is called the context ratio, through which context plays its role.  , p(c)  The context-sensitive posterior probability p(ci Ixi, A) is obtained through the context-free  posterior probability p(ci [xi) modified by the context ratio p(ci, A).  The partial-context maximum likelihood decision rule chooses class label i for element i  such that  ai = argmaxp(cilxi, A)  i  A systematic approach to identify relevant context A is addressed in section 3.3.  (!)  The partial-context approach treats each element in a set individually, but with addi-  tional information from the context-bearing factor A. Once p(cilxi) are known for all  i = 1,..., N, and the context A is obtained, to maximize p(ci[xi, A) from D possible  values that ci can take on and for all i, the total number of multiplications is 2N, and the  complexity for finding the maximum is ND. Both are linear in N. The density estimation  part is also trivial since it is very easy to estimate p(c[A).  3 MICROSCOPIC URINALYSIS  3.1 INTRODUCTION  Urine is one of the most complex body fluid specimens: it potentially contains about 60  meaningful types of elements. Microscopic urinalysis detects the presence of elements that  often provide early diagnostic information concerning dysfunction, infection, or inflamma-  tion of the kidneys and urinary tract. Thus this non-invasive technique can be of great value  in clinical case management. Traditional manual microscopic analysis relies on human op-  erators who read the samples visually and identify them, and therefore is time-consuming,  labor-intensive and difficult to standardize. Automated microscopy of all specimens is more  practical than manual microscopy, because it eliminates variation among different technol-  ogists. This variation becomes more pronounced when the same technologist examines  increasing numbers of specimens. Also, it is less labor-intensive and thus less costly than  manual microscopy. It also provides more consistent and accurate results. An automat-  ed urinalysis system workstation (The YellowIRIS TM, International Remote Imaging  Systems, Inc.) has been introduced in numerous clinical laboratories for automated mi-  croscopy. Urine samples are processed and examined at 100x (low power field) and 400x  magnifications (high power field) with bright-field illumination. The YellowIRIS TM au-  tomated system collects video images of formed analytes in a stream of uncentrifuged urine  passing an optical assembly. Each image has one analyte in it. These images are given to a  computer algorithm for automatic identification of analytes.  Context is rich in urinalysis and plays a crucial role in analyte classification. Some com-  binations of analytes are more likely than others. For instance, the presence of bacteria  indicates the presence of white blood cells, since bacteria tend to cause infection and thus  trigger the production of more white blood cells. If amorphous crystals show up, they tend  to show up in bunches and in all sizes. Therefore, if there are amorphous crystal look-alikes  in various sizes, it is quite possible that they are amorphous crystals. Squamous epithelial  cells can appear both flat or rolled up. If squamous epithelial cells in one form are detected,  966 X. B. Song, J. Sill, Y. Abu-Mostafa and H. Kasdan  Table 1' Features extracted from urine anylates images  feature number feature description  1  2  3  4  5  6  7  8  9  I(}  i1  12  13  14  15  16  area  length of edge  square n)t of area  length of edge  standard deviation of distance from center to edge  mean  sum of length of two longest straight edges  total length of edge  sum of length tit' four longest straight edges  total length of edge  sum of length of two Ionlgest .<mi-straijht ed[es  total length tit' 'edge  sum of length of four Ionl?st semi-strai.eht edges  total length of edge  the mean of red distribution  the mean of blue distribution  the mean of green distribuuon  15 th pcreentile of gray level histogram  85 th pcrcentile of gray level histogram  the standard deviation tit' gray level intensity  energy of the I,aplacian transformatkm of grey level image  then it is likely that there are squamous epithelial cells in the other form. Utilizing such  context is crucial for classification accuracy.  The classes we are looking at are bacteria, calcium oxalate crystals, red blood cells, white  blood cells, budding yeast, amorphous crystals, uric acid crystals, and artifacts. The task  of automated microscopic urinalysis is, given a urine specimen that consists of up to a  few hundred images of analytes, to classify each analyte into one of these classes. The  automated urinalysis system we developed consists of three steps: image processing and  feature extraction, learning and pattern recognition, and context incorporation. Figure 1  shows some example analyte images. Table I gives a list of features extracted from analyte  images. 1  3.2 CONTEXT-FREE CLASSIFICATION  The features are fed into a nonlinear feed-forward neural network with 16 inputs, 15 hidden  units with sigmoid transfer functions, and 8 sigmoid output units. A cross-entropy error  function is used in order to give the output a probability interpretation. Denote the input  feature vector as x, the network outputs a D dimensional vector (D - 8 in our case)  p = {p(dlx) }, d = 1, ..., D, where p(d[x) is  p(dlx ) -- Prob( an analyte belongs to class d[ feature x)  The decision made at this stage is  d(x) = argmax p(dlx)  d  3.3 IDENTIFICATION OF RELEVANT PARTIAL CONTEXT  Not all classes are relevant in terms of carrying contextual information. We propose three  criteria based on which we can systematically investigate the relevance of the class pres-  ence. To use these criteria, we need to know the following distributions: the class prior dis-  tribution p(c) for c = 1,..., D; the conditional class distribution p(c[Aa) for c = 1,..., D  A and A2 are respectively the larger and the smaller eigenvalues of the second moment matrix  of an image.  Image Recognition in Context: Application to Microscopic Urinalysis 967  and d = 1,..., D; and the class presence prior distribution p(Ad) for d = 1,..., D. Ad is  a binary random variable indicating the presence of class d. Ad = I if class d is present,  and Ad = 0 otherwise. All these distributions can be easily estimated from the database.  The first criterion is the correlation coefficient between the presence of any two class-  es; the second one is the classical mutual information I(c; Ad) between the presence of a  class Ad and the class probability p(c), where I(c; Ad) is defined as I(c; Ad) = H(c) -  H(c]Ad) where H(c) = y4D= p(c = i)In(p(c = i)) is the entropy of the class priors and  H(clAa) - P(Ad = 1)H(clAd -- 1)+P(/d = O)H(clAd -- 0)is theconditionalentropy  of c conditioned on Ad. The third criterion is what we call the expected relative entropy  D(cl lad) between the presence of a class Ad and the labeling probability p(c), which we  define as D(c[lAd ) = P(Ad = 1)D(p(c)llp(clAd = 1)) + P(Ad = O)D(p(c)llp(clAd =  0)) where O(p(c)llp(clA d 1)) D  = = -i= p(c = ilAd = 1)ln(P(c!A )) and  O(p(c)llp(clAd - o)) - = p(c - ilAd = O)ln(p(c=ilAd=))  p(c=i)  According to the first criterion, one type of analyte is considered relevant to another if the  absolute value of their correlation coefficient is beyond a certain threshold. It shows that  uric acid crystals, budding yeast and calcium oxalate crystals are not relevant to any other  types even by a generous threshold of 0.10. Similarly, the bigger the mutual information  between the presence of a class and the class distribution, the more relevant this class is.  Ranking the analyte types in terms of I(c; Ad) in a descending manner gives rise to the  following list: bacteria, amorphous crystals, red blood cells, white blood cells, uric acid  crystals, budding yeast and calcium oxalate crystals. Once again, ranking the analyte types  in terms of D(c] lad) in a descending manner gives rise to the following list: bacteria, red  blood cells, amorphous crystals, white blood cells, calcium oxalate crystals, budding yeast  and uric acid crystals. All three criteria lead to similar conclusions regarding the relevance  of class presence - bacteria, red blood cells, amorphous crystals, and white blood cells are  relevant, while calcium oxalate crystals, budding yeast and uric acid crystals are not. (Baed  on prior knowledge, we discard artifacts from the outset as an irrelevant class.)  3.4 ALGORITHM FOR INCORPORATING PARTIAL CONTEXT  Once the M relevant classes are identified, the following algorithm is used to incorporate  partial context.  Step 0 Estimate the priors p(clAd) and p(c), for c  { 1, 2,..., D) and d  { 1, 2,..., D).  Step 1 For a given xi, compute p(ci IXi) for ci = 1, 2,..., D using whichever base machine  learning model is preferred ( in our case, a neural network).  Step 2 Let the M relevant classes be /1,.-.,/M- According to the no-context p(cilxi)  and certain criteria for detecting the presence or absence of all the relevant classes, get  AR ,    , ARx .  Step 3 Let p(ci Ixi, Ao) = p(ci Ixi), where Ao is the null element. Incorporate context from  each relevant class sequentially, i.e., for rn = 1 to M, iteratively compute  p(cilx; A0,..., ARm_l, ARm ) - p(cilx, Ao,..., ARm_x ) p(cIARm)p(AR )  p(c)  Step 4 Recompute ARx , ..., ARM based on the new class labellings. Return to step 3 and  repeat until algorithm converges?  2Hence, the algorithm has an E-M flavor, in that it goes back and forth between finding the most  968 X. B. Song, J. Sill, Y. Abu-Mostafa and H. Kasdan  amorphous crystals  artifacts  calcium oxalate crystals  hyaline casts  Figure 1' Example of some of the analyte images.  Step 5 Label the objects according to the final context-containing  p(cilxi, ARx , . . . , ARM), i.e., 5i = argmax p(cilxi, Ax , . . . , ArtM ) for i = 1,..., N.  ci  This algorithm is invariant with respect to the ordering of the M relevant classes in  (A1,..., AM). The proof is omitted here.  4 RESULTS  The algorithm using partial context was tested on a database of 83 urine specimens, contain-  ing a total of 20,276 analyte images. Four classes are considered relevant according to the  criteria described in section 3.3: bacteria, red blood cells, white blood cells and amorphous  crystals. We measure two types of error: analyte-by-analyte error, and specimen diagnostic  error. The average analyte-by-analyte error is reduced from 44.48% before using context  to 36.66% after, resulting a relative error reduction of 17.6% (Table 2). The diagnosis for a  specimen is either normal or abnormal. Tables 3 and 4 compare the diagnostic performance  with and without using context, and Table 5 lists the relative changes. We can see using  context significantly increases correct diagnosis for both normal and abnormal specimens,  and reduces both false positives and false negatives.  without context with context I  average element-by-element error 44.48 % 36.66 %  Table 2: Comparison of using and not using contextual information for analyte-by-analyte  error.  probable class labels given the context and determining the context given the class labels.  Image Recognition in Context: Application to Microscopic Urinalysis 969  estimated normal estimated abnormal  truly normal 40.96 % 7.23 %  truly abnormal 19.28 % 32.53 %  Table 3: Diagnostic confusion matrix not using context  estimated normal estimated abnormal  truly normal 42.17 % 6.02 %  truly abnormal 16.87 % 34.94 %  Table 4: Diagnostic confusion matrix using context  estimated normal estimated abnormal  truly normal + 2.95 % - 16.73 %  truly abnormal - 12.50 % +7.41%  Table 5: Relative accuracy improvement (diagonal elements) and error reduction (off diag-  onal elements) in the diagnostic confusion matrix by using context.  5 CONCLUSIONS  We proposed a novel framework that can incorporate context in a simple and efficien-  t manner, avoiding exponential computation and high dimensional density estimation. The  application of the partial context technique to microscopic urinalysis image recognition  demonstrated the efficacy of the algorithm. This algorithm is not domain dependent, thus  can be readily generalized to other pattern recognition areas.  ACKNOWLEDGEMENTS  The authors would like to thank Alexander Nicholson, Malik Magdon-Ismail, Amir Atiya  at the Caltech Learning Systems Group for helpful discussions.  References  [1 ] Song, X.B. & Sill, J. & Abu-Mostafa & Harvey Kasdan, (1997) "Incorporating Contextual Infor-  mation in White Blood Cell Identification", In M. Jordan, M.J. Kearns and S.A. Solla (eds.), Advances  in Neural Information Processing Systems 7, 1997, pp. 950-956. Cambridge, MA: MIT Press.  [2] Song, Xubo (1999) "Contextual Pattern Recognition with Application to Biomedical Image Iden-  tiffcation", Ph.D. Thesis, California Institute of Science and Technology.  [3] Boehringer-Mannheim-Corporation, Urinalysis Today, Boehringer-Mannheim-Corporation,  1991.  [4] Kittier, J.,"Relaxation labelling", Pattern Recognition Theory and Applications, 1987, pp. 99-  108., Pierre A. Devijver and Josef Kittier, Editors, Springer-Verlag.  [5] Kittier, J. & Illingworth, J., "Relaxation Labelling Algorithms - A Review", Image and Vision  Computing, 1985, vol. 3, pp. 206-216.  [6] Toussaint, G., "The Use of Context in Pattern Recognition", Pattern Recognition, 1978, vol. 10,  pp. 189-204.  [7] Swain, P. & Vardeman, S. & Tilton, J., "Contextual Classification of Multispectral Image Data",  Pattern Recognition, 1981, Vol. 13, No. 6, pp. 429-441.  
Learning the Similarity of Documents:  An Information-Geometric Approach to  Document Retrieval and Categorization  Thomas Hofmann  Department of Computer Science  Brown University, Providence, RI  hofmann@cs.brown.edu, www.cs.brown.edu/people/th  Abstract  The project pursued in this paper is to develop from first  information-geometric principles a general method for learning  the similarity between text documents. Each individual docu-  ment is modeled as a memoryless information source. Based on  a latent class decomposition of the term-document matrix, a low-  dimensional (curved) multinomial subfamily is learned. From this  model a canonical similarity function - known as the Fisher kernel  - is derived. Our approach can be applied for unsupervised and  supervised learning problems alike. This in particular covers inter-  esting cases where both, labeled and unlabeled data are available.  Experiments in automated indexing and text categorization verify  the advantages of the proposed method.  I Introduction  The computer-based analysis and organization of large document repositories is one  of today's great challenges in machine learning, a key problem being the quantitative  assessment of document similarities. A reliable similarity measure would provide  answers to questions like: How similar are two text documents and which documents  match a given query best? In a time, where searching in huge on-line (hyper-)text  collections like the World Wide Web becomes more and more popular, the relevance  of these and related questions needs not to be further emphasized.  The focus of this work is on data-driven methods that learn a similarity function  based on a training corpus of text documents without requiring domain-specific  knowledge. Since we do not assume that labels for text categories, document classes,  or topics, etc. are given at this stage, the former is by definition an unsupervised  learning problem. In fact, the general problem of learning object similarities pre-  cedes many "classical" unsupervised learning methods like data clustering that al-  ready presuppose the availability of a metric or similarity function. In this paper,  we develop a framework for learning similarities between text documents from first  principles. In doing so, we try to span a bridge from the foundations of statistics  in information geometry [13, 1] to real-world applications in information retrieval  and text learning, namely ad hoc retrieval and text categorization. Although the  developed general methodology is not limited to text documents, we will for sake  of concreteness restrict our attention exclusively to this domain.  Learning the Similarity of Documents 915  2 Latent Class Decomposition  Memoryless Information Sources Assume we have available a set of docu-  ments D = {dl,...,dv} over some fixed vocabulary of words (or terms) W =  {wl,..., wM}. In an information-theoretic perspective, each document di can be  viewed as an information source, i.e. a probability distribution over word sequences.  Following common practice in information retrieval, we will focus on the more re-  stricted case where text documents are modeled on the level of single word occur-  rences. This means that we we adopt the bag-of-words view and treat documents  as memoryless information sources. 1  A. Modeling assumption: Each document is a memoryless information source.  This assumption implies that each document can be represented by a multinomial  probability distribution P(wjldi), which denotes the (unigram) probability that a  generic word occurrence in document di will be wj. Correspondingly, the data can  be reduced to some simple sufficient statistics which are counts n(di, wj) of how  often a word wj occurred in a document di. The rectangular N x M matrix with  coefficients n(di, wj) is also called the term-document matrix.  Latent Class Analysis Latent class analysis is a decomposition technique for  contingency tables (cf. [5, 3] and the references therein) that has been applied to  language modeling [15] ("aggregate Markov model") and in information retrieval [7]  ("probabilistic latent semantic analysis"). In latent class analysis, an unobserved  class variable zk 6 Z = {Zl,..., z} is associated with each observation, i.e. with  each word occurrence (di, wj). The joint probability distribution over D x W is a  mixture model that can be parameterized in two equivalent ways  K K  e(d,, wj) - e(xk)e(d, lx)e(wjlx) - e(d,) e(wlx)e(xld,). (1)  k----1 k=l  The latent class model (1) introduces a conditional independence assumption,  namely that d and wj are independent conditioned on the state of the associated  latent variable. Since the cardinality of zk is typically smaller than the number of  documents/words in the collection, z acts as a bottleneck variable in predicting  words conditioned on the context of a particular document.  To give the reader a more intuitive understanding of the latent class decomposition,  we have visualized a representative subset of 16 "factors" from a K = 64 latent  class model fitted from the Reuters21578 collection (cf. Section 4) in Figure 1.  Intuitively, the learned parameters seem to be very meaningful in that they represent  identifiable topics and capture the corresponding vocabulary quite well.  By using the latent class decomposition to model a collection of memoryless sources,  we implicitly assume that the overall collection will help in estimating parameters  for individual sources, an assumption which has been validated in our experiments.  B. Modeling assumption: Parameters for a collection of memoryless informa-  tion sources are estimated by latent class decomposition.  Parameter Estimation The latent class model has an important geometrical  interpretation: the parameters qb -- P(wjlz ) define a low-dimensional subfamily  of the multinomial family, $(qb) = {r 6 [0;1] M : rj = y] qb for some  6  [0; 1] c, Y]k ;ha = 1}, i.e. all multinomials r that can be obtained by convex combi-  nations from the set of "basis" vectors {qb : I _< k <_ K}. For given d-parameters,   Extensions to the more general case are possible, but beyond the scope of this paper.  916 T. Hofmann  government president banks pct union marks gold billion  tax chairman debt january air currency steel dlrs  budget executive brazil february workers dollar plant year  cut chief new rise strike german mining surplus  spending officer loans rose airlines bundesbank copper deficit  cuts vice dlrs 1986 aircraft central tons foreign  deficit company bankers december port mark silver current  taxes named bank year boeing west metal trade  reform board payments fell employees dollars production account  billion director billion prices airline dealers ounces reserves  trading american trade oil vs areas food house  exchange general japan crude cts weather drug reagan  futures motors japanese energy net area study president  stock chrysler ec petroleum loss normal aids administration  options gm states prices mln good product congress  index car united bpd shr crop treatment white  contracts ford officials barrels qtr damage company secretary  market test community barrel revs caused environmental told  london cars european exploration profit affected products volcker  exchanges motor imports price note people approval reagans  Figure 1:16 selected factors from a 64 factor decomposition of the Reuters21578 col-  lection. The displayed terms are the 10 most probable words in the class-conditional  distribution P(wjlzk ) for 16 selected states zk after the exclusion of stop words.  each b i, b -- P(zkl&), will define a unique multinomial distribution r i e $(b).  Since $(b) defines a submanifold on the multinomial simplex, it corresponds to a  curved exponential subfamily? We would like to emphasis that we propose to learn  both, the parameters within the family (the b's or mixing proportions P(z I&)) and  the parameters that define the subfamily (the 's or class-conditionals  The standard procedure for maximum likelihood estimation in latent variable mod-  els is the Expectation Maximization (EM) algorithm. In the E-step one computes  posterior probabilities for the latent class variables,  P(z)P(d, lz)P(wlzk) P(z)P(dIz)P(wjlz) (2)  P(ldg, - Ez lzz)P(wl,) - P(d, wj)  The M-step formulae can be written compactly as  e(wlz)  N M { in  n=l m=l I  (3)  where d denotes the Kronecker delta.  Related Models As demonstrated in [7], the latent class model can be viewed  as a probabilistic variant of Latent Semantic Analysis [2], a dimension reduction  technique based on Singular Value Decomposition. It is also closely related to the  non-negative matrix decomposition discussed in [12] which uses a Poisson sampling  model and has been motivated by imposing non-negativity constraints on a decom-  position by PCA. The relationship of the latent class model to clustering models  like distributional clustering [14] has been investigated in [8]. [6] presents yet an-  other approach to dimension reduction for multinomials which is based on spherical  models, a different type of curved exponential subfamilies than the one presented  here which is affine in the mean-value parameterization.  2Notice that graphical models with latent variable are in general stratified exponential  families [4], yet in our case the geometry is simpler. The geometrical view also illustrates  the well-known identifiability problem in latent class analysis. The interested reader is  referred to [3]. As a practical remedy, we have used a Bayesian approach with conjugate  (Dirichlet) prior distributions over all multinomials which for the sake of clarity is not  described in this paper since it is very technical but nevertheless rather straightforward.  Learning the Similarity of Documents 917  3 Fisher Kernel and Information Geometry  The Fisher Kernel We follow the work of [9] to derive kernel functions (and  hence similarity functions) from generative data models. This approach yields a  uniquely defined and intrinsic (i.e. coordinate invariant) kernel, called the Fisher  kernel. One important implication is that yardsticks used for statistical models  carry over to the selection of appropriate similarity functions. In spite of the purely  unsupervised manner in which a Fisher kernel can be learned, the latter is also very  useful in supervised learning, where it provides a way to take advantage of addi-  tional unlabeled data. This is important in text learning, where digital document  databases and the World Wide Web offer a huge background text repository.  As a starting point, we partition the data log-likelihood into contributions from  the various documents. The average log-probability of a document di, i.e. the  probability of all the word occurrences in di normalized by document length is  given by,  M K  n(di, w:) (4)  l(d,) - D(wjld,) log P(wlx)P(xld,). P(w Ida)--  j=l k--1 '  which is up to constants the negative Kullback-Leibler divergence between the em-  pirical distribution tS(wjldi) and the model distribution represented by (1).  In order to derive the Fisher kernel, we have to compute the Fisher scores u(di;  i.e. the gradient of l(di) with respect to 0, as well as the Fisher information I(O) in  some parameterization 0 [13]. The Fisher kernel at  is then given by [9]  (di, dn) '- (u(di;),I()-lu(dn;)). (5)  Computational Considerations For computational reasons we propose to ap-  proximate the (inverse) information matrix by the identity matrix, thereby making  additional assumptions about information orthogonality. More specifically, we use  a variance stabilizing parameterization for multinomials - the square-root param-  eterization - which yields an isometric embedding of multinomial families on the  positive part of a hypersphere [11]. In this parameterization, the above approx-  imation will be exact for the multinomial family (disregarding the normalization  constraint). We conjecture that it will also provide a reasonable approximation in  the case of the subfamily defined by the latent class model.  C. Simplifying assumption: The Fisher information in the square-root param-  eterization can be approximated by the identity matrix.  Interpretation of Results Instead of going through the details of the derivation  which is postponed to the end of this section, it is revealing to relate the results back  to our main problem of defining a similarity function between text documents. We  will have a closer look at the two contributions resulting from different sets of pa-  rameters. The contribution which stems from (square-root transformed) parameters  P(zk) is (in a simplified version) given by  -  (6)  / is a weighted inner product in the low-dimensional factor representation of the  documents by mixing weights P(zldi). This part of the kernel thus computes a  "topical" overlap between documents and is thereby able to capture synonyms, i.e.,  words with an identical or similar meaning, as well as words referring to the same  918 T. Hofmann  topic. Notice, that it is not required that di and d, actually have (many) terms in  common in order to get a high similarity score.  The contribution due to the parameters P(wjlzk ) is of a very different type. Again  using the approximation of the Fisher matrix, we arrive at the inner product  g:(d,,d,)-  P(w:ld,)P(wald,) }-' P(x'ld" wa)P(xld''wa)  j  '(wjlz)  / has also a very appealing interpretation: It essentially computes an inner product  between the empirical distributions of di and d,, a scheme that is very popular in  the context of information retrieval in the vector space model. However, common  words only contribute, if they are explained by the same factor(s), i.e., if the re-  specfive posterior probabilities overlap. This allows to capture words with multiple  meanings, so-called polysems. For example, in the factors displayed in Figure I the  term "president" occurs twice (as the president of a company and as the president  of the US). Depending on the document the word occurs in, the posterior proba-  bility will be high for either one of the factors, but typically not for both. Hence,  the same term used in different context and different meanings will generally not  increase the similarity between documents, a distinction that is absent in the naive  inner product which corresponds to the degenerate case of K - 1.  Since the choice of K determines the coarseness of the identified "topics" and dif-  ferent resolution levels possibly contribute useful information, we have combined  models by a simple additive combination of the derived inner products. This com-  bination scheme has experimentally proven to be very effective and robust.  D. Modeling assumption: Similarities derived from latent class decompositions  at different levels of resolution are additively combined.  In summary, the emergence of important language phenomena like synonymy and  polysemy from information-geometric principles is very satisfying and proves in  our opinion that interesting similarity functions can be rigorously derived, without  specific domain knowledge and based on few, explicitly stated assumptions (A-D).  Technical Derivation Define pj = 2v/P(wlz,), then  _ P(wvl&) P(z[di)  Ol(di) _ Ol(di) OP(walz) = )  Op OP(wJzk) Op  Similarly we define p = 2. Applying Bayes' rule to substitute P(zd) in  l(di) (i.e. P(z[di) P(z)P(d,]z)/P(d,)) yields  - v  Op OP(z) Op   _  The lt (optional) approximation step makes sense whenever P(wj ldi)  P(wj ldi).  Notice that we have ignored the normalization constraints which would yield a  (reactive) term that is constant for each multinomial. Experimentally, we have  observed no deterioration in performance by making these additional simplifications.  Learning the Similarity of Docurnents 919  Medline Cranfield CACM CISI  VSM 44.3 29.9 17.9 12.7  VSM++ 67.2 37.9 27.5 20.3  Table 1: Average precision results for the vector space baseline method (VSM)  and the Fisher kernel approach (VSM++) for 4 standard test collections, Medline,  Cranfield, CACM, and CISI.  earn acq money grain crude average improv.  20x sub SVM 5.51 7.67 3.25 2.06 2.50 4.20 -  SVM++ 4.56 5.37 2.08 1.71 1.53 3.05 +27.4%  kNN 5.91 9.64 3.24 2.54 2.42 4.75 -  kNN++ 5.05 7.80 3.11 2.35 1.95 4.05 +14.7%  10x sub SVM 4.88 5.54 2.38 1.71 1.88 3.27 -  SVM++ 4.11 4.84 2.08 1.42 1.45 2.78 +15.0%  kNN 5.51 9.23 2.64 2.55 2.42 4.47 -  kNN++ 4.94 7.47 2.42 2.28 1.88 3.79 +15.2%  5x sub SVM 4.09 4.40 2.10 1.32 1.46 2.67 -  SVM++ 3.64 4.15 1.78 0.98 1.19 2.35 +12.1%  kNN 5.13 8.70 2.27 2.40 2.23 4.14 -  kNN++ 4.74 6.99 2.22 2.18 1.74 3.57 +13.7%  all data SVM 2.92 3.21 1.20 0.77 0.92 1.81 -  10x cv SVM++ 2.98 3.15 1.21 0.76 0.86 1.79 +0.6%  kNN 4.17 6.69 1.78 1.73 1.42 3.16 -  kNN++ 4.07 5.34 1.73 1.58 1.18 2.78 +12.0%  Table 2: Classification errors for k-nearest neighbors (kNN) SVMs (SVM) with the  naive kernel and with the Fisher kernel(++) (derived from K = 1 and K = 64  models) on the 5 most frequent categories of the Reuters21578 corpus (earn, acq,  monex-fx, grain, and crude) at different subsampling levels.  4 Experimental Results  We have applied the proposed method for ad hoc information retrieval, where the  goal is to return a list of documents, ranked with respect to a given query. This  obviously involves computing similarities between documents and queries. In a  follow-up series of experiments to the ones reported in [7] - where kernels/C(di, d,:  k P(zk[di)P(zld, and ](di, dn) = Y']j P(wjldi)5(wjld, have been proposed in  an ad hoc manner - we have been able to obtain a rigorous theoretical justification  as well as some additional improvements. Average precision-recall values for four  standard test collections reported in Table 1 show that substantial performance  gains can be achieved with the help of a generative model (cf. [7] for details on the  conducted experiments).  To demonstrate the utility of our method for supervised learning problems, we have  applied it to text categorization, using a standard data set in the evaluation, the  Reuters21578 collections of news stories. We have tried to boost the performance  of two classifiers that are known to be highly competitive for text categorization:  the k-nearest neighbor method and Support Vector Machines (SVMs) with a linear  kernel [10]. Since we are particularly interested in a setting, where the generative  model is trained on a larger corpus of unlabeled data, we have run experiments where  the classifier was only trained on a subsample (at subsampling factors 20x,10x,Sx).  The results are summarized in Table 2. Free parameters of the base classifiers have  been optimized in extensive simulations with held-out data. The results indicate  920 T. Hofmann  that substantial performance gains can be achieved over the standard k-nearest  neighbor method at all subsampling levels. For SVMs the gain is huge on the  subsampled data collections, but insignificant for SVMs trained on all data. This  seems to indicate that the generative model does not provide any extra information,  if the SVM classifier is trained on the same data. However, notice that many  interesting applications in text categorization operate in the small sample limit  with lots of unlabeled data. Examples include the definition of personalized news  categories by just a few example, the classification and/or filtering of email, on-line  topic spotting and tracking, and many more.  5 Conclusion  We have presented an approach to learn the similarity of text documents from  first principles. Based on a latent class model, we have been able to derive a  similarity function, that is theoretically satisfying, intuitively appealing, and shows  substantial performance gains in the conducted experiments. Finally, we have made  a contribution to the relationship between unsupervised and supervised learning as  initiated in [9] by showing that generative models can help to exploit unlabeled data  for classification problems.  References  [1] Shun'ichi Amari. Differential-geometrical methods in statistics. Springer-Verlag,  Berlin, New York, 1985.  [2] S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Landauer, and R. Harshman.  Indexing by latent semantic analysis. Journal of the American Society for Information  Science, 41:391-407, 1990.  [3] M. J. Evans, Z. Gilula, and I. Guttman. Latent class analysis of two-way contingency  tables by Bayesian methods. Biometrika, 76(3):557-563, 1989.  [4] D. Geiger, D. Heckerman, H. King, and C. Meek. Stratified exponential families:  Graphical models and model selection. Technical Report MSR-TR-98-31, Microsoft  Research, 1998.  [5] Z. Gilula and S. J. Haberman. Canonical analysis of contingency tables by maximum  likelihood. Journal of the American Statistical Association, 81(395):780-788, 1986.  [6] A. Gous. Exponential and Spherical Subfamily Models. PhD thesis, Stanford, Statistics  Department, 1998.  [7] T. Hofmann. Probabilistic latent semantic indexing. In Proceedings of the th Inter-  national Conference on Research and Development in Information Retrieval (SIGIR),  pages 50-57, 1999.  [8] T. Hofmann, J. Puzicha, and M. I. Jordan. Unsupervised learning from dyadic data.  In Advances in Neural Information Processing Systems 11. MIT Press, 1999.  [9] T. Jaakkola and D. Haussler. Exploiting generative models in discriminative classi-  tiers. In Advances in Neural Information Processing Systems 11. MIT Press, 1999.  [10] T. Joachims. Text categorization with support vector machines: Learning with many  relevant features. In International Conference on Machine Learning (ECML), 1998.  [11] R.E. Kass and P. W. Vos. Geometrical foundations of asymptotic inference. Wiley,  New York, 1997.  [12] D. Lee and S. Seung. Learning the parts of objects by non-negative matrix factoriza-  tion. Nature, 401:788-791, 1999.  [13] M. K. Murray and J. W. Rice. Differential geometry and statistics. Chapman &; Hall,  London, New York, 1993.  [14] F.C.N. Pereira, N.Z. Tishby, and L. Lee. Distributional clustering of English words.  In Proceedings of the A CL, pages 183-190, 1993.  [15] L. Saul and F. Pereira. Aggregate and mixed-order Markov models for statistical  language processing. In Proceedings of the nd International Conference on Empirical  Methods in Natural Language Processing, 1997.  
Predictive Sequence Learning in Recurrent  Neocortical Circuits*  R. P. N. Rao  Computational Neurobiology Lab and  Sloan Center for Theoretical Neurobiology  The Salk Institute, La Jolla, CA 92037  raosalk. edu  T. J. Sejnowski  Computational Neurobiology Lab and  Howard Hughes Medical Institute  The Salk Institute, La Jolla, CA 92037  terrysalk. edu  Abstract  Neocortical circuits are dominated by massive excitatory feedback: more  than eighty percent of the synapses made by excitatory cortical neurons  are onto other excitatory cortical neurons. Why is there such massive re-  current excitation in the neocortex and what is its role in cortical compu-  tation? Recent neurophysiological experiments have shown that the plas-  ticity of recurrent neocortical synapses is governed by a temporally asym-  metric Hebbian learning rule. We describe how such a rule may allow  the cortex to modify recurrent synapses for prediction of input sequences.  The goal is to predict the next cortical input from the recent past based on  previous experience of similar input sequences.. We show that a temporal  difference learning rule for prediction used in conjunction with dendritic  back-propagating action potentials reproduces the temporally asymmet-  ric Hebbian plasticity observed physiologically. Biophysical simulations  demonstrate that a network of cortical neurons can learn to predict mov-  ing stimuli and develop direction selective responses as a consequence of  learning. The space-time response properties of model neurons are shown  to be similar to those of direction selective cells in alert monkey V1.  1 INTRODUCTION  The neocortex is characterized by an extensive system of recurrent excitatory connections  between neurons in a given area. The precise computational function of this massive re-  current excitation remains unknown. Previous modeling studies have suggested a role for  excitatory feedback in amplifying feedforward inputs [ 1 ]. Recently, however, it has been  shown that recurrent excitatory connections between cortical neurons are modified accord-  ing to a temporally asymmetric Hebbian learning rule: synapses that are activated slightly  before the cell fires are strengthened whereas those that are activated slightly after are weak-  ened [2, 3]. Information regarding the postsynaptic activity of the cell is conveyed back to  the dendritic locations of synapses by back-propagating action potentials from the soma.  In this paper, we explore the hypothesis that recurrent excitation subserves the function of  prediction and generation of temporal sequences in neocortical circuits [4, 5, 6]. We show  'This research was supported by the Sloan Foundation and Howard Hughes Medical Institute.  Predictive Sequence Learning in Recurrent Neocortical Circuits 165  that a temporal difference based learning rule for prediction applied to backpropagating ac-  tion potentials reproduces the experimentally observed phenomenon of asymmetric Heb-  bian plasticity. We then show that such a learning mechanism can be used to learn temporal  sequences and the property of direction selectivity emerges as a consequence of learning to  predict moving stimuli. Space-time response plots of model neurons are shown to be similar  to those of direction selective cells in alert macaque V1.  2  TEMPORALLY ASYMMETRIC HEBBIAN PLASTICITY AND  TEMPORAL DIFFERENCE LEARNING  To accurately predict input sequences, the recurrent excitatory connections in a network  need to be adjusted such that the appropriate set of neurons are activated at each time step.  This can be achieved by using a "temporal-difference" (TD) learning rule [5, 7]. In this  paradigm of synaptic plasticity, an activated synapse is strengthened or weakened based on  whether the difference between two temporally-separated predictions is positive or nega-  tive. This minimizes the errors in prediction by ensuring that the prediction generated by  the neuron after synaptic modification is closer to the desired value than before (see [7] for  more details).  In order to ascertain whether temporally-asymmetric Hebbian learning in cortical neurons  can be interpreted as a form of temporal-difference learning, we used a two-compartment  model of a cortical neuron consisting of a dendrite and a soma-axon compartment. The com-  partmental model was based on a previous study that demonstrated the ability of such a  model to reproduce a range of cortical response properties [8]. The presence of voltage-  activated sodium channels in the dendrite allowed back-propagation of action potentials  from the soma into the dendrite. To study plasticity, excitatory postsynaptic potentials (EP-  SPs) were elicited at different time delays with respect to postsynaptic spiking by presynap-  tic activation of a single excitatory synapse located on the dendrite. Synaptic currents were  calculated using a kinetic model of synaptic transmission with model parameters fitted to  whole-cell recorded AMPA currents (see [9] for more details). Synaptic plasticity was sim-  ulated by incrementing or decrementing the value for maximal synaptic conductance by an  amount proportional to the temporal-difference in the postsynaptic membrane potential at  time instants t + At and t - At for presynaptic activation at time t. The delay parameter  At was set to 5 ms to yield results consistent with previous physiological experiments [2].  Presynaptic input to the model neuron was paired with postsynaptic spiking by injecting  a alepolarizing current pulse (10 ms, 200 pA) into the soma. Changes in synaptic efficacy  were monitored by applying a test stimulus before and after pairing, and recording the EPSP  evoked by the test stimulus.  Figure 1A shows the results of pairings in which the postsynaptic spike was triggered 5 ms  after and 5 ms before the onset of the EPSP respectively. While the peak EPSP amplitude  was increased 58.5% in the former case, it was decreased 49.4% in the latter case, qualita-  tively similar to experimental observations [2]. The critical window for synaptic modifica-  tions in the model depends on the parameter At as well as the shape of the back-propagating  action potential. This window of plasticity was examined by varying the time interval be-  tween presynaptic stimulation and postsynaptic spiking (with At = 5 ms). As shown in  Figure lB, changes in synaptic efficacy exhibited a highly asymmetric dependence on spike  timing similar to physiological data [2]. Potentiation was observed for EPSPs that occurred  between 1 and 12 ms before the postsynaptic spike, with maximal potentiation at 6 ms. Max-  imal depression was observed for EPSPs occurring 6 ms after the peak of the postsynaptic  spike and this depression gradually decreased, approaching zero for delays greater than 10  ms. As in rat neocortical neurons, Xenopus tectal neurons, and cultured hippocampal neu-  rons (see [2]), a narrow transition zone (roughly 3 ms in the model) separated the potentia-  tion and depression windows.  166 R. P. N. Rao and T. J. Sejnowski  A  before  pairing  'after  15 ms  15 ms  150  IO0  -100  -40  -20 0 20  Time of Synapfic Input (ms)  40  Figure 1: Synaptic Plasticity in a Model Neocortical Neuron. (A) (Left Panel) EPSP in the model  neuron evoked by a presynaptic spike (S 1) at an excitatory synapse ("before"). Pairing this presynap-  tic spike with postsynaptic spiking after a 5 ms delay ("pairing") induces long-term potentiation ("af-  ter"). (Right Panel) If presynaptic stimulation (S2) occurs 5 ms after postsynaptic firing, the synapse  is weakened resulting in a corresponding decrease in peak EPSP amplitude. (B) Critical window for  synaptic plasticity obtained by varying the delay between pre- and postsynaptic spiking (negative de-  lays refer to presynaptic before postsynaptic spiking).  3 RESULTS  3.1 Learning Sequences using Temporally Asymmetric Hebbian Plasticity  To see how a network of model neurons can learn sequences using the learning mechanism  described above, consider the simplest case of two excitatory neurons N 1 and N2 connected  to each other, receiving inputs from two separate input neurons I 1 and 12 (Figure 2A). Sup-  pose input neuron I1 fires before input neuron 12, causing neuron N1 to fire (Figure 2B).  The spike from N1 results in a sub-threshold EPSP in N2 due to the synapse S2. If input  arrives from 12 any time between 1 and 12 ms after this EPSP and the temporal summation  of these two EPSPs causes N2 to fire, the synapse S2 will be strengthened. The synapse S 1,  on the other hand, will be weakened because the EPSP due to N2 arrives a few milliseconds  after N1 has fired. Thus, on a subsequent trial, when input I1 causes neuron N1 to fire, N1  in turn causes N2 to fire several milliseconds before input 12 occurs due to the potentiation  of the recurrent synapse S2 in previous trial(s) (Figure 2C). Input neuron 12 can thus be in-  hibited by the predictive feedback from N2 just before the occurrence of imminent input  activity (marked by an asterisk in Figure 2C). This inhibition prevents input 12 from further  exciting N2. Similarly, a positive feedback loop between neurons N1 and N2 is avoided  because the synapse S 1 was weakened in previous trial(s) (see arrows in Figures 2B and  2C). Figure 2D depicts the process of potentiation and depression of the two synapses as a  function of the number of exposures to the 11-12 input sequence. The decrease in latency  of the predictive spike elicited in N2 with respect to the timing of input 12 is shown in Fig-  ure 2E. Notice that before learning, the spike occurs 3.2 ms after the occurrence of the input  whereas after learning, it occurs 7.7 ms before the input.  3.2 Emergence of Direction Selectivity  In a second set of simulations, we used a network of recurrently connected excitatory neu-  rons as shown in Figure 3A receiving retinotopic sensory input consisting of moving pulses  of excitation (8 ms pulse of excitation at each neuron) in the rightward and leftward direc-  tions. The task of the network was to predict the sensory input by learning appropriate recur-  rent connections such that a given neuron in the network starts firing several milliseconds  before the arrival of its input pulse of excitation. The network was comprised of two paral-  lel chains of neurons with mutual inhibition (dark arrows) between corresponding pairs of  neurons along the two chains. The network was initialized such that within a chain, a given  Predictive Sequence Learning in Recurrent Neocortical Circuits 167  A D  Excitatory Neuron N 1  S1 S2  Excitatory Neuron N2  Input Neuron 11 }   Input I   -< ( Input Neuron 12  In ut 2  B  Before Learning  N2  c  After Learning  N2  15 ms  12  g  15 ms  0.0  0.02  0.02  0.015  0.01  0.005  E  Synapse S2   ,ooooooooooooOOoo   o SynapSl  0 1 2 0 4  Time (number of trials)  -4  -6  -8  2 io 4b  Time (number of trials)  Figure 2: Learning to Predict using Temporally Asymmetric Hebbian Learning. (A) Network  of two model neurons N1 and N2 recurrently connected via excitatory synapses S 1 and S2, with input  neurons I 1 and I2. N 1 and N2 inhibit the input neurons via inhibitory interneurons (darkened circles).  (B) Network activity elicited by the sequence I1 followed by I2. (C) Network activity for the same  sequence after 40 trials of learning. Due to strengthening of recurrent synapse S2, recurrent excita-  tion from N1 now causes N2 to fire several ms before the expected arrival of input I2 (dashed line),  allowing it to inhibit I2 (asterisk). Synapse S 1 has been weakened, preventing re-excitation of N 1  (downward arrows show decrease in EPSP). (D) Potentiation and depression of synapses S 1 and S2  respectively during the course of learning. Synaptic strength was defined as maximal synaptic conduc-  tance in the kinetic model of synaptic transmission [9]. (E) Latency of predictive spike in N2 during  the course of learning measured with respect to the time of input spike in I2 (dotted line).  excitatory neuron received both excitation and inhibition from its predecessors and succes-  sors (Figure 3B). Excitatory and inhibitory synaptic currents were calculated using kinetic  models of synaptic transmission based on properties of AMPA and GABAA receptors as  determined from whole-cell recordings [9]. Maximum conductances for all synapses were  initialized to small positive values (dotted lines in Figure 3C) with a slight asymmetry in  the recurrent excitatory connections for breaking symmetry between the two chains.  The network was exposed alternately to leftward and rightward moving stimuli for a total of  100 trials. The excitatory connections (labeled 'EXC' in Figure 3B) were modified accord-  ing to the asymmetric Hebbian learning rule in Figure lB while the excitatory connections  onto the inhibitory interneuron (labeled 'INH') were modified according to an asymmetric  anti-Hebbian learning rule that reversed the polarity of the rule in Figure lB. The synaptic  conductances learned by two neurons (marked N1 and N2 in Figure 3A) located at corre-  sponding positions in the two chains after 100 trials of exposure to the moving stimuli are  shown in Figure 3C (solid line). Initially, for rightward motion, the slight asymmetry in  168 R. P N. Rao and T. J. Sejnowski  A  Input Stimulus (Rightward)  Recurrent Excitatory Connections (EXC)  -3 -2 -1 0 1 2 3 4  Recurrent Inhibitory Connections (INH)  -3 -2 -1 0 I 2 3 4  C Neuron N 1  o Ol 5  Synapse Number  Neuron N2  -2 0 2 4  o.o1:. A INH  ool   0oo4  o  Synapse Number  D  Neuron N 1  (Right-Selective)  30 ms  Neuron N2  (Left-Selective)  Rightward  Motion  Leftward  Motion  Figure 3.' Direction Selectivity in the Model. (A) A model network consisting of two chains of  recurrently connected neurons receiving retinotopic inputs. A given neuron receives recurrent excita-  tionand recurrent inhibition (white-headed arrows) as well as inhibition (dark-headed arrows) from its  counterpart in the other chain. (B) Recurrent connections to a given neuron (labeled '0') arise from  4 preceding and 4 succeeding neurons in its chain. Inhibition at a given neuron is mediated via a  GABAergic interneuron (darkened circle). (C) Synaptic strength of recurrent excitatory (EXC) and in-  hibitory (INH) connections to neurons N 1 and N2 before (dotted lines) and after learning (solid lines).  Synapses were adapted during 100 trials of exposure to alternating leftward and rightward moving  stimuli. (D) Responses of neurons N1 and N2 to rightward and leftward moving stimuli. As a result  of learning, neuron N 1 has become selective for rightward motion (as have other neurons in the same  chain) while neuron N2 has become selective for leftward motion. In the preferred direction, each  neuron starts firing several milliseconds before the actual input arrives at its soma (marked by an as-  terisk) due to recurrent excitation from preceding neurons. The dark triangle represents the start of  input stimulation in the network.  the initial excitatory connections of neuron N1 allows it to fire slightly earlier than neuron  N2 thereby inhibiting neuron N2. Additionally, since the EPSPs from neurons lying on the  left of N1 occur before N1 fires, the excitatory synapses from these neurons are strength-  ened while the excitatory synapses from these same neurons to the inhibitory interneuron are  weakened according to the two learning rules mentioned above. On the other hand, the ex-  citatory synapses from neurons lying on the right side of N1 are weakened while inhibitory  connections are strengthened since the EPSPs due to these connections occur after N1 has  fired. The synapses on neuron N2 and its associated interneuron remain unaltered since  there is no postsynaptic firing (due to inhibition by N1) and hence no back-propagating ac-  tion potentials in the dendrite. As shown in Figure 3C, after 100 trials, the excitatory and  inhibitory connections to neuron N1 exhibit a marked asymmetry, with excitation originat-  ing from neurons on the left and inhibition from neurons on the right. Neuron N2 exhibits  the opposite pattern of connectivity. As expected, neuron N 1 was found to be selective for  rightward motion while neuron N2 was selective for leftward motion (Figure 3D). More-  over, when stimulus motion is in the preferred direction, each neuron starts firing several  milliseconds before the time of arrival of the input stimulus at its soma (marked by an as-  terisk) due to recurrent excitation from preceding neurons. Conversely, motion in the non-  preferred direction triggers recurrent inhibition from preceding neurons as well as inhibition  Predictive Sequence Learning in Recurrent Neocortical Circuits 169  Monkey Data  limo  Model  50 100  Ostimulus time (ms)  Figure 4: Comparison of Monkey and Model Space-Time Response Plots. (Left) Sequence of  PSTHs obtained by flashing optimally oriented bars at 20 positions across the 5 -wide receptive field  (RF) of a complex cell in alert monkey V 1 (from [11 ]). The cell's preferred direction is from the part  of the RF represented at the bottom towards the top. Flash duration = 56 ms; inter-stimulus delay =  100 ms; 75 stimulus presentations. (Right) PSTHs obtained from a model neuron after stimulating the  chain of neurons at 20 positions to the left and fight side of the given neuron. Lower PSTHs represent  stimulations on the preferred side while upper PSTHs represent stimulations on the null side.  from the active neuron in the corresponding position in the other chain. Thus, the learned  pattern of connectivity allows the direction selective neurons comprising the two chains in  the network to conjointly code for and predict the moving input stimulus in each direction.  The average firing rate of neurons in the network for the preferred direction was 75.7 Hz,  which is in the range of cortical firing rates for moving bar stimuli. Assuming a 200 Fm  separation between excitatory model neurons in each chain and utilizing known values for  the cortical magnification factor in monkey striate cortex, one can estimate the preferred  stimulus velocity of model neurons to be 3.1 /s in the fovea and 27.9/s in the periphery (at  an eccentricity of 8). Both of these values fall within the range of monkey striate cortical  velocity preferences [ 11].  The model predicts that the neuroanatomical connections for a direction selective neuron  should exhibit a pattern of asymmetrical excitation and inhibition similar to Figure 3C. A  recent study of direction selective cells in awake monkey V 1 found excitation on the pre-  ferred side of the receptive field and inhibition on the null side consistent with the pattern of  connections learned by the model [ 11 ]. For comparison with this experimental data, sponta-  neous background activity in the model was generated by incorporating Poisson-distributed  random excitatory and inhibitory alpha synapses on the dendrite of each model neuron. Post  stimulus time histograms (PSTHs) and space-time response plots were obtained by flashing  optimally oriented bar stimuli at random positions in the cell's activating region. As shown  in Figure 4, there is good qualitative agreement between the response plot for a complex cell  and that for the model. Both space-time plots show a progressive shortening of response  onset time and an increase in response transiency going in the preferred direction: in the  model, this is due to recurrent excitation from progressively closer cells on the preferred  side. Firing is reduced to below background rates 40-60 ms after stimulus onset in the up-  per part of the plots: in the model, this is due to recurrent inhibition from cells on the null  side. The response transiency and shortening of response time course appears as a slant in  the space-time maps, which can be related to the neuron's velocity sensitivity [11].  170 R. P. N. Rao and T. J. Sejnowski  4 CONCLUSIONS  Our results show that a network of recurrently connected neurons endowed with a temporal-  difference based asymmetric Hebbian learning mechanism can learn a predictive model of  its spatiotemporal inputs. When exposed to moving stimuli, neurons in a simulated net-  work learned to fire several milliseconds before the expected arrival of an input stimulus  and developed direction selectivity as a consequence of learning. The model predicts that a  direction selective neuron should start responding several milliseconds before the preferred  stimulus enters its retinal input dendritic field (such predictive neural activity has recently  been reported in retinal ganglion cells [10]). Temporally asymmetric Hebbian learning has  previously been suggested as a possible mechanism for sequence learning in the hippocam-  pus [4] and as an explanation for the asymmetric expansion of hippocampal place fields  during route learning [12]. Some of these theories require relatively long temporal win-  dows of synaptic plasticity (on the order of several hundreds of milliseconds) [4] while oth-  ers have utilized temporal windows in the millisecond range for coincidence detection [3].  Sequence learning in our model is based on a window of plasticity in the 10 to 15 ms range  which is roughly consistent with recent physiological observations [2] (see also [13]). The  idea that prediction and sequence learning may constitute an important goal of the neocortex  has previously been suggested in the context of statistical and information theoretic models  of cortical processing [4, 5, 6]. Our biophysical simulations suggest a possible implementa-  tion of such models in cortical circuitry. Given the universality of the problem of encoding  and generating temporal sequences in both sensory and motor domains, the hypothesis of  predictive sequence learning in recurrent neocortical circuits may help provide a unifying  principle for studying cortical structure and function.  References  [lO]  [11]  [12]  [13]  [1] R. J. Douglas et al., Science 269, 981 (1995); H. Suarez et al., J. Neurosci. 15, 6700 (1995);  R. Maex and G. A. Orban, J. Neurophysiol. 75, 1515 (1996); P. Mineiro and D. Zipser, Neural  Cornput. 10, 353 (1998); E S. Chance et al., Nature Neuroscience 2, 277 (1999).  [2] H. Markram et al., Science 275, 213 (1997); W. B. Levy and O. Steward, Neuroscience 8, 791  (1983); D. Debanne et al., Proc. Natl. Acad. Sci. U.S.A. 91, 1148 (1994); L. I. Zhang et al., Na-  ture 395, 37 (1998); G. Q. Bi and M. M. Poo, J. Neurosci. 18, 10464 (1998).  [3] W. Gerstner et al., Nature 383, 76 (1996); R. Kempter et al., inAdvances in Neural Info. Proc.  Systems 11, M. S. Kearns, S. A. Solla and D. A. Cohn, Eds. (MIT Press, Cambridge, MA, 1999),  pp. 125-131.  [4] L. E Abbott and K. I. Blum, Cereb. Cortex6, 406 (1996); W. Gerstner and L. E Abbott, J. Corn-  put. Neurosci. 4, 79 (1997); A. A. Minai and W. B. Levy, in Proceedings of the 1993 World  Congress on Neural Networks II, 505 (1993).  [5] P. R. Montague and T. J. Sejnowski, Learning and Memory 1, 1 (1994); P. R. Montague et al.,  Nature 377, 725 (1995); W. Schultz et al., Science 275, 1593 (1997).  [6] R. P. N. Rao and D. H. Ballard, Neural Computation 9, 721 (1997); R. P. N. Rao and D. H.  Ballard, Nature Neuroscience 2, 79 (1999); H. Barlow, Perception 27, 885 (1998).  [7] R.S. Sutton, Machine Learning 3, 9 (1988); R. S. Sutton and A. G. Barto, in Learning and Com-  putational Neuroscience: Foundations of Adaptive Networks, M. Gabriel and J. W. Moore, ed-  itors (MIT Press, Cambridge, MA, 1990).  [8] Z. E Mainen and T. J. Sejnowski, Nature 382, 363 (1996).  [9] A. Destexhe et al., in Methods in Neuronal Modeling, C. Koch and I. Segev, editors, (MIT Press,  Cambridge, MA, 1998).  M. J. Berry et al., Nature 398, 334 (1999).  M. S. Livingstone, Neuron 20, 509 (1998).  M. R. Mehta et al., Proc. Natl. Acad. Sci. U.S.A. 94, 8918 (1997).  L. E Abbott and S. Song, inAdvances in Neural Info. Proc. Systems 11, M. S. Kearns, S. A. Solla  and D. A. Cohn, Eds. (MIT Press, Cambridge, MA, 1999), pp. 69-75.  
Resonance in a Stochastic Neuron Model  with Delayed Interaction  Toru Ohira*  Sony Computer Science Laboratory  3-14-13 Higashi-gotanda  Shinagawa, Tokyo 141, Japan  ohira @csl. sony. co.jp  Yuzuru Sato  Institute of Physics,  Graduate School of Arts and Science, University of Tokyo  3-8-1 Komaba, Meguro, Tokyo 153 Japan  ysato @sacral. c. u-tokyo. ac.jp  Jack D. Cowan  Department of Mathematics  University of Chicago  5734 S. University, Chicago, IL 60637, U.S.A  cowan@math. uchicago. edu  Abstract  We study here a simple stochastic single neuron model with delayed  self-feedback capable of generating spike trains. Simulations show  that its spike trains exhibit resonant behavior between "noise" and  "delay". In order to gain insight into this resonance, we simplify  the model and study a stochastic binary element whose transition  probability depends on its state at a fixed interval in the past.  With this simplified model we can analytically compute interspike  interval histograms, and show how the resonance between noise and  delay arises. The resonance is also observed when such elements  are coupled through delayed interaction.  I Introduction  "Noise" and "delay" are two elements which are associated with many natural  and artificial systems and have been studied in diverse fields. Neural networks  provide representative examples of information processing systems with noise and  delay. Though much research has gone into the investigation of these two factors  in the community, they have mostly been separately studied (see e.g. [1]). Neural  *Affiliated also with the Laboratory for Information Synthesis, RIKEN Brain Science  Institute, Wako, Saitama, Japan  Resonance in a Stochastic Neuron Model with Delayed Interaction 315  models incorporating both noise and delay are more realistic [2], but their complex  characteristics have yet to be explored both theoretically and numerically.  The main theme of this paper is the study of a simple stochastic neural model  with delayed interaction which can generate spike trains. The most striking feature  of this model is that it can show a regular spike pattern with suitably "tuned"  noise and delay [3]. Stochastic resonance in neural information processing has been  investigated by others (see e.g. [4]). This model, however, introduces a different  type of such resonance, via delay rather than through an external oscillatory signal.  It can be classified with models of stochastic resonance without an external signal  [5].  The novelty of this model is the use of delay as the source of its oscillatory dynamics.  To gain insight into the resonance, we simplify the model and study a stochastic  binary element whose transition probability depends on its state at a fixed inter-  val in the past. With this model, we can analytically compute interspike interval  histograms, and show how the resonance between noise and delay arises. We fur-  ther show that the resonance also occurs when such stochastic binary elements are  coupled through delayed interaction.  2 Single Delayed-feedback Stochastic Neuron Model  Our model is described by the following equations:  utu(t) :  (u(t)) =  -u(t) + m(u(t - + e(t)  2  1 + e-.(v(t) -)  (1)  where r/and 0 are constants, and V is the membrane potential of the neuron. The  noise term L has the following probability distribution.  1  : 2  : 0 (u < -L, > L), (2)  i.e., z; is a time uncorrelated uniformly distributed noise in the range (-L, L). It  can be interpreted as a fluctuation that is much faster than the membrane relaxation  time/. The model can be interpreted as a stochastic neuron model with delayed  self-feedback of weight W, which is an extension of a model with no delay previously  studied using the Fokker-Planck equation [6].  We numerically study the following discretized version:  2  V(t + 1) = 1 + e-,(v(t-') -) - 1 + z; (3)  We fix r/and 0 so that this map has two basins of attractors of differing size with  no delay, as shown in Figure i(A). We have simulated the map (3) with various  noise widths and delays and find regular spiking behavior as shown in Fig i(C) for  tuned noise width and delay. In case the noise width is too large or too small given  self-feedback delay, this rhythmic behavior does not emerge, as shown in Figl(B)  and (D).  We argue that the delay changes the effective shape of the basin of attractors into  an oscillatory one, just like that due to an external oscillating force which, as is  well-known, leads to stochastic resonance with a tuned noise width. The analysis of  the dynamics given by (1) or (3), however, is a non-trivial task, particularly with  316 T. Ohira, Y. Sato and J. D. Cowan  respect to the spike trains. A previous analysis using the Fokker-Planck equation  cannot capture this emergence of regular spiking behavior. This difficulty motivates  us to further simplify our model, as described in the next section.  (A) I (E) l.q  _ _  i(b)v q P  (B)  v(t) x(t)  (c)  v(t)  (D)  v(t)  (G)  x(t)  x(t)  Figure 1: (A) The shape of the sigmoid function qb (b) for r/= 4 and 0 = 0.1. The  straight line (a) is qb = V and the crossings of the two lines indicate the stationary  point of the dynamics. Also, the typical dynamics of V(t) from the map model are  shown as we change noise width L. The values of L are (B) L = 0.2, (C) L = 0.4,  (D) L = 0.7. The data is taken with r = 20, r/ = 4.0, 0 = 0.1 and the initial  condition V(t) = 0.0 for t C [-r, 0]. The plots are shown between t = 0 to 1000.  (E) Schematic view of the single binary model. Some typical dynamics from the  binary model are also shown. The values of parameters are r = 10, q = 0.5, and  (F) p = 0.005, (G) p = 0.05, and (H) p = 0.2.  3 Delayed Stochastic Binary Neuron Model  The model we now discuss is an approximation of the dynamics that retains the  asymmetric stochastic transition and delay. The state X(t) of the system at time  step t is either -1 or 1. With the same noise , the model is described as follows:  f(.)  = o[f(x(t- r)) +  = + b) + - b)),  = 1 (0_<n), -1 (0>n),  (4)  where a and b are parameters such that ]a[ <_ L and lb[ _< L, and r is the delay.  This model is an approximate discretization of the space of map (3) into two states  Resonance in a Stochastic Neuron Model with Delayed Interaction 317  with a and b controlling the bias of transition depending on the state of X r steps  earlier. When a  b, the transition between the two states is asymmetric, reflecting  the two differing sized basins of attractors.  We can describe this model more concisely in probability space (Figure i(E)). The  formal definition is given as follows:  P(1, t+l) = p, X(t-r)=-l,  : i-q, X(t-r)=l,  P(-1, t+l) = q, X(t-r)=l,  = i-p, X(t-r)--1,  p = (1+),  q = (1-), (5)  where P(s, t) is the probability that X(t) = s. Hence, the transition probability of  the model depends on its state r steps in the past, and is a special case of a delayed  random walk [7].  We randomly generate X(t) for the interval t = (-r, 0). Simulations are performed  in which parameters are varied and X(t) is recorded for up to 10 6 steps. They  appear to be qualitatively similar to those generated by the map dynamics (Figure  i(F),(G),(H)). From the trajectory X(t), we construct a residence time histogram  h(u) for the system to be in the state -1 for u consecutive steps. Some examples  of the histograms are shown in Figure 2 (q = I - q = 0.5, r - 10). We note that  with p << 0.5, as in Figure 2(A), the model has a tendency to switch or spike to  the X -- 1 state after the time step interval of r. But the spike trains do not last  long and result in a small peak in the histogram. For the case of Figure 2(C) where  p is closer to 0.5, we observe less regular transitions and the peak height is again  small. With appropriate p as in Figure 2(B), spikes tend to appear at the interval r  more frequently, resulting in higher peaks in the histogram. This is what we mean  by stochastic resonance (Figure 2(D)). Choosing an appropriate p is equivalent to  "tuning" the noise width L, with other parameters appropriately fixed. In this  sense, our model exhibits stochastic resonance.  This model can be treated analytically. The first observation to make with the  model is that given r, it consists of statistically independent r + 1 Markov chains.  Each Markov chain has its state appearing at every r+l interval. With this property  of the model, we label time step t by the two integers s and k as follows  t=s(r+l)+k, (0< s,0<k <r) (6)  Let P+(t) -- P+(s, k) be the probability for the state to be in the 4-1 state at time  t or (s, k). Then, it can be shown that  :  =  p  p+q  - q,  p+q  ' = 1-(p+q). (7)  In the steady state, P+(s -, o, k) -- P+ = a and P_(s -, o, k) =_ P_ = . The  steady state residence time histogram can be obtained by computing the following  318 T. Ohira, Y. Sato and J. D. Cowan  quantity, h(u) _= P(+;-,u; +), which is the probability that the system takes  consecutive -1 states u times between two +1 states. With the definition of the  model and statistical independence between Markov chains in the sequence, the  following expression can be derived:  p(+;_,.; +) = p+(p_).p+ = (1 _<. < 7.) (s)  -- P+(P_)'(1 - q) = (/)'(a)(1 - q) (u = 7') (O)  -- P+(P_)(q)(1 _p)U-(p)_ (/)U(p)2 (u > 7.) (10)  With appropriate normalization, this expression reflects the shape of the histogram  obtained by numerical simulations, as shown in Figure 2. Also, by differentiating  equation (9) with respect to p, we derive the resonant condition for the peak to  reach maximum height as  q--pt (11)  or, equivalently,  L - a = (L + b)7.. (12)  In Figure 2(D), we see that maximum peak amplitude is reached by choosing pa-  rameters according to equation (11). We note that this analysis for the histogram is  exact in the stationary limit, which makes this model unique among those showing  stochastic resonance.   ....... A .........  p  Figure 2: Residence time histogram and dynamics of X(t) as we change p. The  values of p are (A) p = 0.005, (B) p = 0.05, (C) p = 0.2. The solid lie in the  histogram is from the analytical expression given in equatiOns(8-10 ). Also, in (D)  we show a plot of peak height by varying p. The solid line from equation (9).  The parameters are 7. = 10, q = 0.5.  4 Delay Coupled Two Neuron Case  We now consider a circuit comprising two such stochastic binary neurons coupled  with delayed interaction. We observe again that resonance between noise and delay  Resonance in a Stochastic Neuron Model with Delayed Interaction 319  takes place. The coupled two neuron model is a simple extension of the model in  the previous section. The transition probability of each neuron is dependent on the  other neuron's state at a fixed interval in the past. Formally, it can be described in  probability space as follows.  Pl(1, t + 1)  t+ 1)  P2(1, t + 1)  t + 1)  = Pl,  = i -- ql,  ---- ql,  -- 1 -  = P2,  -- 1 -- q2,  = q2,  = 1 -- P2,  X2(t- r2) = -l,  X2(t - r2) = 1,  X2(t - r2) = 1,  X2(t- r2)= -1,  Xl(t - rl) = -1,  xl(t- rl)= 1,  xl(t- rl) = 1,  X(t- rl) = -1  (13)  Pi(s, t) is the probability that the state of the neuron i is Xi(t) = s. We have  performed simulation experiments on the model and have again found resonance  between noise and delay. Though more intricate than the single neuron model, we  can perform a similar theoretical analysis of the histograms and have obtained ap-  proximate results for some cases. For example, we obtain the following approximate  analytical results for the peak height of the interspike histogram of X for the case  r = r2 = r. ( The peak occurs at r + r2 + 1.)  H(pl, P2, ql, q2)  P (P, P2, ql, q2)  //2 (Pl, P2, ql, q2)  Pa (Pl, P2, q, q2)  P4 (Pl, P2, ql, q2)  fl (Pl, P2, ql, q2 )  f 2 (Pl , P2 , q , q2)  S(pl, P2, ql, q2)  : {P3(Pl,P2,ql,q2)ql + P4(Pl,P2,ql,q2)(1 -pl)}  (14)  {Pl(Pl,P2, ql,q2)(qlq2Pl + q1(1 - q2)(1 - ql))  +P2(Pl,P2,ql,q2)((1 - P)q2P + (1 - pl)(1 - q2)(1 -  fl (Pl, P2 , ql , q2 ) f 2 (Pl, P2 , ql , q2 )  = (15)  s(p , P2 , q , q2)  fl (Pl, P2, ql, q2)  = S(pl,P2,ql,q2) (16)  f2 (Pl, P2, ql, q2)  = (17)  s (Pl, P2, ql, q2)  1  = s(pl,P2,ql,q2) (18)  pl(1 --p2) +P2(1 -- ql)  = (19)  ql(1 --q2)+ q2(1 --q)  P2 q- Pl (1 - P2 -- q2 )  = q2+q(1-p2-q2) (20)  = fl (Pl, P2, ql, q2) f2 (Pl, P2, q, q2)  + fl (Pl, P2, ql, q2 ) + f2 (P, P2, ql, q2 ) + i (21)  These analytical results are compared with the simulation experiments, examples  of which are shown in Figure 3. A detailed analysis, particularly for the case of  rl  r2, is quite intricate and is left for the future.  5 Discussion  There are two points to be noted. Firstly, although there are examples which may  indicate that stochastic resonance is utilized in biological information processing,  it is yet to be explored if the resonance between noise and delay has some role in  320 T. Ohira, Y. Sato andJ. D. Cowan  h('c)  o.oo  o.oo  (A) ....    ..... h() .....  0.006  0.004  0.002  0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4  B) o.o1  *C 0.000  h() o.0o  0.004  0.002  (c)  o.1 0.2 0.3 0.4 o.s o.x 0.2 0.3 0.4, o.5  Figure 3: A plot of peak height by varying pa. The solid line is from equation (14-  20). The parameters are r = r2 = 10, q = q2 = 0.5, (A)p = pa, (B) p = 0.005,  (C) p - 0.025.  neural information processing. Secondly, there are many investigations of spiking  neural models and their applications (see e.g., [8]). Our model can be considered  as a new mechanism for generating controlled stochastic spike trains. One can  predict its application to weak signal transmission analogous to recent research  using stochastic resonance with a larger number of units in series [9]. Investigations  of the network model with delayed interactions are currently underway.  References  [1] Hertz, J. A., Krogh, A., & Palmer, R. G. (1991). Introduction to the Theory of Neural  Computation. Redwood City: Addison-Wesley.  [2] Foss, J., Longtin, A., Mensour, B., & Milton, J. G. (1996). Multistability and Delayed  Recurrent Loops. Physical Review Letters, 76, 708-711; Pham, J., Pakdaman, K., Vibert,  J.-F. (1998). Noise-induced coherent oscillations in randomly connected neural networks.  Physical Review E, 58, 3610-3622; Kim, S., Park, S. H., Pyo, H.-B. (1999). Stochastic  Resonance in Coupled Oscillator Systems with Time Delay. Physical Review Letters, 8,  1620-1623; Bressloff, P. C. (1999). Synaptically Generated Wave Propagation in Excitable  Neural Media. Physical Review Letters, 8, 2979-2982.  [3] Ohira, T. & Sato, Y. (1999). Resonance with Noise and Delay. Physical Review Letters,  8, 2811-2815.  [4] Gammaitoni, L., H'-ggi, P., Jung, P., & Marchesoni, F.(1998). Stochastic Resonance.  Review of Modern Physics, 70, 223-287.  [5] Gang, H., Ditzinger, T., Ning, C. Z., & Haken, H.(1993) Stochastic Resonance without  External Periodic Force. Physical Review Letters, 71, 807-810; Rappel, W-J. & Strogatz,  S. H. (1994). Stochastic resonance in an autonomous system with a nonuniform limit cycle.  Physical Review E, 50, 3249-3250; Longtin, A. (1997). Autonomous stochastic resonance  in bursting neurons. Physical Review E, 55, 868-876.  [6] Ohira, T. & Cowan J. D. (1995). Stochastic Single Neurons, Neural Communication,  7 518-528.  [7] Ohira, T. & Milton, J. G. (1995). Delayed Random Walks. Physical Review E, 5,  3277-3280; Ohira, T. (1997). Oscillatory Correlation of Delayed Random Walks, Physical  Review E, 55, R1255-1258.  [8] Maas, W. (1997). Fast Sigmoidal Network via Spiking Neurons. Neural Computation,  9(2), 279-304; Maas, W. (1996). Lower Bounds for the Computational Power of Networks  of Spiking Neurons. Neural Computation, 8(1), 1-40.  [9] LScher, M., Cigna, D., and Hunt, E. R. (1998). Noise Sustained Propagation of a Signal  in Coupled Bistable Electric Elements Physical Review Letters, 80, 5212-5215.  
From Coexpression to Coregulation: An  Approach to Inferring Transcriptional  Regulation among Gene Classes from  Large-Scale Expression Data  Eric Mjolsness  Jet Propulsion Laboratory  California Institute of Technology  Pasadena CA 91109-8099  mjolsness @jp I. nasa. go v  Tobias Mann  Jet Propulsion Laboratory  California Institute of Technology  Pasadena CA 91109-8099  mannaig.jpl. nasa.gov  Rebecca Castafio  Jet Propulsion Laboratory  California Institute of Technology  Pasadena CA 91109-8099  beckyaig.jpl. nasa. gov  Barbara Wold  Division of Biology  California Institute of Technology  Pasadena CA 91125  woldb(its. caltech. edu  Abstract  We provide preliminary evidence that existing algorithms for  inferring small-scale gene regulation networks from gene  expression data can be adapted to large-scale gene expression data  coming from hybridization microarrays. The essential steps are (1)  clustering many genes by their expression time-course data into a  minimal set of clusters of co-expressed genes, (2) theoretically  modeling the various conditions under which the time-courses are  measured using a continious-time analog recurrent neural network  for the cluster mean time-courses, (3) fitting such a regulatory  model to the cluster mean time courses by simulated annealing  with weight decay, and (4) analysing several such fits for  commonalities in the circuit parameter sets including the  connection matrices. This procedure can be used to assess the  adequacy of existing and future gene expression time-course data  sets for determining transcriptional regulatory relationships such as  coregulation.  1 Introduction  In a cell, genes can be turned "on" or "off" to varying degrees by the protein  products of other genes. When a gene is "on" it is transcribed to produce messenger  RNA (mRNA) which can subsequently be translated into protein molecules. Some  of these proteins are transcription factors which bind to DNA at specific sites and  thereby affect which genes are transcribed and how often. This trancriptional  Inferring Transcriptional Regulation among Gene Classes 929  regulation feedback circuitry provides a fundamental mechanism for information  processing in the cell. It governs differentiation into diverse cell types and many  other basic biological processes.  Recently, several new technologies have been developed for measuring the  "expression" of genes as mRNA or protein product. Improvements in conventional  fluorescently labeled antibodies against proteins have been coupled with confocal  microscopy and image processing to partially automate the simultaneous  measurement of small numbers of proteins in large numbers of individual nuclei in  the fruit fly Drosophila melanogaster [1]. In a complementary way, the mRNA  levels of thousands of genes, each averaged over many cells, have been measured by  hybridization arrays for various species including the budding yeast Saccharomyces  cerevisiae [2].  The high-spatial-resolution protein antibody data has been quantitatively modeled  by "gene regulation network" circuit models [3] which use continuous-time, analog,  recurrent neural networks (Hopfield networks without an objective function) to  model transcriptional regulation [4][5]. This approach requires some machine  learning technique to infer the circuit parameters from the data, and a particular  variant of simulated annealing has proven effective [6][7]. Methods in current  biological use for analysing mRNA hybridization data do not infer regulatory  relationships, but rather simply cluster together genes with similar patterns of  expression across time and experimental conditions [8][9]. In this paper, we explore  the extension of the gene circuit method to the mRNA hybridization data which has  much lower spatial resolution but can currently assay a thousand times more genes  than immunofluorescent image analysis.  The essential problem with using the gene circuit method, as employed for  immunoflourescence data, on hybridization data is that the number of connection  strength parameters grows between linearly and quadratically in the number of  genes (depending on sparsity assumptions) . This requires more data on each gene,  and even if that much data is available, simulated annealing for circuit inference  does not seem to scale well with the number of unknown parameters. Some form of  dimensionality reduction is called for. Fortunately dimensionality reduction is  available in the present practice of clustering the large-scale time course expression  data by genes, into gene clusters. In this way one can derive a small number of  cluster-mean time courses for "aggregated genes", and then fit a gene regulation  circuit to these cluster mean time courses. We will discuss details of how this  analysis can be performed and then interpreted. A similar approach using somewhat  different algorithms for clustering and circuit inference has been taken by Hertz  [10].  In the following, we will first summarize the data models and algorithms used, and  then report on preliminary experiments in applying those algorithms to gene  expression data for 2467 yeast genes [9][11]. Finally we will discuss prospects for  and limitations of the approach.  2 Data Models and Algorithms  The data model is as follows. We imagine that there is a small, hidden regulatory  network of "aggregate genes" which regulate one another by the analog neural  network dynamics [3]  z'i = g T'"ivJ + h,. - ,ivi  930 E. Mjolsness, T. Mann, R. Castao and B. WoM  in which i is the continuous-valued state variable for gene product i, r/ is the  matrix of positive, zero, or negative connections by which one transcription factor  can enhance or repress another, and gO is a nonlinear monotonic sigmoidal  activation function. When a particular matrix entry r is nonzero, there is a  regulatory "connection" from gene productj to gene i. The regulation is enhancing  if T is positive and repressing if it is negative. If r is zero there is no connection.  This network is run forwards from some initial condition and time-sampled to  generate a wild-type time course for the aggregate genes. In addition, various other  time courses can be generated under alternative experimental conditions by  manipulating the parameters. For example an entire aggregate gene (corresponding  to a cluster of real genes) could be removed from the circuit or otherwise modified  to represent mutants. External input conditions could be modeled as modifications  to h. Thus we get one or several time courses (trajectories) for the aggregate genes.  From such aggregate time courses, actual gene data is generated by addition of  Gaussian-distributed noise to the logarithms of the concentration variables. Each  time point in each cluster has its own scalar standard deviation parameter (and a  mean arising from the circuit dynamics). Optionally, each gene's expression data  may also be multiplied by a time-independent proportionality constant.  Regulatory aggregate genes  (large circles) and cluster  member genes (small  circles).  Given this data generation model and suitable gene expression data, the problem is  to infer gene cluster memberships and the circuit parameters for the aggregate  genes' regulatory relationships. Then, we would like to use the inferred cluster  memberships and regulatory circuitry to make testable biological predictions.  This data model departs from biological reality in many ways that could prove to be  important, both for inference and for prediction. Except for the Gaussian noise  model, each gene in a cluster is models as fully coregulated with every other one -  they are influenced in the same ways by the same regulatory connection strengths.  Also, the nonlinear circuit model must not only reflect transcriptional regulation,  but all other regulatory circuitry affecting measured gene expression such as kinase-  phosphatase networks.  Under this data model, one could formulate a joint Bayesian inference problem for  the clustering and circuit inference aspects of fitting the data. But given the highly  provisional nature of the model, we simply apply in sequence an existing mixture-  of-Gaussians clustering algorithm to preprocess the data and reduce its  dimensionality, and then an existing gene circuit inference algorithm. Presumably a  joint optimization algorithm could be obtained by iterating these steps.  2.1 Clustering  A widely used clustering algorithm for mixure model estimation is Expectation-  Maximization (EM)[12]. We use EM with a diagonal covariance in the Gaussian, so  that for each feature vector component a (a combination of experimental condition  Inferring Transcriptional Regulation among Gene Classes 931  and time point in a time course) and cluster Ot there is a standard deviation  parameter O',, a. In preprocessing, each concentration data point is divided by its  value at time zero and then a logarithm taken. The log ratios are clustered using  EM. Optionally, each gene's entire feature vector may be normalized to unit length  and the cluster centers likewise normalized during the iterative EM algorithm.  In order to choose the number of clusters, k, we use the cross-validation algorithm  described by Smyth [13]. This involves computing the likelihood of each optimized  fit on a test set and averaging over runs and over divisions of the data into training  and test sets. Then, we can examine the likelihood as a function of k in order to  choose k. Normally one would pick k so as to maximize cross-validated likelihood.  However, in the present application we also want to reward small values of k which  lead to smaller circuits for the circuit inference phase of the algorithm. The choice  of k will be discussed in the next section.  2.2 Circuit Inference  We use the Lam-Delosme variant of simulated annealing (SA) to derive connection  strengths T, time constants x, and decay rates ), as in previous work using this gene  circuit method [4][5]. We set h to zero. The score function which SA optimizes is  S(T,r,A) = A(vi(t;T,r,A ) - Oi(t))2 + 2  it ij  + exp[B(_ T,../2 + + 1  ij i i  The first term represents the fit to data i' The second term is a standard weight  decay term. The third term forces solutions to stay within a bounded region in  weight space. We vary the weight decay coefficient W in order to encourage  relatively sparse connection matrix solutions.  3 Results  3.1 Data  We used the Saccharomyces cerevisiae data set of [9]. It includes three longer time  courses representing different ways to synchronize the normal cell cycle [11], and  five shorter time courses representing altered conditions. We used all eight time  courses for clustering, but just 8 time points of one of the longer time courses (alpha  factor synchronized cell cycle) for the circuit inference. It is likely that multiple  long time courses under altered conditions will be required before strong biological  predictions can be made from inferred regulatory circuit models.  3.2 Clustering  We found that the most likely number of classes as determined by cross validation  was about 27, but that there is a broad plateau of high-likelihood cluster numbers  from 15 to 35 (Figure 1). This is similar to our results with another gene expression  data set for the nematode worm Caenorhabditis elegans supplied by Stuart Kim;  these more extensive clustering experiments are summarized in Figure 2. Clustering  experiments with synthetic data is used to understand these results. These  experiments show that the cross-validated log likelihood curve can indicate the  number of clusters present in the data, justifying the use of the curve for that  932 E. Mjolsness, T. Mann, R. Castao and B. WoM  purpose. In more detail, synthetic data generated from 14 20-dimensional spherical  Gaussian clusters were clustered using the EM/CV algorithm. The likelihoods  showed a sharp peak at k=14 unlike Figures 1 or 2. In another experiment, 14 20-  dimensional spherical Gaussian superclusters were used to generate second-level  clusters (3 subclusters per supercluster), which in turn generated synthetic data  points. This two-level hierarchical model was then clustered with the EM/CV  method. The likelihood curves (not shown) were quite similar to Figures 1 and 2,  with a higher-likelihood plateau from roughly 14 to 40.  x o 4  Figure 1. Cross-validated log-likelihood scores, displayed and averaged over 5 runs, for EM  clustering of S. cerevisiae gene expression data [9]. Horizontal axis: k, the "requested" or  maximal number of cluster centers in the fit. Some cluster centers go unmatched to data.  Vertical axis: log likelihood score for the fit, scatterplotted and averaged. Likelihoods have  not been integrated over any range of parameters for hypothesis testing. k ranges from 2 to  40 in increments of 1. Solid line shows average likelihood value for each k.  Number of Clusters  '!' .i. '1' '1' .i..i..i..i.q.  . '1' .i. '1' '1' .i.q..i '  ++ +++x,,,++++  4-  ++,+  Number of Clusters  From Figures 1 and 2 and the synthetic data experiments mentioned above, we can  guess at appropriate values for k which take into account both the measured  likelihood of clustering and the requirements for few parameters in circuit-fitting.  For example choosing k=-15 clusters would put us at the beginning of the plateau,  losing very little cluster likelihood in return for reducing the aggregate genes circuit  size from 27 to 15 players. The interpretation would be that there are about 15  superclusters in hierarchically clustered data, to which we will fit a 15-player  Figure 2. Cross-validated log-likelihood scores, averaged over 13 runs, for EM clustering of  C. elegans gene expression data from S. Kim's lab. Horizontal axis: k, the "requested" or  maximal number of cluster centers in the fit. Some cluster centers go unmatched to data.  Vertical axis: log likelihood score for the fit, as an average over 13 runs plus or minus one  standard deviation. (Left) Fine-scale plot, k =2 to 60 in increments of 2. (Right) Coarse-  scale plot, k=2 to 202 in increments of 10. Both plots show an extended plateau of relatively  likely fits between roughly k =14 and k =40.  Inferring Transcriptional Regulation among Gene Classes 933  regulatory circuit. Much more aggressive would be to pick k=-7 or 8 clusters, for a  relatively significant drop in log-likelihood in return for a further substantial  decrease in circuit size. An acceptable range of cluster numbers (and circuit sizes)  would seem to be k=-8 to 15.  3.3 Gene Circuit Inference  It proved possible to fit the k=15 time course using weight decay W=I but without  using hidden units. W=0 and W=3 gave less satisfactory results. Four of the 15  clusters are shown in Figure 3 for one good run (W=I). Scores for our first few  (unselected) runs at the current parameter settings are shown in Table 1. Each run  took between 24 and 48 hours on one processor of an Sun Ultrasparc 60 computer.  Even with weight decay, it is possible that successful fits are really overfits with  this particular data since there are about twice as many parameters as data points.  Weight  Decay W  <Score>  <Simulated Annealing Number of runs  Moves>/10/'6  0 1.536 +/- 0.134  1 0.787 +/- 0.394  3 1.438 +/- 0.037  2.803 +/- 0.437 3  2.782 +/- 0.200 10  2.880 +/- 0.090 4  Table 1. Score function parameters were A=I.0. B---0.01. Annealing runs statistics are  reported when the temperature dropped below 0.0001. All the best scores and visually  acceptable fits occurred in W=I runs.  The average values of the data fit, weight decay, and penalty terms in the score  function for W=I were {0.378, 0.332, 0.0667} after slightly more annealing.  There were a few significant similarities between the connection matrices computed  in the two lowest-scoring runs. The most salient feature in the lowest-scoring  network was a set of direct feedback loops among its strongest connections: cluster  8 both excited and was inhibited by cluster 10, and cluster 10 excited and was  inhibited by cluster 15. This feature was preserved in the second-best run. A  systematic search for "concensus circuitry" shows convergence towards a unique  connection matrix for the 8-point time series data used here, but more complete 16-  time-point data gives multiple "clusters" of connection matrices. From parameter-  counting one might expect that making robust and unique regulatory predictions will  require the use of more trajectory data taken under substantially different  conditions. Such data is expected to be forthcoming.  4 Discussion  We have illustrated a procedure for deriving regulatory models from large-scale  gene expression data. As the data becomes more comprehensive in the number and  nature of conditions under which comparable time courses are measured, this  procedure can be used to determine when biological hypotheses about gene  regulation can be robustly derived from the data.  Acknowledgments  This work was supported in part by the Whittier Foundation, the Office of Naval  Research under contract N00014-97-1-0422, and the NASA Advanced Concepts  Program. Stuart Kim (Stanford University) provided the C. elegans gene expression  array data. The GRN simulation and inference code is due in part to Charles Garrett  and George Marnellos. The EM clustering code is due in part to Roberto Manduchi.  934 E. Mjolsness, T. Mann, R. Castao and B. Wold  0'1 2 3 4 5 6 7  1 2 3 4 5 6 7 8   C  --  0 "  0 1 2 3 4 5 6 7 8  2 3 4 5 6 7 8  onglnaJ dala marked wh an x, grn fd marked wfih an o  Figure 3. Four clusters (numbers 9-12) of a 15-cluster mixture of Gausstans model of 2467  genes each assayed over an eight-point time course; cluster means (shown as x) are fit to a  gene regulation network model (shown as o).  References  [1] D. Kosman, J. Reinitz, and D. H. Sharp, "Automated Assay of Gene Expression at  Cellular Resolution" Pacific Symposium on Biocomputing '98. Eds. R. Altman, A. K.  Dunker, L. Hunter, and T. E. Klein,, World Scientific 1998.  [2] J. L. DeRisi, V. R. lyer, and P.O. Brown, "Exploring the Metabolic and Genetic Control  of Gene Expreession on a Genomic Scale". Science 278, 680-686.  [3] A Connectionist Model of Development, E. Mjolsness, D. H. Sharp, and J. Reinitz,  Journal of Theoretical Biology 152:429-453, 1991.  [4] J. Reinitz, E. Mjolsness, and D. H. Sharp, "Model for Cooperative Control of Positional  Information in Drosophila by Bicoid and Maternal Hunchback". J. Experimental Zoology  271:47-56, 1995. Los Alamos National Laboratory Technical Report LAUR-92-2942 1992.  [5] J. Reinitz and D. H. Sharp, "Mechanism of eve Stripe Formation". Mechanisms of  Development 49:133-158, 1995.  [6] [7] J. Lam and J. M. Delosme. "An Efficient Simulated Annealing Schedule: Derivation"  and "... Implementation and Evaluation". Technical Reports 8816 and 8817, Yale University  Electrical Engineering Department, New Haven CT 1988.  [8] X. Wen, S. Fuhrman, G. S. Michaels, D. B. Carr, S. Smith, J. L. Barker, and R. Somogyi,  "Large-Scale Temporal Gene Expression Mapping of Central Nervous System  Development", Proc. Natl. Acal. Sci. USA 95:334-339, January 1998.  [9] M. B. Eisen, P. T. Spellman, P.O. Brown, and D. Botstein, "Cluster Analysis and  Display of Genome-Wide Expression Patterns", Proc. Natl. Acad. Scie. USA 95:14863-  14868, December 1998.  [10] J. Hertz, lecture at Krogerrup Denmark computational biology summer school, July  1998.  [11] Spellman PT, Sherlock G, Zhang MQ, et al., "Comprehensive identification of cell  cycle-regulated genes of the yeast Saccharomyces cerevisiae by microarray hybridization",  Mol. Bio. Cell. 9: (12) 3273-3297 Dec 1998.  [12] Dempster, A. P., Laird, N.M. and Rubin, D. B. "Maximum likelihood from incomplete  data via the EM algorithm," J. Royal Statistical Society, Series B, 39:1-38, 1977.  [13] P. Smyth, "Clustering using Monte Carlo Cross-Validation", Proceedings of the 2 nd  International Conference on Knowledge Discovery and Data Mining, AAAI Press, 1996.  
Can V1 mechanisms account for  figure-ground and medial axis effects?  Zhaoping Li  Gatsby Computational Neuroscience Unit  University College London  zhaopinggat shy. ucl. ac. uk  Abstract  When a visual image consists of a figure against a background, V1  cells are physiologically observed to give higher responses to image  regions corresponding to the figure relative to their responses to  the background. The medial axis of the figure also induces rela-  tively higher responses compared to responses to other locations  in the figure (except for the boundary between the figure and the  background). Since the receptive fields of V1 cells are very smal-  l compared with the global scale of the figure-ground and medial  axis effects, it has been suggested that these effects may be caused  by feedback from higher visual areas. I show how these effects can  be accounted for by V1 mechanisms when the size of the figure  is small or is of a certain scale. They are a manifestation of the  processes of pre-attentive segmentation which detect and highlight  the boundaries between homogeneous image regions.  I Introduction  Segmenting figure from ground is one of the most important visual tasks. We nei-  ther know how to execute it on a computer in general, nor do we know how the  brain executes it. Further, the medial axis of a figure has been Suggested as provid-  ing a convenient skeleton representation of its shape (Blum 1973). It is therefore  exciting to find that responses of cells in V1, which is usually considered a low level  visual area, differentiate between figure and ground (Lamme 1995, Lamme, Zipser,  and Spekreijse 1997, Zipser, Lamme, Schiller 1996) and highlight the medial axis  (Lee, Mumford, Romero, and Lainme 1998). This happens even though the recep-  tive fields in V1 are much smaller than the scale of these global and perceptually  significant phenomena. A common assumption is that feedback from higher visual  areas is mainly responsible for these effects. This is supported by the finding that  the figure-ground effects in V1 can be strongly reduced or abolished by anaesthesia  or lesions in higher visual areas (Lamme et al 1997).  However, in a related experiment (Gallant, van Essen, and Nothdurt 1995), Vl  cells were found to give higher responses to global boundaries between two texture  regions. Further, this border effect was significant only 10-15 milliseconds after the  initial responses of the cells and was present even under anaesthesia. It is thus  Can V1 Mechanisms Account for Figure-Ground and Medial Axis Effects? 137  plausible that Vl mechanisms is mainly responsible for the border effect.  In this paper, I propose that the figure-ground and medial axis effects are manifes-  tations of the border effect, at least for apropriately sized figures. The border effect  is significant within a limited and finite distance from the figure border. Let us  call the image region within this finite distance from the border the effective border  region. When the size of the figure is small enough, all parts of the figure belong  to the effective border region and can induce higher responses. This suggests that  the figure-ground effect will be reduced or diminished as the size of the figure be-  comes larger, and the V1 responses to regions of the figure far away from the border  will not be significantly higher than responses to background. This suggestion is  supported by experimental findings (Lamme et al 1997). Furthermore, the border  effect can create secondary ripples as the effect decays with distance from the bor-  der. Let us call the distance from the border to the ripple the ripple wavelength.  When the size of a figure is roughly twice the ripple wavelength, the ripples from  the two opposite borders of the figure can reinforce each other at the center of the  figure to create the medial axis effect, which, indeed, is observed to occur only for  figures of appropriate sizes (Lee et al 1998).  I validate this proposal using a biologically based model of V1 with intra-cortical  interactions between cells with nearby but not necessarily overlapping receptive  fields. Intra-cortical interactions cause the responses of a cell be modulated by  nearby stimuli outside its classical receptive fields -- the contextual influences that  are observed physiologically (Knierim and van Essen 1992, Kapadia et al 1995).  Contextual influences make V1 cells sensitive to global image features, despite their  local receptive fields, as manifested in the border and other effects.  2 The V1 model  We have previously constructed a V1 model and shown it to be able to highlight  smooth contours against a noisy background (Li 1998, 1999, 1999b) and also the  boundaries between texture regions in images -- the border effect. Its behavior  agrees with physiological observations (Knierim and van Essen 1992, Kapadia et  al 1995) that the neural response to a bar is suppressed strongly by contextual  bars of similar orientatons -- iso-orientation suppression; that the response is less  suppressed by orthogonally or randomly oriented contextual bars; and that it is  enhanced by contextual bars that are aligned to form a smooth contour in which  the bar is within the receptive field -- contour enhancement. Without loss of  generality, the model ignores color, motion, and stereo dimensions, includes mainly  layer 2-3 orientation selective cells, and ignores the intra-hypercolumnar mechanism  by which their receptive fields are formed. Inputs to the model are images filtered  by the edge- or bar-like local receptive fields (RFs) of V1 cells.  Cells influence each  other contextually via horizontal intra-cortical connections (Rockland and Lund  1983, Gilbert, 1992), transforming patterns of inputs to patterns of cell responses.  Fig. I shows the elements of the model and their interactions. At each location  i there is a model V1 hypercolumn composed of K neuron pairs. Each pair (i,  has RF center i and preferred orientation  - k'/K for k -- 1,2, ...K, and is  called (the neural representation of) an edge segment. Based on experimental data  (White, 1989), each edge segment consists of an excitatory and an inhibitory neuron  that are interconnected, and each model cell represents a collection of local cells of  similar types. The excitatory cell receives the visual input; its output is used as  a measure of the response or salience of the edge segment and projects to higher  visual areas. The inhibitory cells are treated as interneurons. Based on observations  The terms 'edge' and 'bar' will be used interchangeably.  138 Z. Li  A Visual space, edge detectors,  and their interactions  B Neural connection pattern.  Solid: J, Dashed: W  A samplin One of the edge  location detectors  C Model Neural Elements  Edge outputs to higher visual areas  Inputs Ic to  inhibitory cells  A interconnected  i-,-,L- neuron pair for  edge segment i 0   I ::"'x. Inhibitory  lntemeurons  Excitatory  neurons  Visual inputs, filtered through the  receptive fields, to the excitatory cells.  Figure 1: A: Visual inputs are sampled in a discrete grid of edge/bar detectors.  Each grid point i has K neuron pairs (see C), one per bar segment, tuned to  different orientations 0 spanning 180 . Two segments at different grid points can  interact with each other via monosynaptic excitation J (the solid arrow from one  thick bar to anothe r) or disynaptic inhibition W (the dashed arrow to a thick  dashed bar). See also C. B: A schematic of the neural connection pattern from the  center (thick solid) bar to neighboring bars within a few sampling unit distances.  J's contacts are shown by thin solid bars. W's are shown by thin dashed bars. The  connection pattern is translation and rotation invariant. C: An input bar segment  is directly processed by an interconnected pair of excitatory and inhibitory cells,  each cell models abstractly a local group of cells of the same type. The excitatory  cell receives visual input and sends output gx (xio) to higher centers. The inhibitory  cell is an interneuron. Visual space is taken as having periodic boundary conditions.  by Gilbert, Lund and their colleagues (Rockland and Lund, 1983, Gilbert 1992)  horizontal connections Jio,jo, (respectively Wio,jo,) mediate contextual influences  via monosynaptic excitation (respectively disynaptic inhibition) from jO  to iO which  have nearby but different RF centers, i  j, and similar orientation preferences,  0 0 0 . The membrane potentials follow the equations:  -ctxxio - E b(AO)gy(Yi,o+,o) + Jog(xio) +  AO  OiO = --OyYiO -]'- gx(gCiO) -]'- E Wi,Jtgx(gcJ') + fc  Jio,jo,g(xjo,) + ho + Zo  jvi,o,  Can V1 Mechanisms Account for Figure-Ground and Medial Axis Effects? 139  where axxe and ayyie model the decay to resting potential, gx(x) and gy(y) are  sigmoid-like functions modeling cells' firing rates in response to membrane potentials  x and y, respectively, b(A) is the spread of inhibition within a hypercolumn,  Zog:r(xi) is self excitation, Ic and Io are background inputs, including noise and  inputs modeling the general and local normalization of activities (see Li (1998) for  more details). Visual input Iis persists after onset, and initializes the activity levels  gz (xis). The activities are then modified by the contextual influences. Depending on  the visual input, the system often settles into an oscillatory state (Gray and Singer,  1989, see the details in Li 1998). Temporal averages of gz (xis) over several oscillation  cycles are used as the model's output. The nature of the computation performed by  the model is determined largely by the horizontal connections J and W, which are  local (spanning only a few hypercolumns), and translation and rotation invariant  (Fig. XB).  A: Input image (]io) to model  B: Model output  ll  ll  '--ll  ll  ll  ll  ll  ll  ll  ............. ll  ll  Figure 2: An example of the performance of the model. A: Input ]i consists of two  regions; each visible bar has the same input strength. B: Model output for A, showing  non-uniform output strengths (temporal averages of g(xi)) for the edges. The input and  output strengths are proportional to the bar widths. Because of the noise in the system,  the saliencies of the bars in the same column are not exactly the same, this is also the case  in other figures.  The model was applied to some texture border and figure-ground stimuli, as shown  in examples in the figures. The input values ie are the same for all visible bars in  each example. The differences in the outputs are caused by intracortical interac-  tions. They become significant about one membrane time constant after the initial  neural response (Li, 1998). The widths of the bars in the figures are proportional  to input and output strengths. The plotted region in each picture is often a small  region of an extended image. The same model parameters (e.g. the dependence  of th synaptic weights on distances and orientations, the thresholds and gains in  the functions g() and gy(), and the level of input noise in Io) are used for all the  simulation examples.  Fig. 2 demonstrates that the model indeed gives higher responses to the boundaries  between texture regions. This border effect is highly significant within a distance of  about 2 texture element spacings from the border. Thus the effective border region  is about 2 in texture element spacings in this example. Furthermore, at about 9  texture element spacings to the right of the texture border there is a much smaller  but significant (visible on the figure) secondary peak in the response amplitude.  Thus the ripple wavelength is about 9 texture element spacings here. The border  effect is mainly caused by the fact that the texture elements at the border expe-  rience less iso-orientation suppression (which reduces the response levels to other  texture bars in the middle of a homogeneous (texture) region) -- the texture el-  ements at the border have fewer neighboring texture bars of a similar orientation  than the texture elements in the centers of the regions. The stronger responses to  the effective border region cause extra iso-orientation suppression to texture bars  near but right outside the effective border region. Let us call this region of stronger  140 Z. Li  Model Input  ----||1|||||||||||||1||||||||||11----  ----III IIII I II IIIII IIII I II IIII III----  ----III IIIII II IIII I III IIIIIIIII II----  ----IIIIIIIIIIIIIIIIIIIIIIIIIIIII----  ----tllllllllllllllllllllllllllll----  ----IIIIIIIIIIIIIIIIIIIIIIIIIIIII----  Model Output  ----11 IIII IIII II III II IIIIII I I Ilg----  --:111flllllllllllllllllllllllll:--  ----III IIII IIII II I IIII IIII II I IIII----  ----IIII III IIII II I I III I I II II IIIII----  ----III IIII IIII II I IIII I III II I IIII----  ----III IIII IIII II I IIII I III II I IIII----  --:11111111111111111111111111111:--  --IIII III IIII III II III III III IIII----  ----IIIIIIIIIIIIIIIIIIIIIIIIIIIII----  ----III IIII IIII II I IIII I IIIII I IIII----  ----IIII I II IIII II I II II IIIIII I III1----  :11  ......  :11  II  ...... ,11  ........  I I  II  II  ii  ii  II  II  ii  II  II  ii  ii  II  II  III,  III,  I  I II,  III,  III'  III,  III,  III,  I  III,  III,  III,  mm mm,  II II'  II II'  II II,  .. I If  Ill--  Ill  mmmmmm--mmm mmm m  m-  mmm-- ...........  gm g  III  mill  mmm--,  mmm  mmmm-  III  -III  Figure 3: Dependence on the size of the figure. The figure-ground effect is most  evident only for small figures, and the medial axis effect is most evident only for  figures of finite and appropriate sizes.  suppression from the border the border suppression region, which is significant and  visible in Fig. (2B). This region can reach no further than the longest length of  the horizontal connenctions (mediating the suppresion) from the effective border  region. Consequently, texture bars right outside the border suppression region not  only escape the stronger suppression from the border, but also experience weaker  iso-orientation suppression from the weakened texture bars in the nearby border  suppression region. As a result, a second saliency peak appears -- the ripple effect,  and we can hence conclude that the ripple wavelength is of the same order of mag-  nitude as the longest connection length of the cortical lateral connections mediating  intra-cortical interactions.  Fig. 3 shows that for very small figures, the whole figure belongs to the effective  border region and is highlighted in the V1 responses. As the figure size increases, the  responses in the inside of the figure become smaller than the responses in the border  region. However, when the size of the figure is appropriate, namely about twice the  ripple wavelength, the center of the figure induces a secondary response highlight. In  this case, the ripples or the secondary saliency peaks from both borders superpose  onto each other at the same spatial location at the center of the figure. This  reinforces the saliency peak at this medial axis since it has two border suppression  regions (from two opposite borders), one on each side of it, as its contextual stimuli.  For even larger figures, the medial axis effect diminishes because the ripples from  Can V1 Mechanisms Account for Figure-Ground and Medial Axis Effects? 141  Model Input  ii  II  ii  II  ii  II  ii  II  ii  II  II  II  Ii  II  ii  Ii  ii  II  II  II  II  ii  II  II  ii  II  II  II  II  II  II  II II  II  II II  ii Ii  II II  II II  ii ii  Iiiii  Iiiii  IIIIII  IIIII  iiiii  IIIII  IIiii  IIIII  IIIII  I IIii  I IIII  I Iiii  i Iiii  I IIII  I IIII  i iiii  IIII  Iiii  IIii  IIII  IIII  IIII  IIii  Iiii  IIii  IIii  IIII  Iiii  IIII  IIII  IIII  II IIII  II IIII  II IIII  II IIIi  ii IIII  II IIII  II IIII  II IIII  Ii IIIi  Model Output  IIIi  IIIi  IIII  IIII  IIII  IIII  IIII  IIII  IIII  IIII  IIII  IIII  IIII  IIII  IIII  IIII  IIII  IIII  IIII  IIII  II  II  II  II  II  II  II  II  II  II  II I  II I  IlmnllHIIlllllmnl  II. '1  II I  II I  I I 4-I  Ilnllllllllllllnnl  Ill Illllll Illl  Ill Illllll Illl  III IIIIIII 1111  I II Illllll Jill  IIII IIIIIII 1111  III IIIIIII IIII  IIII IIIIIII IIII  I II IIIIIII IIII  III IIIIIII IIII  IIII  IIII  IIII  IIII  IIII  IIII  Illl  IIII  IIII  IIII  IIII  IIII  IIII  IIII  IIII  IIII  flit  IIII  IIII  IIII  IIII  IIII  IIII  IIII  IIII  IIII  IIII  IIII  IIII  II  II  II  II  II  II  Ii  II  II  ii  II  II  II  II  II  II  II  II  II  ii  II  II  ii  II  II  II  Ill I I IIIII  1111 IIIIII  Illlllllll  Illlllllll  IIIIIIIIII  II  II IIIIIII  II IIIIIII  II IIIIIII  II Illllll  ..... m IIIIIII  $! IIIllll  ill  ill  ill  ill .......  Ill .......  Ill ........  Ill .......  II  ,.l: .......  Ill ........  I l  ........  Ill .......  I I  .......  --IIll  iiiiii  iiiii I iii I i i i  i i I I  IIIIIIIi I  iiiiiiii i  iiiiiiiiii   ...... !111111111  iii --  I I -  I I Iii I*  i I I I ! I  I  I I III '  Illill  IllIll.  II  II  ii  II  II  II  II  II  II  II  II  II  II  II  II  Ii  II  II  II  Ii  II  II  II  II  II  ii  II  II  II  II  II  II  i I  II  II  II  II  II  II  1 II II  III II  ,,.. ., Ill l  I III II  II II  ,. ..  !11.. .. ,,,  Figure 4: Dependence on the shape and texture feature of the figures.  the two opposite borders of the figure no longer reinforce each other.  Fig. 4 demonstrates that the border effect and its consequences for the medial axis  also depend on the shape of the figures and the nature of the texture they contain  (eg the orientations of the elements). Bars in the texture parallel to the border  induce stronger highlights, and as a consequence, cause stronger ripple effects and  medial axis highlights. This comes from the stronger co-linear, contour enhancing,  inputs these bars receive than bars not parallel to the border.  142 Z. Li  3 Summary and Discussion  The model of V1 was originally proposed to account for pre-attentive contour en-  hancement and visual segmentation (Li 1998, 1999, 1999b). The contextual influ-  ences mediated by intracortical interactions enable each V1 neuron to process inputs  from a local image area substantially larger than its classical receptive field. This  enables cortical neurons to detect image locations where translation invariance in  the input image breaks down, and highlight these image locations with higher neu-  ral activities, making them conspicuous. These highlights mark candidate locations  for image region (or object surface) boundaries, smooth contours and small figures  against backgrounds, serving the purpose of pre-attentive segmentation.  This paper has shown that the figure-ground and medial axis effects observed in the  recent experiments can be accounted for using a purely V1 mechanism for border  highlighting, provided that the sizes of the figures are small enough or of finite and  appropriate scale. This has been the case in the existing experiments. We therefore  suggest that feedbacks from higher visual areas are not necessary to explain the  experimental observations, although we cannot, of course, exclude the possibilities  that they also contribute.  References  [1] Lamme V.A. (1995) Journal of Neuroscience 15(2), 1605-15.  [2] Lee T.S, Mumford D, Romero R. and Lamme V. A.F. (1998) Vis. Res. 38:  2429-2454.  [3] Zipset K., Lamme V. A., and Schiller P. H. (1996) J. Neurosci. 16 (22), 7376-  89.  [4] Lamme V. A. F., Zipset K. and Spekreijse H. Soc. Neuroscience Abstract 603.1,  1997.  [5] Blum H. (1973) Biological shape and visual science J. Theor. Biol. 38: 205-87.  [6] Gallant J.L., van Essen D.C., and Nothdurft H.C. (1995) In Early vision and  beyond eds. T. Papathomas, Chubb C, Gorea A., and Kowler E. (MIT press),  pp 89-98.  [7] C. D. Gilbert (1992) Neuron. 9(1), 1-13.  [8] C. M. Gray and W. Singer (1989) Proc. Natl. Acad. Sci. USA 86, 1698-1702.  [9] M. K. Kapadia, M. Ito, C. D. Gilbert, and G. Westheimer (1995) Neuron.  15(4), 843-56.  [10] J. J. Knierim and D.C. van Essen (1992) J. Neurophysiol. 67, 961-980.  [11] Z. Li (1998) Neural Computation 10(4) p 903-940.  [12] Z. Li (1999) Network: computations in neural systems 10(2). p. 187-212.  [13] Z. Li (1999b) Spatial Vision 13(1) p. 25-50.  [14] K.S. Rockland and J. S. Lund (1983) J. Comp. Neurol. 216, 303-318  [15] E. L. White (1989) Cortical circuits (Birkhauser).  
Neural Network Based Model Predictive  Control  Stephen Pich  Pavilion Technologies  Austin, TX 78758  spiche$pav. com  Jim Keeler  Pavilion Technologies  Austin, TX 78758  jkeeler$pav. com  Greg Martin  Pavilion Technologies  Austin, TX 78758  gmartinpav. com  Gene Boe  Pavilion Technologies  Austin, TX 78758  gboe$pav.com  Doug Johnson  Pavilion Technologies  Austin, TX 78758  djohnson$pav.com  Mark Gerules  Pavilion Technologies  Austin, TX 78758  mgerules$pav. com  Abstract  Model Predictive Control (MPC), a control algorithm which uses  an optimizer to solve for the optimal control moves over a future  time horizon based upon a model of the process, has become a stan-  dard control technique in the process industries over the past two  decades. In most industrial applications, a linear dynamic model  developed using empirical data is used even though the process it-  self is often nonlinear. Linear models have been used because of the  difficulty in developing a generic nonlinear model from empirical  data and the computational expense often involved in using non-  linear models. In this paper, we present a generic neural network  based technique for developing nonlinear dynamic models from em-  pirical data and show that these models can be efficiently used in  a model predictive control framework. This nonlinear MPC based  approach has been successfully implemented in a number of indus-  trial applications in the refining, petrochemical, paper and food  industries. Performance of the controller on a nonlinear industrial  process, a polyethylene reactor, is presented.  I Introduction  Model predictive control has become the standard technique for supervisory control  in the process industries with over 2,000 applications in the refining, petrochemicals,  chemicals, pulp and paper, and food processing industries [1]. Model Predictive  Control was developed in the late 70's and came into wide-spread use, particularly  in the refining industry, in the 80's. The economic benefit of this approach to control  has been documented [1,2].  1030 S. Pich, J. Keeler, G. Martin, G. Boe, D. Johnson andM. Gentles  Several factors have contributed to the wide-spread use of MPC in the process  industries:  o  Multivariate Control:  input multiple-output  mented using MPC.  Industrial processes are typically coupled multiple-  (MIMO) systems. MIMO control can be imple-  Constraints: Constraints on the inputs and outputs of a process due to  safety considerations are common in the process industries. These con-  straints can be integrated into the control calculation using MPC.  Sampling Period: Unlike systems in other industries such as automotive or  aerospace, the open-loop settling times for many processes is on the order  of hours rather than milliseconds. This slow settling time translates to  sampling periods on the order of minutes. Because the sampling period is  sufficiently long, the complex optimization calculations that are required to  implement MPC can be solved at each sampling period.  Commercial Tools: Commercial tools that facilitate model development and  controller implementation have allowed proliferation of MPC in the process  industries.  Until recently, industrial applications of MPC have relied upon linear dynamic  models even though most processes are nonlinear. MPC based upon linear models  is acceptable when the process operates at a single setpoint and the primary use of  the controller is the rejection of disturbances. However, many chemical processes,  including polymer reactors, do not operate at a single setpoint. These processes  are often required to operate at different setpoints depending upon the grade of the  product that is to be produced. Because these processes operate over the nonlinear  range of the system, linear MPC often results in poor performance. To properly  control these processes, a nonlinear model is needed in the MPC algorithm.  This need for nonlinear models in MPC is well recognized. A number of researchers  and commercial companies have developed both simulation and industrial appli-  cations using a variety of different technologies including both first principles and  empirical approaches such as neural networks [3,4]. Although a variety of different  models have been developed, they have not been practical for wide scale industrial  application. On one hand, nonlinear models built using first principle techniques  are expensive to develop and are specific to a process. Conversely, many empirically  based nonlinear models are not appropriate for wide scale use because they require  costly plant tests in multiple operating regions or because they are too computa-  tionally expensive to use in a real-time environment.  This paper presents a nonlinear model that has been developed for wide scale indus-  trial use. It is an empirical model based upon a neural network which is developed  using plant test data from a single operating region and historical data from all  regions. This is in contrast to the usual approach of using plant test data from  multiple regions. This model has been used on over 50 industrial applications and  was recognized in a recent survey paper on nonlinear MPC as the most widely used  nonlinear MPC controller in the process industries[i].  Neural Network Based Model Predictive Control 1031  After providing a brief overview of model predictive control in the next section,  we present details on the formulation of the nonlinear model. After describing the  model, an industrial application is presented that validates the usefulness of the  nonlinear model in an MPC algorithm.  2 Model Predictive Control  Model predictive control is based upon solving an optimization problem for the  control actions at each sampling interval. Using MPC, an optimizer computes  future control actions that minimize the difference between a model of the process  and desired performance over a time horizon (typically the time horizon is greater  than the open-loop settling time of the process). For example, given a linear model  of process,  Yt = -alyt- - a2yt-2 -t- blUt- + b2ut-2  (1)  where u(t) represents the input to the process, the optimizer may be used to mini-  mize an objective function at time t,  T  j = E((yt.]_ i _ t_]_i)2 _ (-t-]-i - -t-l-i-i) 2)  i=l  (2)  where )t is the desired setpoint for the output and T is the length of the time  horizon. In addition to minimizing an objective function, the optimizer is used to  observe a set of constraints. For example, it is common to place upper and lower  bounds on the inputs as well as bounds on the rate of change of the input,  Uupper  ut-]-i  Flower V I < i < T (3)  AUupper _ ttt--i - ttt--i-1 _ AUlower  I < i < T (4)  where Uupper and Flower are the upper and lower input bounds while AUupper and  AUlower are the upper and lower rate of change bounds. After the trajectory of  future control actions is computed, only the first value in the trajectory is sent as a  setpoint to the actuators. The optimization calculation is re-run at each sampling  interval using a model which has been updated using feedback.  The form of the model, the objective function, the constraints and the type of  optimizer have been active areas of research over the past two decades. A number  of excellent survey papers on MPC cover these topics [1,2,4]. As discussed above,  we have selected a MIMO nonlinear model which is presented in the next section.  Although the objective function given above contains two terms (desired output  and input move suppression), the objective function used in our implementation  contains thirteen separate terms. (The details of the objective function are beyond  the scope of this paper.) Our implementation uses the constraints given above in  (3) and (4). Because we use nonlinear models, a nonlinear programming technique  must be used to solve the optimization problem. We use LS-GRG which is a reduced  gradient solver [5].  1032 S. Pichd, J. Keeler, G. Martin, G. Boe, D. Johnson and M. Gerules  3 A Generic and Parsimonious Nonlinear Model  For a nonlinear model to achieve wide-spread industrial use, the model must be  parsimonious so that it can be efficiently used in an optimization problem. Fur-  thermore, it must be developed from limited process data. As discussed below, the  nonlinear model we use is composed of a combination of a nonlinear steady state  model and a linear dynamic model which can be derived from available data. The  method of combining the models results in a parsimonious nonlinear model.  3.1 Process data and component models  The quantity and quality of available data ultimately determines the structure of  an empirical model. In developing our models, the available data dictated the type  of model that could be created. In the process industries, two types of data are  available:  1. Historical data: The values of the inputs and outputs of most processes  are saved at regular intervals to a data base. Furthermore, most process-  ing companies retain historical data associated with their plant for several  years.  2. Plant tests: Open-loop testing is a well accepted practice for determining  the process dynamics for implementation of MPC. However, open-loop test-  ing in multiple operating regions is not well accepted and is impractical in  most cases even if it were accepted.  Most practitioners of MPC models have used plant test data and ignored historical  data. Practitioners have ignored the historical data in the past because it was  difficult to extract and preprocess the data, and build models. Historical data  was also viewed as not useful because it was collected in closed-loop and therefore  process dynamics could not be extracted in many cases. Using only the plant test  data, the practitioner is limited to linear dynamic models.  We chose to use the historical data because it can be used to create nonlinear  steady state models of processes that operate at multiple setpoints. Combining the  nonlinear steady state model with linear dynamic models from the plant test data  provides a generic approach to developing nonlinear models.  To easily facilitate the development of nonlinear models, a suite of tools has been  developed for data extraction and preprocessing as well as model training. The  nonlinear steady state models,  yss - NNss(u) (5)  are implemented by a feedforward neural network and trained using variants of  the backpropagation algorithm [6]. The developer has a great deal of flexibility in  determining the architecture of the network including the ability to select which  inputs affect which outputs. Finally, an algorithm for specifying bounds on the  gain (Jacobian) of the model has recently been implemented [7].  Because of limited plant test data, the dynamic models are restricted to second  order models with input time delay,  Yt = --alyt-1 -- a2yt-2 -t- bl ttt-d-1 -]- b2ttt_d_2 (6)  Neural Network Based Model Predictive Control 1033  The parameters of (6) are identified by minimizing the squared error between the  model and the plant test data. To prevent a biased estimate of the parameters,  the identification problem is solved using an optimizer because of the correlation in  the model inputs [8]. Tools for selecting the identification regions and viewing the  results are provided.  3.2 Combining the nonlinear steady state and dynamic models  A variety of techniques exist for combining nonlinear steady state and linear dy-  namic models. The dynamic models can be used to either preprocess the inputs  or postprocess the outputs of the steady state model. These models, referred to as  Hammerstein and Weiner models respectively [8], contain a large number of parame-  ters and are computationally expensive in an optimization problem when the model  has many inputs and outputs. These models, when based upon neural networks,  also extrapolate poorly.  Gain scheduling is often used to combine nonlinear steady state models and linear  dynamic models. Using a neural network steady state model, the gain at the current  operating point, ui,  Oyss lu=u, (7)  gi --' 0---  is used to update the gain of the linear dynamic model of (6),  6yt = --al6yt-1 -- a26Yt-2 + vl6Ut_d-i + v26Ut-d-2  (8)  where  l+al +a2  Vl --' blgi bl + b2 (9)  l+al -I-a2  v - bgi (10)  b + b.  The difference equation is linearized about the point ui and Yi = NN(ui), thus,  5y - y - yi and 5u - u - ui. To simplify the equations above, a single-input single-  output (SISO) system is used. Gain scheduling results in a parsimonious model that  is efficient to use in the MPC optimization problem, however, because this model  does not incorporate information about the gain over the entire trajectory, its use  leads to suboptimal performance in the MPC algorithm.  Our nonlinear model approach remedies this problem. By solving a steady state  optimization problem whenever a setpoint change is made, it is possible to compute  the final steady state values of the inputs, uf. Given the final steady state input  values, the gain associated with the final steady state can be computed. For a SISO  system, this gain is given by  Oys i,=,, . (11)  gf = OU  Using the initial and final gain associated with a setpoint change, the gain structure  over the entire trajectory can be approximated. This two point gain scheduling  overcomes the limitations of regular gain scheduling in MPC algorithms.  1034 S. Pichd, J. Keeler, G. Martin, G. Boe, D. Johnson andM. GerMes  Combining the initial and final gain with the linear dynamic model, a quadratic  difference equation is derived for the overall nonlinear model,  5yt = -aiSyt_i - a25yt-2 + viSut-t_i + v25ut-t-2 + wiSu_t_i + w25u_t_2 (12)  where  Wl ---- bl (1 + al qu a2)(.qf -- .qi) (13)  + b2)(uf -  w2 = b2 (1 + al + a2)(g.f - gi) (14)  + b2)(u - ui)  and v and v2 are given by (9) and (10). Use of the gain at the final steady state  introduces the last two terms of (12). This model allows the incorporation of gain  information over the entire trajectory in the MPC algorithm. The gain at of (12) at  ui is gi while at ui it is gi. Between the two points, the gain is a linear combination  of gi and gi. For processes with large gain changes, such as polymer reactors, this  can lead to dramatic improvements in MPC controller performance.  An additional benefit of using the model of (12) is that we allow the user to bound  the initial and final gain and thus control the amount of nonlinearity used in the  model. For practitioners who are use to implementing MPC with linear models,  using gain bounds allows them to transition from linear to nonlinear models. This  ability to control the amount of nonlinearity used in the model has been important  for acceptance of this new model in many applications. Finally, bounding the gains  can be used to guarantee extrapolation performance of the model.  The nonlinear model of (12) fits the criteria needed in order to allow wide spread  use of nonlinear models for MPC. The model is based upon readily available data  and has a parsimonious representation allowing models with many inputs and out-  puts to be efficiently used in the optimizer. Furthermore, it addresses the primary  nonlinearity found in processes, that being the significant change in gain over the  operating region.  4 Polymer Application  The nonlinear model described above has been used in a wide-variety of industrial  applications including Kamyr digesters (pulp and paper), milk evaporators and  dryers (food processing), toluene diamine purification (chemicals), polyethylene and  polypropylene reactors (polymers) and a fluid catalytic cracking unit (refining).  Highlights of one such application are given below.  A MPC controller that uses the model described above has been applied to a Gas  Phase High Density Polyethylene reactor at Chevron Chemical Co. in Cedar Bayou,  Texas [9]. The process produces homopolymer and copolymer grades over a wide  range of melt indices. It's average production rate per year is 230,000 tons.  Optimal control of the process is difficult to achieve because the reactor is a highly  coupled nonlinear MIMO system (7 inputs and 5 outputs). For example, a number  of input-output pairs exhibit gains that varying by a factor of 10 or more over the  operating region. In addition, grade changes are made every few days. During these  transitions nonprime polymer is produced. Prior to commissioning these controllers,  Neural Network Based Model Predictive Control 1035  these transitions took several hours to complete. Linear and gain scheduling based  controller have been tried on similar reactors and have delivered limited success.  The nonlinear model was constructed using only historical data. The nonlinear  steady state model was trained upon historical data from a two year period. This  data contained examples of all the products produced by the reactor. Accurate dy-  namic models were derived both from historical data and knowledge of the process,  thus, no step tests were conducted on the process.  Excellent performance of this controller has been reported [9]. A two-fold decrease  in the variance of the primary quality variable (melt index) has been achieved. In  addition, the average transition time has been decreased by 50%. Unscheduled  shutdowns which occurred previously have been eliminated. Finally, the controller,  which has been on-line for two years, has gained high operator acceptance.  5 Conclusion  A generic and parsimonious nonlinear model which can be used in an MPC algo-  rithm has been presented. The model is created by combining a nonlinear steady  state model with a linear dynamic models. They are combined using a two-point  gain scheduling technique. This nonlinear model has been used for control of a  nonlinear MIMO polyethylene reactor at Chevron Chemical Co. The controller has  also been used in 50 other applications in the refining, chemicals, food processing  and pulp and paper industries.  References  [1] Qin, S.J. & Badgwell, T.A. (1997) An overview of industrial model predictive control  technology. In J. Kantor, C. Garcia and B. Carnahan (eds.), Chemical Process Control -  AIChE Symposium Series, pp. 232-256. NY: AIChE.  [2] Seborg, D.E. (1999) A perspective on advanced strategies for Process Control (Revis-  ited). to appear in Proc. of European Control Conf. Karlsruhe, Germany.  [3] Qin, S.J. & Badgwell, T.A. (1998) An overview of nonlinear model predictive control  applications. Proc. IFAC Workshop on Nonlinear Model Predictive Control - Assessment  and Future Directions, Ascona, Switzerland, June 3-5.  [4] Meadow, E.S. & Rawlings, J.B. (1997) Model predictive control. In M. Hesnon and D.  Seborg (eds.), Nonlinear Model Predictive Control, pp. 233-310. N J: Prentice Hall.  [5] Nash, S. & Sofer, A. (1996) Linear and Nonlinear Programming. NY: McGraw-Hill.  [6] Rumelhart D.E, Hinton G.E. & Williams, R.J. (1986) Learning internal representations  by error propagation. In D. Rumelhart and J. McClelland (eds.), Parallel Distributed  Processing, pp. 318-362. Cambridge, MA: MIT Press.  [7] Hartman, E. (2000) Training feedforward neural networks with gain constraints. To  appear in Neural Computation.  [8] Ljung, L. (1987) System Identification. N J: Prentice Hall.  [9] Coif S., Johnson D. &: Gerules, M. (1998) Nonlinear control and optimization of a high  density polyethylene reactor. Proc. Chemical Engineering Expo, Houston, June.  
Understanding stepwise generalization of  Support Vector Machines: a toy model  Sebastian Risau-Gusman and Mirta B. Gordon  DRFMC/SPSMS CEA Grenoble, 17 av. des Martyrs  38054 Grenoble cedex 09, France  Abstract  In this article we study the effects of introducing structure in the  input distribution of the data to be learnt by a simple perceptron.  We determine the learning curves within the framework of Statis-  tical Mechanics. Stepwise generalization occurs as a function of  the number of examples when the distribution of patterns is highly  anisotropic. Although extremely simple, the model seems to cap-  ture the relevant features of a class of Support Vector Machines  which was recently shown to present this behavior.  1 Introduction  A new approach to learning has recently been proposed as an alternative to feedfor-  ward neural networks: the Support Vector Machines (SVM) [1]. Instead of trying to  learn a non linear mapping between the input patterns and internal representations,  like in multilayered perceptrons, the SVMs choose a priori a non-linear kernel that  transforms the input space into a high dimensional feature space. In binary classi-  fication tasks like those considered in the present paper, the SVMs look for linear  separation with optimal margin in feature space. The main advantage of SVMs  is that learning becomes a convex optimization problem. The difficulties of having  many local minima that hinder the process of training multilayered neural networks  is thus avoided. One of the questions raised by this approach is why SVMs do not  overfit the data in spite of the extremely large dimensions of the feature spaces  considered.  Two recent theoretical papers [2, 3] studied a family of SVMs with the tools of  Statistical Mechanics, predicting typical properties in the limit of large dimensional  spaces. Both papers considered mappings generated by polynomial kernels, and  more specifically quadratic ones. In these, the input vectors x  R S are trans-  formed to N(N-b 1)/2-dimensional feature vectors ((x). More precisely, the map-  ping ((x) - (X, XlX, X2X,.--,xkx) has been studied in [3] as a function of k,  the number of quadratic features, and (2 (x) -- (x, XlX/N, xx/N,..., XNX/N) has  been considered in [2], leading to different results. These mappings are particu-  lar cases of quadratic kernels. In particular, in the case of learning quadratically  separable tasks with mapping (, the generalization error decreases up to a lower  bound for a number of examples proportional to N, followed by a further decrease  if the number of examples increases proportionally to the dimension of the feature  322 S. Risau-Gusman and M. B. Gordon  space, i.e. to N 2. In fact, this behavior is not specific of the SVMs. It also arises  in the typical case of Gibbs learning (defined below) in quadratic feature spaces [4]:  on increasing the training set size, the quadratic components of the discriminating  surface are learnt after the linear ones. In the case of learning linearly separable  tasks in quadratic feature spaces, the effect of overfitting is harmless, as it only  slows down the decrease of the generalization error with the training set size. In  the case of mapping (1, overfitting is dramatic, as the generalization error at any  given training set size increases with the number k of features.  The aim of the present paper is to understand the influence of the mapping scaling-  factor on the generalization performance of the SVMs. To this end, it is worth to  remark that features 2 may be obtained by compressing the quadratic subspace  of  by a fixed factor. In order to mimic this contraction, we consider a linearly  separable task in which the input patterns have a highly anisotropic distribution, so  that the variance in one subspace is much smaller than in the orthogonal directions.  We show that in this simple toy model, the generalization error as a function of  the training set size exhibits a cross-over between two different behaviors: a rapid  decrease corresponding to learning the components in the uncompressed space, fol-  lowed by a slow improvement in which mainly the components in the compressed  space are learnt. The latter would correspond, in this highly stylized model, to  learning the scaled quadratic features in the SVM with mapping 2.  The paper is organized as follows: after a short presentation of the model, we de-  scribe the main steps of the Statistical Mechanics calculation. The order parameters  caracterizing the properties of the learning process are defined, and their evolution  as a function of the training set size is analyzed. The two regimes of the generaliza-  tion error are described, and we determine the training set size per input dimension  at the crossover, as a function of the pertinent parameters. Finally we discuss our  results, and their relevance to the understanding of the generalization properties of  SVMs.  2 The model  We consider the problem of learning a binary classification task from examples.  The training data set a contains P = aN N-dimensional patterns (",r ")  (p - 1,--.,P) where r" - sign(  w*) is given by a teacher of weights  w* - (w, w, .... , Wn). Without any loss of generality we consider normalized teach-  ers: w*  w* - N. We assume that the components i, (i - 1,-.., N) of the input  patterns  are independent, identically distributed random variables drawn from  a zero-mean gaussian distribution, with variance a along Nc directions and unit  variance in the Nu remaining ones (Nc + Nu - N):  P(')- H v/2ra2 exp ff-a H x/' exp - ' (1)  i N. i N,  We take a < 1 without any loss of generality, as the case a > 1 may be deduced  from the former through a straightforward rescaling of Nc and Nu. Hereafter, the  subspace of dimension Nc and variance a will be called compressed subspace. The  corresponding orthogonal subspace, of dimension Nu = N - Nc, will be called  uncompressed subspace.  We study the typical generalization error of a student perceptron learning the clas-  sification task, using the tools of Statistical Mechanics. The pertinent cost function  Understanding Stepwise Generalization of SVM's: a Toy Model 323  is the number of misclassified patterns:  P  (2)  The weight vectors in version space correspond to a vanishing cost (2). Choosing a  w at random from the a posteriori distribution  - z -1 PO(W) exp  (3)  in the limit of  -  is called Gibbs' learning. In eq. (3),  is equivalent to an  inverse temperature in the Statistical Mechanics formulation, the cost (2) being the  energy function. We assume that P0, the a priori distribution of the weights, is  uniform on the hypersphere of radius v/:  Po(w) -- (27r) -N/2 ((w  w -- X).  (4)  The normalization constant (2e) N/2 is the leading order term of the hypersphere's  surface in N-dimensional space. Z is the partition function ensuring the correct  normalization of P(wla )'  Z(;T)a) = f dw Po(w) exp (-E(w;T)a)).  (5)  In general, the properties of the student are related to those of the free energy  F(/; a) - -lnZ(; a)/. In the limit N -  with the training set size per  input dimension a -- PIN constant, the properties of the student weights become  independant of the particular training set a. They are deduced from the averaged  free energy per degree of freedom, calculated using the replica trick:  f()_ __1 lnZ(;Va )=_ l___lim lnZn(;Va) (6)  N3 N3 n-0 n  where the overline represents the average over 7), composed of patterns selected  according to (1). In the case of Gibbs learning, the typical behavior of any intensive  quantity is obtained in the zero temperature limit/ - oc. In this limit, only error-  free solutions, with vanishing cost, have non-vanishing posterior probability (3).  Thus, Gibbs learning corresponds to picking at random a student in version space,  i.e. a vector w that classifies correctly the training set 7)a, with a probability  proportional to P0 (w).  In the case of an isotropic pattern distribution, which corresponds to a - I in  (1), the properties of cost function (2) have been extensively studied [5]. The case  of patterns drawn from two gaussian clusters in which the symmetry axis of the  clusters is the same [6] and different [7] from the teacher's axis, have recently been  addressed. Here we consider the problem where, instead of having a single direction  along which the patterns' distribution is contracted (or expanded), there is a finite  fraction of compressed dimensions. In this case, all the properties of the student's  perceptron may be expressed in terms of the following order parameters, that have  to satisfy corresponding extremum conditions of the free energy:  qb 1  = (7)  i6 N.  ua 1  = / N w,aw,) (s)  i N  324 S. Risau-Gusman and M. B. Gordon  1  = waw;) (9)  iNc  1  k = < E waw) (10)  i N  1  Q = ( E (wi)2) (11)  iN  where (---) indicates the average over the posterior (3); a, b are replica indices,  and the subcripts c and u stand for compressed and uncompressed respectively.  Notice that we do not impose that Q, the typical squared norm of the student's  components in the compressed subspace, be equal to the corresponding teacher's  norm Q* = ieN.(W)2/N.  3 Order parameters and learning curves  Assuming that the order parameters are invariant under permutation of replicas,  we can drop the replica indices in equations (7) to (11). We expect that this  hypothesis of replica symmetry is consistent, like it is in other cases of perceptrons  learning realizable tasks. The problem is thus reduced to the determination of  five order parameters. Their meaning becomes clearer if we consider the following  combinations:  qc  qu --  Rc --  Ru --  , (12)  1 -Q' (13)  kc  v/Qx/Q  , (14)  x/i-Z-x/1 _ Q * , (15)  1  q = < E (wi)2)' (16)  N.  qc and qu are the typical overlaps between the components of two student vectors in  the compressed and the uncompressed subspaces respectively. Similarly, Rc and Ru  are the corresponding overlaps between a typical student and the teacher. In terms  of this set of parameters, the typical generalization error is eg = (1/r) arccos R with  R = aRc + Ruv/(1 -Q)(1 -Q*)  (17)  v/a:Q+(1-Q)x/a2Q * + (1 - Q*)  Given a, the general solution to the extremum conditions depends on the three  parameters of the problem, namely a, Q* and nc =- Nc/N. An interesting case  is the one where the teacher's anisotropy is consistent with that of the pattern's  distribution, i.e. Q* - nc. In this case, it easy to show that Q = Q*, qc = Rc and  qu = R. Thus, .  nuRu + a2ncRc  R = , (18)  n u -]- ff2n c  where nu -= Nu/N, Ru and Rc are given by the following equations:  Rc _ a 2 a f Z)t exp (-Rt 2/2)  1-Re - a2nc + nu r V - R H(tV) ' (19)  Understanding Stepwise Generalization of SVM's: a Toy Model 325  t I ' I  I ' I ' I ' I  n =0.9 R  ,o =o.o! u..._ ...............:...: ..... - ........  0,8 : . R ..........  / ../ ,%,,,  I' ,," RG .'"'""'0.,I- ..... ' ' ' '%   ,,''' 04 ........................... '  0,4 / [ -' " 't --- ....................  / ' ', ,.' I 00   I; "-..._'" . 't ........  [ ! ;........_ o- o,o o, o, o, o, ,,o  0'21- I ;-' -""'.-.._  0,0 """ ' ' ;' ..... " ........  / I I I , I t I I I I I  0 2 4 6 8 10  Figure 1: Order parameters and generalization error for the case Q* = nc = 0.9,  2 _ 10-2. The curves for the case of spherically distributed patterns is shown for  comparison. The inset shows the first step of learning and its plateau (see text).  ac = e2 au (20)  I - Rc I - Ru'  where/)t = dt e-t2/2/v/- and H(x) = f l)t. If e2 = 1, we recover the equations  corresponding to Gibbs learning of isotropic pattern distributions [5].  The order parameters are represented as a function of a on figure 1, for a particular  choice of nc and . Ru grows much faster than Rc, meaning that it is easier to  learn the components of the uncompressed space. As a result, R (and therefore the  generalization error %) presents a cross-over between two behaviors. At small a,  both Ru << 1 and Re << 1, so that R(c,  2) = Ra(c(nu+4nc)/(nu+2nc) 2) where  Ra is the overlap for Gibbs learning with an isotropic (2 = 1) distribution [5].  Learning the anisotropic distribution is faster (in a) than learning the isotropic  one. If  << I the anisotropy is very large and R increases like Ra but with an  effective training set size per input dimension  a/n > a. On increasing a,  there is an intermediate regime in which Ru increases but Rc << 1, so that R _  Rn,/(nu + 2nc). The corresponding generalization error seems to reach a plateau  corresponding to R = 1 and Rc = 0. At a >> 1, R(a, 2) _ R(a), the asymptotic  behavior is independent of the details of the distribution, like in [7]. The crossover  between these two regimes, when 2 << 1, occurs at a0  v/2(nu + 2nc)/(2nc).  The cases Q* = 1 and Q* = 0 are also of interest. Q* = I corresponds to a teacher  having all the weights components in the compressed subspace, whereas Q* = 0  326 S. Risau-Gusman and M. B. Gordon  g  0,5  0,4  0,3  0,2  0,1  ! ' I  n =0.9  c  o'2=0.01  100  Q*=O.O  0,0 '1 i I I I i I , I , -  0 2 4 6 8  Figure 2: Generalization errors as a function of a for different teachers (Q* = 1,  Q* = 0.9 and Q* = 1), for the case nc = 0.9 and 2 = 10-2. The curve for  spherically distributed patterns [5] is included for comparison. The inset shows the  large alpha behaviors.  corresponds to a teacher orthogonal to the compressed subspace, i.e. with all the  components in the uncompressed subspace. They correspond respectively to tasks  where either the uncompressed or the compressed components are irrelevant for the  patterns' classification. In Figure 2 we show all the generalization error curves,  including the generalization error egg for a uniform distribution [5] for comparison.  The behaviour of eg(a) is very sensitive to the value of Q*. If Q* = 1, the teacher  is in the compressed subspace where learning is difficult. Consequently, g(a) >  egG(a) as expected. On the contrary, for Q* = 0, only the components in the  uncompressed space are relevant for the classification task. In this subspace learning  is easy and eg(a) < egG(a). At Q*  0, i there is a crossover between these regimes,  as already discussed. All the curves merge in the asymptotic regime a - , as  may be seen in the inset of Figure 2.  4 Discussion  We analyzed the typical learning behavior of a toy perceptton model that allows  to clarify some aspects of generalization in high dimensional feature spaces. In  particular, it captures an element essential to obtain stepwise learning, which is  shown to stem from the compression of high order features. The components in the  compressed space are more difficult to learn than those not compressed. Thus, it  Understanding Stepwise Generalization of SVM ' s : a Toy Model 327  the training set is not large enough, mainly the latter are learnt.  Our results allow to understand the importance of the scaling of high order features  in the SVMs kernels. In fact, with SVMs one has to choose a priori the kernel that  maps the input space to the feature space. If high order features are conveniently  compressed, hierarchical learning occurs. That is, low order features are learnt  first; higher order features are only learnt if the training set is large enough. In the  cases where the higher order features are irrelevant, it is likely that they will not  hinder the learning process. This interesting behavior allows to avoid overfitting.  Computer simulations currently in progress, of SVMs generated by quadratic kernels  with and without the 1/N scaling, show a behavior consistent with the theoretical  predictions [2, 3]. These may be understood with the present toy model.  References  [1] V. Vapnik (1995) The nature of statistical learning theory. Springer Verlag,  New York.  [2] R. Dietrich, M. Opper, and H. Sompolinsky (1999) Statistical Mechanics of  Support Vector Networks. Phys. Rev. Lett. 82, 2975-2978.  [3] A. Buhot and M. B. Gordon (1999) Statistical mechanics of support vector  machines. ESANN'99-European Symposium on Artificial Neural Networks Pro-  ceedings, Michel Verleysen ed. 201-206; A. Buhot and M. B. Gordon (1998)  Learning properties of support vector machines. Cond-Mat/9802179.  [4] H. Yoon and J.-H. Oh (1998) Learning of higher order percepttons with tunable  complexities J. Phys. A: Math. Gen. 31, 7771-7784.  [5] G. GySrgyi and N. Tishby (1990) Statistical Theory of Learning a Rule. In  Neural Networks ad Spin Glasses (W. K. Theumann and R. KSberle, Worls  Scientific), 3-36.  [6] R. Meir (1995) Empirical risk minimizaton. A case study. Neural Comp. 7,  144-157.  [7] C. Marangi, M. Biehl, S. A. Solla (1995) Supervised Learning from Clustered  Examples Europhys. Lett. 30 (2), 117-122.  
Dynamics of Supervised Learning with  Restricted Training Sets and Noisy Teachers  A.C.C. Coolen  Dept of Mathematics  King's College London  The Strand, London WC2R 2LS, UK  tcoolen @mth.kcl.ac. uk  C.W.H. Mace  Dept of Mathematics  King's College London  The Strand, London WC2R 2LS, UK  cmace @m th.kcl. ac. uk  Abstract  We generalize a recent formalism to describe the dynamics of supervised  learning in layered neural networks, in the regime where data recycling  is inevitable, to the case of noisy teachers. Our theory generates reliable  predictions for the evolution in time of training- and generalization er-  rors, and extends the class of mathematically solvable learning processes  in large neural networks to those situations where overfitting can occur.  1 Introduction  Tools from statistical mechanics have been used successfully over the last decade to study  the dynamics of learning in layered neural networks (for reviews see e.g. [1] or [2]). The  simplest theories result upon assuming the data set to be much larger than the number  of weight updates made, which rules out recycling and ensures that any distribution of  relevance will be Gaussian. Unfortunately, both in terms of applications and in terms of  mathematical interest, this regime is not the most relevant one. Most complications and  peculiarities in the dynamics of learning arise precisely due to data recycling, which creates  for the system the possibility to improve performance by memorizing answers rather than  by learning an underlying rule. The dynamics of learning with restricted training sets was  first studied analytically in [3] (linear learning rules) and [4] (systems with binary weights).  The latter studies were ahead of their time, and did not get the attention they deserved just  because at that stage even the simpler learning dynamics without data recycling had not  yet been studied. More recently attention has moved back to the dynamics of learning  in the recycling regime. Some studies aimed at developing a general theory [5, 6, 7],  some at finding exact solutions for special cases [8]. All general theories published so far  have in common that they as yet considered realizable scenario's: the rule to be learned  was implementable by the student, and overfitting could not yet occur. The next hurdle is  that where restricted training sets are combined with unrealizable rules. Again some have  turned to non-typical but solvable cases, involving Hebbian rules and noisy [9] or 'reverse  wedge' teachers [10]. More recently the cavity method has been used to build a general  theory [11] (as yet for batch learning only). In this paper we generalize the general theory  launched in [6, 5, 7], which applies to arbitrary learning rules, to the case of noisy teachers.  We will mirror closely the presentation in [6] (dealing with the simpler case of noise-free  teachers), and we refer to [5, 7] for background reading on the ideas behind the formalism.  238 A. C. C. Coolen and C. W. H. Mace  2 Definitions  As in [6, 5] we restrict ourselves for simplicity to perceptrons. A student perceptron oper-  ates a linear separation, parametrised by a weight vector J  N:  $: {-1,1} N --> {-1,1} $({) = sgn[J.{]  It aims to emulate a teacher oerating a similar rule, which, however, is characterized by a  variable weight vector B   , drawn at random from a distribution P(B) such as  output noise: P(B) = 6[B+B*]+(1-)6[B-B*] (1)  Gaussian weight noise: P(B) = [Ex//N] -N e -N(B-B*)2/2 (2)  The parameters ,X and Y control the amount of teacher noise, with the noise-free teacher  B = B* recovered in the limits ,X -- 0 and Y -- 0. The student modifies J iteratively, using  examples of input vectors  which are drawn at random from a fixed (randomly composed)  training set containing p = aN vectors '  {-1, 1) N with a > 0, and the corresponding  values of the teacher outputs. We choose the teacher noise to be consistent, i.e. the answer  given by the teacher to a question ' will remain the same when that particular question  re-appears during the learning process. Thus T( ') = sgn[B '. '], with p teacher weight  vectors B ', drawn randomly and independently from P(B), and we generalize the training  set accordingly to D = {(1, B1),..., (p, Bp)). Consistency of teacher noise is natural  in terms of applications, and a prerequisite for overfitting phenomena. Averages over the  training set will be denoted as (...lb; averages over all possible input vectors   {-1, 1)N  as (.. '/- We analyze two classes of learning rules, of the form J(+l) = J() 4- AJ():  on-line- AJ() =  { () G [J().(),B().()]- ,J() }  (3)  batch' AJ() =  ( ( 0 [J()', B'])b- 3,J(m) }  In on-line learning one draws at each step  a question/answer pair ((), B ()) at ran-  dom from the training set. In batch learning one iterates a deterministic map which is an  average over all data in the training set. Our performance measures are the training- and  generalization errors, defined as follows (with the step function O[x > O] = 1, O[x < O] = 0):  Et(J) = (O[-(J')(B')])b Eg(J)- (O[-(J.)(B*.)]) (4)  We introduce macroscopic observables, taylored to the present problem, generalizing [5, 6]:  Q[j]_j2, R[J]=J.B*, P[x,y,z;J]=(5[x-J.]5[y-B*.]5[z-B.])b (5)  As in [5, 6] we eliminate technical subtleties by assuming the number of arguments (x, y, z)  for which P[x, y, z; J] is evaluated to go to infinity after the limit N -- cx has been taken.  3 Derivation of Macroscopic Laws  Upon generalizing the calculations in [6, 5], one finds for on-line learning:  Q= 21 xdydz P[x.y.z]xO[x.z]-2l.Q+l  xdydz P[x.y.z]O[x.z] (6)  dt = 1 xdydz P[x. y. z] y6[x. z] - lvR (7)  = - -.[x', d]-  /  -, dx'dy'dz' dx'dy'dz'6[x'.z]A[x.y.z;x'.y'.z'] + , {xP[x.y.z]}  +  dx'dy'dz' P[x'.y'.z']6[x'.z']P[x.y.z] (8)  Supervised Learning with Restricted Training Sets 239  The complexity of the problem is concentrated in a Green's function:  .A[x, y, z; x', y', z'] = lim  N-+oo  ' 6x' J '6 ' B* '6 ' B '  Ill[1-'][x-J']dIy-B']d[z-B'](') [ - ' ] [y- '] [y- '  It involves a conditional average of the form (K[J])cwr;t= fdJ pt(JIQ,R,P)K[J], with  pt(J) 6[Q-Q[J]]6[R- R[J]] I-Iyz 6[P[x, y, z]- P[x, y, z; J]]  Pt(JIQ'R'P) = f dJ pt(J) 5[Q-Q[J]]O[R- R[J]] rlxyz 5[P[x, y, z]- P[x, y, z; J]]  in which Pt(J) is the weight probability density at time t. The solution of (6,7,8) can be  used to generate the N  cx performance measures (4) at any time:  Et -/dxdydz P[x,y,z]O[-xz] Eg = x -1 arccos[R/v/] (9)  Expansion of these equations in powers of r/, and retaining only the terms linear in /, gives  the corresponding equations describing batch learning. So far this analysis is exact.  4 Closure of Macroscopic Laws  As in [6, 5] we close our macroscopic laws (6,7,8) by making the two key assumptions  underlying dynamical replica theory:  (i) For N  cx our macroscopic observables obey closed dynamic equations.  (ii) These equations are self-averaging with respect to the specific realization of  (i) implies that probability variations within {Q, R, P} subshells are either absent or irrel-  evant to the macroscopic laws. We may thus make the simplest choice for Pt (JIQ, R, P):  pt(JlO, R,P) - 5[Q-Q[J]]6[R-R[J]] HS[P[x,y,z]-P[x,y,z;J]] (10)  e procedure (10) leads to exact laws if our observables {Q, R, P) indeed obey closed  equations for N  . It is a mimum engopy approximation if not. (ii) allows us  to average the macroscopic laws over all training sets; it is observed in simulations, and  proven using the formalism of [4]. Our assumptions (10) result in the closure of (6,7,8),  since now the Green's function can be written in terms of {O, R, P). c final ingredient  of dynamical replica theo is doing the average of fractions with the replica identity  f aJ w[Jlbl[JID] lim dJ  willY]  Our problem has been reduced to calculating (non-trivial) integrals and averages. One  finds that P[x, y, z] = P[x, zly]P[y ] with P[y] = (2)-exp[    With e sho-hands  Dy = P[y]dy and (f(x,y,z)) = f Dydxdz P[x, zly]f(x,y,z ) we can write the resulting  macroscopic laws, for the case of output noise (1), in the following compact way:  d 2(V 7Q)+veZ = (W - 7R) (11)  P[x, zly] =  dx'P[x',zly] {5[x-x'-G[x',z]]-5[x-x']}+k ZP[x, zly]  0 {P[x,z[y] [U(x-Ry)+Wy-ffx + [V-RW-(O-R=)U][x,y,z]] } (12)  with  U=([x,y,z]6[x,z]), V=(x6[x,z]), W=(y6[x,z]), Z=(6=[x,z])  The solution of (12) is at any time of the following form:  P[x,z]y] = (1-X)5[y-z]P+[x[y] + XS[y+z]P-[x[y] (13)  240 A. C. C. Coolen and C. W. H. Mace  Finding the function  [x, y, z] (in replica symmetric ansatz) requires solving a saddle-point  problem for a scalar observable q and two functions M+[zly]. Upon introducing  B - v/qQ-R2 fax M+[xlY]eBXSf[x,y]  Q(1-q) (f[x,y]) = f dx M+[xly]e Bx8  (with fdx M[xly] - I for all y) the saddle-point equations acquire the form  for all X,y' ?[Xly] =/Z)s (14)   qQ + Q- W f  ((x-Ry) ) + (qQ-R)[1-] = qQ -R DyDs s[(1-A)(x) + A(x);] (15)  The equations (14) which determine M  [xly ] have the same sgucture as the cogesponding  (single) equation in [5, 6], so the proofs in [5, 6] again apply, d the solutions M[xly],  given a q in the physical range q e [Re/Q, 1], e unique. e function [x,y, z] is then  given by  f Ds s  [X,y,z]= v/qQ_R2P[X, zly ]  ( (1-A)5[z-y](d[X-x]), + + Ad[z+y](5[X-x]); }  (16)  Working out predictions from these equations is generally CPU-intensive, mainly due to  the functional saddle-point equation (14) to be solved at each time step. However, as in [7]  one can construct useful approximations of the theory, with increasing complexity:  (i) Large a approximation (giving the simplest theory, without saddle-point equations)  (ii) Conditionally Gaussian approximation for M[x[y] (with y-dependent moments)  (iii) Annealed approximation of the functional saddle-point equation  5 Benchmark Tests: The Limits a- c and A - 0  We first show that in the limit a -->  our theory reduces to the simple (Q, R) formalism  of infinite training sets, as worked out for noisy teachers in [ 12]. Upon making the ansatz  P[xly ] = P[xly ] = [2w(Q-R")]  e [x-RYI2/(O-R2) (17)  one finds  M[xly] - P[xly], q = R/Q, [x,y,z] = (x-Ry)/(Q-R )  Insertion of our ansatz into (12), followed by rearranging of terms and usage of the above  expression for  [x, y, z], shows that (12) is satisfied. The remaining equations (1 l) involve  only averages over the Gaussian distribution (17), and indeed reduce to those of [12]:  ld  d-Q: (l-A)(2(x[x,y])+ r/([x,y])} +A (2(x[x,--y])+ r/([x,--y])}- 2ffQ  ld  v cttR = (1-)(y[x,y]) + (y[x,]) - R  Next we turn to the limit A  0 (restricted aining sets & noise-free teachers) and show that  here our eow reproduces the focalism of [6, 5]. Now we me e following ansatz:  P+[xly] = P[xly], P[x, zly ] = 5[z-y]P[xly ] (18)  Insertion shows that for A = 0 solutions of this fo indeed solve our equations, giving   [x,y,z]  [/,y] and M+[xly] = M[xly ], and leaving us exactly with the formalism  of [6, 5] describing the case of noise-free teachers and resicted aining sets (apt from  some new terms due to the presence of weight decay, which w absent in [6, 5]).  Supervised Learning with Restricted Training Sets 241  05 0.5  0.4 0=0.5 0.4  0,=2  0.3 0.3  0.2 0=2 0.2  [ \ 0=1  ot  ot  0 5 10 15 0 5 10 15  t t  0=0.5  0=1  0=2  0.=4  0=2  = 1  a=0 5  Figure 1: On-line Hebbian learning: conditionally Gaussian approximation versus exact  solution in [9] (r/= 1, , - 0.2). Left: '), - 0.1, right: '), = 0.5. Solid lines: approximated  theory, dashed lines: exact result. Upper curves: Eg as functions of time (here the two  theories agree), lower curves: Et as functions of time.  6 Benchmark Tests: Hebbian Learning  The special case of Hebbian learning, i.e. [x, z] = sgn(z), can be solved exactly at any  time, for arbitrary {a, , 7} [9], providing yet another excellent benchmark for our theory.  For batch execution of Hebbian learning the macroscopic laws are obtained upon expanding  (11,12) and retaining only those terms which are linear in r/. All integrations can now be  done and all equations solved explicitly, resulting in U-0, Z = 1, W = (1-2)Vr-/r, and  2Ro(1  [ ] [1-e-nvt] 2  q qO e -2nvt q- --2,) e_nvt[l_e_nvt] 2  = +  R= Ro e-n'rt+(1-2X)X/[1-e-n'rt]/q , q=  P[xly ] = [2x(Q-R2)] -& e -[x-Ry*sgncy)[1-e-n*t]/av]2/cQ-R2) (19)  From these results, in turn, follow the peffomance mereurns Eg = x- 1 arccos[R/ and  1  ) [y,R+[1-e-Vt]/affl 1 ) lYlR-[1-e -"t]  Et= - (l-A) y erf[ 2  2(Q-R 2)  Comprison with the exact solution, calculamd along the lines of [9] or, equivalently, ob-  tained upon putting t << -2 in [9], shows that the above expressions e all exact.  For on-line execution we cannot (yet) solve the functional saddle-point equmion in general.  However, some analytical predictions c still be exacted om (11,12,13):  Q = Qoe -2nvt+  =  fax x*[xly] =  with U=0, W= (1-2X) V-/r, V= WR+[1-e-n'rtl/aq,, and Z= 1. Comparison with the  results in [9] shows that the above expressions, and thus also that of Eg, are all fully exact,  at any time. Observables involving P[x, y, z] (including the training error) are not as easily  solved from our equations. Instead we used the conditionally Gaussian approximation  (found to be adequate for the noiseless Hebbian case [5, 6, 7]). The result is shown in  figure 1. The agreement is reasonable, but significantly less than that in [6]; apparently  teacher noise adds to the deformation of the field distribution away from a Gaussian shape.  242 A. C. C. Coolen and C. W. H. Mac  E  0.4  0.0 .... OOOOO  0 2 4 6 8 10  t  0.4 -  0.6  0.4  0.2  0.0  -3  I  0.6   0.4  0.2  -2 -1 0 1 2 3  X  0.0  0 2 4 6 8 10 -3 -2 -1 0 1 2 3  t X  Figure 2: Large c approximation versus numerical simulations (with N = 10,000), for  7 = 0 and A = 0.2. Top row: Perceptron rule, with r/- . Bottom row: Adatron rule,  a Left: training errors Et and generalisation errors E o as functions of time, for  with r/= .  OE{ 1  , 1, 2}. Lines: approximated theory, markers: simulations (circles: Et, squares: Eg).  Right: joint distributions for student field and teacher noise P+ [x] = fdy P[z, , z = +]  (upper: P+ [x], lower: P- [x]). Histograms: simulations, lines: approximated theory.  7 Non-Linear Learning Rules: Theory versus Simulations  In the case of non-linear learning rules no exact solution is known against which to test our  formalism, leaving numerical simulations as the yardstick. We have evaluated numerically  the large a approximation of our theory for Perceptron learning, [x, z] = sgn(z)O[--zz],  and for Adatron learning, {[x,z] = sgn(z)lzlO[-zz]. This approximation leads to the  following fully explicit equation for the field distributions:  e+[xly] = 1 dx'P+[x'ly] + ]  a {P[xly] [Wy-'x + U[x-(Y)-Ryl+(V-RW)[x-x-d:(Y)]]}  with - l  Q _ R2  V = f, Dydx x { (1- X)P+[xlylG[x, y]+ XP-[xly]G[x,-y] }  W = [Dydx y { (1-X)P+[xly]G[x, y]+ AP-[IIy]G[I,---y] }  z = 7Oyax { y]+ xP-[xly]{;= Ix,--y] }  Supervised Learning with Restricted Training Sets 243  and with the short-hands +(y) = fdx xpe[xly]. The result of our comparison is shown  in figure 2. Note: Et increases monotonically with a, and Eg decreases monotonically  with a, at any t. As in the noise-free formalism [7], the large a approximation appears to  capture the dominant terms both for a   and for a  0. The predicting power of our  theory is mainly limited by numerical constraints. For instance, the Adatron learning rule  generates singularities at a; = 0 in the distributions P+[a:[y] (especially for small r/) which,  although predicted by our theory, are almost impossible to capture in numerical solutions.  8 Discussion  We have shown how a recent theory to describe the dynamics of supervised learning with  restricted training sets (designed to apply in the data recycling regime, and for arbitrary on-  line and batch learning rules) [5, 6, 7] in large layered neural networks can be generalized  successfully in order to deal also with noisy teachers. In our generalized approach the joint  distribution P[a:, y, z] for the fields of student, 'clean' teacher, and noisy teacher is taken to  be a dynamical order parameter, in addition to the conventional observables Q and/. From  the order parameter set {Q, R, P} we derive the generalization error Eg and the training  error Et. Following the prescriptions of dynamical replica theory one finds a diffusion  equation for P[a:, y, z], which we have evaluated by making the replica-symmetric ansatz.  We have carried out several orthogonal benchmark tests of our theory: (i) for a  cx (no  data recycling) our theory is exact, (ii) for  -+ 0 (no teacher noise) our theory reduces  to that of [5, 6, 7], and (iii) for batch Hebbian learning our theory is exact. For on-line  Hebbian learning our theory is exact with regard to the predictions for Q, R, Eg and the  y-dependent conditional averages fdz zP + [zly ], at any time, and a crude approximation  of our equations already gives reasonable agreement with the exact results [9] for Et. For  non-linear learning rules (Perceptron and Adatron) we have compared numerical solution  of a simple large a aproximation of our equations to numerical simulations, and found  satisfactory agreement. This paper is a preliminary presentation of results obtained in the  second stage of a research programme aimed at extending our theoretical tools in the arena  of learning dynamics, building on [5, 6, 7]. Ongoing work is aimed at systematic applica-  tion of our theory and its approximations to various types of non-linear learning rules, and  at generalization of the theory to multi-layer networks.  References  [9]  [10]  [11]  [12]  [1] Mace C.W.H. and Coolen A.C.C (1998), Statistics and Computing 8, 55  [2] Saad D. (ed.) (1998), On-Line Learning in Neural Networks (Cambridge: CUP)  [3] Hertz J.A., Krogh A. and Thorgersson G.I. (1989), J. Phys. A 22, 2133  [4] Horner H. (1992a), Z. Phys. B 86, 291 and Homer H. (1992b), Z. Phys. B 87, 371  [5] Coolen A.C.C. and Saad D. (1998), in On-Line Learning in Neural Networks, Saad  D. (ed.), (Cambridge: CLIP)  [6] Coolen A.C.C. and Saad D. (1999), in Advances in Neural Information Processing  Systems 11, Kearns D., Solla S.A., Cohn D.A. (eds.), (MIT press)  [7] Coolen A.C.C. and Saad D. (1999),preprints KCL-MTH-99-32 & KCL-MTH-99-33  [8] Rae H.C., Sollich P. and Coolen A.C.C. (1999), in Advances in Neural Information  Processing Systems 11, Kearns D., Solla S.A., Cohn D.A. (eds.), (MIT press)  Rae H.C., Sollich P. and Coolen A.C.C. (1999), J. Phys. A 32, 3321  Inoue J.I. (1999) private communication  Wong K.Y.M., Li S. and Tong Y.W. (1999),preprint cond-mat/9909004  Biehl M., Riegler P. and Stechert M. (1995), Phys. Rev. E 52, 4624  
The Entropy Regularization  Information Criterion  Alex J. Smola  Dept. of Engineering and RSISE  Australian National University  Canberra ACT 0200, Australia  Alex. Smola@anu.edu.au  John Shawe-Tayior  Royal Holloway College  University of London  Egham, Surrey TW20 0EX, UK  john @dcs.rhbnc.ac.uk  Bernhard Schiilkopf  Microsoft Research Limited  St. George House, 1 Guildhall Street  Cambridge CB2 3NH  bsc@microsoft.com  Robert C. Williamson  Dept. of Engineering  Australian National University  Canberra ACT 0200, Australia  Bob. Williamson @anu.edu.au  Abstract  Effective methods of capacity control via uniform convergence bounds  for function expansions have been largely limited to Support Vector ma-  chines, where good bounds are obtainable by the entropy number ap-  proach. We extend these methods to systems with expansions in terms of  arbitrary (parametrized) basis functions and a wide range of regulariza-  tion methods covering the whole range of general linear additive models.  This is achieved by a data dependent analysis of the eigenvalues of the  corresponding design matrix.  1 INTRODUCTION  Model selection criteria based on the Vapnik-Chervonenkis (VC) dimension are known to  be difficult to obtain, worst case, and often not very tight. Yet they have the theoretical  appeal of providing bounds, with few or no assumptions made.  Recently new methods [8, 7, 6] have been developed which are able to provide a better  characterization of the complexity of function classes than the VC dimension, and more-  over, are easily obtainable and take advantage of the data at hand (i.e. they employ the  concept of luckiness). These techniques, however, have been limited to linear functions  or expansions of functions in terms of kernels as happens to be the case in Support Vector  (SV) machines.  In this paper we show that the previously mentioned techniques can be extended to expan-  sions in terms of arbitrary basis functions, covering a large range of practical algorithms  such as general linear models, weight decay, sparsity regularization [3], and regularization  networks [4].  The Entropy Regularization Information Criterion 343  2 SUPPORT VECTOR MACHINES  Support Vector machines carry out an effective means of capacity control by minimizing a  weighted sum of the training error  Remp[f] .- 1   '- -- c(xi,Yi, f(xi)) (1)  m  i=1  and a regularization term Q[f] =  ][wl[ 2' i.e. they minimize the regularized risk functional  Rreg[f] := Reinpill q- AQ[f] - 1 Z c(xi, yi,f(xi)) + 11wll 2.  m  i=1  (2)  Here X := {x,... Xm) C X denotes the training set, Y := {y,... Ym) C  the cor-  responding labels (target values), C,  the corresponding domains,  > 0 a regularization  constant, c: C x  x  -+ I a cost function, and f: C -+  is given by  f(x) := (x, w>, or in the nonlinear case f(x) :-- ((x), w>.  (3)  Here : C -+ :T is a map into a feature space :T. Finally, dot products in feature space can  be written as ((x), (x')) = k(x, x') where k is a so-called Mercer kernel.  For n E IN, I ' denotes the n-dimensional space of vectors x = (x,..., x,). We de-  fine spaces  as follows: as vector spaces, they are identical to I ', in addition, they are  endowed with p-norms:  ) /P  Ilxlle; :- Ilxllp- Ixjl p for0 <p < oo  j=l  Ilxlle:o :- maxj:,...,n Ixjl forp = oo  We write ep = e. Furthermore let Ut; := {x: Ilxllt; <_  } be the unit -ball.  For model selection purposes one wants to obtain bounds on the richness of the map $x  Sx: w -> (f(x),..., f(xm)) = (((Y(x), w),..., ((I)(xm),W}).  (4)  where w is restricted to an 2 unit ball of some radius A (this is equivalent to choosing an  appropriate value of  an increase in  decreases A and vice versa). By the "richness"  of Sx specifically we mean the  e-covering numbers N(e, Sx(AUG),) of the set  Sx (AU,). In the standard COLT notation, we mean  51'(e,$x(AU;),moo) := min {n  There exists a set {z,... z,} C F such that for all }  z  $x(AUG ) we have min IIz- zillt < e  l<i<n --  See [8] for further details.  When carrying out model selection in this case, advanced methods [6] exploit the distribu-  tion of X mapped into feature space :T, and thus of the spectral properties of the operator  Sx by analyzing the spectrum of the Gram matrix G = [gij]ij, where gij :- k (xi, x j).  All this is possible since k(xi,xj) can be seen as a dot product of xi,xj mapped into  some feature space :T, i.e. k(xi, Xj) -- ((I)(xi), (Xj)). This property, whilst true for SV  machines with Mercer kernels, does not hold in general case where f is expanded in terms  of more or less arbitrary basis functions.  344 A. J. Smola, J. Shawe-Taylor, B. Sch6lkopf and R. C. Vlliamson  3 THE BASIC PROBLEMS  One basic problem is that when expanding f into  f(a:) --  aifi(a:) where cti  IR (5)  i=1  with fi(a:) being arbitrary functions, it is not immediately obvious how to regard f as a  dot product in some feature space. One can show that the VC dimension of a set of r  linearly independent functions is r. Hence one would intuitively try to restrict the class of  admissible models by controlling the number of basis functions n in terms of which f can  be expanded.  Now consider an extreme case. In addition to the n basis functions fi defined previously,  we are given r further basis functions f, linearly independent of the previous ones, which  differ from fi only on a small domain 2C , i.e. filx\x' = fJlx\x'. Since this new set of  functions is linearly independent, the VC dimension of the joint set is given by 2r. On the  other hand, if hardly any data occurs on the domain 2C , one would not notice the difference  between fi and ft. In other words, the joint system of functions would behave as if we  only had the initial system of r basis functions.  An analogous situation occurs if f = fi + #i where  is a small constant and #i was  bounded, say, within [0, 1]. Again, in this case, the additional effect of the set of functions  fJ would be hardly noticable, but still, the joint set of functions would count as one with VC  dimension 2r. This already indicates, that simply counting the number of basis functions  may not be a good idea after all.  Figure 1: From left to right: (a) initial set of functions f,..., f5 (dots on the :r-axis  indicate sampling points); (b) additional set of functions f,..., f which differ globally,  but only by a small amount; (c) additional set of functions f,..., f which differ locally,  however by a large amount; (d) spectrum of the corresponding design matrices - the bars  denote the cases (a)-(c) in the corresponding order. Note that the difference is quite small.  On the other hand, the spectra of the corresponding design matrices (see Figure 1) are very  similar. This suggests the use of the latter for a model selection criterion.  Finally we have the practical problem that capacity control, which in SV machines was  carried out by minimizing the length of the "weight vector" w in feature space, cannot be  done in an analogous way either. There are several ways to do this. Below we consider  three that have appeared in the literature and for which there exist effective algorithms.  1 2.  Example 1 (Weight Decay) Define Q[f] :-  5-]i cti , i.e. the coefficients cti of the func-  tion expansion are constrained to an 2 ball. In this case we can consider the following  operator $()'  -+ moo, where  $?'ct->(f(xi),...,f(xm))-((f(xx),ct},...,(f(xm),ct))=Fa (6)  Here f(x) := (fi(x),... fn(X)), Fij := fi(xj), ct := (cti,...,Ctn) and c  AUto for  some A > O.  The Entropy Regularization Information Criterion 345  Example 2 (Sparsity Regularization) In this case Q[f] := Yi ]ci[, i.e the coefficients  ci of the function expansion are constrained to an  ball to enforce sparseness [3]. Thus  $()'  ---> o with $() mapping c as in (6) except c  AUto. This is similar to expan-  sions encountered in boosting or in linear programming machines.  Example 3 (Regularization Networks) Finally one could set Q [f] :-- a -r Q a for some  positive definite matrix Q. For instance, Q ij could be obtained from { P f i , P f j ) where P is  a regularization operator penalizing non-smooth functions [4]. In this case a lives inside  some n-dimensional ellipsoid. By substituting a' := Q  a one can reduce this setting to the  case of example 1 with a different set of basis functions (f' (x) -- Q- f(x)) and consider  an evaluation operator S?'  -+ t given by  S(f):a '  (f(x),...,f(x,)) - ((Q- f(x),a'},...,(Q- f(xm),a'))  (7)  where c   AUt for some A > 0 and Fij = fi(xj) as in example 1.  Example 4 (Support Vector Machines) An important special case of example 3 are Sup-  port Vector Machines where we have Qij = k(xi,xj) and fi(x) -- k(xi, x), hence Q = F.  Hence the possible values generated by a Support Vector Machine can be written as  S?:a'  (f(x),...,f(xm)) = ((Q- f(x),a'),...,(Q- f(xm),a'>)=  (8)  where a   AUt for some A > O.  4 ENTROPY NUMBERS  Covering numbers characterize the difficulty of learning elements of a function class. En-  tropy numbers of operators can be used to compute covering numbers more easily and  more tightly than the traditional techniques based on VC-like dimensions such as the fat  shattering dimension [1]. Knowing el(Sx) = e (see below for the definition) tells one  that log N(e, F, ) <_ l, where F is the effective class of functions used by the regu-  larised learning machines under consideration. In this section we summarize a few basic  definitions and results as presented in [8] and [2].  The Ith entropy number el (F) of a set F with a corresponding metric d is the precision up  to which F can be approximated b_y l elements of F; i.e. for all f E F there exists some  fi  {f,...,.h} such that d(f, fi) <_ el. Hence el(F) is the functional inverse of the  covering number of F.  The entropy number of an bounded linear operator T: A --> B between normed linear  spaces A and B is defined as el(T) := ei(T(UA)) with the metric d being induced by  II  liB. The dyadic entropy numbers el are defined by el :-- e2t+ (the latter quantity is  often more convenient to deal with since it corresponds to the log of the covering number).  We make use of the following three results on entropy numbers of the identity mapping  from n into n diagonal operators, and products of operators. Let  Pl P2'   TI   ' i TI   ldpx,p.'pl --+ p ' dp,p: x - x  The following result is due to Schtitt; the constants 9.94 and 1.86 were obtained in [9].  Proposition 1 (Entropy numbers for identity operators) Be   IN. Then  !  el(ida,2) _< 9.94 log 1 + -  1  & el(id,o ) <_ 1.86 log 1 +   (9)  346 A. J. Smola, J. Shawe-Taylor, B. Sch61kopf and R. C. I4411iamson  Proposition 2 (Carl and Stephani [2, p. 11]) Let E, F, G be Banach spaces, R: F -->  G, and S: E -> F. Then, for n, t 6 N,  Note that the latter two inequalities follow directly from the fact that q (R) = IlRII for all  R: F -+ G by definition of the operator norm IIRII.  Proposition 3 Let al > a2 _> ... >_ aj _> ... _> O, 1 _< p _< c and  Dx = (ax, a2x2,..., ajxj,...)  (11)  for x = (x, x2,..., xj,...)  p be the diagonal operator from p into itself, generated  by the sequence (aj)j. Then for all n  N,  n--1 1 n--1 1  sup _< _< ... o-j);.  (12)  5 THE MAIN RESULT  We can now state the main theorem which gives bounds on the entropy numbers of $() for  the first three examples of model selection described above (since Support Vector Machines  are a special case of example 3 we will not deal with it separately).  Proposition 4 Let f be expanded in a linear combination of basis functions as f :-  n  -i= aifi and the coefficients a restricted to one of the convex sets as described in the  examples 1 to 3. Moreover denote by Fij := fj(xi) the design matrix on a particular  sample X, and by Q the regularization matrix in the case of example 3. Then the following  bound on $x holds.  1. In the case of weight decay (ex. 1) (with li + 12 >_ 1 + 1)  et(S( )) _< 1.96 (l -x log(1 + rnll))  et2 (E).  (13)  2. In the case of weight sparsity regularization (ex. 2) (with l + 12 q- 13 _> 1 + 2)  et(S( )) < 18.48 (I -x log(1 + mllx))   1  eta(E) (/-1og(1 + m/13))  . (14)  3. Finally, in the case of regularization networks (ex. 3) (with l + 12 _> 1 + 1)  e,(S(ff )) _< 1.96 (l - log(1 + m/l))   (15)  Here E is a diagonal scaling operator (matrix) with (i, i) entries v/ and ( v/-)i are the  eigenvalues (sorted in decreasing order) of the matrix FF -r in the case of examples 1 and  2, and FQ-F -r in the case of example 3.  The entropy number of E is readily bounded in terms of (al)i by using (3). One can see  that the first setting (weight decay) is a special case of the third one, namely when Q - 1,  i.e. when Q is just the identity matrix.  Proof The proof relies on a factorization of S? (i = 1, 2, 3) in the following way. First  we consider the equivalent operator x mapping from  to  and perform a singular  value decomposition [5] of the latter into x -- VEW where V, W are operators of norm  1, and E contains the singular values of S?, i.e. the singular values of F and FQ-  The Entropy Regularization Information Criterion 347  respectively.  FF T or FQ-1F T. Consequently we can factorize S? as in the diagram  The latter, however, are identical to the square root of the eigenvalues of  Finally, in order to compute the entropy number of the overall operator one only has to  use the factorization of Sx into S? = id2,VEW for i  {1,3} and into S( ) =  id2m,VEWid,2 for example 2, and apply Proposition 2 several times. We also exploit  the fact that for singular value decompositions 11vII, IlWll <_ 1. I  The present theorem allows us to compute the entropy numbers (and thus the complexity)  of a class of functions on the current sample X. Going back to the examples of section 3,  which led to large bounds on the VC dimension one can see that the new result is much less  susceptible to such modifications: the addition of f{,... f to f,... f, does not change  the eigenspectrum E of the design matrix significantly (possibly only doubling the nominal  value of the singular values), if the functions f differ from fi only slightly. Consequently  also the bounds will not change significantly even though the number of basis functions  just doubled.  Also note that the current error bounds reduce to the results of [6] in the SV case: here  Oij '- Fij -- k(xi, Xj) (both the design matrix F and the regularization matrix Q are  determined by kernels) and therefore FQ-F -- Q. Thus the analysis of the singular  values of FQ-F leads to an analysis of the eigenvalues of the kernel matrix, which is  exactly what is done when dealing with SV machines.  6 ERROR BOUNDS  To use the above result we need a bound on the expected error of a hypothesis f in terms  of the empirical error (training error) and the observed entropy numbers e,(:Y). We use [6,  Theorem 4.1] with a small modification.  Theorem 1 Let Y be a set of linear functions as described in the previous examples with  en (Sx) as the corresponding bound on the observed entropy numbers of Y on the dataset  X. Moreover suppose that for a fixed threshold b  11 for some f  Y, sgn(f - b) correctly  classifies the set X with a margin 7 :- min<i<m [f(:ei) - b[.  Finally let U := min{n  N with e,(Sx) < 7/8.001} and a(U, 6) := 3.08(1 + In -}).  Then with confidence 1 - 6 over X (drawn randomly from P" where P is some probability  distribution) the expected error of sgn(f - b) is bounded from above by  = 2 (U(l+a(U,)log(5--)log(17m))+log(-)). (17)  U, 6)  The proof is essentially identical to that of [6, Theorem 4.1] and is omitted. [6] also shows  how to compute e, (Sx) efficiently including an explicit formula for evaluating et (E).  7 DISCUSSION  We showed how improved bounds could be obtained on the entropy numbers of a wide  class of popular statistical estimators ranging from weight decay to sparsity regularization  348 A. J. Smola, J. Shawe-Taylor, B. Sch6lkopf and R. C. Vlliamson  (with SV machines being a special case thereof). The results are given in a way that is  directly useable for practicioners without any tedious calculations of the VC dimension or  similar combinatorial quantities. In particular, our method ignores (nearly) linear depen-  dent basis functions automatically. Finally, it takes advantage of favourable distributions  of data by using the observed entropy numbers as a base for stating bounds on the true  entropy numbers with respect to the function class under consideration.  Whilst this leads to significantly improved bounds (we achieved an improvement of ap-  proximately two orders of magnitude over previous VC-type bounds involving only the  radius of the data R and the weight vector IIwll in the experiments) on the expected risk,  the bounds are still not good enough to become predictive. This indicates that possibly  rather than using the standard uniform convergence bounds (as used in the previous sec-  tion) one might want to use other techniques such as a PAC-Bayesian treatment (as recently  suggested by Herbrich and Graepel) in combination with the bounds on eigenvalues of the  design matrix.  Acknowledgements: This work was supported by the Australian Research Council and a  grant of the Deutsche Forschungsgemeinschaft SM 62/1-1.  References  [1] N. Alon, S. Ben-David, N. Cesa-Bianchi, and D. Haussler. Scale-sensitive Dimen-  sions, Uniform Convergence, and Learnability. J. of the ACM, 44(4):615-631, 1997.  [2] B. Carl and I. Stephani. Entropy, compactness, and the approximation of operators.  Cambridge University Press, Cambridge, UK, 1990.  [3] S. Chen, D. Donoho, and M. Saunders. Atomic decomposition by basis pursuit. Tech-  nical Report 479, Department of Statistics, Stanford University, 1995.  [4] F. Girosi, M. Jones, and T Poggio. Regularization theory and neural networks archi-  tectures. Neural Computation, 7:219-269, 1995.  [5] R. A. Horn and C. R. Johnson. Matrix Analysis. Cambridge University Press, Cam-  bridge, 1992.  [6] B. Sch61kopf, J. Shawe-Taylor, A. J. Smola, and R. C. Williamson. Generalization  bounds via eigenvalues of the gram matrix. Technical Report NC-TR-99-035, Neuro-  Colt2, University of London, UK, 1999.  [7] J. Shawe-Taylor and R. C. Williamson. Generalization performance of classifiers in  terms of observed covering numbers. In Proc. EUROCOLT'99, 1999.  [8] R. C. Williamson, A. J. Smola, and B. Sch61kopf. Generalization performance of  regularization networks and support vector machines via entropy numbers of compact  operators. NeuroCOLT NC-TR-98-019, Royal Holloway College, 1998.  [9] R. C. Williamson, A. J. Smola, and B. Sch61kopf. A Maximum Margin Miscellany.  Typescript, 1999.  
Low Power Wireless Communication via  Reinforcement Learning  Timothy X Brown  Electrical and Computer Engineering  University of Colorado  Boulder, CO 80309-0530  timxb@colorado. edu  Abstract  This paper examines the application of reinforcement learning to a wire-  less communication problem. The problem requires that channel util-  ity be maximized while simultaneously minimizing battery usage. We  present a solution to this multi-criteria problem that is able to signifi-  cantly reduce power consumption. The solution uses a variable discount  factor to capture the effects of battery usage.  1 Introduction  Reinforcement learning (RL) has been applied to resource allocation problems in telecom-  munications, e.g., channel allocation in wireless systems, network routing, and admission  control in telecommunication networks [1, 2, 8, 10]. These have demonstrated reinforce-  ment learning can find good policies that significantly increase the application reward  within the dynamics of the telecommunication problems. However, a key issue is how  to treat the commonly occurring multiple reward and constraint criteria in a consistent way.  This paper will focus on power management for wireless packet communication channels.  These channels are unlike wireline channels in that channel quality is poor and varies over  time, and often one side of the wireless link is a battery operated device such as a laptop  computer. In this environment, power management decides when to transmit and receive  so as to simultaneously maximize channel utility and battery life.  A number of power management strategies have been developed for different aspects of  battery operated computer systems such as the hard disk and CPU [4, 5]. Managing the  channel is different in that some control actions such as shutting off the wireless transmitter  make the state of the channel and the other side of the communication unobservable.  In this paper, we consider the problem of finding a power management policy that simul-  taneously maximizes the radio communication's earned revenue while minimizing battery  usage. The problem is recast as a stochastic shortest path problem which in turn is mapped  to a discounted infinite horizon with a variable discount factor. Results show significant  reductions in power usage.  894 T. X. Brown  l Mobile  Application [-[  Radio Channel Radio  Application  Figure 1: The five components of the radio communication system.  2 Problem Description  The problem is comprised of five components as shown in Figure 1: mobile application,  mobile radio, wireless channel, base station radio, and base station application. The ap-  plications on each end generate packets that are sent via a radio across the channel to the  radio and then application on the other side. The application also defines the utility of a  given end-to-end performance. The radios implement a simple acknowledgment/retransmit  protocol for reliable transmission. The base station is fixed and has a reliable power supply  and therefore is not power constrained. The mobile power is limited by a battery and it  can choose to turn its radio off for periods of time to reduce power usage. Note that even  with the radio off, the mobile system continues to draw power for other uses. The channel  adds errors to the packets. The rate of errors depends on many factors such as location  of mobile and base station, intervening distance, and levels of interference. The problem  requires models for each of these components. To be concrete, the specific models used in  this paper are described in the following sections. It should be emphasized that in order to  focus on the machine learning issues, simple models have been chosen. More sophisticated  models can readily be included.  2.1 The Channel  The channel carries fixed-size packets in synchronous time slots. All packet rates are nor-  malized by the channel rate so that the channel carries one packet per unit time in each  direction. The forward and reverse channels are orthogonal and do not interfere.  Wireless data channels typically have low error rates. Occasionally, due to interference or  signal fading, the channel introduces many errors. This variation is possible even when the  mobile and base station are stationary. The channel is modeled by a two state Gilbert-Elliot  model [3]. In this model, the channel is in either a "good" or a "bad" state with a packet  error probabilities pg and pb where pg < pb. The channel is symmetric with the same loss  rate in both directions. The channel stays in each state with a geometrically distributed  holding time with mean holding times ha and hb time slots.  2.2 Mobile and Base Station Application  The traffic generated by the source is a bursty ON/OFF model that alternates between gen-  erating no packets and generating packets at rate tON. The holding times are geometrically  distributed with mean holding times hON and ]tOF F. The traffic in each direction is inde-  pendent and identically distributed.  2.3 The Radios  The radios can transmit data from the application and send it on the channel and simul-  taneously receive data from the other radio and pass it on to its application. The radios  implement a simple packet protocol to ensure reliability. Packets from the sources are  queued in the radio and sent one by one. Packets consist of a header and data. The header  carries acknowledgements (ACK's) with the most recent packet received without error. The  header contains a checksum so that errors in the payload can be detected. Errored packets  Low Power I4qreless Communication via Reinforcement Learning 895  Parameter Name Symbol Value  Channel Error Rate, Good pg 0.01  Channel Error Rate, Bad Pb 0.20  Channel Holding Time, Good hg 100  Channel Holding Time, Bad hb 10  Source On Rate tON 1.0  Source Holding Time, On ]tON 1  Source Holding Time, Off hoF F 10  Power, Radio Off POFF 7 W  Power, Radio On PON 8.5 W  Power, Radio Transmitting PTX 10 W  Real Time Max Delay d,ax 3  Web Browsing Time Scale do 3  Table 1: Application parameters.  cause the receiving radio to send a packet with a negative acknowledgment (NACK) to the  other radio instructing it to retransmit the packet sequence starting from the errored packet.  The NACK is sent immediately even if no data is waiting and the radio must send an empty  packet. Only unerrored packets are sent on to the application. The header is assumed to  always be received without error  .  Since the mobile is constrained by power, the mobile is considered the master and the base  station the slave. The base station is always on and ready to transmit or receive. The mobile  can turn its radio off to conserve power. Every ON-OFF and OFF-ON transition generates  a packet with a message in the header indicating the change of state to the base station.  These message packets carry no data. The mobile expends power at three levels--PoFF,  PON, and Ptx corresponding to the radio off, receiver on but no packet transmitted, and  receiver on packet transmitted.  2.4 Reward Criteria  Reward is earned for packets passed in each direction. The amount depends on the ap-  plication. In this paper we consider three types of applications, an e-mail application, a  real-time application, and a web browsing application. In the e-mail application, a unit  reward is given for every packet received by the application. In the real time application a  unit reward is given for every packet received by the application with delay less than d,a.  The reward is zero otherwise. In the web browsing application, time is important but not  critical. The value of a packet with delay d is (1 - I/d0) a, where do is the desired time  scale of the arrivals.  The specific parameters used in this experiment are given in Table 1. These were gathered  as typical values from [7, 9]. It should be emphasized that this model is the simplest  model that captures the essential characteristics of the problem. More realistic channels,  protocols, applications, and rewards can readily be incorporated but for this paper are left  out for clarity.  A packet error rate of 20% implies a bit error rate of less than 1%. Error correcting codes in the  header can easily reduce this error rate to a low value. The main intent is to simplify the protocol for  this paper so that time-outs and other mechanisms do not need to be considered.  896 T. X. Brown  Component States  Channel {good,bad}  Application { ON,OFF}  Mobile {ON,OFF}  Mobile {List of waiting and unacknowledged packets and their current delay}  Base Station {List of waiting and unacknowledged packets and their current delay}  Table 2: Components to System State.  3 Markov Decision Processes  At any given time slot, t, the system is in a particular configuration, z, defined by the state  of each of the components in Table 2. The system state is s = (z, t) where we include  the time in order to facilitate accounting for the battery. The mobile can choose to toggle  its radio between the ON and OFF state and rewards are generated by successfully received  packets. The task of the learner is to determine a radio ON/OFF policy that maximizes the  total reward for packets received before batteries run out.  The battery life is not a fixed time. First, it depends on usage. Second, for a given drain,  the capacity depends on how long the battery was charged, how long it has sat since being  charged, the age of the battery, etc. In short, the battery runs out at a random time. The  system can be modeled as a stochastic shortest path problem whereby there exists a terminal  state, so, that corresponds to the battery empty in which no more reward is possible and the  system remains permanently at no cost.  3.1 Multi-criteria Objective  Formally, the goal is to learn a policy for each possible system state so as to maximize  J(s) = E c(t) s, ,  t=0  where E{.Is, is the expectation over possible trajectories starting from state s using  policy r, c(t) is the reward for packets received at time t, and T is the last time step before  the batteries run out.  Typically, T is very large and this inhibits fast learning. So, in order to promote faster  learning we convert this problem to a discounted problem that removes the variance caused  by the random stopping times. At time t, given action a (t), while in state s (t) the terminal  state is reached with probability Ps(t) (a(t)). Setting the value of the terminal state to 0, we  can convert our new criterion to maximize:  J(s) = E c(t) (1-ps()(a(r))) s,r ,  t=O r=0  where the product is the probability of reaching time t. In words, future rewards are dis-  counted by 1 - Ps (a), and the discounting is larger for actions that drain the batteries  faster. Thus a more power efficient strategy will have a discount factor closer to one which  correctly extends the effective horizon over which reward is captured.  3.2 Q-learning  RL methods solve MDP problems by learning good approximations to the optimal value  function, J*, given by the solution to the Bellman optimality equation which takes the  Low Power I4qreless Communication via Reinforcement Learning 897  following form:  J*(s) = max [E,{c(s,a,s') + (1 -p(a))J*(st)}] (1)  aA(s)  where A(s) is the set of actions available in the current state s, c(s, a, s t) is the effective  immediate payoff, and E, {.} is the expectation over possible next states s t.  We learn an appr9ximation to J* using Watkin's Q-learning algorithm. Bellman's equation  can be rewritten in Q-factor as  J*(s) = max Q*(s,a) (2)  aA(s)  In every time step the following decision is made. The Q-value of turning on in the next  state is compared to the Q-value of turning off in the next state. If turning on has higher  value the mobile turns on. Else, the mobile turns off.  Whatever our decision, we update our value function as follows: on a transition from state  s to s t on action a,  Q(s,a) = (1-)Q(s,a)+(c(s,a, st)+(1-p(a))max  bA(s')  (3)  where ff is the learning rate. In order for Q-learning to perform well, all potentially impor-  tant state-action pairs (s, a) must be explored. At each state, with probability 0.1 we apply  a random action instead of the action recommended by the Q-value. However, we still use  (3) to update Q-values using the action b recommended by the Q-values.  3.3 Structural Limits to the State Space  For theoretical reasons it is desirable to use a table lookup representation. In practice,  since the mobile radio decides using information available to it, this is impossible for the  following reasons. The state of the channel is never known directly. The receiver only  observes errored packets. It is possible to infer the state, but, only when packets are actually  received and channel state changes introduce inference errors.  Traditional packet applications rarely communicate state information to the transport layer.  This state information could also be inferred. But, given the quickly changing application  dynamics, the application state is often ignored. For the particular parameters in Table 1,  (i.e. tON = 1.0) the application is on if and only if it generates a packet so its state is  completely specified by the packet arrivals and does not need to be inferred.  The most serious deficiency to a complete state space representation is that when the mobile  radio turns OFF, it has no knowledge of state changes in the base station. Even when it is  ON, the protocol does not have provisions for transferring directly the state information.  Again, this implies that state information must be inferred.  One approach to these structural limits is to use a POMDP approach [6] which we leave to  future work. In this paper, we simply learn deterministic policies on features that estimate  the state.  3.4 Simplifying Assumptions  Beyond the structural problems of the previous section we must treat the usual problem that  the state space is huge. For instance, assuming even moderate maximum queue sizes and  maximum wait times yields 1020 states. If one considers e-mail like applications where  898 T. X. Brown  Component Feature  Mobile Radio is radio ON or OFF  Mobile Radio number of packets waiting at the mobile  Mobile Radio wait time of first packet waiting at the mobile  Channel number of errors received in last 4 time slots  Base Radio number of time slots since mobile was last ON  Table 3: Decision Features Measured by Mobile Radio  wait times of minutes (1000's of time slot wait times) with many packets waiting possible,  the state space exceeds 10 m states. Thus we seek a representation to reduce the size and  complexity of the state space. This reduction is taken in two parts. The first is a feature  representation that is possible given the structural limits of the previous section, the second  is a function approximation based on these feature vectors.  The feature vectors are listed in Table 3. These are chosen since they are measurable at  the mobile radio. For function approximation, we use state aggregation since it provably  converges.  4 Simulation Results  This section describes simulation-based experiments on the mobile radio control problem.  For this initial study, we simplified the problem by setting pg = Pb = 0 (i.e. no channel  errors).  State aggregation was used with 4800 aggregate states. The battery termination probability,  Ps (a) was simply P/1000 where P is the power appropriate for the state and action chosen  from Table 1. This was chosen to have an expected battery life much longer than the time  scale of the traffic and channel processes.  Three policies were learned, one for each application reward criteria. The resulting policies  are tested by simulating for 106 time slots.  In each test run, an upper and lower bound on the energy usage is computed. The upper  bound is the case of the mobile radio always on 2. The lower bound is a policy that ignores  the reward criteria but still delivers all the packets. In this policy, the radio is off and packets  are accumulated until the latter portion of the test run when they are sent in one large group.  Policies are compared using the normalized power savings. This is a measure of how close  the policy is to the lower bound with 0% and 100% being the upper and lower bound.  The results are given in Table 4. The table also lists the average reward per packet received  by the application. For the e-mail application, which has no constraints on the packets, the  average reward is identically one.  5 Conclusion  This paper showed that reinforcement learning was able to learn a policy that significantly  reduced the power consumption of a mobile radio while maintaining a high application  utility. It used a novel variable discount factor that captured the impact of different actions  on battery life. This was able to gain 50% to 80% of the possible power savings.  2There exist policies that exceed this power, e.g. if they toggle ONand OFFoften and generate many  notification packets. But, the always on policy is the baseline that we are trying to improve upon.  Low Power Wireless Communication via Reinforcement Learning 899  Normalized Average  Application Power Savings Reward  E-mail 81% 1  Real Time 49% 1.00  Web Browsing 48% 0.46  Table 4: Simulation Results.  In the application the paper used a simple model of the radio, channel, battery, etc. It also  used simple state aggregation and ignored the partially observable aspects of the problem.  Future work will address more accurate models, function approximation, and POMDP ap-  proaches.  Acknowledgment  This work was supported by CAREER Award: NCR-9624791 and NSF Grant NCR-  9725778.  References  [lO]  [1] Boyan, J.A., Littman, M.L., "Packet routing in dynamically changing networks: a  reinforcement learning approach," in Cowan, J.D., et al., ed. Advances in NIPS 6,  Morgan Kauffman, SF, 1994. pp. 671-678.  [2] Brown, T.X, Tong, H., Singh, S., "Optimizing admission control while ensuring qual-  ity of service in multimedia networks via reinforcement learning," in Advances in  Neural Information Processing Systems 12, ed. M. Kearns, et al., MIT Press, 1999,  pp. 982-988.  [3] Goldsmith, A.J., Varaiya, P.P., "Capacity, mutual information, and coding for finite  state Markov channels," IEEE T. on Info. Thy., v. 42, pp. 868-886, May 1996.  [4] Govil, K., Chan, E., Wasserman, H., "Comparing algorithms for dynamic speed-  setting of a low-power cpu;' Proceedings of the First ACM Int. Conf. on Mobile  Computing and Networking (MOBICOM), 1995.  [5] Helmbold, D., Long, D.D.E., Shetrod, B., "A dynamic disk spin-down technique for  mobile computing. Proceedings of the Second ACM Int. Conf. on Mobile Computing  and Networking (MOBICOM), 1996.  [6] Jaakola, T., Singh, S., Jordan, M.I., "Reinforcement Learning Algorithm for Partially  Observable Markov Decision Problems," in Advances in Neural Information Process-  ing Systems 7, ed. G. Tesauro, et al., MIT Press, 1995, pp. 345-352.  [7] Kravits, R., Krishnan, P., "Application-Driven Power Management for Mobile Com-  munication," Wireless Networks, 1999.  [8] Marbach, P., Mihatsch, O., Schulte, M., Tsitsiklis, J.N., "Reinforcement learning for  call admission control and routing in integrated service networks," in Jordan, M., et  al., ed. Advances in NIPS 10, MIT Press, 1998.  [9] Rappaport, T.S., Wireless Communications: Principles and Practice, Prentice-Hall  Pub., Englewood Cliffs, NJ, 1996.  Singh, S.P., Bertsekas, D.P., "Reinforcement learning for dynamic channel allocation  in cellular telephone systems," in Advances in NIPS 9, ed. Mozer, M., et al., MIT  Press, 1997. pp. 974-980.  
Independent Factor Analysis with  Temporally Structured Sources  Hagai Attias  hagai@gatsby. ucl.ac.uk  Gatsby Unit, University College London  17 Queen Square  London WC1N 3AR, U.K.  Abstract  We present a new technique for time series analysis based on dy-  namic probabilistic networks. In this approach, the observed data  are modeled in terms of unobserved, mutually independent factors,  as in the recently introduced technique of Independent Factor Anal-  ysis (IFA). However, unlike in IFA, the factors are not i.i.d.; each  factor has its own temporal statistical characteristics. We derive a  family of EM algorithms that learn the structure of the underlying  factors and their relation to the data. These algorithms perform  source separation and noise reduction in an integrated manner, and  demonstrate superior performance compared to IFA.  I Introduction  The technique of independent factor analysis (IFA) introduced in [1] provides a  tool for modeling L-dim data in terms of L unobserved factors. These factors  are mutually independent and combine linearly with added noise to produce the  observed data. Mathematically, the model is defined by  yt - Hxt q- ut ,  (1)  where xt is the vector of factor activities at time t, Yt is the data vector, H is the  L  x L mixing matrix, and ut is the noise.  The origins of IFA lie in applied statistics on the one hand and in signal processing  on the other hand. Its statistics ancestor is ordinary factor analysis (FA), which as-  sumes Gaussian factors. In contrast, IFA allows each factor to have its own arbitrary  distribution, modeled semi-parametrically by a 1-dim mixture of Gaussians (MOG).  The MOG parameters, as well as the mixing matrix and noise covariance matrix,  are learned from the observed data by an expectation-maximization (EM) algorithm  derived in [1]. The signal processing ancestor of IFA is the independent component  analysis (ICA) method for blind source separation [2]-[6]. In ICA, the factors are  termed sources, and the task of blind source separation is to recover them from the  observed data with no knowledge of the mixing process. The sources in ICA have  non-Gaussian distributions, but unlike in IFA these distributions are usually fixed  by prior knowledge or have quite limited adaptability. More significant restrictions  Dynamic Independent Factor Analysis 387  are that their number is set to the data dimensionality, i.e. L = L  ('square mix-  ing'), the mixing matrix is assumed invertible, and the data are assumed noise-free  (ut = 0). In contrast, IFA allows any L, L  (including more sources than sensors,  L > L'), as well as non-zero noise with unknown covariance. In addition, its use of  the flexible MOG model often proves crucial for achieving successful separation [1].  Therefore, IFA generalizes and unifies FA and ICA. Once tle model has been  learned, it can be used for classification (fitting an IFA model for each class), com-  pleting missing data, and so on. In the context of blind separation, an optimal  reconstruction of the sources xt from data is obtained [1] using a MAP estimator.  However, IFA and its ancestors suffer from the following shortcoming: They are  oblivious to temporal information since they do not attempt to model the temporal  statistics of the data (but see [4] for square, noise-free mixing). In other words, the  model learned would not be affected by permuting the time indices of {yt }. This is  unfortunate since modeling the data as a time series would facilitate filtering and  forecasting, as well as more accurate classification. Moreover, for source separation  applications, learning temporal statistics would provide additional information on  the sources, leading to cleaner source reconstructions.  To see this, one may think of the problem of blind separation of noisy data in terms  of two components: source separation and noise reduction. A possible approach  might be the following two-stage procedure. First, perform noise reduction using,  e.g., Wiener filtering. Second, perform source separation on the cleaned data us-  ing, e.g., an ICA algorithm. Notice that this procedure directly exploits temporal  (second-order) statistics of the data in the first stage to achieve stronger noise re-  duction. An alternative approach would be to exploit the temporal structure of  the data indirectly, by using a temporal source model. In the resulting single-stage  algorithm, the operations of source separation and noise reduction are coupled. This  is the approach taken in the present paper.  In the following, we present a new approach to the independent factor problem  based on dynamic probabilistic networks. In order to capture temporal statistical  properties of the observed data, we describe each source by a hidden Markov model  (HMM). The resulting dynamic model describes a multivariate time series in terms  of several independent sources, each having its own temporal characteristics. Section  2 presents an EM learning algorithm for the zero-noise case, and section 3 presents  an algorithm for the case of isotropic noise. The case of non-isotropic noise turns out  to be computationally intractable; section 4 provides an approximate EM algorithm  based on a variational approach.  Notation: The multivariable Gaussian density is denoted by G(z, E) =l 2rY:, 1-1/2  exp(--zTE-z/2). We wo. rk with T-point time blocks denoted X:T = {xt}=l. The  ith coordinate of xt is xl. For a function f, (f(x:T)} denotes averaging over an  ensemble of x:- blocks.  2 Zero Noise  The MOG source model employed in IFA [1] has the advantages that (i) it is capable  of approximating arbitrary densities, and (ii) it can be learned efficiently from data  by EM. The Gaussians correspond to the hidden states of the sources, labeled by  s. Assume that at time t, source i is in state sl - s. Its signal xl is then generated  i In order  by sampling from a Gaussian distribution with mean uis and variance 's.  to capture temporal statistics of the data, we endow the sources with temporal  structure by introducing a transition matrix i between the states. Focusing on  388 H. Attias  a time block t - 1, ..., T, the resulting probabilistic model is defined by  i  p(s = s l i i p(s = s) = 7rs ,  St-1 -- st) __ as's ,  i = s) -- (x //s e) P(Yl:T) =l det G I T p(Xl:T) (2)  p(x l s t - , , ,  where p(xi:r) is the joint density of all sources x i i = 1, L at all time points, and  the last equation follows from xt = Gyt with G = H - being the unmixing matrix.  As usual in the noise-free scenario (see [2]; section 7 of [1]), we are assuming that  the mixing matrix is square and invertible.  The graphical model for the observed density P(Y:T I w) defined by (2) is  i i i  parametrized by W = {Gij,l.t}, es,7rs,as, s}. This model describes each source as  a first-order HMM; it reduces to a time-independent model if i i Whereas  a$/$  7r$.  temporal structure can be described by other means, e.g. a moving-average [4] or  autoregressive [6] model, the HMM is advantageous since it models high-order tem-  poral statistics and facilitates EM learning. Omitting the derivation, maximization  with respect to Gij results in the incremental update rule  T  5G = G- -  (xt)xtTG, (3)  where (x) = Es 7[(s)(x ' '  - s)/es, and the natural gradient [3] was used; e is an  appropriately chosen learning rate. For the source parameters we obtain the update  rules  u,; = E,,t(8)x ,= ,  Et 7j(s) ' es Et 7(s) , as, s = Et ' (4)  i = ?(s.). We used the standard HMM  with the initial probabilities updated via 7r s  notation 7(s) = p(s = s I X:T), (s',s) -- p(s}_ = s',s = s I X:T). These  posterior densities are computed in the E-step for each source, which is given in  terms of the data via x = yj Gijyt j, using the forward-backward procedure [7].  The algorithm (3-4) may be used in several possible generalized EM schemes. An  efficient one is given by the following two-phase procedure: (i) freeze the source  parameters and learn the separating matrix G using (3); (ii) freeze G and learn the  source parameters using (4), then go back to (i) and repeat. Notice that the rule (3)  is similar to a natural gradient version of Bell and Sejnowski's ICA rule [2]; in fact,  the two coincide for time-independent sources where ck(xi) - -01ogp(xi)/Oxi. We  also recognize (4) as the Baum-Welch method. Hence, in phase (i) our algorithm  separates the sources using a generalized ICA rule, whereas in phase (ii) it learns  an HMM for each source.  Remark. Often one would like to model a given L'-variable time series in terms  of a smaller number L < L' of factors. In the framework of our noise-free model  yt -- Hxt, this can be achieved by applying the above algorithm to the L largest  principal components of the data; notice that if the data were indeed generated by L  factors, the remaining L'-L principal components would vanish. Equivalently, one  may apply the algorithm to the data directly, using a non-square L x L' unmixing  matrix G.  Results. Figure 1 demonstrates the performance of the above method on a 4 x 4  mixture of speech signals, which were passed through a non-linear function to mod-  ify their distributions. This mixture is inseparable to ICA because the source model  used by the latter does not fit the actual source densities (see discussion in [1]). We  also applied our dynamic network to a mixture of speech signals whose distributions  Dynamic Independent Factor Analysis 389  0.8  0.7  0.8  , 0.5  0.4  0.3  0.2  0.1  0  --2 0 2  x  3  HMM--ICA ICA  4 --2 0 2 --2 0 2  Figure 1: Left: Two of the four source distributions. Middle: Outputs of the EM algo-  rithm (3-4) are nearly independent. Right: the outputs of ICA [2] are correlated.  were made Gaussian by an appropriate non-linear transformation. Since temporal  information is crucial for separation in this case (see [4],[6]), this mixture is in-  separable to ICA and IFA; however, the algorithm (3-4) accomplished separation  successfully.  3 Isotropic Noise  We now turn to the case of non-zero noise ut  0. We assume that the noise is white  and has a zero-mean Gaussian distribution with covariance matrix A. In general,  this case is computationally intractable (see section 4). The reason is that the E-  step requires computing the posterior distribution p(S0:T, X:T I Y:T) not only over  the source states (as in the zero-noise case) but also over the source signals, and  this posterior has a quite complicated structure. We now show that if we assume  isotropic noise, i.e. Aij = Sij, as well as square invertible mixing as above, this  posterior simplifies considerably, making learning and inference tractable. This is  done by adapting an idea suggested in [8] to our dynamic probabilistic network.  We start by pre-processing the data using a linear transformation that makes their  covariance matrix unity, i.e., (ytyt T) = I ('sphering'). Here (-) denotes averaging  over T-point time blocks. From (1) it follows that HSH T = 'I, where S = (xtxt z)  is the diagonal covariance matrix of the sources, and ' = 1 - . This, for a square  invertible H, implies that HTH is diagonal. In fact, since the unobserved sources  can be determined only to within a scaling factor, we can set the variance of each  source to unity and obtain the othogonality property HH - 'I. It can be shown  that the source posterior now factorizes into a product over the individual sources,  -- p(SO:T,X]: T  p(S0:T, XX: I YX:) Hi i  ixi  P(SO:T, I:T lYl:T) (x  Yl:T), where   vtp(s t I s_1)  i i  VoP(So) . (5)  The means and variances at time t in (5), as well as the quantities v, depend on  both the data yt and the states s; in particular,  = ('j Hjiyt j + Aus)/(A'vs + A)  and a = Av/(A'ys + A), using s = s; the expression for the v are omitted. The  transition probabilities are the same as in (2). Hence, the posterior distribution  (5) effectively defines a new HMM for each source, with yt-dependent emission and  transition probabilities.  To derive the learning rule for H, we should first compute the conditional mean it  of the source signals at time t given the data. This can be done recursively using  (5) as in the forward-backward procedure. We then obtain  1   H = v/7C(CZC)-/2 , C =  Eytit t . (6)  390 H. Attias  This fractional form results from imposing the orthogonality constraint HTH = 'I  using Lagrange multipliers and can be computed via a diagonalization procedure.  The source parameters are computed using a learning rule (omitted) similar to the  noise-free rule (4). It is easy to derive a learning rule for the noise level  as well; in  fact, the ordinary FA rule would suffice. We point out that, while this algorithm has  been derived for the case L = L ', it is perfectly well defined (though sub-optimah  see below) for  _< L .  4 Non-Isotropic Noise  The general case of non-isotropic noise and non-square mixing is computationally  intractable. This is because the exact E-step requires summing over all possible  source configurations (st s  ) at all times tl. t; -- l, ..., T. The intractability  '"', tL ' ",  problem stems from the fact that, while the sources are independent, the sources  conditioned on a data vector Y:T are correlated, resulting in a large number of  hidden configurations. This problem does not arise in the noise-free case, and can  be avoided in the case of isotropic noise and square mixing using the orthogonality  property; in both cases, the exact posterior over the sources factorizes.  The EM algorithm derived below is based on a variational approach. This approach  was introduced in [9] in the context of sigmoid belief networks, but constitutes a  general framework for ML learning in intractable probabilistic networks; it was  used in a HMM context in [10]. The idea is to use an approximate but tractable  posterior to place a lower bound on the likelihood, and optimize the parameters by  maximizing this bound.  A starting point for deriving a bound on the likelihood/2 is Neal and Hinton's Ill]  formulation of the EM algorithm:  T L  : 1ogp(y1:T)_> E Eqlogp(yt Ixt)+ E Eqlogp(s:T,X:T) -- Eqlogq , (7)  t=l i=1  where Eq denotes averaging with respect to an arbitrary posterior density over the  hidden variables given the observed data, q = q(so:r, Xl:r I yl:r). Exact EM,  as shown in [11], is obtained by maximizing the bound (7) with respect to both  the posterior q (corresponding to the E-step) and the model parameters W (M-  step). However, the resulting q is the true but intractable posterior. In contrast, in  variational EM we choose a q that differs from the true posterior, but facilitates a  tractable E-step.  E-Step. We use q(sb:r,x:r [ Y:T) = I-li i  q(so:r [ y:r) Htq(x  parametrized as  q(s sl i i i i i  = st- = y:r)  q(xt l Y:r) = 6(xt -pt,E:t). (8)  Thus, the variational transition probabilities in (8) are described by multiplying the  i by the parameters i subject to the normalization constraints.  original ones  /s,t'  The source signals x at time t are jointly Gaussian with mean Pt and covariance  It. The means, covariances and transition probabilities are all time- and data-  dependent, i.e., Pt = f(y:T, t) etc. This parametrization scheme is motivated by    i there become  the form of the posterior in (5); notice that the quantities r/, a, vs, t  the variational parameters p, E?, A}, t of (8). A related scheme was used in [10] in  a different context. Since these parameters will be adapted independently of the  model parameters, the non-isotropic algorithm is expected to give superior results  compared to the isotropic one.  Dynamic Independent Factor Analysis 391  o  -lO  . -3c  -40  -50  Mixing Reconstruction  (  w --10  --15  --20  5 10 15 --5 0 5 10 15  SNR (dB) SNR (dB)  Right: quality of the source  Figure 2: Left: quality of the model parameter estimates.  reconstructions. (See text).  Of course, in the true posterior the xt are correlated, both temporally among them-  selves and with st, and the latter do not factorize. To best approximate it, the  variational parameters V = (p, E?, ,k/s,t) are optimized to maximize the bound on  /2, or equivalently to minimize the KL distance between q and the true posterior.  This requirement leads to the fixed point equations  Pt = ( HTA-1H + Bt)-i(HTA-lyt + bt) , ]t = (HTA -1H + Bt) -1 ,  ';,t = 1 exp[-logv -(p-u})2+Ei]  ' (9)  where B? i ' '  -- Es[t($)/l]ij, b Es i i li  fit (s)/z, the z ensure nor-  = / 8, and factors  malization. The HMM quantities '7(s) are computed by the forward-backward  procedure using the variational transition probabilities (8). The variational param-  eters are determined by solving eqs. (9) iteratively for each block Yl.T; in practice,  we found that less then 20 iterations are usually required for convergence.  M-Step. The update rules for W are given for the mixing parameters by  -1  and for the source parameters by  where the (s', s) are computed using the variational transition probabilities (8).  Notice that the learning rules for the source parameters have the Baum-Welch form,  in spite of the correlations between the conditioned sources. In our variational  approach, these correlations are hidden in V, as manifested by the fact that the  fixed point equations (9) couple the parameters V across time points (since  i  depends on ,ks,t=:T) and sources.  Source Reconstruction. From q(xt I Yl:r) (8), we observe that the MAP source  estimate is given by it = Pt(Yl:T), and depends on both W and V.  Results. The above algorithm is demonstrated on a source separation task in Fig-  ure 2. We used 6 speech signals, transformed by non-linearities to have arbitrary  one-point densities, and mixed by a random 8 x 6 matrix H0. Different signal-  to-noise (SNR) levels were used. The error in the estimated H (left, solid line) is  T I T  quantified by the size of the non-diagonal elements of (H H)- H Ho relative to the  H = ytpT ptPt  + Et , A = 7 YtYt - YtPt H ), (10)  t  392 H. Attias  diagonal; the results obtained by IFA [1], which does not use temporal information,  are plotted for reference (dotted line). The mean squared error of the reconstructed  sources (right, solid line) and the corresponding IFA result (right, dashed line) are  also shown. The estimate and reconstruction errors of this algorithm are consis-  tently smaller than those of IFA, reflecting the advantage of exploiting the temporal  structure of the data. Additional experiments with different numbers of sources and  sensors gave similar results. Notice that this algorithm, unlike the previous two,  allows both  (  and   . We also considered situations where the number of  sensors was smaller than the number of sources; the separation quality was good,  although, as expected, less so than in the opposite case.  5 Conclusion  An important issue that has not been addressed here is model selection. When ap-  plying our algorithms to an arbitrary dataset, the number of factors and of HMM  states for each factor should be determined. Whereas this could be done, in princi-  ple, using cross-validation, the required computational effort would be fairly large.  However, in a recent paper [12] we develop a new framework for Bayesian model  selection, as well as model averaging, in probabilistic networks. This framework,  termed Variational Bayes, proposes an EM-like algorithm which approximates full  posterior distributions over not only hidden variables but also parameters and model  structure, as well as predictive quantities, in an analytical manner. It is currently  being applied to the algorithms presented here with good preliminary results.  One field in which our approach may find important applications is speech technol-  ogy, where it suggests building more economical signal models based on combining  independent low-dimensional HMMs, rather than fitting a single complex HMM.  It may also contribute toward improving recognition performance in noisy, multi-  speaker, reverberant conditions which characterize real-world auditory scenes.  References  [1] Attias, H. (1999). Independent factor analysis. Neut. Comp. 11, 803-851.  [2] Bell, A.J. & Sejnowski, T.J. (1995). An information-maximization approach to blind  separation and blind aleconvolution. Neut. Comp. 7, 1129-1159.  [3] Amari, S., Cichocki, A. & Yang, H.H. (1996). A new learning algorithm for blind signal  separation. Adv. Neut. Info. ?roc. Sys. 8, 757-763 (Ed. by Touretzky, D.S. et al). MIT  Press, Cambridge, MA.  [4] Pearlmutter, B.A. & Parra, L.C. (1997). Maximum likelihood blind source separation:  A context-sensitive generalization of ICA. Adv. Neut. Info. Proc. Sys. 9, 613-619 (Ed.  by Mozer, M.C. et al). MIT Press, Cambridge, MA.  [5] Hyv'inen, A. & Oja, E. (1997). A fast fixed-point algorithm for independent compo-  nent analysis. Neut. Comp. 9, 1483-1492.  [6] Attias, H. & Schreiner, C.E. (1998). Blind source separation and aleconvolution: the  dynamic component analysis algorithm. Neut. Comp. 10, 1373-1424.  [7] Rabiner, L. & Juang, B.-H. (1993). Fundamentals of Speech Recognition. Prentice Hall,  Englewood Cliffs, NJ.  [8] Lee, D.D. & Sompolinsky, H. (1999), unpublished; D.D. Lee, personal communication.  [9] Saul, L.K., Jaakkola, T., and Jordan, M.I. (1996). Mean field theory of sigmoid belief  networks. J. Art. Int. Res. 4, 61-76.  [10] Ghahramani, Z. & Jordan, M.I. (1997). Factorial hidden Markov models. Mach.  Learn. 29, 245-273.  [11] Neal, R.M. & Hinton, G.E. (1998). A view of the EM algorithm that justifies incre-  mental, sparse, and other variants. Learning in Graphical Models, 355-368 (Ed. by Jordan,  M.I.). Kluwer Academic Press.  [12] Attias, H. (2000). A variational Bayesian framework for graphical models. Adv. Neut.  Info. Proc. Sys. 12 (Ed. by Leen, T. et al). MIT Press, Cambridge, MA.  
Some Theoretical Results Concerning the  Convergence of Compositions of Regularized  Linear Functions  Tong Zhang  Mathematical Sciences Department  IBM T.J. Watson Research Center  Yorktown Heights, NY 10598  tzhang@watson.ibm.com  Abstract  Recently, sample complexity bounds have been derived for problems in-  volving linear functions such as neural networks and support vector ma-  chines. In this paper, we extend some theoretical results in this area by  deriving dimensional independent covering number bounds for regular-  ized linear functions under certain regularization conditions. We show  that such bounds lead to a class of new methods for training linear clas-  sifiers with similar theoretical advantages of the support vector machine.  Furthermore, we also present a theoretical analysis for these new meth-  ods from the asymptotic statistical point of view. This technique provides  better description for large sample behaviors of these algorithms.  1 Introduction  In this paper, we are interested in the generalization performance of linear classifiers ob-  tained from certain algorithms. From computational learning theory point of view, such  performance measurements, or sample complexity bounds, can be described by a quanti-  ty called covering number [ 11, 15, 17], which measures the size of a parametric function  family. For two-class classification problem, the covering number can be bounded by a  combinatorial quantity called VC-dimension [12, 17]. Following this work, researchers  have found other combinatorial quantities (dimensions) useful for bounding the covering  numbers. Consequently, the concept of VC-dimension has been generalized to deal with  more general problems, for example in [ 15, 11 ].  Recently, Vapnik introduced the concept of support vector machine [ 16] which has been  successful applied to many real problems. This method achieves good generalization by  restricting the 2-norm of the weights of a separating hyperplane. A similar technique has  been investigated by Bartlett [3], where the author studied the performance of neural net-  works when the 1-norm of the weights is bounded. The same idea has also been applied  in [ 13] to explain the effectiveness of the boosting algorithm. In this paper, we will extend  their results and emphasize the importance of dimension independence. Specifically, we  consider the following form of regularization method (with an emphasis on classification  problems) which has been widely studied for regression problems both in statistics and in  Convergence of Regularized Linear Functions 3 71  numerical mathematics:  inf Ex,yL(w, z, y) = inf Ex,y f(w 'zy) + ,g(w), (1)  where E,y is the expectation over a distribution of (z, y), and y E {-1, 1} is the binary  label of data vector z. To apply this formulation for the purpose of training linear classifiers,  we can choose f as a decreasing function, such that f(-) _> 0, and choose g(w) > 0 as  a function that penalizes large w (lirn g(w) -+ oo). X is an appropriately chosen  positive parameter to balance the two terms.  The paper is organized as follows. In Section 2, we briefly review the concept of covering  numbers as well as the main results related to analyzing the performance of learning algo-  rithms. In Section 3, we introduce the regularization idea. Our main goal is to construct  regularization conditions so that dimension independent bounds on covering numbers can  be obtained. Section 4 extends results from the previous section to nonlinear composition-  s of linear functions. In Section 5, we give an asymptotic formula for the generalization  performance of a learning algorithm, which will then be used to analyze an instance of  SVM. Due to the space limitation, we will only present the main results and discuss their  implications. The detailed derivations can be found in [ 18].  2 Covering numbers  We formulate the learning problem as to find a parameter from random observations to  minimize risk: given a loss function L(a, z) and n observations X = {zx,... , z,}  independently drawn from a fixed but unknown distribution D, we want to find a that  minimizes the expected loss over z (risk):  R(): E L(, )= f L(, ) dP(z). (2)  The most natural method for solving (2) using a limited number of observations is by the  empirical risk minimization (ERM) method (cf [15, 16]). We simply choose a parameter  a that minimizes the observed risk:  (3)  ) =  i=1  We denote the parameter obtained in this way as Oerm(X). The convergence behavior  of this method can be analyzed by using the VC theoretical point of view, which relies  on the uniform convergence of the empirical risk (the uniform law of large numbers):  sups IR(a, X) - R(a)[. Such a bound can be obtained from quantities that measure  the size of a Glivenko-Cantelli class. For finite number of indices, the family size can be  measured simply by its cardinality. For general function families, a well known quantity to  measure the degree of uniform convergence is the covering number which can be be dated  back to Kolmogrov [8, 9]. The idea is to discretize (which can depend on the data X) the  parameter space into N values a,... , aN so that each L(a, .) can be approximated by  L(ai, .) for some i. We shall only describe a simplified version relevant for our purposes.  Definition 2.1 Let B be a metric space with metric p. Given a norm p, observations X -  [z, . . . , z,], and vectors f(a, X ) = [f(a, z ), . . . , f(a, z,)] E B ' parameterized by  a, the covering number in p-norm, denoted as A/'p( f, e, X ), is the minimum number of a  collection of vectors vt, . . . , v, E B ' such that Va, 3vi' [Ip(f(.,x?),  We also denote Alp(f, e, n): maxx Alp (f, e, Xp).  Note that from the definition and the Jensen's inequality, we have ./V'p _< ./V'q for p _< q. We  will always assume the metric on R to be [zx - z21 if not explicitly specified otherwise.  The following theorem is due to Pollard [11]'  372 T. Zhang  Theorem 2.1 ([lid n, e > 0 and distribution D.  _he 2  P[supIR(a,X)- R(c)[ > e] <_ 8E[A/(L,e/S,X)]exp(128M2),  where M = sup,,x L(a, z) - inf,.x L(a, ), and X[* = (, . . . , ,) are independently  drawn from D.  The constants in the above theorem can be improved for certain problems: see [4, 6, 15, ! 6]  for related results. However, they yield very similar bounds. The result most relevant for  this paper is a lemma in [3] where the 1-norm covering number is replaced by the cx>-norm  covering number. The latter can be bounded by a scale-sensitive combinatorial dimension  [ 1 ], which can be bounded from the 1-norm covering number if this covering number does  not depend on n. These results can replace Theorem 2.1 to yield better estimates under  certain circumstances.  Since Bartlett's lemma in [3] is only for binary loss functions, we shall give a generalization  so that it is comparable to Theorem 2.1'  Theorem 2.2 Let fx and f2 be two functions.' R '  [0, 1] such that lyx - y2l _< implies  f (y) <_ fa(y2 ) _< f2(Y ) where rs' R  -+ [0, 1] is a reference separating function, then  --he 2  P[sup[Efx(L(a,z)) - Exl f2(L(a,z))] > e] _< 4E[Afoo(L,'x,X)]exp().  Note that in the extreme case that some choice of a achieves perfect generalization:  Ef2(L(a, z)) - 0, and assume that our choices of a(X) always satisfy the condition  Ex? f2 (L(a, z)) = 0, then better bounds can be obtained by using a refined version of the  Chernoffbound.  3 Covering number bounds for linear systems  In this section, we present a few new bounds on covering numbers for the following form  of real valued loss functions:  d  L(w, z): z T w - E ziwi. (4)  i:1  As we shall see later, these bounds are relevant to the convergence properties of (1). Note  that in order to apply Theorem 2.1, since . < A/'2, therefore it is sufficient to estimate  A/'2(L, e, n) for e > 0. It is clear that A/'2(L, e, n- is not finite if no restrictions on z and w  are imposed. Therefore in the following, we will assume that each is bounded, and  study conditions of llwllq so that log A/'( f, e, n) is independent or weakly dependent of d.  Our first result generalizes a theorem of Bartlett [3]. The original results is with p -- oo  and q = 1, and the related technique has also appeared in [10, 13]. The proof uses a lemma  that is attributed to Maurey (cf. [2, 7]).  Theorem 3.1 r/llllp 5 b andllvollq a, where I/p+ 1/q = 1 and2 _< p _< oo, then  a 2 b 2  The above bound on the covering number depends logarithmically on d, which is already  quite weak (as compared to linear dependency on d in the standard situation). However, the  bound in Theorem 3.1 is not tight for p < oo. For example, the following theorem improves  the above bound for p = 2. Our technique of proof relies on the SVD decomposition [5]  for matrices, which improves a similar result in [14] by a logarithmic factor.  Convergence of Regularized Linear Functions $ 73  Theorem 3.2 Ifllill b and IIll a, the,  log..M.(L, e, n)< [ 2a'b' ] log.(4a'b'/e+ 1).  The next theorem shows that if 1/p + 1/q > 1, then the 2-norm covering number is also  ndependent of dimension.  Theorem 3.3 Let (w,z) = z'w. f'llillp 5 bandllwllq 5 a, where 1 <_ q _< 2 and  5 = l/p + l/q- l > O. then  .4a2b 2  log2N'2(,e,n)_< | -2 1g2(2(2ab/e) /+ 1).  One consequence of this theorem is a potentially refined explanation tbr the boosting al-  gorithm. In [ 13], the boosting algorithm has been analyzed by using a technique related to  results in [3] which essentially rely on Theorem 3.1 with p -- oo. Unfommately, the bound  contains a logarithmic dependency on d (in the most general case) which does not seem to  fully explain the fact that in many cases the performance of the boosting algorithm keeps  improving as d increases. However, this seemingly mysterious behavior might be better  understood from Theorem 3.3 under the assumption that the data is more restricted than  simply being cr)-norm bounded. For example, when the contribution of the wrong predic-  tions is bounded by a constant (or grow very slowly as d increases), then we can regard  its p-th norm bounded for some p < oo. In this case, Theorem 3.3 implies dimensional  independent generalization.  If we want to apply Theorem 2.2, then it is necessary to obtain bounds fbr infinity-norm  covering numbers. The following theorem gives such bounds by using a result from online  learning.  Theorem 3.4 I/Ill, lip 5 b and ll11q 5 a, where 2 <_ p < oo and 1/p + 1/q : 1, then  Ve> O,  a2b 2  log2,&(L, e, n ) _< 36(p- 1)--T- log21214ab/e + 2]n + 1].  In the case ofp = oo, an entropy condition can be used to obtain dimensional independent  covering number bounds.  Definition 3.1 Let/ = [/i] be a vector with positive entries such that IIll: i an this  case, we call i  a distribution vector). Let z = [zi]  0 be a vector of the same length, then  we define the weighted relative entropy of z with respect to i  as.'  i  Theorem 3.5 Given a distribution vector i, If 1111 5 b and IIl[  entr%(w) _< c, where we assume that w has non-negative entries, then  > O,  36b2(a 2 q- ac) log21214ab/e q- 2In q- 1].  log2 -/foo (L, e, n) _< e2  < a and  Theorems in this section can be combined with Theorem 4.1 to form more complex cover-  ing number bounds for nonlinear compositions of linear functions.  374 T. Zhang  4 Nonlinear extensions  Consider the following system:  L([a, w], :*) = f(g(a, :*)+ w'h(a, :*)), (5)  where :* is the observation, and [a, w] is the parameter. We assume that f is a nonlinear  function with bounded total variation.  Definition 4.1 A function f: R -+ R is said to satisJ the Lipschitz condition with param-  eter 9' ifV:*, y: If(:*) - f(v)l <_ 3'1:* - Yl.  Definition 4.2 The total variation of a function f: R -+ R is defined as  TV(f, :*) -- sup  If(:*i) - f(:*-)l.  We also denote TV(f, oo) as TV(f).  Theorem 4.1 IfL([ct, w],:*) = f(g(a,:*) + wTh(a,:*)), where TV(f) < c and f is  Lipschitz with parameter % Assume also that w is a d-dimensional vector and Ilwllq _ c,  then Vq, e2 > O, and n > 2(d + 1):  en TV(f)j,1)]+logA([g,h],e/v,n),  + (a+  where the metric of[g, h] is defined as [gx - g21 + cll& - n2ll n/p + 1/q = 1).  Example 4.1 Consider classification by hyperplane: L(w, :*) = I(wT:* < 0) where I is  the set indicator function. Let L'(w, :*) - fo(wT:*) be another loss function where  1 z<i0  fo(z) : 1- z z C ,1].  0 z>  Instead of using ERM for estimating parameter that minimizes the risk of L, consider the  scheme of minimize empirical risk associated with L ', under the assumption that [I :.11 < b  and constraint that Ilwl12 _< a. Denote the estimated parameter by w. It follows from the  covering number bounds and Theorem 2.1 that with probability of at least 1 -  n/2ab In(nab + 2) + In  EzI(wnT :* < 0) < inf Ezfo(w T :*) + O( v ).  If we apply a slight generalization of Theorem 2.2 and the covering number bound of  Theorem 3.4, then with probability of at least 1 - r:  /1 a2 b 2  EI(w:* _< O)_< Exi, I(w:* _< 27) + O(V(--5--ln(a*/7 + 2) +lnn + In ))  for all 7 C (0, 1]. []  Bounds given in this paper can be applied to show that under appropriate regularization  conditions and assumptions on the data, methods based on (1) lead to generalization per-  formances of the form (1/x/), where ( symbol (which is independent of d) is used to  indicate that the hidden constant may include a polynomial dependency on log(n). It is  also important to note that in certain cases, , will not appear (or it has a small influence on  the convergence) in the constant of (, as being demonstrated by the example in the next  section.  Convergence of Regularized Linear Functions 375  5 Asymptotic analysis  The convergence results in the previous sections are in the form of VC style convergence  in probability, which has a combinatorial flavor. However, for problems with differen-  tiable function families involving vector parametem, it is often convenient to derive precise  asymptotic results using the differential structure.  Assume that the parameter a E R " in (2) is a vector and L is a smooth function. Let  a* denote the optimal parameter; V denote the derivative with respect to a; and 9(ct, z)  denote VL(a, ). Assume that  U = f t(*, )t(*, ) T  Then under certain regularity conditions, the asymptotic expected generalization error is  given by  E J(erm)= /(O?)-3' 2--tr(v-lu). (6)  More generally, for any evaluation function h(a) such that Vh(a*): 0:  E h(erm) + 2tr(V-V2' (7)  where Veh is the Hessian matrix of h at a*. Note that this approach assumes that the op-  timal solution is unique. These results are exact asymptotically and provide better bounds  than those from the standard PAC analysis.  Example 5.1 We would like to study a form of the support vector machine: Consider  (c, z) -- f(cTz) + ,kc 2,  {10-z z<l  f(z) : -  z>l  Because of the discontinuity in the derivative of f, the asymptotic formula may not hold.  However, if we make an assumption on the smoothness of the distribution , then the  expectation of the derivative over e can still be smooth. In this case, the smoothness of  f itself is not crucial. Furthermore, in a separate report, we shall illustrate that similar  small sample bounds without any assumption on the smoothness of the distribution can be  obtained by using techniques related to asymptotic analysis.  Consider the optimal parameter a* and let S = {z  a*Tz < 1}. Note that Xa* =  and U = Ees(z - Esz)(z - Esz) T. Assume that 7 > 0 s.t. P(a*Tz _< 7): O,  then V: )I + B where B is a positive semi-definite matrix. It follows that  Ez6s 2  tr(V-S) <_ r(S)/X _< II*ll _< sup Ilzll11*11/%  Now, consider a, obtained from observations X = [ ,..., ,] by minimizing empirical  risk associated with loss function (a, ), then  1  E=L(ae,p, z) _< inaf E=L(a, z) + n sup  asymptotically. Let  --> 0, this scheme becomes the optimal separating hyperplane [ 16].  This asymptotic bound is better than typical PAC bounds with fixed . []  Note that although the bound obtained in the above example is very similar to the mistake  bound for the perceptron online update algorithm, we may in practice obtain much better  estimates from (6) by plugging in the empirical data.  376 T. Zhang  References  Ill  [2]  [3]  [4]  [5]  [6]  [7]  [8]  [9]  [10]  [11]  [12]  [13]  [14]  [15]  [16]  [17]  [18]  N. Alon, S. Ben-David, N. Cesa-Bianchi, and D. Haussler. Scale-sensitive dimen-  sions, uniform convergence, and learnability. Journal of the ACM, 44(4):615-631,  1997.  A.R. Barron. Universal approximation bounds tbr superpositions ofa sigmoidal func-  tion. IEEE Transactions on InJbrmation Theory, 39(3):930-945, 1993.  P.L. Bartlett. The sample complexity of pattern classification with neural networks:  the size of the weights is more important than the size of the network. IEEE Transac-  tions on Information Theory, 44(2):525-536, 1998.  R.M. Dudley. A course on empirical processes, volume 1097 of Lecture Notes in  Mathematics. 1984.  G.H. Golub and C.F. Van Loan. Matrix computations. Johns Hopkins University  Press, Baltimore, MD, third edition, 1996.  D. Haussler. Generalizing the PAC model: sample size bounds from metric  dimension-based uniform convergence results. In Proc. 30th IEEE Symposium on  Foundations of Computer Science, pages 40-45, 1989.  Lee K. Jones. A simple lemma on greedy approximation in Hilbert space and con-  vergence rates for projection pursuit regression and neural network training. Ann.  Statist., 20(1):608--613, 1992.  A.N. Kolmogorov. Asymptotic characteristics of some completely bounded metric  spaces. Dokl. Akad. Nauk. SSSR, 108:585-589, 1956.  A.N. Kolmogorov and V.M. Tihomirov. e-entropy and e-capacity of sets in functional  spaces. Amer. Math. Soc. Transl., 17(2):277-364, 1961.  Wee Sun Lee, P.L. Bartlett, and R.C. Williamson. Efficient agnostic learning of  neural networks with bounded fan-in. IEEE Transactions on In, formation Theory,,  42(6):2118-2132, 1996.  D. Pollard. Convergence of stochastic processes. Springer-Verlag, New York, 1984.  N. Sauer. On the density of families of sets. Journal of Combinatorial Theory (Series  ,4), 13:145-147, 1972.  Robert E. Schapire, Yoav Freun& Peter Bartlett, and Wee Sun Lee. Boosting the  margin: a new explanation for the effectiveness of voting methods. Ann. Statist.,  26(5): 1651-1686, 1998.  J. Shawe-Taylor, P.L. Bartlett, R.C. Williamson, and M. Anthony. Structural risk  minimization over data-dependent hierarchies. IEEE Trans. Inf. Theory, 44(5): 1926-  1940, 1998.  V.N. Vapnik. Estimation of dependences based on empirical data. Springer-Verlag,  New York, 1982. Translated from the Russian by Samuel Kotz.  V.N. Vapnik. The nature of statistical learning theory. Springer-Verlag, New York,  1995.  V.N. Vapnik and A.J. Chervonenkis. On the uniform convergence of relative fre-  quencies of events to their probabilities. Theory of Probability and Applications,  16:264-280, 1971.  Tong Zhang. Analysis of regularized linear functions for classification problems.  Technical Report RC-21572, IBM, 1999.  PART IV  ALGORITHMS AND ARCHITECTURE  
Application of Blind Separation of Sources to  Optical Recording of Brain Activity  Holger Schiiner, Martin Stetter, Ingo SchieBl  Department of Computer Science  Technical University of Berlin Germany  { hfsch, moatl, ingos } @ cs. tu-berlin. de  John E.W. Mayhew  University of Sheffield, UK  j.e. mayhew @ sheffield. ac. uk  Jennifer S. Lund, Niall McLoughlin  Institute of Ophthalmology  University College London, UK  {j. lund, n. mcloughlin} @ucl. ac. uk  Klaus Obermayer  Department of Computer Science,  Technical University of Berlin, Germany  oby@ cs. tu-berlin. de  Abstract  In the analysis of data recorded by optical imaging from intrinsic signals  (measurement of changes of light reflectance from cortical tissue) the re-  moval of noise and artifacts such as blood vessel patterns is a serious  problem. Often bandpass filtering is used, but the underlying assumption  that a spatial frequency exists, which separates the mapping component  from other components (especially the global signal), is questionable.  Here we propose alternative ways of processing optical imaging data, us-  ing blind source separation techniques based on the spatial decorrelation  of the data. We first perform benchmarks on artificial data in order to  select the way of processing, which is most robust with respect to sen-  sor noise. We then apply it to recordings of optical imaging experiments  from macaque primary visual cortex. We show that our BSS technique is  able to extract ocular dominance and orientation preference maps from  single condition stacks, for data, where standard post-processing pro-  cedures fail. Artifacts, especially blood vessel patterns, can often be  completely removed from the maps. In summary, our method for blind  source separation using extended spatial decorrelation is a superior tech-  nique for the analysis of optical recording data.  1 Introduction  One approach in the attempt of comprehending how the human brain works is the analysis  of neural activation patterns in the brain for different stimuli presented to a sensory system.  An example is the extraction of ocular dominance or orientation preference maps from  recordings of activity of neurons in the primary visual cortex of mammals. A common  technique for extracting such maps is optical imaging (OI) of intrinsic signals. Currently  this is the imaging technique with the highest spatial resolution ( 100 ktm) for mapping of  the cortex. This method is explained e.g. in [1], for similar methods using voltage sensitive  dyes see [2, 3]. OI uses changes in light reflection to estimate spatial patterns of stimulus  950 H. Sch6ner, M. Stetter, I. Schiefil, J. E. Mayhew, J. Lund, N. McLoughlin and K. Obermayer  answers. The overall change recorded by a CCD or video camera is the total signal. The  part of the total signal due to local neural activity is called the mapping component and it  derives from changes in deoxyhemoglobin absorption and light scattering properties of the  tissue. Another component of the total signal is a "global" component, which is also cor-  related with stimulus presentation, but has a much coarser spatial reolution. It derives its  part from changes in the blood volume with the time. Other components are blood vessel  artifacts, the vasomotor signal (slow oscillations of neural activity), and ongoing activity  (spontaneous, stimulus-uncorrelated activity). Problematic for the extraction of activity  maps are especially blood vessel artifacts and sensor noise, such as photon shot noise. A  procedure often used for extracting the activity maps from the recordings is bandpass fil-  tering, after preprocessing by temporal, spatial, and trial averaging. Lowpass filtering is  unproblematic, as the spatial resolution of the mapping signal is limited by the scattering  properties of the brain tissue, hence everything above a limiting frequency must be noise.  The motivation for highpass filtering, on the other hand, is questionable as there is no spe-  cific spatial frequency separating local neural activity patterns and the global signal [4]).  A different approach, Blind Source Separation (BSS), models the components of the  recorded image frames as independent sources, and the observations (recorded image  frames) as noisy linear mixtures of the unknown sources. After performing the BSS the  mapping component should ideally be concentrated in one estimated source, the global  signal in another, and blood vessel artifacts, etc. in further ones. Previous work ([5]) has  shown that BSS algorithms, which are based on higher order statistics ([6, 7, 8]), fail for  optical imaging data, because of the high signal to noise ratio.  In this work we suggest and investigate versions of the M&S algorithm [9, 10], which are  robust against sensor noise, and we analyze their performance on artificial as well as real  optical recording data. In stion 2 we describe an improved algorithm, which we later  compare to other methods in stion 3. There an artificial data set is used for the analysis  of noise robustness, and benchmark results are presented. Then, in section 4, it is shown  that the newly developed algorithm is very well able to separate the different components  of the optical imaging data, for ocular dominance as well as orientation preference data  from monkey striate cortex. Finally, section 5 provides conclusions and perspectives for  future work.  2 Second order blind source separation  Let rn be the number of mixtures and r the sample index, i.e. a vector specifying a pixel in  the recorded images. The observation vectors y(r) -- (Yt (r),..., yr(r))Y are assumed  to be linear mixtures of rn unknown sources s(r) -- (st (r),..., sr (r))'t' with A being the  rn x m mixing matrix and n describing the sensor noise:  y(r) = As(r) + n  (1)  The goal of BSS is to obtain optimal source estimates (r) under the assumption that the  original sources are independent. In the noiseless case W - A-t would be the optimal  demixing matrix. In the noisy case, however, W also has to compensate for the added  noise: (r) = Wy(r) -- W. A. s(r) q- W. n. BSS algorithms are generally only able to  recover the original sources up to a permutation and scaling.  Extended Spatial Decorrelation (ESD) uses the second order statistics of the observa-  tions to find the source estimates. If sources are statistical independent all source cross-  correlations  = zx)) r =  1  + zxr) , i 4 j (2)  Application of BSS to Optical Recording of Brain Activity 951  must vanish for all shifts At, while the autocorrelations (i = j) of the sources remain (the  variances). Note that this implies that the sources must be spatially smooth.  Motivated by [10] we propose to optimize the cost function, which is the sum of the  squared cross-correlations of the estimated sources over a set of shifts {At},  Ar ij    (i(r)j (r  r)),  r ij  with respt to the demixing matrix W. The matrix Ci,j(r)  (yi(r)yj(r  r)) de-  notes the mixture cross-cogelations for a shift r. This cost function is minimized using  the Polak Ribiere Conjugate Gradient technique, where the line sech is substituted by a  dynamic step width adaptation ([11]). To keep the demixing matrix W from converging to  the zero matrix, we introduce a constrent which keeps the diagonal elements of T  W- 1  (in the noiseless case and for non-sphered data T is an estimate of the mixing matrix, with  possible permutations) at a value of 1.0. Convergence properties e improved by sphering  the data (transforming their cogelation matrix for shift zero to an identity matrix) prior to  dogelating the mixtures.  Note that use of multiple shifts r allows to use more information about the auto- and  cross-cogelation structure of the mixtures for the sepation process. Two shifts provide  just enough constraints for a unique solution ([10]). Multiple shifts, and the rundancy  they introduce, additionally allow to cancel out pt of the noise by approximate simulta-  neous diagonalization of the cogesponding cross cogelation matrices.  In the presence of sensor noise, added after mixing, the standard sphering technique is  problematic. When calculating the zero-shift cross-correlation matrix the variance of the  noise contaminates the result, and sphering using a shifted cross-correlation matrix, is r-  oremended ([12]). or spatially white sensor noise and sources with reasonable auto cor-  relations this technique is more appropriate. In the following we denote the standard algo-  rithm by pa0, and the viant using noise robust sphering by dpa].  3 Benchmarks for artificial data  The artificial data set used here, whose sources are approximately uncorrelated for all  shifts, is shown in the left part of figure 1. The mixtures were produced by generating a  random mixing matrix (in this case with condition number 3.73), applying it to the sources,  and finally adding white noise of different variances.  In order to measure the performance on the artificial data set we measure a reconstruction  error (FIE) between the estimated and the correct sources via (see [13]):  FIE(W): od (y (r)sT (r)) ,  od(C) =  E N - 1 maxk  i j  The correlation between the real and the estimated sources (the argument to "od"), should  be close to a permutation matrix, if the separation is successful. If the maxima of two rows  are in the same column, the separation is labeled unsuccessful. Otherwise, the normalized  absolute sum of non-permutation (cross-correlation) elements is computed and returned as  the reconstruction error.  We now compare the method based on optimization of (3) by gradient descent with the fol-  lowing variants of second order blind source separation: (1) standard spatial decorrelation  952 H. Sch6ner, M. Stetter, L Schiefil, J. E. Mayhew, J. Lund, N. McLoughlin and K. Obermayer   0.5  -:.-.-.:.-.:- .....,.- 0.2  '*"' :'": "* '0.1  0  6 ' , I  5 10 15 25  Signal to Noise Ratio (dB)  0.5  o0.2  0.1  0  0 5 10 15 20 25  Signal to Noise Ratio (dB)  Figure 1: The set of three approximatdy uncorrelated source images of the artificial data  set (left). The two plots (middle, right) show the reconstruction error versus signal to noise  ratio for different separation algorithms. In the right plot jal and 0pal are very close  together.  using the optimal single shift yielding the smallest reconstruction error (opt). (2) Spatial  decorrelation using the shift selected by  norm (C(Ar) - diag (C(Ar))) (5)  Arco,. = argmax{a.} norm(diag (C(Ar))) "  where "diag" sets all off-diagonal elements of its argument matrix to zero, and "norm"  computes the largest singular value of its argument matrix (cot). Arco,. is the shift for  which the cross correlations are largest, i.e. whose signal to noise ratio (SNR) should be  best. (3) Standard spatial decorrelation using the average reconstruction error for all suc-  cessful shifts in a 61 x 61 square around the zero shift (mean). (4) A multi-shift algorithm  ([ 12]), using several elementary rotations (Jacobi method) to build an orthogonal demixing  matrix, which optimizes the cost function (3). The variants using standard sphering and  noise robust sphering are denoted by (jac0) and (jacl). cor, opt, and mean use two shifts  for their computation; but as one of those is always the zero-shift, there is only one shift to  choose and they are called single-shift algorithms here.  Figure 1 gives two plots which show the reconstruction error (4) versus the SNR (mea-  sured in dB) for single shift (middle) and multi-shift (right) algorithms. The error bars  indicate twice the standard error of the mean (2 x SEM), for 10 runs with the same mix-  ing matrix, but newly generated noise of the given noise level. In each of these runs, the  best result of three was selected for the gradient descent method. This is because, con-  trary to the other algorithms, the gradient descent algorithm depends on the initial choice  of the demixing matrix. All multi-shift algorithms (all except opt and mean), used 8 shifts  (+r, +r), (+r, 0), and (0, +r) for each r C { 1, 3, 5, 10, 20, 30}, so 48 all together.  Several points are noticeable in the plots. (i) The cot algorithm is generally closer to the  optimum than to the average successful shift. (ii) A comparison between the two plots  shows that the multi-shift algorithms (right plot) are able to perform much better than even  the optimal single-shift method. For low to medium noise levels this is even the case when  using the standard sphering method combined with the gradient descent algorithm. (iii)  The advantage of the noise robust sphering method, compared to the standard sphering,  is obvious: the reconstruction error stays very low for all evaluated noise levels, for both  the jacl and 0pal algorithms. (iv) The gradient descent technique is more robust than the  Jacobi method For the standard sphering its performance is much better than that of the  Jacobi method.  Figure 1 shows results which were produced using a single mixing matrix. However, our  simulations show that the algorithms compare qualitatively similar when using mixing ma-  Application of BSS to Optical Recording of Brain Activity 953  .!:: ..:5"' ' : :.'  :-: . .:',- : ,..,x?..''... '%.., ::?. ::.:.... .:' .'."':" ,...:.  '' ....' ''  .":'-'.':.,  :,.:..-. .... :- ,.: ,.. '. .. X ..............  :, . . .- ,;v -   ,,, ,-. .... ...,. :. ......   , :.,,,.,,, ,, .?. .......... -:-' .'  ::" "'" ' ' .... ' "'  .....  ........  ' :.C "" ..... :":"  ':'"'::  .... ,., ,;,  ,, ,,,  , . .,  ,-.,  .............    "  . ,   , . : ..... .  ..... :-...  . . , . .. , .,,,L  . ,  t lsec. tesec. t3sec. ts. tsec. t6sec.  .' .-.....,.%...  t = 7 sec.  Figure 2: Optical imaging stacks. The top stack is a single condition stack from ocular  dominance experiments, the lower one a difference stack from orientation preference ex-  periments (images for 90 o gratings subtracted from those for 0  gratings). The stimulus  was present during recording images 2-7 in each row. Two large blood vessels in the top  and left regions of the raw images were masked out prior to the analysis.  trices with condition numbers between 2 and 10. The noise robust versions of the multi-  shift algorithms generally yield the best separation results of all evaluated algorithms.  4 Application to optical imaging  We now apply extended spatial decorrelation to the analysis of optical imaging data. The  data consists O f recordings from the primary visual cortex of macaque monkeys. Each trial  lasted 8 seconds, which were recorded with frame rates of 15 frames per second. A visual  stimulus (a drifting bar grating of varying orientation) was presented between seconds 2  and 8. Trials were separated by a recovery period of 15 seconds without stimulation. The  cortex was illuminated at a wavelength of 633 nm. One pixel corresponds to about 15 ktm  on the cortex; the image stacks used for further processing, consisting of 256 x 256 pixels,  covered an area of cortex of approximately 3.7 mm e .  Blocks of 15 consecutive frames were averaged, and averaging over 8 trials using the same  visual stimulus further improved the SNR. First frame analysis (subtraction of the first,  blank, frame from the others) was then applied to the resulting stack of 8 frames, fol-  lowed by lowpass filtering with 14 cycles/mm. Figure 2 shows the resulting image stacks  for an ocular dominance and an orientation preference experiment. One observes strong  blood vessel artifacts (particularly in the top row of images), which are superimposed to  the patchy mapping component that pops up over time.  Figure 3 shows results obtained by the application of extended spatial decorrelation (using  dpa0). Only those estimated sources containing patterns different from white noise are  shown. Backprojection of the estimated sources onto the original image stack yields the  amplitude time series of the estimated sources, which is very useful in selecting the map-  ping component: it can be present in the recordings only after the stimulus onset (starting  at ! = 2 sec.). The middle part shows four estimated sources for the ocular dominance  single condition stack. The mapping component (first image) is separated from the global  component (second image) and blood vessel artifacts (second to fourth) quite well. The  time course of the mapping component is plausible as well: calculation of a plausibility  index (sum of squared differences between the normalized time series and a step function,  which is 0 before and 1 after the stimulus onset) gives 0.5 for the mapping component and  2.31 for the next best one. Results for the gradient descent algorithm are similar for this  data set, regardless of the sphering technique used. The Jacobi method also gives simi-  lar results, but a small blood vessel artifact is remaining in the resulting map. The  algorithm usually gives much worse separation results. In the right part of figure 3 two es-  954 H. SchOner, M. Stetter, I. Schiefll, J. E. Mayhew, J. Lund, N. McLoughlin and K. Obermayer  2 4 6  5  -5  2 4 6  4  2 4 6 2 4 6  Figure 3: Left: Summation technique for ocular dominance (OD) experiment (upper) and  orientation preference (OP) experiment (lower). Middle, Right: dpa0 algorithm applied  to the same OD single condition (middle) and OP (right) stacks. The images show the 4  (OD) and 2 (OP) estimated components, which are visually different from white noise. In  the bottom row the respective time courses of the estimated sources are given.  timated sources (those different from white noise) for the orientation preference difference  stack can be seen. Here the proposed algorithm (dpa0) again works very well (plausibility  index is 0.56 for mapping component, compared to 3.04 for the best other component). It  generally has to be applied a few times (usually around 3 times) to select the best separa-  tion result (judging by visual quality of the separation and the time courses of the estimated  sources), because of its dependence on parameter initialization; in return it yields the best  results of all algorithms used, especially when compared to the traditional summation tech-  nique.  The similar results when using standard and noise robust sphering, and the small differ-  ences between the gradient descent and the Jacobi algorithms indicate, that not sensor noise  is the limiting factor for the quality of the extracted maps. Instead it seems that, assuming  a linear mixing model, no better results can be obtained from the used image stacks. It  will remain for further research to analyze, how appropriate the linear mixing model is,  and whether the underlying biophysical components are sufficiently uncorrelated. In the  meantime the maps obtained by the ESD algorithm are superior to those obtained using  conventional techniques like summation of the image stack.  5 Conclusion  The results presented in the previous sections show the advantages of the proposed algo-  rithm: In the comparison with other spatial decorrelation algorithms the benefit in using  multiple shifts compared to only two shifts is demonstrated. The robustness against sen-  sor noise is improved, and in addition, the selection of multiple shifts is less critical than  selecting a single shift, as the resulting multi-shift system of equations contains more re-  dundancy. In comparison with the Jacobi method, which is restricted to find only orthog-  onal demixing matrices, the greater tolerance of demixing by a gradient descent technique  concerning noise and incorrect sphering are demonstrated. The application of second order  blind separation of sources to optical imaging data shows that these techniques represent  an important alternative to the conventional approach, bandpass filtering followed by sum-  mation of the image stack, for extraction of neural activity maps. Vessel artifacts can be  separated from the mapping component better than using classical approaches. The spatial  decorrelation algorithms are very well adapted to the optical imaging task, because of their  use of spatial smoothness properties of the mapping and other biophysical components.  An important field for future research concerning BSS algorithms is the incorporation of  prior knowledge about sources and the mixing process, e.g. that the mixing has to be  causal: the mapping signal cannot occur before the stimulus is presented. Assumptions  Application of BSS to Optical Recording of Brain Activity 955  about the time course of signals could also be helpful, as well as knowledge about their  spatial statistics. Smearing and scattering limit the resolution of recordings of biological  components, and, depending on the wavelength of the light used for illumination, the map-  ping component constitutes only a certain percentage of the changes in total light reflec-  tions.  Acknowledgments  This work has been supported by the Wellcome Trust (050080/Z197).  References  [10]  [11]  [12]  [13]  [1 ] T. Bonhoeffer and A. Grinvald. Optical imaging based on intrinsic signals: The methodology.  In A. Toga and J. C. Maziotta, editors, Brain mapping: The methods, pages 55-97, San Diego,  CA, 1996. Academic Press, Inc.  [2] G. G. Blasdel and G. Salama. Voltage-sensitive dyes reveal a modular organization in monkey  striate cortex. Nature, 321:579-585, 1986.  [3] G. G. Blasdel. Differential imaging of ocular dominance and orientation selectivity in monkey  striate cortex. J. Neurosci., 12:3115-3138, 1992.  [4] M. Stetter, T. Otto, T. Mueller, E Sengpiel, M. Huebener, T. Bonhoeffer, and K. Obermayer.  Temporal and spatial analysis of intrinsic signals from cat visual cortex. Soc. Neurosci. Abstr.,  23:455, 1997.  [5] I. SchieB1, M. Stetter, J. E. W. Mayhew, S. Askew, N. McLoughlin, J. B. Levitt, J. S. Lurid, and  K. Obermayer. Blind separation of spatial signal patterns from optical imaging records. In J.-F.  Cardoso, C. Jutten, and P. Loubaton, editors, Proceedings of the ICA99 workshop, volume 1,  pages 179-184, 1999.  [6] A.J. Bell and T. J. Sejnowski. An information-maximization approach to blind separation and  blind deconvolution. Neural Computation, 7:1129-1159, 1995.  [7] S. Amari. Neural learning in structured parameter spaces - natural riemannian gradient. In  M. C. Mozer, M. I. Jordan, and T. Petsche, editors, Advances in Neural Information Processing  Systems, volume 9, 1996.  [8] A. Hyvarinen and E. Oja. A fast fixed point algorithm for independent component analysis.  Neural Cornput., 9:1483-1492, 1997.  [9] J. C. Platt and F. Faggin. Networks for the separation of sources that are superimposed and  delayed. In J. E. Moody, S. J. Hanson, and R. P. Lippmann, editors, Advances in Neural Infor-  mation Processing Systems, volume 4, pages 730-737, 1991.  L. Molgedey and H. G. Schuster. Separation of a mixture of independent signals using time  delayed correlations. Phys. Rev. Lett., 72:3634-3637, 1994.  S. M. Rtiger. Stable dynamic parameter adaptation. In D. S. Touretzky, M. C. Mozer, and M. E.  Hasselmo, editors, Advances in Neural Information Processing Systems., volume 8, pages 225-  231. MIT Press Cambridge, MA, 1996.  K.-R. Mtiller, Philips P, and A. Ziehe. Jadetd: Combining higher-order statistics and temporal  information for Blind Source Separation (with noise). In J.-E Cardoso, C. Jutten, and P. Louba-  ton, editors, Proceedings of the h ICA99 Workshop, Aussois, volume 1, pages 87-92, 1999.  B.-U. Koehler and R. Orglmeister. Independent component analysis using autoregressive mod-  els. In J.-E Cardoso, C. Jutten, and P. Loubaton, editors, Proceedings of the ICA99 workshop,  volume 1, pages 359-363, 1999.  
A SNoW-Based Face Detector  Ming-Hsuan Yang Dan Roth Narendra Ahuja  Department of Computer Science and the Beckman Institute  University of Illinois at Urbana-Champaign  Urbana, IL 61801  mhyangvision. ai.uiu. edu danrcs.uiu. edu ahuj avision. ai.uiu. edu  Abstract  A novel learning approach for human face detection using a network  of linear units is presented. The SNoW learning architecture is a  sparse network of linear functions over a pre-defined or incremen-  tally learned feature space and is specifically tailored for learning  in the presence of a very large number of features. A wide range of  face images in different poses, with different expressions and under  different lighting conditions are used as a training set to capture  the variations of human faces. Experimental results on commonly  used benchmark data sets of a wide range of face images show that  the SNoW-based approach outperforms methods that use neural  networks, Bayesian methods, support vector machines and oth-  ers. Furthermore, learning and evaluation using the SNoW-based  method are significantly more efficient than with other methods.  I Introduction  Growing interest in intelligent human computer interactions has motivated a recent  surge in research on problems such as face tracking, pose estimation, face expression  and gesture recognition. Most methods, however, assume human faces in their input  images have been detected and localized.  Given a single image or a sequence of images, the goal of face detection is to identify  and locate human faces regardless of their positions, scales, orientations, poses and  illumination. To support automated solutions for the above applications, this has  to be done efficiently and robustly. The challenge in building an efficient and robust  system for this problem stems from the fact that human faces are highly non-rigid  objects with a high degree of variability in size, shape, color and texture.  Numerous intensity-based methods have been proposed recently to detect human  faces in a single image or a sequence of images. Sung and Poggio [24] report an  example-based learning approach for locating vertical frontal views of human faces.  They use a number of Gaussian clusters to model the distributions of face and  non-face patterns. A small window is moved over an image to determine whether a  face exists using the estimated distributions. In [16], a detection algorithm is pro-  posed that combines template matching and feature-based detection method using  hierarchical Markov random fields (MRF) and maximum a posteriori probability  (MAP) estimation. Colmenarez and Huang [4] apply Kullback relative information  for maximal discrimination between positive and negative examples of faces. They  use a family of discrete Markov processes to model faces and background patterns  and estimate the density functions. Detection of a face is based on the likelihood  A SNo W-Based Face Detector 863  ratio computed during training. Moghaddam and Pentland [12] propose a prob-  abilistic method that is based on density estimation in a high dimensional space  using an eigenspace decomposition. In [20], Rowley et al. use an ensemble of neural  networks to learn face and non-face patterns for face detection. Schneiderman et al.  describe a probabilistic method based on local appearance and principal component  analysis [23]. Their method gives some preliminary results on profile face detection.  Finally, hidden Markov models [17], higher order statistics [17], and support vector  machines (SVM) [14] have also been applied to face detection and demonstrated  some success in detecting upright frontal faces under certain lighting conditions.  In this paper, we present a face detection method that uses the SNoW learning  architecture [18, 3] to detect faces with different features and expressions, in different  poses, and under different lighting conditions. SNoW (Sparse Network of Winnows)  is a sparse network of linear functions that utilizes the Winnow update rule [10].  SNoW is specifically tailored for learning in domains in which the potential number  of features taking part in decisions is very large, but may be unknown a priori. Some  of the characteristics of this learning architecture are its sparsely connected units,  the allocation of features and links in a data driven way, the decision mechanism  and the utilization of an efficient update rule. SNoW has been used successfully on  a variety of large scale learning tasks in the natural language domain [18, 13, 5, 19]  and this is its first use in the visual processing domain.  In training the SNoW-based face detector, we use a set of 1,681 face images from  Olivetti [22], UMIST [6], Harvard [7], Yale [1] and FERET [15] databases to cap-  ture the variations in face patterns. In order to compare our approach with other  methods, our experiments involve two benchmark data sets [20, 24] that have been  used in other works on face detection. The experimental results on these benchmark  data sets (which consist of 225 images with 619 faces) show that our method out-  performs all other methods evaluated on this problem, including those using neural  networks [20], Kullback relative information [4], naive Bayes [23] and support vector  machines [14], while being significantly more efficient computationally. Along with  these experimental results we describe further experiments that provide insight into  some of the theoretical and practical considerations of SNoW-based learning sys-  tems. In particular, we study the effect of learning with primitive as well as with  multi-scale features, and discuss some of the sources of the success of the approach.  2 The SNoW System  The SNoW (Sparse Network of Winnows) learning architecture is a sparse network  of linear units over a common pre-defined or incrementally learned feature space.  Nodes in the input layer of the network represent simple relations over the input  and are being used as the input features. Each linear unit is called a target node and  represents relations which are of interes over the input examples; in the current  application, only two target nodes are being used, one as a representation for a .face  pattern and the other for a non-face pattern. Given a set of relations (i.e., types of  features) that may be of interest in the input image, each input image is mapped into  a set of features which are active (present) in it; this representation is presented  to the input layer of SNoW and propagates to the target nodes. (Features may  take either binary value, just indicating the fact that the feature is active (present)  or real values, reflecting its strength; in the current application, all features are  binary. See Sec 3.1.) Target nodes are linked via weighted edges to (some of the)  input features. Let At = {i,...,im} be the set of features that are active in an  example and are linked to the target node t. Then the linear unit is active if and  t t  only if Y.i  wi > Or, where w i is the weight on the edge connecting the ith feature  to the target node t, and Ot is its threshold.  In the current application a single SNoW unit which includes two subnetworks, one  864 M.-H. Yang, D. Roth and N. Ahuja  for each of the targets, is used. A given example is treated autonomously by each  target subnetwork; that is, an image labeled as a face is used as a positive example  for the face target and as a negative example for the non-face target, and vice-versa.  The learning policy is on-line and mistake-driven; several update rules can be used  within SNOW. The most successful update rule, and the only one used in this  work is a variant of Littlestone's Winnow update rule, a multiplicative update rule  tailored to the situation in which the set of input features is not known a priori, as  in the infinite attribute model [2]. This mechanism is implemented via the sparse  architecture of SNOW. That is, (1) input features are allocated in a data driven  way - an input node for the feature i is allocated only if the feature i is active  in the input image and (2) a link (i.e., a non-zero weight) exists between a target  node t and a feature i if and only if i has been active in an image labeled t. Thus,  the architecture also supports augmenting the feature types at later stages or from  external sources in a flexible way, an option we do not use in the current work.  The Winnow update rule has, in addition to the threshold 0t at the target t, two  update parameters: a promotion parameter a > I and a demotion parameter 0 <   < 1. These are being used to update the current representation of the target t (the  set of weights w) only when a mistake in prediction is made. Let At - {i,..., ira}  be the set of active features that are linked to the target node t. If the algorithm  t < 0t) and the received label is 1, the active weights in  predicts 0 (that is, ie,4t wi -  t t  the current example are promoted in a multiplicative fashion: Vi 6 .At, wi - a. w i.  If the algorithm predicts I (ieAt w > 0t) and the received label is 0, the active  t t All other weights  weights in the current example are demoted: Vi  .At, w i - 'wi.  are unchanged. The key property of the Winnow update rule is that the number  of examples  it requires to learn a linear function grows linearly with the number  of relevant features and only logarithmically with the total number of features.  This property seems crucial in domains in which the number of potential features  is vast, but a relatively small number of them is relevant (this does not mean that  only a small number of them will be active, or have non-zero weights). Winnow  is known to learn efficiently any linear threshold function and to be robust in the  presence of various kinds of noise and in cases where no linear-threshold function can  make perfect classification, and still maintain its abovementioned dependence on the  number of total and relevant attributes [11, 9]. Once target subnetworks have been  learned and the network is being evaluated, a winner-take-all mechanism selects  the dominant active target node in the SNoW unit to produce a final prediction.  In general, but not in this work, units' output may be cached and processed along  with the output of other SNoW units to produce a coherent output.  3 Learning to detect faces  For training, we use a set of 1,681 face images (collected from Olivetti [22], UMIST  [6], Harvard [7], Yale [1] and FERET [15] databases) which have wide variations  in pose, facial expression and lighting condition. For negative examples we start  with 8,422 non-face examples from 400 images of landscapes, trees, buildings, etc.  Although it is extremely difficult to collect a representative set of non-face examples,  the bootstrap method [24] is used to include more non-face examples during training.  For positive examples, each face sample is manually cropped and normalized such  that it is aligned vertically and its size is 20 x 20 pixels. To make the detection  method less sensitive to scale and rotation variation, 10 face examples are generated  from each original sample. The images are produced by randomly rotating the  images by up to 15 degrees with scaling between 80% and 120%. This produces  16,810 face samples. Then, histogram equalization is performed that maps the  In the on-line setting [10] this is usually phrased in terms of a mistake-bound but is  known to imply convergence in the PAC sense [25, 8].  A SNo W-Based Face Detector 865  intensity values to expand the range of intensities. The same procedure is applied  to input images in detection phase.  3.1 Primitive Features  The SNoW-based face detector makes use of Boolean features that encode the po-  sitions and intensity values of pixels. Let the pixel at (x, y) of an image with width  w and height h have intensity value I(x, y) (0 <_ I(x, y) <_ 255). This information  is encoded as a feature whose index is 256(y  w + x) + I(x, y). This representation  ensures that different points in the {position x intensity) space are mapped to  different features. (That is, the feature indexed 256(y  w + x) + I(x, y) is active if  and only if the intensity in position (x, y) is I(x, y).) In our experiments, the values  for w and h are 20 since each face sample has been normalized to an image of 20 x 20  pixels. Note that although the number of potential features in our representation  is 102400 (400 x 256), only 400 of those are active (present) in each example, and it  is plausible that many features will never be active. Since the algorithm's complex-  ity depends on the number of active features in an example, rather than the total  number of features, the sparseness also ensures efficiency.  3.2 Multi-scale Features  Many vision problems have utilized multi-scale features to capture the structures  of an object. However, extracting detailed multi-scale features using edge or region  information from segmentation is a computationally expensive task. Here we use the  SNoW paradigm to extract Boolean features that represent multi-scale information.  This is done in a similar way to the {position x intensity} used in Sec. 3.1,  only that in this case we encode, in addition to position, the mean and variance of a  multi-scale pixel. The hope is that the multi-scale feature will capture information  that otherwise requires many pixel-based features to represent, and thus simplify  the learning problem. Uninformative multi-scale features will be quickly assigned  low weights by the learning algorithm and will not degrade performance. Since  each face sample is normalized to be a rectangular image of the same size, it suffices  to consider rectangular sub-images with varying size from face samples, and for  each generate features in terms of the means and variances of their intensity values.  Empirical results show that faces can be described effectively this way.  Instead of using the absolute values of the mean and variance when encoding the  features, we discretize these values into a predefined number of classes. Since the  distribution of the mean values as well as the variance values is normal, the dis-  cretization is finer near the means of these distributions. The total number of  values was determined empirically to be 100, out of which 80 ended up near the  mean. Given that, we use the same scheme as in Sec. 3.1 to map the {position x  intensity mean x intensity variance} space into the Boolean feature space.  This is done separately for four different sub-image scales, of 1 x 1, 2 x 2, 4 x 4 to  10 x 10 pixels. The multi-scale feature vector consists of active features correspond-  ing to all these scales. The number of active features in each example is therefore  400 + 100 + 25 + 4, although the total number of features is much larger.  In recent work we have used more sophisticated conjunctive features for this purpose  yielding even better results. However, the emphasis here is that with the SNoW  approach, even very simplistic features support excellent performance.  4 Empirical Results  We tested the SNoW-based approach with both sets of features on the two sets  of images collected by Rowley [20], and Sung [24]. Each image is scanned with a  rectangular window to determine whether a face exists in the window or not. To  detect faces of different scales, each input image is repeatedly subsampled by a  factor of 1.2 and scanned through for 10 iterations. Table 1 shows the reported  866 M.-H. Yang, D. Roth and iV. Ahuja  experimental results of the SNoW-based face detectors and several face detection  systems using the two benchmark data sets (available at http://www.cs.cmu.edu/  ~bar/faces.html). The first data set consists of 130 images with 507 frontal faces  and the second data set consists of 23 images with 155 frontal faces. There are  a few hand drawn faces and cartoon faces in both sets. Since some methods use  intensity values as their features, systems 1-4 and 7 discard these such hand drawn  and cartoon faces. Therefore, there are 125 images with 483 faces in test set I and  20 images with 136 faces in test set 2 respectively. The reported detection rate is  computed as the ratio between the number of faces detected in the images by the  system and the number of faces identified there by humans. The number of false  detections is the number of non-faces detected as faces.  It is difficult to evaluate the performance of different methods even though they  use the same benchmark data sets because different criteria (e.g. training time,  number of training examples involved, execution time, number of scanned windows  in detection) can be applied to favor one over another. Also, one can tune the  parameters of one's method to increase the detection rates while increasing also the  false detections. The methods using neural networks [20], distribution-based [24],  Kullback relative information [4] and naive Bayes [23] report several experimental  results based on different sets of parameters. Table 1 summarizes the best detection  rates and corresponding false detections of these methods. Although the method  in [4] has the highest detection rates in one benchmark test, this was done by  significantly increasing the number of false detections. Other than that, it is evident  that the SNoW-based face detectors outperforms others in terms of the overall  performance. These results show the credibility of SNoW for these tasks, as well  Table 1: Experimental results on images from test set 1 (125 images with 483 faces)  in [20] and test set 2 (20 images with 136 faces) in [24] (see text for details)  Test Set 1 Test Set 2  Method Detect Rate False Detects Detect Rate False Detects  SNoW w/ primitive features 94.2% 84 93.6% 3  SNoW w/ multi-scale features 94.8% 78 94.1% 3  Mixture of factor analyzers [26], , 92.3% 82 89.4% 3  Fisher linear discriminant [27] 93.6% 74 91.5% 1  Distribution-based [24] N/A N/A 81.9% 13  Neural network [20] 92.5% 862 90.3% 42  Naive Bayes [23] 93.0% 88 91.2% 12  Kullback relative information [4] 98.0% 12758 N/A N/A  Support vector machine [14] N/A N/A 74.2% 20  as exhibit the improvement achieved by increasing the expressiveness of the features.  This may indicate that further elaboration of the features, which can be done in a  very general and flexible way within SNOW, would yield further improvements.  In addition to comparing feature sets, we started to investigate some of the reasons  for the success of SNoW in this domain, which we discuss briefly below. Two  potential contributions are the Winnow update rule and the architecture. First, we  studied the update rule in isolation, independent of the SNoW architecture. The  results we got when using the Winnow simply as a discriminator were fairly poor  (63.9%/65.3% for Test Set 1, primitive and multi-scale features, respectively, and  similar results for the Test Set 2.). The results are not surprising, given that Winnow  is used here only as a discriminator and is using only positive weights. Investigating  the architecture in isolation reveals that weighting or discarding features based on  their contribution to mistakes during training, as is done within SNOW, is crucial.  Considering the active features uniformly (separately for faces and non-faces) yields  poor results. Specifically, studying the resulting SNoW network shows that the total  number of features that were active with non-faces is 102,208, out of 102,400 possible  A SNo W-Based Face Detector 867  (primitive) features. The total number of active features in faces was only 82,608,  most of which are active only a few times. In retrospect, this is clear given the  diverse set of images used as negative examples, relative to the somewhat restricted  (by nature) set of images that constitute faces. (Similar phenomenon occurs with  the multi-scale features, where the numbers are 121572 and 90528, respectively, out  of 135424.) Overall it exhibits that the architecture, the learning regime and the  update rule all contribute significantly to the success of the approach.  Figure I shows some faces detected in our experiments. Note that profile faces and  faces under heavy illumination are detected. Experimental results show that profile  faces and faces under different illumination are detected very well by our method.  Note that although there may exist several detected faces around each face, only  one window is drawn to enclose each detected face for clear presentation.  Figure 1: Sample experimental results using our method on images from two bench-  mark data sets. Every detected face is shown with an enclosing window.  5 Discussion and Conclusion  Many theoretical and experimental issues are to be addressed before a learning sys-  tem of this sort can be used to detect faces efficiently and robustly under general  conditions. In terms of the face detection problem, the presented method is still  not able to detect rotated faces. A recent method [21], addresses this problem by  building upon a upright face detector [20] and rotating each test sample to upright  position. However, it suffers from degraded detection rates and more false detec-  tions. Given our results, we believe that the SNoW approach, if adapted in similar  ways, would generalize very well to detect faces under more general conditions.  In terms of the SNoW architecture, although the main ingredients of it are under-  stood theoretically, more work is required to better understand its strengths. This  is increasingly interesting given that the architecture has been found to perform  very well in large-scale problem in the natural language domain as well  868 M.-H. Yang, D. RotIt and N. Ahuja  The contributions of this paper can be summarized as follows. We have introduced  the SNoW learning architecture to the domain of visual processing and described an  approach that detect faces regardless of their poses, facial features and illumination  conditions. Experimental results show that this method outperforms other methods  in terms of detection rates and false detectionss, while being more efficient both in  learning and evaluation.  References  [1] P. Belhumeur, J. Hespanha, and D. Kriegman. Eigenfaces vs. fisherfaces: Recognition using class  specific linear projection. IEEE Transactions on Pattern Analysis and Machine Intelligence,  19(7):711-720, 1997.  [2] A. Blum. Learning boolean functions in an infinite attribute space. Machine Learning, 9(4):373-  386, 1992.  [3] A. Carleson, C. Cumby, J. Rosen, and D. Roth. The SNoW learning architecture. Technical Report  UIUCDCS-R-99-2101, UIUC Computer Science Department, May 1999.  [4] A. J. Colmenarez and T. S. Huang. Face detection with information-based maximum discrimina-  tion. In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern  Recognition, pages 782-787, 1997.  [5] A. R. Golding and D. Roth. A winnow based approach to context-sensitive spelling correction.  Machine Learning, 34:107-130, 1999. Special Issue on Machine Learning and Natural Language.  [6] D. B. Graham and N.M. Allinson. Characterizing virtual eigensignatures for general purpose face  recognition. In H. Wechsler, P. J. Phillips, V. Bruce, F. Fogelman-Soulie, and T. S. Huang, editors,  Face Recognition: From Theory to Applications, volume 163 of NATO ASI Series F, Computer  and Systems Sciences, pages 446-456. Springer, 1998.  [7] P. Hallinan. A Deformable Model for Face Recognition Under Arbitrary Lighting Conditions.  PhD thesis, Harvard University, 1995.  [8] D. Helmbold and M. K. Warmuth. On weak learning. Journal of Computer and System Sciences,  50(3):551-573, June 1995.  [9] J. Kivinen and M. K. Warmuth. Exponentiated gradient versus gradient descent for linear predic-  tors. In Proceedings of the Annual ACM Symposium on the Theory of Computing, 1995.  [10] N. Littlestone. Learning quickly when irrelevant attributes abound: A new linear-threshold algo-  rithm. Machine Learning, 2:285-318, 1988.  [11] N. Littlestone. Redundant noisy attributes, attribute errors, and linear threshold learning using  winnow. In Proceedings of the fourth Annual Workshop on Computational Learning Theory,  pages 147-156, 1991.  [12] B. Moghaddam and A. Pentland. Probabilistic visual learning for object recognition. IEEE Trans-  actions on Pattern Analysis and Machine Intelligence, 19(7):696-710, 1997.  [13] M. Munoz, V. Punyakanok, D. Roth, and D. Zimak. A learning approach to shallow parsing. In  EMNLP-VLC'9g, the Joint SIGDAT Conference on Empirical Methods in Natural Language  Processing and Very Large Corpora, June 1999.  [14] E. Osuna, R. Freund, and F. Girosi. Training support vector machines: an application to face  detection. In Proceedings of the IEEE Computer Society Conference on Computer Vision and  Pattern Recognition, pages 130-136, 1997.  [15] P. J. Phillips, H. Moon, S. Rizvi, and P. Rauss. The feret evaluation. In H. Wechsler, P. J.  Phillips, V. Bruce, F. Fogelman-Soulie, and T. S. Huang, editors, Face Recognition: From Theory  to Applications, volume 163 of NATO ASI Series F, Computer and Systems Sciences, pages  244-261. Springer, 1998.  [16] R. J. Qian and T. S. Huang. Object detection using hierarchical mrf and map estimation. In  Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recog-  nition, pages 186-192, 1997.  [17] A. N. Rajagopalan, K. S. Kumar, J. Karlekar, R. Manivasakan, and M. M. Patil. Finding faces in  photographs. In Proceedings of the Sixth International Conference on Computer Vision, pages  640-645, 1998.  [18] D. Roth. Learning to resolve natural language ambiguities: A unified approach. In Proceedings of  the Fifteenth National Conference on Artificial Intelligence, pages 806-813, 1998.  [19] D. Roth and D. Zelenko. Part of speech tagging using a network of linear separators. In COLING-  ACL 98, The 17th Int. Conference on Computational Linguistics, pages 1136-1142, 1998.  [20] H. Rowley, S. Baluja, and T. Kanade. Neural network-based face detection. IEEE Transactions  on Pattern Analysis and Machine Intelligence, 20(1):23-38, 1998.  [21] H. Rowley, S. Baluja, and T. Kanade. Rotation invariant neural network-based face detection.  In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern  Recognition, pages 38-44, 1998.  [22] F. S. Samaria. Face Recognition Using Hidden Markov Models. PhD thesis, University of Cam-  bridge, 1994.  [23] H. Schneiderman and T. Kanade. Probabilistic modeling of local appearance and spatial rela-  tionships for object recognition. In Proceedings of the IEEE Computer Society Conference on  Computer Vision and Pattern Recognition, pages 45-51, 1998.  [24] K.-K. Sung and T. Poggio. Example-based learning for view-based human face detection. IEEE  Transactions on Pattern Analysis and Machine Intelligence, 20(1):39-51, 1998.  [22] L. G. Valiant. A theory of the learnable. Commun. ACM, 27(11):1134-1142, Nov. 1984.  M.-H.. Yang, N. Ahuja, and D. Kriegman. Face detection using a mixture of factor analyzers. In  Proceedings of the IEEE International Conference on Image Processing, 1999.  [27] M.-H. Yang, N. Ahuja, and D. Kriegman. Mixtures of linear subspaces for face detection. In Pro-  ceedings of the Foruth IEEE International Conference on Automatic Face and Gesture Recog-  nition, 2000.  
LTD  Facilitates Learning  Environment  In a Noisy  Paul Munro  School of Information Sciences  University of Pittsburgh  Pittsburgh PA 15260  pwm + @pitt. edu  Gerardina Hernandez  Intelligent Systems Program  University of Pittsburgh  Pittsburgh PA 15260  gehst5+ @pitt. edu  Abstract  Long-term potentiation (LTP) has long been held as a biological  substrate for associative learning. Recently, evidence has emerged  that long-term depression (LTD) results when the presynaptic cell  fires after the postsynaptic cell. The computational utility of LTD  is explored here. Synaptic modification kernels for both LTP and  LTD have been proposed by other laboratories based studies of one  postsynaptic unit. Here, the interaction between time-dependent  LTP and LTD is studied in small networks.  1 Introduction  Long term potentiation (LTP) is a neurophysiological phenomenon observed under  laboratory conditions in which two neurons or neural populations are stimulated at a  high frequency with a resulting measurable increase in synaptic efficacy between  them that lasts for several hours or days [1]-[2] LTP thus provides direct evidence  supporting the neurophysiological hypothesis articulated by Hebb [3].  This increase in synaptic strength must be countered by a mechanism for weakening  the synapse [4]. The biological correlate, long-term depression (LTD) has also been  observed in the laboratory; that is, synapses are observed to weaken when low  presynaptic activity coincides with high postsynaptic activity [5]-[6].  Mathematical formulations of Hebbian learning produce weights, w O, (where i is the  presynaptic unit and j is the postsynaptic unit), that capture the covariance [Eq. 1]  between the instantaneous activities of pairs of units, a i and aj [7].  ij (t) = (a i (t ) - i )(a j (t) -  j ) [1 ]  This idea has been generalized to capture covariance between activities that are  shifted in time [8]-[9], resulting in a framework that can model systems with  temporal delays and dependencies [Eq. 2].  ij (t) = lS K(t"-t')a i (t")a j (t')dt"dt' [2]  LTD Facilitates Learning in a Noisy Environment 151  As will be shown in the following sections, depending on the choice of the function  K(At), this formulation encompasses a broad range of learning rules [10]-[12] and  can support a comparably broad range of biological evidence.  Aw  tpre-tpost  Figure 1. Synaptic change as a function of the time difference between spikes from  the presynaptic neuron and the postsynaptic neuron. Note that for tpre < tpo.,t, LTP  results (Aw > 0), and for tpre > tpo.,, the result is LTD.  Recent biological data from [13]-[15], indicates an increase in synaptic strength  (LTP) when presynaptic activity precedes postsynaptic activity, and LTD in the  reverse case (postsynaptic precedes presynaptic). These ideas have started to appear  in some theoretical models of neural computation [10]-[12], [16]-[18]. Thus, Figure  1 shows the form of the dependence of synaptic change, Aw on the difference in  spike arrival times.  2 A General Framework  Given specific assumptions, the integral in Eq. 2 can be separated into two integrals,  one representing LTP and one representing LTD [Eq. 3].  t t  ij(t)= f Kp(t-t')ai(t')aj(.)dt' +  KD(t-t')ai(t)aj(t')dt' [3]  I' = -oo t' =  L TP L TD  The activities that do not depend on t' can be factored out of the integrals, giving  two Hebb-like products, between the instantaneous activity in one cell and a  weighted time average of the activity in the other [Eq. 4]:  ij (t)= (ai (t)) P a j (t) - ai (t)(a j (t)) D  t [41  where (f(t))x --I  Kx(t-t')f(t')dt'l for X  {P,D}  The kernel functions Kv and Ko can be chosen to select precise times out of the  convoluted function f(t), or to average across the functions for an arbitrary range. The  alpha function is useful here [Eq. 5]. A high value of tx selects an immediate time, while  a small value approximates a longer time-average.  K x (v) = flxve -axr for X  {P,D} [5]  with a, >O, ao >O, fi, >O, fio <0  152 P W. Munro and G. Hernandez  For high values of at, and tro, only pre- and post- synaptic activities that are very close  temporally will interact to modify the synapse. In a simulation with discrete step sizes,  this can be reasonably approximated by only considering just a single time step [F.q. 6].  Awij (t ) = a i (t - 1)a j (t) - a i (t)a j (t - 1)  [61  Sumnfing zlwo(t) and zlwo(t+l) gives a net change in the weights zl(2)wo = wo(t+l)-wo(t-1)  over the two time steps:  At2)wij (t ) = ai (t)A(2) a i (t)-a i (t)A(2) ai (t)  [71  The first term is predictive in that it has the form of the delta rule where a(t+l) acts as a  training signal for aj (t-l), as in a temporal Hopfield network [9].  3 Temporal Contrast Enhancement  The computational role of the LTP term in Eq. 3 is well established, but how does the  second term contribute? A possibility is that the term is analogous to lateral inhibition in  the temporal domain; that is, that by suppressing associations in the "wrong" temporal  direction, the system may be more robust against noise in the input. The resulting system  may be able to detect the onset and offset of a signal more reliably than a system not  using an anti-Hebbian LTD term.  The extent to which the LTD term is able to enhance temporal contrast is likely to depend  idiosyncratically on the statistical qualities of a particular system. If so, the parameters of  the system might only be valid for signals with specific statistical properties, or the  parameters might be adaptive. Either of these possibilities lies beyond the scope of  analysis for this paper.  4 Simulations  Two preliminary simulation studies illustrate the use of the learning rule for predictive  behavior and for temporal contrast enhancement. For every simulation, kernel functions  were specified by the parameters a and t, and the number of time steps, nv and nz>, that  were sampled for the approximation of each integral.  4.1 Task 1. A Sequential Shifter  The first task is a simple shifter over a set of 7 to 20 units. The system is trained on these  stimuli and then tested to see if it can reconstruct the sequence given the initial input.  The task is given with no noise and with temporal noise (see Figure 2). Task 1 is  designed to examine the utility of LTD as an approach to learning a sequence with  temporal noise. The ability of the network to reconstruct the noise-free sequence after  training on the noisy sequence was tested for different LTD kernel functions.  Note that the same patterns are presented (for each time slice, just one of the n units is  active), but the shifts either skip or repeat in time. Experiments were run with k = 1, 2, or  3 of the units active.  4.2 Task 2. Time series reconstruction.  In this task, a set of units was trained on external sinusoidal signals that varied according  to frequency and phase. The purpose of this task is to examine the role of LTD in  providing temporal context. The network was then tested under a condition in which the  LTD Facilitates Learning in a Noisy Environment 153  external signals were provided to all but one of the units. The activity of the deprived  unit was then compared with its training signal  T  i  Sequence Reconsction  Clean Noisy LTP alone LTP & LTD  1 2 3 4 5 6 7 1 2 3 4 5 6 7 1 2 3 4 5 7 1 2 3 4 5 7  Figure 2. Reconstruction of clean shifter sequence using as input the noisy stimulus  shifter sequence. For each time slice, just one of the 7 units is active. In the clean  sequence, activity shifts cyclically around the 7 units. The noisy sequence has a random  jitter of +1.  5 ResultsSequential Shifter Results  All networks trained on the clean sequence can learn the task with LTP alone, but  no networks could learn the shifter task based on a noisy training sequence unless  there was also an LTD term. Without an LTD term, most units would saturate to  maximum values. For a range of LTD parameters, the network would converge  without saturating. Reconstruction performance was found to be sensitive to the  LTD parameters. The parameters ac and/9 shown in Table. 1 needed to be chosen  very specifically to get perfect reconstruction (this was done by trial and error). For  a narrow range of parameters near the optimal values, the reconstructed sequence  was close to the noise-free target. However, the parameters ac and /9 shown in  Table 2 are estimated from the experimental result of Zhang,et al [15].  Table 1. Results of the sec uential shifter task.  k rl rl r p ao no Time  1 7 I 1 2.72 1 0.1 -0.4 5 208   7 I 1 2.72 1 0.1 -0.4 4 40  2 0.5 0.4 3 0.2 -0.1 7 192  3 7 1 0.5 0.4 1 0.2 -0.1 6 168  I 10 1 1 2.72 1 0.1 -0.4 8 682  2 10 1 1 2.72 1 0.1 .0.4 7 99  1 15 1 1 2.72 1 0.1 -0.4 13 1136  1 20 1 1 2.72 1 0.1 .0.4 18 4000  The task was to shift a pattern 1 unit with each time step. A block of k of n units was  active. The parameters of the kernel functions (a and ,8), the number of values sampled  from the kernel (the number of time slices used to estimate the integral), n, and nz>, and  the number of steps used to begin the reconstruction, nr (usually nr = 1) are given in the  table. The last column of the table (Time) reports the number of iterations required for  perfect reconstruction.  154 P I. Munro and G. Hernandez  Table 2.. Results of the sequential shifter task using as parameters: nr =1; nv =1;  ao =0.125; ae=0.5; fl *e'0.35; t=te *e*0.8.  : n no Time  l 7 6 288   7 5 96  3 7 4 64  For the above results, the k active units were always adjacent with respect to the shifting  direction. For cases with noncontiguous active units, reconstruction was never exact.  Networks trained with LTP alone would saturate, but would converge to a sequence  "close" to the target (Fig. 3) if an LTD term was added.  Sequence  Clean Noisy  Reconstruction  LTP alone LTP & LTD  6666666 6066666 6 6 6 6 66 6666666  1254567 1234567 1 2 Y 4 5 7 1234567  i DIDID[]D []flllflfl DDI IllD D DIl[]fl  Figure 3. This base pattern (k=2, n=7) with noncontiguous active units was  presented as a shifted sequence with noise. The target sequence is partially  reconstructed only when LTP and LTD are used together.  5.1 Time Series Reconstruction Results  A network of just four units was trained for hundreds of iterations, the units were each  externally driven by a sinusoidally varying input. Networks trained with LTP alone fail  to reconstruct the time series on units deprived of external input during testing. In these  simulations, there is no noise in the patterns, but LTD is shown to be necessary for  reconstruction of the patterns (Fig. 4).  ' I I l  I .  Figure 4. Reconstruction of sinusoids. Target signals, from training (dashed)  plotted with reconstructed signals (solid). Left: The best reconstruction using LTP  alone. Right: A typical result with LTP and LTD together.  For high values of tzv and tzz), the reconstruction of sinusoids is very sensitive to the  values of flz) and fie. Figure 5 shows the results when Iflo I and fie values are close. In  the first case (top), when Iflol is slightly smaller than fie, the first two neurons  (from left to right) saturate. And, in the contrary case (bottom) the first two neurons  LTD Facilitates Learning in a Noisy Environment 155  show almost null activation. However, the third and fourth neurons (from left to  right) in both cases (top and bottom) show predictive behavior.  I la I I: P..  '. ' ' I I ', : "  Figure 5. Reconstruction of sinusoids . Examples of target signals from training  (dashed) plotted with reconstructed signals (solid). Top: When Iflt I<flv. Bottom:  When 1,8v1>,8.  6 Discussion  In the half century that has elapsed since Hebb articulated his neurophysiological  postulate, the neuroscience community has come to recognize its fundamental role  in plasticity. Hebb's hypothesis clearly transcends its original motivation to give a  neurophysiologically based account of associative memory.  The phenomenon of LTP provides direct biological support for Hebb's postulate,  and hence has clear cognitive implications. Initially after its discovery in the  laboratory, the computational role of LTD was thought to be the flip side of LTP.  This interpretation would have synapses strengthen when activities are correlated  and have them weaken when they are anti-correlated. Such a theory is appealing for  its elegance, and has formed the basis many network models [19]-[20]. However,  the dependence of synaptic change on the relative timing of pre- and post- synaptic  activity that has recently been shown in the laboratory is inconsistent with this story  and calls for a computational interpretation. A network trained with such a learning  rule cannot converge to a state where the weights are symmetric, for example, since  Aw u  Awji.  While the simulations reported here are simple and preliminary, they illustrate two  tasks that benefit from the inclusion of time-dependent LTD. In the case of the  sequential shifter, an examination of more complex predictive tasks is planned in  the near future. It is expected that this will require architectures with unclamped  (hidden) units. The role of LTD here is to temporally enhance contrast, in a way  analogous to the role of lateral inhibition for computing spatial contrast  enhancement in the retina. The time-series example illustrates the possible role of  LTD for providing temporal context.  156 P W. Munro and G. Hernandez  7 References  [1] Bliss TVP & Lqmo T (1973) Long-lasting potentiation of synaptic in the dentate area of  the unanaesthetized rabbit following stimulation of the perforant path.J Physiol 232:331-356  [2] Malenka RC (1995) LTP and LTD: dynamic and interactive processes of synaptic  plasticity. The Neuroscientist 1:35-42.  [3] Hebb DO (1949) The Organization of Behavior. Wiley: NY.  [4] Stent G (1973) A physiological, mechanism for Hebb's postulate of learning. Proc. Natl.  Acad. Sci. USA 70:997-1001  [5] Barrionuevo G, Schottler F & Lynch G (1980) The effects of repetitive low frequency  stimulation on control and "pontentiated" synaptic responses in the hippocampus. Life Sci  27:2385-2391.  [6] Thiels E, Xie X, Yeckel MF, Barrionuevo G & Berger TW (1996) NMDA Receptor-  dependent LTD in different subfields of hippocampus in vivo and in vitro. Hippocampus  6:43-51.  [7] Sejnowski T J (1977) Storing covariance with nonlinearly interacting neurons. J. Math.  Biol, 4:303-321.  [8] Sutton RS (1988) Learning to predict by the methods of temporal difference. Machine  Learning. 3:9-44  [9] Sompolinsky H and Kanter I (1986) Temporal association in asymmetric neural networks.  Phys. Rev. Letter. 57:2861-2864.  [10] Gerstner W, Kempter R, van Hemmen JL & Wagner H (1996) A neuronal learning rule  for sub-millisecond temporal coding. Nature 383:76-78.  [11] Kempter R, Gerstner W & van Hemmen JL (1999) Spike-based compared to rate-based  hebbian learning. Kearns, Ms., Solla, S.A and Cohn, D.A. Eds. Advances in Neural  Information Processing Systems 11. MIT Press, Cambridge MA.  [12] Kempter R, Gerstner W, van Hemmen JL & Wagner H (1996) Temporal coding in the  sub-millisecond range: Model of barn owl auditory pathway. Touretzky, D.S, Mozer, M.C,  Hasselmo, M.E, Eds. Advances in Neural Information Processing Systems 8. MIT Press,  Cambridge MA pp. 124-130.  [13] Markram H, Lubke J, Frotscher M & Sakmann B (1997) Regulation of synaptic efficacy  by coincidence of postsynaptic Aps and EPPSPs. Science 275:213-215.  [14] Markram H & Tsodyks MV (1996) Redistribution of synaptic efficacy between  neocortical pyramidal neurons. Nature 382:807-810.  [15] Zhang L, Tao HW, Holt CE & Poo M (1998) A critical window for cooperation and  competition among developing retinotectal synapses. Nature 35:37-44  [16] Abbott LF, & Blum KI (1996) Functional significance of long-term potentiati0n for  sequence learning and prediction. Cerebral Cortex 6: 406-416.  [17] Abbott LF, & Song S (1999) Temporally asymmetric hebbian learning, spike timing and  neuronal response variability. Kearns, Ms., Solla, S.A and Cohn, D.A. Eds. Advances in  Neural Information Processing Systems 11. MIT Press, Cambridge MA.  [18] Goldman MS, Nelson SB & Abbott LF (1998) Decorrelation of spike trains by synaptic  depression. Neurocomputing (in press).  [19] Hopfield J (1982) Neural networks and physical systems with emergent collective  computational properties. Proc. Natl. Acad. Sci. USA. 79:2554-2558.  [20] Ackley DH, Hinton GE, Sejnowski TJ (1985) A learning algorithm for Boltzmann  machines. Cognitive Science 9:147-169.  
Rules and Similarity in Concept Learning  Joshua B. Tenenbaum  Department of Psychology  Stanford University, Stanford, CA 94305  jbt@psych. stanford. edu  Abstract  This paper argues that two apparently distinct modes of generalizing con-  cepts - abstracting rules and computing similarity to exemplars - should  both be seen as special cases of a more general Bayesian learning frame-  work. Bayes explains the specific workings of these two modes - which  rules are abstracted, how similarity is measured - as well as why gener-  alization should appear rule- or similarity-based in different situations.  This analysis also suggests why the rules/similarity distinction, even if  not computationally fundamental, may still be useful at the algorithmic  level as part of a principled approximation to fully Bayesian learning.  1 Introduction  In domains ranging from reasoning to language acquisition, a broad view is emerging of  cognition as a hybrid of two distinct modes of computation, one based on applying abstract  rules and the other based on assessing similarity to stored exemplars [7]. Much support for  this view comes from the study of concepts and categorization. In generalizing concepts,  people's judgments often seem to reflect both rule-based and similarity-based computations  [9], and different brain systems are thought to be involved in each case [8]. Recent psycho-  logical models of classification typically incorporate some combination of rule-based and  similarity-based modules [ 1,4]. In contrast to this currently popular modularity position, I  will argue here that rules and similarity are best seen as two ends of a continuum of possible  concept representations. In [ 11,12], I introduced a general theoretical framework to account  for how people can learn concepts from just a few positive examples based on the principles  of Bayesian inference. Here I explore how this framework provides a unifying explanation  for these two apparently distinct modes of generalization. The Bayesian framework not only  includes both rules and similarity as special cases but also addresses several questions that  conventional modular accounts do not. People employ particular algorithms for selecting  rules and measuring similarity. Why these algorithms as opposed to any others? People's  generalizations appear to shift from similarity-like patterns to rule-like patterns in system-  atic ways, e.g., as the number of examples observed increases. Why these shifts?  This short paper focuses on a simple learning game involving number concepts, in which  both rule-like and similarity-like generalizations clearly emerge in the judgments of human  subjects. Imagine that I have written some short computer programs which take as input a  natural number and return as output either "yes" or "no" according to whether that number  60 J. B. Tenenbaum  satisfies some simple concept. Some possible concepts might be "z is odd", "z is between  30 and 45", "z is a power of 3", or "z is less than 10". For simplicity, we assume that only  numbers under 100 are under consideration. The learner is shown a few randomly chosen  positive examples - numbers that the program says "yes" to - and must then identify the  other numbers that the program would accept. This task, admittedly artificial, nonetheless  draws on people's rich knowledge of number while remaining amenable to theoretical anal-  ysis. Its structure is meant to parallel more natural tasks, such as word learning, that often  require meaningful generalizations from only a few positive examples of a concept.  Section 2 presents representative experimental data for this task. Section 3 describes a  Bayesian model and contrasts its predictions with those of models based purely on rules or  similarity. Section 4 summarizes and discusses the model's applicability to other domains.  2 The number concept game  Eight subjects participated in an experimental study of number concept learning, under es-  sentially the same instructions as those given above [ 11 ]. On each trial, subjects were shown  one or more random positive examples of a concept and asked to rate the probability that  each of 30 test numbers would belong to the same concept as the examples observed. X  denotes the set of examples observed on a particular trial, and n the number of examples.  Trials were designed to fall into one of three classes. Figure 1 a presents data for two repre-  sentative trials of each class. Bar heights represent the average judged probabilities that par-  titular test numbers fall under the concept given one or more positive examples X, marked  by "*"s. Bars are shown only for those test numbers rated by subjects; missing bars do not  denote zero probability of generalization, merely missing data.  On class I trials, subjects saw only one example of each concept: e.g., X = { 16} and X ---  {60}. To minimize bias, these trials preceded all others on which multiple examples were  given. Given only one example, people gave most test numbers fairly similar probabilities  of acceptance. Numbers that were intuitively more similar to the example received slightly  higher ratings: e.g., for X = {16}, 8 was more acceptable than 9 or 6, and 17 more than  87; for X = {60}, 50 was more acceptable than 51, and 63 more than 43.  The remaining trials each presented four examples and occured in pseudorandom order.  On class II trials, the examples were consistent with a simple mathematical rule: X =  {16, 8, 2, 64} or X = {60, 80, 10, 30}. Note that the obvious rules, "powers of two" and  "multiples often", are in no way logically implied by the data. "Multiples of five" is a pos-  sibility in the second case, and "even numbers" or "all numbers under 80" are possibilities  in both, not to mention other logically possible but psychologically implausible candidates,  such as "all powers of two, except 32 or 4". Nonetheless, subjects overwhelmingly followed  an all-or-none pattern of generalization, with all test numbers rated near 0 or 1 according to  whether they satisified the single intuitively "correct" rule. These preferred rules can be  loosely characterized as the most specific rules (i.e., with smallest extension) that include  all the examples and that also meet some criterion of psychological simplicity.  On class III trials, the examples satisified no simple mathematical rule but did have sim-  ilar magnitudes: X = { 16, 23, 19, 20} and X = {60, 52, 57, 55}. Generalization now  followed a similarity gradient along the dimension of magnitude. Probability ratings fell  below 0.5 for numbers more than a characteristic distance  beyond the largest or smallest  observed examples - roughly the typical distance between neighboring examples (~ 2 or  3). Logically, there is no reason why participants could not have generalized according to  Rules and Similarity in Concept Learning 61  various complex rules that happened to pick out the given examples, or according to very  different values off, yet all subjects displayed more or less the same similarity gradients.  To summarize these data, generalization from a single example followed a weak similarity  gradient based on both mathematical and magnitude properties of numbers. When several  more examples were observed, generalization evolved into either an all-or-none pattern de-  termined by the most specific simple rule, or, when no simple rule applied, a more articu-  lated magnitude-based similarity gradient falling off with characteristic distance  roughly  equal to the typical separation between neighboring examples. Similar patterns were ob-  served on several trials not shown (including one with a different value of ) and on two  other experiments in quite different domains (described briefly in Section 4).  3 The Bayesian model  In [12], I introduced a Bayesian framework for concept learning in the context of learn-  ing axis-parallel rectangles in a multidimensional feature space. Here I show that the same  framework can be adapted to the more complex situation of learning number concepts and  can explain all of the phenomena of rules and similarity documented above. Formally, we  observe n positive examples X = {z(),..., z } of concept C and want to compute  p(y E CIX), the probability that some new objet y belongs to C' given the observations  X. Inductive leverage is provided by a hypothesis space 7-/of possible concepts and a prob-  abilistic model relating hypotheses h to data X.  The hypothesis space. Elements of7/correspond to subsets of the universe ofobjts that  are psychologically plausible candidates for the extensions of concepts. Here the universe  consists of numbers between 1 and 100, and the hypotheses correspond to subsets such as  the even numbers, the numbers between 1 and 10, etc. The hypotheses can be thought of  in terms of either rules or similarity, i.e., as potential rules to be abstracted or as features  entering into a similarity computation, but Bayes does not distinguish these interpretations.  Bause we can capture only a fraction of the hypotheses people might bring to this task,  we would like an objetlye way to focus on the most relevant parts of people's hypothesis  space. One such method is additive clustering (ADCLUS) [6,10], which extracts a set of fea-  tures that best accounts for subjects' similarity judgments on a given set of objects. These  features simply correspond to subsets of objects and are thus naturally identified with hy-  potheses for concept learning. Applications of ADCLUS to similarity judgments for the  numbers 0-9 reveal two kinds of subsets [6,10]: numbers sharing a common mathemati-  cal property, such as {2, 4, 8} and {3, 6, 9}, and consecutive numbers of similar magnitude,  such as {1, 2, 3, 4} and {2, 3, 4, 5, 6}. Applying ADCLUS to the full set of numbers from  1 to 100 is impractical, but we can construct an analogous hypothesis space for this domain  based on the two kinds of hypotheses found in the ADCLUS solution for 0-9. One group  of hypotheses captures salient mathematical properties: odd, even, square, cube, and prime  numbers, multiples and powers of small numbers (_< 12), and sets of numbers ending in the  same digit. A second group of hypotheses, representing the dimension of numerical mag-  nitude, includes all intervals of consecutive numbers with endpoints between 1 and 100.  Priors and likelihoods. The probabilistic model consists ofa priorp(h) over 7/and a like-  lihood p(X I h) for each hypothesis h 6 H. Rather than assigning prior probabilities to each  of the 5083 hypotheses individually, I adopted a hierarchical approach based on the intuitive  division of?/into mathematical properties and magnitude intervals. A fraction  of the to-  tal probability was allocated to the mathematical hypotheses as a group, leaving (1 - ) for  62 J. B. Tenenbaum  the magnitude hypotheses. The  probability was distributed uniformly across the mathe-  matical hypotheses. The (1 - ) probability was distributed across the magnitude intervals  as a function of interval size according to an Erlang distribution, p( h ) or ( I h I/2 ,- I h I/*,  to capture the intuition that intervals of some intermediate size are more likely than those  of very large or small size.  and a are treated as free parameters of the model.  The likelihood is determined by the assumption of randomly sampled positive examples.  In the simplest case, each example in X is assumed to be independently sampled from a  uniform density over the concept C. For n examples we then have:  p(Xlh ) = 1/{hl  if Vj, x   h (1)  -- 0 otherwise,  where I hi denotes the size of the subset h. For example, ifh denotes the even numbers, then  Ihl- .50, because there are 50 even numbers between 1 and 100. Equation 1 embodies the  size principle for scoring hypotheses: smaller hypotheses assign greater likelihood than do  larger hypotheses to the same data, and they assign exponentially greater likelihood as the  number of consistent examples increases. The size principle plays a key role in learning  concepts from only positive examples [ 12], and, as we will see below, in determining the  appearance of rule-like or similarity-like modes of generalization.  Given these priors and likelihoods, the posterior p(hIX) follows directly from Bayes' rule.  Finally, we compute the probability of generalization to a new object y by averaging the  predictions of all hypotheses weighted by their posterior probabilities p(h IX):  h7-t  (2)  Equation 2 follows from the conditional independence of X and the membership ofy  C,  given h. To evaluate Equation 2, note that p(y  el h)is simply 1 ify  h, and 0 otherwise.  Model results. Figure lb shows the predictions of this Bayesian model (with  - 1/2,  tr - 10). The model captures the main features of the data, including convergence to the  most specific rule on Class II trials and to appropriately shaped similarity gradients on Class  III trials. We can understand the transitions between graded, similarity-like and all-or-none,  rule-like regimes ofgeneral!zation as arising from the interaction of the sizeprinciple (Equa-  tion 1) with hypothesis averaging (Equation 2). Because each hypothesis h contributes to  the average in Equation 2 in proportion to its posterior probability p(h IX), the degree of  uncertainty in p(hlX) determines whether generalization will be sharp or graded. When  p(hIX) is very spread out, many distinct hypotheses contribute significantly, resulting in a  broad gradient of generalization. When p(h IX) is concentrated on a single hypothesis h*,  only h* contributes significantly and generalization appears all-or-none. The degree of un-  certainty in p(h IX) is in tum a consequence of the size principle. Given a few examples con-  sistent with one hypothesis that is significantly smaller than the next-best competitor- such  as X = { 16, 8, 2, 64}, where "powers of two" is significantly smaller than "even numbers"  - then the smallest hypothesis becomes exponentially more likely than any other and gener-  alization appears to follow this most specific rule. However, given only one example (such  as X - { 16 }), or given several examples consistent with many similarly sized hypotheses -  such as X = { 16, 23, 19, 20}, where the top candidates are all very similar intervals: "num-  bers between 16 and 23", "numbers between 15 and 24", etc. - the size-based likelihood  favors the smaller hypotheses only slightly, p(hlX ) is spread out over many overlapping  hypotheses and generalization appears to follow a gradient of similarity. That the Bayesian  Rules and Similarity in Concept Learning 63  model predicts the right shape for the magnitude-based similarity gradients on Class III trials  is no accident. The characteristic distance ( of the Bayesian generalization gradient varies  with the uncertainty in p(h IX), which (for interval hypotheses) can be shown to covary with  the intuitively relevant factor of average separation between neighboring examples.  Bayes vs. rules or similarity alone. It is instructive to consider two special cases of the  Bayesian model that are equivalent to conventional similarity-based and rule-based algo-  rithms from the concept learning literature. What I call the SIM algorithm was pioneered  by [5] and also described in [2,3] as a Bayesian approach to learning concepts from both  positive and negative evidence. SIM replaces the size-based likelihood with a binary likeli-  hood that measures only whether a hypothesis is consistent with the examples: p(X I h) = 1  ifYj, z j) C h, and 0 otherwise. Generalization under SIM is just a count of the features  shared by y and all the examples in X, independent of the frequency of those features or  the number of examples seen. As Figure lc shows, SIM successfully models generaliza-  tion from a single example (Class I) but fails to capture how generalization sharpens up after  multiple examples, to either the most specific rule (Class II) or a magnitude-based similarity  gradient with appropriate characteristic distance ( (Class III). What I call the MIN algorithm  preserves the size principle but replaces the step of hypothesis averaging with maximization:  p(t  CIX) = 1 if  arg maxh p(Xlh ), and 0 otherwise. MIN is perhaps the oldest al-  gorithm for concept learning [3] and, as a maximum likelihood algorithm, is asymptotically  equivalent to Bayes. Its success for finite amounts of data depends on how peaked p(hIX)  is (Figure 1 d). MIN always selects the most specific consistent rule, which is reasonable  when that hypothesis is much more probable than any other (Class II), but too conservative  in other cases (Classes I and III). In quantitative terms, the predictions of Bayes correlate  much more highly with the observed data (R 2 = 0.91) than do the predictions of either SIM  (R 2 = 0.74) or MIN (R 2 = 0.47). In sum, only the full Bayesian framework can explain  the full range of rule-like and similarity-like generalization patterns observed on this task.  4 Discussion  Experiments in two other domains provide further support for Bayes as a unifying frame-  work for concept learning. In the context of multidimensional continuous feature spaces,  similarity gradients are the default mode of generalization [5]. Bayes successfully mod-  els how the shape of those gradients depends on the distribution and number of examples;  SIM and MIN do not [ 12]. Bayes also successfully predicts how fast these similarity gra-  dients converge to the most specific consistent rule. Convergence is quite slow in this do-  main (n ,-, 50) because the hypothesis space consists of densely overlapping subsets - axis-  parallel rectangles - much like the interval hypotheses in the Class III number tasks.  Another experiment engaged a word-learning task, using photographs of real objects as  stimuli and a cover story of learning a new language [11]. On each trial, subjects saw ei-  ther one example of a novel word (e.g., a toy animal labeled with "Here is a blicket."), or  three examples at one of three different levels of specificity: subordinate (e.g., 3 dalma-  tians labeled with "Here are three blickets."), basic (e.g., 3 dogs), or superordinate (e.g., 3  animals). They then were asked to pick the other instances of that concept from a set of  24 test objects, containing matches to the example(s) at all levels (e.g., other dalmatians,  dogs, animals) as well as many non-matching objects. Figure 2 shows data and predictions  for all three models. Similarity-like generalization given one example rapidly converged to  the most specific rule after only three examples were observed, just as in the number task  (Classes I and II) but in contrast to the axis-parallel rectangle task or the Class III num-  64 J. B. Tenenbaum  ber tasks, where similarity-like responding was still the norm after three or four examples.  For modeling purposes, a hypothesis space was constructed from a hierarchical clustering  of subjects' similarity judgments (augmented by an a priori preference for basic-level con-  cepts) [ 11 ]. The Bayesian model successfully predicts rapid convergence from a similarity  gradient to the minimal rule, because the smallest hypothesis consistent with each example  set is significantly smaller than the next-best competitor (e.g., "dogs" is significantly smaller  than "dogs and cats", just as with "multiples of ten" vs. "multiples of five"). Bayes fits the  full data extremely well (/i 2 = 0.98); by comparison, SIM (/iq = 0.83) successfully ac-  counts for only the n = 1 trials and MIN (/i 2 = 0.76), the n = 3 trials.  In conclusion, a Bayesian framework is able to account for both rule- and similarity-like  modes of generalization, as well as the dynamics of transitions between these modes, across  several quite different domains of concept learning. The key features of the Bayesian  model are hypothesis averaging and the size principle. The former allows either rule-like  or similarity-like behavior depending on the uncertainty in the posterior probability. The  latter determines this uncertainty as a function of the number and distribution of examples  and the structure of the leamer's hypothesis space. With sparsely overlapping hypotheses  - i.e., the most specific hypothesis consistent with the examples is much smaller than its  nearest competitors - convergence to a single rule occurs rapidly, after just a few exam-  pies. With densely overlapping hypotheses - i.e., many consistent hypotheses of compara-  ble size - convergence to a single rule occurs much more slowly, and a gradient of similar-  ity is the norm after just a few examples. Importantly, the Bayesian framework does not so  much obviate the distinction between rules and similarity as explain why it might be useful  in understanding the brain. As Figures 1 and 2 show, special cases of Bayes correspond-  ing to the SIM and MIN algorithms consistently account for distinct and complementary  regimes of generalization. SIM, without the size principle, works best given only one exam-  ple or densely overlappipg hypotheses, when Equation 1 does not generate large differences  in likelihood. M1N, without hypothesis averaging, works best given many examples or  sparsely overlapping hypotheses, when the most specific hypothesis dominates the sum over  7/in Equation 2. In light of recent brain-imaging studies dissociating rule- and exemplar-  based processing [8], the Bayesian theory may best be thought of as a computational-level  account of concept learning, with multiple subprocesses- perhaps subserving SIM and M1N  - implemented in distinct neural circuits. I hope to explore this possibility in future work.  References  [1] M. Erickson & J. Kruschke (1998). Rules and exemplars in category leaming. JEP: General 127,  107-140.  [2] D. Haussler, M. Keams, & R. Schapire (1994). Bounds on the sample complexity of Bayesian  leaming using information theory and the VC-dimension. Machine Learning 14, 83-113.  [3] T Mitchell (1997). Machine Learning. McGraw-Hill.  [4] R. Nosofsky & T Palmeri (1998). A rule-plus-exception model for classifying objects in  continuous-dimension spaces. Psychonomic Bull. & Rev. 5, 345-369.  [5] R. Shepard (1987). Towards a universal law of generalization for psychological science. Science  237, 1317-1323.  [6] R. Shepard & P. Arabie (1979). Additive clustering: Representation of similarities as combina-  tions of discrete overlapping properties. Psych. Rev. 86, 87-123.  [7] S. Sloman & L. Rips (1998). Similarity and Symbols in Human Thinking. MIT Press.  [8] E. Smith, A. Patalano & J. Jonides (1998). Alternative strategies of categorization. In [6].  [9] E. Smith & S. Sloman (1994). Similarity- vs. rule-based categorization. Mem. & Cog. 22, 377.  [10] J. Tenenbaum (1996). Learning the structure of similarity. NIPS 8.  [11] J. Tenenbaum (1999). A Bayesian Framework for Concept Learning. Ph.D. Thesis, MIT.  [12] J. Tenenbaum (1999). Bayesian modeling of human concept learning. NIPS 11.  Rules and Similarity in Concept Learning 65  (a) Average generalization judgments:  Class I 0. I kl [ ! t  Class ,, 0.ll[,.l [ l.  0 I  10 20 30  X= 16  I![ [ d  X=16 8 2 64  X=16 23 19 20  I !    40 50 60 70 80 90 100  [  X= 6O  o. ![I,. !1, !!l.,!h I!11 !1  0 .'. . !'.'.'.a ,.. : . . .  I 60 52 57 55  0.5 .. a   0 I .  10203040 50 60 70 80 90100  (b) Bayesian model:  1  0.5  X=16  I. I.I , d  X=16 8 264  X=16 23 19 20  01  . ,,.a/. I I  I  Ill    10 20 30 40 50 60 70 80 90 100  X= 60  .it,..,.,,, .,.,..  X=60 80 10 30  ..L [. !...,.t4, [., [. [  X=60 52 57 55  o.l ,,,IB,,  10 20 30 40 50 60 70 80 90 100  (c) Pure similarity. model (SIM):  o. Ill, I ,i . i  X=16 8 264  X=16 23 19 20  10 20 30 40 50 60 70 80 90100  ], t ] ! I l[ ,l[ill I Ill I !  X=60 80 10 30  X=60 52 57 55  o.f  ...... :,. ! !l Ill , ,  10 20 30 40 50 60 70 80 90 100  (d) Pure rule model  0.5  0  1  0.5  o  (MIN):  X=16  X=16 8 2 64  X=16 23 19 2O  X=60 52 57 55  o-f Jt  10 20 30 40 50 60 70 80 90100 10 20 30 40 50 60 70 80 90100  Figure 1: Data and model predictions for the number concept task.  (a) Average generalization judgments:  (b) Bayesian model:  1 1 111 ill I  Training  examples:  I 3 subordinate 3 basic 3 superordinate  match level:  (e) Pure similari model (SIM):  (d) Pure rule model (MIN):  Figure 2: Data and model predictions for the word learning task.  
Local probability propagation for factor  analysis  Brendan J. Frey  Computer Science, University of Waterloo, Waterloo, Ontario, Canada  Abstract  Ever since Pearl's probability propagation algorithm in graphs with  cycles was shown to produce excellent results for error-correcting  decoding a few years ago, we have been curious about whether  local probability propagation could be used successfully for ma-  chine learning. One of the simplest adaptive models is the factor  analyzer, which is a two-layer network that models bottom layer  sensory inputs as a linear combination of top layer factors plus in-  dependent Gaussian sensor noise. We show that local probability  propagation in the factor analyzer network usually takes just a few  iterations to perform accurate inference, even in networks with 320  sensors and 80 factors. We derive an expression for the algorithm's  fixed point and show that this fixed point matches the exact solu-  tion in a variety of networks, even when the fixed point is unstable.  We also show that this method can be used successfully to perform  inference for approximate EM and we give results on an online face  recognition task.  I Factor analysis  A simple way to encode input patterns is to suppose that each input can be well-  approximated by a linear combination of component vectors, where the amplitudes  of the vectors are modulated to match the input. For a given training set, the most  appropriate set of component vectors will depend on how we expect the modula-  tion levels to behave emd how we measure the distance between the input and its  approximation. These effects can be captured by a generative probability model  that specifies a distribution p(z) over modulation levels z = (z,... , ZK) T and a  distribution p(xlz ) over sensors x = (x,... ,xv) T given the modulation levels.  Principal component analysis, independent component analysis and factor analysis  can be viewed as maximum likelihood learning in a model of this type, where we as-  sume that over the training set, the appropriate modulation levels are independent  and the overall distortion is given by the sum of the individual sensor distortions.  In factor analysis, the modulation levels are called factors and the distributions  have the following form:  p(z ) = ./V'(z; O, 1),  p(xnlz) 2V(xn; K  ---- Ek=l/nkZk, )n),  p(z) K  = I-[k:lp(zk) = A/(z;0, I),  p(xlz) N  - - 2V(x; Az,  (1)  The parameters of this model are the factor loading matrix A, with elements  emd the diagonal sensor noise covariance matrix , with diagonal elements Pn- A  belief network for the factor analyzer is shown in Fig. la. The likelihood is  p(x) = .hf(z; 0, I).hf(x; Az, I')dz = .hf(x; 0, AA T + I'), (2)  Local Probability Propagation for Factor Analysis 443  Figure 1: (a) A belief network for factor analysis. (b) High-dimensional data (N = 560).  and online factor analysis consists of adapting A and  to increase the likelihood  of the current input, such as a vector of pixels from an image in Fig. lb.  Probabilistic inference - computing or estimating p(zlx ) - is needed to do dimen-  sionality reduction and to fill in the unobserved factors for online EM-type learning.  In this paper, we focus on methods that infer independent factors. p(zlx ) is Gaus-  sian and it turns out that the posterior means and variances of the factors are  E[zlx ] -- (AT,It-IA q- I)-IAT,It-Ix,  diag(COV(zlx)) = diag((AT-lA + 1)-1). (3)  Given A and , computing these values exactly takes O(K2N) computations,  mainly because of the time needed to compute AT-IA. Since there are only KN  connections in the network, exact inference takes at least O(K) bottom-up/top  down iterations.  Of course, if the same network is going to be applied more than K times for inference  (e.g., for batch EM), then the matrices in (3) can be computed once and reused.  However, this is not directly applicable in online learning and in biological models.  One way to circumvent computing the matrices is to keep a separate recognition  network, which approximates E[zlx] with Rx (Dayan et al., 1995). The optimal  recognition network, R = (A T - 1A + I) - 1A T - 1, can be approximated by jointly  estimating the generative network and the recognition network using online wake-  sleep learning (Hinton et al., 1995).  2 Probability propagation in the factor analyzer network  Recent results on error-correcting coding show that in some cases Pearl's prob-  ability propagation algorithm, which does exact probabilistic inference in graphs  that are trees, gives excellent performance even i[ the network contains so many  cycles that its minimal cut set is exponential (Prey and MacKay, 1998; Prey, 1998;  MacKay, 1999). In fact, the probability propagation algorithm for decoding low-  density parity-check codes (MacKay, 1999) and turbocodes (Berrou and Glavieux,  1996) is widely considered to be a major breakthrough in the information theory  community.  When the network contains cycles, the local computations give rise to an iterative  algorithm, which hopefully converges to a good answer. Little is known about the  convergence properties of the algorithm. Networks containing a single cycle have  been successfully analyzed by Weiss (1999) and Smyth et al. (1997), but results for  networks containing many cycles are much less revealing.  The probability messages produced by probability propagation in the factor analyzer  network of Fig. la are Gaussians. Each iteration of propagation consists of passing  a mean and a variance along each edge in a bottom-up pass, followed by passing  a mean and a variance along each edge in a top-down pass. At any instant, the  444 B. J. Fre  bottom-up means and variances can be combined to form estimates of the means  and variances of the modulation levels given the input.  Initially, the variance and mean sent from the kth top layer unit to the nth sensor  is set to k(n ) - 1 ad (o) _ O. The bottom-up pass begins by computing a noise  'lkn  level and an error signal at each sensor using the top-down variances and means  from the previous iteration:  -K 2 , (i--1) e(n/) _ -K , _(i--1)  8( ) -- )n -[- /.k.l'nk"kn , -- Xn Z-k----1 nk'l/kn ' (4)  These are used to compute bottom-up variances and means as follows:  (i) o(i)/2 . (i-1) , (i) = (ni)/,knk + _(i-1)  nk = on - , . (5)  The bottom-up variances and means are then combined to form the current esti-  mates of the modulation vriances and means:  N (i) (i) (i) -v (i) ,.(i)  v? 1/(1 -]- Zn=I 1/bnk),  (6)  -- __-- V k ,n_--l nk/(Pnk  The top-down pass proceeds by computing top-down variances and means as follows:  , (i) 1/(l/v?) (i) (i) , (i) /(i)  -- - 1/bn), _-- (,i)lv?) , (i)  - (7)  'lkn "'kn  Notice that the variance updates are independent of the mean updates, whereas the  mean updates depend on the variance updates.  2.1 Performance of local probability propagation. We created a total of  200,000 factor analysis networks with 20 different sizes ranging from K - 5, N -- 10  to K - 80, N - 320 and for each size of network we measured the inference error as  a function of the number of iterations of propagation. Each of the 10,000 networks of  a given size was produced by drawing the AnS from standard normal distributions  and then drawing each sensor variance n from an exponential distribution with  K 2 . (A similar procedure was used by Neal and Dayan (1997).)  mean ']=  For each random network, a pattern was simulated from the network and probabil-  ity propagation was applied using the simulated pattern as input. We measured the  error between the estimate (i) and the correct value E[zlx ] by computing the dif-  ference between their coding costs under the exact posterior distribution and then  normalizing by K to get an average number of nats per top layer unit.  Fig. 2a shows the inference error on a logarithmic scale versus the number of iter-  ations (maximum of 20) in the 20 different network sizes. In all cases, the median  error is reduced below .01 hats within 6 iterations. The rate of convergence of the  error improves for larger N, as indicated by a general trend for the error curves to  drop when N is increased. In contrast, the rate of convergence of the error appears  to worsen for larger K, as shown by a general slight trend for the error curves to  rise when K is increased.  For K _ N/8, 0.1% of the networks actually diverge. To better understand the di-  vergent cases, we studied the means and variances for all of the divergent networks.  In all cases, the variances converge within a few iterations whereas the means oscil-  late and diverge. For K = 5, N = 10, 54 of the 10,000 networks diverged and 5 of  these are shown in Fig. 2b. This observation suggests that in general the dynamics  are determined by the dynamics of the mean updates.  2.2 Fixed points and a condition for global convergence. When the vari-  ance updates converge, the dynamics of probability propagation in factor analysis  networks become linear. This allows us to derive the fixed point of propagation in  closed form and write an eigenvalue condition for global convergence.  Local Probability Propagation for Factor Analysis 445  (a)  K=5 K=10 K=20  100  :-  o lo  II  0 10  o lO 20  K = 4O K = 8O  o 'o 20  o lO  Figure 2: (a) Performance of probability propagation. Median inference error (bold curve)  on a logarithmic scale as a function of the number of iterations for different sizes of network  parameterized by K and N. The two curves adjacent to the bold curve show the range within  which 98% of the errors lie. 99.9% of the errors were below the fourth, topmost curve. (b)  The error, bottom-up variances and top-down means as a function of the number of iterations  (maximum of 20) for 5 divergent networks of size K = 5, N = 10.  To analyze the system of mean updates, we define the following length KN vec-  tors of means and the input: o(i) = (r/[?,r/?,... ,r/)l,r/?2),... ,r/()N)T , (i) =  ,.., , .., ,.., ,  ( ' '''' '/IK' 21,''' '  -- (Xl,Xl  Xl X2,  X2,XN  XN) T  where each Xn is repeated K times in the last vector. The network parameters are  represented using KN x KN diagonal matrices,  and . The diagonal of  is  An,... , AiK, A2,..  , AK, and the diagonal of  is I, 2I,... , I, where I is  the K x K identity matrix. The converged bottom-up variances are represented  using a diagonal matrix  with diagonal 11,-.- , elK, 21,... , NK.  The summation operations in the propagation formul are represented by a KN x  KN matrix  that sums over means sent down from the top layer and a KN x KN  matrix  that sums over means sent up from the sensory input:  z = 1 , = I ... (8)  These are N x N matrices of K x K blocks, where 1 is the K x K block of ones  and I is the K x K identity matrix.  Using the above representations, the bottom-up pass is given by  (i) = -i- -l(g _ i)o(i_) ' (9)  and the top-down pass is given by  0 () = (I+ diag(g-g) - -)-(g - I)- (). (10)  Substituting (10) into (9), we get the linear update for :  () = -- - (g - I)(I + diag(g-lg)- -1)- (g_ i)-(_1).  (11)  446 B. J. Fre  1.24 1.07 1.49 1.13 1.03 1.02 1.09 1.01 1.11 1.06  Figure 3: The error {log scale) versus number of iterations (log scale, max. of 1000) in 10  of the d)vergent networks with K -- 5, N = 10. The means were initialized to the fixed point  solutions and machine round-off erroFs cause divergence from the fixed points, whose eFFOFS  are shown by horizontam lines.  The fixed point of this dynamic system, when it exists, is  -S* = ( --(z -- I)(I + dig(z-lz)- +-1)-1( z - I))--l. (12)  A fixed point exists if the determinant of the expression in large braces in (12) is  nonzero. We have found a simplified expression for this determinant in terms of the  determinants of smaller, K x K matrices.  Reinterpreting the dynmics in (11) as dynamics for (), the stability of a fixed  point is determined by the lgest eigenvalue of the update matrix, (z -I)(I +  diag(z-z)--)-(z-I)- -. If the modulus of the largest eigenvalue  is less than 1, the fixed point is stable. Since the system is linear, if a stable fixed  point exists, the system will be globally convergent to this point.  Of the 200,000 networks we explored, about 99.9% of the networks converged. For  10 of the divergent networks with K  , N  10, we used 1000 iterations of prob-  ability propagation to compute the steady state variances. Then, we computed the  modulus of the largest eigenvalue of the system and we computed the fixed point.  After initializing the bottom-up means to the fixed point values, we performed 1000  iterations to see if numerical errors due to machine precision would cause divergence  from the fixed point. Fig. 3 shows the error versus number of iterations (on loga-  rithmic scales) for each network, the error of the fixed point, and the modulus of  the largest eigenvalue. In some cases, the network diverges from the fixed point and  reaches a dynamic equilibrium that h a lower average error than the fixed point.  3 Online factor analysis  To perform mimum likelihood factor analysis in an online fashion, each parameter  should be modified to slightly increase the log-probability of the current sensory  input, logp(x). However, since the factors are hidden, they must be probabilistically  "filled in" using inference before an incremental leaning step is performed.  If the estimated mean and variance of the kth factor are  and v, then it turns  out (e.g., Neal and Dayan, 1997) the parameters can be updated as follows:  + -  - - E=z + E=lV], (13)  where V is a learning rate.  Online learning consists of performing some number of iterations of probability prop-  agation for the current input (e.g., 4 iterations) and then modifying the pameters  before processing the next input.  3.1 Results on simulated data. We produced 95 training sets of 200 cases  each, with input sizes ranging from 20 sensors to 320 sensors. For each of 19 sizes  of factor analyzer, we randomly selected 5 sets of parameters as described above  and generated a trning set. The factor analyzer sizes were K G {5, 10, 20, 40, 80,  Local Probability Propagation for Factor Analysis 44 7  K=10 K=20 K=40  K=80  K=10 K=20 K=40 K----80  Figure 4: (a) Achievable errors after the same number of epochs of learning using 4 iterations  versus 1 iteration. The horizontal axis gives the log-probability error (log scale) for learning with  1 iteration and the vertical axis gives the error after the same number of epochs for learning  with 4 iterations. (b) The achievable errors for learning using 4 iterations of propagation versus  wake-sleep learning using 4 iterations.  N  {20, 40, 80, 160, 320}, N > K. For each factor analyzer and simulated data set,  we estimated the optimal log-probability of the data using 100 iterations of EM.  For learning, the size of the model to be trained was set equal to the size of the model  that was used to generate the data. To avoid the issue of how to schedule learning  rates, we searched for achievable learning curves, regardless of whether or not a  simple schedule for the learning rate exists. So, for a given method and randomly  initialized parameters, we performed one separate epoch of learning using each of  the learning rates, 1, 0.5,..., 0.520 and picked the learning rate that most improved  the log-probability. Each successive learning rate was determined by comparing the  performance using the old learning rate and one 0.75 times smaller.  We are mainly interested in comparing the achievable curves for different methods  and how the differences scale with K and N. For two methods with the same K  and N trained on the same data, we plot the log-probability error (optimal log-  probability minus log-probability under the learned model) of one method against  the log-probability error of the other method.  Fig. 4a shows the achievable errors using 4 iterations versus using 1 iteration. Usu-  ally, using 4 iterations produces networks with lower errors than those learned using  1 iteration. The difference is most significant for networks with large K, where in  Sec. 2.1 we found that the convergence of the inference error was slower.  Fig. 4b shows the achievable errors for learning using 4 iterations of probability  propagation versus wake-sleep learning using 4 iterations. Generally, probability  propagation achieves much smaller errors than wake-sleep learning, although for  small K wake-sleep performs better very close to the optimum log-probability. The  most significant difference between the methods occurs for large K, where aside  from local optima probability propagation achieves nearly optimal log-probabilities  while the log-probabilities for wake-sleep learning are still close to their values at  the start of learning.  4 Online face recognition  Fig. lb shows examples from a set of 30,000 20 x 28 greyscale face images of 18  different people. In contrast to other data sets used to test face recognition methods,  these faces include wide variation in expression and pose. To make classification  more difficult, we normalized the images for each person so that each pixel has  448 B. J. Frey  the same mean and variance. We used probability propagation and a recognition  network in a factor analyzer to reduce the dimensionality of the data online from  560 dimensions to 40 dimensions. For probability propagation, we rather arbitrarily  chose a learning rate of 0.0001, but for wake-sleep learning we tried learning rates  ranging from 0.1 down to 0.0001. A multilayer perceptron with one hidden layer of  160 tanh units and one output layer of 18 softmax units was simultaneously being  trained using gradient descent to predict face identity from the mean factors. The  learning rate for the multilayer perceptron was set to 0.05 and this value was used  for both methods.  For each image, a prediction was made before the pa-  rameters were modified. Fig. 5 shows online error  curves obtained by filtering the losses. The curve for  probability propagation is generally below the curves  for wake-sleep learning.  The figure also shows the error curves for two forms of  online nearest neighbors, where only the most recent  W cases are used to make a prediction. The form of  nearest neighbors that performs the worst has W set so  that the storage requirements are the same as for the  factor analysis / multilayer perceptron method. The  better form of nearest neighbors has W set so that the  number of computations is the same as for the factor  analysis / multilayer perceptron method.  Number of pattern presentations  Figure 5: Online error  curves for probability prop-  agation (solid), wake-sleep  learning (dashed), nearest  neighbors (dot-dashed)  5 Summary and guessing (dotted).  It turns out that iterative probability propagation can be fruitful when used for  learning in a graphical model with cycles, even when the model is densely con-  nected. Although we are more interested in extending this work to more complex  models where exact inference takes exponential time, studying iterative probability  propagation in the factor analyzer allowed us to compare our results with exact in-  ference and allowed us to derive the fixed point of the algorithm. We are currently  applying iterative propagation in multiple cause networks for vision problems.  References  C. Berrou and A. Glavieux 1996. Near optimum error correcting coding and decoding:  Turbo-codes. IEEE Trans. on Communications, 44, 1261-1271.  P. Dayan, G. E. Hinton, R. M. Neal and R. S. Zemel 1995. The Helmholtz machine.  Neural Computation 7, 889-904.  B. J. Frey and D. J. C. MacKay 1998. A revolution: Belief propagation in graphs with  cycles. In M. Jordan, M. Kearns and S. Solla (eds), Advances in Neural Information  Processing Systems 10, Denver, 1997.  B. J. Frey 1998. Graphical Models for Machine Learning and Digital Communication.  MIT Press, Cambridge MA. See http://vvv. cs. utoronto. ca/~frey.  G. E. Hinton, P. Dayan, B. J. Frey and R. M. Neal 1995. The wake-sleep algorithm for  unsupervised neural networks. Science 268, 1158-1161.  D. J. C. MacKay 1999. Information Theory, Inference and Learning Algorithms. Book in  preparation, currently available at http://wol. ra. phy. cam. ac. uk/mackay.  R. M. Neal and P. Dayan 1997. Factor analysis using delta-rule wake-sleep learning. Neural  Computation 9, 1781-1804.  P. $myth, R. J. McEliece, M. Xu, $. Aji and G. Horn 1997. Probability propagation in  graphs with cycles. Presented at the workshop on Inference and Learning in Graphical  Models, Vail, Colorado.  Y. Weiss 1998. Correctness of local probability propagation in graphical models. To  appear in Neural Computation.  
Robust Recognition of Noisy and Superimposed  Patterns via Selective Attention  Soo-Young Lee  Brain Science Research Center  Korea Advanced Institute of Science & Technology  Yusong-gu, Taejon 305-701 Korea  sylee@ee. kaist. ac. kr  Michael C. Mozer  Department of Computer Science  University of Colorado at Boulder  Boulder, CO 80309 USA  mozer@cs. colorado. edu  Abstract  In many classification tasks, recognition accuracy is low because input  patrems are corrupted by noise or are spatially or temporally  overlapping. We propose an approach to overcoming these limitations  based on a model of human selective attention. The model, an early  selection filter guided by top-down attentional control, entertains each  candidate output class in sequence and adjusts attentional gain  coefficients in order to produce a strong response for that class. The  chosen class is then the one that obtains the strongest response with the  least modulation of attention. We present simulation results on  classification of corrupted and superimposed handwritten digit pattems,  showing a significant improvement in recognition rates. The algorithm  has also been applied in the domain of speech recognition, with  comparable results.  1 Introduction  In many classification tasks, recognition accuracy is low because input patterns are  corrupted by noise or are spatially or temporally overlapping. Approaches have been  proposed to make classifiers more robust to such perturbations, e.g., by requiring  classifiers to have low input-to-output mapping sensitivity [1 ]. We propose an approach  that is based on human selective attention. People use selective attention to focus on  critical features of a stimulus and to suppress irrelevant features. It seems natural to  incorporate a selective-attention mechanism into pattern recognition systems for noisy  real world applications.  Psychologists have for many years studied the mechanisms of selective attention (e.g.,  [2]-[4]). However, controversy still exists among competing theories, and only a few  models are sufficiently well defined to apply to engineering pattem recognition problems.  Fukushima [5] has incorporated selective attention and attention-switching algorithms  into his Neocognitron model, and has demonstrated good recognition performance on  superimposed digits. However, the Neocognitron model has many unknown parameters  which must be determined heuristically, and its performance is sensitive to the parameter  values. Also, its computational requirements are prohibitively expensive for many real-  time applications. Rao [6] has also recently introduced a selective attention model based  32 S.- Y. Lee and M. C. Mozer  on Kalman filters and demonstrated classifications of superimposed patterns. However,  his model is based on linear systems, and a nonlinear extension is not straightforward.  There being no definitive approach to incorporating selective attention into pattern  recognition, we propose a novel approach and show it can improve recognition accuracy.  2 Psychological Views of Selective Attention  The modern study of selective attention began with Broadbent [7]. Broadbent presented  two auditory channels to subjects, one to each ear, and asked subjects to shadow one  channel. He observed that although subjects could not recall most of what took place in  the unshadowed channel, they could often recall the last few seconds of input on that  channel. Therefore, he suggested that the brain briefly stores incoming stimuli but the  stimulus information fades and is neither admitted to the conscious mind nor is encoded in  a way that would permit later recollection, unless attention is directed toward it. This view  is known as an early filtering or early selection model. Treisman [8] proposed a  modification to this view in which the filter merely attenuates the input rather than  absolutely preventing further analysis. Although late-selection and hybrid views of  attention have been proposed, it is clear that early selection plays a significant role in  human information processing [3].  The question about where attention acts in the stream of processing is independent of  another important issue: what factors drive attention to select one ear or one location  instead of another. Attention may be directed based on low-level stimulus features, such  as the amplitude of a sound or the color of a visual stimulus. This type of attentional  control is often called bottom up. Attention may also be directed based on expectations  and object knowledge, e.g., to a location where critical task-relevant information is  expected. This type of attentional control is often called top down.  3 A Multilayer Perceptron Architecture for Selective Attention  We borrow the notion of an early selection filter with top-down control and integrate it  into a multilayer perceptron (MLP) classifier, as depicted in Figure 1. The dotted box is a  standard MLP classifier, and an attention layer with one-to-one connectivity is added in  front of the input layer. Although we have depicted an MLP with a single hidden layer,  our approach is applicable to general MLP architectures. The kth element of the input  vector, denoted x k, is gated to the kth input of the MLP by an attention gain or filtering  coefficient ak. Previously, the first author has shown a benefit of treating the ak's like  ordinary adaptive parameters during training [9]-[12].  In the present work, we fix the attention gains at 1 during training, causing the architecture  to behave as an ordinary MLP. However, we allow the gains to be adjusted during  classification of test patterns. Our basic conjecture is that recognition accuracy may be  improved if attention can suppress noise along irrelevant dimensions and enhance a weak  signal along relevant dimensions. "Relevant" and "irrelevant" are determined by top-  down control of attention. Essentially, we use knowledge in the trained MLP to determine  which input dimensions are critical for classifying a test pattern. To be concrete, consider  an MLP trained to classify handwritten digits. When a test pattern is presented, we can  adjust the attentional gains via gradient descent so as to make the input as good an example  of the class "0" as possible. We do this for each of the different output classes, "0" through  "9", and choose the class for which the strongest response is obtained with the smallest  Robust Pattern Recognition via Selective Attention 33  attentional modulation (the exact quantitative rule is presented below). The conjecture is  that if the net can achieve a strong response for a class by making a small attentional  modulation, that class is more likely to be correct than whichever class would have been  selected without applying selective anention.  X10  x2 0  x3 0  XN O  al  a2  A  A  Yl  Y2  Y3  YM   W (1) h t2) y  Figure 1: MLP architecture for selective anention  The process of adjusting the attentional gains to achieve a strong response from a  particular class call it the attention class proceeds as follows. First, a target output  vector t s = [ts ts: ... tsM ]T is defined. For bipolar binary output representations, t,  = 1 is for  the anention class and -1 for the others. Second, the anention gain ak's are set to 1. Third,  E s )2  the anention gain ak's are adapted to minimize error --  Z (t,  - Y, with the given  input x -- [x x,... xN ]T and pre-trained and frozen synaptic weights W. The update rule is  based on a gradient-descent algorithm with error back-propagation. At the (n+l)'th  iterative epoch, the anention gain ak is updated as  ak[n+ l] = a[n]-rl(c?E / ca)[n]= a[n]+ r I :,,,, [n]  (la)  8(0) = "l (1) (1)  (lb)  where E denotes the anention output error, 8J l) thej'th attribute of the back-propagated  W(1)  error at the first hidden-layer, and - j the synaptic weight between the input  and the  j'th neuron at the first hidden layer. Finally, r/ is a step size. The anention gains are  thresholded to lie in [0, 1]. The application of selective anention to a test example is  summarized as follows:  Step 1: Apply a test input pattern to the trained MLP and compute output values.  Step 2: For each of the classes with top m activation values,  (1) Initialize all anention gain ak's to 1 and set the target vector t s.  (2) Apply the test pattern and anention gains to network and compute output.  (3) Apply the selective anention algorithm in Eqs.(1) to adapt the anention gains.  (4) Repeat steps (2) and (3) until the anention process converges.  (5) Compute an anention measure M on the asymptotic network state.  34 S.- Y. Lee and M. C. Mozer  Step 3: Select the class with a minimum attention measure M as the recognized class.  The attention measure is defined as  M --DtE o, (2a)  D I  --  (x k -.k )2/2N  --  Xk2 (1-ak )2 / 2N  (2b)  E o - .[t i -yi(:i)] 2/2M, (2c)  where Di is the square of Euclidean distance between two input patterns before and after  the application of selective attention and Eo is the output error after the application of  selective attention. Here, Di and Eo are normalized with the number of input pixels and  number of output classes, respectively. The superscript s for attention classes is omitted  for simplicity. To make the measure M a dimensionless quantity, one may normalize the  D and Eo with the input energy (,x) and the training output error, respectively.  However, it does not affect the selection process in Step 3.  One can think of the attended input :i as the minimal deformation of the test input needed  to trigger the attended class, and therefore the Euclidean distance between x and  is a  good measure for the classification confidence. In fact, D is basically the same quantity  minimized by Rao [6]. However, the MLP classifier in our model is capable of nonlinear  mapping between the input and output patterns. A nearest-neighbor classifier, with the  training data as examples, could also be used to find the minimum-distance class. Our  model with the MLP classifier computes a similar function without the large memory and  computational requirements.  The proposed selective attention algorithm was tested on recognition of noisy numeral  patterns. The numeral database consists of samples of the handwritten digits (0 through 9)  collected from 48 people, for a total of 480 samples. Each digit is encoded as a 16x16  binary pixel array. Roughly 16% of the pixels are black and coded as 1; white pixels are  coded as 0. Four experiments were conducted with different training sets of 280 training  patterns each. A one hidden-layer MLP was trained by back propagation. The numbers of  input, hidden, and output neurons were 256, 30, and 10, respectively. Three noisy test  patterns were generated from each training pattern by randomly flipping each pixel value  with a probability Pf, and the 840 test patterns were presented to the network for  classification.  In Figure 2, the false recognition rate is plotted as a function of the number of candidates  considered for the attentional manipulation, rn. (Note that the run time of the algorithm is  proportional to rn, but that increasing rn does not imply a more lax classification criterion,  or additional external knowledge playing into the classification.) Results are shown for  three different pixel inversion probabilities, Pf =0.05, 0.1, and 0.15. Considering the  average 16% of black pixels in the data, the noisy input patterns with Pf= O. 15 correspond  to a SNR of approximately 0 dB. For each condition in the figure, the false recognition  rates for the four different training sets are marked with an 'o', and the means are  connected by the solid curve.  Robust Pattern Recognition via Selective Attention 35  A standard MLP classifier corresponds  to rn = 1 (i.e., only the most active   output of the MLP is considered as a n- 1.s  candidate response). The false c  o  -.  recognition rate is clearly lower when '  the attentional manipulation is used to o  select a response from the MLP (m > 1). n-  It appears that performance does not  0.5  improve further by considering more ,?  than the top three candidates.  4 Attention Switching for  Superimposed Patterns  Suppose that we superimpose the 20  binary input patterns for two different  handwritten digits using the logical OR  operator (the pixels corresponding to  '.  the black ink have logical value 1).  Can we use attention to recognize the  two patterns in sequence? This is an  extreme case of a situation that is  o 5  common in visual pattem  recognition--where two patterns are  spatially overlapping. 0  We explore the following algorithm.  First, one pattern is recognized with the  selective attention process used in  Section 3. Second, attention is 40  switched from the recognized pattern to  the remaining pixels in the image. n- 30  Switching is accomplished by  o  removing attention from the pixels of c  the recognized pattern: the attentional  20  gain of an input is clamped to 0  following switching if and only if its  value after the first-stage selective  attention process was 1 (i.e., that input  was attended during the recognition of  the first pattern); all other gains are set  to 1. Third, the recognition process  with selective attention is performed  again to recognize the second pattem.  The proposed selective attention and  attention switching algorithm was  tested for recognition of 2  superimposed numeral data. Again,  four experiments were conducted with  1 2 3 4  Number of Candidates  (a) Pf=O.05,  1 2 3 4 5  Number of Candidates  (b) Pf=O. 1 O,  1 2 3 4 5  Number of Candidates  (c) Pf=O. 15,  Figure 2: False recognition rates for noisy  patterns as a function of the number of top  candidates. Each binary pixel of training  pattems is randomly inverted with a  rrobabilitv Pc.  36 $.-Y. Lee and M. C. Mozer  different training sets. For each experiment, 40 patterns were selected from 280 training  patterns, and 720 test patterns were generated by superimposing pairs of patterns from  different output classes. The test patterns were still binary.  Figure 3: Examples of Selective Attention and Attention Switching  Figure 3 shows six examples of the selective attention and attention switching algorithm in  action, each consisting of four panels in a horizontal sequence. The six examples were  formed by superimposing instances of the following digit pairs: (6,3), (9,0), (6,4), (9,3),  (2,6), and (5,2). The first panel for each example shows the superimposed pattern. The  second panel shows the attended input i for the first round classification; because this  input has continuous values, we have thresholded the values at 0.5 to facilitate viewing in  the figure. The third panel shows the masking pattern for attention switching, generated  by thresholding the input pattern at 1.0. The fourth panel shows the residual input pattern  for the second round classification. The attended input i has analog values, but  thresholded by 0.5 to be shown in the second rectangles. Figure 3 shows that attention  switching is done effectively, and the remaining input patterns to the second classifier are  quite visible.  We compared performance for three different methods. First, we simply selected the two  MLP outputs with highest activity; this method utilizes neither selective attention. Second,  we performed attention switching but did not apply selective attention (i.e., m= 1). Third,  we performed both attention switching and selective attention (with m=3). Table 1  summarizes the recognition rates for the first and the second patterns read out of the MLP  for the three methods. As hypothesized, attention switching increases the recognition rate  for the second pattern, and selective attention increases the recognition rate for both the  first and the second pattem.  Table 1: Recognition Rates (%) of Two Superimposed Numeral Pattems  First Pattern Second Pattern  No selective attention or switching 91.3 62.7  Switching only 91.3 75.4  Switching & selective attention 95.9 77.4  Robust Pattern Recognition via Selective Attention 37  5 Conclusion  In this paper, we demonstrated a selective-attention algorithm for noisy and superimposed  patterns that obtains improved recognition rates. We also proposed a simple attention  switching algorithm that utilizes the selective-attention framework to further improve  performance on superimposed patterns. The algorithms are simple and easily  implemented in feedforward MLPs. Although our experiments are preliminary, they  suggest that attention-based algorithms will be useful for extracting and recognizing  multiple patterns in a complex background. We have conducted further simulation studies  supporting this conjecture in the domain of speech recognition, which we will integrate  into this presentation if it is accepted at NIPS.  Acknowledgements  S.Y. Lee acknowledges supports from the Korean Ministry of Science and Technology.  We thank Dr. Y. Le Cun for providing the handwritten digit database.  References  [1] Jeong D.G., and Lee, S.Y. (1996). Merging backpropagation and Hebbian learning  rules for robust classification, Neural Networks, 9:1213-1222.  [2] Cowan, N. (1997). Attention and Memory: An Integrated Framework, Oxford Univ.  Press.  [3] Pashler, H.E. (1998). The Psychology of Attention, MIT Press.  [4] Parasuraman, R. (ed.) (1998). The Attentive Brain, MIT Press.  [5] Fukushima, IC (1987). Neural network model for selective attention in visual pattern  recognition and associative recall, Applied Optics, 26:4985-4992.  [6] Rao, R.P.N. (1998). Correlates of attention in a model of dynamic visual recognition.  In Neural Information Processing Systems 10, MIT Press.  [7] Broadbent, D.E. (1958). Perception and Communication. Pergamon Press.  [8] Treisman, A. (1960). Contextual cues in selective listening, Quarterly Journal of  Experimental Psychology, 12:242-248.  [9] Lee, H.J., Lee, S.Y. Lee, Shin, S.Y., and Koh, B.Y. (1991). TAG: A neural network  model for large-scale optical implementation, Neural Computation, 3:135-143.  [10]Lee, S.Y., Jang, J.S., Shin, S.Y., & Shim, C.S. (1988). Optical Implementation of  Associative Memory with Controlled Bit Significance, Applied Optics, 27:1921-  1923.  [11]Kruschke, J.K. (1992). ALCOVE: An Examplar-Based Connectionist Model of  Category Learning, Psychological Review, 99:22-44.  [12]Lee, S.Y., Kim, D.S., Ahn, K.H., Jeong, J.H., Kim, H., Park, S.Y., Kim, L.Y., Lee,  J.S., & Lee, H.Y. (1997). Voice Command II: a DSP implementation of robust speech  recognition in real-world noisy environments, International Conference on Neural  Information Processing, pp. 1051-1054, Dunedin, New Zealand.  
Recurrent cortical competition: Strengthen or  weaken?  P6ter AdorjJn% Lars Schwabe,  Christian Piepenbrock*, and Klaus Obermayer  Dept. of Comp. Sci., FR2-1, Technical University Berlin  Franklinstrasse 28/29 10587 Berlin, Germany  adorjan @epigenomics.com, { schwabe, oby } @ cs.tu-berlin.de,  piepenbrock@epigenomics.com  http://www. ni.cs.tu-berlin.de  Abstract  We investigate the short term dynamics of the recurrent competition and  neural activity in the primary visual cortex in terms of information pro-  cessing and in the context of orientation selectivity. We propose that af-  ter stimulus onset, the strength of the recurrent excitation decreases due  to fast synaptic depression. As a consequence, the network shifts from  an initially highly nonlinear to a more linear operating regime. Sharp  orientation tuning is established in the first highly competitive phase. In  the second and less competitive phase, precise signaling of multiple ori-  entations and long range modulation, e.g., by intra- and inter-areal con-  nections becomes possible (surround effects). Thus the network first ex-  tracts the salient features from the stimulus, and then starts to process  the details. We show that this signal processing strategy is optimal if  the neurons have limited bandwidth and their objective is to transmit the  maximum amount of information in any time interval beginning with the  stimulus onset.  1 Introduction  In the last four decades there has been a vivid and highly polarized discussion about the  role of recurrent competition in the primary visual cortex (V1) (see [12] for review). The  main question is whether the recurrent excitation sharpens a weakly orientation tuned feed-  forward input, or the feed-forward input is already sharply tuned, hence the massive re-  current circuitry has a different function. Strong cortical recurrency implements a highly  nonlinear mapping of the feed-forward input, and obtains robust and sharply tuned corti-  cal response even if only a weak or no feed-forward orientation bias is present [6, 11, 2].  However, such a competitive network in most cases fails to process multiple orientations  within the classical receptive field and may signal spurious orientations [7]. This moti-  vates the concept that the primary visual cortex maps an already sharply orientation tuned  feed-forward input in a less competitive (more linear) fashion [9, 13].  Although these models for orientation selectivity in V1 vary on a wide scale, they have  one common feature: each of them assumes that the synaptic strength is constant on the  short time scale on which the network operates. Given the phenomenon of fast synaptic  * Current address: Epigenomics GmbH, Kastanienallee 24, D-10435 Berlin, Germany  90 P Adorjtin, L. Schwabe, C. Piepenbrock and K. Oberrnayer  dynamics this, however, does not need to be the case. Short term synaptic dynamics, e.g.,  of the recurrent excitatory synapses would allow a cortical network to operate in both--  competitive and linear--regimes. We will show below (Section 2) that such a dynamic  cortical amplifier network can establish sharp contrast invariant orientation tuning from  a broadly tuned feed-forward input, while it is still able to respond correctly to multiple  orientations.  We then show (Section 3) that decreasing the recurrent competition with time naturally fol-  lows from functional considerations, i.e. from the requirement that the mutual information  between stimuli and representations is maximal for any time interval beginning with stimu-  lus onset. We consider a free-viewing scenario, where the cortical layer represents a series  of static images that are flashed onto the retina for a fixation period (AT -- 200 - 300 ms)  between saccades. We also assume that the spike count in increasing time windows after  stimulus onset carries the information. The key observations are that the signal-to-noise  ratio of the cortical representation increases with time (because more spikes are available)  and that the optimal strength of the recurrent connections (w.r.t. information transfer) de-  creases with the decreasing output noise. Consequently the model predicts that the infor-  mation content per spike (or the SNR for afixed sliding time window) decreases with time  for a flashed static stimulus in accordance with recent experimental studies. The neural  system thus adapts to its own internal changes by modifying its coding strategy, a phe-  nomenon which one may refer to as "dynamic coding"  2 Cortical amplifier with fast synaptic plasticity  To investigate our first hypothesis, we set up a model for an orientation-hypercolumn in  the primary visual cortex with similar structure and parameters as in [7]. The important  novel feature of our model is that fast synaptic depression is present at the recurrent ex-  citatory connections. Neurons in the cortical layer receive orientation-tuned feed-forward  input from the LGN and they are connected via a Mexican-hat shaped recurrent kernel in  orientation space. In addition, the recurrent and feed-forward excitatory synapses exhibit  fast depression due to the activity dependent depletion of the synaptic transmitter [1, 14].  We compare the response of the cortical amplifier models with and without fast synap-  tic plasticity at the recurrent excitatory connections to single and multiple bars within the  classical receptive field.  The membrane potential V(O, t) of a cortical cell tuned to an orientation 0 decreases due  to the leakage and the recurrent inhibition, and increases due to the recurrent excitation  T-tV(O,t ) q- V(0, t) : ILGN(0, t) q- Iexc(0, t) -- Iinh(0, t),  (1)  where r = 15 ms is the membrane time constant and IIa(O, t) is the input received from  the LGN. The recurrent excitatory and inhibitory cortical inputs are given by  I(O,t) = J(O,O',t) exp 2 '  2  (2)  where A(O t, O) is arr periodic circular difference between the preferred orientations,  d(O, 0 , t) are the excitatory and inhibitory connection strengths (with a C {exc, inh},  drx --- 0.2 mV/Hz and dh x -- 0.8mV/Hz), and f is the presynaptic firing rate. The ex-  citatory synaptic efficacy jexc is time dependent due to the fast synaptic depression, while  the efficacy of inhibitory synapses jinh is assumed to be constant. The recurrent excitation  is sharply tuned O'exc = 7.5 , while the inhibition has broad tuning O'in h = 90 o . The map-  ping from the membrane potential to firing rate is approximated by a linear function with  a threshold at 0 (f(O) =/3max(0, V(O)),/3: 15Hz/mV). Gaussian-noise with variances  Recurrent Cortical Competition: Strengthen or Weaken? 91  Feedforward Input  -45 0 45  Orientation [deg]  Static  15 ' ' '  -0906 ']['5 "-0 a  Orientation [deg]  Depressing  15 . ,   -9'o  Orientation [deg]  (a) (b) (c)  Figure 1: The feed-forward input (a), and the response of the cortical amplifier model with  static recurrent synaptic strength (b), and a network with fast synaptic depression (c) if the  stimulus is single bar with different stimulus contrasts (40%dotted; 60%dashed; 80%solid  line). The cortical response is averaged over the first 100 ms after stimulus onset.  of 6 Hz and 1.6 Hz is added to the input intensities and to the output of cortical neurons.  The orientation tuning curves of the feed-forward input I LaN are Gaussians (rrLaN = 18 )  resting on a strong additive orientation independent component which would correspond to  a geniculo-cortical connectivity pattern with an approximate aspect ratio of 1:2. Both, the  orientation dependent and independent components increase with contrast. Considering a  free-viewing scenario where the environment is scanned by saccading around and fixating  for short periods of 200 - 300 ms we model stationary stimuli present for 300 ms. The  stimuli are one or more bars with different orientations.  Feed-forward and recurrent excitatory synapses exhibit fast depression. Fast synaptic de-  p_ression is modeled by the dynamics of the expected synaptic transmitter or "resource"  R(t) for each synapse. The amount of the available transmitter decreases proportionally to  the release probability p and to the presynaptic firing rate f, and it recovers exponentially  _LGN _Ctx pLGN pCtx  *rec = 120 ms, rrec= 850 ms, = 0.35 and = 0.55),  t(t) : 1- (t) f(t)p(t)(t) = (t) 1  r rfr(f(t),p(t)) I-  (3)  Trec  The change of the membrane potential on the postsynaptic cell at time t is proportional  to the released transmitter pR(t). The excitatory connectivity strength between neurons  tuned to orientations 0 and 0' is expressed as jxc (0, 0', t) = dmxPROo, (t). Similarly this  applies to the feed-forward synapses. Fast synaptic plasticity at the feed-forward synapses  has been investigated in more detail in previous studies [3, 4].  In the following, we compare the predictions of the cortical amplifier model with and with-  out fast synaptic depression at the recurrent excitatory connections. In both cases fast  synaptic depression is present at the feed-forward connections limiting the duration of the  effective feed-forward input to 200 - 400 ms. Figure 1 shows the orientation tuning curves  at different stimulus contrasts. The feed-forward input is noisy and broadly tuned (Fig.  la). Both models exhibit contrast invariant tuning (Fig. lb, c). If fast synaptic depression  is present at the recurrent excitation, the cortical network sharpens the broadly tuned feed-  forward input in the initial response phase. Once sharply tuned input is established, the  tuning width does not change, only the response amplitude decreases in time.  The predictions of the two models differ substantially if multiple orientations are present  (Fig. 2). At first, we test the cortical response to two bars separated by 60 o with differ-  ent intensities (Figs. 2a, b). If the recurrent synaptic weights are static and strong enough  (Fig. 2a), then only one orientation is signaled. The cortical network selects the orientation  92 P Adorjdn, L. Schwabe, C. Piepenbrock and K. Obermayer  (a)  (b)  (c)  (d)  Feedforward Input  10  090 -45 0 45 90  0  Orientation [deg] wlO  5  ZO  09 90  Orientation [deg]  Average Cortical Response  20 ,;.  10  z _09 ""  '"" ./  0 -45 0 45 90  Orientation [deg]  90  -90  90  o  -90  9o  io  -90  9O  o  -900  Activity Profile  150 300  Time [ms]  Figure 2: The response of the cortical amplifier model with static (a,c) and fast depressing  recurrent synapses (b, d). In both models the feed-forward synapses are fast depressing. In  the left column the feed-forward input is shown, that is same for both models. Two types  of stimuli were applied. The first stimulus consists of a stronger (c = -30 ) and a weaker  bar (a -- +30 ) (a, b); the second stimulus consists of three equal intensity bars with  orientations that are separated by 60 o (c, d). In the middle column the cortical response is  shown averaged for different time windows ([0..30] dotted; [0..80] dashed; [200..300] solid  line). In the right column the cortical activity profile is plotted as a function of time. Gray  values indicate the activity with bright denoting high activities.  with the highest amplitude in a winner-take-all fashion. In contrast, if synaptic depression  is present at the recurrent excitatory synapses, both bars are signaled in parallel (at low  release probability, Fig. 2b) or after each other (high release probability, data not shown).  First, those cells fire which are tuned to the orientation of the bar with the stronger inten-  sity, and a sharply tuned response emerges at a single orientation--the network operates in  a winner-take-all regime. The synapses of these highly active cells then become strongly  depressed and cortical competition decreases. As the network is shifted to a more linear  operation regime, the second orientation is signaled too. Note that this phenomenon--  together with the observed contrast invariant tuning-cannot be reproduced by simply de-  creasing the static synaptic weights in the cortical amplifier model. The recurrent synap-  tic efficacy changes inhomogeneously in the network depending on the activity. Only the  synapses of the highly active cells depress strongly, and therefore a sharply tuned response  can be evoked by a bar with weak intensity. Fast synaptic depression thus behaves as a lo-  cal self-regulation that modulates competition with a certain delay. This delay, and there-  fore the delay of the rise of the response to the second bar depends on the efftive time  constant ref(f(t),p) = free/(1 + pf(t)rrec) of the synaptic depression at the recurrent  connections. If the depression becomes faster due to an increase in the release probabil-  ity p, then the delay decreases. The delay also scales with the difference between the bar  intensities. The closer to each other they are, the shorter the delay will be.  In Figs. 2c, d the cortical response to three bars with equal intensities is presented. Cells  tuned to the presented three orientations respond in parallel if fast synaptic depression at  the recurrent excitation is present (Figs. 2d). The cortical network with strong static recur-  rent synapses again fails to signal faithfully its feed-forward input. Additive noise on the  Recurrent Cortical Competition: Strengthen or Weaken? 93  feed-forward input introduces a slight symmetry breaking and the network with static re-  current weights responds strongly at the orientation of only one of the presented bars (Fig.  2c).  In summary, our simulations revealed that a recurrent network with fast synaptic depres-  sion is capable of obtaining robust sharpening of its feed-forward input and it also re-  sponds correctly to multiple orientations. Note that other local activity dependent adapta-  tion mechanisms, such as slow potassium current, would have similar effects as the synap-  tic depression on the highly orientation specific excitatory connections. An experimentally  testable prediction of our model is that the response to a flashed bar with lower contrast  can be delayed by masking it with a second bar with higher contrast (Fig. 2b, right). We  also suggest that long range integration from outside of the classical receptive field could  emerge with a similar delay. In the initial phase of the cortical response, strong local fea-  tures are amplified. In the longer, second phase, recurrent competition decreases and then  weak modulatory recurrent or feed-forward input has a stronger relative effect. In the fol-  lowing, we investigate whether this strategy is favorable from the point of view of cortical  encoding.  3 Dynamic coding  In the previous section we have proposed that during cortical processing a highly nonlin-  ear phase is followed by a more linear mode if we consider a short stimulus presentation  or a fixation period. The simulations demonstrated that unless the recurrent competition  is modulated in time, the network fails to account for more than one feature in its input.  From a strictly functional point of view the question arises, why not to use weak recurrent  competition during the whole processing period. We investigate this problem in an abstract  signal-encoder framework  !7 = g() + q, (4)  where  is the input to the "cortical network", g() is a nonlinear mapping and--for the  sake of simplicity--r/is additive Gaussian noise. Naturally, in a real recurrent network  output noise becomes input noise because of the feedback. Here we use the simplifying  assumption that only output noise is present on the transformed input signal (input noise  would lead to different predictions that should be further investigated). Output noise can  be interpreted as a noisy channel that projects out from, e.g., the primary visual cortex.  The nonlinear transformation g(') here is considered as a functional description of a cor-  tical amplifier network without analyzing how actually it is "implemented". Considering  orientation selectivity, the signal ' can be interpreted as a vector of intensities (or contrasts)  of edges with different orientations. Edges which are not present have zero intensity. The  coding capacity of a realistic neural network is limited. Among several other noise sources,  this limitation could arise from imprecision in spike timing and a constraint on the maximal  or average firing rate.  The input-output mapping g() of a cortical amplifier network is approximated with the  soft-max function  exp(flxi) (5)  gi(') = 4 exp(flxi) '  The fi parameter can be interpreted as the level of recurrent competition. As fi - 0 the  network operates in a more linear mode, while  -+ oo puts it into a highly nonlinear  winner-take-all mode. In all cases the average activity in the network is constrained which  has been suggested to minimize metabolic costs [5]. Let us consider a factorizing input  distribution,  1II. exp (--) forx> 0, (6)  z -  94 P Adorjdn, L. Schwabe, C. Piepenbrock and K. Obermayer  6  0.05 0.1 0.15  Noise (stdev)  Figure 3: The optimal competition  parameter /3 as a function of the  standard deviation of the Gaussian  output noise r/. The optimal/3 is cal-  culated for highly super-Gaussian,  Gaussian, and sub-Gaussian stimu-  lus densities. The sparsity parame-  ter a is indicated in the legend.  where the exponent a determines the sparsity of the probability density function, Z is a  normalizing constant, and  determines the variance. If a = 2, the input density is the  positive half of a multivariate Gaussian distribution. With c > 2 the signal distribution  becomes sub-Gaussian, and with a < 2 it becomes super-Gaussian.  For optimal processing in time one needs to gain the maximal information about the signal  for any increasing time window. Let us assume that the stimulus is static and it is pre-  sented for a limited time. As time goes ahead after stimulus onset, the time window for the  encoding and the read-out mechanism increases. During a longer period more samples of  the noisy network output are available, and thus the output noise level decreases with time.  We suggest that the optimal competition parameter/3Pt--at which the mutual informa-  tion between input  and output ff (Eq. 4) is maximized--depends on the noise level. As  the noise decreases with time,/3 or the recurrent cortical competition should also change  during cortical processing. To demonstrate this idea, the mutual information is calculated  numerically for a three-dimensional state space.  One might expect that at higher noise levels the highest information transfer can be ob-  tained if the typical and salient features are strongly amplified. Note that this is only true  if the standard deviation of the noise scales sub-linearly with activity, which is true for an  additive noise process as well as Poisson firing. As noise decreases (e.g., with increas-  ing the time window for estimation), the level of competition should decrease distributing  the available resources (e.g., spikes) among more units and letting the network respond to  finer details at the input. Investigating the level of optimal competition/3 as a function of  the standard deviation of the output noise (Fig. 3) this intuition is indeed justified. The  optimal/3 scales with the standard deviation of the additive noise process. Comparing sig-  nal distributions with the same variance but with different sparsity exponents a, we find  that the sparser the signal distribution is, the higher the optimal competition becomes, be-  cause multiple features are unlikely to be present at the same time if the input distribution  is sparse. By enforcing competition, the optimal encoding strategy also generates an ac-  tivity distribution where only few units fire for a presented stimulus. Since edges with dif-  ferent orientations form a sparse distributed representation of natural scenes [8], our work  suggests that a strongly competitive visual cortical network could achieve a better perfor-  mance on our visual environment than a simple linear network would do.  We can now interpret our simulation results presented in the Section 2 from a functional  point of view and give a prediction for the dynamics of the recurrent cortical competition.  Noting that the output noise is decreasing with increasing time-window for encoding, the  cortical competition should also decrease following a similar trajectory as presented in Fig.  3. If competition is low and static, then the cumulative mutual information between input  and output would converge only slowly towards the overall information that is available  in the stimulus. If the competition is high during the whole observation period, then af-  ter a fast rise the cumulative mutual information would saturate well below the possible  Recurrent Cortical Competition: Strengthen or Weaken? 95  maximum. If the level of competition is dynamic, and it decreases from an initially highly  competitive state, then the network obtains maximal information transfer in time.  One may argue that the valuable information about the signals mainly depends on the in-  terest of the observer. Considering an encoding system for one variable it has been sug-  gested that in a highly attentive state the recurrent competition increases [10]. In the view  of our results we would refine this statement by suggesting that competition increases or  decreases depending on the level of visual detail the observer pays attention to. Whenever  representation of small details is also required, reducing competition is the optimal strategy  given enough bandwidth.  In summary, using a detailed model for an orientation hypercolumn in V1 we have demon-  strated that sharp contrast invariant tuning and faithful representation of multiple features  can be achieved by a recurrent network if the recurrent competition decreases in time after  stimulus onset. The model predicts that the cortical response to weak details in the stim-  ulus emerges with a delay if a second stronger feature is also present. The modulation  from, e.g., outside of the classical receptive field also has a delayed effect on cortical activ-  ity. Our study within an abstract framework revealed that weakening the recurrent cortical  competition on a fast time scale is functionally advantageous, because a maximal amount  of information can be transmitted in any time window after stimulus onset.  Acknowledgments Supported by the Boehringer Ingelheim Fonds (C. P.), by the Ger-  man Science Foundation (DFG grant GK 120-2) and by Wellcome Trust 050080/Z/97.  References  [1] L. E Abbott, J. A. Varela, K. Sen, and S. B. Nelson. Synaptic depression and cortical gain  control. Science, 275:220-224, 1997.  [2] P. Adorjfin, J.B. Levitt, J.S. Lund, and K. Obermayer. A model for the intracortical origin of  orientation preference and tuning in macaque striate cortex. Vis. Neurosci., 16:303-318, 1999.  [3] P. Adorjfin, C. Piepenbrock, and K. Obermayer. Contrast adaptation and info-  max in visual cortical neurons. Rev. Neurosci., 10:181-200, 1999. ftp://ftp.cs.tu-  berlin.de/pub/local/ni/papers/adp99-contrast.ps.gz.  [4] O. B. Artun, H. Z. Shouval, and L. N. Cooper. The effect of dynamic synapses on spatiotem-  poral receptive fields in visual cortex. Proc. Natl. Acad. Sci., 95:11999-12003, 1998.  [5] R. Baddeley. An efficient code in VI? Nature, 381:560-561, 1996.  [6] R. Ben-Yishai, R. Lev Bar-Or, and H. Sompolinsky. Theory of orientation tuning in visual  cortex. Proc. Natl. Acad. Sci., 92:3844-3848, 1995.  [7] M. Carandini and D. L. Ringach. Predictions of a recurrent model of orientation selectivity.  Vision Res., 37:3061-3071, 1997.  [8] D. J. Field. What is the goal of sensory coding. Neural Cornput., 6:559-601, 1994.  [9] D. H. Hubel and T. N. Wiesel. Receptive fields, binocular interaction and functional architec-  ture in cat's visual cortex. J. Physiol., 165:559-568, 1962.  [10] D.K. Lee, L. Itti, C. Kock, and J. Braun. Attention activates winner-take-all competition among  visual filters. Nat. Neurosci., 2:375-381, 1999.  [11] D.C. Somers, S. B. Nelson, and M. Sur. An emergent model of orientation selectivity in cat  visual cortical simple cells. J. Neurosci., 15:5448-65, 1995.  [ 12] H. Sompolinsky and R. Shapley. New perspectives on the mechanisms for orientation selectiv-  ity. Curr. Op. in Neurobiol., 7:514-522, 1997.  [13] T. W. Troyer, A. E. Krukowski, N.J. Priebe, and K. D. Miller. Contrast-invariant orientation  tuning in visual cortex: Feedforward tuning and correlation-based intracortical connectivity. J.  Neurosci., 18:5908-5927, 1998.  [14] M. V. Tsodyks and H. Markram. The neural code between neocortical pyramidal neurons de-  pends on neurotransmitter release probability. Proc. Natl. Acad. Sci., 94:719-723, 1997.  
Spiking Boltzmann Machines  Geoffrey E. Hinton  Gatsby Computational Neuroscience Unit  University College London  London WCiN 3AR, UK  hinton @gatshy. ucl. ac. uk  Andrew D. Brown  Department of Computer Science  University of Toronto  Toronto, Canada  andy@cs. utoronto. ca  Abstract  We first show how to represent sharp posterior probability distribu-  tions using real valued coefficients on broadly-tuned basis functions.  Then we show how the precise times of spikes can be used to con-  vey the real-valued coefficients on the basis functions quickly and  accurately. Finally we describe a simple simulation in which spik-  ing neurons learn to model an image sequence by fitting a dynamic  generafive model.  I Population codes and energy landscapes  A perceived object is represented in the brain by the activities of many neurons, but  there is no general consensus on how the activities of individual neurons combine to  represent the multiple properties of an object. We start by focussing on the case of  a single object that has multiple instantiation parameters such as position, velocity,  size and orientation. We assume that each neuron has an ideal stimulus in the space  of instantiation parameters and that its activation rate or probability of activation  falls off monotonically in all directions as the actual stimulus departs from this ideal.  The semantic problem is to define exactly what instantiation parameters are being  represented when the activities of many such neurons are specified.  Hinton, Rumelhart and McClelland (1986) consider binary neurons with receptive  fields that are convex in instantiation space. They assume that when an object  is present it activates all of the neurons in whose receptive fields its instantiation  parameters lie. Consequently, if it is known that only one object is present, the  parameter values of the object must lie within the feasible region formed by the  intersection of the receptive fields of the active neurons. This will be called a con-  junctive distributed representation. Assuming that each receptive field occupies  only a small fraction of the whole space, an interesting property of this type of  "coarse coding" is that the bigger the receptive fields, the more accurate the repre-  sentation. However, large receptive fields lead to a loss of resolution when several  objects are present simultaneously.  When the sensory input is noisy, it is impossible to infer the exact parameters of  objects so it makes sense for a perceptual system to represent the probability dis-  tribution across parameters rather than just a single best estimate or a feasible  region. The full probability distribution is essential for correctly combining infor-  Spiking Boltzmann Machines 123  E(x)  P(x)  Figure 1: a) Energy landscape over a one-  dimensional space. Each neuron adds a  dimple (dotted line) to the energy land-  scape (solid line). b) The corresponding  probability density. Where dimples over-  lap the corresponding probability density  becomes sharper. Since the dimples decay  to zero, the location of a sharp probabili-  ty peak is not affected by distant dimples  and multimodal distributions can be rep-  resented.  marion from different times or different sources. One obvious way to represent this  distribution (Anderson and van Essen, 1994) is to allow each neuron to represent  a fairly compact probability distribution over the space of instantiation parameters  and to treat the activity levels of neurons as (unnormalized) mixing proportions.  The semantics of this disjunctive distributed representation is precise, but the per-  cepts it allows are not because it is impossible to represent distributions that are  sharper than the individual receptive fields and, in high-dimensional spaces, the  individual fields must be broad in order to cover the space. Disjunctive represen-  tations are used in Kohonen's self-organizing map which is why it is restricted to  very low dimensional latent spaces.  The disjunctive model can be viewed as an attempt to approximate arbitrary smooth  probability distributions by adding together probability distributions contributed  by each active neuron. Coarse coding suggests a multiplicative approach in which  the addition is done in the domain of energies (negative log probabilities). Each  active neuron contributes an energy landscape over the whole space of instantiation  parameters. The activity level of the neuron multiplies its energy landscape and the  landscapes for all neurons in the population are added (Figure 1). If, for example,  each neuron has a full covariance Gaussian tuning function, its energy landscape  is a parabolic bowl whose curvature matrix is the inverse of the covariance matrix.  The activity level of the neuron scales the inverse covariance matrix. If there are  k instantiation parameters then only k -F k(k -F 1)/2 real numbers are required to  span the space of means and inverse covariance matrices. So the real-valued activ-  ities of O(k 2) neurons are sufficient to represent arbitrary full covariance Gaussian  distributions over the space of instantiation parameters.  Treating neural activities as multiplicative coefficients on additive contributions to  energy landscapes has a number of advantages. Unlike disjunctive codes, vague  distributions are represented by low activities so significant biochemical energy is  only required when distributions are quite sharp. A central operation in Bayesian  inference is to combine a prior term with a likelihood term or to combine two  conditionally independent likelihood terms. This is trivially achieved by adding  two energy landscapes  .  We thank Zoubin Ghahramani for pointing out that another important operation,  convolving a probability distribution with Gaussian noise, is a difficult non-linear operation  on the energy landscape.  124 G. E. Hinton and A.D. Brown  2 Representing the coefficients on the basis functions  To perform perception at video rates, the probability distributions over instantiation  parameters need to be represented at about 30 frames per second. This seems  difficult using relatively slow spiking neurons because it requires the real-valued  multiplicative coefficients on the basis functions to be communicated accurately and  quickly using all-or-none spikes. The trick is to realise that when a spike arrives  at another neuron it produces a postsynaptic potential that is a smooth function  of time. So from the perspective of the postsynaptic neuron, the spike has been  convolved with a smooth temporal function. By adding a number of these smooth  functions together, with appropriate temporal offsets, it is possible to represent any  smoothly varying sequence of coefficient values on a basis function, and this makes  it possible to represent the temporal evolution of probability distributions as shown  in Figure 2. The ability to vary the location of a spike in the single dimension of  time thus allows real-valued control of the representation of probability distributions  over multiple spatial dimensions.  a) b)  Encoded Value Time  time  neuron 2  neuron 1  ..,.' /  /..,  Figure 2: a)Two spiking neurons centered at 0 and 1 can represent the time-varying  mean and standard deviation on a single spatial dimension. The spikes are first  convolved with a temporal kernel and the resulting activity values are treated as  exponents on Gaussian distributions centered at 0 and 1. The ratio of the activi-  ty values determines the mean and the sum of the activity values determines the  inverse variance. b) The same method can be used for two (or more) spatial di-  mensions. Time flows from top to bottom. Each spike makes a contribution to the  energy landscape that resembles an hourglass (thin lines). The waist of the hour-  glass corresponds to the time at which the spike has its strongest effect on some  post-synaptic population. By moving the hourglasses in time, it is possible to get  whatever temporal cross-sections are desired (thick lines) provided the temporal  sampling rate is comparable to the time course of the effect of a spike.  Our proposed use of spike timing to convey real values quickly and accurately does  not require precise coincidence detection, sub-threshold oscillations, modifiable time  delays, or any of the other paraphernalia that has been invoked to explain how the  brain could make effective use of the single, real-valued degree of freedom in the  timing of a spike (Hopfield, 1995).  The coding scheme we have proposed would be far more convincing if we could  show how it was learned and could demonstrate that it was effective in a simula-  tion. There are two ways to design a learning algorithm for such spiking neurons.  We could work in the relatively low-dimensional space of the instantiation param-  eters and design the learning to produce the right representations and interactions  between representations in this space. Or we could treat this space as an implicit  emergent property of the network and design the learning algorithm to optimize  Spiking Boltzmann Machines 125  some objective function in the much higher-dimensional space of neural activities  in the hope that this will create representations that can be understood using the  implicit space of instantiation parameters. We chose the latter approach.  3 A learning algorithm for restricted Boltzmann machines  Hinton (1999) describes a learning algorithm for probabilistic generative models  that are composed of a number of experts. Each expert specifies a probability  distribution over the visible variables and the experts are combined by multiplying  these distributions together and renormalizing.  IlmPm(dlOm)  P(dlO'"O') = Y-]i IlmPm(CilOm)  (1)  where d is a data vector in a discrete space, 0, is all the parameters of individual  model m, pm(dlOm ) is the probability of d under model m, and i is an index over  all possible vectors in the data space.  The coding scheme we have described is just a product of experts in which each  spike is an expert. We first summarize the Product of Experts learning rule for a  restricted Boltzmann machine (RBM) which consists of a layer of stochastic binary  visible units connected to a layer of stochastic binary hidden units with no intralayer  connections. We then extend RBM's to deal with temporal data.  In an RBM, each hidden unit is an expert. When it is off it specifies a uniform  distribution over the states of the visible units. When it is on, its weight to each  visible unit specifies the log odds that the visible unit is on. Multiplying together  the distributions specified by different hidden units is achieved by adding the log  odds. Inference in an RBM is much easier than in a causal belief net because there  is no explaining away. The hidden states, sj, are conditionally independent given  the visible states, si, and the distribution of sj is given by the standard logistic  function a: p(sj = 1) = a('] i wijsi). Conversely, the hidden states of an RBM are  marginally dependent so it is easy for an RBM to learn population codes in which  units may be highly correlated. It is hard to do this in causal belief nets with one  hidden layer because the generative model of a causal belief net assumes marginal  independence.  An RBM can be trained by following the gradient of the log likelihood of the data:  Awij: e (< sisj >0 _ < sisj  (2)  where < $i$j >0 is the expected value of $i$j when data is clamped on the visible  units and the hidden states are sampled from their conditional distribution given the  data, and < $i$j >x) is the expected value of sisj after prolonged Gibbs sampling  that alternates between sampling from the conditional distribution of the hidden  states given the visible states and vice versa.  This learning rule not work well because the sampling noise in the estimate of  < $i$j x) swamps the gradient. It is far more effective to maximize the difference  between the log likelihood of the data and the log likelihood of the one-step recon-  structions of the data that are produced by first picking binary hidden states from  their conditional distribution given the data and then picking binary visible states  from their conditional distribution given the hidden states. The gradient of the log  126 G. E. Hinton and A.D. Brown  likelihood of the one-step reconstructions is complicated because changing a weight  changes the probability distribution of the reconstructions:  OL1 1 >c OQ 1 OL1  cqwij - < sisj > - < sisj + Owij x OQ 1 (3)  where Q is the distribution of the one-step reconstructions of the training data and  Q is the equilibrium distribution (i.e. the stationary distribution of prolonged  Gibbs sampling). Fortunately, the cumbersome third term is sufficiently small that  ignoring it does not prevent the vector of weight changes from having a positive  cosine with the true gradient of the difference of the log likelhoods so the following  very simple learning rule works much better than Eq. 2.  Awij -- e (< SiSj >0 _ < SiSj >1)  (4)  4 Restricted Boltzmann machines through time  Using a restricted Boltzmann machine we can represent time by spatializing it, i.e.  taking each visible unit, i, and hidden unit, j, and replicating them through time  with the constraint that the weight Wij r between replica t of i and replica t + r  of j does not depend on t. To implement the desired temporal smoothing, we also  force the weights to be a smooth function of r that has the shape of the temporal  kernel, shown in Figure 3. The only remaining degree of freedom in the weights  between replicas of i and replicas of j is the scale of the temporal kernel and it is  this scale that is learned. The replicas of the visible and hidden units still form  a bipartite graph and the probability distribution over the hidden replicas can be  inferred exactly without considering data that lies further into the future than the  width of the temporal kernel.  One problem with the restricted Boltzmann machine when we spatialize time is  that hidden units at one time step have no memory of their states at previous time  steps; they only see the data. If we were to add undirected connections between  hidden units at different time steps, then the architecture would return to a fully  connected Boltzmann machine in which the hidden units are no longer conditionally  independent given the data. A useful trick borrowed from Elman nets is to allow the  hidden units to see their previous states, but to treat these observations like data  that cannot be modified by future hidden states. Thus, the hidden states may still  be inferred independently without resorting to Gibbs sampling. The connections  between hidden layer weights also follow the time course of the temporal kernel.  These connections act as a predictive prior over the hidden units. It is important  to note that these forward connections are not required for the network to model a  sequence, but only for the purposes of extrapolating into the future.  Figure 3: The form of the temporal kernel.  Spiking Boltzmann Machines 12 7  Now the probability that sj (t) = 1 given the states of the visible units is,  P(sj(t)=l)=er(i wijhi(t)  +  k  where hi(t) is the convolution of the history of visible unit i with the temporal  kernel,  h(t) = s(t-  and h(t), the convolution of the hidden unit history, is computed similarly.  Learning the weights follows immediately from this formula for doing inference. In  the positive phase the visible units are clamped at each time step and the posterior  of the hidden units conditioned on the data is computed (we assume zero boundary  conditions for time before t = 0). Then in the negative phase we sample from the  posterior of the hidden units, and compute the distribution over the visible units  at each time step given these hidden unit states. In each phase the correlations  between the hidden and visible units are computed and the learning rule is,  t----0 r----O  5 Results  We trained this network on a sequence of 8x8 synthetic images of a Gaussian blob  moving in a circular path. In the following diagrams we display the time sequence  of images as a matrix. Each row of the matrix represents a single image with its  pixels stretched out into a vector in scanline order, and each column is the time  course of a single pixel. The intensity f the pixel is represented by the area of the  white patch. We used 20 hidden units. Figure 5a shows a segment (200 time steps)  of the time series which was used in training. In this sequence the period of the  blob is 80 time steps.  Figure 5b shows how the trained model reconstructs the data after we sample from  the hidden layer units. Once we have trained the model it is possible to do fore-  casting by clamping visible layer units for a segment of a sequence and then doing  iterative Gibbs sampling to generate future points in the sequence. Figure 5c shows  that given 50 time steps from the series, the model can predict reasonably far into  the future, before the pattern dies out. One problem with these simulations is that  we are treating the real valued intensities in the images as probabilities. While this  works for the blob images, where the values can be viewed as the probabilities of  pixels in a binary image being on, this is not true for more natural images.  6 Discussion  In our initial simulations we used a causal sigmoid belief network (SBN) rather  than a restricted Boltzmann machine. Inference in an SBN is much more difficult  than in an RBM. It requires Gibbs sampling or severe approximations, and even  if a temporal kernel is used to ensure that a replica of a hidden unit at one time  2Computing the conditional probability distribution over the visible units given the  hidden states is done in a similar fashion, with the caveat that the weights in each direction  must be symmetric. Thus, the convolution is done using the reverse kernel.  128 G. E. Hinton and A.D. Brown  Figure 4: a) The original data, b) reconstruction of the data, and c) prediction of  the data given 50 time steps of the sequence. The black line indicates where the  prediction begins.  has no connections to replicas of visible units at very different times, the posterior  distribution of the hidden units still depends on data far in the future. The Gibbs  sampling made our SBN simulations very slow and the sampling noise made the  learning far less effective than in the RBM. Although the RBM simulations seem  closer to biological plausibility, they too suffer from a major problem. To apply the  learning procedure it is necessary to reconstruct the data from the hidden states and  we do not know how to do this without interfering with the incoming datastream.  In our simulations we simply ignored this problem by allowing a visible unit to have  both an observed value and a reconstructed value at the same time.  Acknowledgements  We thank Zoubin Ghahramani, Peter Dayan, Rich Zemel, Terry Sejnowski and Radford  Neal for helpful discussions. This research was funded by grants from the Gatsby Foun-  dation and NSERC.  References  Anderson, C.H. &: van Essen, D.C (1994). Neurobiological computational systems. In J.M  Zureda, R.J. Marks, &: C.J. Robinson (Eds.), Computational Intelligence Imitating Life  213-222. New York: IEEE Press.  Hinton, G. E. (1999) Products of Experts. ICANN 99: Ninth international conference on  Artificial Neural Networks, Edinburgh, 1-6.  Hinton, G. E., McClelland, J. L., &: Rumelhart, D. E. (1986) Distributed representation-  s. In Rumelhart, D. E. and McClelland, J. L., editors, Parallel Distributed Processing:  Explorations in the Microstructure of Cognition. Volume 1: Foundations, MIT Press,  Cambridge, MA.  Hopfield, J. (1995). Pattern recognition computation using action potential timing for  stimulus representation. Nature, 3]'6, 33-36.  
Transductive Inference for Estimating  Values of Functions  Olivier Chapelle*, Vladimir Vapnik *'t, Jason Westont''*  * AT&T Research Laboratories, Red Bank, USA.  Royal Holloway, University of London, Egham, Surrey, UK.  t Barnhill BioInformatics.com, Savannah, Georgia, USA.  { chapelle, vlad, weston) research. art. corn  Abstract  We introduce an algorithm for estimating the values of a function  at a set of test points Xt+l,   , Xt+ra given a set of training points  (Xl, Yl),-.., (xt, Yt) without estimating (as an intermediate step)  the regression function. We demonstrate that this direct (transduc-  rive) way for estimating values of the regression (or classification  in pattern recognition) can be more accurate than the tradition-  al one based on two steps, first estimating the function and then  calculating the values of this function at the points of interest.  1 Introduction  Following [6] we consider a general scheme of transductive inference. Suppose there  exists a function y* = fo(x) from which we observe the measurements corrupted  with noise  ((/1,Yl),...(xt,Yt)), Yi -- Y n u i. (1)  Find an algorithm A that using both the given set of training data (1) and the given  set of test data  (Xt+i,... ,Xg+ra) (2)  selects from a set of functions {x -> f(x)} a function  y = f(x)= fA(XlXl,Yl,...,xt,yt,xt+l,...,Xt+m) (3)  and minimizes at the points of interest the functional  R(A)-E(t (y--fA(Xi[Xl,Yl,...,xt,yt,Xt+l,...,Xt+m)) 2) (4)  \i=+1  where expectation is taken over x and 6. For the training data we are given the  vector x and the value y, for the test data we are only given x.  Usually, the problem of estimating values of a function at points of interest is  solved in two steps: first in a given set of functions f(x, (), ( E A one estimates  the regression, i.e the function which minimizes the functional  R(a) =/((y- f(x,a))2dF(x,y), (5)  422 O. Chapelle, V.N. Vapnik and J. Weston  (the inductive step) and then using the estimated function y = f(x, at) we calculate  the values at points of interest  = i(x;,-t) (6)  (the deductive step).  Note, however, that the estimation of a function is equivalent to estimating its val-  ues in the continuum points of the domain of the function. Therefore, by solving  the regression problem using a restricted amount of information, we are looking  for a more general solution than is required. In [6] it is shown that using a di-  rect estimation method one can obtain better bounds than through the two step  procedure.  In this article we develop the idea introduced in [5] for estimating the values of a  function only at the given points.  The material is organized as follows. In Section 1 we consider the classical (induc-  tive) Ridge Regression procedure, and the leave-one-out technique which is used to  measure the quality of its solutions. Section 2 introduces the transductive method  of inference for estimation of the values of a function based on this leave-one-out  technique. In Section 3 experiments which demonstrate the improvement given  by transductive inference compared to inductive inference (in both regression and  pattern recognition) are presented. Finally, Section 4 summarizes the results.  2 Ridge Regression and the Leave-One-Out procedure  In order to describe our transductive method, let us first discuss the classical two-  step (inductive plus deductive) procedure of Ridge Regression. Consider the set of  functions linear in their parameters  n  = (7)  i----1  To minimize the expected loss (5), where F(x, y) is unknown, we minimize the  following empirical functional (the so-called Ridge Regression functional [1])  I t  Jte,p() = - f(x, + 11ll (S)  i----1  where 7 is a fixed positive constant, called the regularization parameter. The min-  imum is given by the vector of coefficients  OZt ---- OZ(Xl, Yl,..-, xt, Yt) ---- (KTK 4' ")'I) -1K:ry (9)  where  Y = (Yl,..., yt) T,  and K is a matrix with elements:  (10)  Kij =ckj(xi), i= l,...,e, j=l,...,n. (11)  The problem is to choose the value - which provides small expected loss for training  on a sample St - {(Xl,yl),...,(xt, yt)}.  For this purpose, we would like to choose '7 such that fv minimizing (8) also mini-  mizes   = f(y* - fv(x*[$t))dF(x*,y*)dF($t). (12)  Transductive Inference for Estimating Values of Funca'ons 423  Since F(x, y) is unknown one cannot estimate this minimum directly. To solve this  problem we instead use the leave-one-out procedure, which is an almost unbiased  estimator of (12). The leave-one-out error of an algorithm on the training sample  St is  1  Ttoo(,) =   (y - f(xl& \ (x,yd)  . (13)  i=1  The leave-one-out procedure consists of removing from the training data one el-  ement (say (xi,yi)), constructing the regression function only on the basis of the  remaining training data and then testing the removed element. In this fashion one  tests all/ elements of the training data using/ different decision rules. The mini-  mum over 3' of (13) we consider as the minimum over 3' of (12) since the expectation  of (13) coincides with (12) [2].  For Ridge Regression, one can derive a closed form expression for the leave-one-out  error. Denoting  A;1: (KT K ..]_ 3'_/-)-1 (14)  the error incurred by the leave-one-out procedure is [6]  where  rl(3') --  i--1  "-- k/T J  kt -- (ql(Xt)...,q}n(Xt)) T.  (15)  (16)  Let 3' - ,),0 be the minimum of (15). Then the vector  yO = K,(KTK + 3'oi)-1KT Y  where  K* - ( q(Xt+l) ... qn(Xt+l)  ql(Xt+m) ... qn(Xt+m)  is the Ridge Regression estimate of the unknown values (Yt*+l,'  ,  (17)  (18)  3 Leave-One-Out Error for Transductive Inference  In transductive inference, our goal is to find an algorithm A which minimizes the  functional (4) using both the training data (1) and the test data (2). We suggest the  following method: predict ( t+l .. Yt+m) by finding those values which minimize  the leave-one-out error of Ridge Regression training on the joint set  (Xl, Yl ),''', (It, Yt), (It+l, Y+i ),''', (It+m, Y+m)' (19)  This is achieved in the following way. Suppose we treat the unknown values  (Yt*+l,.",Yt*+m) as variables and for some fixed value of these variables we min-  imize the following empirical functional  , (y- f(x,)? + y}. (y - (x,)? +ll[I .  *P(lY"" Y): e + m  i=+1  (20)  This functional differs only in the second term from the functional (8) and corre-  sponds to performing Ridge Regression with the extra pairs  (+ 1, y,% 1),.--, (,+, y% ). (21)  424 O. Chapelle, K N. Vapnik and J. Weston  Suppose that vector Y* - (y{,... ,y) is taken from some set Y* E Y such that  the pairs (21) can be considered as a sample drawn from the same distribution as  the pairs (Xl,yi),... (xl,y*  , t)- In this case the leave-one-out error of minimizing  (20) over the set (19) approximates the functional (4). We can measure this leave-  one-out error using the same technique as in Ridge Regression. Using the closed  form (15) one obtains  , 1 Z (22)  Tl(fflY1""'Y)--+m i= 1  ;Z^kiii J  where we denote 2 = (Xl,..., xt+),  (Yl,-.., Yt, Yt+l,' ", Yt+m) , and  A;  = (RrR + ,0 -1 (2)  Rij = (fij(2i), i = l,...,g + m, j = l,...,n.  k t ---- ((l(t)...,(n(:t)) r.  (24)  (25)  Now let us rewrite the expression (22) in an equivalent form to separate the terms  with  from the terms with x. Introducing  C = I - RA; 1R T, (26)  and the matrix M with elements  MO =  CikCk  k----1 Ckk2  (27)  we obtain the equivalent expression of (22)  1  Ttoo(ly , -  '"'Y) +m  (28)  In order for the * which minimize the leave-one-out procedure to be valid it  is required that the pairs (21) are drawn from the same distribution as the pairs  (21, y),..., (xi, y). To satisfy this constraint we choose vectors Y* from the set  Y: {Y*: IIY* - Y11 _< R) (29)  where the vector yo is the solution obtained from classical Ridge Regression.  To minimize (28) under constraint (29) we use the functional  TZ(7) = ?TM- + PllY* - Yll (30)  where 7' is a constant depending on R.  Now, to find the values at the given points of interest (2) all that remains is to find  the minimum of (30) in Y*. Note that the matrix M is obtained using only the  vectors 5. Therefore, to find the minimum of this functional we rewrite Equation  (30) as  where  T;(ff)=yTMoY+2y*TMiY+y'TM2 * +PllY* - Y11  (31)  Mo M1  (32)  Transductive Inference for Estimating Values of Functions 425  and M0 is a  x  matrix, M1 is a  x m matrix and M2 is a m x m matrix. Taking  the derivative of (31) in Y* we obtain the condition for the solution  2M1Y + 2M2Y* - 23'*Y  + 23'*Y* = 0 (33)  which gives the predictions  Y* = (3"1 + M2)-(-MY + 3',yO). (34)  In this algorithm (which we will call Transductive Regression) we have two param-  eters to control: 3' and if*. The choice of ff can be found using the leave-one-out  estimator (15) for Ridge Regression. This leaves * as the only free parameter.  4 Experiments  To compare our one-step transductive approach with the classical two-step ap-  proach we performed a series of experiments on regression problems. We also de-  scribe experiments applying our technique to the problem of pattern recognition.  4.1 Regression  We conducted computer simulations for the regression problem using two datasets  from the DELVE repository: boston and kin-32fh.  The boston dataset is a well-known problem where one is required to estimate  house prices according to various statistics based on 13 locational, economic and  structural features from data collected by the U.S Census Service in the Boston  Massachusetts area.  The kin-32fh dataset is a realistic simulation of the forward dynamics of an 8 link  all-revolute robot arm. The task is to predict the distance of the end-effector from  a target, given 32 inputs which contain information on the joint positions, twist  angles and so forth.  Both problems are nonlinear and contain noisy data. Our objective is to com-  pare our transductive inference method directly with the inductive method of  Ridge Regression. To do this we chose the set of basis functions c)i(x) :  exp (-II x -xill2/2rr2), i = 1,... ,, and found the values of 3' and rr for Ridge  Regression which minimized the leave-one-out bound (15). We then used the same  values of these parameters in our transductive approach, and using the basis func-  tions c)i(x) - exp (-IIx - 5i112/2rr 2) , i = 1,... , + m, we then chose a fixed value  of 3'*.  For the boston dataset we followed the same experimental setup as in [4], that  is, we partitioned the training set of 506 observations randomly 100 times into a  training set of 481 observations and a testing set of 25 observations. We chose the  values of 3' and rr by taking the minimum average leave-one-out error over five more  random splits of the data stepping over the parameter space. The minimum was  found at '7 - 0.005 and log a - 0.7. For our transductive method, we also chose  the parameter 3'*: 10. In Figure la we plot mean squared error (MSE) on the test  set averaged over the 100 runs against log rr for Ridge Regression and Transductive  Regression. Transductive Regression outperforms Ridge Regression, especially at  the minimum.  To observe the influence of the number of test points m on the generalization ability  of our transductive method, we ran further experiments, setting if* = f/2m for  426 O. Chapelle, E N. Vapnik and J. Weston  9.2  9  8.8  8.6  8  7.8  7.6  0.14  0.13  m0.12  I--  ' I -- Transductive Regression  ,, 1--- Ridge Regre,n I  o'.4 o'.5 o; ; 1.2  Log sigma  0.15 ',  , -- Transductive Regression  ', - - - Ridge Regression  0.1 ",  0'091 1.5 2  Log sigma  8  7.95  7.9  7.85  uJ   7.8  7'75 t  -- Transductive Regression  - -- R dge Regress on  0.14  0.135  0,13  0.125  0.12  uJ_ 0.115  0.11  0.105  0.1  0.095  0.09  5 10 15 20 25  Test Set Size  I -- Transductive Regression  [ - -- Ridge Regression  2'.5 5'0 100 10 200 250  Test Set Size  (a)  Figure 1: A comparison of Transductive Regression to Ridge Regression on the  boston dataset: (a) error rates for varying rr, (b) varying the test set size, m, and  on the kin-32fh dataset: (c) error rates for varying rr, (d) varying the test set size.  different values of m. In Figure lb we plot m against MSE on the testing set, at  log rr - 0.7. The results indicate that increasing the test set size gives improved  performance in Transductive Regression. For Ridge Regression, of course, the size  of the testing set has no influence on the generalization ability.  We then performed similar experiments on the kin-32fh dataset. This time, as we  were interested in large testing sets giving improved performance for Transductive  Regression we chose 100 splits where we took a subset of only 64 observations for  training and 256 for testing. Again the leave-one-out estimator was used to find the  values ' - 0.1 and log a = 2 for Ridge Regression, and for Transductive Regression  we also chose the parameter '' - 0.1. We plotted MSE on the testing set against  log rr (Figure lc) and the size of the test set m for log rr = 2.75 (also, '' = 50/m)  (Figure ld) for the two algorithms. For large test set sizes our method outperforms  Ridge Regression.  4.2 Pattern Recognition  This technique can also be applied for pattern recognition problems by solving them  based on minimizing functional (8) with y - +1. Such a technique is known as a  Linear Discriminant (LD) technique.  Transductive Inference for Estimating Values of Functions 42 7  AB ABt SVM TLD  Postal - - 5.5 4.7  Banana 12.3 10.9 11.5 11.4  Diabetes 26.5 23.8 23.5 23.3  Titanic 22.6 22.6 22.4 22.4  Breast Cancer 30.4 26.5 26.0 25.7  Heart 20.3 16.6 16.0 15.7  Thyroid 4.4 4.6 4.8 4.0  Table 1: Comparison of percentage test error of AdaBoost (AB), Regularized Ad-  aBoost (ABt), Support Vector Machines (SVM) and Transductive Linear Discrim-  ination (TLD) on seven datasets.  Table 1 describes results of experiments on classification in the following problems:  2 class digit recognition (0 - 4 versus 5 - 9) splitting the training set into 23 runs  of 317 observations and considering a testing set of 2000 observations, and six  problems from the UCI database. We followed the same experimental setup as  in [3]: the performance of a classifier is measured by its average error over one  hundred partitions of the datasets into training and testing sets. Free parameter(s)  are chosen via validation on the first five training datasets. The performance of the  transductive LD technique was compared to Support Vector Machines, AdaBoost  and Regularized AdaBoost [3].  It is interesting to note that in spite of the fact that LD technique is one of the sim-  plest pattern recognition techniques, transductive inference based upon this method  performs well compared to state of the art methods of pattern recognition.  5 Summary  In this article we performed transductive inference in the problem of estimating  values of functions at the points of interest. We demonstrate that estimating the  unknown values via a one-step (transductive) procedure can be more accurate than  the traditional two-step (inductive plus deductive) one.  References  [1] A. Hoerl and R. W. Kennard. Ridge regression: Biased estimation for nonorthog-  onal problems. Technometrics, 12(1):55-67, 1970.  [2] A. Luntz and V. Brailovsky. On the estimation of characters obtained in statis-  tical procedure of recognition,. Technicheskaya Kibernetica, 1969. [In Russian].  [3] G. R5tsch, T. Onoda, and K.-R. Miiller. Soft margins for adaboost. Technical  report, Royal Holloway, University of London, 1998. TR-98-21.  [4] C. Saunders, A. Gammermann, and V. Vovk. Ridge regression learning algo-  rithm in dual variables. In Proccedings of the 15th International Conference on  Machine Learning, pages 515-521. Morgan Kaufmann, 1998.  [5] V. Vapnik. Estimating of values of regression at the point of interest. In Method  of Pattern Recognition. Sovetskoe Radio, 1977. [In Russian].  [6] V. Vapnik. Estimation of Dependences Based on Empirical Data. Springer-  Verlag, New York, 1982.  
Hierarchical Image Probability (HIP) Models  Clay D. Spence and Lucas Parra  Sarnoff Corporation  CN5300  Princeton, NJ 08543-5300  (cspence, lparra) @sarnoff. com  Abstract  We formulate a model for probability distributions on image spaces. We  show that any distribution of images can be factored exactly into condi-  tional distributions of feature vectors at one resolution (pyramid level)  conditioned on the image information at lower resolutions. We would  like to factor this over positions in the pyramid levels to make it tractable,  but such factoring may miss long-range dependencies. To fix this, we in-  troduce hidden class labels at each pixel in the pyramid. The result is  a hierarchical mixture of conditional probabilities, similar to a hidden  Markov model on a tree. The model parameters can be found with max-  imum likelihood estimation using the EM algorithm. We have obtained  encouraging preliminary results on the problems of detecting various ob-  jects in SAR images and target recognition in optical aerial images.  1 Introduction  Many approaches to object recognition in images estimate Pr(class [image). By con-  trast, a model of the probability distribution of images, PrOmage), has many attrac-  tive features. We could use this for object recognition in the usual way by training  a distribution for each object class and using Bayes' rule to get Pr(classlimage ) =  Pr0mage [ class) Pr(class)/Pr0mage ). Clearly there are many other benefits of having a  model of the distribution of images, since any kind of data analysis task can be approached  using knowledge of the distribution of the data. For classification we could attempt 'to de-  tect unusual examples and reject them, rather than trusting the classifier's output. We could  also compress, interpolate, suppress noise, extend resolution, fuse multiple images, etc.  Many image analysis algorithms use probability concepts, but few treat the distribution of  images. Zhu, Wu and Mumford [9] do this by computing the maximum entropy distribution  given a set of statistics for some features. This seems to work well for textures but it is not  clear how well it will model the appearance of more structured objects.  There are several algorithms for modeling the distributions of features extracted from the  image, instead of the image itself. The Markov Random Field (MRF) models are an ex-  ample of this line of development; see, e.g., [5, 4]. Unfortunately they tend to be very  expensive computationally.  In De Bonet and Viola's flexible histogram approach [2, 1], features are extracted at mul-  tiple image scales, and the resulting feature vectors are treated as a set of independent  Hierarchical Image Probability (HIP) Models 849  12  /  Gaussian Feature  Pyramid Pyramid  I1 / >( F1 I I GO   Subsampled  / F's  Figure 1: Pyramids and feature notation.  samples drawn from a distribution. They then model this distribution of feature vectors  with Parzen windows. This has given good results, but the feature vectors from neighbor-  ing pixels are treated as independent when in fact they share exactly the same components  from lower-resolutions. To fix this we might want to build a model in which the features at  one pixel of one pyramid level condition the features at each of several child pixels at the  next higher-resolution pyramid level. The multiscale stochastic process (MSP) methods do  exactly that. Luettgen and Willsky [7], for example, applied a scale-space auto-regression  (AR) model to texture discrimination. They use a quadtree or quadtree-like organization  of the pixels in an image pyramid, and model the features in the pyramid as a stochastic  process from coarse-to-fine levels along the tree. The variables in the process are hidden,  and the observations are sums of these hidden variables plus noise. The Gaussian distribu-  tions are a limitation of MSP models. The result is also a model of the probability of the  observations on the tree, not of the image.  All of these methods seem well-suited for modeling texture, but it is unclear how we might  build the models to capture the appearance of more structured objects. We will argue below  that the presence of objects in images can make local conditioning like that of the flexible  histogram and MSP approaches inappropriate. In the following we present a model for  probability distributions of images, in which we try to move beyond texture modeling.  This hierarchical image probability (HIP) model is similar to a hidden Markov model on  a tree, and can be learned with the EM algorithm. In preliminary tests of the model on  classification tasks the performance was comparable to that of other algorithms.  2 Coarse-to-fine factoring of image distributions  Our goal will be to write the image distribution in a form similar to Pr(I)   Pr(Fo [ F1) Pr(F1 [ F2)..., where Fl is the set of feature images at pyramid level l. We  expect that the short-range dependencies can be captured by the model's distribution of  individual feature vectors, while the long-range dependencies can be captured somehow at  low resolution. The large-scale structures affect finer scales by the conditioning.  In fact we can prove that a coarse-to-fine factoring like this is correct. From an image I  we build a Gaussian pyramid (repeatedly blur-and-subsample, with a Gaussian filter). Call  the/-th level It, e.g., the original image is Io (Figure 1). From each Gaussian level It we  extract some set of feature images Ft. Sub-sample these to get feature images Gr. Note  that the images in Gt have the same dimensions as It+i. We denote by (t the set of images  containing It+i and the images in Gr. We further denote the mapping from It to  Suppose now that o: Io  (o is invertible. Then we can think of o as a change of vari-  850 C. D. Spence and L. Parra  ables. If we have a distribution on a space, its expressions in two different coordinate sys-  tems are related by multiplying by the Jacobian. In this case we get Pr(Io) = Io[ Pr((o).  Since o = (Go,I1), we can factor Pr(o) to get Pr(Io) = Io] Pr(Go [ I1) Pr(I1). If  l is invertible for all l  (0,..., L - 1) then we can simply repeat this change of variable  and factoring procedure to get  L-1  (1)  This is a very general result, valid for all Pr(I), no doubt with some rather mild restrictions  to make the change of variables valid. The restriction that l be invertible is strong, but  many such feature sets are known to exist, e.g., most wavelet transforms on images. We  know of a few ways that this condition can be relaxed, but further work is needed here.  3 The need for hidden variables  For the sake of tractability we want to factor Pr(G/I Ii+) over positions, something like  Pr(I)  1-Il 1-[x,+ Pr(gl(z)[ft+i (z)) where g/(z) and f/+ (z) are the feature vectors  at position z. The dependence of gl on f/+ expresses the persistence of image structures  across scale, e.g., an edge is usually detectable as such in several neighboring pyramid  levels. The flexible histogram and MSP methods share this structure. While it may be  plausible that f/+i (z) has a strong influence on gl (z), we argue now that this factorization  and conditioning is not enough to capture some properties of real images.  Objects in the world cause correlations and non-local dependencies in images. For exam-  ple, the presence of a particular object might cause a certain kind of texture to be visible  at level I. Usually local features f/+l by themselves will not contain enough information  to infer the object's presence, but the entire image Ii+i at that layer might. Thus gl (:c) is  influenced by more of Ii+x than the local feature vector.  Similarly, objects create long-range dependencies. For example, an object class might  result in a kind of texture across a large area of the image. If an object of this class is always  present, the distribution may factor, but if such objects aren't always present and can't  be inferred from lower-resolution information, the presence of the texture at one location  affects the probability of its presence elsewhere.  We introduce hidden variables to represent the non-local information that is not captured by  local features. They should also constrain the variability of features at the next finer scale.  Denoting them collectively by A, we assume that conditioning on A allows the distributions  over feature vectors to factor. In general, the distribution over images becomes  ,L-1  /=0 zI+.  Pr(gt(a:) I ft+),A)Pr(A [ I6)} Pr(IL).  (2)  As written this is absolutely general, so we need to be more specific. In particular we would  like to preserve the conditioning of higher-resolution information on coarser-resolution  information, and the ability to factor over positions.  Hierarchical Image Probability (HIP) Models 851  Al+2  Al+l  Ai  Figure 2: Tree structure of the conditional dependency between hidden variables in the HIP  model. With subsampling by two, this is sometimes called a quadtree structure.  As a first model we have chosen the following structure for our HIP model: 1  ]  Pr(I) oc E H H [Pr(gt(:e)[ft+(:e),at(:e)) Pr(at(:)lat+l(:)) . (3)  Ao,...,Ar- /=0 zEIt+  To each position :e at each level l we attach a hidden discrete index or label at (:e). The  resulting label image At for level l has the same dimensions as the images in  Since at (:e) codes non-local information we can think of the labels At as a segmentation  or classification at the/-th pyramid level. By conditioning at(z) on al+l (:e), we mean  that at (:e) is conditioned on at+l at the parent pixel of :e. This parent-child relationship  follows from the sub-sampling operation. For example, if we sub-sample by two in each  direction to get Gt from Ft, we condition the variable at at (:e, /) in level l on al+l at  location ([:e/2J, [//2J) in level/+ 1 (Figure 2). This gives the dependency graph of the  hidden variables a tree structure. Such a probabilistic tree of discrete variables is sometimes  referred to as a belief network. By conditioning child labels on their parents information  propagates though the layers to other areas of the image while accumulating information  along the way.  For the sake of simplicity we've chosen Pr(g/] f/+l, al) tO be normal with mean l,at '-  Mat f/+l and covariance Eat. We also constrain Mat and Eat to be diagonal.  4 EM algorithm  Thanks to the tree structure, the belief network for the hidden variables is relatively easy to  train with an EM algorithm. The expectation step (summing over at's) can be performed  directly. If we had chosen a more densely-connected structure with each child having  several parents, we would need either an approximate algorithm or Monte Carlo techniques.  The expectation is weighted by the probability of a label or a parent-child pair of labels  given the image. This can be computed in a fine-to-coarse-to-fine procedure, i.e. working  from leaves to the root and then back out to the leaves. The method is based on belief  propagation [6]. With some care an efficient algorithm can be worked out, but we omit the  details due to space constraints.  Once we can compute the expectations, the normal distribution makes the M-step tractable;  we simply compute the updated gat, Eat, Mat, and Pr(a/I 5/+1) as combinations of various  expectation values.  The proportionality factor includes Pr (AL, IL) which we model as  1-I: Pr(g(X) l a(a:)) Pr(a(a:)). This is the I = L factor of Equation 3, which should be  read as having no quantities f+ or aL+.  852 C. D. Spence and L. Parra  HIP  HPNIq  0.4  P1ane ROCareas  O O O O OOO-  Figure 3: Examples of aircraft ROIs. On the right are A values from a jack-knife study of  detection performance of HIP and HPNN models.  Figure 4: SAR images of three types of vehicles to be detected.  5 Experiments  We applied HIP to the problem of detecting aircraft in an aerial photograph of Logan air-  port. A simple template-matching algorithm was used to select forty candidate aircraft,  twenty of which were false positives (Figure 3). Ten of the plane examples were used for  training one HIP model and ten negative examples were used to train another. Because  of thesmall number of examples, we performed a jack-knife study with ten random splits  of the data. For features we used filter kernels that were polynomials of up to third order  multiplying Gaussians. The HIP pyramid used subsampling by three in each direction. The  test set ROC area for HIP had a mean of Az = 0.94, while our HPNN algorithm [8] gave a  mean A of 0.65. The individual values shown in Figure 3. (We compared with the HPNN  because it had given A - 0.86 on a larger set of aircraft images including these with a  different set of features and subsampling by two.)  We also performed an experiment with the three target classes in the MSTAR public targets  data set, to compare with the results of the flexible histogram approach of De Bonet, et al  [1]. We trained three HIP models, one for each of the target vehicles BMP-2, BTR-70 and  T-72 (Figure 4). As in [1] we trained each model on ten images of its class, one image for  each of ten aspect angles, spaced approximately 36  apart. We trained one model for all  ten images of a target, whereas De Bonet et al trained one model per image.  We first tried discriminating between vehicles of one class and other objects by thresholding  log Pr(I I class), i.e., no model of other objects is used. For the tests, the other objects were  taken from the test data for the two other vehicle classes, plus seven other vehicle classes.  Hierarchical Image Probability (HIP) Models 853  1  09  08  07  =08  0.4  O3  O2  0.1  0 0  ROC using Pr( I I target )  - - - BMP-2: Az = 0.77  T-72: Az = 0.64  BTR-70: Az = 0.86      06 07 08  02 03 0.4 05 1  false  a  ROC uam:J Pr ( I I targel1 ) / Pr ( I [ target2 )  0.9  o21-   JJ I BMP-2 vs T-72: Az = 0.79  oll -- BMP-2vsBTR-70:Az=0.82  o r -- T-  o o4 o2 0.3 04 o.s o.s 07 os o9 1  b  Figure 5: ROC curves for vehicle detection in SAR imagery. (a) ROC curves by thresh-  olding HIP likelihood of desired class. (b) ROC curves for inter-class discrimination using  ratios of likelihoods as given by HIP models.  There were 1,838 image from these seven other classes, 391 BMP2 test images, 196 BTR70  test images, and 386 T72 test images. The resulting ROC curves are shown in Figure 5a.  We then tried discriminating between pairs target classes using HIP model likelihood ratios,  i.e., log Pr(I I classl) - log Pr(I I class2). Here we could not use the extra seven vehicle  classes. The resulting ROC curves are shown in Figure 5b. The performance is comparable  to that of the flexible histogram approach.  6 Conditional distributions of features  To further test the HIP model's fit to the image distribution, we computed several distri-  butions of features gl (z) conditioned on the parent feature fl+i (a:). 2 The empirical and  computed distributions for a particular parent-child pair of features are shown in Figure 6.  The conditional distributions we examined all had similar appearance, and all fit the empir-  ical distributions well. Buccigrossi and Simoncelli [3] have reported such "bow-tie" shape  conditional distributions for a variety of features. We want to point out that such condi-  tional distributions are naturally obtained for any mixture of Gaussian distributions with  varying scales and zero means. The present HIP model learns such conditionals, in effect  describing the features as non-stationary Gaussian variables.  7 Conclusion  We have developed a class of image probability models we call hierarchical image proba-  bility or HIP models. To justify these, we showed that image distributions can be exactly  represented as products over pyramid levels of distributions of sub-sampled feature im-  ages conditioned on coarser-scale image information. We argued that hidden variables are  needed to capture long-range dependencies while allowing us to further factor the distri-  butions over position. In our current model the hidden variables act as indices of mixture  2This is somewhat involved; Pr(gt I ft+l) is not just Pr(gt I ft+l, at) Pr(at) summed over at, but  Eat Pr(gt,at l ft+l) = E,t Pr(gt l ft+l,at) Pr(at I ft+l).  854 C. D. Spence and L. Parra  Conditional dislirbution of data  Conditional distirbution of HIP model  f. feature 9 layer 1 f: feature 9 layer 1  Figure 6: Empirical and HIP estimates of the distribution of a feature #t (a:) conditioned on  its parent feature ft+ (:c).  components. The resulting model is somewhat like a hidden Markov model on a tree. Our  early results on classification problems showed good performance.  Acknowledgements  We thank Jeremy De Bonet and John Fisher for kindly answering questions about their  work and experiments. Supported by the United States Government.  References  [1] J. S. De Bonet, P. Viola, and J. W. Fisher III. Flexible histograms: A multiresolution  target discrimination model. In E.G. Zelnio, editor, Proceedings of SPIE, volume  3370, 1998.  [2] Jeremy S. De Bonet and Paul Viola. Texture recognition using a non-parametric multi-  scale statistical model. In Conference on Computer Vision and Pattern Recognition.  IEEE, 1998.  [3] Robert W. Buccigrossi and Eero P. Simoncelli. Image compression via joint statisti-  cal characterization in the wavelet domain. Technical Report 414, U. Penn. GRASP  Laboratory, 1998. Available at ftp://ftp.cis.upenn.edu/pub/eero/buccigrossi97.ps.gz.  [4] Rama Chellappa and S. Chatterjee. Classification of textures using Gaussian Markov  random fields. IEEE Trans. ASSP, 33:959-963, 1985.  [5] Stuart Geman and Donald Geman. Stochastic relaxation, Gibbs distributions, and the  Bayesian restoration of images. IEEE Trans. PAMI, PAMI-6(6): 194-207, November  1984.  [6] Michael I. Jordan, editor. Learning in Graphical Models, volume 89 of NATO Science  Series D: Behavioral and Brain Sciences. Kluwer Academic, 1998.  [7] Mark R. Luettgen and Alan S. Willsky. Likelihood calculation for a class of multiscale  stochastic models, with application to texture discrimination. IEEE Trans. Image Proc.,  4(2): 194-207, 1995.  [8] Clay D. Spence and Paul Sajda. Applications of multi-resolution neural networks to  mammography. In Michael S. Kearns, Sara A. Solla, and David A. Cohn, editors, NIPS  ii, pages 981-988, Cambridge, MA, 1998. MIT Press.  [9] Song Chun Zhu, Ying Nian Wu, and David Mumford. Minimax entropy principle and  its application to texture modeling. Neural Computation, 9(8): 1627-1660, 1997.  
Learning Informative Statistics: A  Nonparametric Approach  John W. Fisher III, Alexander T. Ihler, and Paul A. Viola  Massachusetts Institute of Technology  77 Massachusetts Ave., 35-421  Cambridge, MA 02139  {fisher,ihler, viola  @ ai. rnit. edu  Abstract  We discuss an information theoretic approach for categorizing and mod-  eling dynamic processes. The approach can learn a compact and informa-  tive statistic which summarizes past states to predict future observations.  Furthermore, the uncertainty of the prediction is characterized nonpara-  metrically by a joint density over the learned statistic and present obser-  vation. We discuss the application of the technique to both noise driven  dynamical systems and random processes sampled from a density which  is conditioned on the past. In the first case we show results in which both  the dynamics of random walk and the statistics of the driving noise are  captured. In the second case we present results in which a summarizing  statistic is learned on noisy random telegraph waves with differing de-  pendencies on past states. In both cases the algorithm yields a principled  approach for discriminating processes with differing dynamics and/or de-  pendencies. The method is grounded in ideas from information theory  and nonparametric statistics.  1 Introduction  Noisy dynamical processes abound in the world - human speech, the frequency of sun  spots, and the stock market are common examples. These processes can be difficult to  model and categorize because current observations are dependent on the past in complex  ways. Classical models come in two sorts: those that assume that the dynamics are linear  and the noise is Gaussian (e.g. Weiner etc.); and those that assume that the dynamics are  discrete (e.g. HMM's). These approach are wildly popular because they are tractable and  well understood. Unfortunately there are many processes where the underlying theoretical  assumptions of these models are false. For example we may wish to analyze a system  with linear dynamics and non-Gaussian noise or we may wish to model a system with an  unknown number of discrete states.  We present an information-theoretic approach for analyzing stochastic dynamic processes  which can model simple processes like those mentioned above, while retaining the flexi-  bility to model a wider range of more complex processes. The key insight is that we can  often learn a simplifying informative statistic of the past from samples using nonparametric  estimates of both entropy and mutual information. Within this tYamework we can predict  future states and, of equal importance, characterize the uncertainty accompanying those  Learning Informative Statistics: A Nonparametric Approach 901  predictions. This non-parametric model is flexible enough to describe uncertainty which  is more complex than second-order statistics. In contrast techniques which use squared  prediction error to drive learning are focused on the mode of the distribution.  Taking an example from financial forecasting, while the most likely sequence of pricing  events is of interest, one would also like to know the accompanying distribution of price  values (i.e. even if the most likely outcome is appreciation in the price of an asset, knowl-  edge of lower, but not insignificant, probability of depreciation is also valuable). Towards  that end we describe an approach that allows us to simultaneously learn the dependencies  of the process on the past as well as the uncertainty of future states. Our approach is novel  in that we fold in concepts from information theory, nonparametric statistics, and learning.  In the two types of stochastic processes we will consider, the challenge is to summarize the  past in an efficient way. In the absence of a known dynamical or probabilistic model, can  we learn an informative statistic (ideally a sufficient statistic) of the past which minimizes  our uncertainty about future states? In the classical linear state-space approach, uncertainty  is characterized by mean squared error (MSE) which implicitly assume Gaussian statistics.  There are, however, linear systems with interesting behavior due to non-Gaussian statistics  which violate the assumption underlying MSE. There are also nonlinear systems and purely  probabilistic processes which exhibit complex behavior and are poorly characterized by  mean square error and/or the assumption of Gaussian noise.  Our approach is applicable to both types of processes. Because it is based on non-  parametric statistics we characterize the uncertainty of predictions in a very general way:  by a density of possible future states. Consequently the resulting system captures both the  dynamics of the systems (through a parameterization) and the statistics of driving noise  (through a nonparametric modeling). The model can then be used to classify new signals  and make predictions about the future.  2 Learning from Stationary Processes  In this paper we will consider two related types of stochastic processes, depicted in figure 1.  These processes differ in how current observations are related to the past. The first type of  process, described by the following set of equations, is a discrete time dynamical (possibly  nonlinear) system:  Xk=G({Xk--1}N;Wg) q-T]k ; {Xk}N '= {Xk,..-,X/c-(N-i)} (1)  where, .zk, the state of the process at time k, is a function of the N previous states and  the present value of r/. In general the sequence {xk} is not stationary (in the strict sense);  however, under fairly mild conditions on {r/k}, namely that {r/k} is a sequence of i.i.d.  random variables (which we will always assume to be true), the sequence:  ek = xk - G( {xk_ } %) (2)  is stationary. Often termed an innovation sequence, for our purpose the stationarity of 2 will  suffice. This leads to a prediction framework for estimating the dynamical parameters, w a,  of the system and to which we will adjoin a nonparametric characterization of uncertainty.  The second type of process we consider is described by a conditional probability density:  xk  P(xkll{xk-x }N) (3)  In this case it is only the conditional statistics of {xk } that we are concerned with and they  are, by definition, constant.  3 Learning Informative Statistics with Nonparametric Estimators  We propose to determine the system parameters by minimizing the entropy of the error  residuals for systems of type (a). Parametric entropy optimization approaches have been  902 J. W. Fisher III, .4. T. Ihler and P. A. ola  Xk--1  (a) (b)  I qp(xJ{xA,- }N)'  I 1  '4  [ Z-I,''' ,Z --N  i "k'  I  I  I  I  Xk--1  F({k_i}N)  - I (x,F({-l}N))max  Figure 1: Two related systems: (a) dynamical system driven by stationary noise and (b)  probabilistic system dependent on the finite past. Dotted box indicates source of stochastic  process, while solid box indicates learning algorithm  proposed (e.g. [4]), the novelty of our approach; however, is that we estimate entropy  nonparametrically. That is,  bg = argmin [/(e) [/(e)  fp(e)logp(e)de t3(e): ---  JJ9  1 Al- 1  X!k=O n(ek--e) , (4)  where the differential entropy integral is approximated using a function of the Parzen kernel  density estimator [5] (in all experiments we use the Gaussian kernel). It can be shown that  minimizing the entropy of the error residuals is equivalent to maximizing their likelihood  [ l ]. In this light, the proposed criterion is seeking the maximum likelihood estimate of the  system parameters using a nonparametric description of the noise density. Consequently,  we solve for the system parameters and the noise density jointly.  While there is no explicit dynamical system in the second system type we do assume that  the conditional statistics of the observed sequence are constant (or at worst slowly changing  for an on-line learning algorithm). In this case we desire to minimize the uncertainty of  predictions from future samples by summarizing information from the past. The challenge  is to do so efficiently via a function of recent samples. Ideally we would like to find a  sufficient statistic of the past; however, without an explicit description of the density we  opt instead for an informative statistic. By informative statistic we simply mean one which  reduces the conditional entropy of future samples. If the statistic were sufficient then the  mutual information has reached a maximum [1]. As in the previous case, we propose to  find such a statistic by maximizing the nonparametric mutual information as defined by  wf  wf  = argmin /(xk) +//(F({ };wf)) - /(xk,F({  = argmin /(xk) - /(xklF({ };to/)))  (5)  (6)  (7)  By equation 6 this is equivalent to optimizing the joint and marginal entropies (which we  do in practice) or, by equation 7, minimizing the conditional entropy.  We have previously presented two related methods for incorporating kernel based density  estimators into an information theoretic learning framework [2, 3]. We chose the method of  [3] because it provides an exact gradient of an approximation to entropy, but more impor-  tantly can be converted into an implicit error function thereby reducing computation cost.  Learning Informative Statistics.' A Nonparametric Approach 903  4 Distinguishing Random Walks: An Example  In random walk the feedback function G({xk_  }  ) = xk_ 1. The noise is assumed to be in-  dependent and identically distributed (i.i.d.). Although the sequence,xk, is non-stationary  the increments (xk-xk_x) are stationary. In this context, estimating the statistics of the  residuals allows for discrimination between two random walk process with differing noise  densities. Furthermore, as we will demonstrate empirically, even when one of the pro-  cesses is driven by Gaussian noise (an implicit assumption of the MMSE criterion), such  knowledge may not be sufficient to distinguish one process from another.  Figure 2 shows two random walk realizations and their associate noise densities (solid  lines). One is driven by Gaussian noise (lk  N(0, 1)), while the other is driven by  1 r.  a bi-modal mixture of gaussians (r] :N(0.9o, 0.3) + -0.95, 0.3)) (note: both  :v(  densities are zero-mean and unit variance). During learning, the process was modeled as  fifth-order auto-regressive (ARs). One hundred samples were drawn from a realization of  each type and the AR parameters were estimated using the standard MMSE approach and  the approach described above. With regards to parameter estimation, both methods (as  expected) yield essentially the same parameters with the first coefficient being near unity  and the remaining coefficients being near zero.  We are interested in the ability to distinguish one process from another. As mentioned,  the current approach jointly estimates the parameters of the system as well as the den-  sity of the noise. The nonparametric estimates are shown in figure 2 (dotted lines).  These estimates are then be used to compute the accumulated average log-likelihood  1 k  (L(ek) = i: i= 1ogp(xi)) of the residual sequence (e  rl) under the known and  learned densities (figure 3). It is striking (but not surprising) that L(e) of the hi-modal  mixture under the Gaussian model (dashed lines, top) does not differ significantly from the  Gaussian driven increments process (solid lines, top). The explanation follows from the  fact that  lim L(e.) = -(H(p(e))+ D(p(e)llp(e)) )  (8)  where p(e) is the true density ofe (bi-modal), p(e) is the assumed density of the likelihood  test (unit-variance Gaussian), and D([!) is the Kullback-Leibler divergence [1]. In this  case, D(p(e)[lp(e)) is relatively small (not true for D(p(e)l[p(e)) and H(p(e)) is less  than the entropy of the unit-variance Gaussian (for fixed variance, the Gaussian density  has maximum entropy). The consequence is that the likelihood test under the Gaussian  assumption does not reliably distinguish the two processes. The likelihood test under the  bi-modal density or its nonparametric estimate (figure 3, bottom) does distinguish the two.  The method described is not limited to linear dynamic models. It can certainly be used  for nonlinear models, so long as the dynamic can be well approximated by differentiable  functions. Examples for multi-layer perceptrons are described in [3].  5 Learning the Structure of a Noisy Random Telegraph Wave  A noisy random telegraph wave (RTW) can be described by figure 1 (b). Our goal is not to  demonstrate that we can analyze random telegraph waves, rather that we can robustly learn  an informative statistic of the past for such a process. We define a noisy random telegraph  wave as a sequence x ,,,, N(t,, a) where ltk is binomially distributed:  i N  ' Zz=l I k-zl  (9)  N(lak, o') is Gaussian and o < 1. This process is interesting because the parameters are  random functions of a nonlinear combination of the set {Xk ;. Depending on the value of  N, we observe different switching dynamics. Figure 4 shows examples of such signals for  904 d. W. Fisher III, A. T. Ihler and P. A. Iqola  . laudelan vL learned denait.,F  bi-modal radom wlk  4O  0.0  hi-model u. leemeal denit,v  Figure 2: Random walk examples (left), comparison of known to learned densities (right).  known f[ouma model  :_  known bi-modl model  lelrroed model from bi-model stit.isLios  Figure 3: L(ek) under known models (left) as compared to learned models (right).  N = 20 (left) and N = 4 (right). Rapid switching dynamics are possible for both signals  while N = 20 has periods with longer duration than N = 4.  0.0  Figure 4: Noisy random telegraph wave: N = 20 (left), N = 4 (right)  In our experiments we learn a sufficient statistic which has the form  F({xk }past) = a ( wfx_) , (10)  i=1  where a( ) is the hyperbolic tangent function (i.e. F{ } is a one layer perceptron). Note  that a multi-layer perceptron could also be used [3].  In our experiments we train on 100 samples of noisy RTW(v=20) and RTW(v=4). We  then learn statistics for each type of process using M = {4, 5, 15, 20, 25}. This tests for  situations in which the depth is both under-specified and over-specified (as well as perfectly  Learning Informative Statistics: A Nonparametric Approach 905  RTW ._=4,ld=. - Wener 8vt. hesi.  4OO  Figure 5: Comparison of Wiener filter (top) nonparametric approach (bottom) for synthesis.  Figure 6: Informative Statistics for noisy random telegraph waves. M -- 25 trained on N  equal 4 (left) and 20 (right).  specified). We will denote FN({Xk}M) as the statistic which was trained on an RTWuv )  process with a memory depth of M.  Since we implicitly learn a joint density over (xk, FN({Xk}M)) synthesis is possible by  sampling from that density. Figure 5 compares synthesis using the described method (bot-  tom) to a Wiener filter (top) estimated over the same data. The results using the information  theoretic approach (bottom) preserve the structure of the RTW while the Wiener filter re-  sults do not. This was achieved by collapsing the information of past samples into a single  statistic (avoiding high dimension density estimation). Figure 6 shows the joint density  over (x, FN({Xk}M)) for N = {4, 20} and M = 25. We see that the estimated den-  sities are not separable and by virtue of this fact the learned statistic conveys information  about the future. Figure 7 shows results from 100 monte carlo trials. In this case the depth  of the statistic is matched to the process. Each plot shows the accumulated conditional log  likelihood (L(e&)  k  '-- Ei--11glb(xilFv({X&-l}M))underthelearnedstatisticwitherror  bars. Figure 8 shows similar results after varying the memory depth M = {4, 5, 15, 20, 25}  of the statistic. The figures illustrate robustness to choice of memory depth M. This is not  to say that memory depth doesn't matter; that is, them must be some information to exploit,  but the empirical results indicate that useful information was extracted.  6 Conclusions  We have described a nonparametric approach for finding informative statistics. The ap-  proach is novel in that learning is derived from nonparametric estimators of entropy and  mutual information. This allows for a means by which to 1) efficiently summarize the  past, 2) predict the future and 3) characterize the uncertainty of those predictions beyond  second-order statistics. Futhermore, this was accomplished without the strong assumptions  accompanying parametric approaches.  906 J. W. Fisher III, ,4. T. Ihler and P. ,4. Iola  M tm4 MO vs M4 rocelm  M4 LelL, G  144 room  Figure 7: Conditional L(ek). Solid line indicates RTW(N=20) while dashed line indicates  RTW(N=4). Thick lines indicate the average over all monte carlo runs while the thin lines  indicate 4-1 standard deviation. The left plot uses a statistic trained on RTW(N=20) while  the right plot uses a statistic trained on RTW(N=4).  1420 tmst.M.20 v* u4 p__roee ....  Figure 8: Repeat of figure 7 for cases with M = {4, 5, 15, 20, 25}. Obvious breaks indicate  a new set of trials  We also presented empirical results which illustrated the utility of our approach. The exam-  ple of random walk served as a simple illustration in learning a dynamic system in spite of  the over-specification of the AR model. More importantly, we demonstrated the ability to  learn both the dynamic and the statistics of the underlying noise process. This information  was later used to distinguish realizations by their nonparametric densities, something not  possible using MMSE error prediction.  An even more compelling result were the experiments with noisy random telegraph waves.  We demonstrated the algorithms ability to learn a compact statistic which efficiently sum-  marized the past for process identification. The method exhibited robustness to the number  of parameters of the learned statistic. For example, despite overspecifying the dependence  of the memory-4 in three of the cases, a useful statistic was still found. Conversely, despite  the memory-20 statistic being underspecified in three of the experiments, useful informa-  tion from the available past was extracted.  It is our opinion that this method provides an alternative to some of the traditional and  connectionist approaches to time-series analysis. The use of nonparametric estimators adds  flexibility to the class of densities which can be modeled and places less of a constraint on  the exact form of the summarizing statistic.  References  [1] T. Cover and J. Thomas. Elements of Information Theory. John Wiley & Sons, New York, 1991.  [2] P. Viola et al. Empricial entropy manipulation for real world problems. In Mozer Touretsky and  Hasselmo, editors, Advances in Neural Information Processing Systems, pages ?-?, 1996.  [3] J.W. Fisher and J.C. Principe. A methodology for information theoretic feature extraction. In  A. Stuberud, editor, Proc. of the IEEE !nt Joint Conf on Neural Networks, pages ?-?, 1998.  [4] J. Kapur and H. Kesavan. Entropy Optimization Principles with Applications. Academic Press,  New York, 1992.  [5] E. Parzen. On estimation of a probability density function and mode. Ann. of Math Stats.,  33:1065-1076, 1962.  
The Relevance Vector Machine  Michael E. Tipping  Microsoft Research  St George House, 1 Guildhall Street  Cambridge CB2 3NH, U.K.  mtippingmicrosoft. com  Abstract  The support vector machine (SVM) is a state-of-the-art technique  for regression and classification, combining excellent generalisation  properties with a sparse kernel representation. However, it does  suffer from a number of disadvantages, notably the absence of prob-  abilistic outputs, the requirement to estimate a trade-off parameter  and the need to utilise 'Mercer' kernel functions. In this paper we  introduce the Relevance Vector Machine (RVM), a Bayesian treat-  ment of a generalised linear model of identical functional form to  the SVM. The RVM suffers from none of the above disadvantages,  and examples demonstrate that for comparable generalisation per-  formance, the RVM requires dramatically fewer kernel functions.  I Introduction  In supervised learning we are given a set of examples of input vectors (Xn N  }n----!  along with corresponding targets N  (tn}n=l, the latter of which might be real values  (in regression) or class labels (classification). From this 'training' set we wish to  learn a model of the dependency of the targets on the inputs with the objective of  making accurate predictions of t for previously unseen values of x. In real-world  data, the presence of noise (in regression) and class overlap (in classification) implies  that the principal modelling challenge is to avoid 'over-fitting' of the training set.  A very successful approach to supervised learning is the support vector machine  (SVM) [8]. It makes predictions based on a function of the form  N  y(x) = wnK(x, xn) + w0,  (1)  n----1  where {wn} are the model 'weights' and K(., .) is a kernel function. The key feature  of the SVM is that, in the classification case, its target function attempts to minimise  the number of errors made on the training set while simultaneously maximising the  'margin' between the two classes (in the feature space implicitly defined by the  kernel). This is an effective 'prior' for avoiding over-fitting, which leads to good  generalisation, and which furthermore results in a sparse model dependent only on  a subset of kernel functions: those associated with training examples Xn that lie  either on the margin or on the 'wrong' side of it. State-of-the-art results have been  reported on many tasks where SVMs have been applied.  The Relevance Vector Machine 653  However, the support vector methodology does exhibit significant disadvantages:  Predictions are not probabilistic. In regression the SVM outputs a point  estimate, and in classification, a 'hard' binary decision. Ideally, we desire to  estimate the conditional distribution p(tlx ) in order to capture uncertainty  in our prediction. In regression this may take the form of 'error-bars', but it  is particularly crucial in classification where posterior probabilities of class  membership are necessary to adapt to varying class priors and asymmetric  misclassification costs.  Although relatively sparse, SVMs make liberal use of kernel functions, the  requisite number of which grows steeply with the size of the training set.  It is necessary to estimate the error/margin trade-off parameter 'C' (and  in regression, the insensitivity parameter 'e' too). This generally entails a  cross-validation procedure, which is wasteful both of data and computation.  The kernel function K(-,-) must satisfy Mercer's condition.  In this paper, we introduce the 'relevance vector machine' (RVM), a probabilistic  sparse kernel model identical in functional form to the SVM. Here we adopt a  Bayesian approach to learning, where we introduce a prior over the weights governed  by a set of hyperparameters, one associated with each weight, whose most probable  values are iteratively estimated from the data. Sparsity is achieved because in  practice we find that the posterior distributions of many of the weights are sharply  peaked around zero. Furthermore, unlike the support vector classifier, the non-  zero weights in the RVM are not associated with examples close to the decision  boundary, but rather appear to represent 'prototypical' examples of classes. We  term these examples 'relevance' vectors, in deference to the principle of automatic  relevance determination (ARD) which motivates the presented approach [4, 6].  The most compelling feature of the RVM is that, while capable of generalisation per-  formance comparable to an equivalent SVM, it typically utilises dramatically fewer  kernel functions. Furthermore, the RVM suffers from none of the other limitations  of the SVM outlined above.  In the next section, we introduce the Bayesian model, initially for regression, and  define the procedure for obtaining hyperparameter values, and thus weights. In  Section 3, we give brief examples of application of the RVM in the regression case,  before developing the theory for the classification case in Section 4. Examples of  RVM classification are then given in Section 5, concluding with a discussion.  2 Relevance Vector Regression  Given a dataset of input-target pairs (Xn, t, v  )n----l, we follow the standard formula-  tion and assume p(t]x) is Gaussian Af(t]y(x), a2). The mean of this distribution for  a given x is modelled by y(x) as defined in (1) for the SVM. The likelihood of the  dataset can then be written as  P(tlw, a2)=(2ra)-N/exp{ 21[]t - ,I'w)]12 }, (2)  where t - (tl ... tN), w -- (Wo... WN) and  is the N x (N q- 1) 'design' matrix  with nm = K(xn, xm-1) and (I)nl ---- 1. Maximum-likelihood estimation of w and   from (2) will generally lead to severe overfitting, so we encode a preference for  smoother functions by defining an ARD Gaussian prior [4, 6] over the weights:  N  P(Wle) -- H Jr(Wi10' O/1)' (3)  i----0  654 M. E. Tipping  with c a vector of N + 1 hyperparameters. This introduction of an individual hy-  perparameter for every weight is the key feature of the model, and is ultimately  responsible for its sparsity properties. The posterior over the weights is then ob-  tained from Bayes' rule:  with  {1  p(wlt, a,a 2) = (2r)-(N+l)/21l-1/2 exp -(w -/./)Ty]--l(w --  , (4)  =(?B+A) -1,  / = TBt,  where we have defined A = diag(a0, al,..., aN) and B =  also treated as a hyperparameter, which may be estimated from the data.  By integrating out the weights, we obtain the marginal likelihood, or evidence [2],  for the hyperparameters:  p(tla,a 2) = (2r)-/2lB-1 + A-IT1-1/2 exp -t (B -1 + ,I,A- .  (7)  For ideal Bayesian inference, we should define hyperpriors over e and a , and  integrate out the hyperparameters too. However, such marginalisation cannot be  performed in closed-form here, so we adopt a pragmatic procedure, based on that  of MacKay [2], and optimise the marginal likelihood (7) with respect to  which is essentially the type H maximum likelihood method [1]. This is equivalent  to finding the maximum of p(e, alt), assuming a uniform (and thus improper)  hyperprior. We then make predictions, based on (4), using these maximising values.  (5)  (6)  Note that a 2 is  2.1 Optimising the hyperparameters  Values of e, and a 2 which maximise (7) cannot be obtained in closed form, and  we consider two alternative formulae for iterative re-estimation of e,. First, by  considering the weights as 'hidden' variables, an EM approach gives:  1 1  onew . _-- .  i <W/2> p(w{t,o,er 2) ii q- ] (8)  Second, direct differentiation of (7) and rearranging gives:  onew i  i ---  /, (9)  where we have defined the quantities 7i = 1 - cqZ, which can be interpreted as a  measure of how 'well-determined' each parameter w is by the data [2]. Generally,  this latter update was observed to exhibit faster convergence.  For the noise variance, both methods lead to the same re-estimate:  ((72) new -- lit - 'ulI2/(N - (10)  i  In practice, during re-estimation, we find that many of the ai approach infinity,  and from (4), p(wiJt, c,a 2) becomes infinitely peaked at zero -- implying that  the corresponding kernel functions can be 'pruned'. While space here precludes a  detailed explanation, this occurs because there is an 'Occam' penalty to be paid for  smaller values of ai, due to their appearance in the determinant in the marginal  likelihood (7). For some ai, a lesser penalty can be paid by explaining the data  with increased noise a 2, in which case those ai  c.  The Relevance Vector Machine 655  3 Examples of Relevance Vector Regression  3.1 Synthetic example: the 'sinc' function  The function sinc(x) = [Xl --1 sin [x I is commonly used to illustrate support vector  regression [8], where in place of the classification margin, the e-insensitive region is  introduced, a 'tube' of +e around the function within which errors are not penalised.  In this case, the support vectors lie on the edge of, or outside, this region. For  example, using linear spline kernels and with e = 0.01, the approximation of sinc(x)  based on 100 uniformly-spaced noise-free samples in [-10, 10] utilises 39 support  vectors [8].  By comparison, we approximate the same function with a relevance vector model  utilising the same kernel. In this case the noise variance is.fixed at 0.012 and a  alone re-estimated. The approximating function is plotted in Figure I (left), and  requires only 9 relevance vectors. The largest error is 0.0087, compared to 0.01 in  the SV case. Figure I (right) illustrates the case where Gaussian noise of standard  deviation 0.2 is added to the targets. The approximation uses 6 relevance vectors,  and the noise is automatically estimated, using (10), as a = 0.189.  1  0.8  0.6  0.4  0.2  o  -0.2  -lO  1.2  0.8 . ,,/ .  0.(5 ,  0.4. ,  0.2      0.2   0.4 -10 - 0 5 10  Figure 1' Relevance vector approximation to sinc(x): noise-free data (left), and with  added Gaussian noise of a = 0.2 (right). The estimated functions are drawn as solid lines  with relevance vectors shown circled, and in the added-noise case (right) the true function  is shown dashed.  3.2 Some benchmarks  The table below illustrates regression performance on some popular benchmark  datasets -- Friedman's three synthetic functions (results averaged over 100 ran-  domly generated training sets of size 240 with a 1000-example test set) and the  'Boston housing' dataset (averaged over 100 randomised 481/25 train/test splits).  The prediction error obtained and the number of kernel functions required for both  support vector regression (SVR) and relevance vector regression (RVR) are given.  __ errors _ kernels _  Dataset SVR RVR SVR RVR  Friedman #1 2.92 2.80 116.6 59.4  Friedman #2 4140 3505 110.3 6.9  Friedman #3 0.0202 0.0164 106.5 11.5  Boston Housing 8.04 7.46 142.8 39.0  656 M. E. Tipping  4 Relevance Vector Classification  We now extend the relevance vector approach to the case of classification -- i.e.  where it is desired to predict the posterior probability of class membership given the  input x. We generalise the linear model by applying the logistic sigmoid function  o'(y) = 1/(1 + e -y) to y(x) and writing the likelihood as  N  P(tlw) = II {Y{y(Xn)}tn [1 --(Y{y(Xn)}] 1-t (11)  n----1  However, we cannot integrate out the weights to obtain the marginal likelihood  analytically, and so utilise an iterative procedure based on that of MacKay [3]:  1. For the current, fixed, values of e we find the most probable weights wMp  (the location of the posterior mode). This is equivalent to a standard opti-  misation of a regularised logistic model, and we use the efficient iteratively-  reweighted least-squares algorithm [5] to find the maximum.  2. We compute the Hessian at WMp:  X7Vlogp(t,w]ct) WMr--- --(TB + A), (12)  where Bnn = a{y(xn)} [1- a{y(xn)}], and this is negated and inverted to  give the covariance  for a Gaussian approximation to the posterior over  weights, and from that the hyperparameters et are updated using (9). Note  that there is no 'noise' variance a 2 here.  This procedure is repeated until some suitable convergence criteria are satisfied.  Note that in the Bayesian treatment of multilayer neural networks, the Gaussian  approximation is considered a weakness of the method if the posterior mode is  unrepresentative of the overall probability mass. However, for the RVM, we note  that p(t, wle ) is log-concave (i.e. the Hessian is negative-definite everywhere), which  gives us considerably more confidence in the Gaussian approximation.  5 Examples of RVM Classification  5.1 Synthetic example: Gaussian mixture data  We first utilise artificially generated data in two dimensions in order to illustrate  graphically the selection of relevance vectors. Class I (denoted by ' x ') was sampled  from a single Gaussian, and overlaps to a small degree class 2 ('e'), sampled from a  mixture of two Gaussians.  A relevance vector classifier was compared to its support vector counterpart, using  the same Gaussian kernel. A value of C for the SVM was selected using 5-fold cross-  validation on the training set. The results for a typical dataset of 200 examples  are given in Figure 2. The test errors for the RVM (9.32%) and SVM (9.48%)  are comparable, but the remarkable feature of contrast is the complexity of the  classifiers. The support vector machine utilises 44 kernel functions compared to  just 3 for the relevance vector method.  It is also notable that the relevance vectors are some distance from the decision  boundary (in x-space). Given further analysis, this observation can be seen to be  consistent with the hyperparameter update equations. A more qualitative explana-  tion is that the output of a basis function lying on or near the decision boundary  is a poor indicator of class membership, and such basis functions are naturally  'penalised' under the Bayesian framework.  The Relevance Vector Machine 657  SVM: error=9.48% vectors=44  Figure 2: Results of training functionally identical SVM (left) and RVM (right) clas-  sifters on a typical synthetic dataset. The decision boundary is shown dashed, and rele-  vance/support vectors are shown circled to emphasise the dramatic reduction in complexity  of the RVM model.  5.2 Real examples  In the table below we give error and complexity results for the 'Pima Indian diabetes'  and the 'U.S.P.S. handwritten digit' datasets. The former task has been recently  used to illustrate Bayesian classification with the related Gaussian Process (GP)  technique [9], and we utilised those authors' split of the data into 200 training and  332 test examples and quote their result for the GP case. The latter dataset is a  popular support vector benchmark, comprising 7291 training examples along with  a 2007-example test set, and the SVM result is quoted from [7].  errors  kernels   Dataset SVM GP RVM SVM GP RVM  Pima Indians 67 68 65 109 200 4  U.S.P.S. 4.4% - 5.1% 2540 - 316  In terms of prediction accuracy, the RVM is marginally superior on the Pima set,  but outperformed by the SVM on the digit data. However, consistent with other  examples in this paper, the RVM classifiers utilise many fewer kernel functions.  Most strikingly, the RVM achieves state-of-the-art performance on the diabetes  dataset with only 4 kernels. It should be noted that reduced set methods exist  for subsequently pruning support vector models to reduce the required number of  kernels at the expense of some increase in error (e.g. see [7] for some example results  on the U.S.P.S. data).  6 Discussion  Examples in this paper have effectively demonstrated that the relevance vector  machine can attain a comparable (and for regression, apparently superior) level of  generalisation accuracy as the well-established support vector approach, while at the  same time utilising dramatically fewer kernel functions -- implying a considerable  658 M. E. Tipping  saving in memory and computation in a practical implementation. Importantly, we  also benefit from the absence of any additional nuisance parameters to set, apart  from the need to choose the type of kernel and any associated parameters.  In fact, for the case of kernel parameters, we have obtained improved (both in  terms of accuracy and sparsity) results for all the benchmarks given in Section  3.2 when optimising the marginal likelihood with respect to multiple input scale  parameters in Gaussian kernels (q.v. [9]). Furthermore, we may also exploit the  Bayesian formalism to guide the choice of kernel itself [2], and it should be noted  that the presented methodology is applicable to arbitrary basis functions, so we are  not limited, for example, to the use of 'Mercer' kernels as in the SVM.  A further advantage of the RVM classifier is its standard formulation as a prob-  abilistic generalised linear model. This implies that it can be extended to the  multiple-class case in a st?aightforward and principled manner, without the need  to train and heuristically combine multiple dichotomous classifiers as is standard  practice for the SVM. Furthermore, the estimation of posterior probabilities of class  membership is a major benefit, as these convey a principled measure of uncertainty  of prediction, and are essential if we wish to allow adaptation for varying class  priors, along with incorporation of asymmetric misclassification costs.  However, it must be noted that the principal disadvantage of relevance vector meth-  ods is in the complexity of the training phase, as it is necessary to repeatedly com-  pute and invert the Hessian matrix, requiring O(N 2) storage and O(N 3) computa-  tion. For large datasets, this makes training considerably slower than for the SVM.  Currently, memory constraints limit us to training on no more than 5,000 examples,  but we have developed approximation methods for handling larger datasets which  were employed on the U.S.P.S. handwritten digit databas& We note that while the  case for Bayesian methods is generally strongest when data is scarce, the sparseness  of the resulting classifier induced by the Bayesian framework presented here is a  compelling motivation to apply relevance vector techniques to larger datasets.  Acknowledgements  The author wishes to thank Chris Bishop, John Platt and Bernhard SchSlkopf for  helpful discussions, and JP again for his Sequential Minimal Optimisation code.  References  [1] J. O. Berger. Statistical decision theory and Bayesian analysis. Springer, New York,  second edition, 1985.  [2] D. J. C. Mackay. Bayesian interpolation. Neural Computation, 4(3):415-447, 1992.  [3] D. J. C. Mackay. The evidence framework applied to classification networks. Neural  Computation, 4(5):720-736, 1992.  [4] D. J. C. Mackay. Bayesian non-linear modelling for the prediction competition. In  ASHRAE Transactions, vol. 100, pages 1053-1062. ASHRAE, Atlanta, Georgia, 1994.  [5] I. T. Nabney. Efficient training of RBF networks for classification. In Proceedings of  ICANN99, pages 210-215, London, 1999. IEE.  [6] R. M. Neal. Bayesian Learning for Neural Networks. Springer, New York, 1996.  [7] B. SchSlkopf, S. Mika, C. J. C. Burges, P. Knirsch, K.-R. Mfiller, G. Rtsch, and A. J.  Smola. Input space versus feature space in kernel-based methods. IEEE Transactions  on Neural Networks, 10(5):1000-1017, 1999.  [8] V. N. Vapnik. Statistical Learning Theory. Wiley, New York, 1998.  [9] C. K. I. Williams and D. Barber. Bayesian classification with Gaussian processes.  IEEE Trans. Pattern Analysis and Machine Intelligence, 20(12):1342-1351, 1998.  
Optimal Kernel Shapes for Local Linear  Regression  Dirk Ormoneit Trevor Hastie  Department of Statistics  Stanford University  Stanford, CA 94305-4065  ormoneit@stat. stanford. edu  Abstract  Local linear regression performs very well in many low-dimensional  forecasting problems. In high-dimensional spaces, its performance  typically decays due to the well-known "curse-of-dimensionality".  A possible way to approach this problem is by varying the "shape"  of the weighting kernel. In this work we suggest a new, data-driven  method to estimating the optimal kernel shape. Experiments us-  ing an artificially generated data set and data from the UC Irvine  repository show the benefits of kernel shaping.  I Introduction  Local linear regression has attracted considerable attention in both statistical and  machine learning literature as a flexible tool for nonparametric regression analysis  [Cle79, FG96, AMS97]. Like most statistical smoothing approaches, local modeling  suffers from the so-called "curse-of-dimensionality", the well-known fact that the  proportion of the training data that lie in a fixed-radius neighborhood of a point  decreases to zero at an exponential rate with increasing dimension of the input  space. Due to this problem, the bandwidth of a weighting kernel must be chosen  very big so as to contain a reasonable sample fraction. As a result, the estimates  produced are typically highly biased. One possible way to reduce the bias of local  linear estimates is to vary the "shape" of the weighting kernel. In this work, we  suggest a method for estimating the optimal kernel shape using the training data.  For this purpose, we parameterize the kernel in terms of a suitable "shape matrix",  L, and minimize the mean squared forecasting error with respect to L. For such an  approach to be meaningful, the "size" of the weighting kernel must be constrained  during the minimization to avoid overfitting. We propose a new, entropy-based  measure of the kernel size as a constraint. By analogy to the nearest neighbor  approach to bandwidth selection [FG96], the suggested measure is adaptive with  regard to the local data density. In addition, it leads to an efficient gradient descent  algorithm for the computation of the optimal kernel shape. Experiments using an  artificially generated data set and data from the UC Irvine repository show that  kernel shaping can improve the performance of local linear estimates substantially.  The remainder of this work is organized as follows. In Section 2 we briefly review  Optimal Kernel Shapes for Local Linear Regression 541  local linear models and introduce our notation. In Section 3 we formulate an objec-  tive function for kernel shaping, and in Section 4 we discuss entropic neighborhoods.  Section 5 describes our experimental results and Section 6 presents conclusions.  2 Local Linear Models  Consider a nonlinear regression problem where a continuous response y E JR is to  be predicted based on a d-dimensional predictor x E JRa. Let D -- ((xt,yt),t =  1,...,T) denote a set of training data. To estimate the conditional expectation  f(xo)  E[y[xo], we consider the local linear expansion f(x)  ao + (x - Xo)t/o in  the neighborhood of Xo. In detail, we minimize the weighted least squares criterion  T  c(s, a; xo) = - s - (x, -  (1)  to determine estimates of the parameters so and/o. Here k (xt, xo) is a non-negative  weighting kernel that assigns more weight to residuals in the neighborhood of xo  than to residuals distant from xo. In multivariate problems, a standard way of  defining k(xt, xo) is by applying a univariate, noranegative "mother kernel" b(z) to  the distance measure I[xt - Xol[ =- V/(Xt - Xo)'ft(xt - Xo):  (2)  T '  - o11)  Here  is a positive definite d x d matrix determining the relative importance  assigned to different directions of the input space. For example, if b(z) is a stan-  dard normal density, k(xt,xo) is a normalized multivariate Gaussian with mean  xo and covariance matrix -1. Note that k(xt, xo) is normalized so as to satisfy  Y.tT= k(xt,xo) - 1. Even though this restriction is not relevant directly with re-  gard to the estimation of so and/o, it will be needed in our discussion of entropic  neighborhoods in Section 4.  Using the shorthand notation (xo, ) = (&o,/), the solution of the minimization  problem (1) may be written conveniently as  (X0,)-- (XtWX)-lxtwr,  where X is the T x (d+ 1) design matrix with rows (1,x - x))', Y is the vector of  response values, and W is a T x T diagonal matrix with entries Wt,t = k(xt, xo).  The resulting local linear fit at xo using the inverse covariance matrix f is simply  f(Xo; f) = &o. Obviously, ](xo; f) depends on f through the definition of the  weighting kernel (2). In the discussion below, our focus is on choices of f that lead  to favorable estimates of the unknown function value f(xo).  3 Kernel Shaping  The local linear estimates resulting from different choices of f vary considerably  in practice. A common strategy is to choose f proportional to the inverse sample  covariance matrix. The remaining problem of finding the optimal scaling factor is  equivalent to the problem of bandwidth selection in univariate smoothing [FG96,  BBB99]. For example, the bandwidth is frequently chosen as a function of the  distance between xo and its kth nearest neighbor in practical applications [FG96].  In this paper, we take a different viewpoint and argue that optimizing the "shape"  542 D. Ormoneit and T. Hastie  of the weighting kernel is at least as important as optimizing the bandwidth. More  specifically, for a fixed "volume" of the weighting kernel, the bias of the estimate  can be reduced drastically by shrinking the kernel in directions of large nonlinear  variation of f (x), and stretching it in directions of small nonlinear variation. This  idea is illustrated using the example shown in Figure 1. The plotted function  is sigmoidal along an index vector n and constant in directions orthogonal to n.  Therefore, a "shaped" weighting kernel is shrunk in the direction n and stretched  orthogonally to n, minimizing the exposure of the kernel to the nonlinear variation.  Figure 1: Left: Example of a single index model of the form y = g(x'n) with n -- (1, 1)  and g(z) = tanh(3z). Right: The contours of g(z) are straight lines orthogonal to n.  To distinguish formally the metric and the bandwidth of the weighting kernel, we  rewrite f/as follows:  f _= A. (LL' + I). (4)  Here A corresponds to the inverse bandwidth, and L may be interpreted as a metric-  or shape-matrix. Below we suggest an algorithm which is designed to minimize  the bias with respect to the kernel metric. Clearly, for such an approach to be  meaningful, we need to restrict the "volume" of the weighting kernel; otherwise, the  bias of the estimate could be minimized trivially by choosing a zero bandwidth. For  example, we might define A contingent on L so as to satisfy [f[ - c for some constant  c. A serious disadvantage of this idea is that, by contrast to the nearest neighbor  approach, ]ft[ is independent of the design. As a more appropriate alternative, we  define A in terms of a measure of the number of neighboring observations. In detail,  we fix the volume of k(xt, xo) in terms of the "entropy" of the weighting kernel.  Then, we choose A so as to satisfy the resulting entropy constraint. Given this  definition of the bandwidth, we determine the metric of k(xt, Xo) by minimizing the  mean squared prediction error:  T  C(L;D) -- -(Yt- f(xt; f)) 2 (5)  t----1  with respect to L. In this way, we obtain an approximation of the optimal kernel  shape because the expectation of C(L; D) differs from the bias only by a variance  term which is independent of L. Details of the entropic neighborhood criterion and  of the numerical minimization procedure are described next.  4 Entropic Neighborhoods  We mentioned previously that, for a given shape matrix L, we choose the bandwidth  parameter A in (4) so as to fulfill a volume constraint on the weighting kernel. For  this purpose, we interpret the kernel weights k(xt, xo) as probabilities. In particular,  Optimal Kernel Shapes for Local Linear Regression 543  as k(xt, Xo) > 0 and -.t k(xt, xo) = 1 by definition (2), we can formulate the local  entropy of k(xt, x0):  T  H() -- -  k(xt,xo) log k(xt,xo). (6)  The entropy of a probability distribution is typically thought of as a measure of  uncertainty. In the context of the weighting kernel k(xt,xo), H() can be used  as a smooth measure of the "size" of the neighborhood that is used for averaging.  To see this, note that in the extreme case where equal weights are placed on all  observations in D, the entropy is maximized. At the other extreme, if the single  nearest neighbor of Xo is assigned the entire weight of one, the entropy attains its  minimum value zero. Thus, fixing the entropy at a constant value c is similar to  fixing the number k in the nearest neighbor approach. Besides justifying (6), the  correspondence between k and c can also be used to derive a more intuitive volume  parameter than the entropy level c. We specify c in terms of a hypothetical weighting  kernel that places equal weight on the k nearest neighbors of x0 and zero weight  on the remaining observations. Note that the entropy of this hypothetical kernel  is log k. Thus, it is natural to characterize the size of an entropic neighborhood in  terms of k, and then to determine A by numerically solving the nonlinear equation  system (for details, see [OH99])  H(fi) = logk. (7)  More precisely, we report the number of neighbors in terms of the equivalent sample  fraction p  kit to further intuition. This idea is illustrated in Figure 2 using a  one- and a two-dimensional example. The equivalent sample fractions are p = 30%  and p - 50%, respectively. Note that in both cases the weighting kernel is wider  in regions with few observations, and narrower in regions with many observations.  As a consequence, the number of observations within contours of equal weighting  remains approximately constant across the input space.  04 O O6 0.? 08 0.9  0 o! 02 03 04 05 06 07 08 09  Figure 2: Left: Univariate weighting kernel k(-, xo) evaluated at xo: 0.3 and xo = 0.7  based on a sample data set of 100 observations (indicated by the bars at the bottom). Right:  Multivariate weighting kernel k(-, xo) based on a sample data set of 200 observations. The  two ellipsoids correspond to 95% contours of a weighting kernel evaluated at (0.3, 0.3)' and  (0.6,0.6)'.  To summarize, we define the value of A by fixing the equivalent sample fraction  parameter p, and subsequently minimize the prediction error on the training set  with respect to the shape matrix L. Note that we allow for the possibility that  L may be of reduced rank I _< d as a means of controlling the number of free  parameters. As a minimization procedure, we use a variant of gradient descent that  544 D. Ormoneit and T. Hastie  accounts for the entropy constraint. In particular, our algorithm relies on the fact  that (7) is differentiable with respect to L. Due to space limitations, the interested  reader is referred to [OH99] for a formal derivation of the involved gradients and  for a detailed description of the optimization procedure.  5 Experiments  In this section we compare kernel shaping to standard local linear regression using  a fixed spherical kernel in two examples. First, we evaluate the performance using  a simple toy problem which allows us to estimate confidence intervals for the pre-  diction accuracy using Monte Carlo simulation. Second, we investigate a data set  from the machine learning data base at UC Irvine [BKM98].  5.1 Mexican Hat Function  In our first example, we employ Monte Carlo simulation to evaluate the performance  of kernel shaping in a five-dimensional regression problem. For this purpose, 20 sets  of 500 data points each are generated independently according to the model  y = cos(5v/x2 + x22)  exp(-(Xl 2 + x22)). (8)  Here the predictor variables x,..., x5 are drawn according to a five-dimensionai  standard normal distribution. Note that, even though the regression is carried out  in a five-dimensional predictor space, y is really only a function of the variables  Xl and x2. In particular, as dimensions two through five do not contribute any  information with regard to the value of y, kernel shaping should effectively discard  these variables. Note also that there is no noise in this example.  Figure 3: Left: "True" Mexican hat function. Middle: Local linear estimate using a  spherical kernel (p = 2%). Right: Local linear estimate using kernel shaping (p -- 2%).  Both estimates are based on a training set consisting of 500 data points.  Figure 3 shows a plot of the true function, the spherical estimate, and the estimate  using kernel shaping as functions of x and x2. The true function has the familiar  "Mexican hat" shape, which is recovered by the estimates to different degrees. We  evaluate the local linear estimates for values of the equivalent neighborhood fraction  parameter p in the range from 1 to 15%. Note that, to warrant a fair comparison,  we used the entropic neighborhood also to determine the bandwith of the spherical  estimate. For each value of p, 20 models are estimated using the 20 artificially  generated training sets, and subsequently their performance is evaluated on the  training set and on the test set of 31 x 31 grid points shown in Figure 3. The shape  matrix L has maximal rank I = 5 in this experiment. Our results for local linear  regression using the spherical kernel and kernel shaping are summarized in Table  1. Performance is measured in terms of the mean R2-value of the 20 models, and  standard deviations are reported in parenthesis.  Optima[ Kerne! Shapes for Loca! Linear Regression  Algorithm  Training R 2 Test  spherical kernel p = 1% 0.961 (0.005) 0.215 (0.126)  spherical kernel p = 2% 0.871 (0.014) 0.293 (0.082)  spherical kernel p = 5% 0.680 (0.029) 0.265 (0.043)  spherical kernel p = 10% 0.507 (0.038) 0.213 (0.030)  spherical kernel p = 20% 0.341 (0.039) 0.164 (0.021)  kernel shaping p = 1% 0.995 (0.001) 0.882 (0.024)  kernel shaping p = 2% 0.984 (0.002) 0.909 (0.017)  kernel shaping p = 5% 0.923 (0.009) 0.836 (0.023)  kernel shaping p = 15% 0.628 (0.035) 0.517 (0.035)  Table 1: Performances in the toy problem. The results for kernel shaping were obtained  using 200 gradient descent steps with step size c = 0.2.  The results in Table I indicate that the optimal performance on the test set is  obtained using the parameter values p = 2% both for kernel shaping (R 2 = 0.909)  and for the spherical kernel (R 2 = 0.293). Given the large difference between the  R  values, we conclude that kernel shaping clearly outperforms the spherical kernel  on this data set.  Figure 4: The eigenvectors of the estimate of  obtained on the first of 20 training sets.  The graphs are ordered from left to right by increasing eigenvalues (decreasing extension  of the kernel in that direction): 0.76, 0.76, 0.76, 33.24, 34.88.  Finally, Figure 4 shows the eigenvectors of the optimized  on the first of the 20  training sets. The eigenvectors are arranged according to the size of the correspond-  ing eigenvalues. Note that the two rightmost eigenvectors, which correspond to the  directions of minimum kernel extension, span exactly the x-x-space where the  true function lives. The kernel is stretched in the remaining directions, effectively  discarding nonlinear contributions from x3, x4, and  5.2 Abalone Database  The task in our second example is to predict the age of abalone based on several  measurements. More specifically, the response variable is obtained by counting  the number of rings in the shell in a time-consuming procedure. Preferably, the  age of the abalone could be predicted from alternative measurements that may be  obtained more easily. In the data set, eight candidate measurements including sex,  dimensions, and various weights are reported along with the number of rings of the  abalone as predictor variables. We normalize these variables to zero mean and unit  variance prior to estimation. Overall, the data set consists of 4177 observations. To  prevent possible artifacts resulting from the order of the data records, we randomly  draw 2784 observations as a training set and use the remaining 1393 observations  as a test set. Our results are summarized in Table 2 using various settings for  the rank l, the equivalent fraction parameter p, and the gradient descent step size  c. The optimal choice for p is 20% both for kernel shaping (R  = 0.582) and for  the spherical kernel (R  - 0.572). Note that the performance improvement due to  kernel shaping is negligible in this experiment.  546 D. Ormoneit and T. Hastie  Kernel Training R 2 Test  spherical kernel p = 0.05 0.752 0.543  spherical kernel p = 0.10 0.686 0.564  spherical kernel p = 0.20 0.639 0.572  spherical kernel p -- 0.50 0.595 0.565  spherical kernel p = 0.70 0.581 0.552  spherical kernel p = 0.90 0.568 0.533  kernel shaping I = 5, p = 0.20, c = 0.5 0.705 0.575  kernel shaping I = 5, p = 0.20, c = 0.2 0.698 0.577  kernel shaping I = 2, p -- 0.10, c = 0.2 0.729 0.574  kernel shaping I = 2, p = 0.20, c = 0.2 0.663 0.582  kernel shaping I = 2, p = 0.50, c = 0.2 0.603 0.571  kernel shaping I = 2, p = 0.20, c = 0.5 0.669 0.582  Table 2: Results using the Abalone database after 200 gradient descent steps.  6 Conclusions  We introduced a data-driven method to improve the performance of local linear  estimates in high dimensions by optimizing the shape of the weighting kernel. In  our experiments we found that kernel shaping clearly outperformed local linear re-  gression using a spherical kernel in a five-dimensional toy example, and led to a  small performance improvement in a second, real-world example. To explain the  results of the second experiment, we note that kernel shaping aims at exploiting  global structure in the data. Thus, the absence of a larger performance improve-  ment may suggest simply that no corresponding structure prevails in that data set.  That is, even though optimal kernel shapes exist locally, they may vary accross the  predictor space so that they cannot be approximated by any particular global shape.  Preliminary experiments using a localized variant of kernel shaping did not lead to  significant performance improvements in our experiments.  Acknowledgments  The work of Dirk Ormoneit was supported by a grant of the Deutsche Forschungsge-  meinschaft (DFG) as part of its post-doctoral program. Trevor Hastie was partially  supported by NSF grant DMS-9803645 and NIH grant ROI-CA-72028-01. Carrie  Grimes pointed us to misleading formulations in earlier drafts of this work.  References  JAMS97]  [BBB99]  [BKM9S]  [Cle79]  [FG96]  [OH99]  C. G. Atkeson, A. W. Moore, and S. Schaal. Locally weighted learning. Artificial  Intelligence Review, 11:11-73, 1997.  M. Birattari, G. Bontempi, and H. Bersini. Lazy learning meets the recursive  least squares algorithm. In M. J. Kearns, S. A. Solla, and D. A. Cohn, editors,  Advances in Neural Information Processing Systems 11. The MIT Press, 1999.  C. Blake, E. Koegh, and C. J. Merz. UCI Repository of machine learning  databases. http://www. ics. uci. edu/-mlearn/MLRepository. html.  W. S. Cleveland. Robust locally weighted regression and smoothing scatterplots.  Journal of the American Statistical Association, 74:829-836, 1979.  J. Fan and I. Gijbels. Local Polynomial Modelling and Its Applications. Chap-  man & Hall, 1996.  D. Ormoneit and T. Hastie. Optimal kernel shapes for local linear regression.  Tech. report 1999-11, Department of Statistics, Stanford University, 1999.  
Topographic Transformation as a  Discrete Latent Variable  Nebojsa Jojic  Beckman Institute  University of Illinois at Urbana  www.ifp.uiuc.edu/~jojic  Brendan J. Frey  Computer Science  University of Waterloo  www.cs.uwaterloo.ca/~frey  Abstract  Invariance to topographic transformations such as translation and  shearing in an image has been successfully incorporated into feed-  forward mechanisms, e.g., "convolutional neural networks", "tan-  gent propagation". We describe a way to add transformation invari-  ance to a generafive density model by approximating the nonlinear  transformation manifold by a discrete set of transformations. An  EM algorithm for the original model can be extended to the new  model by computing expectations over the set of transformations.  We show how to add a discrete transformation variable to Gaussian  mixture modeling, factor analysis and mixtures of factor analysis.  We give results on filtering microscopy images, face and facial pose  clustering, and handwritten digit modeling and recognition.  I Introduction  Imagine what happens to the point in the N-dimensional space corresponding to an  N-pixel image of an object, while the object is deformed by shearing. A very small  amount of shearing will move the point only slightly, so deforming the object by  shearing will trace a continuous curve in the space of pixel intensities. As illustrated  in Fig. la, extensive levels of shearing will produce a highly nonlinear curve (consider  shearing a thin vertical line), although the curve can be approximated by a straight  line locally.  Linear approximations of the transformation manifold have been used to signif-  icantly improve the performance of feedforward discriminative classifiers such as  nearest neighbors (Simard et at., 1993) and multilayer perceptrons (Simard et at.,  1992). Linear generarive models (factor analysis, mixtures of factor analysis) have  also been modified using linear approximations of the transformation manifold to  build in some degree of transformation invariance (Hinton et al., 1997).  In general, the linear approximation is accurate for transformations that couple  neighboring pixels, but is inaccurate for transformations that couple nonneighboring  pixels. In some applications (e.g., handwritten digit recognition), the input can be  blurred so that the linear approximation becomes more robust.  For significant levels of transformation, the nonlinear manifold can be better mod-  eled using a discrete approximation. For example, the curve in Fig. la can be  4 78 N. Jojic and B. J. Fret  (b)  (c)  p(z)  (d)  Y  (e)  Figure 1: (a) An N-pixel greyscale image is represented by a point (untilled disc) in an N-  dimensional space. When the object being imaged is deformed by shearing, the point moves  along a continuous curve. Locally, the curve is linear, but high levels of shearing produce a  highly nonlinear curve, which we approximate by discrete points (filled discs) indexed by . (b)  A graphical model showing how a discrete transformation variable  can be added to a density  model p(z) for a latent image z to model the observed image x. The Gaussian pdf p(x{, z)  captures the th transformation plus a small amount of pixel noise. (We use a box to represent  variables that have Gaussian conditional pdfs.) We have explored (c) transformed mixtures  of Gaussians, where c is a discrete cluster index; (d) transformed component analysis (TCA),  where y is a vector of Gaussian factors, some of which may model locally linear transformation  perturbations; and (e) mixtures of transformed component analyzers, or transformed mixtures  of factor analyzers.  represented by a set of points (filled discs). In this approach, a discrete set of possi-  ble transformations is specified beforehand and parameters are learned so that the  model is invariant to the set of transformations. This approach has been used to  design "convolutional neural networks" that are invariant to translation (Le Cun  et al., 1998) and to develop a general purpose learning algorithm for generarive  topographic maps (Bishop et al., 1998).  We describe how invariance to a discrete set of known transformations (like transla-  tion) can be built into a generafive density model and we show how an EM algorithm  for the original density model can be extended to the new model by computing ex-  pectations over the set of transformations. We give results for 5 different types of  experiment involving translation and shearing.  2 Transformation as a Discrete Latent Variable  We represent transformation  by a sparse transformation generating matrix Gt that  operates on a vector of pixel intensities. For example, integer-pixel translations of  an image can be represented by permutation matrices. Although other types of  transformation matrix may not be accurately represented by permutation matrices,  many useful types of transformation can be represented by sparse transformation  matrices. For example, rotation and blurring can be represented by matrices that  have a small number of nonzero elements per row (e.g., at most 6 for rotations).  The observed image x is linked to the nontransformed latent image z and the  transformation index   {1,... , L} as follows:  p(xlt, z) - Af(x; Gez, (1)  where  is a diagonal matrix of pixel noise variances. Since the probability of  a transformation may depend on the latent image, the joint distribution over the  latent image z, the transformation index  and the observed image x is  p(x, , z) = Af(x; Gez, 'I)P(lz)p(z). (2)  The corresponding graphical model is shown in Fig. lb. For example, to model noisy  transformed images of just one shape, we choose p(z) to be a Gaussian distribution.  Topographic Transformation as a Discrete Latent Variable 479  2.1 Transformed mixtures of Gaussians (TMG). Fig. lc shows the graph-  ical model for a TMG, where different clusters may have different transformation  probabilities. Cluster c has mixing proportion 7re, mean/c and diagonal covariance  matrix I'c. The joint distribution is  p(x, , z, c) = A/'(x; Gtz, )A/'(z; Ic, c)Ptc7rc,  (3)  where the probability of transformation t for cluster c is Ptc. Marginalizing over  the latent image gives the cluster/transformation conditional likelihood,  p(xl, c) - JV'(x; Gt/,c, GtcGt T + 9),  (4)  which can be used to compute p(x) and the cluster/transformation responsibility  P(, clx ). This likelihood looks like the likelihood for a mixture of factor analyzers  (Ghahramani and Hinton, 1997). However, whereas the likelihood computation for  N latent pixels takes order N 3 time in a mixture of factor analyzers, it takes linear  time, order N, in a TMG, because Gtc(]/T +  is sparse.  2.2 Transformed component analysis (TCA). Fig. ld shows the graphical  model for TCA (or "transformed factor analysis"). The latent image is modeled  using linearly combined Gaussian factors, y. The joint distribution is  p(x, t, z, y) = At(x; Gtz, )Ar(z;/ + Ay, )Ar(y; 0, I)pt,  (5)  where/ is the mean of the latent image, A is a matrix of latent image components  (the factor loading matrix) and  is a diagonal noise covariance matrix for the latent  image. Marginalizing over the factors and the latent image gives the transformation  conditional likelihood,  p(xl ) - Af(x; Ge/, Ge(AA T + I,)G T + 9),  (6)  which can be used to compute p(x) and the transformation responsibility p(lx).  G/(AA T + I,)(]/T is not sparse, so computing this likelihood exactly takes N 3  time. However, the likelihood can be computed in linear time if we assume  IGi(AA T + I')G + 91 m IGt( AAT + I')GTI, which corresponds to assuming  that the observed noise is smaller than the variation due to the latent image, or  that the observed noise is accounted for by the latent noise model, I,. In our ex-  periments, this approximation did not lead to degenerate behavior and produced  useful models.  By setting columns of A equal to the derivatives of/ with respect to continuous  transformation parameters, a TCA can accommodate both a local linear approxi-  mation and a discrete approximation to the transformation manifold.  2.3 Mixtures of transformed component analyzers (MTCA). A combi-  nation of a TMG and a TCA can be used to jointly model clusters, linear compo-  nents and transformations. Alternatively, a mixture of Ganssians that is invariant  to a discrete set of transformations and locally linear transformations can be ob-  tained by combining a TMG with a TCA whose components are all set equal to  transformation derivatives.  The joint distribution for the combined model in Fig. le is  p(x,, z, c,y) - A/'(x; Gtz, 9)A/'(z;/ c + Acy,c).N'(y;O,I)ptc7rc. (7)  The cluster/transformation likelihood is p(x[, c) = A/'(x; Gi/ c, Gi(AcA +  I,c)G/T + ), which can be approximated in linear time as for TCA.  480 N. Jojic and B. J. Frey  3 Mixed Transformed Component Analysis (MTCA)  We present an EM algorithm for MTCA; EM algorithms for TMG or TCA emerge  by setting the number of factors to 0 or setting the number of clusters to 1.  Let 0 represent a parameter in the generarive model. For i.i.d. data, the derivative  of the log-likelihood of a training set Xl,... , XT with respect to 0 can be written  T  Ologp(xl,...,XT) -.E[ologp(xt,c,t,z,Y)lxt ] (8)  ..--_  O0 '  where the expectation is taken over p(c, , z, ylxt). The EM algorithm iteratively  solves for a new set of parameters using the old parameters to compute the expec-  tations. This procedure consistently increases the likelihood of the training data.  By setting (8) to 0 and solving for the new parameter values, we obtain update equa-  tions based on the expectations given in the Appendix. Notation: ] = 7 Y-t= [')  is a sufficient statistic computed by averaging over the training set; aiag(A) gives a  vector containing the diagonal elements of matrix A; diag(a) gives a diagonal matrix  whose diagonal contains the elements of vector a; and a o b gives the element-wise  product of vectors a and b. Denoting the updated parameters by "~", we have  = (P(clx,)), = (p(tlx,,c)),  c = (?(clx,)E[z- Acylx,,c])  (P(clx,)) ' (10)  c = diag({P(clxt) E[(z- t c - Acy)o(z- t c - Acy)Ixt,  <P(ctx,)) , (11)   = diag((E[(xt- Gtz)o (xt-Gtz)Ixt]}), (12)  ,c = <P(clxt)E[(z - Ic)yTlxt]><P(clxt)E[yyTtxt]>-l. (13)  To reduce the number of parameters, we will sometimes assume Ptc does not depend  on c or even that Ptc is held constant at a uniform distribution.  4 Experiments  4.1 Filtering Images from a Scanning Electron Microscope (SEM).  SEM images (e.g., Fig. 2a) can have a very low signal to noise ratio due to a  high variance in electron emission rate and modulation of this variance by the im-  aged material (Golem and Cohen, 1998). To reduce noise, multiple images are  usually averaged and the pixel variances can be used to estimate certainty in ren-  dered structures. Fig. 2b shows the estimated means and variances of the pixels  from 230 140 x 56 SEM images like the ones in Fig. 2a. In fact, averaging images  does not take into account spatial uncertainties and filtering in the imaging process  introduced by the electron detectors and the high-speed electrical circuits.  We trained a single-cluster TMG with 5 horizontal shifts and 5 vertical shifts on  the 230 SEM images using 30 iterations of EM. To keep the number of parameters  almost equal to the number of parameters estimated using simple averaging, the  transformation probabilities were not learned and the pixel variances in the observed  image were set equal after each M step. So, TMG had I more parameter. Fig. 2c  shows the mean and variance learned by the TMG. Compared to simple averaging,  the TMG finds sharper, more detailed structure. The variances are significantly  lower, indicating that the TMG produces a more confident estimate of the image.  Topographic Transformation as a Discrete Latent Variable 481  (a) ..... (b)  (c)  Figure 2: (a) 140 x 56 pixel SEM images. (b) The mean and variance of the image pixels.  (c) The mean and variance found by a TMG reveal more structure and less uncertainty.  (a) ' ..,, ,, (d)  (b) .. (f) -  (c) (g) '-" '  Figure 3: (a) Frontal face images of two people. (b) Cluster means learned by a TMG and  (c) a mixture of Gaussians. (d) Images of one person with different poses. (e) Cluster means  learned by a TMG. (f) Less detailed cluster means learned by a mixture of Gaussians. (g) Mean  and first 4 principal components of the data, which mostly model lighting and translation.  4.2 Clustering Faces and Poses. Fig. 3a shows examples from a training  set of 400 jerky images of two people walking across a cluttered background. We  trained a TMG with 4 clusters, 11 horizontal shifts and 11 vertical shifts using  15 iterations of EM after initializing the weights to small, random values. The  loop-rich MATLAB script executed in 40 minutes on a 500MHz Pentium processor.  Fig. 3b shows the cluster means, which include two sharp representations of each  person's face, with the background clutter suppressed. Fig. 3c shows the much  blurrier means for a mixture of Gaussians trained using 15 iterations of EM.  Fig. 3d shows examples from a training set of 400 jerky images of one person with  different poses. We trained a TMG with 5 clusters, 11 horizontal shifts and 11  vertical shifts using 40 iterations of EM. Fig. 3e shows the cluster means, which  capture 4 poses and mostly suppress the background clutter. The mean for cluster  4 includes part of the background, but this cluster also has a low mixing proportion  of 0.1. A traditional mixture of Gaussians trained using 40 iterations of EM finds  blurrier means, as shown in Fig. 3f. The first 4 principal components mostly try to  account for lighting and translation, as shown in Fig. 3g.  482 N. Jojic and B. J. Frey  (c)  (b)  Figure 4: Modeling handwritten digits. (a) Means and components and (b) the sheared +  translated means (dimmed transformations have low probability) for each of 10 TEA models  trained on 200 examples of each digit. (c) Means and components of 10 FA models trained  on the same data. (d) Digits generated from the 10 TCA models and (e) the 10 FA models.  (f) The means for a mixture of 10 Gaussians, a mixture of 10 factor analyzers and a 10-cluster  TMG trained on all 2000 digits. In each case, the best of 10 experiments was selected.  4.3 Modeling Handwritten Digits. We performed both supervised and un-  supervised learning experiments on 8 x 8 greyscale versions of 2000 digits from the  CEDAR CDROM (Hull, 1994). Although the preprocessed images fit snugly in the  8 x 8 window, there is wide variation in "writing angle" (e.g., the vertical stroke  of the 7 is at different angles). So, we produced a set of 29 shearing+translation  transformations (see the top row of Fig. 4b) to use in transformed density models.  In our supervised learning experiments, we trained one 10-component TCA on each  class of digit using 30 iterations of EM. Fig. 4a shows the mean and 10 components  for each of the 10 models. The lower 10 rows of images in Fig. 4b show the sheared  and translated means. In cases where the transformation probability is below 1%,  the image is dimmed. We also trained one 10-component factor analyzer on each  class of digit using 30 iterations of EM. The means and components are shown  in Fig. 4c. The means found by TCA are sharper and whereas the components  found by factor analysis often account for writing angle (e.g., see the components  for 7) the components found by TCA tend to account for line thickness and arc  size. Fig. 4d and e show digits that were randomly generated from the TCAs and  the factor analyzers. Since different components in the factor analyzers account  for different stroke angles, the simulated digits often have an extra stroke, whereas  digits simulated from the TCAs contain fewer spurious strokes.  To test recognition performance, we trained 10-component factor analyzers and  TCAs on 200 examples of each digit using 50 iterations of EM. Each set of models  used Bayes rule to classify 1000 test patterns and while factor analysis gave an error  rate of 3.2%, TCA gave an error rate of only 2.7%.  In our unsupervised learning experiments, we fit 10-cluster mixture models to the  entire set of 2000 digits to see which models could identify all 10 digits. We tried a  mixture of 10 Gaussians, a mixture of 10 factor analyzers and a 10-cluster TMG. In  each case, 10 models were trained using 100 iterations of EM and the model with  Topographic Transformation as a Discrete Latent Variable 483  the highest likelihood was selected and is shown in Fig. 4f. Compared to the TMG,  the first two methods found blurred and repeated classes. After identifying each  cluster with its most prevalent class of digit, we found that the first two methods  had error rates of 53% and 49%, but the TMG had a much lower error rate of 26%.  5 Summary  In many learning applications, we know beforehand that the data includes transfor-  mations of an easily specified nature (e.g., shearing of digit images). If a generafive  density model is learned from the data, the model must extract a model of both  the transformations and the more interesting and potentially useful structure. We  described a way to add transformation invariance to a generatire density model by  approximating the transformation manifold with a discrete set of points. This re-  leases the generatire model from needing to model the transformations. 5 different  types of experiment show that the method is effective and quite efficient.  Although the time needed by this method scales exponentially with the dimensional-  ity of the transformation manifold, we believe that it will be useful in many practical  applications and that it illustrates what is possible with a generafive model that  incorporates a latent transformation variable. We are exploring the performance of  a faster variational learning method and extending the model to time series.  Acknowledgements. We used CITO, NSERC, NSF and Beckman Foundation grants.  References  C. M. Bishop, M. Svensen and C. K. I. Williams 1998. GTM: The generative topographic mapping.  Neural Computation 10:1,215-235.  G. E. Hinton, P. Dayan and M. Revow 1997. Modeling the manifolds of images of handwritten digits.  IEEE Trans. on Neural Networks 8, 65-74.  Z. Ghahramani and G. E. Hinton 1997. The F,M algorithm for mixtures of factor analyzers. University  of Toronto Technical Report CRG-TR-96-1. Available at www.gatsby. ucl.ac.uk/zoubin.  R. Golem and I. Cohen 1998. Scanning electron microscope image enhancement. School of Computer  and Electrical Engineering project report, Ben-Gurion University.  J. J. Hull 1994. A database for handwritten text recognition research. IEEE Trans. on Pattern Analysis  and Machine Intelligence 16:5, 550-554.  Y. Le Cun, L. Bottou, Y. Bengio and P. Haffner 1998. Gradient-based learning applied to document  recognition. Proceedings of the IEEE 86:11, November, 2278-2324.  P. Y. Simard, B. Victorri, Y. Le Cun and J. Denker 1992. Tangent Prop - A formalism for specifying  selected invariances in an adaptive network. In Advances in Neural Information Processing Systems  4, Morgan Kaufmann, San Mateo, CA.  P. Y. Simard, Y. Le Cun and J. Denker 1993. Efficient pattern recognition using a new transformation  distance. In S. J. Hanson, J. D. Cowan and C. L. Giles, Advances in Neural Information Processing  Systems 5, Morgan Kaufmann, San Mateo, CA.  Appendix: The Sufficient Statistics Found in the E-Step  The sufficient statistics for the M~Step are computed in the E-Step using sparse linear algebra dur-  ing a single pass through the training set. Before making this pass, the following matrices are com-  puted: t,c COV(z[x,y,,c) = (I'  + ' - -  = Gt Gt) , Dt,c = COV(ytx,,c) = (I + A?I'IAc --  A'et,e'Ae) -. For each case in the training set, P(c, [xt) is first computed for each combina-  tion of c,, before computing E[y[xt,, c] ' -  ' -   =t,cAcc [tt,,at x-(I-t,,7)t,,l,E[.lx,&c] =  ' - AcI3t,cAcI'c l']t,cGtq, (xt -Gtlac) , E[(uc):(uc)lxt, , c] =  1 , 1  (E[z[xt,,c]-c)o(E[zlxt,,c]-)+diag(flt,)+diag(flt,cI,- AcDt,cAcI, 11t,c), E[(z-/c)y'lxt, , c] =  (E[zlx, , c] -t.)E[ylx,e, 4'+ 11,.'I'7AcDt,c  The expectations needed in (10)-(13) are then com-  puted from P(c[xt)E[z -- A,ylxt, c] = Et P(c, lx)W,['-Ix, , c] - AE[ylx, , c]), P(clx)E[(--- -  Acy)o(z-tc-Acy) Ixt, c] = Yt P(c, 1xt ) {E[(z-tc)o(z-tc)lxt, , c] + diag(AcDt,cA2) -- 2diag(AcE[(z-  tc)y' [x, , c]') + (AcE[ylxt, , c])o(Ac E[ylxt, , c]) }, E[(xt-Gtz)o(xt-Gtz)Ixt] = Ec,t P(c, l[xt ) { (xt-  GtE[zlxt, , c]) o (xt - GeE[zlxt, , c]) + diag(Gefle,eG) + diag(Gtflt,e-AeDt,eA;-flt,eG[)},  P(clxt)E[(z-t)y'lxt, c] = E P(c, llxt)E[(z-t)y'lxt, l, c], P(clxt)E[yy'lxt, c] = Et P(c,llxt)Dt,, +  Et P(c, tlxt)E[ylxt, l, c]E[ylxt, l, c]'.  
Algebraic Analysis for Non-Regular  Learning Machines  Sumio Watanabe  Precision and Intelligence Laboratory  Tokyo Institute of Technology  4259 Nagatsuta, Midori-ku, Yokohama 223 Japan  swatanabpi. titech. ac.jp  Abstract  Hierarchical learning machines are non-regular and non-identifiable  statistical models, whose true parameter sets are analytic sets with  singularities. Using algebraic analysis, we rigorously prove that  the stochastic complexity of a non-identifiable learning machine  is asymptotically equal to Allogn- (ml - 1)loglogn q-const.,  where n is the number of training samples. Moreover we show that  the rational number A1 and the integer ml can be algorithmically  calculated using resolution of singularities in algebraic geometry.  Also we obtain inequalities 0  /1  d/2 and I _< ml _< d, where d  is the number of parameters.  I Introduction  Hierarchical learning machines such as multi-layer perceptrons, radial basis func-  tions, and normal mixtures are non-regular and non-identifiable learning machines.  If the true distribution is almost contained in a learning model, then the set of  true parameters is not one point but an analytic variety [4][9][3][10]. This paper  establishes the mathematical foundation to analyze such learning machines based  on algebraic analysis and algebraic geometry.  Let us consider a learning machine represented by a conditional probability density  p(xlw ) where x is an M dimensional vector and w is a d dimensional parameter. We  assume that n training samples X n -- {Xi; i = 1, 2, ..., n} are independently taken  from the true probability distribution q(x), and that the set of true parameters  *Vo = {w c ,v ; p(xlw)= q(x) (a.s. q(x)) }  is not empty. In Bayes statistics, the estimated distribution p(xIX n) is defined by  / in  p(xlX n) = p(xlw ) pn(w)dw, pn(W) -- n IIP(Xilw)  i----1  where (w) is an a priori probability density on R a, and Zn is a normalizing con-  stant. The generalization error is defined by  f q(x) x  K(n) = Ex,{ q(x) logp(xlX,)  Algebraic Analysis for Non-regular Learning Machines 357  where Ex, {.} shows the expectation value over all training samples X '. One of  the main purposes in learning theory is to clarify how fast K(n) converges to zero  as n tends to infinity. Using the log-loss function h(x, w) = log q(x) - logp(x, w),  we define the Kullback distance and the empirical one,  I  h(Xi, w).  Z(w) ---- h(x, w)q(x)dx, Z(w,X n) -- n i--1  Note that the set of true parameters is equal to the set of zeros of H(w), W0 =  {w E W; H(w) = 0}. If the true parameter set W0 consists of only one point, the  learning machine p(xlw ) is called identifiable, if otherwise non-identifiable. It should  be emphasized that, in non-identifiable learning machines, W0 is not a manifold  but an analytic set with singular points, in general. Let us define the stochastic  complexity by  F(n) = -Ex.{log / exp(-nH(w, Xn)p(w)dw}.  (1)  Then we have an important relation between the stochastic complexity F(n) and  the generalization error K(n)  K(n) = F(n + 1)- F(n),  which represents that K(n) is equal to the increase of F(n) [1]. In this paper, we  show the rigorous asymptotic form of the stochastic complexity F(n) for general  non-identifiable learning machines.  2 Main Results  We need three assumptions upon which the main results are proven.  (A.1) The probability density q(w) is infinite times continuously differentiable and  its support, W = supp q, is compact. In other words, q E C.  (A.2) The log loss function, h(x, w) = log q(x) - log p(x, w), is continuous for x in  the support Q = suppq, and is analytic for w in an open set W  D W.  (A.3) Let {rj(x, w*);j = 1,2,..., d} be the associated convergence radii of h(x, w)  at w*, in other words, Taylor expansion of h(x, w) at w* = (w, ..., w),  h(x,w) --  kl,..,kd=O  aklk2...kl(x)(w 1 -- w)kl(w2 -- w)k2 ...(W d -- W) kd  absolutely converges in Iwj -  j = 1,2,...,d.  Assume inf inf  xEQw*EW  rj(x, w*) > 0 for  Theorem 1 Assume (A.1),(A.2), and (A.3). Then, there exist a rational number  A1 > O, a natural number ml, and a constant C, such that  IF(n)-/1 logn q-(ml -- 1)loglogn I < C  holds for an arbitrary natural number n.  Remarks. (1) If q(x) is compact supported, then the assumption (A.3) is automat-  ically satisfied. (2) Without assumptions (A.1) and (A.3), we can prove the upper  bound, F(n) _</1 logn - (ml - 1) loglogn q- const.  358  anabe  From Theorem 1, if the generalization error K(n) has the asymptotic expansion,  then it should be  hi ml -- 1 1  K(n) - +o(--).  n n log n n log n  As is well known, if the model is identifiable and has the positive definite Fisher  information matrix, then /1 - d/2 (d is the dimension of the parameter space) and  ml = 1. However, hierarchical learning models such as multi-layer percepttons,  radial basis functions, and normal mixtures have smaller/1 and larger ral, in other  words, hierarchical models are better learning machines than regular ones if Bayes  estimation is applied. Constants 1 and ral are characterized by the following  theorem.  Theorem 2 Assume the same conditions as theorem 1. Let e > 0 be a sufficiently  small constant. The holomorphic function in Re(z)  O,  (z) = fu I(w)Z(w)dw'  ()<  can be analytically continued to the entire complex plane as a meromorphic function  whose poles are on the negative part of the real axis, and the constants -A1 and ml  in theorem i are equal to the largest pole of J(z) and its multiplicity, respectively.  The proofs of above theorems are explained in the following section. Let w = g(u)  is an arbitrary analytic function from a set U C R d to W. Then J(z) is invariant  under the mapping,  {H(w), (w)}  (H(g(u)),  where [g'(u)l = [det(Owi/Ouj) I is Jacobian. This fact shows that /1 and ml are in-  variant under a bi-rational mapping. In section 4, we show an algorithm to calculate  /1 and ral by using this invariance and resolution of singularities.  3 Mathematical Structure  In this section, we present an outline of the proof and its mathematical structure.  3.1 Upper bound and b-function  For a sufIiciently small constant e > 0, we define F* (n) by  F*(n) = -log/u exp(-nH(w)) (w) dw.  ()<  Then by using the Jensen's inequality, we obtain F(n) _< F* (n). To evaluate F* (n),  we need the b-function in algebraic analysis [6][7]. Sato, Bernstein, Bj6rk, and  Kashiwara proved that, for an arbitrary analytic function H(w), there exist a dif-  ferential operator D(w, Ow, z) which is a polynomial for z, and a polynomial b(z)  whose zeros are rational numbers on the negative part of the real axis, such that  D(w, Ow, z)H(w) z+l -- b(z)H(w)   (2)  for any z C C and any w c Wc = {w  W; H(w) < e}. By using the relation eq.(2),  the holomorphic function J(z) in Re(z) > 0,  Ju 1 fu H w Z+lD*  J(z) -- H(w) =(w)dw = bz) ( ) wo(w)dw'  (w)<c ()<  ,41gebraic ,4nalysis for Non-regular Learning Machines 359  can be analytically continued to the entire complex plane as a meromorphic func-  tion whose poles are on the negative part of the real axis. The poles, which are  rational numbers and ordered from the origin to the minus infinity, are referred to  as -A1, -A2, -As, ..., and their multiplicities are also referred to as ml, m2, m3, ...  Let Ckm be the coefficient of the m-th order of Laurent expansion of J(z) at -Ak.  Then,  K  Ckm  k=l m=l  is holomorphic in Re(z) > -A:+i, and  Let us define a function  (t) = f (t - S(w))(w)dw  for 0 < t < e and I(t) = 0 for e _< t _< 1. Then I(t) connects the function F*(n)  with J(z) by the relations,  (z) - t z (t) dr,  1  F*(n) = -log f0 exp(-nt) I(t) dt.  The inverse Laplace transform gives the asymptotic expansion of I(t) as t - O,  o mk  Ckm tX-I (_ log t) m-1  I(t)=   (m-l)! '  k----1 m=l  resulting in the asymptotic expansion of F*(n),  fo n t dt  F*(n) = -log exp(-t) I() n  = A11ogn--(m1 -- 1)1oglogn +O(1),  which is the upper bound of F(n).  3.2 Lower Bound  We define a random variable  A(X n) -- sup I nl/2(H(w,X n) - H(w)) / H(w) 1/2 I. (a)  wEW  Then, we prove in Appendix that there exists a constant co which is independent  of n such that  sx {A(X) 2} < co.  By using an inequality ab _< (a 2 + b2)/2,  TI, H(w,X n) _ Tl, H(w) -- A(xn)(TI, H(w)) 1/2 _ {TtH(w)- A(xn)2},  which derives a lower bound,  _> -Ex {log /exp(--{nH(w)- A(Xn)2})(w)dw}  -1E {A(Xn) 2} - log/exp(nil(w)  = 2 x 2 )(w)w  F(n)  (5)  (6)  360 S. Watanabe  The first term in eq.(6) is bounded. Let the second term be F,(n), then  F,(n) = -log(Z1 4- Z)  Z1 --- /H exp(nil(w)  (,)< 2  Z2 = /H exp(nil(w)  (w)>e 2 ) 99(w)dw -< exp(--),  which proves the lower bound of F(n),  F(n) _> A1 logn- (ml - 1) loglogn +const.  ) 99(w)dw  const. n -A1 (log n) ml-1  4 Resolution of Singularities  In this section, we construct a method to calculate A1 and mi. First of all, we cover  the compact set 14/0 with a finite union of open sets W a. In other words, 14/0 C  OaW a. Hironaka's resolution of singularities [5][2] ensures that, for an arbitrary  analytic function H(w), we can algorithmically find an open set U a C R d (U a  contains the origin) and an analytic function ga  U a -4 W a such that  H(go(/)) m a() 1 2 . . . d (  U ) (7)  where a(u) > 0 is a positive function and k  0 (1  i  d) are even integers (a(u)  and ki depend on Ua). Note that Jacobian [g(u)l = 0 if and only if u  gl(W0).  finite  (9(=))19(=)1 Cpl,P2 .... ,Pd ' ' + (8)  =   (p,p .... ,p)  By combining eq.(7) with eq.(8), we obtain  J,(z)  /w H(w)Z99(w)  ---- /Uc a(Tl) {?Jl 1 U2 k2 ... ,dkd} Z  For real z, max Jo,(z) _< J(z) _< -'.o, J(z),  ?,1 ,12o.. Ud dul du2... dud.  /1 : min min min Pq + 1  a (Pl .... ,pd) l_<q_<d kq  and ml is equal to the number of q which attains the minimum, min.  l _< q_< d  Remark. In a neighborhood of w0 c W0, the analytic function H(w) is equivalent  to a polynomial Hwo(W), in other words, there exists constants Cl, c > 0 such that  clHwo(W) < H(w) _< cHwo(W ). Hironaka's theorem constructs the resolution map  ga for any polynomial Hwo(W) algorithmically in the finite procedures ( blowing-  ups for nonsingular manifolds in singularities are recursively applied [5]). From the  above discussion, we obtain an inequality, I _< m _< d. Moreover there exists 3' > 0  such that H(w) _< 3'l TM - w0[ 2 in the neighborhood of w0 c W0, we obtain/1 _ d/2.  Example. Let us consider a model (x, y) G R 2 and w = (a, b, c, d) G R 4,  I 1  p(x, ylw) = po(x) (2701/2 exp(-(y - b(x,w))2),  b(x,a,b,c,d) = atanh(bx) + ctanh(dx),  Algebraic Analysis for Non-regular Learning Machines 361  where po(x) is a compact support probability density (not estimated). We also  assume that the true regression function is y = p(x, 0, 0, 0, 0). The set of true  parameters is  Wo -- {Ex b(X, a, b, c, d) 2 =0} = {ab + cd : O and ab a + cd a -0}.  Assumptions (A.1),(A.2), and (A.3) are satisfied. The singularity in W0 which gives  the smallest A is the origin and the average loss function in the neighborhood W   of the origin is equivalent to the polynomial Ho(a, b, c, d) = (ab +cd) 2 + (ab a +cd3) 2,  (see[9]). Using blowing-ups, we find a map g: (x, y, z, w) H (a, b, c, d),  a-x, b = yaw - yzw, c = zwx, d = y,  by which the singularity at the origin is resolved.  J(z) = /w H(a'b'c'd)ZT(a'b'c'd)dadbdcdd  o  = f{ x2y6w211 + (z + w2(y 2 -- z)a)2]}lxyawl(g(x,y,z,w))dxdydzdw,  which shows that A = 2/3 and ra = 1, resulting that F(n) = (2/3) log n + Const.  If the generalization error can be asymptotically expanded, then K(n)  (2/3n).  5 Conclusion  Mathematical foundation for non-identifiable learning machines is constructed based  on algebraic analysis and algebraic geometry. We obtained both the rigorous asymp-  totic form of the stochastic complexity and an algorithm to calculate it.  Appendix  In the appendix, we show the inequality eq.(5).  Lemma 1  Assume conditions (A.1), (A.2) and (A.3). Then  n  E x, { sup 1  wEwl [ n(Xi, w)-Exn(X,w) ] l")  i=1  This lemma is proven by using just the same method as [10]. In order to prove  (5), we divide 'supwew' in eq.(4) into 'suPH(w)> c' and 'suPH(w)<c'. Finiteness of  the first half is directly proven by Lemma 1. Let us prove the second half is also  finite. We can assume without loss of generality that w is in the neighborhood  of w0 c W0, because W can be covered by a finite union of neighborhoods. In  each neighborhood, by using Taylor expansion of an analytic function, we can find  functions {fj(x, w)} and {gj(w) = 1-gi(wi- woi) a } such that  J  h(x, w) -- E gj(w)fj(x, w), (9)  j=l  where {fj(x, w0)} are linearly independent functions of x and gj(wo) = 0. Since  gj(w)fj(x, w) is a part of Taylor expansion among w0, fj(x, w) satisfies  n  Ex,, { sup I 1---- E(fj(Xi, w ) - Exfj(X,w))l 2} < o. (10)  wEW  i=1  362 S. Watanabe  By using a definition ff/(w) --]H(w,X n) - H(w)],  ;/(w)  n J  1 E{Egj(w)(fj(Xi, w ) _ Exfj(X,w))}l 2  i=l j=l  J J n  j= j=  where we used Cauchy-Schwarz's inequality. On the other hand, the inequality  logx _) (1/2) (log x) 2 - x + 1 (x > 0) shows that  H(w) -- / q(x) log  q(x) dx_) 1/ q(x) ao a  p(x, w)  q(/)(log )2dx > -- E gj(w)  p(x, W) -- 2 j----1  where a0 > 0 is the smallest eigen value of the positive definite symmetric matrix  E x {fj(X, wo)fk(X, wo)}. Lastly, combining  A(X ) = sup  wCW  1 n  nff/(w) 2 ao sup {E(fj(Xi, w)-Exfj(X,w))}2  H(w) -( - wEW j--1 i----1  with eq.(10), we obtain eq.(5).  Acknowledgments  This research was partially supported by the Ministry of Education, Science, Sports  and Culture in Japan, Grant-in-Aid for Scientific Research 09680362.  References  [1] Amari,S., Murata, N.(1993) Statistical theory of learning curves under entropic loss.  Neural Computation, 5 (4) pp.140-153.  [2] Atiyah, M.F. (1970) Resolution of singularities and division of distributions. Comm.  Pure and Appl. Math., 13 pp.145-150.  [3] Fukumizu,K. (1999) Generalization error of linear neural networks in unidentifiable  cases. Lecture Notes in Computer Science, 1720 Springer, pp.51-62.  [4] Hagiwara,K., Toda,N., Usui,S. (1993) On the problem of applying AIC to determine  the structure of a layered feed-forward neural network. Proc. of IJCNN, 3 pp.2263-2266.  [5] Hironaka, H. (1964) Resolution of singularities of an algebraic variety over a field of  characteristic zero, I,II. Annals of Math., ?9 pp.109-326.  [6] Kashiwara, M. (1976) B-functions and holonomic systems, Invent. Math., 38 pp.33-53.  [7] Oaku, T. (1997) An algorithm of computing b-funcitions. Duke Math. J., 8? pp.115-  132.  [8] Sato, M., Shintani,T. (1974) On zeta functions associated with prehomogeneous vector  space. Annals of Math., 100, pp.131-170.  [9] Watanabe, S.(1998) On the generalization error by a layered statistical model with  Bayesian estimation. IEICE Trans., J81-A pp.1442-1452. English version: Elect. Comm.  in Japan., to appear.  [10] Watanabe, S. (1999) Algebraic analysis for singular statistical estimation. Lecture  Notes in Computer Science, 1720 Springer, pp.39-50.  
Recognizing Evoked Potentials in a Virtual  Environment *  Jessica D. Bayliss and Dana H. Ballard  Department of Computer Science  University of Rochester  Rochester, NY 14627  { bayliss, dana}@ cs. rochester. edu  Abstract  Virtual reality (VR) provides immersive and controllable experimen-  tal environments. It expands the bounds of possible evoked potential  (EP) experiments by providing complex, dynamic environments in or-  der to study cognition without sacrificing environmental control. VR  also serves as a safe dynamic testbed for brain-computer interface (BCI)  research. However, there has been some concern about detecting EP sig-  nals in a complex VR environment. This paper shows that EPs exist at  red, green, and yellow stop lights in a virtual driving environment. Ex-  perimental results show the existence of the P3 EP at "go" and "stop"  lights and the contingent negative variation (CNV) EP at "slow down"  lights. In order to test the feasibility of on-line recognition in VR, we  looked at recognizing the P3 EP at red stop lights and the absence of this  signal at yellow slow down lights. Recognition results show that the P3  may successfully be used to control the brakes of a VR car at stop lights.  1 Introduction  The controllability of VR makes it an excellent candidate for use in studying cognition. It  expands the bounds of possible evoked potential (EP) experiments by providing complex,  dynamic environments in order to study decision making in cognition without sacrificing  environmental control. We have created a flexible system for real-time EEG collection and  analysis from within virtual environments.  The ability of our system to give quick feedback enables it to be used in brain-computer in-  terface (BCI) research, which is aimed at helping individuals with severe motor deficits  to become more independent. Recent BC! work has shown the feasibility of on-line  averaging and biofeedback methods in order to choose characters or move a cursor on  a computer screen with up to 95% accuracy while sitting still and concentrating on  the screen [McFarland et al., 1993; Pfurtscheller et al., 1996; Vaughn et al., 1996;  Farwell and Donchin, 1988]. Our focus is to dramatically extend the BC! by allowing  evoked potentials to propel the user through alternate virtual environments. For example, a  *This research was supported by NIH/PHS grantl-P41-RR09283. It was also facilitated in part  by a National Physical Science Consortium Fellowship and by stipend support from NASA Goddard  Space Flight Center.  4 J.D. Bayliss and D. H. Ballard  Figure 1' (Left) An individual demonstrates driving in the modified go cart.  stoplight scene in the virtual environment  (Right) A typical  user could choose a virtual living room from a menu of rooms, navigate to the living room  automatically in the head-mounted display, and then choose to turn on the stereo.  As shown in [Farwell and Donchin, 1988], the P3 EP may be used for a brain-computer  interface that picks characters on a computer monitor. Discovered by [Chapman and Brag-  don, 1964; Sutton et al., 1965] and extensively studied (see [Polich, 1998] for a literature  review), the P3 is a positive waveform occurring approximately 300-500 ms after an in-  frequent task-relevant stimulus. We show that requiring subjects to stop or go at virtual  traffic lights elicits this EP. The contingent negative variation (CNV), an EP that happens  preceding an expected stimulus, occurs at slow down lights.  In order to test the feasibility of on-line recognition in the noisy VR environment, we  recognized the P3 EP at red stop lights and the lack of this signal at yellow slow down  lights. Results using a robust Kalman filter for off-line recognition indicate that the car  may be stopped reliably with an average accuracy of 84.5% while the on-line average for  car halting is 83%.  2 The Stoplight Experiments  The first experiment we performed in the virtual driving environment shows that a P3 EP  is obtained when subjects stop or go at a virtual light and that a CNV occurs when subjects  see a slow down light. Since all subjects received the same light colors for the slow down,  go, and stop conditions we then performed a second experiment with different light colors  in order to disambiguate light color from the occurrence of the P3 and CNV.  Previous P3 research has concentrated primarily on static environments such as the contin-  uous performance task [Rosvold et al., 1956]. In the visual continuous performance task  (VCPT), static images are flashed on a screen and the subject is told to press a button when  a rare stimulus occurs or to count the number of occurrences of a rare stimulus. This makes  the stimulus both rare and task relevant in order to evoke a P3. As an example, given red  and yellow stoplight pictures, a P3 should occur if the red picture is less frequent than the  yellow and subjects are told to press a mouse button only during the red light. We assumed  a similar response would occur in a VR driving world if certain lights were infrequent and  subjects were told to stop or go at them. This differs from the VCPT in two important  ways:  1. In the VCPT subjects sit passively and respond to stimuli. In the driving task,  Recognizing Evoked Potentials in a lrtual Environment 5  subjects control when the stimuli appear by where they drive in the virtual world.  Since subjects are actively involved and fully immersed in the virtual world, they  make more eye and head movements. The movement amount can be reduced by  a particular experimental paradigm, but it can not be eliminated.  The first difference makes the VR environment a more natural experimental enviroranent.  The second difference means that subjects create more data artifacts with extra movement.  We handled these artifacts by first manipulating the experimental environment to reduce  movements where important stimulus events occurred. This meant that all stoplights were  placed at the end of straight stretches of road in order to avoid the artifacts caused by turning  a corner. For our on-line recognition, we then used the eye movement reduction technique  described in [Semlitsch et aI., 1986] in order to subtract a combination of the remaining  eye and head movement artifact.  2.1 Experimental Setup  All subjects used a modified go cart in order to control the virtual car (see Figure 1). The  virtual reality interface is rendered on a Silicon Graphics Onyx machine wih 4 proces-  sors and an Infinite Reality Graphics Engine. The environment is presented to the subject  through a head-mounted display (HMD). Since scalp EEG recordings are measured in mi-  crovolts, electrical signals may easily interfere during an experiment. We tested the effects  of wearing a VR4 HMD containing an ISCAN eye tracker and discovered that the noise  levels inside of the VR helmet were comparable to noise levels while watching a laptop  screen [Bayliss and Ballard, 1998].  A trigger pulse containing information about the color of the light was sent to the EEG  acquisition system whenever a light changed. While an epoch size from -100 ms to 1 sec  was specified, the data was recorded continuously. Information about head position as well  as gas, braking, and steering position were saved to an external file. Eight electrodes sites  (FZ, CZ, CPZ, PZ, P3, P4, as well as 2 vertical EOG channels) were arranged on the heads  of seven subjects with a linked mastoid reference. Electrode impedances were between  2 and 5 kohms for all subjects. Subjects ranged in age from 19 to 52 and most had no  previous experiences in a virtual environment. The EEG signal was amplified using Grass  amplifiers with an analog bandwidth from 0.1 to 100 Hz. Signals were then digitized at a  rate of 500 Hz and stored to a computer.  2.2 Ordinary Traffic Light Color Experiment  Five subjects were instructed to slow down on yellow lights, stop for red lights, and go for  green lights. These are normal traffic light colors. Subjects were allowed to drive in the  environment before the experiment to get used to driving in VR.  In order to make slow down lights more frequent, all stoplights turned to the slow down  color when subjects were further than 30 meters aways from them. When the subject  drove closer than 30 meters the light then turned to either the go or stop color with equal  probability. The rest of the light sequence followed normal stoplights with the stop light  turning to the go light after 3 seconds and the go light not changing.  We calculated the grand averages over red, green, and yellow light trials (see Figure 2a).  Epochs affected by artifact were ignored in the averages in order to make sure that any  existing movements were not causing a P3-1ike signal. Results show that a P3 EP occurs  for both red and green lights. Back averaging from the green/red lights to the yellow light  shows the existence of a CNV starting at approximately 2 seconds before the light changes  to red or green.  6 J.D. Bayliss and D. H. Ballard  a)  StopLight  Go Light  Slow Down Light  -5 uv  +10 uv  b)  -8 uv  200ms I +12 uv  Figure 2: a) Grand averages for the red stop, green go, and yellow slow down lights. b) Grand  averages for the yellow stop, red go, and green slow down lights. All slow down lights have been  back-averaged from the occurrence of the go/stop light in order to show the existence of a CNV.  2.3 Alternative Traffic Light Colors  The P3 is related to task relevance and should not be related to color, but color needed to  be disambiguated as the source of the P3 in the experiment. We had two subjects slow  down at green lights, stop at yellow lights, and go at red lights. In order to get used to this  combination of colors, subjects were allowed to drive in the town before the experiment.  The grand averages for each light color were calculated in the same manner as the averages  above and are shown in Figure 2b. As expected, a P3 signal existed for the stop condition  and a CNV for the slow down condition. The go condition P3 was much noisier for these  two subjects, although a slight P3-1ike signal is still visible.  3 Single Trial Recognition Results  While averages show the existence of the P3 EP at red stop lights and the absence of such  at yellow slow down lights, we needed to discover if the signal was clean enough for single  trial recognition as the quick feedback needed by a BCI depends on quick recognition.  While there were three light conditions to recognize, there were only two distinct kinds of  evoked potentials. We chose to recognize the difference between the P3 and the CNV since  their averages are very different. Recognizing the difference between two kinds of EPs  gives us the ability to use a BCI in any task that can be performed using a series of binary  decisions. We tried three methods for classification of the P3 EP: correlation, independent  component analysis (ICA), and a robust Kalman filter.  Approximately, 90 slow down yellow light and 45 stop red light trials from each subject  were classified. The reason we allowed a yellow light bias to enter recognition is because  the yellow light currently represents an unimportant event in the environment. In a real  BCI unimportant events are likely to occur more than user-directed actions, making this  bias justifiable.  Recognizing Evoked Potentials in a Virtual Environment 7  Table 1: Recognition Results (p < 0.01)  Correlation%Correct ICA%Correct Robust Kaiman Filter%Correct  Suects Red Yel Toml Red Yel Total Red Yel Total  S1 81 51 64 76 77 77 55 86 77  S2 95 63 73 86 88 87 82 94 90  S3 89 56 66 72 87 82 74 85 81  S4 81 60 67 73 69 71 65 91 82  S5 63 66 65 65 79 74 78 92 87  Table 2: Recognition Results for Return Subjects  Robust K-Filmr%Correct  Suects Red Yel Total  S4 73 90 85  S5 67 87 80  As expected, the data obtained while driving contained artifacts, but in an on-line BCI  these artifacts must be reduced in order to make sure that what the recognition algorithm is  recognizing is not an artifact such as eye movement. In order to reduce these artifacts, we  performed the on-line linear regression technique described in [Semlitsch et al., 1986] in  order to subtract a combination of eye and head movement artifact.  In order to create a baseline from which to compare the performance of other algorithms,  we calculated the correlation of all sample trials with the red and yellow light averages  from each subject's maximal P3 electrode site using the following formula:  correlation =  (sample  ave T)/(ll sample II * I[ ave II)  (1)  where sample and ave are both 1 x 500 vectors representing the trial epochs and light  averages (respectively). We used the whole trial epoch for recognition because it yielded  better recognition than just the time area around the P3. If the highest correlation of a trial  epoch with the red and yellow averages was greater than 0.0, then the signal was classified  as that type of signal. If both averages correlated negatively with the single trial, then the  trial was counted as a yellow light signal. As can be seen in Table 1, the correct signal  identification of red lights was extremely high while the yellow light identification pulled  the results down. This may be explained by the greater variance of the yellow light epochs.  Correlations in general were poor with typical correlations around 0.25.  ICA has successfully been used in order to minimize artifacts in EEG data [Jung et al.,  1997; Vigario, 1997] and has also proven useful in separating P3 component data from an  averaged waveform [Makeig et al., 1997]. The next experiment used ICA in order to try  to separate the background EEG signal from the P3 signal. Independent component anal-  ysis (ICA) assumes that n EEG data channels x are a linear combination of n statistically  independent signals s:  x = As (2)  where x and s are n x 1 vectors. We used the matlab package mentioned in [Makeig et al.,  1997] with default learning values, which finds a matrix W by stochastic gradient descent.  8 J.D. Bayliss and D. H. Ballard  This matrix W performs component separation. All data was sphered in order to speed  convergence time.  After training the W matrix, the source channel showing the closest P3-1ike signal (using  correlation with the average) for the red light average data was chosen as the signal with  which to correlate individual epochs. The trained W matrix was also used to find the  sources of the yellow light average. The red and yellow light responses were then correlated  with individual epoch sources in the manner of the first experiment.  The third experiment used the robust Kalman filter framework formulated by Rao [Rao,  1998]. The Kalman filter assumes a linear model similar to the one of ICA in equation 2,  but assumes the EEG output x is the observable output of a generative or measurement  matrix A and an internal state vector s of Gaussian sources. The output may also have an  additional noise component n, a Gaussian stochastic noise process with mean zero and a  covariance matrix given by  = E[nnW], leading to the model expression: x = As + n. In  order to find the most optimal value of s, a weighted least-squares criterion is formulated:  J = (x - As)TE - (x - As) + (s - g)TM - (s - g)  (3)  where s follows a Gaussian distribution with mean g and covariance M. Minimizing this  criterion by setting o3   = 0 and using the substitution N = (ATE-U q- M-i) - yields  the Kalman filter equation, which is basically equal to the old estimate plus the Kalman  gain times the residual error.  s = g + NAT -x (x - Ag)  (4)  In an analogous manner, the measurement matrix A may be estimated (learned) if one  assumes the physical relationships encoded by the measurement matrix are relatively stable.  The learning rule for the measurement matrix may be derived in a manner similar to the  rule for the internal state vector. In addition, a decay term is often needed in order to avoid  overfitting the data set. See [Rao, 1998] for details.  In our experiments both the internal state matrix s and the measurement matrix A were  learned by training them on the average red light signal and the average yellow light signal.  The signal is measured from the start of the trial which is known since it is triggered by the  light change. We used a Kalman gain of 0.6 and a decay of 0.3. After training, the signal  estimate for each epoch is correlated with the red and yellow light signal estimates in the  manner of experiment 1. We made the Kalman filter statistically robust by ignoring parts  of the EEG signal that fell outside a standard deviation of 1.0 from the training signals.  The overall recognition results in Table 1 suggest that both the robust Kalman filter and ICA  have a statistically significant advantage over correlation (p < 0.01). The robust Kalman  filter has a very small advantage over ICA (not statistically significant).  In order to look at the reliability of the best algorithm and its ability to be used on-line  two of the Subjects (S4 and S5) returned for another VR driving session. In these sessions  the brakes of the driving simulator were controlled by the robust Kalman filter recognition  algorithm for red stop and yellow slow down lights. Green lights were ignored. The results  of this session using the Robust Kalman Filter trained on the first session are shown in  Table 2. The recognition numbers for red and yellow lights between the two sessions were  compared using correlation. Red light scores between the sessions correlated fairly highly  - 0.82 for S4 and 0.69 for S5. The yellow light scores between sessions correlated poorly  with both S4 and S5 at approximately -0.1. This indicates that the yellow light epochs tend  to correlate poorly with each other due to the lack of a large component such as the P3 to  tie them together.  Recognizing Evoked Potentials in a Virtual Environment 9  4 Future Work  This paper showed the viability of recognizing the P3 EP in a VR environment. We plan  to allow the P3 EP to propel the user through alternate virtual rooms through the use of  various binary decisions. In order to improve recognition for the BC! we need to experi-  ment with a wider and more complex variety of recognition algorithms. Our most recent  work has shown a dependence between the human computer interface used in the BCI and  recognition. We would like to explore this dependence in order to improve recognition as  much as possible.  References  [Bayliss and Ballard, 1998] J.D. Bayliss and D.H. Ballard, "The Effects of Eye Tracking in a VR  Helmet on EEG Recording;' TR 685, University of Rochester National Resource Laboratory for  the Study of Brain and Behavior, May 1998.  [Chapman and Bragdon, 1964] R.M. Chapman and H.R. Bragdon, "Evoked responses to numerical  and non-numerical visual stimuli while problem solving.," Nature, 203:1155-1157, 1964.  [Farwell and Donchin, 1988] L.A. Farwell and E. Donchin, "Talking off the top of your head: toward  a mental prosthesis utilizing event-related brain potentials," Electroenceph. Clin. Neurophysiol.,  pages 510-523, 1988.  [Jung et al., 1997] TP. Jung, C. Humphties, T Lee, S. Makeig, M.J. McKeown, V. Iragui, and TJ.  Sejnowski, "Extended ICA Removes Artifacts from Electroencephalographic Recordings," to  Appear in Advances in Neural Information Processing Systems, 10, 1997.  [Makeig et al., 1997] S. Makeig, T Jung, A.J. Bell, D. Ghahremani, and T.J. Sejnowski, "Blind  Separation of Auditory Event-related Brain Responses into Independent Components," Proc.  Nat'l Acad. Sci. USA, 94:10979-10984, 1997.  [McFarland et al., 1993] D.J. McFarland, G.W. Neat, R.F. Read, and J.R. Wolpaw, "An EEG-based  method for graded cursor control;' Psychobiology, 21(1):77-81, 1993.  [Pfurtschelleretal., 1996] G. Pfurtscheller, D. Flotzinger, M. Pregenzer, J. Wolpaw, and D. Mc-  Farland, "EEG-based Brain Computer Interface (BCI);' Medical Progress through Technology,  21:111-121, 1996.  [Polich, 1998] J. Polich, "P300 Clinical Utility and Control of Variability," J. of Clinical Neuro-  physiology, 15(1):14-33, 1998.  [Rao, 1998] R. P.N. Rao, "Visual Attention during Recognition," Advances in Neural Information  Processing Systems, 10, 1998.  [Rosvold et al., 1956] H.E. Rosyold, A.F. Mirsky, I. Sarason, E.D. Bransome Jr., and L.H. Beck, "A  Continuous Performance Test of Brain Damage," J. Consult. Psychol., 20, 1956.  [Semlitsch et al., 1986] H.V. Semlitsch, P. Anderer, P Schuster, and O. Presslich, "A solution for  reliable and valid reduction of ocular artifacts applied to the P300 ERP," Psychophys., 23:695-  703, 1986.  [Sutton et al., 1965] S. Sutton, M. Braren, J. Zublin, and E. John, "Evoked potential correlates of  stimulus uncertainty," Science, 150:1187-1188, 1965.  [Vaughn et al., 1996] TM. Vaughn, J.R. Wolpaw, and E. Donchin, "EEG-Based Communication:  Prospects and Problems;' IEEE Trans. on Rehabilitation Engineering, 4(4):425-430, 1996.  [Vigario, 1997] R. Vigario, "Extraction of ocular artifacts from eeg using independent component  analysis," Electroenceph. Clin. Neurophysiol., 103:395-404, 1997.  
Acquisition in Autoshaping  Sham Kakade Peter Dayan  Gatsby Computational Neuroscience Unit  17 Queen Square, London, England, WC1N 3AR.  sham@gatsby. ucl. ac. uk dayan@gatsby. ucl. ac. uk  Abstract  Quantitative data on the speed with which animals acquire behav-  ioral responses during classical conditioning experiments should  provide strong constraints on models of learning. However, most  models have simply ignored these data; the few that have attempt-  ed to address them have failed by at least an order of magnitude.  We discuss key data on the speed of acquisition, and show how to  account for them using a statistically sound model of learning, in  which differential reliabilities of stimuli play a crucial role.  1 Introduction  Conditioning experiments probe the ways that animals make predictions about  rewards and punishments and how those predictions are used to their advantage.  Substantial quantitative data are available as to how pigeons and rats acquire con-  ditioned responses during autoshaping, which is one of the simplest paradigTns  of classical conditioning. 4 These data are revealing about the statistical, and ulti-  mately also the neural, substrate underlying the ways that animals learn about the  causal texture of their environments.  In autoshaping experiments on pigeons, the birds acquire a peck response to a  lighted key associated (irrespective of their actions) with the delivery of food. One  attractive feature of autoshaping is that there is no need for separate 'probe trials'  to assess the degree of association formed between the light and the food by the  animal rather, the rate of key pecking during the light (and before the food) can  be used as a direct measure of this association. In particular, acquisition speeds are  often measured by the number of trials until a certain behavioral criterion is met,  such as pecking during the light on three out of four successive trials. a, 8' 0  As stressed persuasively by Gallistel & Gibbon 4 (GG; forthcoming), the critical  feature of autoshaping is that there is substantial experimental evidence on how  acquisition speed depends on the three critical variables shown in figure 1A. The  first is l, the inter-trial interval; the second is T, the time during the trial for which  the light is presented; the third is the training schedule, l/S, which is the fractional  number of deliveries per light -- some birds were only partially reinforced.  Figure 1 makes three key points. First, figure lB shows that the median number of  trials to the acquisition criterion depends on the ratio of I/T, and not on I and T  separately - experiments reporte d for the same I/2" are actually performed with l  and T differing by more than an order of magnitude? 8 Second, figure lB shows  convincingly that the number of reinforcements is approximately inversely pro-  portional to I/2" the relatively shorter presentation of light, the faster the learn-  Acquisition in Autoshaping 25  A B   light   \ /x time -'  reward S = 2  time  1 2 5 10 20 50  I/T  10o00  lO 2 3 4 5 lO  s  Figure 1: Autoshaping. A) Experimental paradigm. Top: the light is presented for T seconds every C  seconds and is always followed by the delivery of food (filled circle). Bottom: the food is delivered with  probability 1IS = 1/2 per trial. In some cases I is stochastic, with the appropriate mean. B) Log-log  plot 4 of the number of reinforcements to a given acquisition criterion versus the I/T ratio for S = 1.  The data are median acquisition times from 12 different laboratories. C) Log-log acquisition curves for  various I/T ratios and S values. The main graph shows trials versus S; the inset shows reinforcements  versus S. (1999).  ing. Third, figure 1C shows that partial reinforcement has almost no effect when  measured as a function of the number of reinforcements (rather than the number  of trials), 4' 0 since although it takes $ times as many trials to acquire, there are rein-  forcements on only 1/S trials. Changing $ does not change the effective I/T when  measured as a function of reinforcements, so this result might actually be expected  on the basis of figure lB, and we only consider S = 1 in this paper. Altogether, the  data show that:  n  300 * T/I (1)  where n is the number of rewards to the acquisition criterion. Remarkably, these  effects seem to hold for over an order of magnitude in both I/T and S.  These quantitative data should be a most seductive target for statistically sound  models of learning. However, few models have even attempted to capture the  strong constraints they provide, and those that have attempted, all fail in critical  aspects. The best of them, rate estimation theory 4 (RET), is closely related to the  Rescorla-Wagner 13 (RW) model, and actually captures the proportionality in equa-  tion 1. However, as shown below, RET grossly overestimates the observed speed  of acquisition (underestimating the proportionality constant). Further, RET is de-  signed to account for the time at which a particular, standard, acquisition criterion  is met. Figure 2A shows that this is revealing only about the very early stages of  learning -- RET is silent about the remainder of the learning curve.  We look at additional quantitative data on learning, which collectively suggest  that stimuli compete to predict the delivery of reward. Dayan & Long 3 (DL) dis-  cussed various statistically inspired competitive models of classical conditioning,  concluding with one in which stimuli are differently reliable as predictors of re-  ward. However, DL ignored the data shown in figures 1 and 2, basing their anal-  ysis on conditioning paradigms in which I/T was not a factor. Figures 1 and 2  demand a more sophisticated statistical model -- building such a model is the  focus of this paper.  2 Rate Estimation Theory  Gallistel & Gibbon 4 (GG; forthcoming) are amongst the strongest proponents of the  quantitative relationships in figure 1. To account for them, GG suggest that animals  are estimating the rates of rewards -- one, ,Xt, for the rate associated with the light  and another, ,Xb, for the rate associated with the background context. The context is  the ever-present environment which can itself gain associative value. The overall  26 S. Kakade and P Dayan  0.5  r- 14o  B  o  .120  _o 80   6o  E  , 40   20  ._c  E o  100 200 300 400 0 4 8 64 128 256 1200  reinforcements prior context reinforcements  Figure 2: Additional Autoshaping Data. A) Acquisition of keypecking. The figure shows response  rate versus reinforcements. 6 The acquisition criterion is satisfied at a relatively early time when the  response curve crosses the acquisition criterion line. B) The effects of prior context reinforcements on  subsequent acquisition speed. The data are taken from two experiments, 1,2 with I/T= 6.  predicted reward rate while the light is on is At + Ao, and the rate without the light  is just Ao.  The additive form of the model makes it similar to Rescorla-Wagner's 13 (RW) s-  tandard delta-rule model, for which the net prediction of the expected reward in a  trial is the sum of the associative values of each active predictor (in this case, the  context and light). If the rewards are modeled as being just present or absent, the  expected value for a reward is just its probability of occurrence. Instead, RET uses  rates, which are just probabilities per unit time.  GG 4 formulated their model from a frequentist viewpoint. However, it is easier to  discuss a closely related Bayesian model which suffers from the same underlying  problem. Instead of using RW's delta-rule for learning the rates, GG assume that  reinforcements come from a constant rate Poisson process, and make sound statis-  tical inferences about the rates given the data on the rewards. Using an improper  flat prior over the rates, we can write the joint distribution as:  7(AtA0 I data) vc 7(n]A1Aotlto) c (At + A0)"e-(x*+xb)t*e  (2)  since all n rewards occur with the light, at rate At + A0. Here, tt = nT is the total  time the light is on, and to = nI is the total time the light is off.  GG take the further important step of relating the inferred rates At and A0 to the  decision of the animals to start responding (ie to satisfy the acquisition criterion).  GG suggest that acquisition should occur when the animals have strong evidence  that the fractional increase in the reward rate, whilst the light is on, is greater than  some threshold. More formally, acquisition should occur when:  79((At + A0)/A0 > filn) = I- a  (3)  where ct is the uncertainty threshold and fi is slightly greater than one, reflect-  ing the fractional increase. The n that first satisfies equation 3 can be found by  integrating the joint probability in equation 2. It turns out that n oc tt/to, which  has the approximate, linear dependence on the ratio I/T (as in figure lB), since  h/to = nT/n! = T/I. It also has no dependence on partial reinforcement, as  observed in figure 1C.  However, even with a very low uncertainty, ct = 0.001, and a reasonable fractional  increase, fi = 1.5, this model predicts that learning should be more than ten times  as fast as observed, since we get n  20  T/! as opposed to the 300  T/I observed.  20 50  Equation 1 can only be satisfied by setting ct between 10- and 10- (depending  on the precise values of I/T and fi)! This spells problems for GG as a normafive,  ideal detector model of learning  it cannot, for instance, be repaired with any  reasonable prior for the rates, as ct drops drastically with n. In other circumstances,  A cquisia'on in 4utoshaping 2 7  though, Gallistel, Mark & King 5 (forthcoming) have shown that animals can be  ideal detectors of changes in rates.  One hint of the flaw with GG is that simple manipulations to the context before  starting autoshaping (in particular extinction) can produce very rapid learning. 2  More generally, the data show that acquisition speed is strongly controlled by pri-  or rewards being given only in the context (without the light present). 2 Figure 2B  shows a parametric study of subsequent acquisition speeds during autoshaping as  a function of the number of rewards given only with the context. This effect cannot  simply be modeled by assuming a different prior distribution for the rates (which  does not fix the problem of the speed of acquisition in any case), since the rate at  which these prior context rewards were given has little effect on subsequent ac-  quisition speed for a given number of prior reinforcements. 9 Note that the data in  figure 2B (ie equation 1) suggest that there were about thirty prior rewards in the  context -- this is consistent with the experimental procedures used, s-l although  prior experience was not a carefully controlled factor.  3 The Competitive Model  Five sets of constraints govern our new model. First, since animals can be ideal  detectors of rates in some circumstances? we only consider accounts under which  their acquisition of responding has a rational statistical basis. Second, the number  of reinforcements to acquisition must be n  300  T/I, as in equation 1. This re-  quires that the constant of proportionality should come from rational, not absurd,  uncertainties. Third, pecking rates after the acquisition criterion is satisfied should  also follow the form of figure 2A (in the end, we are preventing from a norma five  account of this by a dearth of data). Fourth, the overall learning speed should be  strongly affected by the number of prior context rewards (figure 2B), but not by the  rate at which they were presented. That is, the context, as an established predic-  tor, regardless of the rate it predicts, should be able to substantially block learning  to a less established predictor. Finally, the asymptotic accuracy of rate estimates  should satisfy the substantial experimental data on the intrinsic uncertainty in the'  predictions in the form of a quantitative account called scalar expectancy theory ?  (SET).  In our model, as in DL, an independent prediction of the rate of reward delivery is  made on the basis of each stimulus that is present (we, for the context; cot for the  light). These separate predictions are combined based on estimated reliabilities of  the predictions. Here, we present a heuristic version of a more rigorously specified  model. 2  3.1 Rate Predictions  SET 7 was originally developed to capture the nature of uncertainty in the way  that animals estimate time intervals. Its most important result is that the standard  deviation of an estimate is consistently proportional to the mean, even after an  asymptotic number of presentations of the interval. Since the estimated time to a  reward is just the inverse rate, asymptotic rate estimates might also be expected  to have constant coefficients of variation. Therefore, we constrain the standard  deviations of rate estimates not to drop below a multiple of their means. Evidence  suggests that this multiple is about 0.2. 7 RET clearly does not satisfy this constraint  as the joint distribution (equation 2) becomes arbitrarily accurate over time.  Inspired by Sutton, 14 we consider Kalman filter models for independent log-  predictions, log Wc (m) and log cot (m), on trial m. The output models for the filters  28 $. Kakade and P Dayan  specify the relationship between the predicted and observed rates. We use a simple  log-normal, Af, approximation (to an underlying truly Poisson model):  P(o(m) I w(m)) V(w(m) v   '" , c) P(ol(m) {wl(m)) ", EJV'(wl(m),v) (4)  where o. (m) is the observed average reward whilst predictor  is present, so if a  reward occurs with the light in trial m, then or(m) = 1/T and oc(m) = 1/6' (where  2  6' = T + I). The values of v. can be determined, from the Poisson model, to be  2  v c = v: 1.  The other part of the Kalman filter is a model of change in the world for the co's:  logcoc(m) - logcoc(m -1) + ec(m) ec(m)  Af(O, (707 +1)) -) (5)  logcot(m) = logcot(m - 1) + et(m) et(m) - A/'(0, (07 + 1)) -) (6)  We use log(rates) so that there is no inherent scale to change in the world. Here,  /is a constant chosen to satisfy the SET constraint, imposed as er. = */V at  asymptote. Notice that r/acts as the effective number of rewards remembered,  which will be less than 30, to get the observed coefficient of variation above 0.2.  After observing the data from m trials, the posterior distributions for the predic-  tions will become approximately:  P(coc(m) I data) '-A/'(1/C, ac(m)) P(cot(m) { data) ,-,N'(1/T,a(m)) (7)  and, in about m = /trials, ac(rn) - (1/C)/x/ and at(m) - (liT)Ix/. This  captures the fastest acquisition in figure 2, and also extinction.  3.2 Cooperative Mixture of Experts  The two predictions (equation 7) are combined using the factorial experts model of  Jacobs et al ii that was also used by DL. For this, during the presentation of the light  (and the context, of course), we consider that, independently, the relationships  between the actual reward rate r(m) and the outputs col (m) and coo(m) of 'experts'  associated with each stimulus are:  P(cot(m)lr(m))  JV(r(m), m--) ' P(coc(m)lr(m)) ' J(r(m), 1  (8)  where pt(m) - and pc(m) - are inverse variances, or reliabilities for the stimuli.  These reliabilities reflect the belief as to how close col(m) and coc(m) are to r(m).  The estimates are combined, giving  P(r(m) l cot(m),coc(m))  JV(V(m), (pt(m) + pc(m)) -)  '(rn) = 7rt(m)wt(m) + (1- 7rt(m))wc(m) 7rt(m) = pt(m)/(pt(m) + pc(m))  The prediction of the reward rate without the light rc (m) is determined just by the  context value coc (m).  In this formulation, the context can block the light's prediction if it is more reliable  (Pc >> Pt), since rt  0, making the mean '(m)  coo(m), and this blocking occurs  regardless of the context's rate, co (m). If Pt slowly increases, then ?(m) - cot slowly  as rt (m) - 1. We expect this to model the post-acquisition part of the learning  shown in figure 2A.  A fully norma five model of acquisition would come from a statistically correct ac-  count of how the reliabilities should change over time, which, in turn, would come  from a statistical model of the expectations the animal has of how predictabilities  change in the world. Unfortunately, the slow phase of learning in figure 2A, which  should provide the most useful data on these expectations, is almost ubiquitously  Acquisition in Autoshaping 29  ....  100 200 300 400  reinforcements  1/C 100 200 300 400  reinforcements  1 2 5 10 20 50  I/T  Figure 3: Satisfaction of the Constraints. A) The fit to the behavioral response curve (figure 2B), using  equation 9 and r0 - 0.004. B) Possible acquisition curves showing (rn) versus m. The < > on the  criterion line denotes the range of 15 to 120 reinforcements that are indicated by figure 2B. The --  curve is the same as in Fig 3A. The parameters displayed are values for r0 in multiples of r0 for the  center curve. C) A theoretical fit to the data using equation 11. Here, a = 5% and r0v/- = 0.004.  ignored in experiments. We therefore make two assumptions about this, which are  chosen to fit the acquisition data, but whose normative underpinnings are unclear.  The first assumption, chosen to obtain the slow learning curve, is that:  rt(m) = tanh r0m (9)  Assuming that the strength of the behavioral response is approximately propor-  tional to r(m) - re (m), which we will estimate by rt (m) (t (m) - (m)), figure 3A  compares the rate of key pecking in the model with the data from figure 2A. Fig-  ure 3B shows the effect on the behavioral response of varying r0. Within just a half  an order magnitude of variation of r0, the acquisition speeds (judged at the criteri-  on line shown) due to between 1200 and 0 prior context rewards (figure 2B) can be  obtained. Note the slightly counter-intuitive explanation -- the actual reward rate  associated with the light is established very quickly -- slow learning comes from  slow changes in the importance paid to these rates.  We make a second assumption that the coefficient of variation of the context's pre-  diction, from equation 8, does not change significantly for the early trials before  the acquisition criterion is met (it could change thereafter). This gives:  pc(m)  po/c(rn) 2 for early rn (10)  It is plausible that the context is not becoming a relatively worse 'expert' for early  m, since no other predictor has yet proven more reliable.  Following GG's suggestion, we model acquisition as occurring on trial rn if  7(r(rn) > rc(rn)ldata ) >_ 1 - c, ie if the animal has sound reasons to expect a  higher reward rate with the light. Integrating over the Kalman filter distributions  in equation 7 gives the distribution of r(m) - re(m) for early rn as  P(r(rn) - re(m) I data) ,,0 A/'((tanh rom)(1/T - l/C), (p0C2) -)  where a, (m) has dropped out due to rt (m) being small at early m. Finding the  number of rewards, n, that satisfies the acquisition criterion gives:  c T  n m (11)  I  where the factor of c depends on the uncertainty, c, used. Figure 3C shows the  theoretical fit to the data.  4 Discussion  Although a noble attempt, RET fails to satisfy the strong body of constraints under  which any acquisition model must labor. Under RET, the acquisition of respond-  ing cannot have a rational statistical basis, as the animal's modeled uncertainty in  30 S. Kakade and P Dayan  the association between light and reward at the time of acquisition is below 10 -20 .  Further, RET ignores constraints set forth by the data establishing SET and also  data on prior context manipulations. These latter data show that the context, re-  gardless of the rate it predicts, will substantially block learning to a less established  predictor. Additive models, such as RET, are unable to capture this effect.  We have suggested a model in which each stimulus is like an 'expert' that learns  independently about the world. Expert predictions can adapt quickly to changes  in contingencies, as they are based on a Kalman filter model, with variances chosen  to satisfy the constraint suggested by SET, and they can be combined based on their  reliabilities. We have demonstrated the model's close fit to substantial experimental  data. In particular, the new model captures the I/T dependence of the number  of rewards to acquisition, with a constant of proportionality that reflects rational  statistical beliefs. The slow learning that occurs in some circumstances, is due to  a slow change in the reliabilities of predictors, not due to the rates being unable  to adapt quickly. Although we have not shown it here, the model is also able  to account for quantitative data as to the speed of extinction of the association  between the light and the reward.  The model leaves many directions for future study. In particular, we have not  specified a sound statistical basis for the changes in reliabilities given in equation-  s 9 and 10. Such a basis is key to understanding the slow phase of learning. Second,  we have not addressed data from more sophisticated conditioning paradigms. For  instance, overshadowing, in which multiple conditioned stimuli are similarly pre-  dictive of the reward, should be able to be incorporated into the model in a natural  way.  Acknowledgements  We are most grateful to Randy Gallistel and John Gibbon for freely sharing, prior  to publication, their many ideas about timing and conditioning. We thank Sam  Roweis for comments on an earlier version of the manuscript. Funding is from a  NSF Graduate Research Fellowship (SK) and the Gatsby Charitable Foundation.  References  [1] Balsam, PD, & Gibbon, J (1988). Journal of ExperimentaI Psychology: Animal Behavior Processes, 14:  401-412.  [2] Balsam, PD, & Schwartz, AL (1981). Journal of Experimental Psychology: Animal Behavior Processes,  7: 382-393.  [3] Dayan, P, & Long, T, (1997) Neural Information Processing Systems, 10:117-124.  [4] Gallistel, CR, & Gibbon, J (1999). Time, Rate, and Conditioning. Forthcoming.  [5] Gallistel, CR, Mark, TS & King, A (1999). Is the Rat an Ideal Detector of Changes in Rates of Reward?.  Forthcoming.  [6] Gamzu, ER, & Williams, DR (1973). Journal of the Experimental Analysis of Behavior, 19:225-232.  [7] Gibbon, J (1977). Psychological Review 84:279-325.  [8] Gibbon, J, Baldock, MD, Locurto, C, Gold, L & Terrace, HS (1977). Journal of Experimental Psychol-  ogy: Animal Behavior Processes, 3: 264-284.  [9] Gibbon, J & Balsam, P (1981). In CM Locurto, HS Terrace, & J Gibbon, editors, Autoshaping and  Conditioning Theory. 219-253. New York, NY: Academic Press.  [10] Gibbon, J, Farrell, L, Locurto, CM, Duncan, JH & Terrace, HS (1980). Animal Learning and Behavior,  8:45-59.  [11] Jacobs, RA, Jordan, MI, & Barto, AG (1991). Cognitive Science 15:219-250.  [12] Kakade, S & Dayan, P (2000). In preparation.  [13] Rescorla, RA & Wagner, AR (1972). In AH Black & WF Prokasy, editors, Classical Conditioning II:  Current Research and Theory, 64-69. New York, NY: Appleton-Century-Crofts.  [14] Sutton, R (1992). In Proceedings of the 7th Yale Workshop on Adaptive and Learning Systems.  
Constrained Hidden Markov Models  Sam Roweis  roweis@atsby.ucl .ac .uk  Gatsby Unit, University College London  Abstract  By thinking of each state in a hidden Markov model as corresponding to some  spatial region of a fictitious topology space it is possible to naturally define neigh-  bouring states as those which are connected in that space. The transition matrix  can then be constrained to allow transitions only between neighbours; this means  that all valid state sequences correspond to connected paths in the topology space.  I show how such constrained HMMs can learn to discover underlying structure  in complex sequences of high dimensional data, and apply them to the problem  of recovering mouth movements from acoustics in continuous speech.  1 Latent variable models for structured sequence data  Structured time-series are generated by systems whose underlying state variables change in  a continuous way but whose state to output mappings are highly nonlinear, many to one and  not smooth. Probabilistic unsupervised learning for such sequences requires models with  two essential features: latent (hidden) variables and topology in those variables. Hidden  Markov models (HMMs) can be thought of as dynamic generalizations of discrete state  static data models such as Gaussian mixtures, or as discrete state versions of linear dynam-  ical systems (LDSs) (which are themselves dynamic generalizations of continuous latent  variable models such as factor analysis). While both HMMs and LD$s provide probabilistic  latent variable models for time-series, both have important limitations. Traditional HMMs  have a very powerful model of the relationship between the underlying state and the associ-  ated observations because each state stores a private distribution over the output variables.  This means that any change in the hidden state can cause arbitrarily complex changes in the  output distribution. However, it is extremely difficult to capture reasonable dynamics on  the discrete latent variable because in principle any state is reachable from any other state at  any time step and the next state depends only on the current state. LDSs, on the other hand,  have an extremely impoverished representation of the outputs as a function of the latent  variables since this transformation is restricted to be global and linear. But it is somewhat  easier to capture state dynamics since the state is a multidimensional vector of continuous  variables on which a matrix "flow" is acting; this enforces some continuity of the latent  variables across time. Constrained hidden Markov models address the modeling of state  dynamics by building some topology into the hidden state representation. The essential  idea is to constrain the transition parameters of a conventional HMM so that the discrete-  valued hidden state evolves in a structured way. 1 In particular, below I consider parameter  restrictions which constrain the state to evolve as a discretized version of a continuous  multivariate variable, i.e. so that it inscribes only connected paths in some space. This  lends a physical interpretation to the discrete state trajectories in an HMM.  A standard trick in traditional speech applications of HMMs is to use "]eft-to-right" transition  matrices which are a special case of the type of constraints investigated in this paper. However, ]eft-  to-right (Bakis) HMMs force state trajectories that are inherently one-dimensional and uni-directional  whereas here I also consider higher dimensional topology and free omni-directional motion.  Constrained Hidden Markov Models 783  2 An illustrative game  Consider playing the following game: divide a sheet of paper into several contiguous, non-  overlapping regions which between them cover it entirely. In each region inscribe a symbol,  allowing symbols to be repeated in different regions. Place a pencil on the sheet and move it  around, reading out (in order) the symbols in the regions through which it passes. Add some  noise to the observation process so that some fraction of the time incorrect symbols are  reported in the list instead of the correct ones. The game is to reconstruct the configuration  of regions on the sheet from only such an ordered list(s) of noisy symbols. Of course, the  absolute scale, rotation and reflection of the sheet can never be recovered, but learning the  essential topology may be possible. 2 Figure 1 illustrates this setup.  True Generafive Map  21, 2,... :  18,19,10,3,... 22  16, 18  15,  Iteration:030 logLikelihood:- 1.9624  Figure 1: (left) True map which generates symbol sequences by random movement between con-  nected cells. (centre) An example noisy output sequence with noisy symbols circled. (right) Learned  map after training on 3 sequences (with 15% noise probability) each 200 symbols long. Each cell  actually contains an entire distribution over all observed symbols, though in this case only the upper  right cell has significant probability mass on more than one symbol (see figure 3 for display details).  Without noise or repeated symbols, the game is easy (non-probabilistic methods can solve  it) but in their presence it is not. One way of mitigating the noise problem is to do statistical  averaging. For example, one could attempt to use the average separation in time of each  pair of symbols to define a dissimilarity between them. It then would be possible to use  methods like multi-dimensional scaling or a sort of Kohonen mapping though time 3 to ex-  plicitly construct a configuration of points obeying those distance relations. However, such  methods still cannot deal with many-to-one state to output mappings (repeated numbers in  the sheet) because by their nature they assign a unique spatial location to each symbol.  Playing this game is analogous to doing unsupervised learning on structured sequences.  (The game can also be played with continuous outputs, although often high-dimensional  data can be effectively clustered around a manageable number of prototypes; thus a vector  time-series can be converted into a sequence of symbols.) Constrained HMMs incorporate  latent variables with topology yet retain powerful nonlinear output mappings and can deal  with the difficulties of noise and many-to-one mappings mentioned above; so they can  "win" our game (see figs. 1 & 3). The key insight is that the game generates sequences ex-  actly according to a hidden Markov process whose transition matrix allows only transitions  between neighbouring cells and whose output distributions have most of their probability  on a single symbol with a small amount on all other symbols to account for noise.  2The observed symbol sequence must be "informative enough" to reveal the map structure (this  can be quantified using the idea of persistent excitation from control theory).  3Consider a network of units which compete to explain input data points. Each unit has a position  in the output space as well as a position in a lower dimensional topology space. The winning unit has  its position in output space updated towards the data point; but also the recent (in time) winners have  their positions in topology space updated towards the topology space location of the current winner.  Such a rule works well, and yields topological maps in which nearby units code for data that typically  occur close together in time. However it cannot learn many-to-one maps in which more than one unit  at different topology locations have the same (or very similar) outputs.  784 $. Rowels  3 Model definition: state topologies from cell packings  Defining a constrained HMM involves identifying each state of the underlying (hidden)  Markov chain with a spatial cell in a fictitious topology space. This requires selecting  a dimensionality d for the topology space and choosing a packing (such as hexagonal or  cubic) which fills the space. The number of cells in the packing is equal to the number of  states M in the original Markov model. Cells are taken to be all of equal size and (since  the scale of the topology space is completely arbitrary) of unit volume. Thus, the packing  covers a volume M in topology space with a side length  of roughly  - M i/d. The  dimensionality and packing together define a vector-valued function x(rn), rn = 1... M  which gives the location of cell rn in the packing. (For example, a cubic packing of d  dimensional space defines x(m+ 1) to be [m, m/f, m/2,..., rn/ d-] mod .) State rn  in the Markov model is assigned to to cell rn in the packing, thus giving it a location x(m)  in the topology space. Finally, we must choose a neighbourhood rule in the topology space  which defines the neighbours of cell m; for example, all "connected" cells, all face neigh-  bours, or all those within a certain radius. (For cubic packings, there are 3 a- 1 connected  neighbours and 2d face neighbours in a d dimensional topology space.) The neighbourhood  rule also defines the boundary conditions of the space - e.g. periodic boundary conditions  would make cells on opposite extreme faces of the space neighbours with each other.  The transition matrix of the HMM is now preprogrammed to only allow transitions between  neighbours. All other transition probabilities are set to zero, making the transition matrix  very sparse. (I have set all permitted transitions to be equally likely.) Now, all valid  state sequences in the underlying Markov model represent connected ("city block") paths  through the topology space. Figure 2 illustrates this for a three-dimensional model.  64  Figure 2: (left) Physical depiction of the  topology space for a constrained HMM with  d=3,---4 and M=64 showing an example state  trajectory. (right) Corresponding transition  matrix structure for the 64-state HMM com-  puted using face-centred cubic packing. The  gaps in the inner bands are due to edge effects.  4 State inference and learning  The constrained HMM has exactly the same inference procedures as a regular HMM: the  forward-backward algorithm for computing state occupation probabilities and the Vterbi  decoder for finding the single best state sequence. Once these discrete state inferences have  been performed, they can be transformed using the state position function x(rn) to yield  probability distributions over the topology space (in the case of forward-backward) or paths  through the topology space (in the case of Viterbi decoding). This transformation makes  the outputs of state decodings in constrained HMMs comparable to the outputs of inference  procedures for continuous state dynamical systems such as Kalman smoothing.  The learning procedure for constrained HMMs is also almost identical to that for HMMs.  In particular, the EM algorithm (Baum-Welch) is used to update model parameters. The  crucial difference is that the transition probabilities which are precomputed by the topology  and packing are never updated during learning. In fact, this makes learning much easier  in some cases. Not only do the transition probabilities not have to be learned, but their  structure constrains the hidden state sequences in such a way as to make the learning of the  output parameters much more efficient when the underlying data really does come from a  spatially structured generative model. Figure 3 shows an example of parameter learning  for the game discussed above. Notice that in this case, each part of state space had only a  single output (except for noise) so the final learned output distributions became essentially  minimum entropy. But constrained HMMs can in principle model stochastic or multimodal  output processes since each state stores an entire private distribution over outputs.  Constrained Hidden Markov Models 785  24 1  Random imtnliztaon IogLikellhood -3 2321  )  Figure 3: Snapshots of model parameters during constrained HMM learning for the game described  in section 2. At every iteration each cell in the map has a complete distribution over all of the observed  symbols. Only the top three symbols of each cell's histogram are show, with font size proportional  to the square root of probability (to make ink roughly proportional). The map was trained on 3 noisy  sequences each 200 symbols long generated from the map on the left of figure 1 using 15% noise  probability. The final map after convergence (30 iterations) is shown on the right of figure 1.  5 Recovery of mouth movements from speech audio  I have applied the constrained HMM approach described above to the problem of recover-  ing mouth movements from the acoustic waveform in human speech. Data containing si-  multaneous audio and articulator movement information was obtained from the University  of Wisconsin X-ray microbeam database [9]. Eight separate points (four on the tongue, one  on each lip and two on the jaw) located in the midsaggital plane of the speaker's head were  tracked while subjects read various words, sentences, paragraphs and lists of numbers. The  x and t coordinates (to within about + lmm) of each point were sampled at 146Hz by an X-  ray system which located gold beads attached to the feature points on the mouth, producing  a 16-dimensional vector every 6.9ms. The audio was sampled at 22kHz with roughly 14  bits of amplitude resolution but in the presence of machine noise.  These data are well suited to the constrained HMM architecture. They come from a system  whose state variables are known, because of physical constraints, to move in connected  paths in a low degree-of-freedom space. In other words the (normally hidden) articulators  (movable structures of the mouth), whose positions represent the underlying state of the  speech production systemfi move slowly and smoothly. The observed speech signal--the  system's output--can be characterized by a sequence of short-time spectral feature vectors,  often known as a spectrogram. In the experiments reported here, I have characterized the  audio signal using 12 line spectral frequencies (LSFs) measured every 6.9ms (to coincide  with the articulatory sampling rate) over a 25ms window. These LSF vectors character-  ize only the spectral shape of the speech waveform over a short time but not its energy.  Average energy (also over a 25ms window every 6.9ms) was measured as a separate one  dimensional signal. Unlike the movements of the articulators, the audio spectrum/energy  can exhibit quite abrupt changes, indicating that the mapping between articulator positions  and spectral shape is not smooth. Furthermore, the mapping is many to one: different  articulator configurations can produce very similar spectra (see below).  The unsupervised learning task, then, is to explain the complicated sequences of observed  spectral features (LSFs) and energies as the outputs of a system with a low-dimensional  state vector that changes slowly and smoothly. In other words, can we learn the parameters 5  of a constrained HMM such that connected paths through the topology space (state space)  generate the acoustic training data with high likelihood? Once this unsupervised learning  task has been performed, we can (as I show below) relate the learned trajectories in the  topology space to the true (measured) articulator movements.  4Articulator positions do not provide complete state information. For example, the excitation  signal (voiced or unvoiced) is not captured by the bead locations. They do, however, provide much  important information; other state information is easily accessible directly from acoustics.  SModel structure (dimensionality and number of states) is currently set using cross validation.  786  Rowe  While many models of the speech production process predict the many-m-one and non-  smooth properties of the articulatory to acoustic mapping, it is useful to confirm these  features by looking at real data. Figure 4 shows the experimentally observed distribution  of articulator configurations used to produce similar sounds. It was computed as follows.  All the acoustic and articulatory data for a single speaker are collected together. Starting  with some sample called the key sample, I find the 1000 samples "nearest" to this key  by two measures: articulatory distance, defined using the Mahalanobis norm between two  position vectors under the global covariance of all positions for the appropriate speaker,  and spectral shape distance, again defined using the Mahalanobis norm but now between  two line spectral frequency vectors using the global LSF covariance of the speaker's audio  data. In other words, I find the 1000 samples that "look most like" the key sample in mouth  shape and that "sound most like" the key sample in spectral shape. I then plot the tongue  bead positions of the key sample (as a thick cross), and the 1000 nearest samples by mouth  shape (as a thick ellipse) and spectral shape (as dots). The points of primary interest are the  dots; they show the distribution of tongue positions used to generate very similar sounds.  (The thick ellipses are shown only as a control to ensure that many nearby points to the key  sample do exist in the dataset.) Spread or multimodality in the dots indicates that many  different articulatory configurations are used to generate the same sound.  tongue lip  [mm]  20  .5-   ,o .: L-'.'i   -1  -50 ..4O  to, xay2 x [mml  -10 i  -30 -_20./  :[  -50 -40 -30 -20 -50 -40 -0  tongue bodyl  [ram] tongue bodyl  [ram]  2oi  -60 -50 -40 -30 IO --60 -50  tongue domm  [mm] tongue body2 x [mini ransue aorn  [mini  Figure 4: Inverse mapping from acoustics to articulation is ill-posed in real speech production data.  Each group of four articulator-space plots shows the 1000 samples which are "nearest" to one key  sample (thick cross). The dots are the 1000 nearest samples using an acoustic measure based on line  spectral frequencies. Spread or multimodality in the dots indicates that many different articulatory  configurations are used to generate very similar sounds. Only the positions of the four tongue beads  have been plotted. Two examples (with different key samples) are shown, one in the left group of four  panels and another in the right group. The thick ellipses (shown as a control) are the two-standard  deviation contour of the 1000 nearest samples using an articulatory position distance metric.  Why not do direct supervised learning from short-time spectral features (LSFs) to the artic-  ulator positions? The ill-posed nature of the inverse problem as shown in figure 4 makes this  impossible. To illustrate this difficulty, I have attempted to recover the articulator positions  from the acoustic feature vectors using Kalman smoothing on a LDS. In this case, since we  have access to both the hidden states (articulator positions) and the system outputs (LSFs)  we can compute the optimal parameters of the model directly. (In particular, the state  transition matrix is obtained by regression from articulator positions and velocities at time  t onto positions at time t + 1; the output matrix by regression from articulator positions  and velocities onto LSF vectors; and the noise covariances from the residuals of these  regressions.) Figure 5b shows the results of such smoothing; the recovery is quite poor.  Constrained HMMs can be applied to this recovery problem, as previously reported [6].  (My earlier results used a small subset of the same database that was not continuous speech  and did not provide the hard experimental verification (fig. 4) of the many-to-one problem.)  Constrained Hidden Markov Models 787  Figure 5: (A) Recovered articulator movements using state inference on a constrained HMM. A  four-dimensional model with 4096 states was trained on data (all beads) from a single speaker but  not including the test utterance shown. Dots show the actual measured articulator movements for a  single bead coordinate versus time; the thin lines are estimated movements from the corresponding  acoustics. (B) Unsuccessful recovery of articulator movements using Kalman smoothing on a global  LDS model. All the (speaker-dependent) parameters of the underlying linear dynamical system are  known; they have been set to their optimal values using the true movement information from the  training data. Furthermore, for this example, the test utterance shown was included in the training  data used to estimate model parameters. (C) All 16 bead coordinates; all vertical axes are the same  scale. Bead names are shown on the left. Horizontal movements are plotted in the left-hand column  and vertical movements in the right-hand column. The separation between the two horizontal lines  near the centre of the right panel indicates the machine measurement error.  Recovery of tongue tip vertical motion from acoustics  30  20  0  -10  -200 1 2 3 4 5 6 7 8  time [sec]  Kalman smoothing on optimal linear dynamical system  30 , ,  - 0 1 2 3 4 5 6 7  time [sec]  horizontal movements vertical movements  The basic idea is to train (unsupervised) on sequences of acoustic-spectral features and  then map the topology space state trajectories onto the measured articulatory movements.  Figure 5 shows movement recovery using state inference in a four-dimensional model with  4096 states (d=4,=8,M=4096) trained on data (all beads) from a single speaker. (Naive  unsupervised learning runs into severe local minima problems. To avoid these, in the sim-  ulations shown above, models were trained by slowly annealing two learning parameters6:  a term d s was used in place of the zeros in the sparse transition matrix, and 7f was used  in place of 7t = p(mt[observations) during inference of state occupation probabilities.  Inverse temperature/3 was raised from 0 to 1.) To infer a continuous state trajectory from  an utterance after learning, I first do Viterbi decoding on the acoustics to generate a discrete  state sequence mt and then interpolate smoothly between the positions x (mr) of each state.  6An easier way (which I have used previously) to find good minima is to initialize the models  using the articulatory data themselves. This does not provide as impressive "structure discovery" as  annealing but still yields a system capable of inverting acoustics into articulatory movements on pre-  viously unseen test data. First, a constrained HMM is trained on just the articulatory movements; this  works easily because of the natural geometric (physical) constraints. Next, I take the distribution of  acoustic features (LSFs) over all times (in the training data) when Viterbi decoding places the model  in a particular state and use those LSF distributions to initialize an equivalent acoustic constrained  HMM. This new model is then retrained until convergence using Baum-Welch.  788 S. Roweis  After unsupervised learning, a single linear fit is performed between these continuous state  trajectories and actual articulator movements on the training data. (The model cannot dis-  cover the units system or axes used to represent the articulatory data.) To recover articulator  movements from a previously unseen test utterance, I infer a continuous state trajectory as  above and then apply the single linear mapping (learned only once from the training data).  6 Conclusions, extensions and other work  By enforcing a simple constraint on the transition parameters of a standard HMM, a link  can be forged between discrete state dynamics and the motion of a real-valued state vector  in a continuous space. For complex time-series generated by systems whose underlying  latent variables do in fact change slowly and smoothly, such constrained HMMs provide a  powerful unsupervised learning paradigm. They can model state to output mappings that  are highly nonlinear, many to one and not smooth. Furthermore, they rely only on well  understood learning and inference procedures that come with convergence guarantees.  Results on synthetic and real data show that these models can successfully capture the low-  dimensional structure present in complex vector time-series. In particular, I have shown  that a speaker dependent constrained HMM can accurately recover articulator movements  from continuous speech to within the measurement error of the data. This acoustic to  articulatory inversion problem has a long history in speech processing (see e.g. [7] and  references therein). Many previous approaches have attempted to exploit the smoothness  of articulatory movements for inversion or modeling: Hogden et. al (e.g. [4]) provided early  inspiration for my ideas, but do not address the many-to-one problem; Simon Blackburn [ 1 ]  has investigated a forward mapping from articulation to acoustics but does not explicitly  attempt inversion; early work at Waterloo [5] suggested similar constraints for improving  speech recognition systems but did look at real articulatory data, more recent work at  Rutgers [2] developed a very similar system much further with good success. Perpififin [3],  considers a related problem in sequence learning using EPG speech data as an example.  While in this note I have described only "diffusion" type dynamics (transitions to all neigh-  bours are equally likely) it is also possible to consider directed flows which give certain  neighbours of a state lower (or zero) probability. The left-to-right HMMs mentioned earlier  are an example of this for one-dimensional topologies. For higher dimensions, flows can  be derived from discretization of matrix (linear) dynamics or from other physical/structural  constraints. It is also possible to have many connected local flow regimes (either diffusive  or directed) rather than one global regime as discussed above; this gives rise to mixtures of  constrained HMMs which have block-structured rather than banded transition matrices.  Smyth [8] has considered such models in the case of one-dimensional topologies and  directed flows; I have applied these to learning character sequences from English text.  Another application I have investigated is map learning from multiple sensor readings.  An explorer (robot) navigates in an unknown environment and records at each time many  local measurements such as altitude, pressure, temperature, humidity, etc. We wish to  reconstruct from only these sequences of readings the topographic maps (in each sensor  variable) of the area as well as the trajectory of the explorer. A final application is tracking  (inferring movements) of articulated bodies using video measurements of feature positions.  References  [1] S. Blackburn & S. Young. ICSLP 1996, Philadephia, v.2 pp.969-972  [2] S. Chennoukh et. al, Eurospeech 1997, Rhodes, Greece, v. 1 pp.429-432  [3] M. Carreira-Perpifiin. NIPS'12, 2000. (This volume.)  [4] D. Nix & J. Hogden. NIPS'11, 1999, pp.744-750  [5] G. Ramsay & L. Deng. J. Acoustical Society of America, 95(5), 1994, p.2873  [6] S. Roweis & A. Alwan. Eurospeech 1997, Rhodes, Greece, v. 3 pp. 1227-1230  [7] J. Schroeter & M. Sondhi. IEEE Trans. Speech & Audio Processing, 2(lp2), 1994, pp. 133-150  [8] P. Smyth. NIPS'9, 1997, pp.648-654  [9] J. Westbury. X-ray microbeam speech production database user's handbook version 1.0.  University of Wisconsin, Madison, June 1994.  
Spike-based learning rules and stabilization of  persistent neural activity  Xiaohui Xie and H. Sebastian Seung  Dept. of Brain & Cog. Sci., MIT, Cambridge, MA 02139  xhxie, seung @mit.edu  Abstract  We analyze the conditions under which synaptic learning rules based  on action potential timing can be approximated by learning rules based  on firing rates. In particular, we consider a form of plasticity in which  synapses depress when a presynaptic spike is followed by a postsynaptic  spike, and potentiate with the opposite temporal ordering. Such differen-  tial anti-Hebbianplasticity can be approximated under certain conditions  by a learning rule that depends on the time derivative of the postsynaptic  firing rate. Such a learning rule acts to stabilize persistent neural activity  patterns in recurrent neural networks.  1 INTRODUCTION  Recent experiments have  demonstrated types of synaptic  plasticity that depend on the  temporal ordering of presynap-  tic and postsynaptic spiking. At  cortical[ 1] and hippocampal[2]  synapses, long-term potenti-  ation is induced by repeated  pairing of a presynaptic spike  and a succeeding postsynaptic  spike, while long-term depres-  sion results when the order  is reversed. The dependence  of the change in synaptic  strength on the difference  /X't = tpost -- tpre between  postsynaptic and presynaptic  spike times has been measured  quantitatively. This pairing  function, sketched in Figure  1A, has positive and negative  width of tens of milliseconds.  pairing function as differential  A  B  0  o  tpost - tpr e  (re II Illl IllIll II II Illlllll![I I llll  post IIIIlllllll Illllllllllll  o.51  o 1 ooo 2000  time (ms)  Figure 1: (A) Pairing function for differential Heb-  bian learning. The change in synaptic strength is plot-  ted versus the time difference between postsynaptic  and presynaptic spikes. (B) Pairing function for dif-  ferential anti-Hebbian learning. (C) Differential anti-  Hebbian learning is driven by changes in firing rates.  The synaptic learning rule of Eq. (1) is applied to two  Poisson spike trains. The synaptic strength remains  roughly constant in time, except when the postsynap-  tic rate changes.  lobes correspond to potentiation and depression, and a  We will refer to synaptic plasticity associated with this  Hebbian plasticity--Hebbian because the conditions for  200 X. Xie and H. S. Seung  potentiation are as predicted by Hebb[3], and differential because it is driven by the  difference between the opposing processes of potentiation and depression.  The pairing function of Figure 1A is not characteristic of all synapses. For example, an  opposite temporal dependence has been observed at electrosensory lobe synapses of elec-  tric fish[4]. As shown in Figure lB, these synapses depress when a presynaptic spike is  followed by a postsynaptic one, and potentiate when the order is reversed. We will refer to  this as differential anti-Hebbian plasticity.  According to these experiments, the maximum ranges of the differential Hebbian and anti-  Hebbian pairing functions are roughly 20 and 40 ms, respectively. This is fairly short, and  seems more compatible with descriptions of neural activity based on spike timing rather  than instantaneous firing rates[5, 6]. In fact, we will show that there are some conditions  under which spike-based learning rules can be approximated by rate-based learning rules.  Other people have also studied the relationship between spike-based and rate-based learn-  ing rules[7, 8].  The pairing functions of Figures 1A and lB lead to rate-based learning rules like those  traditionally used in neural networks, except that they depend on temporal derivatives of  firing rates as well as firing rates themselves. We will argue that the differential anti-  Hebbian learning rule of Figure lB could be a general mechanism for tuning the strength  of positive feedback in networks that maintain a short-term memory of an analog variable  in persistent neural activity. A number of recurrent network models have been proposed  to explain memory-related neural activity in motor [9] and prefrontal[10] cortical areas,  as well as the head direction system [11] and oculomotor integrator[12, 13, 14]. All of  these models require precise tuning of synaptic strengths in order to maintain continuously  variable levels of persistent activity. As a simple illustration of tuning by differential anti-  Hebbian learning, a model of persistent activity maintained by an integrate-and-fire neuron  with an excitatory autapse is studied.  2 SPIKE-BASED LEARNING RULE  Pairing functions like those of Figure 1 have been measured using repeated pairing of a  single presynaptic spike with a single postsynaptic spike. Quantitative measurements of  synaptic changes due to more complex patterns of spiking activity have not yet been done.  We will assume a simple model in which the synaptic change due to arbitrary spike trains is  the sum of contributions from all possible pairings of presynaptic with postsynaptic spikes.  The model is unlikely to be an exact description of real synapses, but could turn out to be  approximately valid.  We will write the spike train of the ith neuron as a series of Dirac delta functions, si (t) =  y', 6(t - T), where T is the nth spike time of the ith neuron. The synaptic weight from  neuron j to i at time t is denoted by Wij (t). Then the change in synaptic weight induced  by presynaptic spikes occurring in the time interval [0, T] is modeled as  Wij(T + A) - Wij(A) = dtj dti f(ti - tj)si(ti)sj(tj)  (1)  Each presynaptic spike is paired with all postsynaptic spikes produced before and after.  For each pairing, the synaptic weight is changed by an amount depending on the pairing  function f. The pairing function is assumed to be nonzero inside the interval [-r, r], and  zero outside. We will refer to r as the pairing range.  According to our model, each presynaptic spike results in induction of plasticity only after  a latency A. Accordingly, the arguments T + A and  of Wij on the left hand side of the  equation are shifted relative to the limits T and 0 of the integral on the right hand side. We  Spike-based Learning and Stabilization of Persistent Neural Activity 201  will assume that the latency A is greater than the pairing range r, so that Wi3 at any time is  only influenced by presynaptic and postsynaptic spikes that happened before that time, and  therefore the learning rule is causal.  3 RELATION TO RATE-BASED LEARNING RULES  The learning rule of Eq. (1) is driven by correlations between presynaptic and postsynaptic  activities. This dependence can be made explicit by making the change of variables u --  ti - tj in Eq. (1), which yields  Wij(T + )- WO(A)  where we have defined the cross-correlation  duf(u)Ci(u) (2)  T  Ci(u) = fo dt si(t + u) s(t) .  (3)  and made use of the fact that f vanishes outside the interval I-r, r]. Our immediate goal  is to relate Eq. (2) to learning rules that are based on the cross-correlation between firing  rates,  j0 T  C[jate(u) - dtvi(t + u) vj(t) (4)  There are a number of ways of defining instantaneous firing rates. Sometimes they are  computed by averaging over repeated presentations of a stimulus. In other situations, they  are defined by temporal filtering of spike trains. The following discussion is general, and  should apply to these and other definitions of firing rates.  The "rate correlation" is commonly subtracted from the total correlation to obtain the "spike  correlation" Cid. pik = Cij - C ate. To derive a rate-based approximation to the learning  rule (2), we rewrite it as  : /: spike  Wij(T + ) - Wij(A) = du f(u)C[?(u) + du f(u)Cij (u)  T T  (5)  and simply neglect the second term. Shortly we will discuss the conditions under which  this is a good approximation. But first we derive another form for the first term by applying  the approximation t/i(t + u)  t/i(t) + ubi(t) to obtain   fo r  /_ duf(u)C[jate(u)  dt[/3o"i(t) + lPi(t)]"j(t)  T  (6)  where we define  = au(u)  T  /3 = duuf(u)  T  (7)  This approximation is good when firing rates vary slowly compared to the pairing range  r. The learning rule depends on the postsynaptic rate through/30'i +/31hi. When the  first term dominates the second, then the learning rule is the conventional one based on  correlations between firing rates, and the sign of/30 determines whether the rule is Hebbian  or anti-Hebbian.  In the remainder of the paper, we will discuss the more novel case where/30 = 0. This  holds for the pairing functions shown in Figures 1A and lB, which have positive and neg-  ative lobes with areas that exactly cancel in the definition of/30. Then the dependence on  202 X. Xie and H. S. Seung  postsynaptic activity is purely on the time derivative of the firing rate. Differential Hebbian  learning corresponds to/52 > 0 (Figure 1A), while differential anti-Hebbian learning leads  to/52 < 0 (Figure lB). To summarize the/50 = 0 case, the synaptic changes due to rate  correlations are approximated by  (diff. Hebbian)  Wij cr -biv (diff. anti-Hebbian) (8)  for slowly varying rates. These formulas imply that a constant postsynaptic firing rate  causes no net change in synaptic strength. Instead, changes in rate are required to induce  synaptic plasticity.  To illustrate this point, Figure 1C shows the result of applying differential anti-Hebbian  learning to two spike trains. The presynaptic spike train was generated by a 50 Hz Poisson  process, while the postsynaptic spike train was generated by an inhomogeneous Poisson  process with rate that shifted from 50 Hz to 200 Hz at 1 sec. Before and after the shift,  the synaptic strength fluctuates but remains roughly constant. But the upward shift in firing  rate causes a downward shift in synaptic strength, in accord with the sign of the differential  anti-Hebbian rule in Eq. (8).  The rate-based approximation works well for this example, because the second term of Eq.  (5) is not so important. Let us return to the issue of the general conditions under which  this term can be neglected. With Poisson spike trains, the spike correlations C ike (u) are  zero in the limit T ---> cx), but for finite T they fluctuate about zero. The integral over u in  the second term of (5) dampens these fluctuations. The amount of dampening depends on  the pairing range r, which sets the limits of integration. In Figure 1C we used a relatively  long pairing range of 100 ms, which made the fluctuations small even for small T. On the  other hand, if r were short, the fluctuations would be small only for large T. Averaging  over large T is relevant when the amplitude of f is small, so that the rate of learning is  slow. In this case, it takes a long time for significant synaptic changes to accumulate, so  that plasticity is effectively driven by integrating over long time periods T in Eq. (1).  In the brain, nonvanishing spike correlations are sometimes observed even in the T ---> oc  limit, unlike with Poisson spike trains. These correlations are often roughly symmetric  about zero, in which case they should produce little plasticity if the pairing functions are  antisymmetric as in Figures 1A and lB. On the other hand, if the spike correlations are  asymmetric, they could lead to substantial effects[6].  4 EFFECTS ON RECURRENT NETWORK DYNAMICS  The learning rules of Eq. (8) depend on both presynaptic and postsynaptic rates, like learn-  ing rules conventionally used in neural networks. They have the special feature that they  depend on time derivatives, which has computational consequences for recurrent neural  networks of the form  + = Wrr(x ) + bi (9)  J  Such classical neural network equations can be derived from more biophysically realistic  models using the method of averaging[ 15] or a mean field approximation[ 16]. The firing  rate of neuron j is conventionally identified with v = a(xj).  1  The cost function E({ xi }; { Wij }) =  Y'i v quantifies the amount of drift in firing rate at  the point x,... , xv in the state space of the network. If we consider bi to be a function of  x and Wij defined by (9), then the gradient of the cost function with respect to Wij is given  by OE/OW 0 = a' (x)t)i vj. Assuming that rr is a monotonically increasing function so that  a' (xi) > 0, it follows that the differential Hebbian update of (8) increases the cost function,  Spike-based Learning and Stabilization of Persistent Neural Activity 203  and hence increases the magnitude of the drift velocity. In contrast, the differential anti-  Hebbian update decreases the drift velocity. This suggests that the differential anti-Hebbian  update could be useful for creating fixed points of the network dynamics (9).  5 PERSISTENT ACTIVITY IN A SPIKING AUTAPSE MODEL  The preceding arguments about drift velocity were based on approximate rate-based de-  scriptions of learning and network dynamics. It is important to implement spike-based  learning in a spiking network dynamics, to check that our approximations are valid.  Therefore we have numerically simu-  lated the simple recurrent circuit of  integrate-and-fire neurons shown in Fig-  ure 2. The core of the circuit is the  "memory neuron," which makes an exci-  tatory autapse onto itself. It also receives  synaptic input from three input neurons:  a tonic neuron, an excitatory burst neu-  ron, and an inhibitory burst neuron. It is  known that this circuit can store a short-  term memory of an analog variable in  persistent activity, if the strengths of the  autapse and tonic synapse are precisely  tuned[17]. Here we show that this tun-  ing can be accomplished by the spike-  based learning rule of Eq. (1), with a d-  ifferential anti-Hebbian pairing function  like that of Figure 1B.  The memory neuron is described by the equations  IIIIIIIIIIIIIlllllllllttlllllllllll  TONIC  EXCITATORY BURST  MEMORY  II II I I I I lltlllllltllll!11111111t IIIIIIIIIII  INHIBITORY BURST  Figure 2: Circuit diagram for autapse model  = -gL(v' - v'L) - gE(v - v'E) - g(v - )  dV  C. dt  dr  + r = 5(t- (1 l)  (10)  where V is the membrane potential. When V reaches h,-es, a spike is considered to have  occurred, and V is reset to V,.,et. Each spike at time T, causes a jump in the synaptic  activation r of size a,./ry,, after which r decays exponentially with time constant  until the next spike.  The synaptic conductances of the memory neuron are given by  g = Wr + Woro + W+r+ gr = W_r_ (12)  The term Wr is recurrent excitation from the autapse, where W is the strength of the au-  tapse. The synaptic activations r0, r+, and r_ of the tonic, excitatory burst, and inhibitory  burst neurons are governed by equations like (10) and (11), with a few differences. These  neurons have no synaptic input; their firing patterns are instead determined by applied cur-  rents Ipp,o, Ipp,+ and Ipp,_. The tonic neuron has a constant applied current, which  makes it fire repetitively at roughly 20 Hz (Figure 3). For the excitatory and inhibitory  burst neurons the applied current is normally zero, except for brief 100 ms current pulses  that cause bursts of action potentials.  As shown in Figure 3, if the synaptic strengths W and Wo are arbitrarily set before learning,  the burst neurons cause only transient changes in the firing rate of the memory neuron.  After applying the spike-based learning rule (1) to tune both W and Wo, the memory  204 X. Xie and H. S. Seung  llllllllllll I IIIIIIII1111 I IIIIIIIlllll I  untuned  I I  tuned  IIIIIIII I I I I I I I II III I  !  1 sec  II ]iiiiiiiiiiiii:---ir'lm#", I I IIIIIIIIIIIIlllll  Figure 3: Untuned and tuned autapse activity. The middle three traces are the membrane  potentials of the three input neurons in Figure 2 (spikes are drawn at the reset times of  the integrate-and-fire neurons). Before learning, the activity of the memory neuron is not  persistent, as shown in the top trace. After the spike-based learning rule (1) is applied to  the synaptic weights W and W0, then the burst inputs cause persistent changes in activity.  Cm = 1 nF, g = 0.025 itS, V = -70 mV, VE = 0 mV, VI = -70 mV, Vtnre8 = -52  mV, V.eset = -59 mV, c = 1, ry = 100 ms, Iapp,o = 0.5203 hA, Iapp,+= 0 or 0.95  hA, rymo = 100 ms, ryn,+ = *-sy,- = 5 ms, W+ = 0.1, W_ = 0.05.  neuron is able to maintain persistent activity. During the interburst intervals (from A after  one burst until A before the next), we made synaptic changes using the differential anti-  Hebbian pairing function f(t) = -A sin(rt/r) for spike time differences in the range  [-r, r] with A - 1.5 x 10 -4 and r=A=120 ms. The resulting increase in persistence time  can be seen in Figure 4A, along with the values of the synaptic weights versus time.  To quantify the performance of the system at maintaining persistent activity, we determined  the relationship between dv/dt and v using a long sequence of interburst intervals, where v  was defined as the reciprocal of the interspike interval. If W and W0 are fixed at optimally  tuned values, there is still a residual drift, as shown in Figure 4B. But if these parameters are  allowed to adapt continuously, even after good tuning has been achieved, then the residual  drift is even smaller in magnitude. This is because the learning rule tweaks the synapfic  weights during each interburst interval, reducing the drift for that particular firing rate.  Autapse learning is driven by the autocorrelation of the spike train, rather than a cross-  correlation. The peak in the autocorrelogram at zero lag has no effect, since the pairing  function is zero at the origin. Since the autocorrelation is zero for small time lags, we used  a fairly large pairing range in our simulations. In a recurrent network of many neurons, a  shorter pairing range would suffice, as the cross-correlation does not vanish near zero.  6 DISCUSSION  We have shown that differential anti-Hebbian learning can tune a recurrent circuit to main-  tain persistent neural activity. This behavior can be understood by reducing the spike-based  learning rule (1) to the rate-based learning rules of Eqs. (6) and (8). The rate-based approx-  imations are good if two conditions are satisfied. First, the pairing range must be large, or  the rate of learning must be slow. Second, spike synchrony must be weak, or have little  effect on learning due to the shape of the pairing function.  The differential anti-Hebbian pairing function results in a learning rule that uses -hi as a  negative feedback signal to reduce the amount of drift in firing rate, as illustrated by our  simulations of an integrate-and-fire neuron with an excitatory autapse. More generally,  the learning rule could be relevant for tuning the strength of positive feedback in network-  s that maintain a short-term memory of an analog variable in persistent neural activity.  Spike-based Learning and Stabilization of Persistent Neural Activity 205  250  200  5O  wor w 1 w  3850 10.12  B  % 5 10 15 20 25 20 40 60 80 100  ttme (s) rate (Hz)  Figure 4: Tuning the autapse. (A) The persistence time of activity increases as the weight-  s W and Wo are tuned. Each transition is driven by pseudorandom bursts of input (B)  Systematic relationship between drift dr/dr in firing rate and v, as measured from a long  sequence of interburst intervals. If the weights are continuously fine-tuned ('*') the drift is  less than with fixed well-tuned weights (' o').  For example, the learning rule could be used to improve the robustness of the oculomotor  integrator[12, 13, 14] and head direction system[11] to mistuning of parameters. In deriv-  ing the differential forms of the learning rules in (8), we assumed that the areas under the  positive and negative lobes of the pairing function are equal, so that the integral defining  /o vanishes. In reality, this cancellation might not be exact. Then the ratio of/ and/o  would limit the persistence time that can be achieved by the learning rule.  Both the oculomotor integrator and the head direction system are also able to integrate  vestibular inputs to produce changes in activity patterns. The problem of finding general-  izations of the present learning rules that train networks to integrate is still open.  References  [11  [2]  [31  [41  [51  [61  [71  [8]  [9]  [10]  [111  [121  [131  [14]  [15]  [161  [17]  H. Markram, J. Lubke, M. Frotscher, and B. Sakmann. Science, 275(5297):213-5, 1997.  G. Q. Bi and M. M. Poo. JNeurosci, 18(24):10464-72, 1998.  D. O. Hebb. Organization of behavior. Wiley, New York, 1949.  C. C. Bell, V. Z. Han, Y. Sugawara, and K. Grant. Nature, 387(6630):278-81, 1997.  W. Gerstner, R. Kempter, J. L. van Hemmen, and H. Wagner. Nature, 383(6595):76-81, 1996.  L. F. Abbott and S. Song. Adv. Neural Info. Proc. Syst., 11, 1999.  P. D. Roberts. J. Cornput. Neurosci., 7:235-246, 1999.  R. Kempter, W. Gerstner, and J. L. van Hemmen. Phys. Rev. E, 59(4):4498-4514, 1999.  A. P. Georgopoulos, M. Taira, and A. Lukashin. Science, 260:47-52, 1993.  M. Camperi and X. J. Wang. J Cornput Neurosci, 5(4):383-405, 1998.  K. Zhang. J. Neurosci., 16:2112-2126, 1996.  S.C. Cannon, D. A. Robinson, and S. Shamma. Biol. Cybern., 49:127-136, 1983.  H. S. Seung. Proc. Natl. Acad. Sci. USA, 93:13339-13344, 1996.  H. S. Seung, D. D. Lee, B. Y. Reis, and D. W. Tank. Neuron, 2000.  B. Ermentrout. Neural Cornput., 6:679-695, 1994.  O. Shriki, D. Hansel, and H. Sompolinsky. Soc. Neurosci. Abstr., 24:143, 1998.  H. S. Seung, D. D. Lee, B. Y. Reis, and D. W. Tank. J. Cornput. Neurosci., 2000.  PART III  THEORY  
Learning from user feedback in image retrieval  systems  Nuno Vasconcelos Andrew Lippman  MIT Media Laboratory, 20 Ames St, E 15-354, Cambridge, MA 02139,  {nuno,lip} @media.mit.edu, http ://www. media.mit.edu/-nuno  Abstract  We formulate the problem of retrieving images from visual databases  as a problem of Bayesian inference. This leads to natural and effective  solutions for two of the most challenging issues in the design of a retrieval  system: providing support for region-based queries without requiring  prior image segmentation, and accounting for user-feedback during a  retrieval session. We present a new learning algorithm that relies on  belief propagation to account for both positive and negative examples of  the user's interests.  1 Introduction  Due to the large amounts of imagery that can now be accessed and managed via comput-  ers, the problem of content-based image retrieval (CBIR) has recently attracted significant  interest among the vision community [ 1, 2, 5]. Unlike most traditional vision applications,  very few assumptions about the content of the images to be analyzed are allowable in the  context of CBIR. This implies that the space of valid image representations is restricted to  those of a generic nature (and typically of low-level) and consequently the image under-  standing problem becomes even more complex. On the other hand, CBIR systems have  access to feedback from their users that can be exploited to simplify the task of finding the  desired images. There are, therefore, two fundamental problems to be addressed. First, the  design of the image representation itself and, second, the design of learning mechanisms to  facilitate the interaction. The two problems cannot, however, be solved/in isolation as the  careless selection of the representation will make learning more difficult and vice-versa.  The impact of a poor image representation on the difficulty of the learning problem is visible  in CBIR systems that rely on holistic metrics of image similarity, forcing user-feedback to  be relative to entire images. In response to a query, the CBIl system suggests a few images  and the user rates those images according to how well they satisfy the goals of the search.  Because each image usually contains several different objects or visual concepts, this rating  is both difficult and inefficient. How can the user rate an image that contains the concept  of interest, but in which this concept only occupies 30% of the field of view, the remaining  70% being filled with completely unrelated stuff? Andhow many example images will the  CBIR system have to see, in order to figure out what the concept of interest is?  A much better interaction paradigm is to let the user explicitly select the regions of the  image that are relevant to the search, i.e. user-feedback at the region level. However,  region-based feedback requires sophisticated image representations. The problem is that  the most obvious choice, object-based representations, is difficult to implement because  it is still too hard to segment arbitrary images in a meaningful way. We have argued  978 N. Vasconcelos and A. Lippman  that a better formulation is to view the problem as one of Bayesian inference and rely on  probabilistic image representations. In this paper we show that this formulation naturally  leads to 1) representations with support for region-based interaction without segmentation  and 2) intuitive mechanisms to account for both positive and negative user feedback.  2 Retrieval as Bayesian inference  The standard interaction paradigm for CBIR is the so-called "query by example", where the  user provides the system with a few examples, and the system retrieves from the database  images that are visually similar to these examples. The problem is naturally formulated  as one of statistical classification: given a representation (or feature) space Y' the goal is  to find a map #: Y' -+ M = {1,..., K} from Y' to the set M of image classes in the  database. K, the cardinality of M, can be as large as the number of items in the database  (in which case each item is a class by itself), or smaller. If the goal of the retrieval system  is to minimize the probability of error, it is well known that the optimal map is the Bayes  classifier [3]  g*(x) = argmaxP(Si = llx ) = argmaxP(xlSi = 1)P(Si = 1) (1)   i  where x are the example features provided by the user and $i is a binary variable indicating  the selection of class i. In the absence of any prior information about which class is most  suited for the query, an uninformative prior can be used and the optimal decision is the  maximum likelihood criteria  #*(x) = argmaxP(xISi = 1). (2)  i  Besides theoretical soundness, Bayesian retrieval has two distinguishing properties of prac-  tical relevance. First, because the features x in equation (1) can be any subset of a given  query image, the retrieval criteria is valid for both region-based and image-based queries.  Second, due to its probabilistic nature, the criteria also provides a basis for designing  retrieval systems that can account for user-feedback through belief propagation.  3 Bayesian relevance feedback  Suppose that instead of a single query x we have a sequence of t queries {x,..., xt },  where t is a time stamp. By simple application of Bayes rule  P(Si = 7tP(xtlSi = 1)P(Si = (3)  where 3't is a normalizing constant and we have assumed that, given the knowledge of the  correct image class, the current query xt is independent of the previous ones. This basically  means that the user provides the retrieval system with new information at each iteration of  the interaction. Equation (3) is a simple but intuitive mechanism to integrate information  over time. It states that the system's beliefs about the user's interests at time t - 1 simply  become the prior beliefs for iteration t. New data provided by the user at time t is then  used to update these beliefs, which in turn become the priors for iteration t + 1. From  a computational standpoint the procedure is very efficient since the only quantity that has  to be computed at each time step is the likelihood of the data in the corresponding query.  Notice that this is exactly equation (2) and would have to be computed even in the absence  of any learning.  By taking logarithms and solving for the recursion, equation (3) can also be written as  t-1 t-1  logP(Si = l[Xl,...,xt)- y'. log')'t-k + ZlogP(xt-klSi = 1)+1ogP(Si = 1),  k=0 k=0  (4)  Learning from User Feedback in Image Retrieval Systems 979  exposing the main limitation of the belief propagation mechanism: for large t the con-  tribution, to the right-hand side of the equation, of the new data provided by the user is  very small, and the posterior probabilities tend to remain constant. This can be avoided by  penalizing older terms with a decay factor  t--1 t--1  1ogV($i = llx,...,xt ) = ) m-k 1og7t-t + E rt-t logV(xt_lS, = 1) +  k=O k=O  rologP($i = 1),  where rt is a monotonically decreasing sequence. In particular, if  (0, 1] we have  logP(Si = llx,...,xt ) = 1og? + alogP(xtlS = 1)+  (1-a)logP(Si = llx,,...,xt_ ).  Because ? does not depend on i, the optimal class is  $=argm.ax{logP(xtlSi= 1)+(1-)1ogP(Si= 11x,...,xt_)}. (5)  4 Negative feedback  In addition to positive feedback, there are many situations in CBIR where it is useful to  rely on negative user-feedback. One example is the case of image classes characterized by  overlapping densities. This is illustrated in Figure 1 a) where we have two classes with  a common attribute (e.g. regions of blue sky) but different in other aspects (class A also  contains regions of grass while class B contains regions of white snow). If the user starts  with an image of class B (e.g. a picture of a snowy mountain), using regions of sky as  positive examples is not likely to quickly take him/her to the images of class A. In fact, all  other factors being equal, there is an equal likelihood that the retrieval system will return  images from the two classes. On the other hand, if the user can explicitly indicate interest  in regions of sky but not in regions of snow, the likelihood that only images from class A  will be returned increases drastically.  d)  Figure 1: a) two overlapping image classes. b) and c) two images in the tile database. d) three  examples of pairs of visually similar images that appear in different classes.  Another example of the importance of negative feedback are local minima of the search  space. These happen when in response to user feedback, the system returns exactly the  same images as in a previous iteration. Assuming that the user has already given the system  all the possible positive feedback, the only way to escape from such minima is to choose  some regions that are not desirable and use them as negative feedback. In the case of the  example above, if the user gets stuck with a screen full of pictures of white mountains,  he/she can simply select some regions of snow to escape the local minima.  In order to account for negative examples, we must penalize the classes under which these  score well while favoring the classes that assign a high score to the positive examples.  980 N. Vasconcelos and A. Lippman  Unlike positive examples, for which the likelihood is known, it is not straightforward to  estimate the likelihood of a particular negative example given that the user is searching for a  certain image class. We assume that the likelihood with which y will be used as a negative  example given that the target is class i, is equal to the likelihood with which it will be used  as a positive example given that the target is any other class. Denoting the use of y as a  negative example by y, this can be written as  P(YISi = 1) = P(yISi = 0). (6)  This assumption captures the intuition that a good negative example when searching for  class i, is one that would be a good positive example if the user were looking for any class  other than i. E.g. if class i is the only one in the database that does not contain regions of  sky, using pieces of sky as negative examples will quickly eliminate the other images in the  database.  Under this assumption, negative examples can be incorporated into the learning by simply  choosing the class i that maximizes the posterior odds ratio [4] between the hypotheses  "class i is the target" and "class i is not the target"  S = argm/ax P(Si = llxt,... ,xl,Yt,... ,y) = argmx P(Si = llxt,... ,Xl)   P(Si = 0lxt,... ,x,yt,... ,y) P($i -- 0lYt,.-. ,Y)  where x are the positive and y the negative examples, and we have assumed that, given the  positive (negative) examples, the posterior probability of a given class being (not being) the  target is independent of the negative (positive) examples. Once again, the procedure of the  previous section can be used to obtain a recursive version of this equation and include a  decay factor which penalizes ancient terms  . { '(xtl& =  S i = argm/ax c log p(ytlSi = O)  Using equations (4) and (6)  P(S - 01,... ,t)  P(Si = llxl,..  ,xt___!) }  + (1 -- o)log P(Si = 0[yl, ,Yt-1) '  or II P(YtISi = O) = II P(Yt[Si = 1)  k k  or P(Si: 11Y,...,Yt),  we obtain  . { P(xtl&- 1)  Si = argm/ax c1og P(YtI& = 1)  P(Si = }  +(1-c01gP(Si = lye-,) '  (7)  While maximizing the ratio of posterior probabilities is a natural way to favor image  classes that explain well the positive examples and poorly the negative ones, it tends to  over-emphasize the importance of negative examples. In particular, any class with zero  probability of generating the negative examples will lead to a ratio of oo, even if it explains  very poorly the positive examples. To avoid this problem we proceed in two steps:   start by solving equation (5), i.e. sort the classes according to how well they  explain the positive examples.   select the subset of the best N classes and solve equation (7) considering only the  classes in this subset.  5 Experimental evaluation  We performed experiments to evaluate 1) the accuracy of Bayesian retrieval on region-  based queries and 2) the improvement in retrieval performance achievable with relevance  Learning from User Feedback in Image Retrieval Systems 981  feedback. Because in a normal browsing scenario it is difficult to know the ground truth for  the retrieval operation (at least without going through the tedious process of hand-labeling  all images in the database), we relied instead on a controlled experimental set up for which  ground truth is available. All experiments reported on this section are based on the widely  used Brodatz texture database which contains images of 112 textures, each of them being  represented by 9 different patches, in a total of 1008 images. These were split into two  groups, a small one with 112 images (one example of each texture), and a larger one with  the remaining 896. We call the first group the test database and the second the Brodatz  database. A synthetic database with 2000 images was then created from the larger set by  randomly selecting 4 images at a time and making a 2 x 2 tile out of them. Figure 1 b) and  c) are two examples of these tiles. We call this set the tile database.  5.1 Region-based queries  We performed two sets of experiments to evaluate the performance of region-based queries.  In both cases the test database was used as a test set and the image features were the coeffi-  cients of the discrete cosine transform (DCT) of an 8 x 8 block-wise image decomposition  over a grid containing every other image pixel. The first set of experiments was performed  on the Brodatz database while the tile database was used in the second. A mixture of 16  Gaussians was estimated, using EM, for each of the images in the two databases.  In both sets of experiments, each query consisted of selecting a few image blocks from an  image in the test set, evaluating equation (2) for each of the classes and returning those that  best explained the query. Performance was measured in terms of precision (percent of the  retrieved images that are relevant to the query) and recall (percent of the relevant images  that are retrieved) averaged over the entire test set. The query images contained a total  of 256 non-overlapping blocks. The number of these that were used in each query varied  between 1 (0.3 % of the image size) and 256 (100 %). Figure 2 depicts precision-recall  plots as a function of this number.  Figure 2: Precision-recall curves as a function of the number of feature vectors included in the query.  Left: Brodatz database. Right: Tile database.  The graph on the left is relative to the Brodatz database. Notice that precision is generally  high even for large values of recall and the performance increases quickly with the per-  centage of feature vectors included in the query. In particular, 25% of the texture patch  (64 blocks) is enough to achieve results very close to those obtained with all pixels. This  shows that the retrieval criteria is robust to missing data. The graph on the left presents  similar results for the tile database. While there is some loss in performance, this loss is  not dramatic - a decrease between 10 and 15 % in precision for any given recall. In fact,  the results are still good: when a reasonable number of feature vectors is included in the  query, about 8.5 out of the 10 top retrieved images are, on average, relevant. Once again,  performance improves rapidly with the number of feature vectors in the query and 25% of  982 N. Vasconcelos and A. Lippman  the image is enough for results comparable to the best. This confirms the argument that  Bayesian retrieval leads to effective region-based queries even for imagery composed by  multiple visual stimulae.  5.2 Learning  The performance of the learning algorithm was evaluated on the tile database. The goal was  to determine if it is possible to reach a desired target image by starting from a weakly related  one and providing positive and negative feedback to the retrieval system. This simulates  the interaction between a real user and the CBIR system and is an iterative process, where  each iteration consists of selecting a few examples, using them as queries for retrieval and  examining the top M retrieved images to find examples for the next iteration. M should be  small since most users are not willing to go through lots of false positives to find the next  query. In all experiments we set M - 10 corresponding to one screenful of images.  The most complex problem in testing is to determine a good strategy for selecting the  examples to be given to the system. The closer this strategy is to what a real user would  do, the higher the practical significance of the results. However, even when there is clear  ground truth for the retrieval (as is the case of the tile database) it is not completely clear  how to make the selection. While it is obvious that regions of texture classes that appear in  the target should be used as positive feedback, it is much harder to determine automatically  what are good negative examples. As Figure 1 d) illustrates, there are cases in which  textures from two different classes are visually similar. Selecting images from one of these  classes as a negative example for the other will be a disservice to the learner.  While real users tend not to do this, it is hard to avoid such mistakes in an automatic setting,  unless one does some sort of pre-classification of the database. Because we wanted to avoid  such pre-classification we decided to stick with a simple selection procedure and live with  these mistakes. At each step of the iteration, examples were selected in the following way:  among the 10 top images returned by the retrieval system, the one with most patches from  texture classes also present in the target image was selected to be the next query. One block  from each patch in the query was then used as a positive (negative) example if the class of  that patch was also (was not) represented in the target image.  This strategy is a worst-case scenario. First, the learner might be confused by conflicting  negative examples. Second, as seen above, better retrieval performance can be achieved  if more than one block from each region is included in the queries. However, using only  one block reduced the computational complexity of each iteration, allowing us to average  results over several runs of the learning process. We performed 100 runs with randomly  selected target images. In all cases, the initial query image was the first in the database  containing one class in common with the target.  The performance of the learning algorithm can be evaluated in various ways. We considered  two metrics: the percentage of the runs which converged to the right target, and the number  of iterations required for convergence. Because, to prevent the learner from entering loops,  any given image could only be used once as a query, the algorithm can diverge in two ways.  Strong divergence occurs when, at a given time step, the images (among the top 10) that  can be used as queries do not contain any texture class in common with the target. In such  situation, a real user will tend to feel that the retrieval system is incoherent and abort the  search. Weak divergence occurs when all the top 10 images have previously been used.  This is a less troublesome situation because the user could simply look up more images  (e.g. the next 10) to get new examples.  We start by analyzing the results obtained with positive feedback only. Figure 3 a) and b)  present plots of the convergence rate and median number of iterations as a function of the  decay factor o. While when there is no learning (o -- 1) only 43% of the runs converge,  Learning from User Feedback in Image Retrieval Systems 983  the convergence rate is always higher when learning takes place and for a significant range  of a (a  [0.5, 0.8]) it is above 60%. This not only confirms that learning can lead to  significant improvements of retrieval performance but also shows that a precise selection of  a is not crucial. Furthermore, when convergence occurs it is usually very fast, taking from  4 to 6 iterations. On the other hand, a significant percentage of runs do not converge and  the majority of these are cases of strong divergence.  As illustrated by Figure 3 c) and d), this percentage decreases significantly when both  positive and negative examples are allowed. The rate of convergence is in this case usually  between 80 and 90 % and strong divergence never occurs. And while the number of  iterations for convergence increases, convergence is still fast (usually below 10 iterations).  This is indeed the great advantage of negative examples: they encourage some exploration  of the database which avoids local minima and leads to convergence. Notice that, when  there is no learning, the convergence rate is high and learning can actually increase the  rate of divergence. We believe that this is due to the inconsistencies associated with the  negative example selection strategy. However, when convergence occurs, it is always faster  if learning is employed.  Figure 3: Learning performance as a function of t. Left: Percent of runs which converged. Right:  Median number of iterations. Top: positive examples. Bottom: positive and negative examples.  References  [1 ] S. Belongie, C. Carson, H. Greenspan, and J. Malik. Color-and texture-based image segmentation  using EM and its application to content-based image retrieval. In International Conference on  Computer Vision, pages 675-682, Bombay, India, 1998.  [2] I. Cox, M. Miller, S. Omohundro, and P. Yiamlos. PicHunter: Bayesian Relevance Feedback for  Image Retrieval. In Int. Conf. on Pattern Recognition, Vienna, Austria, 1996.  [3] L. Devroye, L. Gyorfi, and G. Lugosi. A Probabilistic Theory of Pattern Recognition. Springer-  Verlag, 1996.  [4] A. Gelman, J. Carlin, H. Stem, and D. Rubin. Bayesian Data Analysis. Chapman Hall, 1995.  [5] A. Pentland, R. Picard, and S. Sclaroff. Photobook: Content-based Mampulation of Image  Databases. International Journal of Computer Vision, Vol. 18(3):233-254, June 1996.  PART IX  CONTROL, NAVIGATION AND PLANNING  
Distributed Synchrony of Spiking Neurons  in a Hebbian Cell Assembly  David Horn Nir Levy  School of Physics and Astronomy,  Raymond and Beverly Sackler Faculty of Exact Sciences,  Tel Aviv University, Tel Aviv 69978, Israel  hornneuron. tau. ac. il nirlevypost. tau. ac. il  Isaac Meilijson Eytan Ruppin  School of Mathematical Sciences,  Raymond and Beverly Sackler Faculty of Exact Sciences,  Tel Aviv University, Tel Aviv 69978, Israel  isacomath. tau. ac. il ruppinmath. tau. ac. il  Abstract  We investigate the behavior of a Hebbian cell assembly of spiking  neurons formed via a temporal synaptic learning curve. This learn-  ing function is based on recent experimental findings. It includes  potentiation for short time delays between pre- and post-synaptic  neuronal spiking, and depression for spiking events occuring in the  reverse order. The coupling between the dynamics of the synaptic  learning and of the neuronal activation leads to interesting results.  We find that the cell assembly can fire asynchronously, but may  also function in complete synchrony, or in distributed synchrony.  The latter implies spontaneous division of the Hebbian cell assem-  bly into groups of cells that fire in a cyclic manner. We invetigate  the behavior of distributed synchrony both by simulations and by  analytic calculations of the resulting synaptic distributions.  i Introduction  The Hebbian paradigm that serves as the basis for models of associative memory is  often conceived as the statement that a group of excitatory neurons (the Hebbian  cell assembly) that are coupled synaptically to one another fire together when a  subset of the group is being excited by an external input. Yet the details of the  temporal spiking patterns of neurons in such an assembly are still ill understood.  Theoretically it seems quite obvious that there are two general types of behavior:  synchronous neuronal firing, and asynchrony where no temporal order exists in the  assembly and the different neurons fire randomly but with the same overall rate.  Further subclassifications were recently suggested by [Brunel, 1999]. Experimen-  tally this question is far from being settled because evidence for the associative  130 D. Horn, N. Levy, I. Meilijson and E. Ruppin  memory paradigm is quite scarce. On one hand, one possible realization of associa-  tive memories in the brain was demonstrated by [Miyashita, 1988] in the inferotem-  poral cortex. This area was recently reinvestigated by [Yakovlev et al., 1998] who  compared their experimental results with a model of asynchronized spiking neurons.  On the other hand there exists experimental evidence [Abeles, 1982] for temporal  activity patterns in the frontal cortex that Abeles called synfire-chains. Could they  correspond to an alternative type of synchronous realization of a memory attractor?  To answer these questions and study the possible realizations of attractors in  cortical-like networks we investigate the temporal structure of an attractor assuming  the existence of a synaptic learning curve that is continuously applied to the mem-  ory system. This learning curve is motivated by the experimental observations of  [Markram et al., 1997, Zhang et al., 1998] that synaptic potentiation or depression  occurs within a critical time window in which both pre- and post-synaptic neurons  have to fire. If the pre-synaptic neuron fires first within 30ms or so, potentiation  will take place. Depression is the rule for the reverse order.  The regulatory effects of such a synaptic learning curve on the synapses of  a single neuron that is subjected to external inputs were investigated by  [Abbott and Song, 1999] and by [Kempter et al., 1999]. We investigate here the  effect of such a rule within an assembly of neurons that are all excited by the  same external input throughout a training period, and are allowed to influence one  another through their resulting sustained activity.  2 The Model  We study a network composed df NE excitatory and Ni inhibitory integrate-and-fire  neurons. Each neuron in the network is described by its subthreshold membrane  potential V/(t) obeying  ,(t) = _1) + SIc(t) (1)  'n  where rn is the neuronal integration time constant. A spike is generated when V/(t)  reaches the threshold Vrest + 0, upon which a refractory period of rnp is set on and  the membrane potential is reset to Vrest where Vt < Vt  Vet + . I(t) is  the sum of recurrent and external synaptic current inputs. The net synaptic input  charging the membrane of excitatory neuron i at time t is  -- Jj (t) E  l E1 (t -- I et  j ! j m  (2)  summing over the different synapses of j = 1,...,NE excitatory neurons and of  j- 1,...,Ni inhibitory neurons, with postsynaptic efficacies JibE(t) and jI re-  spectively. The sum over I (m) represents a sum on different spikes arriving at  l (t = where t (t?) is the emission time of  synapse j, at times t = tj q- Td t? q- Td), tj  the l-th (m-th) spike from the excitatory (inhibitory) neuron j and rd is the synap-  tic delay. I ext, the external current, is assumed to be random and independent at  each neuron and each time step, drawn from a Poisson distribution with mean M xt.  Analogously, the synaptic input to the inhibitory neuron i at time t is  - ' - - (t- t? +  RIi(t) = E JiS E  (t t rd) E iI E  -- I ext.  j l j m  (3)  We assume full connectivity among the excitatory neurons, but only partial con-  nectivity between all other three types of possible connnections, with connection  Distributed Synchrony of Spiking Neurons in a Hebbian Cell Assembly 131  probabilities denoted by C EI, C IE and C II. In the following we will report simula-  tion results in which the synaptic delays rd were assigned to each synapse, or pair of  neurons, randomly, chosen from some finite set of values. Our analytic calculation  will be done for one fixed value of this delay parameter.  The synaptic efficacies between excitatory neurons are assumed to be potentiated  or depressed according to the firing patterns of the pre- and post-synaptic neurons.  In addition we allow for a uniform synaptic decay. Thus each excitatory synapse  obeys  'EE - ---Jj (t) + Fij(t) (4)  Jij (t)-- 1 EE  rs  where the synaptic decay constant rs is assumed to be very large compared to the  membrane time constant rn. JibE(t) are constrained to vary in the range [0, Jm:].  The change in synaptic efficacy is defined by Fij (t), as  Fib(t) = Z [6(t ti)Kp(tt 5 - tf) + 6(t - t t _  -  k,l  (5)  where Kp and KD are the potentiation and depression branches of the kernel func-  tion  K(5) = -cSexp f-(aS + b) 2] (6)  plotted in Figure 1. Following [Zhang et al., 1998] we distinguish between the sit-  uation where the postsynaptic spike, at t k  i, appears after or before the presynaptic  spike, at tzj, using the asymmetric kernel that captures the essence of their experi-  mental observations.  t I t k  Figure 1: The kernel function whose left part, Kp, leads to potentiation of the  synapse, and whose right branch, KD, causes synaptic depression.  3 Distributed Synchrony of a Hebbian Assembly  We have run our system with synaptic delays chosen randomly to be either 1, 2, or  3ms, and temporal parameters rn chosen as 40ms for excitatory neurons and 20ms  for inhibitory ones. Turning external input currents off after a while we obtained  sustained firing activities in the range of 100-150 Hz. We have found, in addition to  synchronous and asynchronous realizations of this attractor, a mode of distributed  synchrony. A characteristic example of a long cycle is shown in Figure 2: The 100  excitatory neurons split into groups such that each group fires at the same frequency  and at a fixed phase difference from any other group. The Ji E synaptic efficacies  132 D. Horn, N. Levy, I. Meilijson and E. Ruppin  Figure 2: Distributed synchronized firing mode. The firing patterns of six cell as-  semblies of excitatory neurons are displayed vs time (in ms). These six groups of  neurons formed in a self-organized manner for a kernel function with equal poten-  tiation and depression. The delays were chosen randomly from three values, I 2 or  3ms, and the system is monitored every 0.5ms.  are initiated as small random values. The learning process leads to the self-organized  synaptic matrix displayed in Figure 3(a). The block form of this matrix represents  the ordered couplings that are responsible for the fact that each coherent group of  neurons feeds the activity of groups that follow it. The self-organized groups form  spontaneously. When the synapses are affected by some external noise, as can come  about from Hebbian learning in which these neurons are being coupled with other  pools of neurons, the groups will change and regroup, as seen in Figure 3(b) and  3(c).  (a) (b) (c)  Figure 3: A synaptic matrix for n = 6 distributed synchrony. The synaptic matrix  between the 100 excitatory neurons of our system is displayed in a grey-level code  with black meaning zero efficacy and white standing for the synaptic upper-bound.  (a) The matrix that exists during the distributed synchronous mode of Figure 2.  Its basis is ordered such that neurons that fire together are grouped together. (b)  Using the same basis as in (a) a new synaptic matrix is shown, one that is formed  after stopping the sustained activity of Figure 2, introducing noise in the synaptic  matrix, and reinstituting the original memory training. (c) The same matrix as (b)  is shown in a new basis that exhibits connections that lead to a new and different  realization of distributed synchrony.  A stable distributed synchrony cycle can be simply understood for the case of a  single synaptic delay setting the basic step, or phase difference, of the cycle. When  several delay parameters exist, a situation that probably more accurately represents  the c-function character of synaptic transmission in cortical networks, distributed  Distributed Synchrony of Spiking Neurons in a Hebbian Cell Assembly 133  synchrony may still be obtained, as is evident from Figure 2. After some time  the cycle may destabilize and regrouping may occur by itself, without external  interference. The likelihood of this scenario is increased because different synaptic  connections that have different delays can interfere with one another. Nonetheless,  over time scales of the type shown in Figure 2, grouping is stable.  4 Analysis of a Cycle  In this section we analyze the dynamics of the network when it is in a stable state  of distributed synchrony. We assume that n groups of neurons are formed and  calculate the stationary distribution of JE(t). In this state the firing pattern of  every two neurons in the network can be characterized by their frequency v(t) and  by their relative phase 5. We assume that 5 is a random normal variable with  mean/5 and standard deviation as. Thus, Eq. 4 can be rewritten as the following  stochastic differential equation  djiE(t) = [lF,j(t) -- 1---.l.E.E(t)] dt-t-aF, j(t)dW(t) (7)  Ts  3  where Fij(t) (Eq. 5) is represented here by a drift term YF (t) and a diffusion  term aF (t) which are its mean and standard deviation. W(t) describes a Wiener  process. Note that both YF (t) and aF (t) are calculated for a specific distribution  of 5 and are functions of Y6 and  The stochastic process that satisfies Eq. 7 will satisfy the Fokker-Plank equation  for the probability distribution f of Ji E,  ' - (s)  with reflecting boundary conditions imposed by the synaptic bounds, 0 and  Since we are interested in the stable state of the process we solve the stationary  equation. The resulting density function is  [1(  f(Ji ,Ua, a,)= a},(t) exp a2 2baJi e j (9)  where  = i(J;e,  JO  Eq. 9 enables us to calculate the stationary distribution of the synaptic efficacies  between the presynaptic neuron i and the post-synaptic neuron j given their fre-  quency  and the parameters 5 d a,. An example of a solution for a 3-cycle is  shown in Figure 4. In this ce all neurons fire with frequency  = (3r) - and  takes one of the values -r, 0, r.  Simulation results of a 3-cycle in a network of excitatory and inhibitory integrate-  and-fire neurons described in Section 2 are given in Figure 5. As can be seen the  results obtained from the analysis match those observed in the simulation.  5 Discussion  The interesting experimental observations of synaptic learning curves  [Markram et al., 1997, Zhang et al., 1998] have led us to study their implica-  tions for the firing patterns of a Hebbian cell assembly. We find that, in addition  134 D. Horn, N. Levy, I. Meilijson and E. Ruppin  (a)  (b)  7O  01 0.2 O.3 O4  60  50  40  3O  20  10  0  0.1 0.2 0.3  jEE  0.4 0.5  Figure 4: Results of the analysis for n = 3, a5 = 2ms and Td = 2.5ms. (a) The  synaptic matrix. Each of the nine blocks symbolizes a group of connections between  neurons that have a common phase-lag . The mean of jE was calculated for  each cell by Eq. 9 and its value is given by the gray scale tone. (b) The distribution  of synaptic values between all excitatory neurons.  (a)  0 0.1 0.2 0.3 0.4 0.5  (b)  5000  4500  4000  3500  3O00  2500  2000  1500  1000  500  0  0 0.1 0.2 0.3 0.4 0.5  Figure 5: Simulation results for a network of Ne = 100 and N/ = 50 integrate-  and-fire neurons, when the network is in a stable n = 3 state. T, = lores for both  excitatory and inhibitory neurons. The average frequency of the neurons is 130 Hz.  (a) The excitatory synaptic matrix. (b) Histogram of the synaptic efficacies.  to the expected synchronous and asynchronous modes, an interesting behavior  of distributed synchrony can emerge. This is the phenomenon that we have  investigated both by simulations and by analytic evaluation.  Distributed synchrony is a mode in which the Hebbian cell assembly breaks into an  n-cycle. This cycle is formed by instantaneous symmetry breaking, hence specific  classification of neurons into one of the n groups depends on initial conditions, noise,  etc. Thus the different groups of a single cycle do not have a semantic invariant  meaning of their own. It seems perhaps premature to try and identify these cycles  with synfire chains [Abeles, 1982] that show recurrence of firing patterns of groups  of neurons with periods of hundreds of ms. Note however, that if we make such an  identification, it is a different explanation from the model of [Herrmann et al., 1995],  which realizes the synfire chain by combining sets of preexisting patterns into a cycle.  The simulations in Figures 2 and 3 were carried out with a learning curve that  possessed equal potentiation and depression branches, i.e. was completely anti-  symmetric in its argument. In that case no synaptic decay was allowed. Figure 5,  on the other hand, had stronger potentiation than depression, and a finite synaptic  Distributed Synchrony of Spiking Neurons in a Hebbian Cell Assembly 135  decay time was assumed. Other conditions in these nets were different too, yet  both had a window of parameters where distributed synchrony showed up. Using  the analytic approach of section 4 we can derive the probability distribution of  synaptic values once a definite cyclic pattern of distributed synchrony is formed.  An analytic solution of the combined dynamics of both the synapses and the spiking  neurons is still an open challenge. Hence we have to rely on the simulations to prove  that distributed synchrony is a natural spatiotemporal behavior that follows from  combined neuronal dynamics and synaptic learning as outlined in section 2. To the  extent that both types of dynamics reflect correctly the dynamics of cortical neural  networks, we may expect distributed synchrony to be a mode in which neuronal  attractors are being realized.  The mode of distrbuted synchrony is of special significance to the field of neural com-  putation since it forms a bridge between the feedback and feed-forward paradigms.  Note that whereas the attractor that is formed by the Hebbian cell assembly is of  global feedback nature, i.e. one may regard all neurons of the assembly as being  connected to other neurons within the same assembly, the emerging structure of  distributed synchrony shows that it breaks down into groups. These groups are  connected to one another in a self-organized feed-forward manner, thus forming the  cyclic behavior we have observed.  References  [Abbott and Song, 1999] L. F. Abbott and S. Song. Temporally asymmetric heb-  bian learning, spike timing and neuronal response variability. In M. S. Kearns,  S. A. Solla, and D. A. Cohn, editors, Advances in Neural Information Processing  Systems 11: Proceedings of the 1998 Conference, pages 69- 75. MIT Press, 1999.  [Abeles, 1982] M. Abeles. Local Cortical Circuits. Springer, Berlin, 1982.  [Brunel, 1999] N. Brunel. Dynamics of sparsely connected networks of excitatory  and inhibitory spiking neurons. Journal of Computational Neuroscience, 1999.  [Herrmann et al., 1995] M. Herrmann, J. Hertz, and A. Prfigel-Bennet. Analysis of  synfire chains. Network: Comp. in Neural Systems, 6:403 - 414, 1995.  [Kempter et al., 1999] R. Kempter, W. Gerstner, and J. Leo van Hemmen. Spike-  based compared to rate-based hebbian learning. In M. S. Kearns, S. A. Solla,  and D. A. Cohn, editors, Advances in Neural Information Processing Systems 11:  Proceedings of the 1998 Conference, pages 125 - 131. MIT Press, 1999.  [Markram et al., 1997] H. Markram, J. Liibke, M. Frotscher, and B. Sakmann. Reg-  ulation of synaptic efficacy by coincidence of postsynaptic aps and epsps. Science,  275(5297):213- 215, 1997.  [Miyashita, 1988] Y. Miyashita. Neuronal correlate of visual associative long-term  memory in the primate temporal cortex. Nature, 335:817- 820, 1988.  [Yakovlev et al., 1998] V. Yakovlev, S. Fusi, E. Berman, and E. Zohary. Inter-trial  neuronal activity in inferior temporal cortex: a putative vehicle to generate long-  term visual associations. Nature Neurosc., 1(4):310 - 317, 1998.  [Zhang et al., 1998] L. I. Zhang, H. W. Tao, C. E. Holt, W. A. Harris, and M. Poo.  A critical window for cooperation and competition among developing retinotectal  synapses. Nature, 395:37- 44, 1998.  
Online Independent Component Analysis  With Local Learning Rate Adaptation  Nicol N. Schraudolph  nicidsia. ch  Xavier Giannakopoulos  xavieridsia. ch  IDSIA, Corso Elvezia 36  6900 Lugano, Switzerland  http://ww. idsia. ch/  Abstract  Stochastic meta-descent (SMD) is a new technique for online adap-  tation of local learning rates in arbitrary twice-differentiable sys-  tems. Like matrix momentum it uses full second-order information  while retaining O(n) computational complexity by exploiting the  efficient computation of Hessian-vector products. Here we apply  SMD to independent component analysis, and employ the result-  ing algorithm for the blind separation of time-varying mixtures.  By matching individual learning rates to the rate of change in each  source signal's mixture coefficients, our technique is capable of si-  multaneously tracking sources that move at very different, a priori  unknown speeds.  1 Introduction  Independent component analysis (ICA) methods are typically run in batch mode in  order to keep the stochasticity of the empirical gradient low. Often this is combined  with a global learning rate annealing scheme that negotiates the tradeoff between  fast convergence and good asymptotic performance. For time-varying mixtures,  this must be replaced by a learning rate adaptation scheme. Adaptation of a single,  global learning rate, however, facilitates the tracking only of sources whose mixing  coefficients change at comparable rates [1], resp. switch all at the same time [2].  In cases where some sources move much faster than others, or switch at different  times, individual weights in the unmixing matrix must adapt at different rates in  order to achieve good performance.  We apply stochastic meta-descent (SMD), a new online adaptation method for local  learning rates [3, 4], to an extended Bell-Sejnowski ICA algorithm [5] with natural  gradient [6] and kurtosis estimation [7] modifications. The resulting algorithm is  capable of separating and tracking a time-varying mixture of 10 sources whose  unknown mixing coefficients change at different rates.  790 N. N. Schraudolph and X. Giannakopoulos  2 The SMD Algorithm  Given a sequence 0, ,... of data points, we minimize the expected value of a  twice-differentiable loss function f() with respect to its parameters  by stochas-  tic gradient descent:  _ (1)  where _ Off  and  denotes component-wise multiplication. The local learning rates fi are best  adapted by exponentiated gradient descent [8, 9], so that they can cover a wide  dynamic range while staying strictly positive:  Of(gt)  lntYt = ln/-i- /a OlntY  tYt = fft_l . exp(lt . t) ,  where Yt -- 01nil (2)  and p is a global meta-learning rate. This approach rests on the assumption that  each element of fi affects f() only through the corresponding element of . With  considerable variation, (2) forms the basis of most local rate adaptation methods  found in the literature.  In order to avoid an expensive exponentiation [10] for each weight update, we typ-  ically use the linearization e u  1 + u, valid for small lu], giving  /: -- fft_l-max(o, i + It'Yt), (3)  where we constrain the multiplier to be at least (typically)  = 0.1 as a safeguard  against unreasonably small -- or negative -- values. For the meta-level gradient  descent to be stable,/a must in any case be chosen such that the multiplier for fidoes  not stray far from unity; under these conditions we find the linear approximation  (3) quite sufficient.  Definition of 7. The gradient trace 7 should accurately measure the effect that  a change in local learning rate has on the corresponding weight. It is tempting to  consider only the immediate effect of a change in fit on tt+' declaring tt and (t  in (1) to be independent of fit, one then quickly arrives at  _  t4-1 -- O]nYt -- Yt'fft (4)  However, this common approach [11, 12, 13, 14, 15] fails to take into account the  incremental nature of gradient descent: a change in fi affects not only the current  update of , but also future ones. Some authors account for this by setting  to  an exponential average of past gradients [2, 11, 16]; we found empirically that the  method of Alineida et al. [15] can indeed be improved by this approach [3]. While  such averaging serves to reduce the stochasticity of the product t ' t- implied by  (3) and (4), the average remains one of immediate, single-step effects.  By contrast, Sutton [17, 18] models the long-term effect of fi on future weight  updates in a linear system by carrying the relevant partials forward through time,  as is done in real-time recurrent learning [19]. This results in an iterative update  rule for , which we have extended to nonlinear systems [3, 4]. We define  as an  Online ICA with Local Rate Adaptation 791  exponential average of the effect of all past changes in ff on the current weights:   = (1-A)Ai  i=o Olnp-_i (5)  The forgetting factor 0 _ A _ 1 is a free parameter of the algorithm. Inserting (1)  =  OlnIt-i  i=0  ,xt + Yt-gt  = (6)  into (5) gives  where Ht denotes the instantaneous Hessian of f, () at time t. The approximation  in (6) assumes that (Vi > 0)Ofit/Ofit-i - 0; this signifies a certain dependence on  an appropriate choice of meta-learning rate p. Note that there is an efficient O (n)  algorithm to calculate HtYt without ever having to compute or store the matrix  Ht itself [20]; we shall elaborate on this technique for the case of independent  component analysis below.  Meta-level conditioning. The gradient descent in ff at the meta-level (2) may  of course suffer from ill-conditioning just like the descent in t at the main level  (1); the meta-descent in fact squares the condition number when  is defined as the  previous gradient, or an exponential average of past gradients. Special measures to  improve conditioning are thus required to make meta-descent work in non-trivial  systems.  Many researchers [11, 12, 13, 14] use the sign function to radically normalize the  if-update. Unfortunately such a nonlinearity does not preserve the zero-mean prop-  erty that characterizes stochastic gradients in equilibrium -- in particular, it will  translate any skew in the equilibrium distribution into a non-zero mean change in  if. This causes convergence to non-optimal step sizes, and renders such methods un-  suitable for online learning. Notably, Almeida et al. [15] avoid this pitfall by using  a running estimate of the gradient's stochastic variance as their meta-normalizer.  In addition to modeling the long-term effect of a change in local learning rate, our  iterative gradient trace serves as a highly effective conditioner for the meta-descent:  the fixpoint of (6) is given by  7 = [ AHt + (1-,)diag(1/ff)]-l (7)  -- a modified Newton step, which for typical values of A (i.e., close to 1) scales with  the inverse of the gradient. Consequently, we can expect the product t - Yt in (2)  to be a very well-conditioned quantity. Experiments with feedforward multi-layer  perceptrons [3, 4] have confirmed that SMD does not require explicit meta-level  normalization, and converges faster than alternative methods.  3 Application to ICA  We now apply the SMD technique to independent component analysis, using the  Bell-Sejnowski algorithm [5] as our base method. The goal is to find an unmixing  792 N. N. Schraudolph and X. Giannakopoulos  matrix Wt which -- up to scaling and permutation -- provides a good linear esti-  mate t -= Wtt of the independent sources gt present in a given mixture signal t-  The mixture is generated linearly according to t - At gt, where At is an unknown  (and unobservable) full rank matrix.  We include the well-known natural gradient [6] and kurtosis estimation [7] modifi-  cations to the basic algorithm, as well as a matrix Pt of local learning rates. The  resulting online update for the weight matrix Wt is  Wt+l  Wt - Pt' Dr, (8)  where the gradient Dt is given by  0fw(t)  Ot -= OWt  -- ([fit q- tanh(7t)] 7t T- I) Wt ,  (9)  with the sign for each component of the tanh(t) term depending on its current  kurtosis estimate.  Following Pearlmutter [20], we now define the differentiation operator  ?Z(g(Wt)) = Og(Wt +rVt)[ (10)  01' r=O  which describes the effect on g of a perturbation of the weights in the direction of  Vt. We can use T to efficiently calculate the Hessian-vector product  Ht*Vt  vec-[Ht vec(Vt)] - ?Z(Dt) (11)  where "vec" is the operator that concatenates all Columns of a matrix into a single  column vector. Since T is a linear operator, we have  ?Z(Wt) = Vt, (12)  ?Zvt(t) = ?Zvt(Wtt)= Vtt, (13)  Tg (tanh(fft)) = diag(tanh'(7t)) Vt:t, (14)  and so forth (cf. [20]). Starting from (9), we apply the T operator to obtain  Ht.Vt  (15)  In conjunction with the matrix versions of our learning rate update (3)  Pt = Pt- max(ao, 1 - /Dt. Vt )  (16)  and gradient trace (6)  Vt+l = Vt  -- Pt ' (Dt + /kHt*Vt)  (17)  this constitutes our SMD-ICA algorithm.  Online IC.4 qth Local Rate Adaptation 793  4 Experiment  The algorithm was tested on an artificial problem where 10 sources follow elliptic  trajectories according to  ' = (Abase + A1 sin(t) + A2 cos(t))g  (18)  where Abase is a normally distributed mixing matrix, as well as A and A2, whose  columns represent the axes of the ellipses on which the sources travel. The velocities   are normally distributed around a mean of one revolution for every 6 000 data  samples. All sources are supergaussian.  The ICA-SMD algorithm was implemented with only online access to the data,  including on-line whitening [21]. Whenever the condition number of the estimated  whitening matrix exceeded a large threshold (set to 350 here), updates (16) and (17)  were disabled to prevent the algorithm from diverging. Other parameters settings  were/a -- 0.1, A = 0.999, and p - 0.2.  Results that were not separating the 10 sources without ambiguity were discarded.  Figure I shows the performance index from [6] (the lower the better, zero being  the ideal case) along with the condition number of the mixing matrix, showing that  the algorithm is robust to a temporary confusion in the separation. The ordinate  represents 3 000 data samples, divided into mini-batches of 10 each for efficiency.  Figure 2 shows the match between an actual mixing column and its estimate, in  the subspace spanned by the elliptic trajectory. The singularity occurring halfway  through is not damaging performance. Globally the algorithm remains stable as  long as degenerate inputs are handled correctly.  5 Conclusions  Once SMD-ICA has found a separating solution, we find it possible to simultane-  ously track ten sources that move independently at very different, a priori unknown  Error index --  cond(A)/20 ----+---  0 50 1 O0 150 200 250 300  Figure 1: Global view of the quality of separation  794 N. N. Schraudolph and X. Giannakopoulos  Estimation error --  I ,, I I I I I I I I  -2.5 -2 1.5 -1 -0.5 0 0.5 1 1.5 2 2.5  Figure 2: Projection of a column from the mixing matrix. Arrows link the exact  point with its estimate; the trajectory proceeds from lower right to upper left.  speeds. To continue tracking over extended periods it is necessary to handle mo-  mentary singularities, through online estimation of the number of sources or some  other heuristic solution. SMD's adaptation of local learning rates can then facilitate  continuous, online use of ICA in rapidly changing environments.  Acknowledgments  This work was supported by the Swiss National Science Foundation under grants  number 2000-052678.97/1 and 2100-054093.98.  References  [1]  J. Karhunen and P. Pajunen, "Blind source separation and tracking using  nonlinear PCA criterion: A least-squares approach", in Proc. IEEE Int. Conf.  on Neural Networks, Houston, Texas, 1997, pp. 2147-2152.  [2]  N. Murata, K.-R. Mfiller, A. Ziehe, and S.-i. Amari, "Adaptive on-line learning  in changing environments", in Advances in Neural Information Processing  Systems, M. C. Mozer, M. I. Jordan, and T. Petsche, Edso 1997, vol. 9, pp.  599-605, The MIT Press, Cambridge, MA.  [3]  N. N. Schraudolph, "Local gain adaptation in stochastic gradient descent", in  Proceedings of the 9th International Conference on Artificial Neural Networks,  Edinburgh, Scotland, 1999, pp. 569-574, IEE, London, tp://tp.d$a.  pub/nic/smd. ps. gz.  [4]  N. N. Schraudolph, "Online learning with adaptive local step sizes", in Neural  Nets -- WIRN Vietri-99: Proceedings of the 11th Italian Workshop on Neural  Nets, M. Marinaro and R. Tagliaferri, Eds., Vietri sul Mare, Salerno, Italy,  1999, Perspectives in Neural Computing, pp. 151-156, Springer Verlag, Berlin.  Online ICA with Local Rate Adaptation 795  [5] A. J. Bell and T. J. Sejnowski, "An information-maximization approach to  blind separation and blind deconvolution", Neural Computation, 7(6):1129-  1159, 1995.  [6] S.-i. Amari, A. Cichocki, and H. H. Yang, "A new learning algorithm for blind  signal separation", in Advances in Neural Information Processing Systems,  D. S. Touretzky, M. C. Mozer, and M. E. Hasselmo, Eds. 1996, vol. 8, pp.  757-763, The MIT Press, Cambridge, MA.  [7] M. Girolami and C. Fyfe, "Generalised independent component analysis  through unsupervised learning with emergent bussgang properties", in Proc.  IEEE Int. Conf. on Neural Networks, Houston, Texas, 1997, pp. 1788-1791.  [8] J. Kivinen and M. K. Warmuth, "Exponentiated gradient versus gradient  descent for linear predictors", Tech. Rep. UCSC-CRL-94-16, University of  California, Santa Cruz, June 1994.  [9] J. Kivinen and M. K. Warmuth, "Additive versus exponentiated gradient  updates for linear prediction", in Proc. 27th Annual ACM Symposium on  Theory of Computing, New York, NY, May 1995, pp. 209-218, The Association  for Computing Machinery.  [10] N. N. Schraudolph, "A fast, compact approximation of the exponential func-  tion", Neural Computation, 11(4):853-862, 1999.  [11] R. Jacobs, "Increased rates of convergence through learning rate adaptation",  Neural Networks, 1:295-307, 1988.  [12] T. Tollenaere, "SuperSAB: fast adaptive back propagation with good scaling  properties", Neural Networks, 3:561-573, 1990.  [13] F. M. Silva and L. B. Almeida, "Speeding up back-propagation", in Advanced  Neural Computers, R. Eckmiller, Ed., Amsterdam, 1990, pp. 151-158, Elsevier.  [14] M. Riedmiller and H. Braun, "A direct adaptive method for faster backprop-  agation learning: The RPROP algorithm", in Proc. International Conference  on Neural Networks, San Francisco, CA, 1993, pp. 586-591, IEEE, New York.  [15] L. B. Almeida, T. Langlois, J. D. Amaral, and A. Plakhov, "Parameter adap-  tation in stochastic optimization", in On-Line Learning in Neural Networks,  D. Saad, Ed., Publications of the Newton Institute, chapter 6. Cambridge Uni-  versity Press, 1999, ftp://146. 193.2. 131/pub/lba/paper$/adsteps. p$. gz.  [16] M. E. Harmon and L. C. Baird III, "Multi-player residual advantage learning  with general function approximation", Tech. Rep. WL-TR-1065, Wright Labo-  ratory, WL/AACF, 2241 Avionics Circle, Wright-Patterson Air Force Base, OH  45433-7308, 1996, http://w. leemon. com/papers/sim_tech/sim_tech. ps. gz.  [17] R. S. Sutton, "Adapting bias by gradient descent: an incremental version of  delta-bar-delta", in Proc. loth National Conference on Artificial Intelligence.  1992, pp. 171-176, The MIT Press, Cambridge, MA, ftp://ftp. cs .amass.edu/  pub/an/pub/sutt on/sutt on-92 a. ps. gz.  [18] R. S. Sutton, "Gain adaptation beats least squares?", in Proc. 7th Yale Work-  shop on Adaptive and Learning Systems, 1992, pp. 161-166, ftp://ftp.cs.  amass. edu/pub/an/pub/sutt on/sutt on-92b. ps. gz.  [19] R. Williams and D. Zipser, "A learning algorithm for continually running fully  recurrent neural networks", Neural Computation, 1:270-280, 1989.  [20] B. A. Pearlmutter, "Fast exact multiplication by the Hessian", Neural Com-  putation, 6(1):147-160, 1994.  [21] J. Karhunen, E. Oja, L. Wang, R. Vigario, and J. Joutsensalo, "A class of  neural networks for independent component analysis", IEEE Trans. on Neural  Networks, 8(3):486-504, 1997.  
Potential Boosters ?  Nigel Duffy  Department of Computer Science  University of California  Santa Cruz, CA 95064  nigedufjfcse. ucsc. edu  David Helmbold  Department of Computer Science  University of California  Santa Cruz, CA 95064  dphcse. ucsc. edu  Abstract  Recent interpretations of the Adaboost algorithm view it as per-  forming a gradient descent on a potential function. Simply chang-  ing the potential function allows one to create new algorithms re-  lated to AdaBoost. However, these new algorithms are generally  not known to have the formal boosting property. This paper ex-  mines the question of which potential functions lead to new al-  gorithms that are boosters. The two main results are general sets  of conditions on the potential; one set implies that the resulting  algorithm is a booster, while the other implies that the algorithm  is not. These conditions are applied to previously studied potential  functions, such as those used by LogitBoost and Doom II.  I Introduction  The first boosting algorithm appeared in Rob Schapire's thesis [1]. This algorithm  was able to boost the performance of a weak PAC learner [2] so that the resulting  algorithm satisfies the strong PAC learning [3] criteria. We will call any method  that builds a strong PAC learning algorithm from a weak PAC learning algorithm  a PAC boosting algorithm. Freund and Schapire later found an improved PAC  boosting algorithm called AdaBoost [4], which also tends to improve the hypotheses  generated by practical learning algorithms [5].  The AdaBoost algorithm takes a labeled training set and produces a master hy-  pothesis by repeatedly calling a given learning method. The given learning method  is used with different distributions on the training set to produce different base  hypotheses. The master hypothesis returned by AdaBoost is a weighted vote of  these base hypotheses. AdaBoost works iteratively, determining which examples  are poorly classified by the current weighted vote and selecting a distribution on  the training set to emphasize those examples.  Recently, several researchers [6, 7, 8, 9, 10] have noticed that Adaboost is performing  a constrained gradient descent on an exponential potential function of the margins  of the examples. The margin of an example is yF(x) where y is the :51 valued label  of the example x and F(x)   is the net weighted vote of master hypothesis F.  Once Adaboost is seen this way it is clear that further algorithms may be derived  by changing the potential function [6, 7, 9, 10].  Potential Boosters? 259  The exponential potential used by AdaBoost has the property that the influence  of a data point increases exponentially if it is repeatedly misclassified by the base  hypotheses. This concentration on the "hard" examples allows AriaBoost to rapidly  obtain a consistent hypothesis (assuming that the base hypotheses have certain  properties). However, it also means that an incorrectly labeled or noisy example  can quickly attract much of the distribution. It appears that this lack of noise-  tolerance is one of AdaBoost's few drawbacks [tt]. Several researchers [7, 8, 9, t0]  have proposed potential functions which do not concentrate as much on these "hard"  examples. However, they generally do not show that the derived algorithms have  the PAC boosting property.  In this paper we return to the original motivation behind boosting algorithms and  ask: "for which potential functions does gradient descent lead to PAC boosting  algorithms" (i.e. boosters that create strong PAC learning alg. orithms from arbitrary  weak PAC learners). We give necessary conditions that are met by some of the  proposed potential functions (most notably the LogitBoost potential introduced by  Friedman et al. [7]). Furthermore, we show that simple gradient descent on other  proposed potential functions (such as the sigmoidal potential used by Mason et  al. It0]) cannot convert arbitrary weak PAC learning algorithms into strong PAC  learners. The aim of this work is to identify properties of potential functions required  for PAC boosting, in order to guide the search for more effective potentials.  Some potential functions have an additional tunable parameter [10] or change over  time [12]. Our results do not yet apply to such dynamic potentials.  2 PAC Boosting  Here we define the notions of PAC learning  and boosting, and define the notation  used throughout the paper.  A concept C is a subset of the learning domain X. A random example of C is a pair  (x  X,y  {-1, +1)) where x is drawn from some distribution on X and y = I if  x  C and -1 otherwise. A concept class is a set of concepts.  Definition 1 A (strong) PAC learner for concept class C has the property that for  every distribution D on 2d, all concepts C E C, and all 0 < e, 6 < 1/2: with probabil-  ity at least 1- 6 the algorithm outputs a hypothesis h where PD[h(x)  C(x)] _ e.  The learning algorithm is given C, e, 6, and the ability to draw random examples of  C (w.r.t. distribution D), and must run in time bounded by poly(1/e, 1/6).  Definition 2 A weak PAC learner is similar to a strong PA C learner, except that  it need only satisfy the conditions for a particular 0 < co, 6o < 1/2 pair, rather than  for all e,6 pairs.  Definition 3 A PAC boosting algorithm is a generic algorithm which can leverage  any weak PA C learner to meet the strong PA C learning criteria.  In the remainder of the paper we emphasize boosting the accuracy e as it is much  easier to boost the confidence 6, see Haussler et al. [13] and Freund [14] for details.  Furthermore, we emphasize boosting by re-sampling, where the strong PAC learner  draws a large sample, and each iteration the weak learning algorithm is called with  some distribution over this sample.  XTo simplify the presentation we omit the instance space dimension and taxget repre-  sentation length parameters.  260 N. DuJ and D. Helmbold  Throughout the paper we use the following notation.   m is the cardinality of the fixed sample ((xl,Yl),..., (xm,y,)).   ht(x) is the +1 valued weak hypothesis created at iteration t.   at is the weight or vote of ht in the master hypothesis, the a's may or may  not be normalized so that tt,=l oft, = 1.  t t   Ft(x) .t,=l(ott, ht,(x) , is the master hypothesis 2 at iter-  ation t.   Ui,t -- Yi tt,=l ott, ht,(x) is the margin of xi after iteration t; the t sub-  script is often omitted. Note that the margin is positive when the master  Zt':l t"  hypothesis is correct, and the normalized margin is Ui,t/ t   p(u) is the potential of an instance with margin u, and the total potential  is Zim=l p(ui).   P D[ ],P$[ ], and Es[] are the probability with respect to the unknown  distribution over the domain, and the probability aud expectations with  respect to the uniform distribution over the sample, respectively.  m U  Our results apply to total potential functions of the form Y-i=l P(i) where p is  positive and strictly decreasing.  3 Leveraging Learners by Gradient Descent  AdaBoost [4] has recently been interpreted as gradient descent independently by  several groups [6, 7, 8, 9, 10. Under this interpretation AdaBoost is seen as minimiz-  ing the total potential Y-i=l p(ui) = Y',im_-i exp(-ui) via feasible direction gradient  descent. On each iteration t + 1, AdaBoost chooses the direction of steepest descent  as the distribution on the sample, and calls the weak learner to obtain a new base  hypothesis ht+. The weight ct+l of this new weak hypothesis is calculated to min-  imize 3 the resulting potential '.im=l P(Ui,t+l ) -- im_-i exp(--(ui,t -{- ott+lYiht+l (xi) ) ).  This gradient descent idea has been generalized to other potential functions [6, 7,  10]. Duffy et al. [9] prove bounds for a similar gradient descent technique using a  non-componentwise, non-monotonic potential function.  Note that if the weak learner returns a good hypothesis ht (with training error at  m  most e < 1/2), then Y]-i=l Dt(xi)yiht(xi) ) I - 2e ) 0. We set r -- I - 2e, and  assume that each base hypothesis produced satisfies Zim__l Dt(xi)yiht(xi) _ r.  In this paper we consider this general gradient descent approach applied to various  potentials -]-i1 p(ui). Note that each potential function p has two corresponding  gradient descent algorithms (see [6]). The un-normalized algorithms (like AdaBoost)  continually add in new weak hypotheses while preserving the old a's. The normal-  ized algorithms re-scale the a's so that they always sum to 1. In general, we call  such algorithms "leveraging algorithms", reserving the term "boosting" for those  that actually have the PAC boosting property.  4 Potentials that Don't Boost  In this section we describe sufficient conditions on potential functions so that the  corresponding leveraging algorithm does not have the PAC boosting property. We  2The prediction of the master hypothesis on instance x is the sign of Ft(x).  SOur current proofs require that the actual ct's be no greater than a constant (say 1).  Therefore, this minimizing c may need to be reduced.  Potential Boosters? 261  apply these conditions to show that two potentials from the literature do not lead  to boosting algorithms.  Theorem 1 Let p(u) be a potential function for which:  1) the derivative, p'(u), is increasing (-p'(u) decreasing) in +, and  2) 3/ > 0 such that for all u  O, -/p'(u) _ -p'(-2u).  Then neither the normalized nor the un-normalized leveraging algorithms  sponding to potential p have the PA C boosting property.  co rre -  This theorem is proven by an adversary argument. Whenever the concept class is  sufficiently rich 4, the adversary can keep a constant fraction of the sample from  being correctly labeled by the master hypothesis. Thus as the error tolerance e goes  to zero, the master hypotheses will not be sufficiently accurate.  We now apply this theorem to two potential functions from the literature.  Friedman et al. [7] describe a potential they call "Squared Error(p)" where the  potentialatxiis (yi+ I er(x,) )2  2 eF(x) -]-e_F(x ) This potential can be re-written  1(  asP$1(ui)-- 1 + 2u u' +    e-U k eU  e-U  Corollary 1 Potential "Squared Error(p)" does not lead to a boosting algorithm.  Proof: This potential satisfies the conditions of Theorem 1. It is strictly decreas-  ing, and the second condition holds for/ = 2.  Mason et al. [10] examine a normalized algorithm using the potential pa(u) ----  I -- tanh (Au). Their algorithm optimizes over choices of A via cross-validation, and  uses weak learners with slightly different properties. However, we can plug this  potential directly into the gradient descent framework and examine the resulting  algorithms.  Corollary 2 The DOOMII potential PD does not lead to a boosting algorithm for  any .fixed A.  Proof: The potential is strictly decreasing, and the second condition of Theorem 1  holds for/ = 1.  Our techniques show that potentials that are sigmoidal in nature do not lead to  algorithms with the PAC boosting property. Since sigmoidal potentials are gen-  erally better over-estimates of the 0, 1 loss than the potential used by AdaBoost,  our results imply that boosting algorithms must use a potential with more subtle  properties than simply upper bounding the 0, 1 loss.  5 Potential Functions That Boost  In this section we give sufficient conditions on a potential function for it's corre-  sponding un-normalized algorithm to have the PAC boosting property. This result  implies that AdaBoost [4] and LogitBoost [7] have the PAC boosting property (Al-  though this was previously known for AdaBoost [4], we believe this is a new result  for LogitBoost).  4The VC-dimension 4 concept class consisting of pairs of intervals on the real line is  sufficient for our adversary.  262 N. DuJ] and D. Helmbold  One set of conditions on the potential imply that it decreases roughly exponen-  tially when the (un-normalized) margins are large. Once the margins are in this  exponential region, ideas similar to those used in AdaBoost's analysis show that the  minimum normalized margin quickly becomes bounded away from zero. This allows  us to bound the generalization error using a theorem from Bartlett et al. [15].  A second set of conditions governs the behavior of the potential function before  the un-normalized margins are large enough. These conditions imply that the total  potential decreases by a constant factor each iteration. Therefore, too much time  will not be spent before all the margins enter the exponential region.  t  The margin value bounding the exponential region is U, and once -]i=l p(ui) _  p(U), all margins p(ui) will remain in the exponential region. The following theorem  gives conditions on p ensuring that = p(ui) quickly becomes less than p(U).  Theorem 2 If the following conditions hold for p(u) and U:  1. -p'(u) is strictly decreasing-and 0  p"(u) _ B, and  2. 3q  0 such that p(u) _ -qp'(u) Vu  U,  then im__ p(ui) _ p(U) aer Ti _  4Bq2m2p(O)ln(mp(O) )  p(V)  p(U)2r 2  iterations.  The proof of this theorem approximates the new total potential by the old potential  minus ct times a linear term, plus an error. By bounding the error as a function of  ct and minimizing we demonstrate that some values of ct give a sufficient decrease  in the total potential.  Theorem 3 If the following conditions hold for p(u), U, q, and iteration Tx'  3/? _ v such that -p'(u -t- v) _ p(u -t- v) _ -p'(u)/?-Vq whenever -1 _  v _ l and u ) U,  2. im__x p(ui,T) _ p(U),  3. -p(u) is strictly decreasing, and  d. 3C > O, ff > 1 such that Cp(u) _ ff- Vu > U  then P s[yFT(x) _ O] _  nentially in T - T if 0   _70Txp(U) (frO q) (T-Tx)  -ln(qv-r 2)  ln(7)  which decreases expo-  The proof of this theorem is a generalization of the AdaBoost proof.  Combining these two theorems, and the generalization bound from Theorem 2 of  Bartlett et al. [15] gives the following result, where d is the VC dimension of the  weak hypothesis class.  Theorem 4 If for all edges 0  r  1/2 there exists Tx,r _ poly(m, l/r), Ur,  and q satisfying the conditions of Theorem 3 such that p(U) _ poly(r) and  qv/ - r  = l(r) ( 1 - poly(r), then in time poly(m, l/r) all examples have nor-  Potential Boosters? 263  malized margin at least 0 = In 2t(r) /ln(7) and  (1(  Pv[yFr(x) <_ 0] e O (ln(/(r)+ 1)-ln(21(r))) 2 +log(I/5)  Choosing m appropriately makes the error rate sufficiently small so that the algo-  rithm corresponding to p has the PA C boosting property.  We now apply Theorem 4 to show that the AdaBoost and LogitBoost potentials  lead to boosting algorithms.  6 Some Boosting Potentials  In this section we show as a direct consequence of our Theorem 4 that the potential  functions for AdaBoost and LogitBoost lead to boosting algorithms. Note that  the LogitBoost algorithm we analyze is not exactly the same as that described by  Friedman et al. [7], their "weak learner" optimizes a square loss which appears to  better fit the potential. First we re-derive the boosting property for AdaBoost.  Corollary 3 AdaBoost's [16] potential boosts.  Proof: To prove this we simply need to show that the potential p(u) = exp(-u)  satisfies the conditions of Theorem 4. This is done by setting U = - ln(m), q = 1,  3'-fi-e,C=l, andT =0.  Corollary 4 The log-likelihood potential (as used in LogitBoost [7]) boosts.  Proof: In this case p(u) = ln(1 + e -u) and -p'(u) = x+e-' We set 7 =/? = e,  C-2, U=-ln(X/1-e2/2-1) andq=l+exp(-Ur)= lv/-27 Now The-  x/T- e 2  orem 2 shows that after T < poly(m, I/r) iterations the conditions of Theorem 4  are satisfied.  7 Conclusions  In this paper we have examined leveraging weak learners using a gradient descent  approach [9]. This approach is a direct generalization of the Adaboost [4, 16] algo-  rithm, where Adaboost's exponential potential function is replaced by alternative  potentials. We demonstrated properties of potentials that are sufficient to show  that the resulting algorithms are PAC boosters, and other properties that imply  that the resulting algorithms are not PAC boosters. We applied these results to  several potential functions from the literature [7, 10, 16].  New insight can be gained from examining our criteria carefully. The conditions  that show boosting leave tremendous freedom in the choice of potential function  for values less than some U, perhaps this freedom can be used to choose potential  functions which do not overly concentrate on noisy examples.  There is still a significant gap between these two sets of properties, we are still a long  way from classifying arbitrary potential functions as to their boosting properties.  There are other classes of leveraging algorithms. One class looks at the distances  between successive distributions [17, 18]. Another class changes their potential  264 N. Duf and D. HelmboM  over time [6, 8, 12, 14]. The criteria for boosting may change significantly with  these different approaches. For example, Freund recently presented a boosting  algorithm [12] that uses a time-varying sigmoidal potential. It would be interesting  to adapt our techniques to such dynamic potentials.  References  [1] Robert E. Schapire. The Design and Analysis of Ecient Learning Algorithms. MIT  Press, 1992.  [2] Michael Kearns and Leslie Valiant. Cryptographic limitations on learning Boolean  formulae and finite automata. Journal of the ACM, 41(1):67-95, January 1994.  [3] L. G. Valiant. A theory of the learnable. Commun. ACM, 27(11):1134-1142, Novem-  ber 1984.  [4] Yoav Freund and Robert E. Schapire. A decision-theoretic generalization of on-line  learning and an application to boosting. Journal of Computer and System Sciences,  55(1):119-139, August 1997.  [5] Eric Bauer and Ron Kohavi. An empirical comparison of voting classification algo-  rithms: Bagging, boosting and variants. Machine Learning, 36(1-2):105-39, 1999.  [6] Leo Breiman. Arcing the edge. Technical Report 486, Department of Statistics,  University of California, Berkeley., 1997. available at www.stat.berkeley. edu.  [7] Jerome Friedman, Trevor Hastie, and Robert Tibshirani. Additive logistic regression:  a statistical view of boosting. Technical report, Stanford University, 1998.  [8] G. R/itsch, T. Onoda, and K.-R. Mfiller. Soft margins for AdaBoost. Machine Learn-  ing, 2000. To appear.  [9] Nigel Duffy and David P. Helmbold. A geometric approach to leveraging weak learn-  ers. In Paul Fischer and Hans Ulrich Simon, editors, Computational Learning Theory:  Jth European Conference (EuroCOLT '99), pages 18-33. Springer-Verlag, March 1999.  [10] Llew Mason, Jonathan Baxter, Peter Bartlett, and Marcus Frean. Boosting algorithms  as gradient descent. To appear in NIPS 2000.  [11] Thomas G. Dietterich. An experimental comparison of three methods for construct-  ing ensembles of decision trees: Bagging, Boosting, and Randomization. Machine  Learning. To appear.  [12] Yoav Freund. An adaptive version of the boost-by-majority algorithm. In Proc. 12th  Annu. Conf. on Cornput. Learning Theory, pages 102-113. ACM, 1999.  [13] David Haussler, Michael Kearns, Nick Littlestone, and Manfred K. Warmuth. Equiva-  lence of models for polynomial learnability. Information and Computation, 95(2):129-  161, December 1991.  [14] Y. Freund. Boosting a weak learning algorithm by majority. Information and Com-  putation, 121(2):256-285, September 1995.  [15] Robert E. Schapire, Yoav Freund, Peter Bartlett, and Wee Sun Lee. Boosting the  margin: A new explanation for the effectiveness of voting methods. The Annals of  Statistics, 26(5):1651-1686, 1998.  [16] Robert E. Schapire and Yoram Singer. Improved boosting algorithms using  confidence-rated predictions. Machine Learning, 37(3):297-336, December 1999.  [17] Jyrki Kivinen and Manfred K. Warmuth. Boosting as entropy projection. In Proc.  12th Annu. Conf. on Cornput. Learning Theory, pages 134-144. ACM, 1999.  [18] John Lafferty. Additive models, boosting, and inference for generalized divergences.  In Proc. 12th Annu. Conf. on Cornput. Learning Theory, pages 125-133. ACM.  
Support Vector Method for Multivariate  Density Estimation  Vladimir N. Vapnik  Royal Halloway College and  AT&T Labs, 100 Schultz Dr.  Red Bank, NJ 07701  vlad res earch. a tt. corn  Sayan Mukherjee  CBCL, MIT E25-201  Cambridge, MA 02142  sayanai. mit. edu  Abstract  A new method for multivariate density estimation is developed  based on the Support Vector Method (SVM) solution of inverse  ill-posed problems. The solution has the form of a mixture of den-  sities. This method with Gaussian kernels compared favorably to  both Parzen's method and the Gaussian Mixture Model method.  For synthetic data we achieve more accurate estimates for densities  of 2, 6, 12, and 40 dimensions.  I Introduction  The problem of multivariate density estimation is important for many applications,  in particular, for speech recognition [1] [7]. When the unknown density belongs to a  parametric set satisfying certain conditions one can estimate it using the maximum  likelihood /ML) method. Often these conditions are too restrictive. Therefore,  non-parametric methods were proposed.  The most popular of these, Parzen's method [5], uses the following estimate given  data X 1 , ..., X:  1  p(t, = xd, (1)  i----1  where K (t - xi) is a smooth function such that f K (t - x)dt = 1. Under some  conditions on ff and K (t- x), Parzen's method converges with a fast asymptotic  rate. An example of such a function is a Gaussian with one free parameter 72 (the  width)  i --1  K(t - xi) -- (27r)n/2ff exp {-(t - xi) T (7I) (t - xi)}. (2)  The structure of the Parzen estimator is too complex: the number of terms in (1)  is equal to the number of observations (which can be hundreds of thousands).  660 V. N. Vapnik and S. Mukherjee  Researchers believe that for practical problems densities can be approximated by  a mixture with few elements (Gaussians for Gaussian Mixture Models (GMM)).  Therefore, the following parametric density model was introduced  m m  P(x, a, E) = Z aiP(x, ai, Ei), a _> 0, Z ai = 1, (3)  i----1 i=1  where P(x, ai, Zi) are Gaussians with different vectors ai and different diagonal  covariance matrices Zi; ai is the proportion of the i-th Gaussian in the mixture.  It is known [9] that for general forms of Gaussian mixtures the ML estimate does not  exist. To use the ML method two values are specified: a lower bound on diagonal  elements of the covariance matrix and an upper bound on the number of mixture  elements. Under these constraints one can estimate the mixture parameters using  the EM algorithm. This solution, however, is based on predefined parameters.  In this article we use an $VM approach to obtain an estimate in the form of a  mixture of densities. The approach has no free parameters. In our experiments it  performs better than the GMM method.  2 Density estimation is an ill-posed problem  A density p(t) is defined as the solution of the equation  t) dt - F(x), (4)  where F(x) is the probability distribution function. Estimating a density from data  involves solving equation (4) on a given set of densities when the distribution func-  tion F(x) is unknown but a random i.i.d. sample x, ...,xt is given. The empirical  distribution function Ft (x) is a good approximation of the actual distribution,  1   i----1  where O(u) is the step-function. In the univariate case, for sufficiently large  the distribution of the supremum error between F(x) and Ft(x) is given by the  Kolmogorov-Smirnov distribution  P{sup IF(x)- F(x)l < e/x/} = 1- 2 Z(-1) k-1 exp{-2e2k2}.  x k----1  (5)  Hence, the problem of density estimation can be restated as solving equation (4)  but replacing the distribution function F(x) with the empirical distribution function  Ft(x) which converges to the true one with the (fast) rate O(5 )' for univariate and  multivariate cases.  The problem of solving the linear operator equation Ap = F with approximation  Ft(x) is ill-posed.  In the 1960's methods were proposed for solving ill-posed problems using approx-  imations Ft converging to F as i increases. The idea of these methods was to  Support Vector Method for Multivariate Density Estimation 661  introduce a regularizing functional Ft(p) (a semi-continuous, positive functional for  which Ft(p) <_ c, c > 0 is a compactum) and define the solution pt which is a  trade-off between ft(p) and IIAp- Ftll.  The following two methods which are asymptotically equivalent [11] were proposed  by Tikhonov [8] and Phillips [6]  min [11Ap- Ftll= + t(P)], t > 0, t  0, (6)  p  min Ft(p) s.t. IIAP- Ftll< et, et > 0, et  0. (7)  p  For the stochastic case it can be shown for both methods that if Ft (x) converges in  probability to F(x) and "Yt - 0 then for sufficiently large  and arbitrary  and/  the following inequality holds [10] [9] [3]  P(PE(P, Pt) > ) <_ P(PE2(F, Ft) >  (8)  where t > t0(v,u) and PE(P, Pt), pE2(F, Ft) are metrics in the spaces p and F.  1  Since Ft(x) - F(x) in probability with the rate O(), from equation (8) it follows  that if "Yt > O() the solutions of equation (4) are consistent.  3 Choice of regularization parameters  For the deterministic case the residual method [2] can be used to set the regular-  ization parameters (fit in (6) and st in (7)) by setting the parameter (fit or st) such  that Pt satisfies the following  IIApt -Ftll = IIF(x) - Ft(x)11--  (9)  where at is the known accuracy of approximation of F(x) by Ft (x). We use this idea  for the stochastic case. The Kolmogorov-Smirnov distribution is used to set at, at =  c/x/, where c corresponds to an appropriate quantile. For the multivariate case  one can either evaluate the appropriate quantile analytically [4] or by simulations.  The density estimation problem can be solved using either regularization method  (6) or (7). Using method (6) with a L2 norm in image space F and regularization  functional Ft(p) = (Tp, Tp) where 7- is a convolution operator, one obtains Parzen's  method [10] [9] with kernels defined by operator 7-.  4 SVM for density estimation  We apply the SVM technique to equation (7) for density estimation. We use the C  norm in (7) and solve equation (4) in a set of functions belonging to a Reproducing  Kernel Hilbert Space (RKHS). We use the regularization functional  ft(p) -IIpllt - (p,p)t. (lO)  A RKHS can be defined by a positive definite kernel K(x, y) and an inner product  (f, g)t in Hilbert space 7-/such that  (f(x),K(x,y)) = f(y) Vf e 7. (11)  662 V. N. Vapnik and S. Mukherjee  Note that any positive definite function K(x, y) has an expansion  K(x,y) --  Aiqbi(x)qbi(y) (12)  i----1  where Ai and (hi(x) are eigenvalues and eigenfunctions of K(x, y). Consider the set  of functions  oo  and the inner product  f(x,c) = Z cici(x)  i----1  (13)  max  i  subject to the constraint  Ft(x) - p(t)dt  Xri  We look for a solution of equation (4) with the form  p(t)- Z/iK(xi,t).  Accounting for (16) and (11) minimizing (10) is equivalent to minimizing    ft(p,p) -- (P,P)t --  /i/jK(xi,xj)  i,j-----1  (16)  (17)  (f(x,c*) f(x,c**) =  cici (14)  '  i----1  Kernel (12), inner product (14), and set (13) define a RKHS and  (f(x),K(x,y))t=(ci(bi(x),K(x,y)) =  i=1  = =  ki=l i=1   For functions from a RKHS the functional (10) has the form   2  =  where Ai is the i-th eigenvalue of the kernel K(x, y). The choice of the kernel defines  smoothness properties on the solution.  To use method (7) to solve for the density in equation (4) in a RKHS with a solution  satisfying condition (9) we minimize  (p) = (p,p)  Support Vector Method for Multivariate Density Estimation 663  subject to constraints  max  i  =at, (18)  /i _ 0, Z/i = 1. (19)  i----1  This optimization problem is closely related to the SV regression problem with an  at-insensitive zone [9]. It can be solved using the standard SVM technique.  Generally, only a few of the/i will be nonzero, the xi corresponding to these/i are  called support vectors.  Note that kernel (2) has width parameter fit. We call the value of this parameter  admissible if it satisfies constraint (18) (the solution satisfies condition (9)). The  admissible set "min __ " __ "ma:r is not empty since for Parzen's method (which  also has form (16)) such a value does exist. Among the fit in this admissible set  we select the one for which (p) is smallest or the number of support vectors is  minimum.  Choosing other kernels (for example Laplacians) one can estimate densities using  non-Gaussian mixture models which for some problems are more appropriate [1].  5 Experiments  Several trials of estimates constructed from sampling distributions were examined.  Boxplots were made of the Li(p) norm over the trials. The horizontal lines of the  boxplot indicate the 5%, 25%, 50%, 75%, and 95% quantiles of the error distribution.  For the SVM method we set at = c//, where c = .36, .41, .936, and 1.75 for  two, six, twelve and forty dimensions. For Parzen's method t was selected using  a leave-one out procedure. The GMM method uses the EM algorithm and sets all  parameters except n, the upper bound on the number of terms in the mixture [7].  Figure (1) shows plots of the SVM estimate using a Gaussian kernel and the GMM  estimate when 60 points were drawn form a mixture of a Gaussian and Laplacian  in two dimensions.  Figure (2a) shows four boxplots of estimating a density defined by a mixture of  two Laplacians in a two dimensional space using 200 observations. Each boxplot  shows outcomes of 100 trials: for the SVM method, Parzen's method, and the GMM  method with parameters n = 2, and n = 4. Figure (2c) shows the distribution of  the number of terms for the SVM method.  Figure (2b) shows boxplots of estimating a density defined by the mixture of four  Gaussians in a six dimensional space using 600 observations. Each boxplot shows  outcomes of 50 trials: for the SVM method, Parzen's method, and the GMM method  with parameters n - 4, and n -- 8. Figure (2c) shows the distribution of the number  of terms for the SVM method.  Figure (3a) shows boxplots of outcomes of estimating a density defined by the  mixture of four Gaussians and four Laplacians in a twelve dimensional space using  664 V. N. Vapnik and S. Mukherjee  400 observations. Each boxplot shows outcomes of. 50 trials: for the SVM method,  Parzen's method, and the GMM method with parameter n = 8. Figure (3c) shows  the distribution of the number of terms for the SVM method.  Figure (3b) shows boxplots of outcomes of estimating a density defined by the  mixture of four Gaussians and four Laplacians in a forty dimensional space using  480 observations. Each box-plot shows outcomes of 50 trials: for the SVM method,  Parzen's method, and the GMM method with parameter n - 8. Figure (3c) shows  the distribution of the number of terms for the SVM method.  6 Summary  A method for multivariate density estimation based on the SVM technique for  solving ill-posed problems is introduced. This method has a form of a mixture of  densities. The estimate in general has only a few terms. In experiments on synthetic  data this method is more accurate than the GMM method.  References  [1]  S. Basu and C.A. Micchelli. Parametric density estimation for the classification of  acoustic feature vectors in speech recognition. In Nonlinear Modeling, Advanced  Black-Box Techniques. Kluwer Publishers, 1998.  [2] V.A. Morozov. Methods for solving incorrectly posed problems. Springer-Verlag,  Berlin, 1984.  [3] S. Mukherjee and V. Vapnik. Multivariate density estimation: An svm approach. AI  Memo 1653, Massachusetts Institute of Technology, 1999.  [4] S. Paramasamy. On multivariate kolmogorov-smirnov distribution. Statistics 4 Prob-  ability Letters, 15:140-155, 1992.  [5] E. Parzen. On estimation of a probability density function and mode. Ann. Math.  Statis., 33:1065-1076, 1962.  [6] D.L. Phillips. A technique for the numerical solution of integral equations of the first  kind. J. Assoc. Cornput. Machinery, 9:84-97, 1962.  [7]  D. Reynolds and R. Rose. Robust text-independent speaker identification using gaus-  sian mixture speaker models. IEEE Trans on Speech and Audio Processing, 3(1):1-27,  1995.  [8] A. N. Tikhonov. Solution of incorrectly formulated problems and the regularization  method. Soviet Math. Dokl., 4:1035-1038, 1963.  [9] V. N. Vapnik. Statistical learning theory. J. Wiley, 1998.  [10] V.N. Vapnik and A.R. Stefanyuk. Nonparametric methods for restoring probability  densities. Avtomatika i Telemekhanika, (8):38-52, 1978.  [11] V.V. Vasin. Relationship of several variational methods for the approximate solution  of ill-posed problems. Math Notes, 7:161-166, 1970.  Support Vector Method for Multivariate Density Estimation 665  (a) (b) (c)  Figure 1: (a) The true distribution (b) the GMM case with 4 mixtures (c) the  Parzen case (d) the SVM case for 60 points.  0.16  0.14  0.12   0.1  .o.o  0.0  0.04  0.02  x 10 -a  1.9 f  1.?I  1.6  1.5  $VM  SVM Parzen GMM 2 GMM 4 Parzen GMM 4 GMM 8 2 rens,on. e d,rnenons  (b)  Figure 2: (a) Boxplots of the Li(p) error for the mixture of two Laplacians in two  dimensions for the SVM method, Parzen's method, and the GMM method with 2  and 4 Gaussians. (b) Boxplots of the Li(p) error for mixture of four Gaussians  in six dimensions with the $VM method, Parzen's method, and the GMM method  with 4 mixtures. (c) Boxplots of distribution of the number of terms for the SVM  method for the two and six dimensional cases.  0.16  0.14  0.12  0.1  0.04  0.12  011  0.08  .q o.o8  0.04 /  0.02  SVM Parzen GMM 8 SVM Paten GMM 8 12 Dim 40 Dim  (b)  Figure 3: (a) Boxplots of the Li(p) error for the mixture of four Laplacians and  four Gaussians in twelve dimensions for the SVM method, Parzen's method, and the  GMM method with 8 Gaussians. (b) Boxplots of the Li(p) error for the mixture  of four Laplacians and four Gaussians in forty dimensions for the SVM method,  Parzen's method, and the GMM method with 8 Gaussians. (c) Boxplots of dis-  tribution of the number of terms for the SVM method for the twelve and forty  dimensional cases.  
Learning Factored Representations for Partially  Observable Markov Decision Processes  Brian Sallans  Department of Computer Science  University of Toronto  Toronto M5S 2Z9 Canada  Gatsby Computational Neuroscience Unit*  University College London  London WC1N 3AR U.K.  sallans @ cs. toronto. edu  Abstract  The problem of reinforcement learning in a non-Markov environment is  explored using a dynamic Bayesian network, where conditional indepen-  dence assumptions between random variables are compactly represented  by network parameters. The parameters are learned on-line, and approx-  imations are used to perform inference and to compute the optimal value  function. The relative effects of inference and value function approxi-  mations on the quality of the final policy are investigated, by learning to  solve a moderately difficult driving task. The two value function approx-  imations, linear and quadratic, were found to perform similarly, but the  quadratic model was more sensitive to initialization. Both performed be-  low the level of human performance on the task. The dynamic Bayesian  network performed comparably to a model using a localist hidden state  representation, while requiring exponentially fewer parameters.  1 Introduction  Reinforcement learning (RL) addresses the problem of learning to act so as to maximize  a reward signal provided by the environment. Online RL algorithms try to find a policy  which maximizes the expected time-discounted reward. They do this through experience  by performing sample backups to learn a value function over states or state-action pairs.  If the decision problem is Markov in the observable states, then the optimal value function  over state-action pairs yields all of the information required to find the optimal policy for  the decision problem. When complete knowledge of the environment is not available, states  which are different may look the same; this uncertainty is called perceptual aliasing [1],  and causes decision problems to have dynamics which are non-Markov in the perceived  state.  * Correspondence address  Learning Factored Representations for POMDPs 1051  1.1 Partially observable Markov decision processes  Many interesting decision problems are not Markov in the inputs. A partially observable  Markov decision process (POMDP) is a formalism in which it is assumed that a process is  Markov, but with respect to some unobserved (i.e. "hidden") random variable. The state of  the variable at time t, denoted s t, is dependent only on the state at the previous time step and  on the action performed. The currently-observed evidence is assumed to be independent of  previous states and observations given the current state.  The state of the hidden variable is not known with certainty, so a belief state is maintained  instead. At each time step, the beliefs are updated by using Bayes' theorem to combine the  belief state at the previous time step (passed through a model of the system dynamics) with  newly observed evidence. In the case of discrete time and finite discrete state and actions, a  POMDP is typically represented by conditional probability tables (CPTs) specifying emis-  sion probabilities for each state, and transition probabilities and expected rewards for states  and actions. This corresponds to a hidden Markov model (HMM) with a distinct transition  matrix for each action. The hidden state is represented by a single random variable that can  take on one of K values. Exact belief updates can be computed using Bayes' rule.  The value function is not over the discrete state, but over the real-valued belief state. It has  been shown that the value function is piecewise linear and convex [2]. In the worst case,  the number of linear pieces grows exponentially with the problem horizon, making exact  computation of the optimal value function intractable.  Notice that the localist representation, in which the state is encoded in a single random  variable, is exponentially inefficient: Encoding r bits of information about the state of the  process requires 2 n possible hidden states. This does not bode well for the abilities of  models which use this representation to scale up to problems with high-dimensional inputs  and complex non-Markov structure.  1.2 Factored representations  A Bayesian network can compactly represent the state of the system in a set of random  variables [3]. A two time-slice dynamic Bayesian network (DBN) represents the system at  two time steps [4]. The conditional dependencies between random variables from time t to  time t + 1, and within time step t, are represented by edges in a directed acyclic graph. The  conditional probabilities can be stored explicitly, or parameterized by weights on edges in  the graph.  If the network is densely-connected then inference is intractable [5]. Approximate infer-  ence methods include Markov chain Monte Carlo [6], variational methods [7], and belief  state simplification [8].  In applying a DBN to a large problem there are three distinct issues to disentangle: How  well does a parameterized DBN capture the underlying POMDP; how much is the DBN  hurt by approximate inference; and how good must the approximation of the value function  be to achieve reasonable performance? We try to tease these issues apart by looking at the  performance of a DBN on a problem with a moderately large state-space and non-Markov  structure.  2 The algorithm  We use a fully-connected dynamic sigmoid belief network (DSBN) [9], with K units at  each time slice (see figure 1). The random variables si are binary, and conditional proba-  1052 B. Sallans  time t t+l  Figure 1: Architecture of the  dynamic sigmoid belief network.  Circles indicate random variables,  where a filled circle is observed  and an empty circle is unobserved.  Squares are action nodes, and dia-  monds are rewards.  bilities relating variables at adjacent time-steps are encoded in action-specific weights:  k=l  a t  where wik is the weight from the i th unit at time step t to the k th unit at time step t + 1,  assuming action a t is taken at time t. The nonlinearity is the usual sigmoid function:  a(z) = 1/1 + exp{-z}. Note that a bias can be incorporated into the weights by clamping  one of the binary units to 1.  The observed variables are assumed to be discrete; the conditional distribution of an output  given the hidden state is multinomial and parameterized by output weights. The probability  of observing an output with value I is given by:  P(o t l[{s K exp {rkK__l  = = (2)  __1  exp {  where o t E 0 and ukl denotes the output weight from hidden unit k to output value I.  2.1 Approximate inference  Inference in the fully-connected Bayesian network is intractable. Instead we use a varia-  tional method with a fully-factored approximating distribution:  K  P(stls*-X,a*-,o*)  Ps'  1-I/(1-/) 1-L (3)  k=l  where the/,t are variational parameters to be optimized. This is the standard mean-field  approximation for a sigmoid belief network [ 10]. The parameters/,t are optimized by iterat-  ing the mean-field equations, and converge in a few iterations. The values of the variational  parameters at time t are held fixed while computing the values for step t + 1. This is  analogous to running only the forward portion of the HMM forward-backward algorithm  [11].  The parameters of the DSBN are optimized online using stochastic gradient ascent in the  2 exp] }  log-likelihood:  (4)  Learning Factored Representations for POMDPs 1053  where W and U are the transition and emission matrices respectively, aw and au are  learning rates, the vector/.t contains the fully-factored approximate belief state, and v is a  vector of zeros with a one in the o tth place. The notation [']k denotes the k th element of a  vector (or k  column of a matrix).  2.2 Approximating the value function  Computing the optimal value function is also intractable. If a factored state-space represen-  tation is appropriate, it is natural (if extreme) to assume that the state-action value function  can be decomposed in the same way:  K  q(vst,a t) qF(u, *)  k=l  This simplifying assumption is still not enough to make finding the optimal value func-  tion tractable. Even if the states were completely independent, each Qk would still be  piecewise-linear and convex, with the number of pieces scaling exponentially with the hori-  zon. We test two approximate value functions, a linear approximation:  K  and a quadratic approximation:  K  at) = E q3k,at tX + qk,t k + ba t  = + [q]t +  (6)  (7)  Where (I,, Q and b are parameters of the approximations. The notation [.]i denotes the  i t column of a matrix, [-]- denotes matrix transpose and  denotes element-wise vector  multiplication.  We update each term of the factored approximation with a modified Q-learning rule [12],  which corresponds to a delta-rule where the target for input/x is r t + ? maxa Qv(lu t+x , a)'  qk,a t 3-- qk,a  + O k EB  ba t t- bat + a EB  (8)  Here oz is a learning rate, ? is the temporal discount factor, and EB is the Bellman residual:  EB = r t + ?maxQr(/xt+, a) - Q'(txt, a t) (9)  a  3 Experimental results  The "New York Driving" task [ 13] involves navigating through slower and faster one-way  traffic on a multi-lane highway. The speed of the agent is fixed, and it must change lanes to  avoid slower cars and move out of the way of faster cars. If the agent remains in front of a  faster car, the driver of the fast car will honk its horn, resulting in a reward of -1.0. Instead  of colliding with a slower car, the agent can squeeze past in the same lane, resulting in a  reward of -10.0. A time step with no horns or lane-squeezes constitutes clear progress,  and is rewarded with +0.1. See [13] for a detailed description of this task.  1054 B. Sallans  Table 1: Sensory input for the New York driving task  Dimension ]Size[Values  Hear horn 2 yes, no  Gaze object 3 truck, shoulder, road  Gaze speed 2 looming, receding  Gaze distance 3 far, near, nose  Gaze refined distance 2 far-half, near-half  Gaze colour 6 red, blue, yellow, white, gray, tan  A modified version of the New York Driving task was used to test our algorithm. The  task was essentially the same as described in [13], except that the "gaze side" and "gaze  direction" inputs were removed. See table 1 for a list of the modified sensory inputs.  The performance of a number of algorithms and approximations were measured on the task:  a random policy; Q-learning on the sensory inputs; a model with a localist representation  (i.e. the hidden state consisted of a single multinomial random variable) with linear and  quadratic approximate value functions; the DSBN with mean-field inference and linear and  quadratic approximations; and a human driver. The localist representation used the linear  Q-learning approximation of [14], and the corresponding quadratic approximation. The  quadratic approximations were trained both from random initialization, and from initial-  ization with the corresponding learned linear models (and random quadratic portion). The  non-human algorithms were each trained for 100000 iterations, and in each case a constant  learning rate of 0.01 and temporal decay rate of 0.9 were used. The human driver (the au-  thor) was trained for 1000 iterations using a simple character-based graphical display, with  each iteration lasting 0.5 seconds.  Stochastic policies were used for all RL algorithms, with actions being chosen from a  Boltzmann distribution with temperature decreasing over time:  1  P(atlt) - Zs exp{qv(t'at)/T (10)  The DSBN had 4 hidden units per time slice, and the localist model used a multinomial  with 16 states. The Q-learner had a table representation with 2160 entries. After training,  each non-human algorithm was tested for 20 trials of 5000 time steps each. The human was  tested for 2000 time steps, and the results were renormalized for comparison with the other  methods. The results are shown in figure 2. All results were negative, so lower numbers  indicate better performance in the graph. The error bars show one standard deviation across  the 20 trials.  There was little performance difference between the localist representation and the DSBN  but, as expected, the DSBN was exponentially more efficient in its hidden-state represen-  tation. The linear and quadratic approximations performed comparably, but well below  human performance. However, the DSBN with quadratic approximation was more sensi-  tive to initialization. When initialized with random parameter settings, it failed to find a  good policy. However, it did converge to a reasonable policy when the linear portion of the  quadratic model was initialized with a previously learned linear model.  The hidden units in the DSBN encode useful features of the input, such as whether a car  was at the "near" or "nose" position. They also encode some history, such as current gaze  direction. This has advantages over a simple stochastic policy learned via Q-learning: If the  Q-learner knows that there is an oncoming car, it can randomly select to look left or right.  The DSBN systematically looks to the left, and then to the right, wasting fewer actions.  Learning Factored Representations for POMDPs 1055  4000  3500  3000  - 2500  2000  1500  1000  500  0  QCL  Algorithm  QDR  Figure 2: Results on the New York  Driving task for nine algorithms:  R=random; Q=Q-learning; LC=linear  multinomial; QCR=quadratic multi-  nomial, random init.; QCL=quadratic  multinomial, linear init; LD=linear  DSBN; QDR=quadratic DSBN, ran-  dom init.; QDL=quadratic DSBN,  linear init.; H=human  H  4 Discussion  The DSBN performed better than a standard Q-learner, and comparably to a model with  a localist representation, despite using approximate inference and exponentially fewer pa-  rmeters. This is encouraging, since an efficient encoding of the state is a prerequisite  for tackling larger decision problems. Less encouraging was the value-function approxi-  mation: When compared to human performance, it is clear that all methods are far from  optimal, although again the factored approximation of the DSBN did not hurt performance  relative to the localist multinomial representation. The sensitivity to initialization of the  quadratic approximation is worrisome, but the success of initializing from a simpler model  suggests that staged learning may be appropriate, where simple models are learned and  used to initialize more complex models. These findings echo those of [ 14] in the context of  learning a non-factored approximate value function.  There are a number of related works, both in the fields of reinforcement learning and  Bayesian networks. We use the sigmoid belief network mean-field approximation given  in [10], and discussed in the context of time-series models (the "fully factored" approxi-  mation) in [ 15]. Approximate inference in dynamic Bayesian networks has been discussed  in [15] and [8]. The additive factored value function was used in the context of factored  MDPs (with no hidden state) in [16], and the linear Q-learning approximation was given  in [ 14]. Approximate inference was combined with more sophisticated value function ap-  proximation in [17]. To our knowledge, this is the first attempt to explore the practicality  of combining all of these techniques in order to solve a single problem.  There are several possible extensions. As described above, the representation learned by  the DSBN is not tuned to the task at hand. The reinforcement information could be used  to guide the learning of the DSBN parameters[18, 13]. Also, if this were done, then the  reinforcement signals would provide additional evidence as to what state the POMDP is in,  and could be used to aid inference. More sophisticated function approximation could be  used [ 17]. Finally, although this method appears to work in practice, there is no guarantee  that the reinforcement learning will converge. We view this work as an encouraging first  step, with much further study required.  5 Conclusions  We have shown that a dynamic Bayesian network can be used to construct a compact rep-  resentation useful for solving a decision problem with hidden state. The parameters of the  DBN can be learned from experience. Learning occurs despite the use of simple value-  1056 B. Sallans  function approximations and mean-field inference. Approximations of the value function  result in good performance, but are clearly far from optimal. The fully-factored assump-  tions made for the belief state and the value function do not appear to impact performance,  as compared to the non-factored model. The algorithm as presented runs entirely on-line  by performing "forward" inference only. There is much room for future work, including  improving the utility of the factored representation learned, and the quality of approximate  inference and the value function approximation.  Acknowledgments  We thank Geoffrey Hinton, Zoubin Ghahramani and Andy Brown for helpful discussions,  the anonymous referees for valuable comments and criticism, and particularly Peter Dayan  for helpful discussions and comments on an early draft of this paper. This research was  funded by NSERC Canada and the Gatsby Charitable Foundation.  References  [11]  [12]  [13]  [14]  [15]  [16]  [17]  [18]  [1] S.D. Whitehead and D.H. Ballard. Learning to perceive and act by trial and error. Machine  Learning, 7, 1991.  [2] E.J. Sondik. The optimal control of partially observable Markov processes over the infinite  horizon: Discounted costs. Operations Research, 26:282-304, 1973.  [3] J. Pearl. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Mor-  gan Kaufmann, San Mateo, CA, 1988.  [4] T Dean and K. Kanazawa. A model for reasoning about persistence and causation. Computa-  tional Intelligence, 5, 1989.  [5] Gregory F. Cooper. The computational complexity of probabilistic inference using Bayesian  belief networks. Artificial Intelligence, 42:393-405, 1990.  [6] R.M. Neal. Probabilistic inference using Markov chain Monte Carlo methods. Technical Report  CRG-TR-93-1, Department of Computer Science, University of Toronto, 1993.  [7] M.I. Jordan, Z. Ghahramani, T.S. laakkola, and L.K. Saul. An introduction to variational meth-  ods for graphical models. Machine Learning, 1999. in press.  [8] X. Boyen and D. Koller. Tractable inference for complex stochastic processes. In Proc. UAI'98,  1998.  [9] R.M. Neal. Connectionist learning of belief networks. Artificial Intelligence, 56:71-113, 1992.  [10] L. K. Saul, T Jaakkola, and M. I. Jordan. Mean field theory for sigmoid belief networks.  Journal of Artificial Intelligence Research, 4:61-76, 1996.  Lawrence R. Rabiner and Biing-Hwang Juang. An introduction to hidden Markov models.  IEEEASSAP Magazine, 3:4-16, January 1986.  C.J.C.H. Watkins and P. Dayan. Q-learning. Machine Learning, 8:279-292, 1992.  A.K. McCallurn. Reinforcement learning with selective perception and hidden state. Dept. of  Computer Science, Universiy of Rochester, Rochester NY, 1995. Ph.D. thesis.  M.L. Littman, A.R. Cassandra, and L.P. Kaelbling. Learning policies for partially observable  environments: Scaling up. In Proc. International Conference on Machine Learning, 1995.  Z. Ghahramani and M. I. Jordan. Factorial hidden Markov models. Machine Learning, 1997.  D. Koller and R. Parr. Computing factored value functions for policies in structured MDPs. In  Proc. IJCAI'99, 1999.  A. Rodriguez, R. Parr, and D. Koller. Reinforcement learning using approximate belief states. In  S. A. Solla, T. K. Leen, and K.-R. Mtiller, editors, Advances in Neural Information Processing  Systems, volume 12. The MIT Press, Cambridge, 2000.  L. Chrisman. Reinforcement learning with perceptual aliasing: The perceptual distinctions  approach. In Tenth National Conference on AI, 1992.  
A MCMC approach to Hierarchical Mixture  Modelling  Christopher K. I. Williams  Institute for Adaptive and Neural Computation  Division of Informatics, University of Edinburgh  5 Forrest Hill, Edinburgh EH 1 2QL, Scotland, UK  ckiw@dai. ed.ac.uk http: //anc. ed. ac .uk  Abstract  There are many hierarchical clustering algorithms available, but these  lack a firm statistical basis. Here we set up a hierarchical probabilistic  mixture model, where data is generated in a hierarchical tree-structured  manner. Markov chain Monte Carlo (MCMC) methods are demonstrated  which can be used to sample from the posterior distribution over trees  containing variable numbers of hidden units.  1 Introduction  Over the past decade or two mixture models have become a popular approach to clustering  or competitive learning problems. They have the advantage of having a well-defined ob-  jective function and fit in with the general trend of viewing neural network problems in a  statistical framework. However, one disadvantage is that they produce a "flat" cluster struc-  ture rather than the hierarchical tree structure that is returned by some clustering algorithms  such as the agglomerative single-link method (see e.g. [12]). In this paper I formulate a  hierarchical mixture model, which retains the advantages of the statistical framework, but  also features a tree-structured hierarchy.  The basic idea is illustrated in Figure 1 (a). At the root of the tree (level 1) we have a single  centre (marked with a x). This is the mean of a Gaussian with large variance (represented  by the large circle). A random number of centres (in this case 3) are sampled from the level  1 Gaussian, to produce 3 new centres (marked with o's). The variance associated with the  level 2 Gaussians is smaller. A number of level 3 units are produced and associated with  the level 2 Gaussians. The centre of each level 3 unit (marked with a +) is sampled from  its parent Gaussian. This hierarchical process could be continued indefinitely, but in this  example we generate data from the level 3 Gaussians, as shown by the dots in Figure l(a).  A three-level version of this model would be a standard mixture model with a Gaussian  prior on where the centres are located. In the four-level model the third level centres are  clumped together around the second level means, and it is this that distinguishes the model  from a flat mixture model. Another view of the generative process is given in Figure l(b),  where the tree structure denotes which nodes are children of particular parents. Note also  that this is a directed acyclic graph, with the arrows denoting dependence of the position of  the child on that of the parent.  A MCMC Approach to Hierarchical Mixture Modelling 681  In section 2 we describe the theory of probabilistic hierarchical clustering and give a dis-  cussion of related work. Experimental results are described in section 3.  (a)  (b)  Figure 1: The basic idea of the hierarchical mixture model. (a) x denotes the root of the  tree, the second level centres are denoted by o's and the third level centres by +'s. Data is  generated from the third level centres by sampling random points from Gaussians whose  means are the third level centres. (b) The corresponding tree structure.  2 Theory  We describe in turn (i) the prior over trees, (ii) the calculation of the likelihood given a  data vector, (iii) Markov chain Monte Carlo (MCMC) methods for the inference of the tree  structure given data and (iv) related work.  2.1 Prior over trees  We describe first the prior over the number of units in each layer, and then the prior on  connections between layers. Consider a L layer hierarchical model. The root node is in  level 1, there are n2 nodes in level 2, and so on down to n nodes on level L. These  n's are collected together in the vector n. We use a Markovian model for P(n), so that  P(n) = P(n)P(n2ln)...P(nLlnL_) with P(n) = 6(n, 1). Currently these are  taken to be Poisson distributions offset by 1, so that P(rti+ Irti) ,--, Po(,Xini) + 1, where  ,Xi is a parameter associated with level i. The offset is used so that there must always be at  least one unit in any layer.  Given n, we next consider how the tree is formed. The tree structure describes which node  in the ith layer is the parent of each node in the (i + 1)th layer, for i - 1,... , L - 1. Each  unit has an indicator vector which stores the index of the parent to which it is attached. We  collect all these indicator vectors together into a matrix, denoted Z(n). The probability of  a node in layer (i + 1) connecting to any node in layer i is taken to be 1/rt i. Thus  P(n, Z(n)):  L--1  II  i=1  We now describe the generation of a random tree given n and Z(n). For simplicity we  describe the generation of points in 1-d below, although everything can be extended to  arbitrary dimension very easily. The mean/ of the level 1 Gaussian is at the origin I. The  lit is easy to relax this assumption so that ttt has a prior Gaussian distribution, or is located at  some point other than the origin.  682 C. K. I. Williams  level 2 means tt, j = 1, are generated from .A/'(tt 1 , cry), where cr 2 is the variance   . /g 2  associated with the level i node. Similarly, the position of each level 3 node is generated  from its level 2 parent as a displacement from the position of the level 2 parent. This  displacement is a Gaussian RV with zero mean and variance cr22. This process continues  on down to the visible variables. In order for this model to be useful, we require that  cr 2 > cr > ... > cr_l, i.e. that the variability introduced at successive levels declines  monotonically (cf scaling of wavelet coefficients).  2.2 Calculation of the likelihood  The data that we observe are the positions of the points in the final layer; this is denoted  x. To calculate the likelihood of x under this model, we need to integrate out the locations  of the means of the hidden variables in levels 2 through to L - 1. This can be done expli-  citly, however, we can shorten this calculation by realizing that given Z(n), the generative  distribution for the observables x is Gaussian A/'(0, C). The covariance matrix C can be  calculated as follows. Consider two leaf nodes indexed by k and 1. The Gaussian RVs that  generated the position of these two leaves can be denoted   .,  (L-) (;-)  xk = wk +w. +... +.% , xt = w +w +... +w  To calculate the covariance between xk and xt, we simply calculate {xkxt}. This depends  crucially on how many of the w's are shared between nodes k and 1 (cf path analysis). For  example, if w}  w], i.e. the nodes lie in different branches of the tree at level l, their  covariance is zero. If k = l, the variance is just the sum of the variances of each RV in the  tree. In between, the covariance of xk and xt can be determined by finding at what level in  the tree their common parent occurs.  Under these assumptions, the log likelihood L of x given Z(n) is  lxTC_x_ 1 nL log2r. (1)  L = log Ic[ - 2  In fact this calculation can be speeded up by taking account of the tree structure (see e.g.  [8]). Note also that the posterior means (and variances) of the hidden variables can be  calculated based on the covariances between the hidden and visible nodes. Again, this  calculation can be carried out more efficiently; see Pearl [1 1] (section 7.2) for details.  2.3 Inference for n and Z(n)  Given n we have the problem of trying to infer the connectivity structure Z given the  observations x. Of course what we are interested in is the posterior distribution over Z,  i.e. P(ZIx, n). One approach is to use a Markov chain Monte Carlo (MCMC) method  to sample from this posterior distribution. A straightforward way to do this is to use the  Metropolis algorithm, where we propose changes in the structure by changing the parent of  a single node at a time. Note the similarities of this algorithm to the work of Williams and  Adams [14] on Dynamic Trees (DTs); the main differences are (i) that disconnections are  not allowed, i.e. we maintain a single tree (rather than a forest), and (ii) that the variables  in the DT image models are discrete rather than Gaussian.  We also need to consider moves that change n. This can be effected with a split/merge  move. In the split direction, consider a node with a parent and several children. Split this  node and randomly assign the children to the two split nodes. Each of the split nodes keeps  the same parent. The probability of accepting this move under the Metropolis-Hastings  scheme is  (P(nt'Z(nt)'x)Q(nt'Z(nt);n'Z(n)))  a=min 1, p(n,Z(n)lx)Q(n,Z(n);n,,Z(n,) ) ,  A MCMC Approach to Hierarchical Mixture Modelling 683  where Q(n', Z(n'); n, Z(n)) is the proposal probability of configuration (n', Z(n')) given  configuration (n, Z(n)). This scheme is based on the work on MCMC model composition  (MG '3) by Madigan and York [9], and on Green's work on reversible jump MCMC [5].  Another move that changes n is to remove "dangling" nodes, i.e. nodes which have no  children. This occurs when all the nodes in a given layer "decide" not to use one or more  nodes in the layer above.  An alternative to sampling from the posterior is to use approximate inference, such as  mean-field methods. These are currently being investigated for DT models [ 1 ].  2.4 Related work  There are a very large number of papers on hierarchical clustering; in this work we have fo-  cussed on expressing hierarchical clustering in terms of probabilistic models. For example  Ambros-Ingerson et al [2] and Mozer [ 10] developed models where the idea is to cluster  data at a coarse level, subtract out mean and cluster the residuals (recursively). This paper  can be seen as a probabilistic interpretation of this idea.  The reconstruction of phylogenetic trees from biological sequence (DNA or protein) in-  formation gives rise to the problem of inferring a binary tree from the data. Durbin et al  [3] (chapter 8) show how a probabilistic formulation of the problem can be developed, and  the link to agglomerative hierarchical clustering algorithms as approximations to the full  probabilistic method (see 8.6 in [3]). Much of the biological sequence work uses discrete  variables, which diverges somewhat from the focus of the current work. However work  by Edwards (1970) [4] concerns a branching Brownian-motion process, which has some  similarities to the model described above. Important differences are that Edwards' model is  in continuous time, and the the variances of the particles are derived from a Wiener process  (and so have variance proportional to the lifetime of the particle). This is in contrast to the  decreasing sequence of variances at a given number of levels assumed in the above model.  One important difference between the model discussed in this paper and the phylogenetic  tree model is that points in higher levels of the phylogenetic tree are taken to be individuals  at an earlier time in evolutionary history, which is not the interpretation we require here.  An very different notion of hierarchy in mixture models can be found in the work on the  AutoClass system [6]. They describe a model involving class hierarchy and inheritance,  but their trees specify over which dimensions sharing of parameters occurs (e.g. means and  covariance matrices for Gaussians). In contrast, the model in this paper creates a hierarchy  over examples labelled 1,... , n rather than dimensions.  Xu and Pearl [ 15] discuss the inference of a tree-structured belief network based on know-  ledge of the covariances of the leaf nodes. This algorithm cannot be applied directly in  our case as the covariances are not known, although we note that if multiple runs from a  given tree structure were available the covariances might be approximated using sample  estimates.  Other ideas concerning hierarchical clustering are discussed in [13] and [7].  3 Experiments  We describe two sets of experiments to explore these ideas.  3.1 Searching over Z with n fixed  100 4-level random trees were generated from the prior, using values of A = 1.5, A2 = 2,  )3 = 3, and cr = 10, cr : 1, cr : 0.01. These trees had between 4 and 79 leaf  684 C. K. L Williams  nodes, with an average of 30. For each tree n was kept the same as in the generafive  tree, and sampling was carried out over Z starting from a random initial configuration.  A given node proposes changing its parent, and this proposal is accepted or rejected with  the usual Metropolis probability. In one sweep, each node in levels 3 and 4 makes such  a move. (Level 2 nodes only have one parent so there is no point in such a move there.)  To obtain a representative sample of P(Z(n)ln , x), we should run the chain for as long  as possible. However, we can also use the chain to find configurations with high posterior  probability, and in this case running for longer only increases the chances of finding a better  configuration. In our experiments the sampler was run for 100 sweeps. As P(Z(n)In) is  uniform for fixed n, the posterior is simply proportional to the likelihood term. It would  also be possible to run simulated annealing with the same move set to search explicitly for  the maximum a posteriori (MAP) configuration.  The results are that for 76 of the 100 cases the tree with the highest posterior probability  (HPP) configuration had higher posterior probability than the generative tree, for 20 cases  the same tree was found and in 4 cases the HPP solution was inferior to the generative tree.  The fact that in almost all cases the sampler found a configuration as good or better than  the generative one in a relatively small number of sweeps is very encouraging.  In Figure 2 the generative (left column) and HPP trees for fixed n (middle column) are  plotted for two examples. In panel (b) note the "dangling" node in level 2, which means  that the level 3 nodes to the left end up in a inferior configuration to (a). By contrast, in  panel (e) the sampler has found a better (less tangled) configuration than the generative  model (d).  (a)  (b)  (c)  (e)  (d) (f)  Figure 2: (a) and (d) show the generafive trees for two examples. The corresponding HPP  trees for fixed n are plotted in (b) and (e) and those for variable n in (c) and (f). The number  in each panel is the log posterior probability of the configuration. The nodes in levels 2 and  3 are shown located at their posterior means. Apparent non-tree structures are caused by  two nodes being plotted almost on top of each other.  3.2 Searching over both n and Z  Given some data x we will not usually know appropriate numbers of hidden units. This  motivates searching over both Z and n, which can be achieved using the split/merge moves  discussed in section 2.3.  In the experiments below the initial numbers of units in levels 2 and 3 (denoted 52 and  A MCMC Approach to Hierarchical Mixture Modelling 685  3) were set using the simple-minded formulae 3 = [dim(x)/Xal = r/x,]. A  proper inferential calculation for n2 and n3 can be carried out, but it requires the solution  of a non-linear optimization problem. Given f2 and 3, the initial connection configuration  was chosen randomly.  The search method used was to propose a split/merge move (with probability 0.5:0.5) in  level 2, then to sample the level 2 to level 3 connections, and then to propose a split-merge  move in level 3, and then update the level 3 to level 4 connections. This comprised a single  sweep, and as above 100 sweeps were used.  Experiments were conducted on the same trees used in section 3.1. In this case the results  were that for 50 out of the 100 cases, the HPP configuration had higher posterior probability  than the generative tree, for 11 cases the same tree was found and in 39 cases the HPP  solution was inferior to the generative tree. Overall these results are less good than the  ones in section 3.1, but it should be remembered that the search space is now much larger,  and so it would be expected that one would need to search longer. Comparing the results  from fixed n against those with variable n shows that in 42 out of 100 cases the variable  n method gave a higher posterior probability, in 45 cases it was lower and in 13 cases the  same trees were found.  The rightmost column of Figure 2 shows the HPP configurations when sampling with vari-  able n on the two examples discussed above. In panel (c) the solution found is not very  dissimilar to that in panel (b), although the overall probability is lower. In (f), the solution  found uses just one level 2 centre rather than two, and obtains a higher posterior probability  than the configurations in (e) and (d).  4 Discussion  The results above indicate that the proposed model behaves sensibly, and that reasonable  solutions can be found with relatively short amounts of search. The method has been  demonstrated on univariate data, but extending it to multivariate Gaussian data for which  each dimension is independent given the tree structure is very easy as the likelihood calcu-  lation is independent on each dimension.  There are many other directions is which the model can be developed. Firsfly, the model as  presented has uniform mixing proportions, so that children are equally likely to connect to  each potential parent. This can be generalized so that there is a non-uniform vector of con-  nection probabilities in each layer. Also, given a tree structure and independent Dirichlet  priors over these probability vectors, these parameters can be integrated out analytically.  Secondly, the model can be made to generate iid data by regarding the penultimate layer  as mixture centres; in this case the term P(,LIn_) would be ignored when computing  the probability of the tree. Thirdly, it would be possible to add the variance variables to the  MCMC scheme, e.g. using the Metropolis algorithm, after defining a suitable prior on the  sequence of variances cry,.. 2 The constraint that all variances in the same level are   O'L_ 1 .  equal could also be relaxed by allowing them to depend on hyperparameters set at every  level. Fourthly, there may be improved MCMC schemes that can be devised. For example,  in the current implementation the posterior means of the candidate units are not taken into  account when proposing merge moves (cf [5]). Fifthly, for the multivariate Gaussian ver-  sion we can consider a tree-structured factor analysis model, so that higher levels in the  tree need not have the same dimensionality as the data vectors.  One can also consider a version where each dimension is a multinomial rather than a con-  tinuous variable In this case one might consider a model where a multinomial parameter  vector Ot in the tree is generated from its parent by Ot = 0_ + (1 - 3)r where 2/ [0, 1]  and r is a random vector of probabilities. An alternative model could be to build a tree  686 C. K. I. 'lliams  structured prior on the c parameters of the Dirichlet prior for the multinomial distribution.  Acknowledgments  This work is partially supported through EPSRC grant GR/L78161 Probabilistic Models  for Sequences. I thank the Gatsby Computational Neuroscience Unit (UCL) for organizing  the "Mixtures Day" in March 1999 and supporting my attendance, and Peter Green, Phil  Dawid and Peter Dayan for helpful discussions at the meeting. I also thank Amos Storkey  for helpful discussions and Magnus Rattray for (accidentally !) pointing me towards the  chapters on phylogenetic trees in [3].  References  [10]  [11]  [12]  [13]  [14]  [15]  [1] N.J. Adams, A. Storkey, Z. Ghahramani, and C. K. I. Williams. MFDTs: Mean Field  Dynamic Trees. Submitted to ICPR 2000, 1999.  [2] J. Ambros-Ingerson, R. Granger, and G. Lynch. Simulation of Paleocortex Performs  Hierarchical Clustering. Science, 247:1344-1348, 1990.  [3] R. Durbin, S. Eddy, A. Krogh, and G. Mitchison. Biological Sequence Analysis.  Cambridge University Press, Cambridge, UK, 1998.  [4] A. W. F. Edwards. Estimation of the Branch Points of a Branching Diffusion Process.  Journal of the Royal Statistical Society B, 32(2): 155-174, 1970.  [5] P. J. Green. Reversible Jump Markov chain Monte Carlo computation and Bayesian  model determination. Biometrika, 82(4):711-732, 1995.  [6] R. Hanson, J. Stutz, and P. Cheeseman. Bayesian Classification with Correlation and  Inheritance. In IJCAI-91: Proceedings of the Twelfth International Joint Conference  on Artificial Intelligence, 1991. Sydney, Australia.  [7] T. Hofmann and J. M. Buhmann. Hierarchical Pairwise Data Clustering by Mean-  Field Annealing. In F. Fogelman-Soulie and P. Gallinari, editors, Proc. ICANN 95.  EC2 et Cie, 1995.  [8] M. R. Luettgen and A. S. Willsky. Likelihood Calculation for a Class of Multiscale  Stochastic Models, with Application to Texture Discrimination. IEEE Trans. Image  Processing, 4(2): 194-207, 1995.  [9] D. Madigan and J. York. Bayesian Graphical Models for Discrete Data. International  Statistical Review, 63:215-232, 1995.  M. C. Mozer. Discovering Discrete Distributed Representations with Iterated Com-  petitive Learning. In R. P. Lippmann, J. E. Moody, and D. S. Touretzky, editors,  Advances in Neural Information Processing Systems 3. Morgan Kaufmann, 1991.  J. Pearl. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Infer-  ence. Morgan Kaufmann, San Mateo, CA, 1988.  B. Ripley. Pattern Recognition and Neural Networks. Cambridge University Press,  Cambridge, UK, 1996.  N. Vasconcelos and A. Lippmann. Learning Mixture Hierarchies. In M. S. Kearns,  S. A. Solla, and D. A. Cohn, editors, Advances in Neural Information Processing  Systems 11, pages 606-612. MIT Press, 1999.  C. K. I. Williams and N.J. Adams. DTs: Dynamic Trees. In M. J. Kearns, S. A.  Solla, and D. A. Cohn, editors, Advances in Neural Information Processing Systems  11. MIT Press, 1999.  L. Xu and J. Pearl. Structuring Causal Tree Models with Continuous Variables. In  L. N. Kanal, T S. Levitt, and J. F. Lemmer, editors, Uncertainty in Artificial Intelli-  gence 3. Elsevier, 1989.  
Unmixing Hyperspectral Data  Lucas Parra Clay Spence Paul Sajda  Samoff Corporation, CN-5300, Princeton, NJ 08543, USA  { lparra, cspence,psajda sarnoff. corn  Andreas Ziehe, Klaus-Robert Mfiller  GMD FIRST.IDA, Kekulstr. 7, 12489 Berlin, Germany  { ziehe, klaus) first. gmd. de  Abstract  In hyperspectral imagery one pixel typically consists of a mixture  of the reflectance spectra of several materials, where the mixture  coefficients correspond to the abundances of the constituting ma-  terials. We assume linear combinations of reflectance spectra with  some additive normal sensor noise and derive a probabilistic MAP  framework for analyzing hyperspectral data. As the material re-  flectance characteristics are not know a priori, we face the problem  of unsupervised linear unmixing. The incorporation of different  prior information (e.g. positivity and normalization of the abun-  dances) naturally leads to a family of interesting algorithms, for  example in the noise-free case yielding an algorithm that can be  understood as constrained independent component analysis (ICA).  Simulations underline the usefulness of our theory.  i Introduction  Current hyperspectral remote sensing technology can form images of ground surface  reflectance at a few hundred wavelengths simultaneously, with wavelengths ranging  from 0.4 to 2.5m and spatial resolutions of 10-30m. The applications of this  technology include environmental monitoring and mineral exploration and mining.  The benefit of hyperspectral imagery is that many different objects and terrain  types can be characterized by their spectral signature.  The first step in most hyperspectral image analysis systems is to perform a spectral  unmixing to determine the original spectral signals of some set of prime materials.  The basic difficulty is that for a given image pixel the spectral reflectance patterns  of the surface materials is in general not known a priori. However there are gen-  eral physical and statistical priors which can be exploited to potentially improve  spectral unmixing. In this paper we address the problem of unmixing hyperspectral  imagery through incorporation of physical and statistical priors within an unsuper-  vised Bayesian framework.  We begin by first presenting the linear superposition model for the reflectances  measured. We then discuss the advantages of unsupervised over supervised systems.  Unmixing Hyperspectral Data 943  We derive a general maximum a posteriori (MAP) framework to find the material  spectra and infer the abundances. Interestingly, depending on how the priors are  incorporated, the zero noise case yields (i) a simplex approach or (ii) a constrained  ICA algorithm. Assuming non-zero noise our MAP estimate utilizes a constrained  least squares algorithm. The two latter approaches are new algorithms whereas the  simplex algorithm has been previously suggested for the analysis of hyperspectral  data.  Linear Modeling To a first approximation the intensities X (xix) measured in  each spectral band A = 1,... ,L for a given pixel i = 1,...,N are linear combi-  nations of the reflectance characteristics S (Smx) of the materials ra = 1,..., M  present in that area. Possible errors of this approximation and sensor noise are  taken into account by adding a noise term N (nix). In matrix form this can be  summarized as  X--AS+N, subject to: AIM=I, A_>0,  (1)  where matrix A (aim) represents the abundance of material ra in the area cor-  responding to pixel i, with positivity and normalization constraints. Note that  ground inclination or a changing viewing angle may cause an overall scale factor for  all bands that varies with the pixels. This can be incorporated in the model by sim-  ply replacing the constraint AIM= 1L with AIM _< 1 which does does not affect  the discussion in the remainder of the paper. This is clearly a simplified model of  the physical phenomena. For example, with spatially fine grained mixtures, called  intimate mixtures, multiple reflectance may causes departures from this first or-  der model. Additionally there are a number of inherent spatial variations in real  data, such as inhomogeneous vapor and dust particles in the atmosphere, that will  cause a departure from the linear model in equation (1). Nevertheless, in practical  applications a linear model has produced reasonable results for areal mixtures.  Supervised vs. Unsupervised techniques Supervised spectral unmixing re-  lies on the prior knowledge about the reflectance patterns S of candidate surface  materials, sometimes called endmembers, or expert knowledge and a series of semi-  automatic steps to find the constituting materials in a particular scene. Once the  user identifies a pixel i containing a single material, i.e. aim -- I for a given ra and  i, the corresponding spectral characteristics of that material can be taken directly  from the observations, i.e., Smx = xix [4]. Given knowledge about the endmembers  one can simply find the abundances by solving a constrained least squares problem.  The problem with such supervised techniques is that finding the correct S may re-  quire substantial user interaction and the result may be error prone, as a pixel that  actually contains a mixture can be misinterpreted as a pure endmember. Another  approach obtains endmembers directly from a database. This is also problematic  because the actual surface material on the ground may not match the database en-  tries, due to atmospheric absorption or other noise sources. Finding close matches  is an ambiguous process as some endmembers have very similar reflectance charac-  teristics and may match several entries in the database.  Unsupervised unmixing, in contrast, tries to identify the endmembers and mixtures  directly from the observed data X without any user interaction. There are a variety  of such approaches. In one approach a simplex is fit to the data distribution [7, 6, 2].  The resulting vertex points of the simplex represent the desired endmembers, but  this technique is very sensitive to noise as a few boundary points can potentially  change the location of the simplex vertex points considerably. Another approach by  Szu [9] tries to find abundances that have the highest entropy subject to constraints  that the amount of materials is as evenly distributed as possible - an assumption  944 L. Parra, C. D. Spence, P Sajda, A. Ziehe and K.-R. Mailer  which is clearly not valid in many actual surface material distributions. A relatively  new approach considers modeling the statistical information across wavelength as  statistically independent AR processes [1]. This leads directly to the contextual  linear ICA algorithm [5]. However, the approach in [1] does not take into account  constraints on the abundances, noise, or prior information. Most importantly, the  method [1] can only integrate information from a small number of pixels at a time  (same as the number of endmembers). Typically however we will have only a few  endmembers but many thousand pixels.  2 The Maximum A Posterior Framework  2.1 A probabilistic model of unsupervised spectral unmixing  Our model has observations or data X and hidden variables A, S, and N that  are explained by the noisy linear model (1). We estimate the values of the hidden  variables by using MAP  p(A, SIX)= p(XIA'S)p(A'S) = pn(XIA'S)pa(A)ps(S) (2)  p(X) p(X)  with pa(A), ps(S), p,(N) as the a priori assumptions of the distributions. With  MAP we estimate the most probable values for given priors after observing the data,  AMAP, SMAP -- arg maxp(A, SIX ) (3)  A,S  Note that for maximization the constant factor p(X) can be ignored. Our first as-  sumption, which is indicated in equation (2) is that the abundances are independent  of the reflectance spectra as their origins are completely unrelated: (A0) A and S  are independent.  The MAP algorithm is entirely defined by the choices of priors that are guided by  the problem of hyperspectral unmixing: (A1) A represent probabilities for each  pixel i. (A2) S are independent for different material ra. (A3) N are normal i.i.d.  for all i, . In summary, our MAP framework includes the assumptions A0-A3.  2.2 Including Priors  Priors on the abundances Positivity and normalization of the abundances can  be represented as,  pa(A) - 5(AIM -- 1N)O(A), (4)  where 50 represent the Kronecker delta function and O 0 the step function. With  this choice a point not satisfying the constraint will have zero a posteriori probabil-  ity. This prior introduces no particular bias of the solutions other then abundance  constraints. It does however assume the abundances of different pixels to be inde-  pendent.  Prior on spectra Usually we find systematic trends in the spectra that cause  significant correlation. However such an overall trend can be subtracted and/or  filtered from the data leaving only independent signals that encode the variation  from that overall trend. For example one can capture the conditional dependency  structure with a linear auto-regressive (AR) model and analyze the resulting "inno-  vations" or prediction errors [3]. In our model we assume that the spectra represent  independent instances of an AR process having a white innovation process era), dis-  tributed according to pe(e). With a Toeplitz matrix T of the AR coefficients we  Unmixing Hyperspectral Data 945  can write, em -- smT. The AR coefficients can be found in a preprocessing step on  the observations X. If S now represents the innovation process itself, our prior can  be represented as,  34 L L  pe(S) cr pe(ST) = II H Pe(Z 8m)dtXX' )' (5)  m=l X=l  Additionally Pe (e) is parameterized by a mean and scale parameter and potentially  parameters determining the higher moments of the distributions. For brevity we  ignore the details of the parameterization in this paper.  Prior on the noise As outlined in the introduction there are a number of prob-  lems that can cause the linear model X -- AS to be inaccurate (e.g. multiple  reflections, inhomogeneous atmospheric absorption, and detector noise.) As it is  hard to treat all these phenomena explicitly, we suggest to pool them into one noise  variable that we assume for simplicity to be normal distributed with a wavelength  dependent noise variance  L  p(XlA, S) -- pn(N) - iv(X - AS, - II A;(xx - Asx, axI), (6)  =1  where iV(., .) represents a zero mean Gaussian distribution, and I the identity matrix  indicating the independent noise at each pixel.  2.3 MAP Solution for Zero Noise Case  Let us consider the noise-free case. Although this simplification may be inaccurate it  will allow us to greatly reduce the number of free hidden variables - from NM + ML  to M 2. In the noise-free case the variables A, S are then deterministically dependent  on each other through a NL-dimensional &distribution, p(XIAS ) = 5(X - AS).  We can remove one of these variables from our discussion by integrating (2). It is  instructive to first consider removing A  p(SlX) / dAS(X - AS)pa(A)ps(S) = IS-lpa(XS-X)ps(S). (7)  We omit tedious details and assume L -- M and invertible S so that we can perform  the variable substitution that introduces the Jacobian determinant IS-l . Let us  consider the influence of the different terms. The Jacobian determinant measures  the volume spanned by the endmembers S. Maximizing its inverse will therefore try  to shrink the simplex spanned by S. The term pa (XS -) should guarantee that all  data points map into the inside of the simplex, since the term should contribute zero  or low probability for points that violate the constraint. Note that these two terms,  in principle, define the same objective as the simplex envelope fitting algorithms  previously mentioned [2].  In the present work we are more interested in the algorithm that results from  removing S and finding the MAP estimate of A. We obtain (cf. Eq.(7))  p(AlX ) c / dS6(X - AS)pa(A)ps(S) = [A-11ps(A-X)pa(A). (8)  For now we assumed N = M.  If ps(S) factors over m, i.e. endmembers are inde-  pendent, maximizing the first two terms represents the ICA algorithm. However,  1In practice more frequently we have N > M. In that case the observations X can be  mapped into a M dimensional subspace using the singular value decomposition (SVD),  X = UDV T. The discussion applies then to the reduced observations  = UX with  U/ being the first M columns of U.  946 L. Parra, C. D. Spence, P. Sajda, ,4. Ziehe and K.-R. Miiller  the prior on A will restrict the solutions to satisfy the abundance constraints and  bias the result depending on the detailed choice of pa(A), so we are led to con-  strained ICA.  In summary, depending on which variable we integrate out we obtain two methods  for solving the spectral unmixing problem: the known technique of simplex fitting  and a new constrained ICA algorithm.  2.4 MAP Solution for the Noisy Case  Combining the choices for the priors made in section 2.2 (Eqs.(4), (5) and (6)) with  (2) and (3) we obtain  AMAP, SMAP --- argmax J(xi) -- aisx, e/X) Pe SmX'txx') , (9)  A,S A=q i=1 m=l =  subject to AIM = 1L, A _> 0. The logarithm of the cost function in (9) is denoted  by L -- L(A, S). Its gradient with respect to the hidden variables is  OL = _Am m diag(er)_ x _ fs(Sra ) (10)  OSm  Olnpe(s) In (10)  where N = X - AS, nm are the M column vectors of N, rs(s) = os  fs is applied to each element of Sm.  The optimization with respect to A for given S can be implemented as a standard  weighted least squares (LS) problem with a linear constraint and positivity bounds.  Since the constraints apply for every pixel independently one can solve N separate  constrained LS problems of M unknowns each. We alternate between gradient steps  for S and explicit solutions for A until convergence. Any additional parameters of  Pe (e) such as scale and mean may be obtained in a maximum likelihood (ML) sense  by maximizing L. Note that the nonlinear optimization is not subject to constraints;  the constraints apply only in the quadratic optimization.  3 Experiments  3.1 Zero Noise Case: Artificial Mixtures  In our first experiment we use mineral data from the United States Geological Sur-  vey (USGS) 2 to build artificial mixtures for evaluating our unsupervised unmixing  framework. Three target endmembers where chosen (Almandine WS479, Montmo-  rillonite+Illi CM42 and Dickite NMNH106242). A spectral scene of 100 samples  was constructed by creating a random mixture of the three minerals. Of the 100  samples, there were no pure samples (i.e. no mineral had more than a 80% abun-  dance in any sample). Figure 1A is the spectra of the endmembers recovered by the  constrained ICA technique of section 2.3, where the constraints were implemented  with penalty terms added to the conventional maximum likelihood ICA algorithm.  These are nearly identical to the spectra of the true endmembers, shown in fig-  ure lB, which were used for mixing. Interesting to note is the scatter-plot of the  100 samples across two bands. The open circles are the absorption values at these  two bands for endmembers found by the MAP technique. Given that each mixed  sample consists of no more than 80% of any endmember, the endmember points  on the scatter-plot are quite distant from the cluster. A simplex fitting technique  would have significant difficulty recovering the endmembers from this clustering.  2see http://speclab.cr.usgs.gov/spectral.lib.456.descript/decript04.html  Unmixing Hyperspectral Data 94 7  0.8  o.s  0.4.  0.2  found endmembers  target endmembers  1  0.s  0.4  0.2  obsen/ed X and found S  50 100 150 200 50 100 150 200 0.4 0.6 0.8  wavelength wavelength wavelength=30  Figure 1: Results for noise-free artificial mixture. A recovered endmembers using  MAP technique. B "true" target endmembers. C scatter plot of samples across 2  bands showing the absorption of the three endmembers computed by MAP (open  circles).  3.2 Noisy Case: Real Mixtures  To validate the noise model MAP framework of section 2.4 we conducted an ex-  periment using ground truthed USGS data representing real mixtures. We selected  10x10 blocks of pixels from three different regions 3 in the AVIRIS data of the  Cuprite, Nevada mining district. We separate these 300 mixed spectra assuming  two endmembers and an AR detrending with 5 AR coecients and the MAP tech-  niques of section 2.4. Overall brightness was accounted for as explain in the linear  modeling of section 1. The endmembers are shown in figure 2A and B in comparison  to laboratory spectra from the USGS spectral library for these minerals [8]. Figure  2C shows the corresponding abundances, which match the ground truth; region  (III) mainly consists of Muscovite while regions (I)+(II) contain (areal) mixtures of  Kaolinire and Muscovite.  4 Discussion  Hyperspectral unmixing is a challenging practical problem for unsupervised learn-  ing. Our probabilistic approach leads to several interesting algorithms: (1) simplex  fitting, (2) constrained ICA and (3) constrained least squares that can eciently use  multi-channel information. An important element of our approach is the explicit  use of prior information. Our simulation examples show that we can recover the  endmembers, even in the presence of noise and model uncertainty. The approach  described in this paper does not yet exploit local correlations between neighboring  pixels that are well known to exist. Future work will therefore exploit not only  spectral but also spatial prior information for detecting objects and materials.  Acknowledgments  We would like to thank Gregg Swayze at the USGS for assistance in obtaining the  data.  3The regions were from the image plate2.cuprite95.alpha. 2um.image.wlocals.gif in  ftp://speclab.cr.usgs.gov/pub/cuprite/gregg.thesis.images/, at the coordinates (265,710)  and (275,697), which contained Kaolinire and Muscovite 2, and (143,661), which only  contained Muscovite 2.  948 L. Parra, C. D. Spence, P Sajda, A. Ziehe and K.-R. Miiller  0.8  0.75  0.7  0.65  0.6  0.55  0.5  0.45  0.4  180  Muscovite  i' '.. ..  190 20 210 220  wavelength  0.8  0.7  0.6  0.5  0.4  0.3  A B C  Figure 2: A Spectra of computed endmember (solid line) vs Muscovite sample  spectra from the USGS data base library. Note we show only part of the spectrum  since the discriminating features are located only between band 172 and 220. B  Computed endmember (solid line) vs Kaolinite sample spectra from the USGS data  base library. C Abundances for Kaolinite and Muscovite for three regions (lighter  pixels represent higher abundance). Region 1 and region 2 have similar abundances  for Kaolinite and Muscovite, while region 3 contains more Muscovite.  References  [1] J. Bayliss, J. A. Gualtieri, and R. Cromp. Analyzing hyperspectral data with  independent component analysis. In J. M. Selander, editor, Proc. SPIE Applied  Image and Pattern Recognition Workshop, volume 9, P.O. Box 10, Bellingham  WA 98227-0010, 1997. SPIE.  [2] J.W. Boardman and F.A. Kruse. Automated spectral analysis: a geologic exam-  ple using AVIRIS data, north Grapevine Mountains, Nevada. In Tenth Thematic  Conference on Geologic Remote Sensing, pages 407-418, Ann arbor, MI, 1994.  Environmental Research Institute of Michigan.  [3] S. Haykin. Adaptive Filter Theory. Prentice Hall, 1991.  [4] F. Maselli, , M. Pieri, and C. Conese. Automatic identification of end-members  for the spectral decomposition of remotely sensed scenes. Remote Sensing for  Geography, Geology, Land Planning, and Cultural Heritage (SPIE), 2960:104-  109, 1996.  [5] B. Pearlmutter and L. Parra. Maximum likelihood blind source separation: A  context-sensitive generalization of ICA. In M. Mozer, M. Jordan, and T. Petsche,  editors, Advances in Neural Information Processing Systems 9, pages 613-619,  Cambridge MA, 1997. MIT Press.  [6] J.J. Settle. Linear mixing and the estimation of ground cover proportions. In-  ternational Journal of Remote Sensing, 14:1159-1177, 1993.  M.O. Smith, J.B. Adams, and A.R. Gillespie. Reference endmembers for spectral  mixture analysis. In Fifth Australian remote sensing conference, volume 1, pages  331-340, 1990.  U.S. Geological Survey. USGS digital spectral library. Open File Report 93-592,  1993.  H. Szu and C. Hsu. Landsat spectral demixing a la superresolution of blind  matrix inversion by constraint MaxEnt neural nets. In Wavelet Applications  IV, volume 3078, pages 147-160. SPIE, 1997.  [7]  [8]  [9]  
Spectral Cues in Human Sound Localization  Craig T. Jin  Department of Physiology and  Department of Electrical Engineering,  Univ. of Sydney, NSW 2006, Australia  Anna Corderoy  Department of Physiology  Univ. of Sydney, NSW 2006, Australia  Simon Carlile  Department of Physiology  and Institute of Biomedical Research  Univ. of Sydney, NSW 2006, Australia  Andr6 van Schaik  Department of Electrical Engineering,  Univ. of Sydney, NSW 2006, Australia  Abstract  The differential contribution of the monaural and interaural spectral  cues to human sound localization was examined using a combined psy-  chophysical and analytical approach. The cues to a sound's location  were correlated on an individual basis with the human localization re-  sponses to a variety of spectrally manipulated sounds. The spectral cues  derive from the acoustical filtering of an individual's auditory periphery  which is characterized by the measured head-related transfer functions  (HRTFs). Auditory localization performance was determined in virtual  auditory space (VAS). Psychoacoustical experiments were conducted in  which the amplitude spectra of the sound stimulus was varied indepen-  dently at each ear while preserving the normal timing cues, an impossibil-  ity in the free-field environment. Virtual auditory noise stimuli were gen-  erated over earphones for a specified target direction such that there was  a "false" flat spectrum at the left eardrum. Using the subject's HRTFs,  the sound spectrum at the right eardrum was then adjusted so that either  the true right monaural spectral cue or the true interaural spectral cue  was preserved. All subjects showed systematic mislocalizations in both  the true right and true interaural spectral conditions which was absent in  their control localization performance. The analysis of the different cues  along with the subjects' localization responses suggests there are signif-  icant differences in the use of the monaural and interaural spectral cues  and that the auditory system's reliance on the spectral cues varies with  the sound condition.  1 Introduction  Humans are remarkably accurate in their ability to localize transient, broadband noise, an  ability with obvious evolutionary advantages. The study of human auditory localization has  a considerable and rich history (recent review [ 1]) which demonstrates that there are three  general classes of acoustical cues involved in the localization process: (l) interaural time  differences, ITDs; (2) interaural level differences, ILDs; and (3) the spectral cues resulting  Spectral Cues in Human Sound Localization 769  from the auditory periphery. It is generally accepted that for humans, the ITD and ILD  cues only specify the location of the sound source to within a "cone of confusion" [1],  i.e., a locus of points approximating the surface of a cone symmetric with respect to the  interaural axis. It remains, therefore, for the localization system to extract a more precise  sound source location from the spectral cues.  The utilization of the outer ear spectral cues during sound localization has been analyzed  both as a statistical estimation problem, (e.g., [2]) and as optimization problem, often using  neural networks, (e.g., [3]). Such computational models show that sufficient localization  information is provided by the spectral cues to resolve the cone of confusion ambiguity  which corroborates the psychoacoustical evidence. Furthermore, it is commonly argued  that the interaural spectral cue, because of its natural robustness to level and spectral vari-  ations, has advantages over the monaural spectral cues alone. Despite these observations,  there is still considerable contention as to the relative role or contribution of the monaural  versus the interaural spectral cues.  In this study, each subject's spectral cues were characterized by measuring their head re-  lated transfer functions (HRTFs) for 393 evenly distributed positions in space. Measure-  ments were carried out in an anechoic chamber and were made for both ears simultane-  ously using a "blocked ear" technique [ 1 ]. Sounds filtered with the HRTFs and played over  earphones, which bypass the acoustical filtering of the outer ear, result in the illusion of  free-field sounds which is known as virtual auditory space (VAS). The HRTFs were used to  generate virtual sound sources in which the spectral cues were manipulated systematically.  The recorded HRTFs along with the Glasberg and Moore cochlear model [4] were also  used to generate neural excitation patterns (frequency representations of the sound stim-  ulus within the auditory nerve) which were used to estimate the different cues available  to the subject during the localization process. Using this analysis, the interaural spectral  cue was characterized and the different localization cues have been correlated with each  subjects' VAS localization responses.  2 VAS Sound Localization  The sound localization performance of four normal hearing subjects was examined in VAS  using broadband white noise (300 - 14 000 Hz). The stimuli were filtered under three  differing spectral conditions. (1) control: stimuli were filtered with spectrally correct left  and right ear HRTFs for a given target location, (2) veridical interaural: stimuli at the  left ear were made spectrally flat with an appropriate dB sound level for the given target  location, while the stimuli at the right ear were spectrally shaped to preserve the correct  interaural spectrum, (3) veridical right monaural: stimuli at the left ear were spectrally  flat as in the second condition, while the stimuli at the right ear were filtered with the  correct HRTF for the given target location, resulting in an inappropriate interaural spectral  difference. For each condition, a minimum-phase filter spectral approximation was made  and the interaural time difference was modeled as an all-pass delay [5]. Sounds were  presented at approximately 70 dB SPL and with duration 150 ms (with 10 ms raised-cosine  onset and offset ramps). Each subject performed five trials at each of 76 test positions for  each stimulus condition. Detailed sound localization methods can be found in [ 1 ]. A short  summary is presented below.  2.1 Sound Localization Task  The human localization experiments were carded out in a darkened anechoic chamber.  Virtual auditory sound stimuli were presented using earphones (ER-2, Etymbtic Research,  with a flat frequency response, within 3 dB, between 200-16 000 Hz). The perceived  location of the virtual sound source was indicated by the subject pointing his/her nose in  770 C. T. din, A. Corderoy, S. Carlile and A. v. Schaik  the direction of the perceived source. The subject's head orientation and position were  monitored using an electromagnetic sensor system (Polhemus, Inc.).  2.2 Human Sound Localization Performance  The sound localization performance of two subjects in the three different stimulus condi-  tions are shown in Figure 1. The pooled data across 76 locations and five trials is presented  for both the left (L) and right (R) hemispheres of space from the viewpoint of an outside  observer. The target location is shown by a cross and the centroid of the subjects responses  for each location is shown by a black dot with the standard deviation indicated by an ellipse.  Front-back confusions are plotted, although, they were removed for calculating the standard  deviations. The subjects localized the control broadband sounds accurately (Figure 1 a). In  contrast, the subjects demonstrated systematic mislocalizations for both the veridical inter-  aural and veridical monaural spectral conditions (Figures lb,c). There is clear pulling of  the localization responses to particular regions of space with evident intersubject variations.  (a) Subject 1: Broadband Control  Nose Cross: Target location  (b) Subject 1: Veridical Interaural Spectrum  (c) Subject 1: Veridical Right Monaural Spectrum  Subject 2: Broadband Control  Dot. Centtold location  of Responses  Ellipse: Standard Deviation  K.: of Responses  : R  .... :._ _ .-'T.  ""'+' :,  Subject 2: Veridical Interaural Spectrum  Subject 2: Veridical Right Monaural Spectrum  Figure 1: Localization performance for two subjects in the three sound conditions: (a)  control broadband; (b) veridical interaural; (c) veridical monaural. See text for details.  3 Extraction of Acoustical Cues  With accurate measurements of each individual's outer ear filtering, the different acousti-  cal cues can be compared with human localization performance on an individual basis. In  order to extract the different acoustical cues in a biologically plausible manner, a model  of peripheral auditory processing was used. A virtual source sound stimulus was prepared  as described in Secion 2 for a particular target location. The stimulus was then filtered  using a cochlear model based on the work of Glasberg and Moore [4]. This cochlear model  consisted of a set of modified rounded-exponential auditory filters. The width and shape  of the auditory filters change as a function of frequency (and sound level) in a manner  Spectral Cues in Human Sound Localization 771  consistent with the known physiological and psychophysical data. These filters were log-  arithmically spaced on the frequency axis with a total of 200 filters between 300 Hz and  14 kHz. The cochlea's compressive non-linearity was modelled mathematically using a  logarithmic function. Thus the logarithm of the output energy of a given filter indicated the  amount of neural activity in that particular cochlear channel.  The relative activity across the different cochlear channels was representative of the neu-  ral excitation pattern (EP) along the auditory nerve and it is from this excitation pattern  that the different spectral cues were estimated. For a given location, the left and right EPs  themselves represent the monaural spectral cues. The difference in the total energy (calcu-  lated as the area under the curve) between the left and right EPs was taken as a measure  of the interaural level difference and the interaural spectral shape cue was calculated as  the difference between the left and right EPs. The fourth cue, interaural time difference,  is a measure of the time lag between the signal in one ear as compared to the other and  depends principally upon the geometrical relationship between the sound source and the  human subject. This time delay was calculated using the acoustical impulse response for  both ears as measured during the HRTF recordings.  4 Correlation of Cues and Location  For each stimulus condition and location, the acoustical cues were calculated as described  above for all 393 HRTF locations. Locations at which a given cue correlates well with  the stimulus cue for a particular target location were taken as analytical predictions of the  subject's response locations according to that cue. As the spectral content of the signal is  varied, the cue(s) available may strongly match the cue(s) normally arising from locations  other than the target location. Therefore the aim of this analysis is to establish for which  locations and stimulus conditions a given response most correlated with a particular cue.  The following analyses (using a Matlab toolbox developed by the authors) hinge upon the  calculation of"cue correlation values". To a large extent, these calculations follow the ex-  amples described by [6] and are briefly described here. For each stimulus condition and  target location, the subject performed five localizations trials. For each of the subject's five  response locations, each possible cue was estimated (Section 3) assuming a flat-spectrum  broadband Gaussian white noise as the stimulus. A mathematical quantity was then calcu-  lated which would give a measure of the similarity of the response location cues with the  corresponding stimulus cues. The method of calculation depended on the cue and several  alternative methods were tried. Generally, for a given cue, these different methods demon-  strated the same basic pattern and the term "cue correlation value" has been given to the  mathematical quantity that was used to measure cue similarity. The methods are as follows.  For the ITD cue, the negative of the absolute value of the difference between possible  response location ITDs and the stimulus ITD was used as the ITD cue correlation values  (the more positive a value, the higher its correlation). The ILD cue correlation value was  calculated in a similar fashion. The cue correlation values for the left and right monaural  spectral cues (in this case, the shape of the neural excitation pattern) was calculated by  taking the difference between the stimulus EP and the possible response location EPs and  then summing across frequency the variation of this difference about its mean value. For the  interaural spectral cue, the vector difference between the left and right EPs was calculated  for both the stimulus and the possible response locations. The dot product between the  stimulus and the possible response location vectors gave the ISD cue correlation values.  The cue correlation values were normalized in order to facilitate meaningful comparisons  across the different acoustical cues. Following Middlebrooks [6], a "z-score normalized"  cue value, for each response location corresponding to a given target location, was obtained  by subtracting the mean correlation value (across all possible locations) and dividing by the  772 C.T. din, A. Corderoy, S. Carlile and A. v. $chaik  standard deviation. For these new cue values, termed the cue z-score values, a score of 1.0  or greater indicates onod correlation.  5 Relationship between the ISD and the Cone-of-Confusion  The distribution of a given cue's z-score values around the sphere of space surrounding the  subject reveals the spatial directions for that cue that correlate best with the given stimulus  and target location being examined. An examination of the interaural spectral cue indicated  that, unlike the other cues, the range of its cue z-score variation was relatively restricted on  the ipsilateral hemisphere of space relative to the sound stimulus (values on the ipsilateral  side were approximately 1.0, those on the contralateral side, -1.0). This was the first indi-  cation of the more moderate variation of the ISD cue across space as compared with the  monaural spectral cues.  Closer examination of the ISD cue revealed more detailed variational properties. In order  to facilitate meaningful comparisons with the other cues, the ISD cue z-score values were  adjusted such that all negative values (i.e., those values at locations generally contralateral  to the stimulus) were set to 0.0 and the cue z-score values recalculated. The spatial dis-  tribution of the rescaled ISD cue z-score values, as compared with the cue z-score values  for the other cues, is shown in Figure 2. The cone of confusion described by the ITD and  ILD is clearly evident (Fig. 2a,b) and it can be seen that the ISD cue is closely aligned with  these cues (Fig. 2c). Furthermore, the ISD cue demonstrates significant asymmetry along  the front-back dimensions. These novel observations demonstrate that while previous work  [3, 2] indicates that the ISD cue provides sufficient information to determine a sound's lo-  cation exactly along the cone of confusion, the variation of the cue z-score values along  the cone is substantially less than that for the monaural spectral cues (Fig. 2d), sugges*::'.g  perhaps that this acts to make the monaural spectral cue a more salient cue.  (a) Interaural Time Difference  4O  0  : -40  o  > (b) Interaural Level Difference  [] 40  0  -40  0 90  (c)  4O  0  0 0 180 0  (d)  4O  0  -4O  180 0  Azimuth  Interaural Spectrum  90 180  Ipsilateral Menaural Spectrum  90 180  1.5  1.1  Figure 2: Spatial plot of the cue z-score values for a single target location (46  azimuth,  20  elevation) and broadband sound condition. Gray-scale color values indicate the cue's  correlation in different spatial directions with the stimulus cue at the target location. (Z-  score values for the ISD cue have been rescaled, see text.)  6 Analysis of Sub|ects Responses using Cue Z-score Values  A given cue's z-score values for the subject's responses across all 76 test locations and  five trials were averaged. The mean and standard deviation are presented in a bar graph  (Fig. 3). The subjects' response locations correlate highly with the ITD and ILD cue and  Spectral Cues in Human Sound Localization 773  the standard deviation of the correlation was low (Fig. 3a,b). In other words, subjects' re-  sponses stayed on the cone of confusion of the target location. A similar analysis of the  more restricted, rescaled version of the interaural spectral cue shows that despite the spec-  tral manipulations and systematic mislocalizations, subject's were responding to locations  which were highly correlated with the interaural spectral cue (Fig. 3c). The bar graphs  for the monaural spectral cues ipsilateral and contralateral to the target location show the  average correlation of the subjects' responses with these cues varied considerably with the  stimulus condition (Fig. 3d-g) and to a lesser extent across subjects.  2 (a) ITD  I;':'!ii:! :%',<.'!' ::,ii,;: ;x-< :i:? : !::;  :':j,!::':'!!;'"'.-.."''.' :" (d) Left Contralateral  ntml Veddl Veddl   ' (f) Left I psi lateral  0 trol Veddil Veddil Spectrum  Bmadba Intemuml Right Monauml 2  Control Venial Veddil  Bmadbd Iemural Right Mouml  nt Veddil Veddil  Bmad Intemural Rig Monaural  (e) Right Contralateral  Spectrum  Control Veridical Veddical  Bdnd Intemur Rht Monauml  (g) Right Ipsilateml  Spectrum  ntl Veddil Veddil  Bmadnd InterauBl Right omuml  Figure 3' Correlation of the four subjects' (indicated by different gray bars) localization  responses with the different acoustical cues for each stimulus condition. The bar heights  indicates the mean cue z-score value, while the error bars indicate standard deviation.  7 Spatial Plots of Correlation Regions  As the localization responses tended to lie along the cone of confusion, the relative im-  portance of the spectral cues along the cone of confusion was examined. The correlation  values for the spectral cues associated with the subjects' responses were recalculated as a  z-score value using only the distribution of values restricted to the cone of confusion. This  demonstrates whether the spectral cues associated with the subjects' response locations  were better correlated with the stimulus cues, than for any random location on the cone of  confusion.  Spatial plots of the recalculated response cue z-score values for the spectral cues of one  subject (similar trends across subjects), obtained for each stimulus location and across the  three different sound conditions, is shown in Figure 4. Spatial regions of both high and  low correlation are evident that vary with the stimulus spectrum. The z-score values for the  ISD cue shows greater bilateral correlation across space in the veridical interaural condi-  tion (Fig. 4d) than for the veridical monaural condition (Fig. 4g), while the right monaural  spectral cue demonstrates higher correlation in the right hemisphere of space for the veridi-  cal monaural condition (Fig. 4i) as opposed to the veridical interaural condition (Fig. 4f).  This result (although not surprising) demonstrates that the auditory system is extracting  cues to source location in a manner dependent on the input sound spectrum and in a man-  ner consistent with the spectral information available in the sound spectrum. Figures 4e,h  clearly demonstrate that the flat sound spectrum in the left ear was strongly correlated with  and influenced the subject's localization judgements for specific regions of space.  774 C. T. din, A. Corderoy, S. Carlile and A. v. Schaik  Broadband  (a)  -180 0 180  (b)  -180 0 180  Veridical Interaural Veridical Mortaural  (d) (g)  -180 0 180 -180 0  (e) (h)  -180 0 180 -180 0  180  (c) (f) (i)  -1so o 18o -18o o 18o -1so o 1so  Azimuth  2  -0.5  Figure 4: Spatial plot of the spectral cue z-score values for one subject's localization re-  sponses across the three different sound conditions.  8 Conclusions  The correlation of human sound localization responses with the available acoustical cues  across three spectrally. different sound conditions has provided insights into the human au-  ditory system and its integration of cues to produce a coherent percept of spatial location.  These data suggest an interrelationship between the interaural spectral cue and the cone  of confusion. The ISD cue is front-back asymmetrical along the cone and its cue cor-  relation values vary more moderately as a function of space than those of the monaural  spectral cues. These data shed light on the relative role and importance of the interaural  and monaural spectral cues.  Acknowledgments  This research was supported by the ARC, NHMRC, and Dora Lush Scholarship to CJ.  References  [1] S. Carlile, Virtual auditory space: Generation and applications. New York: Chapman and Hall,  1996.  [2] R. O. Duda, "Elevation dependence of the interaural transfer function," in Binaural and spatial  hearing in real and virtual environments (R. H. Gilkey and T. R. Anderson, eds.), ch. 3, pp. 49-  75, Mahwah, New Jersey: Lawrence Erlbaum Associates, 1997.  [3] J. A. Janko, T. R. Anderson, and R. H. Gilkey, "Using neural networks to evaluate the viability  of monaural and interaural cues for sound localization," in Binaural and Spatial Hearing in real  and virtual environments (R. H. Gilkey and T. R. Anderson, eds.), ch. 26, pp. 557-570, Mahwah,  New Jersey: Lawrence Erlbaum Associates, 1997.  [4] B. Glasberg and B. Moore, "Derivation of auditory filter shapes from notched-noise data," Hear-  ingResearch, vol. 47, no. 1-2, pp. 103-138, 1990.  [5] F. Wightman and D. Kistler, "The dominant role of low-frequency interaural time differences in  sound localization," J. Acoust. Soc. Am., vol. 91, no. 3, pp. 1648-1661, 1992.  [6] J. Middlebrooks, "Narrow-band sound localization related to extemal ear acoustics," J. Acoust.  Soc. Am., vol. 92, no. 5, pp. 2607-2624, 1992.  
Speech Modelling Using Subspace and EM  Techniques  Gavin Smith  Cambridge University  Engineering Department  Cambridge CB2 1PZ  England  gas1003 @eng.cam.ac.uk  Jo5o FG de Freitas  Computer Science Division  487 Soda Hall  UC Berkeley  CA 94720-1776, USA.  jfgf@cs.berkeley.edu   Tony Robinson  Cambridge University  Engineering Department  Cambridge CB2 1PZ  England  ajr@eng.cam.ac.uk  Mahesan Niranjan  Computer Science  Sheffield University  Sheffield. S1 4DP  England  m.niranjan@dcs.shef. ac.uk  Abstract  The speech waveform can be modelled as a piecewise-stationary linear  stochastic state space system, and its parameters can be estimated using  an expectation-maximisation (EM) algorithm. One problem is the ini-  tialisation of the EM algorithm. Standard initialisation schemes can lead  to poor formant trajectories. But these trajectories however are impor-  tant for vowel intelligibility. The aim of this paper is to investigate the  suitability of subspace identification methods to initialise EM.  The paper compares the subspace state space system identification  (4SID) method with the EM algorithm. The 4SID and EM methods are  similar in that they both estimate a state sequence (but using Kalman fil-  ters and Kalman smoothers respectively), and then estimate parameters  (but using least-squares and maximum likelihood respectively). The sim-  ilarity of 4SID and EM motivates the use of 4SID to initialise EM. Also,  4SID is non-iterative and requires no initialisation, whereas EM is itera-  tive and requires initialisation. However 4SID is sub-optimal compared  to EM in a probabilistic sense. During experiments on real speech, 4SID  methods compare favourably with conventional initialisation techniques.  They produce smoother formant trajectories, have greater frequency res-  olution, and produce higher likelihoods.  Work done while in Cambridge Engineering Dept., UK.  Speech Modelling Using Subspace and EM Techniques 797  1 Introduction  This paper models speech using a stochastic state space model, where model parameters  are estimated using the expectation-maximisation (EM) technique. One problem is the  initialisation of the EM algorithm. Standard initialisation schemes can lead to poor formant  trajectories. These trajectories are however important for vowel intelligibility. This paper  investigates the suitability of subspace state space system identification (4SID) techniques  [ 10,11 ], which are popular in system identification, for EM initialisation.  Speech is split into fixed-length, overlapping frames. Overlap encourages temporally  smoother parameter transitions between frames. Due to the slow non-stationary behaviour  of speech, each frame of speech is assumed quasi-stationary and represented as a linear  time-invariant stochastic state space (SS) model.  xt+ = Axt +wt (1)  tt = cxt+vt (2)  The system order is p. xt E Mox is the state vector. A E I x and c E Mxo are  system parameters. The output tt E I is the speech signal at the microphone. Process  and observation noises are modelled as white zero-mean Gaussian stationary noises wt E  Mox ., N(0, Q) and vt E M  N(0, R) respectively. The problem definition is to  estimate parameters t9 = (A, c, Q, R) from speech tt only.  The structure of the paper is as follows. The theory section describes EM and 4SID applied  to the parameter estimation of the above SS model. The similarity of 4SID and EM moti-  vates the use of 4SID to initialise EM. Experiments on real speech then compare 4SID with  more conventional initialisation methods. The discussion then compares 4SID with EM.  2 Theory  2.1 The Expectation-Maximisation (EM) Technique  Given a sequence of N observations Yl:N of a signal such as speech, the maximum like-  lihood estimate for the parameters is tML = arg maxop(yx:vlO ). EM breaks the  maximisation of this potentially difficult likelihood function down into an iterative max-  imisation of a simpler likelihood function, generating a new estimate Ot each iteration.  Rewriting p(yx:N IO) in terms of a hidden state sequence Xl:N, and taking expectations  over p(Xl:NlY:N, O/)  logp(y:[O) = logp(x1:,y1:N[O) - logp(x:ly:,O) (3)  logp(Y1:vl) - E[logp(xx:vly1:v, t9)] (4)  Iterative maximisation of the first expectation in equation 4 guarantees an increase in  19+ = arg maxE[logp(x:v,y:v119)] (5)  This converges to a local or global maximum depending on the initial parameter estimate  190. Refer to [8] for more details. EM can thus be applied to the stochastic state space  798 G. Smith, d. F. G. d. Freitas, T. Robinson and M. Niranjan  model of equations 1 and 2 to determine optimal parameters O. An explanation is given  in [3]. The EM algorithm applied to the SS system consists of two stages per iteration.  Firstly, given current parameter estimates, states are estimated using a Kalman smoother.  Secondly, given these states, new parameters are estimated by maximising the expected  log likelihood function. We employ the Rauch-Tung-Striebel formulation of the Kalman  smoother [2].  2.2 The State-Space Model  Equations 1 and 2 can be cast in block matrix form and are termed the state sequence and  block output equations respectively [10]. Note that the use of blocking and fixed-length  signals applies restrictions to the general model in section 1. i > p is the block size.  (6)  (7)  Xi+,i+ is a state sequence matrix; its columns are the state vectors from time (i + 1) to  (i+j). X, is similarly defined. Y_l.i is a Hankel matrix of outputs from time 1 to (/+j-I).  W and V are similarly defined. A' is a reversed extended controllability-type matrix, ri  is the extended observability matrix and H' is a Toeplitz matrix. These are all defined  below where I pxp is an identity matrix.  I c  Xl,j de__f [Xl X2 X3---Xj] def cA  ri --  Zv de__f [A i-1 A i-2 ... I] cAi_ 1  def  Yli =  Y2 Y3 ..' Yj+i H' de___f C 0  Yi Yi+ ... Yi+j- cA i-2 ... c 0  A sequence of outputs can be separated into two block output equations containing past  and future outputs denoted with subscripts p andf respectively. With Yp ae__.f y li, Yf ae=f  Yi+I2i and similarly for W and V, and Xp &f X,j and Xf &f  -- : Xi+l,i+j, past and  future are related by the equations  Xf = AiXp q- zVWp  Yp - rixp q- HVWp q- Vp  YI = + H'WI + Vl  (8)  (9)  (10)  2.3 Subspace State Space System Identification (4SID) Techniques  Comments throughout this section on 4SID are largely taken from the work of Van Over-  schee and De Moor [10]. 4SID methods are related to instrumental variable (IV) methods  [11]. 4SID algorithms are composed of two stages. Stage one involves the low-rank ap-  proximation and estimation of the extended observability matrix directly from the output  Speech Modelling Using Subspace and EM Techniques 799  data. tor example, consider the future output block equation 10. Yf undergoes an orthogo-  nal projection onto the row space of Yp. This is denoted by Yf/p T T '  = YIYp (YpYp) Yp,  where t is the Moore-Penrose inverse.  YI/p = I'XI/p + H'WI/p + VI/p  YI/p = I'iX/p (11)  Stage two involves estimation of system parameters. The singular value decomposition of  Yf/p allows the observability and state sequence matrices to be estimated to within a  similarity transform from the column and row spaces respectively. From these two matri-  ces, system parameters (A, c, Q, R) can be determined by least-squares.  There are two interesting comments. Firstly, the orthogonal projection from stage one co-  incides with a minimum error between true data YI and its linear prediction from p in  the Frobenius norm. Greater flexibility is obtained by weighting the projection with ma-  trices W and W2 and analysing this: W(Yf/tp)W2. 4SID and IV methods differ  with respect to these weighting matrices. Weighting is similar to prefilter!ng the observa-  tions prior to analysis to preferentially weight some frequency domain, as is common in  identification theory [6]. Secondly, the state estimates from stage two can be considered as  outputs from a parallel bank of Kalman filters, each one estimating a state from the previous  i observations, and initialised using zero conditions.  The particular subspace algorithm and software used in this paper is the sto_pos algorithm  as detailed in [10]. Although this algorithm introduces a small bias into some of the pa-  rameter estimates, it guarantees positive realness of the covariance sequence, which in turn  guarantees the definition of a forward innovations model.  3 Experiments  Experiments are conducted on the phrase "in arithmetic", spoken by an adult male. The  speech waveform is obtained from the Eurom 0 database [4] and sampled at 16 kHz. The  speech waveform is divided into fixed-length, overlapping frames, the mean is subtracted  and then a hamming window is applied. Frames are 15 ms in duration, shifted 7.5 ms  each frame. Speech is modelled as detailed in section 1. All models are order 8. A frame  is assumed silent and no analysis done when the mean energy per sample is less than an  empirically defined threshold.  For the EM algorithm, a modified version of the software in [3] is used. The initial state  vector and covariance matrix are set to zero and identity respectively, and 50 iterations are  applied. Q is updated by taking its diagonal only in the M-step for numerical stability (see  [3]).  In these experiments, three schemes are compared at initialising parameters for the EM  algorithm, that is the estimation of O0. These schemes are compared in terms of their  formant trajectories relative to the spectrogram and their likelihoods. The three schemes  are   4SID. This is the subspace method in section 2.3 with block size 16.   ARMA. This estimates O0 using the customised Matlab armax function , which  models the speech waveform as an autoregressive moving average (ARMA) pro-  cess, with order 8 polynomials.  armax minimises a robustified quadratic prediction error criterion using an iterative Gauss-  Newton algorithm, initialised using a four-stage least-squares instrumental variables algorithm [7].  800 G. Smith, . E G. d. Freitas, T. Robinson and M. Niranjan   AR(1). This uses a simplistic method, and models the speech waveform as a  first order autoregressive (AR) process with some randomness introduced into the  estimation. It still initialises all parameters fully 2.  Results are shown in Figures 1 and 2. Figure 1 shows the speech waveform, spectrogram  and formant trajectories for EM with all three initialisation schemes. Here formant frequen-  cies are derived from the phase of the positive phase eigenvalues of A after 50 iterations of  EM. Comparison with the spectrogram shows that for this order 8 model, 4SID-EM pro-  duces best formant trajectories. Figure 2 shows mean average plots of likelihood against  EM iteration number for each initialisation scheme. 4SID-EM gives greater likelihoods  than ARMA-EM and AR(1 )-EM. The difference in formant trajectories between subspace-  EM and ARMA-EM despite the high likelihoods, demonstrates the multi-modality of the  likelihood function. For AR(1)-EM, a few frames were not estimated due to numerical  instability.  4 Discussion  Both the 4SID and EM algorithms employ similar methodologies: states are first estimated  using a Kalman device, and then these states are used to estimate system parameters ac-  cording to similar criteria. However in EM, states are estimated using past, present and  future observations with a Kalman smoother; system parameters are then estimated using  maximum likelihood (ML). Whereas in 4SID, states are estimated using the previous i  observations only with non-steady state Kalman filters. System parameters are then esti-  mated using least-squares (LS) subject to a positive realness constraint for the covariance  sequence. Refer also to [5] for a similar comparison.  4SID algorithms are sub-optimal for three reasons. Firstly, states are estimated using only  partial observations sequences. Secondly, the LS criterion is only an approximation to the  ML criterion. Thirdly, the positive realness constraint introduces bias. A positive realness  constraint is necessary due to a finite amount of data and any lacking in the SS model. For  this reason, 4SID methods are used to initialise rather than replace EM in these experi-  ments.  4SID methods also have some advantages. Firstly, they are linear and non-iterative, and  do not suffer from the disadvantages typical of iterative algorithms (including EM) such  as sensitivity to initial conditions, convergence to local minima, and the definition of con-  vergence criteria. Secondly, they require little prior parameterisation except the definition  of the system order, which can be determined in situ from observation of the singular val-  ues of the orthogonal projection. Thirdly, the use of the SVD gives numerical robustness  to the algorithms. Fourthly, they have higher frequency resolution than prediction error  minimisation methods such as ARMA and AR [1 ].  5 Conclusions  4SID methods can be used to initialise EM giving better formant tracks, higher likelihoods  and better frequency resolution than more conventional initialisation methods. In the future  we hope to compare 4SID methods with EM in a principled probabilistic manner, investi-  gate weighting matrices further, and apply these methods to speech enhancement. Further  work is done by Smith et al. in [9], and similar work done by Grivel et al. in [5].  Acknowledgements  We are grateful for the use of 4SID software supplied with [10] and the EM software of  2presented in the software in [3], this method is best used when the dimensions of the state space  and observations are the same.  Speech Modelling Using Subspace and EM Techniques 801  (a) Time waveform  -le4  I I  0.1 0.3 time / s 0.5  0  0.7  o  o. 1 0.3 time / s 0.5  0 (c) 4SID   * ,.i,++ . 4,+,.,3.+. +  ,, ,., ,? ,,,,, , =. +,,,.,2, +. ..,,,,,,,,,,,,  o ,,, ....... ,- ....... 26..,-, '"' " ...... ,i .... .,',}I',+ ';h''""*'o 'm" '"" 8o '"  8 (d) AA  I  o..,,,., .... 26'""' ' ' '""6'" 'hh; ..... b '"'" .... 8o '"'  4  ++i+ +, ++_ ++t +++, ,+  ++'+ %'+2+,,++  "'""'"""'5'6'"""'"'" ' '6'"  frame  0.7  lOO  I  'lob  Figure 1: (a) Time waveform and (b) spectrogram for "in arithmetic". Formant trajectories  are estimated using EM and a SS model initialised with three different schemes: (d) 4SID,  (e) ARMA and (f) AR(1).  802 G. Smith, J. F. G. d. Freitas, T. Robinson and M. Niranjan  Zoubin Ghahramani [3]. Gavin Smith is supported by the Schiff Foundation, Cambridge  University. At the time of writing, Nando de Freitas was supported by two University of  the Witwatersrand Merit Scholarships, a Foundation for Research Development Scholar-  ship (South Africa), an ORS award and a Trinity College External Research Studentship  (Cambridge).  - 1400  -- -1500  -1600  -1700  2 0 iteration nun%er 50  Figure 2: Likelihood convergence plots for EM and the SS model initialised with  4SID [- -], ARMA [-] and AR(1) [-.] for the experiments in Figure 1. Plots are the mean  average over all frames where analysed.  6 References  [1] Arun, K.S. & Kung, S.Y. (1990) Balanced Approximation of Stochastic Systems.  SIAM Journal on Matrix Analysis and Applications, vol. 11, no. 1, pp. 42-68.  [2] Gelb, A. ed., (1974) Applied Optimal Estimation. Cambridge, MA: MIT Press.  [3] Ghahramani, Z. & Hinton, G. (1996) Parameter Estimation for Linear Dynamical Sys-  tems, Tech. rep. CRG-TR-96-2, Dept. of Computer Science, Univ. of Toronto. Software  at www. gatsby. ucl. ac. uk/oubin/sofiware. html.  [4] Grice, M. & Ban'y, W. (1989) Multi-lingual Speech Input/Output: Assessment, Method-  ology and Standardization, Tech. rep., University College, London, ESPRIT Project 1541  (SAM), extension phase final report.  [5] Grivel, E., Gabrea, M. & Najim, M. (1999) Subspace State Space Model Identification  For Speech Enhancement, Paper 1622, ICASSP'99.  [6] Ljung, L. (1987) System Identification: Theory for the User. Englewood Cliffs, NJ:  Prentice-Hall, Inc.  [7] Ljung, L. (1991) System Identification Toolbox For Use With MatLab. 24 Prime Park  Way, Natrick, MA, USA: The MathWorks, Inc.  [8] McLachlan, G.J. & Krishnan, T. (1997) The EMAlgorithm and Extensions. John Wiley  and Sons Inc.  [9] Smith, G.A. & Robinson, A.J. & Niranjan, M. (2000) A Comparison Between the EM  and Subspace Algorithms for the Time-Invariant Linear Dynamical System. Tech. rep.  CUED/F-INFENG/TR.366, Engineering Dept., Cambridge Univ., UK.  [10] Van Overschee, P. & De Moor, B. (1996) Subspace Identification for Linear Sys-  tems: Theory, Implementation, Applications. Dordrecht, Netherlands: Kluwer Academic  Publishers.  [11] Viberg, M. & Wahlberg, B. & Ottersten, B. (1997) Analysis of State Space System  Identification Methods Based on Instrumental Variables and Subspace Fitting. Automatica,  vol. 33, no. 9, pp. 1603-1616.  
Robust Full Bayesian Methods for Neural  Networks  Christophe Andrieu*  Cambridge University  Engineering Department  Cambridge CB2 1PZ  England  ca226@eng.cam.ac.uk  Joo FG de Preitas  UC Berkeley  Computer Science  387 Soda Hall, Berkeley  CA 94720-1776 USA  j fgf@ cs. berkeley. edu  Arnaud Doucet  Cambridge University  Engineering Department  Cambridge CB2 1PZ  England  ad2@eng.cam.ac.uk  Abstract  In this paper, we propose a full Bayesian model for neural networks.  This model treats the model dimension (number of neurons), model  parameters, regularisation parameters and noise parameters as ran-  dom variables that need to be estimated. We then propose a re-  versible jump Markov chain Monte Carlo (MCMC) method to per-  form the necessary computations. We find that the results are not  only better than the previously reported ones, but also appear to  be robust with respect to the prior specification. Moreover, we  present a geometric convergence theorem for the algorithm.  I Introduction  In the early nineties, Buntine and Weigend (1991) and Macly (1992) showed that a  principled Bayesian learning approach to neural networks can lead to many improve-  ments [1,2]. In particular, Macly showed that by approximating the distributions  of the weights with Gaussians and adopting smoothing priors, it is possible to obtain  estimates of the weights and output variances and to automatically set the regular-  isation coefficients. Neal (1996) cast the net much further by introducing advanced  Bayesian simulation methods, specifically the hybrid Monte Carlo method, into the  analysis of neural networks [3]. Bayesian sequential Monte Carlo methods have also  been shown to provide good training results, especially in time-varying scenarios  [4]. More recently, Rios Insua and Milllet (1998) and Holmes and Mallick (1998)  have addressed the issue of selecting the number of hidden neurons with growing  and pruning algorithms from a Bayesian perspective [5,6]. In particular, they apply  the reversible jump Markov Chain Monte Carlo (MCMC) algorithm of Green [7]  to feed-forward sigmoidal networks and radial basis function (RBF) networks to  obtain joint estimates of the number of neurons and weights.  We also apply the reversible jump MCMC simulation algorithm to RBF networks so  as to compute the joint posterior distribution of the radial basis parameters and the  number of basis functions. However, we advance this area of research in two impor-  tant directions. Firstly, we propose a full hierarchical prior for RBF networks. That  Authorship based on alphabetical order.  380 C. Andrieu, J. F. G. d. Freitas and A. Doucet  is, we adopt a full Bayesian model, which accounts for model order uncertainty and  regularisation, and show that the results appear to be robust with respect to the  prior specification. Secondly, we present a geometric convergence theorem for the  algorithm. The complexity of the problem does not allow for a comprehensive dis-  cussion in this short paper. We have, therefore, focused on describing our objectives,  the Bayesian model, convergence theorem and results. Readers are encouraged to  consult our technical report for further results and implementation details [8] 1 .  2 Problem statement  Many physical processes may be described by the following nonlinear, multivariate  input-output mapping:  Yt = f(xt) + nt (1)  where xt  IR d corresponds to a group of input variables, yt G c to the target vari-  ables, n t G c to an unknown noise process and t = {1, 2,--. } is an index variable  over the data. In this context, the learning problem involves computing an approx-  imation to the function f and estimating the characteristics of the noise process  given a set of N input-output observations: (9 = {Xl,X2,..- ,xv,yl, Y2,'" , yv}  Typical examples include regression, where Yl:N,l:c 2 is continuous; classification,  where y corresponds to a group of classes and nonlinear dynamical system identi-  fication, where the inputs and targets correspond to several delayed versions of the  signals under consideration.  We adopt the approximation scheme of Holmes and Mallick (1998), consisting of  a mixture of k RBFs and a linear regression term. Yet, the work can be easily  extended to other regression models. More precisely, our model A4 is:  A/to  y = b + 'x + n k = 0  g4k' Yt = E=i aj(llx, - tjll) + b + n, k > 1 (2)  where II' II denotes a distance metric (usually Euclidean or Mahalanobis), U  IRd  denotes the j-th RBF centre for a model with k RBFs, aj  IR c the j-th RBF  amplitude and b  IR c and/  IR  x IR c the linear regression parameters. The noise  sequence nt  IRc is assumed to be zero-mean white Gaussian. It is important to  mention that although we have not explicitly indicated the dependency of b,/ and  nt on k, these parameters are indeed affected by the value of k. For convenience,  we express our approximation model in vector-matrix form:  yl,1  ' ' Yl,c  y2,1 ' "Y2,c  yN,1 ' ' ' yN,c  1 Xl,1 ' ' ' Xl,d  I X2,1  '  X2,d  1 xN,1 -''xN,d  {(Xl, [1)''' {(Xl, [k)  {(X2, [1)'' ' {(X2, [k)  ak,1    ak,c  q- nl:N  1The software is available at http://www. cs .berkeley.edu/~jfgf.  2yl:N,l:c is an N by c matrix, where N is the number of data and c the number of  outputs. We adopt the notation yl:,j A (yl,j,y2,j,... ,y,j) to denote all the obser-  vations corresponding to the j-th output (j-th column of y). To simplify the notation,  y is equivalent to y,l:. That is, if one index does not 'appear, it is implied that we  are referring to all of its possible values. Similarly, y is equivalent to yl:N,l:c. We will  favour the shorter notation and only adopt the longer notation to avoid ambiguities and  emphasise certain dependencies.  Robust Full Bayesian Methods for Neural Networks 381  where the noise process is assumed to be normally distributed nt  JV'(0, cr) for  i - 1,... , c. In shorter notation, we have:  y = D(t:k,,:a,X,:V,::a)C:+a+k,:c + nt (3)  We assume here that the number k of RBFs and their parameters 0   {a:m,:c,th:k,:a, er12:c}, with m = 1 + d + k, are unknown. Given the data set  {x,y}, our objective is to estimate k and 0 C Ok.  3 Bayesian model and aims  We follow a Bayesian approach where the unknowns k and 0 are regarded as being  drawn from appropriate prior distributions. These priors reflect our degree of belief  on the relevant values of these quantities [9]. Furthermore, we adopt a hierarchical  prior structure that enables us to treat the priors' parameters (hyper-parameters)  as random variables drawn from suitable distributions (hyper-priors). That is, in-  stead of fixing the hyper-parameters arbitrarily, we acknowledge that there is an  inherent uncertainty in what we think their values should be. By devising proba-  bilistic models that deal with this uncertainty, we are able to implement estimation  techniques that are robust to the specification of the hyper-priors.  The overall parameter space O x  can be written as a finite union of sub-  kmax Ok) X  where 0o a (IRa+l) c x (I +)c and  spaces O x  = (Uk= 0{k} X =  Ok -- (a++) X (+) X  for k  {1,... ,kmax}. That is, a  (a++),  a  (+) and   . The hyper-parmeter space   (+)+, with ele-  ments  A {A,52}, will be discussed at the end of this section. The space of  the radial basis cemres  is defined as a compact set including the input data:  k  {;l:k,i  [min(xl:N,i)--tEi,m(Xl:N,i)+tEi] k for/= 1,... ,d with j,i   "t,i for j  l}. Ei = ]1 m(xl:,i) - min(xl:,i)l] denotes the Euclidean distance  for the i-th dimension of the input and, is a user specified parameter that we only  need to consider if we wish to place basis functions outside the region where the  input data lie. That is, we allow  to include the space of the input data and  extend it by a factor which is proportional to the spread of the input data. The  : d k  hyper-volume of this space is:  a (i=l (1 + 2,)Ei) .  The mimum number of basis functions is defined  kma  (N - (d + 1)) We  also define  a ' (k '  = V=o t I x  with o  . Under the sumption of indepen-  dent outputs given (k, 0), the likelihood p(ylk, O, , x) for the approximation model  described in the previous section is:  exp 2a (Yl:'i- D(l:,X)al:m,i) (yl:,i- D(l:,X)al:m,i  i=1  We assume the following structure for the prior distribution:  p(k,O,) = p(al:lk, a 2,52)p(u:}k)p(k}A)p(e)p(A)p(52 )  2  where the scale parameters a, are assumed to be independent of the hyper-  parameters (i.e. p(a2lA, 5 2) = p(a2)), independent of each other (p(a 2) =  c P(i)) and distributed according to conjugate inverse-Gamma prior distri-  i=1 2  2  Z( ). When Wo = 0 and o = O, we obtain Jeeys' un-  butions: i 2 ,   2  informative prior [9]. For a given a2 the prior distribution p(k, a:m, :[ , A, 62)  is:  [i 22 I ] [ln(tr,:) ] / 1  12xriSilml-12exp( 2 2[:m,il:m'i)  382 C. Andrieu, J. E G. d. Freitas and A. Doucet  where Im denotes the identity matrix of size m x m and ]In(k, ul:k) is the indicator  function of the set F (1 if (k, u:k)  F, 0 otherwise).  The prior model order distribution p(klA) is a truncated Poisson distribution. Con-  ditional upon k, the RBF centres are uniformly distributed. Finally, conditional  upon (k,u:), the coefficients ot:m,i are assumed to be zero-mean Gaussian with  2 2  variance 5ier i . The hyper-parameters 52  (IR+) c and A  IR + can be respec-  tively interpreted as the expected signal to noise ratios and the expected num-  ber of radial basis. We assume that they are independent of each other, i.e.  p(A, 52) = p(A)p(52). Moreover, p(52) = 1-IiC=l p(5/2). As 52is a scale parameter,  we ascribe a vague conjugate prior density to it: 5/2 - :/;(aa2, fia2) for i = 1,... , c,  with aa2 = 2 and fia2> 0. The variance of this hyper-prior with aa2 = 2 is infinite.  We apply the same method to A by setting an uninformative conjugate prior [9]:  A6a(1/2--1,2) (i << 1i= 1,2).  3.1 Estimation and inference aims  The Bayesian inference of k, 0 and p is based on the joint posterior distribution  p(k, O, p[x, y) obtained from Bayes' theorem. Our aim is to estimate this joint dis-  tribution from which, by standard probability marginalisation and transformation  techniques, one can "theoretically" obtain all posterior features of interest. We  propose here to use the reversible jump MCMC method to perform the necessary  computations, see [8] for details. MCMC techniques were introduced in the mid  1950's in statistical physics and started appearing in the fields of applied statis-  tics, signal processing and neural networks in the 1980's and 1990's [3,5,6,10,11].  The key idea is to build an ergodic Markov chain (k(i),o(i),(i))iN whose equi-  librium distribution is the desired posterior distribution. Under weak additional  assumptions, the P >> 1 samples generated by the Markov chain are asymptotically  distributed according to the posterior distribution and thus allow easy evaluation  of all posterior features of interest. For example:  P  1 yi{j}(k(i)) and (01k - j,x,y) - /P=I o(i)[{J}(k(i)) (4)  (k - jlx, y) =  i= E/P=I ][{J}(k(i))  In addition, we can obtain predictions, such as:  P  ^ 1  rt, (i),xv+).(i)  E(YN+llXI:N+I,Yl:N) ----  -'\l:k -l:m  i=1  (5)  3.2 Integration of the nuisance parameters  According to Bayes theorem, we can obtain the posterior distribution as follows:  p(k, O, blx, y ) or p(ylk, O, b,x)p(k, O, b)  In our case, we can integrate with respect to c:m (Gaussian distribution) and with  2 (inverse Gamma distribution) to obtain the following expression for  respect to  the posterior:  2 --m/2 ]1/2 q- Yl:N,iPi,kYl:N,i N+O  p(k, u1:,A, 521x, y) or () IM, 2 )(-  ) x  ' (- ) (-  = =  Robust Full Bayesian Methods for Neural Networks 383  It is worth noticing that the posterior distribution is highly non-linear in the RBF  centres/z k and that an expression of p(klx , y) cannot be obtained in closed-form.  4 Geometric convergence theorem  It is easy to prove that the reversible jump MCMC algorithm applied to our model  converges, that is, that the Markov chain (k (i) u (i) A(0 52(0) is ergodic. We  ' l:k' '  present here a stronger result, namely that (k (i),/(i) A(i), 52(0) converges to  l:k' i&N  the required posterior distribution at a geometric rate:  Theorem 1 Let (k (i) ' (i) A(i) 52(0)  ' l:k' '  kernel has been described in Section 3.  bility distribution p (k,/:k, A, 521 x, y)  be the Markov chain whose transition  This Markov chain converges to the proba-   Furthermore this convergence occurs at a  geometric rate, that is, for almost every initial point (k ()' (0) A(0), 52(0)) C  x   , ltl: k ,  there exists a function of the initial states Co > 0 and a constant and p  [0, 1) such  that  p(i) (k,/l:,A, 52) _p(k,l.x:,A, 521x, Y) TV --< Cp[i/mxJ (7)  where p(i) (k,/l:,A ,62) is the distribution of (k (i)  (i) A(i) ,52(0) and II'lITv is  ' l:k'  the total variation norm [11]. Proof. See [8]   Corollary 1 If for each iteration i one samples the nuisance parameters (Ol:m, O')  then the distribution of the series (k(i), ,(i) I (i) 2(i) A(i) 52(i))ier converges ge-  -l:m, l:k,O'k , ,  ometrically towards p(k, cx:m,/x:k, er 2 A, 521x, y) at the same rate p.  k,  5 Demonstration: robot arm data  This data is often used as a benchmark to compare learning algorithms 3. It involves  implementing a model to map the joint angle of a robot arm (Xl, x2) to the position  of the end of the arm (y, Y2). The data were generated from the following model:  = 2.0cos(x) + 1.3cos(x +x2) + q  Y2 = 2.0sin(xx) + 1.3sin(x +x2) +e2  where ei  Af(0, a2); a = 0.05. We use the first 200 observations of the data set  to train our models and the last 200 observations to test them. In the simulations,  we chose to use cubic basis functions. Figure I shows the 3D plots of the training  data and the contours of the training and test data. The contour plots also in-  clude the typical approximations that were obtained using the algorithm. We chose  uninformative priors for all the parameters and hyper-parameters (Table 1). To  demonstrate the robustness of our algorithm, we chose different values for fi52 (the  only critical hyper-parameter as it quantifies the mean of the spread/i of c). The  obtained mean square errors and probabilities for 61, 62, cr 2 cr 2 and k, shown in  1,k' 2,k  Figure 2, clearly indicate that our algorithm is robust with respect to prior specifi-  cation. Our mean square errors are of the same magnitude as the ones reported by  other researchers [2,3,5,6]; slightly better (Not by more than 10%). Moreover, our  algorithm leads to more parsimonious models than the ones previously reported.  aThe robot arm data set can be found in David Mackay's home page:  http://wol. ra. phy. cam. ac. uk/mackay/  384 C. Andrieu, d. E G. d. Freitas and A. Doucet  4 2 ' - '""2  x2 o -2 xl  4  -1 0 1 2  x2 xl  -2 -1 0 1  -1 0 1 2 -2 -1 0 1  Figure 1: The top plots show the training data surfaces corresponding to each  coordinate of the robot arm's position. The middle and bottom plots show the  training and validation data [- -] and the respective RBF network mappings [--].  Table 1- Simulation parameters and mean square test errors.  c5 fi5 v0 "/0 1 e2 MS ERROR  2 0.1 0 0 0.0001 0.0001 0.00505  2 10 0 0 0.0001 0.0001 0.00503  2 100 0 0 0.0001 0.0001 0.00502  6 Conclusions  We presented a general methodology for estimating, jointly, the noise variance, pa-  rameters and number of parameters of an RBF model. In adopting a Bayesian  model and the reversible jump MCMC algorithm to perform the necessary integra-  tions, we demonstrated that the method is very accurate. Contrary to previous  reported results, our experiments indicate that our model is robust with respect  to the specification of the prior. In addition, we obtained more parsimonious RBF  networks and better approximation errors than the ones previously reported in the  literature. There are many avenues for further research. These include estimating  the type of basis functions, performing input variable selection, considering other  noise models and extending the framework to sequential scenarios. A possible so-  lution to the first problem can be formulated using the reversible jump MCMC  flamework. Variable selection schemes can also be implemented via the reversible  jump MCMC algorithm. We are presently working on a sequential version of the  algorithm that allows us to perform model selection in non-stationary environments.  References  [1] Buntine, W.L. & Weigend, A.S. (1991) Bayesian back-propagation.  5:603-643.  Complex Systems  Robust Full Bayesian Methods for Neural Networks 385  0 0.2  II  x 0.1  C)  0 0.2  II  :: 0.1  2  and 2  o  o lOO  lOO  0.0  0.0  0.0;  lOO  200  O.C  O.C  O.C  200  O.C  O.C  O.C  o  o 200  land'2 k  o 2 4  x 10 -  2 4  xO -  o 2 4  x 10 "  0.8 I  0.6  0.4  0.2  0  12  0.8 I  0.6  0.4  0.2  0  12  Oo  O,  O.  O,  12  I I .  14 16  I I -  14 16  14 16  Figure 2: Histograms of smoothness constraints (61 and 62), noise variances  and  r2,k) and model order (k) for the robot arm data using 3 different values for  fi. The plots confirm that the algorithm is robust to the setting of  [2] Mackay, D.J.C. (1992) A practical Bayesian framework for backpropagation networks.  Neural Computation 4:448-472.  [3] Neal, R.M. (1996) Bayesian Learning for Neural Networks. New York: Lecture Notes  in Statistics No. 118, Springer-Verlag.  [4] de Freitas, J.F.G., Niranjan, M., Gee, A.H. & Doucet, A. (1999) Sequential Monte  Carlo methods to train neural network models. To appear in Neural Computation.  [5] Rios Insua, D. & Mfiller, P. (1998) Feedforward neural networks for nonparametric  regression. Technical report 98-02. Institute of Statistics and Decision Sciences, Duke  University, http://,,r,-. star. duke. edu.  [6] Holmes, C.C. & Mallick, B.K. (1998) Bayesian radial basis functions of variable dimen-  sion. Neural Computation 10:1217-1233.  [7] Green, P.J. (1995) Reversible jump Markov chain Monte Carlo computation and  Bayesian model determination. Biometrika 82:711-732.  [8] Andrieu, C., de Freitas, J.F.G. & Doucet, A. (1999) Robust full Bayesian learning  for neural networks. Technical report CUED/F-INFENG/TR 343. Cambridge University,  http://svr-www. eng. cam. ac. uk/.  [9] Bernardo, J.M. & Smith, A.F.M. (1994) Bayesian Theory. Chichester: Wiley Series in  Applied Probability and Statistics.  [10] Besag, J., Green, P.J., Hidgon, D. & Mengersen, K. (1995) Bayesian computation and  stochastic systems. Statistical Science 10:3-66.  [11] Tierney, L. (1994) Markov chains for exploring posterior distributions. The Annals of  Statistics. 22(4):1701-1762.  
The Relaxed Online  Maximum Margin Algorithm  Yi Li and Philip M. Long  Department of Computer Science  National University of Singapore  Singapore 119260, Republic of Singapore  { liyi, plong} @comp. nus. edu. sg  Abstract  We describe a new incremental algorithm for training linear thresh-  old functions: the Relaxed Online Maximum Margin Algorithm, or  ROMMA. ROMMA can be viewed as an approximation to the algorithm  that repeatedly chooses the hyperplane that classifies previously seen ex-  amples correctly with the maximum margin. It is known that such a  maximum-margin hypothesis can be computed by minimizing the length  of the weight vector subject to a number of linear constraints. ROMMA  works by maintaining a relatively simple relaxation of these constraints  that can be efficiently updated. We prove a mistake bound for ROMMA  that is the same as that proved for the perceptron algorithm. Our analysis  implies that the more computationally intensive maximum-margin algo-  rithm also satisfies this mistake bound; this is the first worst-case perfor-  mance guarantee for this algorithm. We describe some experiments us-  ing ROMMA and a variant that updates its hypothesis more aggressively  as batch algorithms to recognize handwritten digits. The computational  complexity and simplicity of these algorithms is similar to that of per-  ceptron algorithm, but their generalization is much better. We describe a  sense in which the performance of ROMMA converges to that of SVM  in the limit if bias isn't considered.  1 Introduction  The perceptron algorithm [10, 11] is well-known for its simplicity and effectiveness in the  case of linearly separable data. Vapnik' s support vector machines (SVM) [13] use quadratic  programming to find the weight vector that classifies all the training data correctly and  maximizes the margin, i.e. the minimal distance between the separating hyperplane and the  instances. This algorithm is slower than the perceptron algorithm, but generalizes better.  On the other hand, as an incremental algorithm, the perceptron algorithm is better suited  for online learning, where the algorithm repeatedly must classify patterns one at a time,  then finds out the correct classification, and then updates its hypothesis before making the  next prediction.  In this paper, we design and analyze a new simple online algorithm called ROMMA (the  Relaxed Online Maximum Margin Algorithm) for classification using a linear threshold  The Relaxed Online Maximum Margin Algorithm 499  function. ROMMA has similar time complexity to the perceptron algorithm, but its gener-  alization performance in our experiments is much better on average. Moreover, ROMMA  can be applied with kernel functions.  We conducted experiments similar to those performed by Cortes and Vapnik [2] and Freund  and Schapire [3] on the problem of handwritten digit recognition. We tested the standard  perceptron algorithm, the voted perceptron algorithm (for details, see [3]) and our new  algorithm, using the polynomial kernel function with d = 4 (the choice that was best  in [3]). We found that our new algorithm performed better than the standard perceptron  algorithm, had slightly better performance than the voted perceptron.  For some other research with aims similar to ours, we refer the reader to [9, 4, 5, 6].  The paper is organized as follows. In Section 2, we describe ROMMA in enough detail  to determine its predictions, and prove a mistake bound for it. In Section 3, we describe  ROMMA in more detail. In Section 4, we compare the experimental results of ROMMA  and an aggressive variant of ROMMA with the perceptron and the voted perceptron algo-  rithms.  2 A mistake-bound analysis  2.1 The online algorithms  For concreteness, our analysis will concern the case in which instances (also called pat-  terns) and weight vectors are in R ' . Fix n G N. In the standard online learning model [7],  learning proceeds in trials. In the tth trial, the algorithm is first presented with an instance  t G R '. Next, the algorithm outputs a prediction )t of the classification of t. Finally,  the algorithm finds out the correct classification Yt G {-1, 1}. If )t  Yt, then we say that  the algorithm makes a mistake. It is worth emphasizing that in this model, when making  its prediction for the tth trial, the algorithm only has access to instance-classification pairs  for previous trials.  All of the online algorithms that we will consider work by maintaining a weight vector  which is updated between trials, and predicting )t = sign(tt  a7t), where sign(z) is 1 if  is positive, -1 if z is negative, and 0 otherwise.   The perceptton  off with tl: 0.  by tt + 1 = tt +  algorithm. The perceptron algorithm, due to Rosenblatt [10, 11], starts  When its prediction differs from the label Yt, it updates its weight vector  Yt t. If the prediction is correct then the weight vector is not changed.  The next three algorithms that we will consider assume that all of the data seen by the  online algorithm is collectively linearly separable, i.e. that there is a weight vector if such  that for all each trial t, Yt '- sign(if. at). When kernel functions are used, this is often the  case in practice.  The ideal online maximum margin algorithm. On each trial t, this algorithm chooses a  weight vector tt for which for all previous trials s _< t, sign(tt  a7s) = ys, and which  maximizes the minimum distance of any  to the separating hyperplane. It is known [ 1, 14]  that this can be implemented by choosing v7t to minimize [[tt]] subject to the constraints  that y (tt  a?) >_ 1 for all s _< t. These constraints define a convex polyhedron in weight  space which we will refer to as Pt.  The relaxed online maximum margin algorithm. This is our new algorithm. The first  difference is that trials in which mistakes are not made are ignored. The second difference  IThe prediction of 0, which ensures a mistake, is to make the proofs simpler. The usual mistake  bound proof for the perceptron algorithm goes through with this change.  500 Y. Li and P M. Long  is in how the algorithm responds to mistakes. The relaxed algorithm starts off like the ideal  algorithm. Before the second trial, it sets t2 to be the shortest weight vector such that  li (t2  1) _> 1. If there is a mistake on the second trial, it chooses ta as would the ideal  algorithm, to be the smallest element of  {t' Yl(Z' '1) _ 1} V1 {' Y2(-2) _> 1}.  (1)  However, if the third trial is a mistake, then it behaves differently. Instead of choosing  to be the smallest element of  {-' Yl( ' '1) _ 1} F1 {t  y2(t. '2) _> 1} F3 {t' ya(t-3) _> 1},  it lets 4 be the smallest element of  origin  Figure 1: In ROMMA, a convex  polyhedron in weight space is re-  placed with the halfspace with the  same smallest element.  IIall 2} n 1}.  This can be thought of as, before the third trial, replacing the polyhedron defined by (1)  with the halfspace {t  ta. tg >_ [[ta[[ 2} (see Figure 1).  Note that this halfspace contains the polyhedron  of (1); in fact, it contains any convex set whose  smallest element is ta. Thus, it can be thought of  as the least restrictive convex constraint for which  the smallest satisfying weight vector is ta. Let  us call this halfspace Ha. The algorithm contin-  ues in this manner. If the tth trial is a mistake,  then tt+ is chosen to be the smallest element of  Ht 91 { t  yt ( t . t ) >_ 1}, and Ht + l is set to be   _ t 1 2  { t+'> [[ + [[ }. If the tth trial is not a  mistake, then tt+ = t and Ht+ -' Hr. We will  call Ht the old constraint, and {t  yt(t  't) 2 1}  the new constraint.  Note that after each mistake, this algorithm needs only to solve a quadratic programming  problem with two linear constraints. In fact, there is a simple closed-form expression for  tt+ as a function of tt, :t and lit that enables it to be computed incrementally using time  similar to that of the perceptron algorithm. This is described in Section 3.  The relaxed online maximum margin algorithm with aggressive updating. The algo-  rithm is the same as the previous algorithm, except that an update is made after any trial in  which lit(tt  t) < 1, not just after mistakes.  2.2 Upper bound on the number of mistakes made  Now we prove a bound on the number of mistakes made by ROMMA. As in previous  mistake bound proofs (e.g. [8]), we will show that mistakes result in an increase in a  "measure of progress", and then appeal to a bound on the total possible progress. Our  proof will use the squared length of t as its measure of progress.  First we will need the following lemmas.  Lemma 1 On any run of ROMMA on linearly separable data, if trial t was a mistake, then  the new constraint is binding at the new weight vector, i.e. !It (tt+  t) = 1.  Proof: For the purpose of contradiction suppose the new constraint is not binding at the  new weight vector tt+l. Since tt fails to satisfy this constraint, the line connecting t+l  and t intersects with the border hyperplane of the new constraint, and we denote the  intersecting point tq. Then tq can be represented as tq = at+(1 -ct)tt+i, 0 < ct < 1.  The Relaxed Online Maximum Margin Algorithm 501  Since the square of Euclidean length ll' II 2 is a convex function, the following holds:  llql[ = < llttll = + (1- )ll,t+ll =  -' 2  Since tt is the unique smallest member of Ht and t+  tt, we have I[tt][ = < []wt+l ]] ,  which implies  llll = < II,t+11 = (2)  Since tt and tt+ are both in Ht, tq is too, and hence (2) contradicts the definition of  tt+. [-]  Lemma 2 On any run of ROMMA on linearly separable data, if trial t was a mistake, and  not the first one, then the old constraint is binding at the new weight vector, i.e. tt +   tt =  -' 2  Ilwtll   Proof: Let At be the plane of weight vectors that make the new constraint tight, i.e. At --  = . --  be the element  {t  lit(t- t) 1} By Lemma 1, tFt+  At. Let gt !It t/lltll =  of At that is perpendicular to it. Then each tF  At satisfies IIll  -- IItll  q- II - tll 2,  and therefore the length of a vector t in At is minimized when t - gt and is monotone  in the distance from t to gr. Thus, if the old constraint is not binding, then tt+ -- gt,  since otherwise the solution could be improved by moving tt+ a little bit toward gr. But  the old constraint requires that (tt  tt+x) _> IItll =, and if tt+l = 7t = lItxt/lltll, this  t . Rearranging, we get lIt(tt. 3t) _> Ilr. tll=lltll = > 0  means that tt. (lItt/lltll 2) >_ II tll=  ([[zt[[ > 0 follows from the fact that the data is linearly separable, and IIwtll > 0 follows  from the fact that there was at least one previous mistake). But since trial t was a mistake,  lIt (tt  t) <_ 0, a contradiction. [-]  Now we' re ready to prove the mistake bound.  Theorem 3 Choose m  N, and a sequence (  , lI ) , . . . , ( , , li, ) of pattern-  classification pairs in R n x {-1, +1}. Let/r/ =maxt IIr'tll. f there is a weight vector  ff such that lit ( ff  gt ) >_ 1 for all 1 _< t < m, then the number of mistakes made by  ROMMA on (, li), . . . , (m, l/m) is at most WII11 .  Proof: First, we claim that for all t, ff G Hr. This is easily seen since ff satisfies all the  constraints that are ever imposed on a weight vector, and therefore all relaxations of such  constraints. Since tt is the smallest element of Ht, we have II t ll _<  =  = 1/R which implies IIV=ll ' >  We have  xs/llxll  and therefore II =l X/llsll   " 2  1/R e. We claim that if any trial t > 1 is a mistake, then IIt+sll  IIll + x/W. This  will imply by induction that after M mistakes, the squed length of the algorithm' s weight  vector is at least M/R e, which, since all of the algorithm's weight vectors e no longer  than , will complete the proof.  Bt  Figure 2: At, Bt , and Pt  Choose an index t > 1 of a trial in which a mistake  is made. Let At = {t: lit (t' Zt) = 1} and Bt =  {: (. t) - 11,ll}. By Lemmas 1 and 2,  tt +   At ffl tt.  The distance from tt to At (call-it Pt) satisfies  IItll -II,l---j -> ' (3)  since the fact that there was a mistake in trial t im-  plies lIt ('t  tt) _< 0. Also, since tt+  At,  [[Wtq-1- wtll _> pt. (4)  502 Y. Li and P M. Long  Because tt is the normal vector of Bt and tt+l G Bt, we have  [l+111  -II[I 2 + [1+1 - 11    -IIll --I1+1- ell  _> d >_ 1/R2,  Thus, applying (3) and (4), we have II t+lll e- e -  which, as discussed above, completes the proof. [-]  Using the fact, easily proved using induction, that for all t, Pt C_ Ht, we can easily prove  the following, which complements analyses of the maximum margin algorithm using inde-  pendence assumptions [1, 14, 12]. Details are omitted due to space constraints.  Theorem 4 Choose m e N, and a sequence (1, Yl),'", (7m, Ym) of pattern-  classification pairs in R n x {-1, +1}. Let R =maxt IItll. If there is a weight vector  ff such that Yt(ff  gt) _> l for all 1 < t _< rn, then the number of mistakes made by the  ideal online maximum margin algoritmon (l, Yl), " ' , (gin, Ym) is at most R2 IIll e.  In the proof of Theorem 3, if an update is made and yt (tt  gt) < 1 - d instead of yt (tt .  zt) _< 0, then the progress made can be seen to be at least de/R e. This can be applied to  prove the following.  Theorem 5 Choose d > O, rn e N, and a sequence ( gl , Yi ) , ' " , (gin, Ym ) of pattern-  classification pairs in R n x {- 1, + 1 }. Let R =maxt [lt 1[. If there is a weight vector ff  such that Yt (if' ;t )  1 for all 1 < t < m, then if ( 1, Yl ),''', ( grn, Yrn ) are presented on  line the number of trials in which aggressive ROMMA has yt(tt  t) < 1 -- d is at most  Theorem 5 implies that, in a sense, repeatedly cycling through a dataset using aggressive  ROMMA will eventually converge to SVM; note however that bias is not considered.  3 An efficient implementation  When the prediction of ROMMA differs from the expected label, the algorithm chooses   to At+l = . --  tt+l to minimize II t+ll] subject b, where A = and b -  (titStile / . Simple calculation shows that  t+l  = Ar(AAr)-xb  _ (IItllelltll e  yt(ft &t)  IIt ,lell II e- (t:--) *+ (  t e t t  11 11 (y - (   (5)  II'=11'll=-u'("') and dt --  If on trials t in which a mistake is made, ct =  II*[l=(u*-(*'*)) and on other trials ct = landdt = 0, then always tt+l = ctttq-dt t  lit-, II 11, II 3_ (, .r-,) ,   Note that based on Lemmas 1 and 2, the denominators in (5) will never be equal to zero.  Since the computations required by ROMMA involve inner products together with a few  operations on scalars, we can apply the kernel method to our algorithm, efficiently solving  the original problem in a very high dimensional space. Computationally, we only need to  modify the algorithm by replacing each inner product computation (i  j) with a kernel  function computation/E(Zi, ;j).  To make a prediction for the tth trial, the algorithm must compute the inner product between  t and prediction vector tt. In order to apply the kernel function, as in [1, 3], we store each  prediction vector 7t in an implicit manner, as the weighted sum of examples on which  The Relaxed Online Maximum Margin Algorithm 503  mistakes occur during the training. In particular, each tt is represented as  \j= j= \,=j+  The above formula may seem daunting; however, making use of the recurrence (tt+ 1  ;) --  ct (t  :if) + dt (t  Z), it is obvious that the complexity of our new algorithm is similar to  that of perceptton algorithm. This was born out by our experiments.  The implementation for aggressive ROMMA is similar to the above.  4 Experiments  We did some experiments using the ROMMA and aggressive ROMMA as batch algorithms  on the MNIST OCR database. 2 We obtained a batch algorithm from our online algorithm  in the usual way, making a number of passes over the dataset and using the final weight  vector to classify the test data.  Every example in this database has two parts, the first is a 28 x 28 matrix which rep-  resents the image of the corresponding digit. Each entry in the matrix takes value from  {0,..-, 255}. The second part is a label taking a value from {0,.-., 9}. The dataset  consists of 60,000 training examples and 10,000 test examples. We adopt the following  polynomial kernel: K (Zi, Zj): (1 + (i- j)) /. This corresponds to using an expanded  collection of features including all products of at most d components of the original fea-  ture vector (see [14]). Let us refer to the mapping from the original feature vector to the  expanded feature vector as . Note that one component of () is always 1, and therefore  the component of the weight vector corresponding to that component can be viewed as a  bias. In our experiments, we set t = () rather than  to speed up the learning of the  coefficient corresponding to the bias. We chose d = 4 since in experiments on the same  problem conducted in [3, 2], the best results occur with this value.  To cope with multiclass data, we trained ROMMA and aggressive ROMMA once for each  of the 10 labels. Classification of an unknown pattern is done according to the maximum  output of these ten classifiers.  As every entry in the image matrix takes value from {0,  ., 255}, the order of magnitude  of K (, ) is at least 1026, which might cause round-offerror in the computation of ci and  di. We scale the data by dividing each entry with 1100 when training with ROMMA.  Table 1: Experimental results on MNIST data  T=I T=2 T=3 T=4  Err MisNo Err MisNo Err MisNo Err MisNo  percep 2.84 7970 2.27 10539 1.99 11945 1.85 12800  voted-percep 2.26 7970 1.88 10539 1.76 11945 1.69 12800  ROMMA 2.48 7963 1.96 9995 1.79 10971 1.77 11547  agg-ROMMA 2.14 6077 1.82 7391 1.71 7901 1.67 8139  agg-ROMMA (NC) 2.05 5909 1.76 6979 1.67 7339 1.63 7484  Since the performance of online learning is affected by the order of sample sequence, all  the results shown in Table 1 average over 10 random permutations. The columns marked  2National Institute for Standards and Technology, special database 3.  http://www. research.att.com/yann/ocr for information on obtaining this dataset.  See  504 Y. Li and P M. Long  "MisNo" in Table 1 show the total number of mistakes made during the training for the 10  labels. Although online learning would involve only one epoch, we present results for a  batch setting until four epochs (T in Table 1 represents the number of epochs).  To deal with data which are linearly inseparable in the feature space, and also to improve  generalization, Friess et al [4] suggested the use of quadratic penalty in the cost function,  which can be implemented using a slightly different kernel function [4, 5]: ](:k, :j) =  ]C (:k, :j) + 6j, where j is the Kronecker delta function. The last row in Table 1 is the  result of aggressive ROMMA using this method to control noise ( = 30 for 10 classifiers).  We conducted three groups of experiments, one for the perceptron algorithm (denoted "per-  cep"), the second for the voted perceptron (denoted "voted-percep") whose description is  in [3], the third for ROMMA, aggressive ROMMA (denoted "agg-ROMM'), and aggres-  sive ROMMA with noise control (denoted "agg-ROMMA(NC)"). Data in the third group  are scaled. All three groups set t = ().  The results in Table 1 demonstrate that ROMMA has better performance than the standard  perceptron, aggressive ROMMA has slightly better performance than the voted perceptron.  Aggressive ROMMA with noise control should not be compared with perceptrons without  noise control. Its presentation is used to show what performance our new online algorithm  could achieve (of course it's not the best, since all 10 classifiers use the same ). A remark-  able phenomenon is that our new algorithm behaves very well at the first two epochs.  References  [1] B. E. Boser, I. M. Guyon, and V. N. Vapnik. A training algorithm for optimal margin classifiers.  Proceedings of the 1992 Workshop on Computational Learning Theory, pages 144-152, 1992.  [2] C. Cortes and V. Vapnik. Support-vector networks. Machine Learning, 20(3):273-297, 1995.  [3] Y. Freund and R. E. Schapire. Large margin classification using the perceptton algorithm.  Proceedings of the 1998 Conference on Computational Learning Theory, 1998.  [4] T T. Friess, N. Cristianini, and C. Campbell. The kernel adatron algorithm: a fast and simple  learning procedure for support vector machines. In Proc. 15th lnt. Conf. on Machine Learning.  Morgan Kaufman Publishers, 1998.  [5] S.S. Keerthi, S. K. Shevade, C. Bhattacharyya, and K. R. K. Murthy. A fast iterative nearest  point algorithm for support vector machine classiifer design. Technical report, Indian Institute  of Science, 99. TR-ISL-99-03.  [6] Adam Kowalczyk. Maximal margin perceptton. In Smola, Bartlett, Scholkopf, and Schuur-  mans, editors, Advances in Large Margin Classifiers, 1999. MIT-Press.  [7] N. Littlestone. Learning quickly when irrelevant attributes abound: a new linear-threshold  algorithm. Machine Learning, 2:285-318, 1988.  [8] N. Littlestone. Mistake Bounds and Logarithmic Linear-threshold Learning Algorithms. PhD  thesis, UC Santa Cruz, 1989.  [9] John C. Platt. Fast training of support vector machines using sequential minimal optimization.  In B. Scholkopf, C. Burges, A. Smola, editors, Advances in Kernel Methods: Support Vector  Machines, 1998. MIT Press.  [10] F. Rosenblatt. The perceptton: A probabilistic model for information storage and organization  in the brain. PsychologicalReview, 65:386-407, 1958.  [ 11] F. Rosenblatt. Principles ofNeurodynamics: Perceptrons and the Theory of Brain Mechanisms.  Spartan Books, Washington, D.C., 1962.  [12] J. Shawe-Taylor, P. Bartlett, R. Williamson, and M. Anthony. A framework for structural risk  minimization. In Proc. of the 1996 Conference on Computational Learning Theory, 1996.  [ 13] V. N. Vapnik. Estimation of Dependencies based on Empirical Data. Springer Verlag, 1982.  [14] V. N. Vapnik. The Nature of StatisticalLearning Theory. Springer, 1995.  
Memory Capacity of Linear vs. Nonlinear  Models of Dendritic Integration  Panayiota Poirazi*  Biomedical Engineering Department  University of Southern California  Los Angeles, CA 90089  poiraziscf . usc. edu  Bartlett W. Mel*  Biomedical Engineering Department  University of Southern California  Los Angeles, CA 90089  mel @lnc. us c. edu  Abstract  Previous biophysical modeling work showed that nonlinear interac-  tions among nearby synapses located on active dendritic trees can  provide a large boost in the memory capacity of a cell (Mel, 1992a,  1992b). The aim of our present work is to quantify this boost by  estimating the capacity of (1) a neuron model with passive den-  dritic integration where inputs are combined linearly across the  entire cell followed by a single global threshold, and (2) an active  dendrite model in which a threshold is applied separately to the  output of each branch, and the branch subtotals are combined lin-  early. We focus here on the limiting case of binary-valued synaptic  weights, and derive expressions which measure model capacity by  estimating the number of distinct input-output functions available  to both neuron types. We show that (1) the application of a fixed  nonlinearity to each dendritic compartment substantially increases  the model's flexibility, (2) for a neuron of realistic size, the capacity  of the nonlinear cell can exceed that of the same-sized linear cell by  more than an order of magnitude, and (3) the largest capacity boost  occurs for cells with a relatively large number of dendritic subunits  of relatively small size. We validated the analysis by empirically  measuring memory capacity with randomized two-class classifica-  tion problems, where a stochastic delta rule was used to train both  linear and nonlinear models. We found that large capacity boosts  predicted for the nonlinear dendritic model were readily achieved  in practice.  *http://lnc.usc.edu  158 P Poirazi and B. 144. Mel  Introduction  Both physiological evidence and connectionist theory support the notion that in  the brain, memories are stored in the pattern of learned synaptic weight values.  Experiments in a variety of neuronal preparations however, ind, icate that the ef-  ficacy of synaptic transmission can undergo substantial fluctuations up or down,  or both, during brief trains of synaptic stimuli. Large fluctuations in synaptic ef-  ficacy on short time scales seem inconsistent with the conventional connectionist  assumption of stable, high-resolution synaptic weight values. Furthermore, a recent  experimental study suggests that excitatory synapses in the hippocampus--a region  implicated in certain forms of explicit memory--may exist in only a few long-term  stable states, where the continuous grading of synaptic strength seen in standard  measures of long-term potentiation (LTP) may exist only in the average over a large  population of two-state synapses with randomly staggered thresholds for learning  (Petersen, Malenka, Nicoll, & Hopfield, 1998). According to conventional connec-  tionist notions, the possibility that individual synapses hold only one or two bits of  long-term state information would seem to have serious implications for the storage  capacity of neural tissue. Exploration of this question is one of the main themes of  this paper.  In a related vein, we have found in previous biophysical modeling studies that  nonlinear interactions between synapses co-activated on the same branch of an ac-  tive dendritic tree could provide an alternative form of long-term storage capacity.  This capacity, which is largely orthogonal to that tied up in conventional synaptic  weights, is contained instead in the spatial permutation of synaptic connections  onto the dendritic tree which could in principle be modified in the course of learn-  ing or development (Mel, 1992a, 1992b). In a more abstract setting, we recently  showed that a large repository of model flexibility lies in the choice as to which of  a large number of possible interaction terms available in high dimension is actually  included in a learning machine's discriminant function, and that the excess capac-  ity contained in this "choice flexibility" can be quantified using straightforward  counting arguments (Poirazi & Mel, 1999).  2 Two Alternative Models of Dendritic Integration  In this paper, we use a similar function-counting approach to address the more  biologically relevant case of a neuron with multiple quasi-independent dendritic  compartments (fig. 1). Our primary objective has been to compare the memory  capacity of a cell assuming two different modes of dendritic integration. According  to the linear model, the neuron's activation level a(x) prior to thresholding is  given by a weighted sum of of its inputs over the cell as a whole. According to the  nonlinear model, the k synaptic inputs to each branch are first combined linearly,  a static (e.g. sigmoidal) nonlinearity is applied to each of the rn branch subtotals,  and the resulting branch outputs are summed to produce the cell's overall activity  aN(x)..  aN(x) Ei= k  ---- m g(Ej=l X ) (1)  The expressions for a and aN were written in similar form to emphasize that the  models have an identical number of synaptic weights, differing only in the presence  or absence of a fixed nonlinear function g applied to the branch subtotals. Though  individual synaptic weights in both models are constrained to have a value of 1,  any of the d input lines may form multiple connections on the same or different  Memory Capacity of Linear vs. Nonlinear Models of Dendritic Integration 159  m 1  k  Figure 1: A cell is modeled as a set of m identical branches connected to a soma,  where each branch contains k synaptic contacts driven by one of d distinct input  lines.  branches as a means of representing graded synaptic strengths. Similarly, an input  line which forms no connection has an implicit weight of 0. In light of this restriction  to positive (or zero) weight values, both the linear and nonlinear models are split  into two opponent channels a + and a- dedicated to positive vs. negative coefficients,  respectively. This leads to a final output for each model:  yL(x) = sgn  yN(x): sgn - (2)  where the sgn operator maps the total activation level into a class label of {-1, 1}.  In the following, we derive expressions for the number of distinct parameter states  available to the linear rs. nonlinear models, a measure which we have found to be  a reliable predictor of storage capacity under certain restrictions (Poirazi & Mel  1999). Based on these expressions, we compute the capacity boost provided by  the branch nonlinearity as a function of the number of branches m, synaptic sites  per branch k, and input space dimensionality d. Finally, we test the predictions of  the analytical model by training both linear and nonlinear models on randomized  classification problems using a stochastic delta rule, and empirically measure and  compare the storage capacities of the two models.  3 Results  3.1 Counting Parameter States: Linear vs. Nonlinear Model  We derived expressions for B and BN, which estimate the total number of param-  eter bits available to the linear vs. nonlinear models, respectively:  ((k+d-1))  BN = 2 log 2 k + m - 1 B: 2 log 2 s + d - 1  8  m  These expressions estimate the number of non-redundant states in each neuron  type, i.e., those assignments of input lines to dendritic sites which yield distinct  160 P Poirazi and B. W. Mel  input-output functions YL or YN.  These formulae are plotted in figure 2A with d = 100, where each curve represents  a cell with a fixed number of branches (indicated by m). In each case, the capac-  ity increases steadily as the number of synapses per branch, k, is increased. The  logarithmic growth in the capacity of the linear model (evident in an asymptotic  analysis of the expression for B) is shown at the bottom of the graph (circles),  from which it may be seen that the boost in capacity provided by the dendritic  branch nonlinearity increases steadily with the number of synaptic sites. For a cell  with 100 branches containing 100 synaptic sites each, the capacity boost relative to  the linear model exceeds a factor of 20.  Figure 2B shows that for a given total number of synaptic sites, in this case s =  m. k = 10,000, the capacity of the nonlinear cell is maximized for a specific choice  of m and k. The peak of each of the three curves (computed for different values  of d) occurs for a cell containing 1,250 branches with 8 synapses each. However,  the capacity is only moderately sensitive to the branch count: the capacity of a cell  with 100 branches of 100 synapses each, for example, lies within a factor of two of  the optimal configuration. The linear cell capacities can be found at the far right  edge of the plot (m = 10,000), since a nonlinear model with one synapse per branch  has a number of trainable states identical to that of a linear model.  3.2 Validating the Analytical Model  To test the predictions of the analytical model, we trained both linear and non-  linear cells on randomized two-class classification problems. Training samples were  drawn from a 40-dimensional spherical Gaussian distribution and were randomly  assigned positive or negative labels--in some runs, training patterns were evenly  divided between positive and negative labels, with similar results. Each of the 40  original input dimensions was recoded using a set of 10 1-dimensional binary, non-  overlapping receptive fields with centers spaced along each dimension such that all  receptive fields would be activated equally often. This manipulation mapped the  original 40-dimensional learning problem into 400 dimensions, thereby increasing  the discriminability of the training samples. The relative memory capacity of linear  vs. nonlinear cells was then determined empirically by comparing the number of  training patterns learnable at a fixed error rate of 2%.  The learning rule used for both cell types was similar to the "clusteron" learning  rule described in (Mel, 1992a), and involved two mechanisms known to contribute to  neural development: (1) random activity-independent synapse formation, and (2)  activity-dependent synapse stabilization. In each iteration, a set of 25 synapses was  chosen at random, and the "worst" synapse was identified based on the correlation  over the training set of (i) the input's pre-synaptic activity, (ii) the post-synaptic  activity (i.e. the local nonlinear branch response for the nonlinear energy model or  a constant of I for the linear model), and (iii) a global "delta" signal with a value  of 0 if the cell responded correctly to the input pattern, or 4-1 if the cell responded  incorrectly. The poorest-performing synapse on the branch was then targeted for  replacement with a new synapse drawn at random from the d input lines. The  probability that the replacement actually occurred was given by a Boltzmann equa-  tion based on the difference in the training set error rates before and after the  replacement. A "temperature" variable was gradually lowered over the course of  the simulation, which was terminated when no further improvement in error rates  was seen.  Results of the learning runs are shown in fig. 3 where the analytical capacity (mea-  sured in bits) was scaled to the numerical capacity (measured in training patterns  Memory Capacity of Linear vs. Nonlinear Models of Dendritic Integration 161  A Capacity of Linear vs. Nonlinear B  8  7  Model for Various Geometries  x104  d = 100 m = 1000.  Nonlinear Model "  '/ Linear Mod_.el  2000 4000 6000 8000 10000  Total Synaptic Sites (  Capacity of Linear vs. Nonlinear Model  for Different Input Space Dimensions  x104  ta: oo  -?  s = 10,000  , Nonlinear Model  -,,  -,,  -,,. ,  d = 50.  2000 4000 6000 8000 10000  Number of Branches (m)  m  Figure 2: Comparison of linear vs. nonlinear model capacity as a function of branch  geometry. A. Capacity in bits for linear and several nonlinear cells with different  branch counts (for d = 100). For each curve indexed by branch count m, sites per  branch k increases from left to right as indicated iconically beneath the x-axis. For  all cells, capacity increases with an increasing number of sites, though the capacity  of the linear model grows logarithmically, leading to an increasingly large capacity  boost for the size-matched nonlinear cells. B. Capacity of a nonlinear model with  10,000 sites for different values of input space dimension d. Branch count m grows  along the x-axis. Cells at right edge of plot contain only one synapse per branch,  and thus have a number of modifiable parameters (and hence capacity) equivalent  to that of the linear model. All three curves show that there exist an optimal  geometry which maximizes the capacity of the nonlinear model (in this case 1,250  branches with 8 synapses each).  learned at 2% error). Two key features of the theoretical curves (dashed lines) are  echoed in the empirical performance curves (solid lines), including the much larger  storage capacity of the nonlinear cell model, and the specific cell geometry which  maximizes the capacity boost.  4 Discussion  We found using both analytical and numerical methods that in the limit of low-  resolution synaptic weights, application of a fixed output nonlinearity to each com-  partment of a dendritic tree leads to a significant boost in capacity relative to a  cell whose post-synaptic integration is linear. For example, given a cell with 10,000  synaptic contacts originating from 400 distinct input lines, the analysis predicts a  23-fold increase in capacity for the nonlinear cell, while numerical simulations using  a stochastic delta rule actually achieve a 15-fold boost.  Given that a linear and a nonlinear model have an identical number of synaptic con-  tacts with uniform synaptic weight values, what accounts for .the capacity boost?  The principal insight gained in this work is that the attachment of a fixed non-  linearity to each branch in a neuron substantially increases its underlying "model  162 P Poirazi and B. kE. Mel  x 102  40 I .................. \,  Analytical  (Bits/14)  Numerical  (Training Patterns  Nonlinear Model  30  2  1  Figure 3: Comparison of ca-  pacity boost predicted by analy-  sis vs. that observed empirically  when linear and nonlinear mod-  els were trained using the same  stochastic delta rule. Dashed  lines: analytical curves for lin-  ear vs. nonlinear model for a cell  with 10,000 sites show capacity  for varying cell geometries. Solid  lines: empirical performance for  same two cells at 2% error cri-  _ ,,,,.,, terion, using a subunit nonlin-  '- ",, earity g(X) ---- X 10 (similar re-  " sults were seen using a sigmoidal  nonlinearity, though the param-  'x,k,,x eters of the optimal sigmoid de-  pended on the cell geometry).  Linear Model "-,",, For both analytical and numeri-   .I-?-,-: _- ............ - 1A2cal curves, peak capacity is seen  0 10 20 30 40 50 60 70 80 90 100 u for cel] with 1,000branches (10  synapses per branch). Capacity  Number of Branches (m) exceeds that of same-sized linear  , model by a factor of 15 at the  rn peak, and by more than a factor  of 7 for cells ranging from about  3 to 60 synapses per branch (hor-  izontal dotted line).  flexibility", i.e. confers upon the cell a much larger choice of distinct input-output  relations from which to select during learning. This may be illustrated as follows.  For the linear model, branching structure is irrelevant so that y depends only on  the number of input connections formed from each of the d input lines. All spatial  permutations of a set of input connections are thus interchangeable and produce  identical cell responses. This massive redundancy confines the capacity of the linear  model to grow only logarithmically with an increasing number of synaptic sites (fig.  1A), an unfortunate limitation for a brain in which the formation of large num-  bers of synaptic contacts between neurons is routine. In contrast, the model with  nonlinear subunits contains many fewer redundancies: most spatial permutations  of the same set of input connections lead to non-identical values of YN, since an  input x swapped from branch bl to branch b2 leads to the elimination of the k - 1  interaction terms involving x on branch bl and the creation of k - 1 new interaction  terms on branch b2.  Interestingly, the particular form of the branch nonlinearity has virtually no effect  on the capacity of the cell as far as the counting arguments are concerned (though  it can have a profound effect on the cell's "representational bias"--see below), since  the principal effect of the nonlinearity in our capacity calculations is to break the  symmetry among the different branches.  The issue of representational bias is a critical one, however, and must be considered  when attempting to predict absolute or relative performance rates for particular  classifiers confronted with specific learning problems. Thus, intrinsic differences in  the geometry of linear vs. nonlinear discriminant functions mean that the param-  Memory Capacity of Linear vs. Nonlinear Models of Dendritic Integration 163  eters available to the two models may be better or worse suited to solve a given  learning problem, even if the two models were equated for total parameter flexibility.  While such biases are not taken into account in our analysis, they could nonetheless  have a substantial effect on measured error rates--and could thus throw a perfor-  mance advantage to one machine or the other. One danger is that performance  differences measured empirically could be misinterpreted as arising from differences  in underlying model capacity, when in fact they arise from differential suitability  of the two classifiers for the learning problem at hand. To avoid this difficulty, the  random classification problems we used to empirically assess memory capacity were  chosen to level the playing field for the linear vs. nonlinear cells, since in a previous  study we found that the coefficients on linear vs. nonlinear (quadratic) terms were  about equally efficient as features for this task. In this way, differences in measured  performance on these tasks were primarily attributable to underlying capacity dif-  ferences, rather than differences in representational bias. This experimental control  permitted more meaningful comparisons between our analytical and empirical tests  (fig. 3).  The problem of representational bias crops up in a second guise, wherein the an-  alytical expressions for capacity in eq. I can significantly overestimate the actual  performance of the cell. This occurs when a particular ensemble of learning prob-  lems fails to utilize all of the entropy available in the cell's parameter space for  example, by requiring the cell to visit only a small subset of its parameter states rel-  atively often. This invalidates the maximum parameter entropy assumption made  in the derivation of eq. 1, so that measured performance will tend to fall below  predicted values. The actual performance of either model when confronted with  an ensemble of learning problems will thus be determined by (1) the number of  trainable parameters available to the neuron (as measured by eq. 1), (2) the suit-  ability of the neuron's parameters for solving the assigned learning problems, and  (3) the utilization of parameters, which relates to the entropy in the joint proba-  bility of the parameter values averaged over the ensemble of learning problems. In  our comparisons here of linear and nonlinear cells, we we have calculated (1), and  have attempted to control for (2) and (3).  In conclusion, our results build upon the results of earlier biophysical simulations,  and indicate that in the limit of a large number of low-resolution synaptic weights,  nonlinear dendritic processing could nonetheless have a major impact on the storage  capacity of neural tissue.  References  Mel, B. W. (1992a). The clusteron: Toward a simple abstraction for a complex  neuron. In Moody, J., Hanson, S., & Lippmann, R. (Eds.), Advances in Neural  Information Processing Systems, vol. J, pp. 35-42.' Morgan Kaufmann, San  Mateo, CA.  Mel, B. W. (1992b). NMDA-based pattern discrimination in a modeled cortical  neuron. Neural Comp., J, 502-516.  Petersen, C. C. H., Malenka, R. C., Nicoll, R. A., & Hopfield, J. J. (1998). All-or-  none potentiation and CA3-CA1 synapses. Proc. Natl. Acad. Sci. USA, 95,  4732-4737.  Poirazi, P., & Mel, B. W. (1999). Choice and value flexibility jointly contribute to  the capacity of a subsampled quadratic classifier. Neural Comp., in press.  
Efficient Approaches to Gaussian Process  Classification  Lehel Csat6, Ernest Fokou, Manfred Opper, Bernhard Schottky  Neural Computing Research Group  School of Engineering and Applied Sciences  Aston University Birmingham B4 7ET, UK.  {opperm, csat ol}aston. ac. uk  Ole Winther  Theoretical Physics II, Lund University, S51vegatan 14 A,  S-223 62 Lund, Sweden  wintherthep. lu. se  Abstract  We present three simple approximations for the calculation of  the posterior mean in Gaussian Process classification. The first  two methods are related to mean field ideas known in Statistical  Physics. The third approach is based on Bayesian online approach  which was motivated by recent results in the Statistical Mechanics  of Neural Networks. We present simulation results showing: 1. that  the mean field Bayesian evidence may be used for hyperparameter  tuning and 2. that the online approach may achieve a low training  error fast.  I Introduction  Gaussian processes provide promising non-parametric Bayesian approaches to re-  gression and classification [2, 1]. In these statistical models, it is assumed that the  likelihood of an output or target variable y for a given input x  R N can be written  as P(yla(x)) where a: R S -- R are functions which have a Gaussian prior distri-  bution, i.e. a is (a priori) assumed to be a Gaussian random field. This means that  any finite set of field variables a(xi), i - 1,...,l are jointly Gaussian distributed  with a given covariance E[a(xi)a(xj)] - K(xi,xj) (we will also assume a zero mean  throughout the paper).  Predictions on a(x) for novel inputs x, when a set D of m training examples (xi, yi)  i - 1,..., m, is given, can be computed from the posterior distribution of the m + 1  variables a(x) and a(xl),...,a(x,). A major technical problem of the Gaussian  process models is the difficulty of computing posterior averages as high dimensional  integrals, when the likelihood is not Gaussian. This happens for example in classifi-  cation problems. So far, a variety of approximation techniques have been discussed:  Monte Carlo sampling [2], the MAP approach [4], bounds on the likelihood [3] and  a TAP mean field approach [5]. In this paper, we will introduce three different novel  methods for approximating the posterior mean of the random field a(x), which we  think are simple enough to be used in practical applications. Two of the techniques  252 L. Csat6, E. Fokoud, M. Opper, B. Schottky and O. lt4nther  are based on mean field ideas from Statistical Mechanics, which in contrast to the  previously developed TAP approach are easier to implement. They also yield simple  approximations to the total likelihood of the data (the evidence) which can be used  to tune the hyperparameters in the covariance kernel K (The Bayesian evidence (or  MLII) framework aims at maximizing the likelihood of the data).  We specialize to the case of a binary classification problem, where for simplicity,  the class label y - -+-1 is assumed to be noise free and the likelihood is chosen as  P(y[a) = O(ya) , (1)  where (x) is the unit step function, which equals 1 for x > 0 and zero else. We  are interested in computing efficient approximations to the posterior mean (a(x)),  which we will use for a prediction of the labels via y = sign(a(x)), where (...)  denotes the posterior expectation. If the posterior distribution of a(x) is symmetric  around its mean, this will give the Bayes optimal prediction.  Before starting, let us add two comments on the likelihood (1). First, the MAP  approach (i.e. predicting with the fields a that maximize the posterior) would not  be applicable, because it gives the trivial result a(x) = 0. Second, noise can be  easily introduced within a probit model [2], all subsequent calculations will only be  slightly altered. Moreover, the Gaussian average involved in the definition of the  probit likelihood can always be shifted from the likelihood into the Gaussian process  prior, by a redefinition of the fields a (which does not change the prediction), leaving  us with the simple likelihood (1) and a modified process covariance [5].  2 Exact Results  At first glance, it may seem that in order to calculate (a(x)/ we have to deal with  the joint posterior of the fields ai = a(xi), i - 1,..., m together with the field at  the test point a(x). This would imply that for any test point, a different new m + 1  dimensional average has to be performed. Actually, we will show that this is not the  case. As above let E denote the expectation over the Gaussian prior. The posterior  expectation at any point, say x  E [a(x)1-Ij P(yjlay)]  (x)) = (2)  can by integration by parts-for any likelihood-be written as  ( 01nP(Yjlaj) I (3)  (a(x)) = y. K(x, xj)ojyj and oj '- yj Oaj  showing that cj is not dependent on the test point x. It is therefore not necessary  to compute a m + 1 dimensional average for every prediction.  We have chosen the specific definition (3) in order to stress the similarity to pre-  dictions with Support Vector Machines (for the likelihood (1), the aj will come  out nonnegative). In the next sections we will develop three approaches for an  approximate computation of the aj.  3 Mean Field Method I: Ensemble Learning  Our first goal is to approximate the true posterior distribution  m  P(alD") = Z V/(2vr)- detK e  (4)  Efficient Approaches to Gaussian Process Classification 253  of a =' (a,. .. ,am) by a simpler, tractable distribution q. Here, K denotes the  covariance matrix with elements Kij '- K(xi, xj). In the variational mean field  approach-known as ensemble learning in the Neural Computation Community,-  the relative entropy distance KL(q,p) = f da q(a)In - is minimized in the family  m  of product distributions q(a) = Hj=x qj(aj). This is in contrast to [3], where a  variational bound on the likelihood is computed. We get  KL(q,p) = E / daiqi(ai)ln qi(ai) +   P(yilai)  $  1 1 (ai)o  +  i,j,ij i  where (...)0 denotes expectation w.r.t. q. By setting the functional derivative  of KL(q,p) with respect to qi(a) equal to zero, we find that the best product  distribution is a Gaussian prior times the original Likelihood:  1 (-"i) 2  qi(a) cr P(yila) e  , (5)  where rni = -Ai j,jvi(K-x)ij(aj)o and Ai = [K-x]/. Using this specific form  for the approximated posterior q(a), replacing the average over the true posterior in  (3) by the approximation (5), we get (using the likelihood (1)) a set of rn nonlinear  equations in the unknowns cj:  (01nP(yjlaj)) 1 D()  o-  and rnj - E KjiYiti - jyjotj ,  i  (6)  where D(z) - e-Z2/2/x/- and (z) = f_Zoodt D(t). As a useful byproduct of  the variational approximation, an upper bound on the Bayesian evidence P(D) =  f da ,r(a)P(Dla ) can be derived. (,r denotes the Gaussian process prior and  m  P(Dla) = 1-Ij= P(Yjlaj))  The bound can be written in terms of the mean field  'free energy' as  -lnP(D) <_ Sqlnq(a) - Eqln[vr(a)P(Dla)]  _Eln  [ rnj \ 1  + 2. (7)  i 1  +  In der K -  E In hi  i  which can be used as a yardstick for selecting appropriate hyperparameters in the  covariance kernel.  The ensemble learning approach has the little drawback, that it requires inversion  of the covariance matrix K and, for the free energy (7) one must compute a deter-  minant. A second, simpler approximation avoids these computations.  4 Mean Field Theory II: A 'Naive' Approach  The second mean field theory aims at working directly with the variables otj. As a  starting point, we consider the partition function (evidence),  m  Z = P(D) = / dze -"T<" H P(yjlzj), (8)  j=l  254 L. Csat6, E. Fokoud, 3/1. Opper, B. Schottky and O. Winther  which follows from (4) by a standard Gaussian integration, introducing the Fourier  eaeiazP(y a) with i being the imaginary  transform of the Likelihood 15(ylz) - f   unit. It is tempting to view (8) as a normalizing partition function for a Gaussian  process zi having covariance matrix K -x and likelihood/5. Unfortunately, /5 is  not a real number and precludes a proper probabilistic interpretation. Neverthe-  less, dealing formally with the complex measure defined by (8), integration by parts  shows that one has yjaj = -i<zj>,, where the brackets (...), denote a average over  the complex measure. This suggests a simple approximation for calculating the  aj. One may think of trying a saddle-point (or steepest descent) approximation  to (8) and replace <zj), by the value of zj (in the complex z plane) which makes  the integrand stationary thereby neglecting the fluctuations of the zj. Hence, this  approximation would treat expectations of products as (zizj), as (zi), Zj),, which  may be reasonable for i  j, but definitely not for the self-correlation i - j. Ac-  cording to the general formalism of mean field theories (outlined e.g. in [6]), one  2 separately. This can  can improve on that idea, by treating the 'self-interactions' z i  be done by replacing all zi (except in the form z) by a new variable/i by inserting  a Dirac 5 function representation 5(z -/) - f into (8) and integrate  over the z and a variables exactly (the integral factorizes), and finally perform a  saddle-point integration over the m and/ variables. The details of this calculation  will be given elsewhere. Within the saddle-point approximation, we get the system  of nonlinear equations  I  and mj--  Kji(-ii)-  KjiYiOq (9)  cj -- -iyj/j --  (yj m   V/- ! i,ij i,ij  which is of the same form as (6) with Aj replaced by the simpler Kjj. These  equations have also been derived by us in [5] using a Callen identity, but our present  derivation allows also for an approximation to the evidence. By plugging the saddle-  point values back into the partition function, we get  -lnP(D)  - ln Yi +  y.yici(Ki -SijKii)yjcj   j  which is also simpler to compute than (7) but does not give a bound on the true  evidence.  5 A sequential Approach  Both previous algorithms do not give an explicit expression for the posterior mean,  but require the solution of a set of nonlinear equations. These must be obtained  by an iterative procedure. We now present a different approach for an approximate  computation of the posterior mean, which is based on a single sequential sweep  through the whole dataset giving an explicit update of the posterior.  The algorithm is based on a recently proposed Bayesian approach to online learning  (see [8] and the articles of Opper and Winther& Solla in [9]). Its basic idea applied  to the Gaussian process scenario, is as follows: Suppose, that qt is a Gaussian  approximation to the posterior after having seen t examples. This means that we  approximate the posterior process by a Gaussian process with mean <a(x))t and  covariance Kt(x,y), starting with (a(x)>0 - 0 and K0(x,y) = K(x,y). After a  new data point Yt+l is observed, the posterior is updated according to Bayes rule.  The new non-Gaussian posterior {t is projected back into the family of Gaussians  by choosing the closest Gaussian qt+l minimizing the relative entropy KL({t, qt+l)  Efficient Approaches to Gaussian Process Classifican'on 255  in order to keep the loss of information small. This projection is equivalent to a  matching of the first two moments of t and qt+l. E.g., for the first moment we get  = (x)  (p(yt+lla(xt+l))>t -- (a(x)>t q- l(t)Kt(x, xt+l)  where the second line follows again from an integration by parts and n(t) :  = y+<(+)>  '+ '(*') with zt and a 2(t) : Kt (xt+, xt+). This recursion and  7 (,,) (t)  the corresponding one for Kt can be solved by the ansatz  =  j=l  = +  i,j  where the vector a(t) = (a,..., at, 0, 0,...) and the matrix C(t) (which has also  only t x t nonzero elements) are updated as  a(t+l) = a(t)+(t)(C(t)k++e+)y  C(t + 1) = C(t) + 2(t)(C(t)kt+ + e+)(C(t)kt+ + e+)  (12)  where 2(t) =  (z) -(,)} , k is the vector with elements Kts,  j = 1 ..., t and  denotes the element-wise product between vectors. The sequen-  tial algorithm defined by (10)-(12) has the advantage of not requiring any matrix  inversions. There is also no need to solve a numerical optimization problem at each  time as in the approach of [11] where a different update of a Gaussian posterior ap-  proximation was proposed. Since we do not require a linearization of the likelihood,  the method is not equivalent to the extended Kalman Filter approach.  Since it is possible to compute the evidence of the new datapoint P(yt+) =  based on the old posterior, we can compute a further approximation  to the log evidence for m data via In P(D) -  6 Simulations  We present two sets of simulations for the mean field approaches. In the first, we test  the Bayesian evidence framework for tuning the hyperparameters of the covariance  function (kernel). In the second, we test the ability of the sequential approach to  achieve low training error and a stable test error for fixed hyperparameters.  For the evidence framework, we give simulation results for both mean field free  energies (7) and (10) on a single data set, 'Pima Indian Diabetes (with 200/332  training/test-examples and input dimensionality d = 7) [7]. The results should  therefore not be taken as a conclusive evidence for the merits of these approaches,  but simply as an indication that they may give reasonable results. We use the  ( 1EldWl(X l --X) 2) A  radial basis function covariance function K(x, x ) = exp -3   diagonal term v is added to the covariance matrix corresponding to a Gaussian noise  added to the fields with variance v [5]. The free energy, -lnP(D) is minimized  by gradient descent with respect to v and the lengthscale parameters wx,..., wd  and the mean field equations for cj are solved by iteration before each update of  the hyperparameters (further details will be given elsewhere). Figure I shows the  evolution of the naive mean free energy and the test error starting from uniform  256 L. Csat6, E. Fokou, M. Opper, B. Schottky and O. Winther  ws. It typically requires of the order of 10 iteration steps of the cj-equations  between each hyperparameter update. We also used hybrid approaches, where the  free energy was minimized by one mean field algorithm and the hyperparameters  used in the other. As it may be seen from table 1, the naive mean field theory can  overestimate the free energy (since the ensemble free energy is an upper bound to  the free energy). The overestimation is not nearly as severe at the minimum of the  naive mean field free energy. Another interesting observation is that as long as the  same hyperparameters are used the actual performance (as measured by the test  error) is not very sensitive to the algorithm used. This also seems to be the case  for the TAP mean field approach and Support Vector Machines [5].  115 74  70  ' 66  62  0 20 40 60 80 0 20 40 60 80  Iterations Iterations  Figure 1: Hyperparameter optimization for the Pima Indians data set using the  naive mean field free energy. Left figure: The free energy as a function of the  number of hyperparameter updates. Right figure: The test error count (out of 332)  as a function of the number of hyperparameter updates.  '114 ,,  e-   I ,  -113 ,  e-  '"112  111  Table 1: Pima Indians dataset. Hyperparameters found by free energy minimiza-  tion. Left column gives the free energy - In P(D) used in hyperparameter optimiza-  tion. Test error counts in range 63- 75 have previously been reported [5]  Ensemble MF Naive MF  Free Energy minimization Error - In P(D) Error - In P(D)  Ensemble Mean Field, eq. (7) 72 100.6 70 183.2  Naive Mean Field, eq. (10) 62 107.0 62 110.9  For the sequential algorithm, we have studied the sonar [10] and crab [7] datasets.  Since we have not computed an approximation to the evidence so far, a simple fixed  polynomial kernel was used. Although a probabilistic justification of the algorithm  is only valid, when a single sweep through the data is used (the independence of the  data is assumed), it is tempting to reuse the same data and iterate the procedure as a  heuristic. The two plots show that in this way, only a small improvement is obtained,  and it seems that the method is rather efficient in extracting the information from  the data in a single presentation. For the sonar dataset, a single sweep is enough  to achieve zero training error.  Acknowledgements: BS would like to thank the Leverhulme Trust for their support  (F/250/K). The work was also supported by EPSRC Grant GR/L52093.  Efficient Approaches to Gaussian Process Classification 257  5O  45  40  35  30  25  20  15  10  5  0  0  I -- Training Error  [ - - Test Error  20 40 60 80 1 O0 120 140 160  Iteration #  0 50 1 O0 150 200  Iteration #  Figure 2: Training and test errors during learning for the sonar (left) and crab  dataset (right). The vertical dash-dotted line marks the end of the training set  and the starting point of reusing of it. The kernel function used is K(x, x ) =  (1 + x. x/m) k with order k = 2 (m is the dimension of inputs).  References  [1] Williams C.K.I. and Rasmussen C.E., Gaussian Processes for Regression, in Neural  Information Processing Systems 8, Touretzky D.S, Mozer M.C. and Hasselmo M.E.  (eds.), 514-520, MIT Press (1996).  [2] Neal R.M, Monte Carlo Implementation of Gaussian Process Models for Bayesian  Regression and Classification, Technical Report 9702, Department of Statistics, Uni-  versity of Toronto (1997).  [3] Gibbs M.N. and Mackay D.J.C., Variational Gaussian Process Classifiers, Preprint  Cambridge University (1997).  [4] Williams C.K.I. and Barber D, Bayesian Classification with Gaussian Processes, IEEE  Trans Pattern Analysis and Machine Intelligence, 20 1342-1351 (1998).  [5] Opper M. and Winther O. Gaussian Processes for Classification: Mean  Field Algorithms, Submitted to Neural Computation, http://www.thep.lu.se  /tf2/staff/winther/ (1999).  [6] Zinn-Justin J, Quantum Field Theory and Critical Phenomena, Clarendon Press Ox-  ford (1990).  [7] Ripley B.D, Pattern Recognition and Neural Networks, Cambridge University Press  (1996).  [8] Opper M., Online versus Offiine Learning from Random Examples: General Results,  Phys. Rev. Lett. 77, 4671 (1996).  [9] Online Learning in Neural Networks, Cambridge University Press, D. Saad (ed.)  (1998).  [10] Gorman R.P and Sejnowski T.J, Analysis of hidden units in a layered network trained  to classify sonar targets, Neural Networks 1, (1988).  [11] Jaakkola T. and Haussler D. Probabilistic kernel regression, In Online Proceedings  of 7-th Int. Workshop on AI and Statistics (1999),  http://uncertainty99.microsoft.com/proceedings.htm.  
Scale Mixtures of Gaussians and the  Statistics of Natural Images  Martin J. Wainwright  Stochastic Systems Group  Electrical Engineering & CS  MIT, Building 35-425  Cambridge, MA 02139  mjwain@mit. edu  Eero P. Simoncelli  Ctr. for Neural Science, and  Courant Inst. of Mathematical Sciences  New York University  New York, NY 10012  eero. simoncel li @nyu. edu  Abstract  The statistics of photographic images, when represented using  multiscale (wavelet) bases, exhibit two striking types of non-  Gaussian behavior. First, the marginal densities of the coefficients  have extended heavy tails. Second, the joint densities exhibit vari-  ance dependencies not captured by second-order models. We ex-  amine properties of the class of Gaussian scale mixtures, and show  that these densities can accurately characterize both the marginal  and joint distributions of natural image wavelet coefficients. This  class of model suggests a Markov structure, in which wavelet coeffi-  cients are linked by hidden scaling variables corresponding to local  image structure. We derive an estimator for these hidden variables,  and show that a nonlinear "normalization" procedure can be used  to Gaussianize the coefficients.  Recent years have witnessed a surge of interest in modeling the statistics of natural  images. Such models are important for applications in image processing and com-  puter vision, where many techniques rely (either implicitly or explicitly) on a prior  density. A number of empirical studies have demonstrated that the power spectra  of natural images follow a 1If v law in radial frequency, where the exponent  is  typically close to two [e.g., 1]. Such second-order characterization is inadequate,  however, because images usually exhibit highly non-Gaussian behavior. For in-  stance, the marginals of wavelet coefficients typically have much heavier tails than  a Gaussian [2]. Furthermore, despite being approximately decorrelated (as sug-  gested by theoretical analysis of 1If processes [3]), orthonormal wavelet coefficients  exhibit striking forms of statistical dependency [4, 5]. In particular, the standard  deviation of a wavelet coefficient typically scales with the absolute values of its  neighbors [5].  A number of researchers have modeled the marginal distributions of wavelet coef-  ficients with generalized Laplacians, p(y) c exp(-ly/AI p) [e.g. 6, 7, 8]. Special  cases include the Gaussian (p - 2) and the Laplacian (p - 1), but appropriate ex-  Research supported by NSERC 1969 fellowship 160833 to MJW, and NSF CAREER grant  MIP-9796040 to EPS.  856 M. J. Wainwright and E. P Sirnoncelli  Mixing density  Positive, V/ - stable  No explicit form  GSM density GSM char. function  symmetrized Gamma  Student:  , 1  [1/(), + >  a-stable  generalized Laplacian:  exp(-[y/XlP), pe (0,2]  t2  (l+2-X-  , >0  No explicit form  exp (-Itla), a e (0,2]  No explicit form  Table 1. Example densities from the class of Gaussian scale mixtures. Z(7 ) de-  notes a positive gamma variable, with density p(z)= [1/F(y)]z *- exp(-z).  The characteristic function of a random variable x is defined as  opt(t) -- f_oo p(x) exp (jxt) dx.  ponents for natural images are typically less than one. Simoncelli [5, 9] has modeled  the variance dependencies of pairs of wavelet coefficients. Romberg et al. [10] have  modeled wavelet densities using two-component mixtures of Gaussians. Huang and  Mumford [11] have modeled marginal densities and cross-sections of joint densities  with multi-dimensional generalized Laplacians.  In the following sections, we explore the semi-parametric class of Gaussian scale  mixtures. We show that members of this class satisfy the dual requirements of  being heavy-tailed, and exhibiting multiplicative scaling between coefficients. We  also show that a particular member of this class, in which the multiplier variables  are distributed according to a gamma density, captures the range of joint statistical  behaviors seen in wavelet coefficients of natural images. We derive an estimator for  the multipliers, and show that a nonlinear "normalization" procedure can be used  to Gaussianize the wavelet coefficients. Lastly, we form random cascades by linking  the multipliers on a multiresolution tree.  I Scale Mixtures of Gaussians  d d  A random vector Y is a Gaussian scale mixture (GSM) if Y - zU, where = denotes  equality in distribution; z > 0 is a scalar random variable; U ,- iV'(0, Q) is a  Gaussian random vector; and z and U are independent.  As a consequence, any GSM variable has a density given by an integral:  /_ 1 ( YTo-Y'   PY(Y) = oo Iz2QI1/2 exp  / c)(z)dz.  where bz is the probability density of the mixing variable z (henceforth the mul-  tiplier). A special case of a GSM is a finite mixture of Gaussians, where z is a  discrete random variable. More generally, it is straightforward to provide condi-  tions on either the density [12] or characteristic function of X that ensure it is a  GSM, but these conditions do not necessarily provide an explicit form of bz. Nev-  ertheless, a number of well-known distributions may be written as Gaussian scale  mixtures. For the scalar case, a few of these densities, along with their associated  characteristic functions, are listed in Table 1. Each variable is characterized by a  scale parameter A, and a tail parameter. All of the GSM models listed in Table 1  produce heavy-tailed marginal and variance-scaling joint densities.  Scale Mixtures of Gaussians and the Statistics of Natural Images 857  baboon boats flower frog  -500 0 5430  _2[  -500 0 500 1000 -1000 -500 0 500  ['7, ,'k2] = [0.97, 15.04]  AH/H = 0.00079  [0.45, 13.77] [0.78, 26.83] [0.80, 15.39]  0.0030 0.0030 0.0076  Figure 1. GSMs (dashed lines) fitted to empirical histograms (solid lines). Below  each plot are the parameter values, and the relative entropy between the histogram  (with 256 bins) and the model, as a fraction of the histogram entropy.  2 Modeling Natural Images  As mentioned in the introduction, natural images exhibit striking non-Gaussian  behavior, both in their marginal and joint statistics. In this section, we show that  this behavior is consistent with a GSM, using the first of the densities given in  Table I for illustration.  2.1 Marginal distributions  We begin by examining the symmetrized Gamma class as a model for marginal  distributions of wavelet coefficients. Figure I shows empirical histograms of a par-  ticular wavelet subband 1 for four different natural images, along with the best fitting  instance of the symmetrized Gamma distribution. Fitting was performed by min-  imizing the relative entropy (i.e., the Kullback-Leibler divergence, denoted AH)  between empirical and theoretical histograms. In general, the fits are quite good:  the fourth plot shows one of the worst fits in our data set.  2.2 Normalized components  For a GSM random vector Y __.a zU, the normalized variable Y/z formed by  component-wise division is Gaussian-distributed. In order to test this behavior  empirically, we model a given wavelet coefficient Y0 and a collection of neighbors  {Yl,.-  , YN) as a GSM vector. For our examples, we use a neighborhood of N = 11  coefficients corresponding to basis functions at 4 adjacent positions, 5 orientations,  and 2 scales. Although the multiplier z is unknown, we can estimate it by max-  imizing the log likelihood of the observed coefficients:  _a argmaxz  logp(Y]z)).  Under reasonable conditions, the normalized quantity Y/ should converge in dis-  tribution to a Gaussian as the number of neighbors increases. The estimate  is  simple to derive:   = argmax{logp(Y[z)}  z  = argmin {Nlog(z) + YrQ-1y/2z2}  z  = v/YrQ-1y/N,  1We use the steerable pyramid, an overcomplete multiscale representation described  in [13]. The marginal and joint statistics of other multiscale oriented representations are  similar.  858 M. d. Wainwright and E. P. Simoncelli  baboon  boats  flowers frog  o  AH/H = 0.00035  0.00041  o  0.00042  0.00043  Figure 2. Marginal log histograms (solid lines) of the normalized coefficient v for a  single subband of four natural images. Each shape is close to an inverted parabola,  in agreement with Gaussians (dashed lines) of equivalent empirical variance. Below  each plot is the relative entropy between the histogram (with 256 bins) and a  variance-matched Gaussian, as a fraction of the total histogram entropy.  where Q __a E [UU T] is the positive definite covariance matrix of the underlying  Gaussian vector U.  Given the estimate , we then compute the normalized coefficient y  Yo/. This is  a generalization of the variance normalization proposed by Ruderman and Bialek[1],  and the weighted sum of squares normalization procedure used by Simoncelli [5, 14].  Figure 2 shows the marginal histograms (in the log domain) of this normalized  coefficient for four natural images, along with Gaussians of equal empirical variance.  In contrast to histograms of the raw coefficients (shown in Figure 1), the histograms  of normalized coefficients are nearly Gaussian.  The GSM model makes a stronger prediction: that normalized quantities corre-  sponding to nearby wavelet pairs should be jointly Gaussian. Specifically, a pair  of normalized coefficients should be either correlated or uncorrelated Gaussians,  depending on whether the underlying Gaussians U = Jul u2] T are correlated or un-  correlated. We examine this prediction by collecting joint conditional histograms of  normalized coefficients. The top row of Figure 3 shows joint conditional histograms  for raw wavelet coefficients (taken from the same four natural images as Figure 2).  The first two columns correspond to adjacent spatial scales; though decorrelated,  they exhibit the familiar form of multiplicative scaling. The latter two columns cor-  respond to adjacent orientations; in addition to being correlated, they also exhibit  the multiplicative form of dependency.  The bottom row shows the same joint conditional histograms, after the coefficients  have been normalized. Whereas Figure 2 demonstrates that normalized coefficients  are close to marginally Gaussian, Figure 3 demonstrates that they are also approx-  imately jointly Gaussian. These observations support the use of a Gaussian scale  mixture for modeling natural images.  2.3 Joint distributions  The GSM model is a reasonable approximation for groups of nearby wavelet coef-  ficients. However, the components of GSM vectors are highly dependent, whereas  the dependency between wavelet coefficients decreases as (for example) their spa-  tial separation increases. Consequently, the simple GSM model is inadequate for  global modeling of coefficients. We are thus led to use a graphical model (such as  tree) that specifies probabilistic relations between the multipliers. The wavelet co-  efficients themselves are considered observations, and are linked indirectly by their  shared dependency on the (hidden) multipliers.  Scale Mixtures of Gaussians and the Statists'cs of Natural Images 859  baboon boats flowers frog  -500 0 500 -500 0 500 -500 0 500 -500 0 500  -25 0 25 -25 0 25 -25 0 25 -25 0  AH/H = 0.0643 0.0743 0.0572 0.0836  25  Figure 3. Top row: joint conditional histograms of raw wavelet coefficients for  four natural images. Bottom row: joint conditional histograms of normalized pairs  of coefficients. Below each plot is the relative entropy between the joint histogram  (with 256 x 256 bins) and a covaxiance-matched Gaussian, as a fraction of the total  histogram entropy.  For concreteness, we model the wavelet coefficient at node s as y(s)  where x(s) is Gaussian, so that z - ]]xl] is the square root of a gamma variable of  index 0.5. For illustration, we assume that the multipliers are linked by a multiscale  autoregressive (MAR) process [15] on a tree:  x(8) = x(p)) + x/T-  where p(s) is the parent of node s. Two wavelet coefficients y(s) and y(t) are linked  through the multiplier at their common ancestral node denoted s A t. In particular,  the joint distributions are given by  - *^*) x(, A ,) + (*)11 u(,)  where v, v2 are independent white noise processes; and d(, ) denotes the distance  between a node d one of its ancestors on the tree (e.g., d(s,p(s)) = 1). For  nodes s and t at the same scale and oriemation but spatially separated by a dis-  tance of A(s, t), the distance between s and the coon ancestor s A t ows  d(s,s  t)  gog2(A(s, t)) + 1].  The first row of Figure 4 shows the range of behaviors seen in joint distributions  taken from a wavelet subband of a particul natural image, compared to simulated  GSM gamma distributions with  = 0.92. The first column corresponds to a pair  of wavelet filters in quadrature phe (i.e., related by a Hilbert transform). Note  that for this pair of coefficients, the contours are nearly circular, an observation  that h been previously made by Zetzsche [4]. Nevertheless, these two coefficients  e dependent,  shown by the multiplicative scaling in the conditional histo,am  of the third row. This type of scaling dependency h been extensivdy documemed  by Simoncelli [5, 9]. Analogous plots for the simulated Gamma model, with zero  spatial separation are shown in rows 2 and 4. As in the image data, the contours of  the joint density are very close to circular, and the conditional distribution shows  a string variance dependency.  860 M. J. Wainwright and E. P. Simoncelli  image  data  simulated  model  image  data  quad. pair overlapping near distant  simulated  model  Figure 4. Examples of empirically observed distributions of wavelet coefficients,  compared with simulated distributions from the GSM gamma model. First row:  Empirical joint histograms for the "mountain" image, for four pairs of wavelet coef-  ficients, corresponding to basis functions with spatial separations A -- (0, 4, 8, 128).  Second row: Simulated joint distributions for Gamma variables with p -- 0.92 and  the same spatial separations. Contour lines are drawn at equal intervals of log  probability. Third row: Empirical conditional histograms for the "mountain" im-  age. Fourth row: Simulated conditional histograms for Gamma variables. For  these conditional distributions, intensity corresponds to probability, except that  each column has been independently rescaled to fill the full range of intensities.  The remaining three columns of figure 4 show pairs of coefficients drawn from iden-  tical wavelet filters at spatial displacements A = (4, 8, 128), corresponding to a  pair of overlapping filters, a pair of nearby filters, and a distant pair. Note the pro-  gression in the contour shapes from off-circular, to a diamond shape, to a concave  "star" shape. The model distributions behave similarly, and show the same range of  contours for simulated pairs of coefficients. Thus, consistent with empirical obser-  vations, a GSM model can produce a range of dependency between pairs of wavelet  coefficients. Again, the marginal histograms retain the same form throughout this  range.  3 Conclusions  We have proposed the class of Gaussian scale mixtures for modeling natural images.  Models in this class typically exhibit heavy-tailed marginals, as well as multiplicative  scaling between adjacent coefficients. We have demonstrated that a particular GSM  (the symmetrized Gamma family) accounts well for both the marginal and joint  distributions of wavelet coefficients from natural images. More importantly, this  model suggests a hidden Markov structure for natural images, in which wavelet  coefficients are linked by hidden multipliers. Romberg et al. [10] have made a related  proposal using two-state discrete multipliers, corresponding to a finite mixture of  Gaussians.  Scale Mixtures of Gaussians and the Statistics of Natural Images 861  We have demonstrated that the hidden multipliers can be locally estimated from  measurements of wavelet coefficients. Thus, by conditioning on fixed values of the  multipliers, estimation problems may be reduced to the classical Gaussian case.  Moreover, we described how to link the multipliers on a multiresolution tree, and  showed that such a random cascade model accounts well for the drop-off in de-  pendence of spatially separated coefficients. We are currently exploring EM-like  algorithms for the problem of dual parameter and state estimation.  Acknowledgements  We thank Bill Freeman, David Mumford, Mike Schneider, Ilya Pollak, and Alan  Willsky for helpful discussions.  References  [1] D. L. Ruderman and W. Bialek. Statistics of natural images: Scaling in the woods.  Phys. Rev. Letters, 73(6):814-817, 1994.  [2] D. J. Field. Relations between the statistics of natural images and the response  properties of cortical cells. J. Opt. Soc. Am. A, 4(12):2379-2394, 1987.  [3] A. H. Tewfik and M. Kim. Correlation structure of the discrete wavelet coefficients  of fractional Brownian motion. IEEE Trans. Info. Theory, 38:904-909, Mar. 1992.  [4] C. Zetzsche, B. Wegmann, and E. Barth. Nonlinear aspects of primary vision: Entropy  reduction beyond decorrelation. In Int 'l Syrup. Soc. for Info. Display, volume 24, pages  933-936, 1993.  [5] E. P. Simoncelli. Statistical models for images: Compression, restoration and synthe-  sis. In 31st Asilomar Conf., pages 673-678, Nov. 1997.  [6] S. G. Mallat. A theory for multiresolution signal decomposition: the wavelet repre-  sentation. IEEE Pat. Anal. Mach. Intell., 11:674-693, July 1989.  [7] E. P. Simoncelli and E. H. Adelson. Noise removal via Bayesian wavelet coring. In  Proc. IEEE ICIP, volume I, pages 379-382, September 1996.  [8] P. Moulin and J. Liu. Analysis of multiresolution image denoising schemes using a  generalized Gaussian and complexity priors. IEEE Trans. Info. Theory, 45:909-919,  Apr. 1999.  [9] R. W. Buccigrossi and E. P. Simoncelli. Image compression via joint statistical char-  acterization in the wavelet domain. IEEE Trans. Image. Proc., 8(12):1688-1701, Dec.  1999.  [10] J.K. Romberg, H. Choi, and R.G. Baraniuk. Bayesian wavelet domain image modeling  using hidden Markov trees. In Proc. IEEE ICIP, Kobe, Japan, Oct. 1999.  [11] J. Huang and D. Mumford. Statistics of natural images and models. In CVPR, paper  216, 1999.  [12] D.F. Andrews and C.L. Mallows. Scale mixtures of normal distributions. J. Royal  Star. Soc., 36:99-102, 1974.  [13] E. P. Simoncelli and W. T. Freeman. The steerable pyramid: A flexible architecture  for multi-scale derivative computation. In Proc. IEEE ICIP, volume III, pages 444-  447, Oct. 1995.  [14] E. P. Simoncelli and O. Schwartz. Image statistics and cortical normalization models.  In M. S. Kearns, S. A. Solla, and D. A. Cobh, editors, Adv. Neural Information  Processing Systems, volume 11, pages 153-159, Cambridge, MA, May 1999.  [15] K. Chou, A. Willsky, and R. Nikoukhah. Multiscale systems, Kalman filters, and  Riccati equations. IEEE Trans. Automatic Control, 39(3):479-492, Mar. 1994.  
Coastal Navigation with Mobile Robots  Nicholas Roy and Sebastian Thrun  School of Computer Science  Carnegie Mellon University  Pittsburgh, PA 15213  { nicholas. roy I sebastian. thrun } @ cs. cmu. edu  Abstract  The problem that we address in this paper is how a mobile robot can plan in order  to arrive at its goal with minimum uncertainty. Traditional motion planning algo-  rithms often assume that a mobile robot can track its position reliably, however, in real  world situations, reliable localization may not always be feasible. Partially Observable  Markov Decision Processes (POMDPs) provide one way to maximize the certainty of  reaching the goal state, but at the cost of computational intractability for large state  spaces.  The method we propose explicitly models the uncertainty of the robot's position as  a state variable, and generates trajectories through the augmented pose-uncertainty  space. By minimizing the positional uncertainty at the goal, the robot reduces the  likelihood it becomes lost. We demonstrate experimentally that coastal navigation  reduces the uncertainty at the goal, especially with degraded localization.  1 Introduction  For an operational mobile robot, it is essential to prevent becoming lost. Early motion  planners assumed that a robot would never be lost - that a robot could always know its  position via dead reckoning without error [7]. This assumption proved to be untenable due  to the small and inevitable inconsistencies in actual robot motion; robots that rely solely on  dead reckoning for their position estimates lose their position quickly. Mobile robots now  perform position tracking using a combination of sensor data and odometry [2, 10, 5].  However, the robot's ability to track its position can vary considerably with the robot's  position in the environment. Some parts of the environment may lack good features for lo-  calization [ 11 ]. Other parts of the environment can have a large number of dynamic features  (for example, people) that can mislead the localization system. Motion planners rarely, if  ever, take the robot's position tracking ability into consideration. As the robot's localiza-  tion suffers, the likelihood that the robot becomes lost increases, and as a consequence, the  robot is less likely to complete the given trajectory.  Most localization systems therefore compensate by adding environment-specific knowl-  edge to the localization system, or by adding additional sensing capabilities to the robot,  to guarantee that the robot can complete every possible path. In general, however, such  alterations to the position tracking abilities of the robot have limitations, and an alternative  scheme must be used to ensure that the robot can navigate with maximum reliability. The  conventional planners represent one end of a spectrum of approaches (figure 1), in that a  plan can be computed easily, but at the cost of not modelling localization performance.  At opposite end of the spectrum is the Partially Observable Markov Decision Process  1044 N. Roy and S. Thrun  Conventional  Path Planner POMDP  Tractable Intractable  Not Robust Robust  Figure 1: The continuum of possible approaches to the motion planning, from the robust but in-  tractable POMDP, to the potentially failure-prone but real-time conventional planners. Coastal navi-  gation lies in the middle of this spectrum.  (POMDP). POMDPs in a sense are the brass ring of planning with uncertainty; a POMDP  policy will make exactly the right kind of compromise between conventional optimality  considerations and certainty of achieving the goal state. Many people have examined the  use of POMDPs for mobile robot navigation [5, 6, 8]. However, computing a POMDP  solution is computationally intractable (PSPACE-hard) for large state systems - a mobile  robot operating in the real world often has millions of possible states. As a result, many  of the mobile robot POMDP solutions have made simplifying assumptions about the world  in order to reduce the state space size. Many of these assumptions do not scale to larger  environments or robots. In contrast, our hypothesis is that only a small number of the  dimensions of the uncertainty matter, and that we can augment the state with these dimen-  sions to approximate a solution to the POMDP.  The coastal navigation model developed in this paper represents a tradeoff between robust  trajectories and computational tractability, and is inspired by traditional navigation of ships.  Ships often use the coasts of continents for navigation in the absence of better tools such  as GPS, since being close to the land allows sailors to determine with high accuracy where  they are. The success of this method results from coast lines containing enough information  in their structure for accurate localization. By navigating sufficiently close to areas of the  map that have high information content, the likelihood of getting lost can be minimized.  2 Modelling Uncertainty  The problem that we address in this paper is how a mobile robot can plan in order to arrive  at its goal with minimum uncertainty. Throughout this discussion, we will be assuming a  known map of the environment [9]. The position, x, of the robot is given as the location  (z, y) and direction 0, defined over a space X = (X, Y, O). Our localization method is  a grid-based implementation of Markov localization [3, 5]. This method represents the  robot's belief in its current position using a 3-dimensional grid over X - (X, Y, ), which  allows for a discrete approximation of arbitrary probability distributions. The probability  that the robot has a particular pose x is given by the probability p(x).  State Augmentation We can extend the state of the robot from the 3-dimensional pose  space to an augmented pose-uncertainty space. We can represent the uncertainty of the  robot's positional distribution as the entropy,  H(px) = - fp(x)log(p(x))  x  (1)  We therefore represent the state space of the robot as the tuple  S =  : (x, (x))  State Transitions In order to construct a plan between two points in the environment,  we need to be able to represent the effect of the robot's sensing and moving actions. The  implementation of Markov localization provides the following equations for the tracking  Coastal Navigation with Mobile Robots 1045  the robot's pose from x to x':  = f,(x'lx, (2)  x  = ap(z[x)p(x) (3)  These equations are taken from [3, 12], where equation (2) gives the prediction phase of  localization (after motion u), and equation (3) gives the update phase of localization (after  receiving observation z). c is a normalizing constant. We extend these equations to the  fourth dimension as follows:  3 Planning  p(slu) : (4)  p(sl,.) :  Equations (4) and (5) provide a mechanism for tracking the robot's state, and in fact contain  redundant information, since the extra state variable H (x) is also contained in the probabil-  ity distribution p(x). However, in order to make the planning problem tractable, we cannot  in fact maintain the probabilistic sensing model. To do so would put the planning problem  firmly in the domain of POMDPs, with the associated computational intractability. Instead,  we make a simplifying assumption, that is, that the positional probability distribution of  the robot can be represented at all times by a Gaussian centered at the mean x. This allows  us to approximate the positional distribution with a single statistic, the entropy. In POMDP  terms, we using the assumption of Gaussian distributions to compress the belief space to a  single dimension. We can now represent the positional probability distribution completely  with the vector s, since the width of the Gaussian is represented by the entropy H (x).  More importantly, the simplifying assumption allows us to track the state of the robot de-  terministically. Although the state transitions are stochastic (as in equation (4)), the obser-  vations are not. At any point in time, the sensors identify the true state of the system, with  some certainty given by H(p(xlz)). This allows us to compress the state transitions into a  single rule:  p(sl)- (6)  The final position of the robot depends only on the motion command t and can be identified  by sensing z. However, the uncertainty of the pose, H(p(xlt , z)), is a function not only  of the motion command but also the sensing. The simplifying assumption of Gaussian  models is in general untenable for localization; however, we shall see that this assumption  is sufficient for the purpose of motion planning.  One final modification must be made to the state transition rule. In a perfect world, it  would be possible to predict exactly what observation would be made. However, it is  exactly the stochastic and noisy nature of real sensors that generates planning difficulty,  yet the update rule (6) assumes that it is possible to predict measurement z at pose x.  Deterministic prediction is not possible; however, it is possible to compute probabilities  for sensor measurements, and thus generate an expected value for the entropy based on the  probability distribution of observations Z, which leads to the final state transition rule:  pl,) = (p(xl,),Ez[r(p(xl,,,.))]) (7)  where Ez [H(p(xl u, z))] represents the expected value of the entropy of the pose distribu-  tion over the space of possible sensor measurements.  With the transition rule in equation (7), we can now compute the transition probabilities  for any particular state using a model of the robot's motion, a model of the robot's sensor  and a map of the environment. The probability p(xlu) is given by a model of the robot's  motion, and can be easily precomputed for each action u. The expectation term Ez [HI  1046 N. Roy and S. Thrun  can also be precomputed for each possible state s. The precomputation of these transition  probabilities is very time-intensive, because it requires simulating sensing at each state in  the environment, and then computing the posterior distribution. However, as the precom-  putation is a one-time operation for the environment and robot, planning itself can be an  online operation and is (in the limit) unaffected by the speed of computing the transition  probabilities.  3.1 Computing Trajectories  With the state update rule given in equation (7), we can now compute the optimal trajectory  to a particular goal. We would in fact like to compute not just the optimal trajectory from  the current robot position, but the optimal action from any position in the world. If the robot  should deviate from the expected trajectory for any reason (such as error in the motion, or  due to low-level control constraints), interests of efficiency suggest precomputing actions  for continuing to the goal, rather than continually replanning as these contingencies arise.  Note that the motion planning problem as we have now phrased it can be viewed as the  problem of computing the optimal policy for a given problem. The Markovian, stochastic  nature of the transitions, coupled with the need to compute the optimal policy for all states,  suggests a value iteration approach.  Value iteration attempts to find the policy that maximizes the long-term reward [ 1, 4]. The  problem becomes one of finding the value function, J(s) which assigns a value to each  state. The optimal action at each state can then be easily computed by determining the  expected value of each action at each state, from the neighboring values. We use a modified  form of Bellman's equations to give the value of state J(s) and policy as  N  J(si) - max[R(si)+ C(s,u)+ J(s:)] (8)  j=l  N  rr(si) = argmax[R(si) + C(s, u) + p(sj[s/, u). J(sj)] (9)  u j=l  By iterating equation (8), the value function iteratively settles to a converged value over all  states. Iteration stops when no state value changes above some threshold value.  In the above equations, R(si) is the immediate reward at state si, p(sj [si, u) is the transition  probability from state si to state sj, and C(s, u) is the cost of taking action u at state s. Note  that the form of the equations is undiscounted in the traditional sense, however, the additive  cost term plays a similar role in that the system is penalized for policies that take longer  trajectories. The cost in general is simply the distance of one step in the given direction u,  although the cost of travel close to obstacles is higher, in order to create a safety margin  around obstacles. The cost of an action that would cause a collision is infinite, preventing  such actions from being used.  The immediate reward is localized only at the goal pose. However, the goal pose has a  range of possible values for the uncertainty, creating a set of goal states, 6. In order to  reward policies that arrive at a goal state with a lower uncertainty, the reward is scaled  linearly with goal state uncertainty.  /r(Xi ) __ {-- /r/(S) S( (10)  otherwise  By implementing the value iteration given in the equations (8) and (9) in a dynamic pro-  gram, we can compute the value function in O(nkcrit) where n is the number of states in  the environment (number of positions x number of entropy levels) and kcrit is the num-  ber of iterations to convergence. With the value function computed, we can generate the  optimal action for any state in O(a) time, where a is the number of actions out of each  state.  Coastal Navigation with Mobile Robots 1047  4 Experimental Results  Figure 2 shows the mobile robot, Minerva, used for this research. Minerva is a RWI B-18,  and senses using a 360  field of view laser range finder at 1  increments.  Figure 2: Minerva, the B-18 mobile robot used for this research, and an example environment map,  the Smithsonian National Museum of American History. The black areas are the walls and obstacles.  Note the large sparse areas in the center of the environment.  Also shown in figure 2 is an example environment,the Smithsonian National Museum of  American History. Minerva was used to generate this map, and operated as a tour-guide in  the museum for two weeks in the summer of 1998. This museum has many of the features  that make localization difficult- large open spaces, and many dynamic obstacles (people)  that can mislead the sensors.  Goal  arti t'ositio.  StartingSPosition  .... GalaQ i S a  (a) Conventional  (b) Coastal  (c) Sensor Map  Figure 3: Two examples in the museum environment. The left trajectory is given by a conventional,  shortest-path planner. The middle trajectory is given by the coastal navigation planner. The black  areas correspond to obstacles, the dark grey areas correspond to regions where sensor information is  available, the light grey areas to regions where no sensor information is available.  Figure 3 shows the effect of different planners in the sample environment. Panel (a) shows  the trajectory of a conventional, shortest distance planner. Note that the robot moves di-  1048 N. Roy and S. Thrun  rectly towards the goal. Panel (b) shows the trajectory given by the coastal planner. In both  examples, the robot moves towards an obstacle, and relocalizes once it is in sensor range of  the obstacle, before moving towards the goal. These periodic relocalizations are essential  for the robot to arrive at the goal with minimum positional uncertainty, and maximum reli-  ability. Panel (c) shows the sensor map of the environment. The black areas show obstacles  and walls, and the light grey areas are where no information is available to the sensors, be-  cause all environmental features are outside the range of the sensors. The dark grey areas  indicate areas where the information gain from the sensors is not zero; the darker grey the  area, the better the information gain from the sensors.  20  18  16  14  12  10  8  6  4  2  0  -2  0  Positional Uncertainty at Goal  Conventional Navigation  Coastal Navigation  0'.5 i 1'.5  2.5  3'.5 ' 4'.5   Maximum Range of Laser Range Sensor in Meters  5.5  Figure 4: The performance of the coastal navigation algorithm compared to the coastal motion plan-  ner. The graph depicts the entropy of the position probability distribution against the range of the  laser sensor. Note that the coastal navigation dramatically improves the certainty of the goal position  with shorter range laser sensing.  Figure 4 is a comparison of the average positional certainty (computed as entropy of the  positional probability) of the robot at its goal position, compared to the range of the laser  range sensor. As the range of the laser range gets shorter, the robot can see fewer and  fewer environmental features - this is essentially a way of reducing the ability of the robot  to localize itself. The upper line is the performance of a conventional shortest-distance  path planner, and the lower line is the coastal planner. The coastal planner has a lower  uncertainty for all ranges of the laser sensor, and is substantially lower at shorter ranges,  confirming that the coastal navigation has the most effect when the localization is worst.  5 Conclusion  In this paper, we have described a particular problem of motion planning- how to guarantee  that a mobile robot can reach its goal with maximum reliability. Conventional motion  planners do not typically plan according to the ability of the localization unit in different  areas of the environment, and thus make no claims about the robustness of the generated  trajectory. In contrast, POMDPs provide the correct solution to the problem of robust  trajectories, however, computing the solution to a POMDP is intractable for the size of the  state space for typical mobile robot environments.  We propose a motion planner with an augmented state space that represents positional  uncertainty explicitly as an extra dimension. The motion planner then plans through pose-  uncertainty space, to arrive at the goal pose with the lowest possible uncertainty. This can  be seen to be an approximation to a POMDP where the multi-dimensional belief space is  represented as a subset of statistics, in this case the entropy of the belief space.  We have shown some experimental comparisons with a conventional motion planner. Not  only did the coastal navigation generated trajectories that provided substantial improve-  ment of the positional certainty at the goal compared to the conventional planner, but the  improvement became more pronounced as the localization was degraded.  Coastal Navigation with Mobile Robots 1049  The model presented here, however, is not complete. The entire methodology hinges upon  the assumption that the robot's probability distribution can be adequately represented by  the entropy of the distribution. This assumption is valid if the distribution is restricted  to a uni-modal Gaussian, however, most Markov localization methods that are based on  this assumption fail, because multi-modal, non-Gaussian positional distributions are quite  common for moving robots. Nonetheless, it may be that multiple uncertainty statistics  along multiple dimensions (e.g., z and g/) may do a better job of capturing the uncertainty  sufficiently. It is an question for future work as to how many statistics can capture the  uncertainty of a mobile robot, and under what environmental conditions.  Acknowledgments  The authors gratefully acknowledge the advice and collaboration of Tom Mitchell throughout the  development of this work. Wolfram Burgard and Dieter Fox played an instrumental role in the de-  velopment of earlier versions of this work, and their involvement and discussion of this new model is  much appreciated. This work was partially funded by the Fonds pour la Formation de Chercheurs et  l'Aide h la Recherche (FCAR).  References  [1] R. Bellman. Dynamic Programming. Princeton University Press, NJ, 1957.  [2] W. Burgard, D. Fox, D. Hennig, and T. Schmidt. Estimating the absolute position of a mobile  robot using position probability grids. In AAAI, 1996.  [3] D. Fox, W. Burgard, and S. Thrun. Active Markov localization for mobile robots. Robotics and  Autonomous Systems, 25(3-4), 1998.  [4] R. A. Howard. Dynamic Programming and Markov Processes. MIT, 1960.  [5] L. Kaelbling, A. R. Cassandra, and J. A. Kurien. Acting under uncertainty: Discrete Bayesian  models for mobile-robot navigation. In IROS, 1996.  [6] S. Koenig and R. Simmons. The effect of representation and knowledge on goal-directed explo-  ration with reinforcement learning algorithms. Machine Learning Journal, 22:227-250, 1996.  [7] J.-C. Latombe. Robot Motion Planning. Kluwer Academic Publishers, 1991.  [8] S. Mahadevan and N. Khaleeli. Robust mobile robot navigation using partially-observable  semi-Markov decision processes. 1999.  [9] H. P. Moravec and A. Elfes. High resolution maps from wide angle sonar. In ICRA, 1985.  [10] R. Sim and G. Dudek. Mobile robot localization from learned landmarks. In IROS, 1998.  [11] H. Takeda, C. Facchinetti, and J.-C. Latombe. Planning the motions of mobile robot in a sensory  uncertainty field. IEEE Trans. on Pattern Analysis and Machine Intelligence, 16(10), 1994.  [12] S. Thrun, D. Fox, and W. Burgard. A probabilistic approach to concurrent mapping and local-  ization for mobile robots. Machine Learning, 431, 1998.  
Bayesian Transduction  Thore Graepel, Ralf Herbrich and Klaus Obermayer  Department of Computer Science  Technical University of Berlin  Franklinstr. 28/29, 10587 Berlin, Germany  { graepe12, ralfh, oby} @cs. tu-berlin. de  Abstract  Transduction is an inference principle that takes a training sam-  ple and aims at estimating the values of a function at given points  contained in the so-called working sample as opposed to the whole  of input space for induction. Transduction provides a confidence  measure on single predictions rather than classifiers -- a feature  particularly important for risk-sensitive applications. The possibly  infinite number of functions is reduced to a finite number of equiv-  alence classes on the working sample. A rigorous Bayesian analysis  reveals that for standard classification loss we cannot benefit from  considering more than one test point at a time. The probability  of the label of a given test point is determined as the posterior  measure of the corresponding subset of hypothesis space. We con-  sider the PAC setting of binary classification by linear discriminant  functions (perceptrons) in kernel space such that the probability of  labels is determined by the volume ratio in version space. We  suggest to sample this region by an ergodic billiard. Experimen-  tal results on real world data indicate that Bayesian Transduction  compares fa-vourably to the well-known Support Vector Machine,  in particular if the posterior probability of labellings is used as a  confidence measure to exclude test points of low confidence.  I Introduction  According to Vapnik [9], when solving a given problem one should avoid solving a  more 9eneral problem as an intermediate step. The reasoning behind this principle is  that in order to solve the more general task resources may be wasted or compromises  may have to be made which would not have been necessary for the solution of the  problem at hand. A direct application of this common-sense principle reduces the  more general problem of inferring a functional dependency on the whole of input  space to the problem of estimating the values of a function at given points (working  sample), a paradigm referred to as transductive inference. More formally, given a  probability measure Px on the space of data l' x y = 2c' x {-1, +1}, a training  sample $ = {(x, y),..., (xt, yt)} is generated i.i.d. according to Px. Additional  m data points W - {xl+,... ,xl+,} are drawn: the working sample. The goal  is to label the objects of the working sample W using a fixed set 7/ of functions  Bayesian Transduction 457  f : A'  {-1,+1} so as to minimise a predefined loss. In.contrast, inductive  inference, aims at choosing a single function ft 7-I best suited to capture the  dependency expressed by the unknown Px. Obviously, if we have a transductive  algorithm .A (W, S, 7/) that assigns to each working sample W a set of labels given  the training sample $ and the set 7/of functions, we can define a function fs: 2   {- 1, + 1 } by fs (x) = A ({x}, $, 7/) as a result of the transduction algorithm. There  are two crucial differences to induction, however: i) A ({x}, S, 7/) is not restricted  to select a single decision function f E 7/for each x, ii) a transduction algorithm  can give performance guarantees on particular labellings instead of functions. In  practical applications this difference may be of great importance.  After all, in risk sensitive applications (medical diagnosis, financial and critical  control applications) it often matters to know how confident we are about a given  prediction. In this case a general confidence measure of the classifier w.r.t. the  whole input distribution would not provide the desired warranty at all. Note that  for linear classifiers some guarantee can be obtained by the margin [7] which in  Section 4 we will demonstrate to be too coarse a confidence measure. The idea of  transduction was put forward in [8], where also first algorithmic ideas can be found.  Later [1] suggested an algorithm for transduction based on linear programming and  [3] highlighted the need for confidence measures in transduction.  The paper is structured as follows: A Bayesian approach to transduction is formu-  lated in Section 2. In Section 3 the function class of kernel perceptrons is introduced  to which the Bayesian transduction scheme is applied. For the estimation of volumes  in parameter space we present a kernel billiard as an efficient sampling technique.  Finally, we demonstrate experimentally in Section 4 how the confidence measure  for labellings helps Bayesian Transduction to achieve low generalisation error at a  low rejection rate of test points and thus to outperform Support Vector Machines  (SVMs).  2 Bayesian Transductive Classification  Suppose we are given a training sample $ = { (Xl, Vl),..., (xl, yl) ) drawn i.i.d. from  Px and a working sample W = {Xt+l,... ,xt+,} drawn i.i.d. from Px. Given  a prior PH over the set 7 of functions and a likelihood P(x)tlH=! we obtain a  def  posterior probability PHi(X)t=S = PHIS by Bayes' rule. This posterior measure  induces a probability measure on labellings b E {-1, +1} " of the working sample  by I  PY'"IS,W (b) dej PHIS ({f' Vx+i  W f (x+i) = hi})   (1)  For the sake of simplicity let us assume a PAC style setting, i.e. there exists a  function f* in the space 7 such that P1x=x (Y) = 5 (y - f* (x)). In this case one  can define the so-called version-space as the set of functions that is consistent with  the training sample  V($)={f:V(xi,yi)eS f(xi)=yi}, (2)  outside which the posterior Psis vanishes. Then PY"lS,W (b) represents the prior  measure of functions consistent with the training sample $ and the labelling b  on the working sample W normalised by the prior measure of functions consistent  with $ alone. The measure PH can be used to incorporate prior knowledge into  Note that the number of different labellings b implementable by 7/is bounded above  by the value of the growth function IIt (IWl) [8, p. 321].  458 T. Graepel, R. Herbrich and K. Obermayer  the inference process. If no such knowledge is available, considerations of symmetry  may lead to "uninformative" priors.  Given the measure P,qs,w over labellings, in order to arrive at a risk minimal  decision w.r.t. the labelling we need to define a loss function I: ym x ym  IR+  between labellings and minimise its expectation,  / (b, $, W) - EYlS,W It (b, Y")] =  l (b, b') PYlS,W (b'), (3)  {b'}  where the summation runs over all the 2 " possible labellings b  of the working  sample. Let us consider two scenarios:  1. A 0-I-loss on the exact labelling b, i.e. for two labellings b and b   /(b,b')=l-l-IS(bi-b)  R(b,S,W)= l-P.s,w(b) . (4)  i=1  In this case choosing the labelling be = argmin b Re (b, $, W) of the highest  joint probability P"lS,W (b) minimises the risk. This non-labelwise loss is  appropriate if the goal is to exactly identify a combination of labels, e.g. the  combination of handwritten digits defining a postal zip code. Note that  classical SVM transduction (see, e.g. [8, 1]) by maximising the margin on  the combined training and working sample approximates this strategy and  hence does not minimise the standard classification risk on single instances  as intended.  2. A 0-i-loss on the single labels bi, i.e. for two labellings b and b   m  /,(b,b') = ly(1-(bi-b)), (5)  i=1  m  (b,S, W) =  - (0, - o',)) (b')  i=1 {b,}  -  i=1  Due to the independent treatment of the loss at working sample points the  risk R, (b, S, W) is minimised by the labelling of highest marginal proba-  bility of the labels, i.e.  b = argmaxvey P.lS ({f: f (xt+i) = y)).  Thus in the case of the labelwise loss (5) a working sample of m > 1  point does not offer any advantages over larger working samples w.r.t. the  Bayes-optimal decision. Since this corresponds to the standard classifica-  tion setting, we will restrict ourselves to working samples of size m = 1,  i.e. to one working point xt+l.  3 Bayesian Transduction by Volume  3.1 The Kernel Perceptron  We consider transductive inference for the class of kernel perceptrons. The decision  functions are given by  f (x) = sign ((w,b (x))j,) = sign ctik (xi,x) w =  climb (xi) e ',  i=1 i=1  Bayesian Transduction 459  Figure 1: Schematic view of data space (left) and parameter space (right) for a  classification toy example. Using the duality given by (w, b (x)}y = 0 data points  on the left correspond to hyperplanes on the right, while hyperplanes on the left  can be thought of as points on the right.  where the mapping b: X  Y maps from input space X to a feature space   completely determined by the inner product function (kernel) k : 2 x 2d    (see [9, 10]). Given a training sample $ = {(xi,yi)}i=l we can define the version  space -- the set of all perceptrons compatible with the training data -- as in (2)  having the additional constraint Ilwll - i ensuring uniqueness. In order to obtain  a prediction on the label bl of the working point Xl+l we note that Xl+l may  bisects the volume V of version space into two sub-volumes V + and V-, where the  perceptrons in V + would classify Xl+l as b = +1 and those in V- as b = -1.  The ratio p+ = V+/V is the probability of the labelling b = +1 given a uniform  prior P, over w and the class of kernel perceptrons, accordingly for bl = -1 (see  Figure 1). Already Vapnik in [8, p. 323] noticed that it is troublesome to estimate  sub-volumes of version space. As the solution to this problem we suggest to use a  billiard algorithm.  3.2 Kernel Billiard for Volume Estimation  The method of playing billiard in version space was first introduced by Rujan [6]  for the purpose of estimating its centre of mass and consequently refined and ex-  tended to kernel spaces by [4]. For Bayesian Transduction the idea is to bounce  the billiard ball in version space and to record how much time it spends in each  of the sub-volumes of interest. Under the assumption of ergodicity [2] w.r.t. the  uniform measure in the limit the accumulated flight times for each sub-volume are  proportional to the sub-volume itself.  Since the trajectory is located in  each position w and direction v of the ball can  be expressed as linear combinations of the b (xi), i.e.  w= v =  i=1 i=1 i,j=l  where c,/ are real vectors with  components and fully determine the state of the  billiard. The algorithm for the determination of the label b of Xt+l proceeds as  follows:  1. Initialise the starting position w0 in V (S) using any kernel perceptron  algorithm that achieves zero training error (e.g. SVM [9]). Set V + = V- =  0.  460 T. Graepel, R. Herbrich and K. Obermayer  2. Find the closest boundary of V ($) starting from current w into direction  v, where the flight times vj for all points including Xt+l are determined  using  The smallest positive flight time vc - minj:rs>0 vj in kernel space corre-  sponds to the closest data point boundary  (xc) on the hypersphere. Note,  that if vc   we randomly generate a direction v pointing towards version  space, i.e. y(v,(x)) p 0 suming the lt bounce w at (x).  3. Calculate the hall's new position w  according to  w+rcv  W t =  IIw +  Calculate the distance t = []w- W']]sphere = arccos (1- ]]w- w'[]  on the hypersphere and add it to the volume estimate V corresponding to  the current label V = sign ({w + w',0(xt+))). If the test point 0 (x+)  w hit, i.e. e =  + 1, keep the old direction vector v. Otherwise update  to the reflection direction v',  v' = v- 2 (v, 0(xc)) 0 (xc).  Go back to step 2 unless the stopping criterion (8) is met.  Note that in practice one trajectory can be calculated in advance and can be used  for all test points. The estimators of the probability of the labellings are then given  by  = V+/(V + q- V-) and - = V-/(V + q- V-). Thus, the algorithm outputs  b with confidence ctrans according to  bl der  = argmaxvy 'Y, (6)  .. der (2 max( + -) 1)  [0,1]. (7)  Ctrans -- . , --  Note that the Bayes Point Machine (BPM) [4] aims at an optimal approximation  of the transductive classification (6) by a single function f E 7/ and that the well  known SVM can be viewed as an approximation of the BPM by the centre of the  largest ball in version space. Thus, treating the real valued output If(xl+x)l 4ef    Cind  of SVM classifiers  a confidence measure can be considered an approximation of  (7). The consequences will be demonstrated experimentally in the following section.  Disregarding the issue of mixing time [2] and the dependence of trajectories we  sume for the stopping criterion that the fraction p of time t spent in volume  V + on trajectory i of length (t + t) is a random variable having expectation p+.  Hoeffding's inequality [5] bounds the probability of deviation from the expectation  p+ by more than e,  P p p+ >e < exp (-2he 2) def  - _ _ = v. (8)  i=1  Thus if we want the deviation e from the true label probability to be less than  e < 0.05 with probability at let 1 - V = 0.99 we need approximately n m 1000  bounces. The computational effort of the above algorithm for a working set of size  m is of order O (n (m + )).  Bayesian Transduction 461  rejection rate  '"""t,,h. ' '-I--+.+.  ' q,.., ' ''t"-+.or.+. '  oo o,s 050 o oo os oo  rejection rate  Figure 2: Generalisation error vs. rejection rate for Bayesian Transduction and  SVMs for the ;hyroid data set (or = 3) (a) and the heart data set (or = 10).  The error bars in both directions indicate one standard deviation of the estimated  means. The upper curve depicts the result for the SVM algorithm; the lower curve  is the result obtained by Bayesian Transduction.  4 Experimental Results  We focused on the confidence Xtrans Bayesian Transduction provides together with  the prediction 1 of the label. If the confidence C'rans reflects reliability of a label  estimate at a given test point then rejecting those test points whose predictions carry  low confidence should lead to a reduction in generalisation error on the remaining  test points. In the experiments we varied a rejection threshold 0 between [0, 1] thus  obtaining for each 0 a rejeection rate together with an estimate of the generalisation  error at non-rejected points. Both these curves were linked by their common O-axis  resulting in a generalisation error versus rejection rate plot.  We used the UCI 2 data sets ;hyrod and hear; because they are medical ap-  plications for which the confidence of single predictions is particularly important.  Also a high rejection rate due to too conservative a confidence measure may in-  cur considerable costs. We trained a Support Vector Machine using RBF kernels  k (x, x ) - exp (-tlx - xll 2/2or 2) with cr chosen such as to insure the existence of a  version space. We used 100 different training samples obtained by random 60%:40%  splits of the whole data set. The margin C'nd of each test point was calculated as a  confidence measure of SVM classifications. For comparison we determined the la-  bels b and resulting confidences Ctrans using the Bayesian Transduction algorithm  (see Section 3) with the same value of the kernel parameter. Si.nce the rejection for  the Bayesian Transduction was in both cases higher than for SVMs at the same level  0 we determined 0max which achieves the same rejection rate for the SVM confi-  dence measures as Bayesian Transduction achieves at 0: 1 (l:hyrod: 0ma x --- 2.15,  hear1:: 0max = 1.54). The results for the two data sets are depicted in Figure 2.  In the l:hyrod example Figure 2 (a) one can see that crans is indeed an appropriate  indicator of confidence: at a rejection rate of approximately 20% the generalisation  error approaches zero at minimal variance. For any desired generalisation error  Bayesian Transduction needs to reject significantly less examples of the test set as  compared to SVM classifiers, e.g. 4% less at 2.3% generalisation error. The results of  the hear1: data set show even more pronounced characteristics w.r.t. to the rejection  2UCI University of California at Irvine: Machine Learning Repository  462 T. Graepel, R. Herbrich and K. Obermayer  rate. Note that those confidence measures considered cannot capture the effects of  noise in the data which leads to a generalisation error of 16.4% even at maximal  rejection 0 = 1 corresponding to the Bayes error under the given function class.  5 Conclusions and Future Work  In this paper we a presented a Bayesian analysis of transduction. The required  volume estimates for kernel perceptrons in version space are performed by an ergodic  billiard in kernel space. Most importantly, transduction not only determines the  label of a given point but also returns a confidence measure of the classification  in the form of the probability of the label under the model. Using this confidence  measure to reject test examples then lead to improved generalisation error over  SVMs. The billiard algorithm can be extended to the case of non-zero training  error by allowing the ball to penetrate walls, a property that is cap, tured by adding  a constant A to the diagonal of the kernel matrix [4]. Further research will aim at  the discovery of PAC-Bayesian bounds on the generalisation error of transduction.  Acknowledgements  We are greatly indebted to U. Kockelkorn for many interesting suggestions and  discussions. This project was partially funded by Technical University of Berlin via  rip 13/41.  References  [1] K. Bennett. Advances in Kernel Methods -- Support Vector Learning, chapter 19,  Combining Support Vector and Mathematical Programming Methods for Classifica-  tion, pages 307-326. MIT Press, 1998.  [2] I. Cornfeld, S. Fomin, and Y. Sinai. Ergodic Theory. Springer Verlag, 1982.  [3] A. Gammerman, V. Vovk, and V. Vapnik. Learning by transduction. In Proceedings  of Uncertainty in AI, pages 148-155, Madison, Wisconsin, 1998.  [4] R. Herbrich, T. Graepel, and C. Campbell. Bayesian learning in reproducing kernel  Hilbert spaces. Technical report, Technical University Berlin, 1999. TR 99-11.  [5] W. Hoeffding. Probability inequalities for sums of bounded random variables. Journal  of the American Statistical Association, 58:13-30, 1963.  [6] P. Rujkn. Playing billiard in version space. Neural Computation, 9:99-122, 1997.  [7] J. Shawe-Taylor. Confidence estimates of classification accuracy on new examples.  Technical report, Royal Holloway, University of London, 1996. NC2-TR-1996-054.  [8] V. Vapnik. Estimation of Dependences Based on Empirical Data. Springer, 1982.  [9] V. Vapnik. The Nature of Statistical Learning Theory. Springer, 1995.  [10] G. Wahba. Spline Models for Observational Data. Society for Industrial and Applied  Mathematics, Philadelphia, 1990.  
v-Arc: Ensemble Learning  in the Presence of Outliers  G. Ritsch t, B. Sch51kopf t, A. Smola*,  K.-R. Miillert, T. Onoda**, and S. Mika*  t GMD FIRST, Rudower Chaussee 5, 12489 Berlin, Germany  Microsoft Research, I Guildhall Street, Cambridge CB2 3NH, UK   Dep. of Engineering, ANU, Canberra ACT 0200, Australia  t CRIEPI, 2-11-1, Iwado Kita, Komae-shi, Tokyo, Japan  {raetsch, klaus, mika}first.gmd.de, bscmicrosoft. com,  Alex. Smolaanu. edu. au, onodacriepi. denken. or. j p  Abstract  AdaBoost and other ensemble methods have successfully been ap-  plied to a number of classification tasks, seemingly defying prob-  lems of overfitting. AdaBoost performs gradient descent in an error  function with respect to the margin, asymptotically concentrating  on the patterns which are hardest to learn. For very noisy prob-  lems, however, this can be disadvantageous. Indeed, theoretical  analysis has shown that the margin distribution, as opposed to just  the minimal margin, plays a crucial role in understanding this phe-  nomenon. Loosely speaking, some outliers should be tolerated if  this has the benefit of substantially increasing the margin on the  remaining points. We propose a new boosting algorithm which al-  lows for the possibility of a pre-specified fraction of points to lie in  the margin area or even on the wrong side of the decision boundary.  1 Introduction  Boosting and related Ensemble learning methods have been recently used with great  success in applications such as Optical Character Recognition (e.g. [8, 16]).  The idea of a large minimum margin [17] explains the good generalization perfor-  mance of AdaBoost in the low noise regime. However, AdaBoost performs worse  on noisy tasks [10, 11], such as the iris and the breast cancer benchmark data sets  [1]. On the latter tasks, a large margin on all training points cannot be achieved  without adverse effects on the generalization error. This experimental observation  was supported by the study of [13] where the generalization error of ensemble meth-  ods was bounded by the sum of the fraction of training points which have a margin  smaller than some value p, say, plus a complexity term depending on the base hy-  potheses and p. While this bound can only capture part of what is going on in  practice, it nevertheless already conveys the message that in some cases it pays to  allow for some points which have a small margin, or are misclassified, if this leads  to a larger overall margin on the remaining points.  To cope with this problem, it was mandatory to construct regularized variants of  AdaBoost, which traded off the number of margin errors and the size of the margin  562 G. Riitsch, B. Sch6lkopf, A. J. Smola, K.-R. Mgller, T. Onoda andS. Mika  [9, 11]. This goal, however, had so far been achieved in a heuristic way by introduc-  ing regularization parameters which have no immediate interpretation and which  cannot be adjusted easily.  The present paper addresses this problem in two ways. Primarily, it makes an algo-  rithmic contribution to the problem of constructing regularized boosting algorithms.  However, compared to the previous efforts, it parameterizes the above trade-off in  a much more intuitive way: its only free parameter directly determines the fraction  of margin errors. This, in turn, is also appealing from a theoretical point of view,  since it involves a parameter which controls a quantity that plays a crucial role in  the generalization error bounds (cf. also [9, 13]). Furthermore, it allows the user  to roughly specify this parameter once a reasonable estimate of the expected error  (possibly from other studies) can be obtained, thus reducing the training time.  2 Boosting and the Linear Programming Solution  Before deriving a new algorithm, we briefly discuss the properties of the solution  generated by standard AdaBoost and, closely related, Arc-GV [2], and show the  relation to a linear programming (LP) solution over the class of base hypotheses G.  Let {gt(x)  t = 1,... ,T} be a sequence of hypotheses and c = [o1...OT] their  weights satisfying ct > 0. The hypotheses gt are elements of a hypotheses class  G = {g' x + [-1, 1]}, which is defined by a base learning algorithm.  The ensemble generates the label which is the weighted majority of the votes by  sign(f(x)) where f(x)- iillgz(x). (1)  In order to express that f and therefore also the margin p depend on c and for ease  of notation we define  p(z,a) := yf(x) where z := (x,y) and f is defined as in (1). (2)  Likewise we use the normalized margin:  p(c) := min p(zi, or) (3)  l<i<m '  Ensemble learning methods have to find both, the hypotheses gt  G used for the  combination and their weights c. In the following we will consider only AdaBoost  algorithms (including Arcing). For more details see e.g. [4, 2]. The main idea of  AdaBoost is to introduce weights wt(zi) on the training patterns. They are used to  control the importance of each single pattern in learning a new hypothesis (i.e. while  repeatedly running the base algorithm). Training patterns that are difficult to learn  (which are misclassified repeatedly) become more important.  The minimization objective of AdaBoost can be expressed in terms of margins as  (c,) := y exp(-Ilc, llp(zi, c,)) . (4)  i----1  In every iteration, AdaBoost tries to minimize this error by a stepwise maximization  of the margin. It is widely believed that AdaBoost tries to maximize the smallest  margin on the training set [2, 5, 3, 13, 11]. Strictly speaking, however, a general  proof is missing. It would imply that AdaBoost asymptotically approximates (up to  scaling) the solution of the following linear programming problem over the complete  hypothesis set G (cf. [7], assuming a finite number of basis hypotheses):  maximize p  subject to p(zi,e) >p for alll<i<rn  ct,p>_0 for alll<t<lGI (5)  Ilalll-- 1  u-Arc: Ensemble Learning in the Presence of Outliers 563  Since such a linear program cannot be solved exactly for a infinite hypothesis set  in general, it is interesting to analyze approximation algorithms for this kind of  problems.  Breiman [2] proposed a modification of AriaBoost - Arc-GV - making it possible  to show the asymptotic convergence of p(ot t) to the global solution pip:  Theorem I (Breiman [2]). Choose at in each iteration as  at := argmin  exp [-II*ll (P(Zi, Oft) -- P(O?-))] , (6)  ce[O,1]   and assume that the base learner always finds the hypothesis g  G which minimizes  the weighted training error with respect to the weights. Then  lira p(c t) = pip.  Note that the algorithm above can be derived from the modified error function  gv(O?) :: E exp [-[[otl[1 (p(zi, o t) - p(t-))]. (7)  i  The question one might ask now is whether to use AdaBoost or rather Arc-GV  in practice. Does Arc-GV converge fast enough to benefit from its asymptotic  properties? In [12] we conducted experiments to investigate this question. We  empirically found that (a) AdaBoost has problems finding the optimal combination  if plp < 0, (b) Arc-GV's convergence does not depend on plp, and (c) for plp > 0,  AdaBoost usually converges to the maximum margin solution slightly faster than  Arc-GV. Observation (a) becomes clear from (4): (c) will not converge to 0 and  IIlll can be bounded by some value. Thus the asymptotic case cannot be reached,  whereas for Arc-GV the optimum is always found.  Moreover, the number of iterations necessary to converge to a good solution seems to  be reasonable, but for a near optimal solution the number of iterations is rather high.  This implies that for real world hypothesis sets, the number of iterations needed  to find an almost optimal solution can become prohibitive, but we conjecture that  in practice a reasonably good approximation to the optimum is provided by both  AdaBoost and Arc-GV.  3 v-Algorithms  For the LP-AdaBoost [7] approach it has been shown for noisy problems that the  generalization performance is usually not as good as the one of AdaBoost [7, 2, 11].  From Theorem 5 in [13] (cf. Theorem 3 on page 5) this fact becomes clear, as  the minimum of the right hand side of inequality (cf. (13)) need not necessarily be  achieved with a maximum margin. We now propose an algorithm to directly control  the number of margin errors and therefore also the contribution of both terms in the  inequality separately (cf. Theorem 3). We first consider a small hypothesis class  and end up with a linear program - v-LP-AdaBoost. In subsection 3.2 we then  combine this algorithm with the ideas from section 2 and get a new algorithm -  v-Arc - which approximates the v-LP solution.  3.1 v-LP-AdaBoost  Let us consider the case where we are given a (finite) set G = {g: x + [-1, 1]} of T  hypotheses. To find the coefficients c for the combined hypothesis f(x) we extend  the LP-AdaBoost algorithm [7, 11] by incorporating the parameter v [15] and solve  the following linear optimization problem:  m  1 Ei----1 i  maximize P ,m  subject to p(zi,c)_>p-i for alll<i<m  - - (8)  i,at,P_>0 for alll_<t_<Tandl<i<m  I1111- 1  564 G. Riitsch, B. SchOlkopf, A. J. Smola, K.-R. Miiller, T. Onoda andS. Mika  This algorithm does not force all margins to be beyond zero and we get a soft  margin classification (cf. SVMs) with a regularization constant i . The following  proposition shows that v has an immediate interpretation:  Proposition 2 (Rfitsch et al. [12]). Suppose we run the algorithm given in (8)  on some data with the resulting optimal p > O. Then  1. v upper bounds the fraction of margin errors.  2. 1 - v upper bounds the fraction of patterns with margin larger than p.  Since the slack variables i only enter the cost function linearly, their absolute size  is not important. Loosely speaking, this is due to the fact that for the optimum  of the primal objective function, only derivatives wrt. the primal variables matter,  and the derivative of a linear function is constant.  In the case of SVMs [14], where the hypotheses can be thought of as vectors in  some feature space, this statement can be translated into a precise rule for dis-  torting training patterns without changing the solution: we can move them locally  orthogonal to a separating hyperplane. This yields a desirable robustness property.  Note that the algorithm essentially depends on the number of outliers, not on the  size of the error [15].  3.2 The v-Arc Algorithm  Suppose we have a very large (but finite) base hypothesis class G. Then it is difficult  to solve (8) as (5) directly. To this end, we propose a new algorithm - v-Arc - that  approximates the solution of (8).  The optimal p for fixed margins p(zi, Or) in (8) can be written as  ( I m  p(c) := argrnax p - Z(p- p(zi,oO)+  (9)  pe[O,1] vm i=1  where ()+ := max(,O). Setting i := (pv(a)- p(zi a))+ and subtracting  I m '  v, i=1 i from the resulting inequality on both sides yields (for all I < i < m)  m m  P(zi,o0 + i 1 Zi _> p(c) 1 Zi' (10)  i=1 i=1  Two more substitutions are needed to transform the problem into one which can  be solved by the AdaBoost algorithm. In particular we have to get rid of the slack  variables i again by absorbing them into quantities similar to p(zi, or) and p(c).  This works as follows: on the right hand side of (10) we have the objective function  (cf. (8)) and on the left hand side a term that depends nonlinearly on c. Defining  fig m  tS(c) := p(c) - __1 Z i and tS(zi,c):= p(zi,c) + i- __1  i, (11)  i=1 i=1  which we substitute for p(c) and p(z,c) in (5), respectively, we obtain a new  optimization problem. Note that tSv(c) and tS(zi,c) play the role of a corrected  or virtual margin. We obtain a nonlinear min-max problem  maximize  subject to tS(zi,c) _>/5(c) for all 1 _< i _< rn  at>_0 for all 1_< t <_ T  (12)  which Arc-GV can solve approximately (cf. section 2). Hence, by replacing the mar-  gin p(z, c) by/5(z, c) in equation (4) and the other formulas for Arc-GV (cf. [2]),  u-Arc: Ensemble Learning in the Presence of Outliers 565  we obtain a new algorithm which we refer to as v-Arc.  We can now state interesting properties for v-Arc by using Theorem 5 of [13] that  bounds the generalization error R(f) for ensemble methods. In our case Rp (f) _< v  by construction (i.e. the number of patterns with a margin smaller than p, cf. Propo-  sition 2), thus we get the following simple reformulation of this bound:  Theorem 3. Let p(x, y) be a distribution over A' x [-1, 1], and let X be a sample  of m examples chosen lid according to p. Suppose the base-hypothesis space G has  VC dimension h, and let 5  O. Then with probability at least I - 5 over the  random choice of the training set X, Y, every function f generated by v-Arc, where  v  (0, 1) and pv  O, satisfies the following bound:  R(f) _< v + + log ). (13)  So, for minimizing the right hand side we can tradeoff between the first and the  second term by controlling an easily interpretable regularization parameter v.  4 Experiments  We show a set of toy experiments to illustrate the general behavior of v-Arc. As  base hypothesis class G we use the RBF networks of [11], and as data a two-class  problem generated from several 2D Gauss blobs (cf. Banana shape dataset from  bttp://www.first.gmd.de/~data/banana.btm[.). We obtain the following results:   v-Arc leads to approximately vrn patterns that are effectively used in  the training of the base learner: Figure I (left) shows the fraction  of patterns that have high average weights during the learning process  (i.e. EtT= wt(zi) ) 1/2m). We find that the number of the latter increases  (almost) linearly with v. This follows from (11) as the (soft) margin of  patterns with p(z, c) ( pv is set to p and the weight of those patterns will  be the same.   The (estimated) test error, averaged over 10 training sets, exhibits a rather  fiat minimum in v (Figure I (lower)). This indicates that just as for v-  SVMs, where corresponding results have been obtained, v is a well-behaved  parameter in the sense that a slight misadjustment it is not harmful.   v-Arc leads to the fraction v of margin errors (cf. dashed line in Figure 1)  exactly as predicted in Proposition 2.   Finally, a good value of v can already be inferred from prior knowledge of  the expected error. Setting it to a value similar to the latter provides a  good starting point for further optimization (cf. Theorem 3).  Note that for v = 1, we recover the Bagging algorithm (if we used bootstrap  samples), as the weights of all patterns will be the same (wt(zi) = 1/m for all  i = 1,... , m) and also the hypothesis weights will be constant (at  1IT for all  t= 1,... ,T).  Finally, we present a small comparison on ten benchmark  data sets obtained from the UCI [1] benchmark repository  (cf. http://da.frst.gmd.de/~raetsch/data/benchraaxks.htral). We an-  alyze the performance of single RBF networks, AdaBoost, v-Arc and RBF-SVMs.  For AdaBoost and v-Arc we use RBF networks [11] as base hypothesis. The  model parameters of RBF (number of centers etc.), v-Arc (v) and SVMs (rr, C) are  optimized using 5-fold cross-validation. More details on the experimental setup can  566 G. Riitsch, B. SchGlkopf A. J. Smola, K.-R. Miller, T. Onoda andS. Mika  0.8 number of important   patterns  _ o.6 number of   margin errors _.    0.4 t ]  training error  0.2  Figure 1: Toy experiment (a = 0): the left  0'16 t .........  0.12  0.11  graph shows the average fraction of important  patterns, the av. fraction of margin errors and the av. training error for different values  of the regularization constant y for y-Arc. The right graph shows the corresponding  generalization error. In both cases, the parameter v allows us to reduce the test errors  to values much lower than for the hard margin algorithm (for y -- 0 we recover Arc-  GV/AdaBoost, and for y - I we get Bagging.)  be found in [11]. Fig. 1 shows the generalization error estimates (after averaging  over 100 realizations of the data sets) and the confidence interval. The results  of the best classifier and the classifiers that are not significantly worse are set in  bold face. To test the significance, we used a t-test (p = 80%). On eight out of  the ten data sets, v-Arc performs significantly better than AdaBoost. This clearly  shows the superior performance of v-Arc for noisy data sets and supports this soft  margin approach for AdaBoost. Furthermore, we find comparable performances  for v-Arc and SVMs. In three cases the SVM performs better and in two cases  v-Arc performs best. Summarizing, AdaBoost is useful for low noise cases, where  the classes are separable. v-Arc extends the applicability of boosting to problems  that are difficult to separate and should be applied if the data are noisy.  5 Conclusion  We analyzed the AdaBoost algorithm and found that Arc-GV and AdaBoost are  efficient for approximating the solution of non-linear min-max problems over huge  hypothesis classes. We re-parameterized the LPReg-AdaBoost algorithm (cf. [7, 11])  and introduced a new regularization constant v that controls the fraction of pat-  terns inside the margin area. The new parameter is highly intuitive and has to be  optimized only on a fixed interval [0, 1].  Using the fact that Arc-GV can approximately solve min-max problems, we found a  formulation of Arc-GV - v-Arc- that implements the v-idea for Boosting by defining  an appropriate soft margin. The present paper extends previous work on regular-  izing boosting (DOOM [9], AdaBoostReg [11]) and shows the utility and flexibility  of the soft margin approach for AdaBoost.  Banana  B.Cancer  Diabetes  German  Heart  Ringnorm  F.Sonar  Thyroid  Titanic  Waveform  RBF  10.8 + 0.06  27.6 4- 0.47  24.3 + 0.19  24.7 4- 0.24  AB v-Arc  12.3 + 0.07 10.6 + 0.05  30.4 4- 0.47 25.8 4- 0.46  26.5 + 0.23 23.7 + 0.20  27.5 4- 0.25 24.4 4- 0.22  SVM  11.5 + 0.07  26.0 + 0.47  23.5 4- 0.17  23.6 4- 0.21  17.6 4- 0.33  1.7 4- 0.02  34.4 4- 0.20  4.5 4- 0.21  23.3 4- 0.13  10.7 4- 0.11  20.3 4- 0.34  1.9 4- 0.03  35.7 4- 0.18  4.4 4- 0.22  22.6 4- 0.12  10.8 + 0.06  16.5 4- 0.36  1.7 4- 0.02  34.4 4- 0.19  4.4 4- 0.22  23.0 4- 0.14  10.0 4- 0.07  16.0 4- 0.33  1.7 4- 0.01  32.4 + 0.18  4.8 4- 0.22  22.4 4- 0.10  9.9 4- 0.04  Table 1: Generalization error estimates and confidence intervals. The best classifiers for a  particular data set are marked in bold face (see text).  u-Arc: Ensemble Learning in the Presence of Outliers 567  We found empirically that the generalization performance in v-Arc depends only  slightly on the choice of the regularization constant. This makes model selection  (e.g. via cross-validation) easier and faster.  Future work will study the detailed regularization properties of the regularized ver-  sions of AdaBoost, in particular in comparison to v-LP Support Vector Machines.  Acknowledgments: Partial funding from DFG grant (Ja 379/52) is gratefully  acknowledged. This work was done while AS and BS were at GMD FIRST.  References  [1] C. Blake, E. Keogh, and C. J. Merz. UCI repository of machine learning databases,  1998. http://www.ics.uci.edu/-mlearn/MLRepository. html.  [2] L. Breiman. Prediction games and arcing algorithms. Technical Report 504, Statistics  Department, University of California, December 1997.  [3] M. Frean and T. Downs. A simple cost function for boosting. Technical report, Dept.  of Computer Science and Electrical Eng., University of Queensland, 1998.  [4] Y. Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning  and an application to boosting. In Computational Learning Theory: Eurocolt '95,  pages 23-37. Springer-Verlag, 1995.  [5] Y. Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning  and an application to boosting. J. of Comp. Fj Syst. Sc., 55(1):119-139, 1997.  [6] J. Friedman, T. Hastie, and R. Tibshirani. Additive logistic regression: a statistical  view of boosting. Technical report, Stanford University, 1998.  [7] A. Grove and D. Schuurmans. Boosting in the limit: Maximizing the margin of  learned ensembles. In Proc. of the 15th Nat. Conf. on AI, pages 692-699, 1998.  [8] Y. LeCun, L. D. Jackel, L. Bottou, C. Cortes, J. S. Denker, H. Drucker, I. Guyon,  U. A. Miiller, E. S/ickinger, P. Simard, and V. Vapnik. Learning algorithms for  classification: A comparison on handwritten digit recognition. Neural Networks, pages  261-276, 1995.  [9] L. Mason, P. L. Bartlett, and J. Baxter. Improved generalization through explicit  optimization of margins. Machine Learning, 1999. to appear.  [10] J. R. Quinlan. Boosting first-order learning (invited lecture). Lecture Notes in Com-  puter Science, 1160:143, 1996.  [11] G. R/itsch, T. Onoda, and K.-R. Milllet. Soft margins for AdaBoost. Technical Report  NC-TR-1998-021, Department of Computer Science, Royal Holloway, University of  London, Egham, UK, 1998. To appear in Machine Learning.  [12] G. Ritsch, B. SchSkopf, A. Smola, S. Mika, T. Onoda, and K.-R. Milllet. Robust  ensemble learning. In A.J. Smola, P.L. Bartlett, B. SchSlkopf, and D. Schuurmans,  editors, Advances in LMC, pages 207-219. MIT Press, Cambridge, MA, 1999.  [13] R. Schapire, Y. Freund, P. L. Bartlett, and W. Sun Lee. Boosting the margin: A  new explanation for the effectiveness of voting methods. Annals of Statistics, 1998.  (Earlier appeared in: D. H. Fisher, Jr. (ed.), Proc. ICML97, M. Kaufmann).  [14] B. SchSlkopf, C. J. C. Burges, and A. J. Smola. Advances in Kernel Methods --  Support Vector Learning. MIT Press, Cambridge, MA, 1999.  [15] B. SchSlkopf, A. Smola, R. C. Williamson, and P. L. Bartlett. New support vector  algorithms. Neural Computation, 12:1083 - 1121, 2000.  [16] H. Schwenk and Y. Bengio. Training methods for adaptive boosting of neural net-  works. In Michael I. Jordan, Michael J. Kearns, and Sara A. Solla, editors, Advances  in Neural Inf. Processing Systems, volume 10. The MIT Press, 1998.  [17] V. Vapnik. The Nature of Statistical Learning Theory. Springer Verlag, New York,  1995.  
Wiring optimization in the brain  Dmitri B. Chklovskii  Sloan Center for  Theoretical Neurobiology  The Salk Institute  La Jolla, CA 92037  mitya@salk. edu  Charles F. Stevens  Howard Hughes Medical Institute  and Molecular Neurobiology Lab  The Salk Institute  La Jolla, CA 92037  stevens@salk. edu  Abstract  The complexity of conical circuits may be characterized by the number  of synapses per neuron. We study the dependence of complexity on the  fraction of the conical volume that is made up of"wire" (that is, of axons  and dendrites), and find that complexity is maximized when wire takes  up about 60% of the conical volume. This prediction is in good agree-  ment with experimental observations. A consequence of our arguments  is that any rearrangement of neurons that takes more wire would sacrifice  computational power.  Wiring a brain presents formidable problems because of the extremely large number of con-  nections: a microliter of cortex contains approximately 105 neurons, 109 synapses, and 4  km of axons, with 60% of the conical volume being taken up with "wire", half of this by  axons and the other half by dendrites.[ 1 ] Each conical neighborhood must have exactly the  right balance of components; if too many cell bodies were present in a particular mm cube,  for example, insufficient space would remain for the axons, dendrites and synapses. Here  we ask "What fraction of the conical volume should be wires (axons + dendrites)?" We ar-  gue that physiological properties of axons and dendrites dictate an optimal wire fraction of  0.6, just what is actually observed.  To calculate the optimal wire fraction, we start with a real conical region containing a fixed  number of neurons, a mm cube, for example, and imagine perturbing it by adding or sub-  tracting synapses and the axons and dendrites needed to support them. The rules for per-  turbing the conical cube require that the existing circuit connections and function remain  intact (except for what may have been removed in the perturbation), that no holes are cre-  ated, and that all added (or subtracted) synapses are typical of those present; as wire volume  is added, the volume of the cube of course increases. The ratio of the number of synapses  per neuron in the perturbed cortex to that in the real cortex is denoted by t, a parameter  we call the relative complexity. We require that the volume of non-wire components (cell  bodies, blood vessels, glia, etc) is unchanged by our perturbation and use b to denote the  volume fraction of the perturbed conical region that is made up of wires (axons + dendrites;  b can vary between zero and one), with the fraction for the real brain being b0. The relation  between relative complexity t and wire volume fraction b is given by the equation (derived  in Methods)  104 D. B. Chklovskii and C. F. Stevens  VVire fraction ({)  Figure 1: Relative complexity (0) as a function of volume wire fraction (b). The graphs are  calculated from equation (1) for three values of the parameter A as indicated; this parameter  determines the average length of wire associated with a synapse (relative to this length for  the real cortex, for which (A = 1). Note that as the average length of wire per synapse  increases, the maximum possible complexity decreases.  For the following discussion assume that A = 1; we return to the meaning of this parameter  later. To derive this equation two assumptions are made. First, we suppose that each added  synapse requires extra wire equal to the average wire length and volume per synapse in the  unperturbed cortex. Second, because adding wire for new synapses increases the brain vol-  ume and therefore increases the distance axons and dendrites must travel to maintain the  connections they make in the real cortex, all of the dendrite and unmyelinated axon diam-  eters are increased in proportion to the square of their length changes in order to maintain  the intersynaptic conduction times[2] and dendrite cable lengths[3] as they are in the actual  cortex. If the unmyelinated axon diameters were not increased as the axons become longer,  for example, the time for a nerve impulse to propagate from one synapse to the next would  be increased and we would violate our rule that the existing circuit and its function be un-  changed. We note that the vast majority of conical axons are unmyelinated.[1] The plot of  0 as a function of b is parabolic-like (see Figure 1) with a maximum value at qb = 0.6, a  point at which dO/d(b = 0. This same maximum value is found for any possible value of  b0, the real conical wire fraction.  Why does complexity reach a maximum value at a particular wire fraction? When wire and  synapses are added, a series of consequences can lead to a runaway situation we call the  wiring catastrophe. If we start with a wire fraction less than 0.6, adding wire increases the  cortical volume, increased volume makes longer paths for axons to reach their targets which  requires larger diameter wires (to keep conduction delays or cable attenuation constant from  one point to another), the larger wire diameters increase cortex volume which means wires  must be longer, etc. While the wire fraction b is less than 0.6, increasing complexity is  accompanied by finite increases in b. At b = 0.6 the rate at which wire fraction increases  with complexity becomes infinite dqb/dO - cx); we have reached the wiring catastrophe.  At this point, adding wire becomes impossible without decreasing complexity or making  other changes - like decreasing axon diameters - that alter cortical function. The physical  cause of the catastrophe is a slow growth of conduction velocity and dendritic cable length  with diameter combined with the requirement that the conduction times between synapses  (and dendrite cable lengths) be unchanged in the perturbed cortex.  We assumed above that each synapse requires a certain amount of wire, but what if we could  14ring Optimization in the Brain 105  add new synapses using the wire already present? We do not know what factors determine  the wire volume needed to support a synapse, but if the average amount of wire per synapse  could be less (or more) than that in the actual cortex, the maximum wire fraction would still  be 0.6. Each curve in Figure 1 corresponds to a different assumed average wire length re-  quired for a synapse (determined by ,X), and the maximum always occurs at 0.6 independent  of ,X. In the following we consider only situations in which ,X is fixed.  For a given ,X, what complexity should we expect for the actual cortex? Three arguments  favor the maximum possible complexity. The greatest complexity gives the largest num-  ber of synapses per neuron and this permits more bits of information to be represented per  neuron. Also, more synapses per neuron decreases the relative effect caused by the loss or  malfunction of a single synapse. Finally, errors in the local wire fraction would minimally  affect the local complexity because dO/do) = 0 at b = 0.13. Thus one can understand why  the actual cortex has the wire fraction we identify as optimal.[ 1 ]  This conclusion that the wire fraction is a maximum in the real cortex has an interesting  consequence: components of an actual cortical circuit cannot be rearranged in a way that  needs more wire without eliminating synapses or reducing wire diameters. For example,  if intermixing the cell bodies of left and right eye cells in primate primary visual cortex  (rather than separating them in ocular dominance columns) increased the average length of  the wire[4] the existing circuit could not be maintained just by a finite increase in volume.  This happens because a greater wire length demanded by the rearrangement of the same  circuit would require longer wire per synapse, that is, an increased ,X. As can be seen from  Figure 1, brains with ,X > 1 can never achieve the complexity reached at the maximum of  the ,X - 1 curve that corresponds to the actual cortex.  Our observations support the notion that brains are arranged to minimize wire length. This  idea, dating back to Cajal[5], has recently been used to explain why retinotopic maps ex-  ist[6],[7], why cortical regions are separated, why ocular dominance columns are present in  primary visual cortex[4],[8],[9] and why the cortical areas and fiat worm ganglia are placed  as they are.[ 10-13] We anticipate that maximal complexity/minimal wire length arguments  will find further application in relating functional and anatomical properties of brain.  Methods  The volume of the cube of cortex we perturb is V, the volume of the non-wire portion is  W (assumed to be constant), the fraction of V consisting of wires is b, the total number of  synapses is N, the average length of axonal wire associated with each synapse is s, and the  average axonal wire volume per unit length is h; the corresponding values for dendrites are  indicated by primes (s t and h). The unperturbed value for each variable has a 0 subscript;  thus the volume of the cortical cube before it is perturbed is  t t  Vo = Wo + No(soho + Soho).  (2)  We now define a "virtual" perturbation that we use to explore the extent to which the actual  cortical region contains an optimal fraction of wire. If we increase the number of synapses  by a factor 0 and the length of wire associated with each synapse by a factor A, then the  perturbed cortical cube's volume becomes  Vo = Wo + AO Nosohoo  + Nos[h[ (V/Vo) '/s  (3)  This equation allows for the possibility that the average wire diameter has been perturbed  and increases the length of all wire segments by the "mean field" quantity (V/Vo) /a to  take account of the expansion of the cube by the added wire; we require our perturbation  disperses the added wire as uniformly as possible throughout the cortical cube.  106 D. B. Chklovskii and C. F. Stevens  To simplify this relation we must eliminate h/ho and h'/h[; we consider these terms in turn.  When we perturb the brain we require that the average conduction time (s/u, where u is the  conduction velocity) from one synapse to the next be unchanged so that s/u = so/uo, or  u =  = Aso(V/Vo) Ua = A(V/Vo)Ua. (4)  0 80 80  Because axon diameter is proportional to the square of conduction velocity u and the axon  volume per unit length h is proportional to diameter squared, h is proportional to u a and the  ratio h/ho can be written as  = =- = xa(V/Vo) .  80  (5)  For dendrites, we require that their length from one synapse to the next in units of the cable  length constant be unchanged by the perturbation. The dendritic length constant is propor-  tional to the square root of the dendritic diameter d, so s/x/' = so/v/ or  = = =  (6)  Because dendritic volumes per unit length (h and h') vary as the square of the diameters,  we have that  = = ,x  (VIVo) /3  (7)  The equation (2) can thus be rewritten as  V = Wo + No(soho + 8[h)O,X 5 (VlVo) 5/a  (8)  Divide this equation by Vo, define v = V/Vo, and recognize that Wo/Vo = (1 - &o) and  that &o = No(80ho + 8[h[)/Vo; the result is  v = (1 - 0) + 0As0v 5/a. (9)  Because the non-wire volume is required not to change with the perturbation, we know that  Wo = (1 - bo)Vo = (1 - b)V which means that v = (1 - bo)/(1 - b); substitute this in  equation (9) and rearrange to give  1 (1-4) 2/a 4 (1)  w  the equation used in the main text.  We have assumed that conduction velocity and the dendritic cable length constant vary ex-  actly with the square root of diameter[2],[ 14] but if the actual power were to deviate slightly  from 1/2 the wire fraction that gives the maximum complexity would also differ slightly  from 0.6.  Acknowledgments  This work was supported by the Howard Hughes Medical Institute and a grant from NIH to  C.ES. D.C. was supported by a S1oan Fellowship in Theoretical Neurohiology.  145'ring Optimization in the Brain 107  References  [1] Braitenberg, V. & Schuz, A. Cortex: Statistics and Geometry of Neuronal Connectivity (Springer,  1998).  [2] Rushton, W.^.H. ^ Theory of the Effects of Fibre Size in Medullated Nerve. J. Physiol. 115,  101-122 (1951).  [3] Bekkers, J.M. & Stevens, C.E Two different ways evolution makes neurons larger. Prog Brain Res  83, 37-45 (1990).  [4] Mitchison, G. Neuronal branching patterns and the economy of cortical wiring. Proc R Soc Lond  B Biol Sci 245, 151-158 (1991).  [5] Cajal, S.R.Y. Histology of the Nervous System 1-805 (Oxford University Press, 1995).  [6] Cowey, A. Cortical maps and visual perception: the Grindley Memorial Lecture. Q J Exp Phychol  31, 1-17 (1979).  [7] Allman J.M. & Kaas J.H. The organization of the second visual area (VII) in the owl monkey: a  second order transformation of the visual hemifield. Brain Res 76:247-65 (1974).  [8] Durbin, R. & Mitchison, G. A dimension reduction framework for understanding cortical maps.  Nature 343, 644-647 (1990).  [9] Mitchison, G. Axonal trees and cortical architecture. Trends Neurosci 15, 122-126 (1992).  [ 10] Young, M.P. Objective analysis of the topological organization of the primate cortical visual sys-  tem. Nature 358, 152-154 (1992).  [11] Cherniak, C. Local optimization of neuron arbors. Biol Cybern 66, 503-510 (1992).  [12] Cherniak, C. Component placement optimization in the brain. J Neurosci 14, 2418-2427 (1994).  [13] Cherniak, C. Neural component placement. Trends Neurosci 18, 522-527 (1995).  [14] Rail, W. in Handbook of Physiology, The Nervous Systems, Cellular Biology of Neurons (ed.  Brookhart, J.M.M., V.B.) 39-97 (Am. Physiol. Soc., Bethesda, MD, 1977).  
The Infinite Gaussian Mixture Model  Carl Edward Rasmussen  Department of Mathematical Modelling  Technical University of Denmark  Building 321, DK-2800 Kongens Lyngby, Denmark  carl@imm.dtu.dk http://bayes.imm.dtu.dk  Abstract  In a Bayesian mixture model it is not necessary a priori to limit the num-  ber of components to be finite. In this paper an infinite Gaussian mixture  model is presented which neatly sidesteps the difficult problem of find-  ing the "right" number of mixture components. Inference in the model is  done using an efficient parameter-free Markov Chain that relies entirely  on Gibbs sampling.  1 Introduction  One of the major advantages in the Bayesian methodology is that "overfitting" is avoided;  thus the difficult task of adjusting model complexity vanishes. For neural networks, this  was demonstrated by Neal [ 1996] whose work on infinite networks led to the reinvention  and popularisation of Gaussian Process models [Williams & Rasmussen, 1996]. In this  paper a Markov Chain Monte Carlo (MCMC) implementation of a hierarchical infinite  Gaussian mixture model is presented. Perhaps surprisingly, inference in such models is  possible using finite amounts of computation.  Similar models are known in statistics as Dirichlet Process mixture models and go back  to Ferguson [1973] and Antoniak [1974]. Usually, expositions start from the Dirichlet  process itself [West et al, 1994]; here we derive the model as the limiting case of the well-  known finite mixtures. Bayesian methods for mixtures with an unknown (finite) number  of components have been explored by Richardson & Green [ 1997], whose methods are not  easily extended to multivariate observations.  2 Finite hierarchical mixture  The finite Gaussian mixture model with k components may be written as:  k  P(/Ii,--' ,k,$1,... ,$k,71'1,... ,71'k)-- Z71'jJV'(j,$j-1), (1)  j=l  where/j are the means, sj the precisions (inverse variances), 7rj the mixing proportions  (which must be positive and sum to one) and iV' is a (normalised) Gaussian with specified  mean and variance. For simplicity, the exposition will initially assume scalar observations,  n of which comprise the training data y -- {y,... , Yn}. First we will consider these  models for a fixed value of k, and later explore the properties in the limit where k --> oc.  The Infinite Gaussian Mixture Model 555  Gibbs sampling is a well known technique for generating samples from complicated mul-  tivariate distributions that is often used in Monte Carlo procedures. In its simplest form,  Gibbs sampling is used to update each variable in turn from its conditional distribution  given all other variables in the system. It can be shown that Gibbs sampling generates sam-  ples from the joint distribution, and that the entire distribution is explored as the number of  Gibbs sweeps grows large.  We introduce stochastic indicator variables, ci, one for each observation, whose role is to  encode which class has generated the observation; the indicators take on values 1... k.  Indicators are often referred to as "missing data" in a mixture model context.  In the following sections the priors on component parameters and hyperparameters will  be specified, and the conditional distributions for these, which will be needed for Gibbs  sampling, will be derived. In general the form of the priors are chosen to have (hopefully)  reasonable modelling properties, with an eye to mathematical convenience (through the use  of conjugate priors).  2.1 Component parameters  The component means,/j, are given Gaussian priors:  p(/jlX, r) ,-, Af(A, r-x), (2)  whose mean, ,k, and precision, r, are hyperparameters common to all components. The  hyperparameters themselves are given vague Normal and Gamma priors:  p(,k) A/'(y, ay2), p(r) (1,-2 r-W2exp(_ray2/2), (3)  2 are the mean and variance of the observations . The shape parameter of  where/y and ay  the Gamma prior is set to unity, corresponding to a very broad (vague) distribution.  The conditional posterior distributions for the means are obtained by multiplying the like-  lihood from eq. (1) conditioned on the indicators, by the prior, eq. (2):  p(/ujlc, y, sj,X,r)  jf(jrtjsj + Xr 1 ) 1 >. Yi, (4)  rtj $j q- r ' rtj $j q- r ' J ---- nj   i:ci----j  where the occupation number, n j, is the number of observations belonging to class j, and  j is the mean of these observations. For the hyperparameters, eq. (2) plays the role of  the likelihood which together with the priors from eq. (4) give conditional posteriors of  standard form:  A/'( trytry2 q-  E:i J  try-2 + kr  1 )  try + kr  (5)  P(rlfil,...,fi k ,) ,. (k q- 1, [.1_ (try2 + E(j _ X)2)]-).   k+l  j=l  The component precisions, $j, are given Gamma priors:  p(sjl3, w ) ,, O(3, w-), (6)  whose shape, , and mean, w -, are hypeameters coon to all components, with  priors of inverse Gaa and Gaa form:  p(-)G(1,1)p()m-a/2exp(-1/(2)), p(w)G(1, a). (7)  Stfictly spewing, the priors ought not to depend on the observations. The cent procede is  equivflent to normrising the obseations d using unit priors. A wide vety of reasonable priors  will lead to sil results.  556 C. E. Rasmussen  The conditional posterior precisions are obtained by multiplying the likelihood from eq. (1)  conditioned on the indicators, by the prior, eq. (6):  1  p(sSIc, y, ttS,/?, w) -,, O(/? + nj, [/? + n---. (w/? + E (Y'- tt5)2)]-1) '  i:ci=j  (8)  For the hyperparameters, eq. (6) plays the role of likelihood which together with the priors  from eq. (7) give:  k  ( )  p(wlsl,..., s,3)  G k + 1, [k3 + I Y '  j=l  P(/[$1, . . . ,$k, W) O(F(-)  k  exp ( )( )(a-a)/2 H(sjw)a/2 exp( 2 )'  (9)  The latter density is not of standard form, but it can be shown that p(log(/)lsl,..., sk, w)  is log-concave, so we may generate independent samples from the distribution for log(/)  using the Adaptive Rejection Sampling (ARS) technique [Gilks & Wild, 1992], and trans-  form these to get values for/.  The mixing proportions, 7rj, are given a symmetric Dirichlet (also known as multivariate  beta) prior with concentration parameter a/k:  II ,  p(7rl,... ,7rklc )  Dirichlet(c/k,... ,a/k) = F(c/k) j=l  (10)  where the mixing proportions must be positive and sum to one. Given the mixing propor-  tions, the prior for the occupation numbers, n, is multinomial and the joint distribution of  the indicators becomes:  p(Cl,... ,Ck17r1,... ,7rk) --  k n  Tj = E (JKronecker(Ci,j). (11)  j=l i=1  Using the standard Dirichlet integral, we may integrate out the mixing proportions and  write the prior directly in terms of the indicators:  p(cl,... f (12)  k  f II +  = r(/) k 5+/k-ldj -- r( + ) j=l  j=l  In order to be able to use Gibbs sampling for the (discrete) indicators, ci, we need the  conditional prior for a single indicator given all the others; this is easily obtained from  eq. (12) by keeping all but a single indicator fixed:  p(ci -- jlc_i,c) - n_,a + c/k (13)  n-l+c  where the subscript -i indicates all indexes except i and n-i,j is the number of observa-  tions, excluding Yi, that are associated with component j. The posteriors for the indicators  are derived in the next section.  Lastly, a vague prior of inverse Gamma shape is put on the concentration parameter  p(c-l)  (1, 1) ?p(o)cro-3/2exp(-1/(2o)). (14)  The Infinite Gaussian Mixture Model 557  The likelihood for a may be derived from eq. (12), which together with the prior from  eq. (14) gives:  akr(a) p(alk, n) cr ak-a/2 exp(- 1/(2a))r(a) (15)  = + +  Notice, that the conditional posterior for a depends only on number of observations, n, and  the number of components, k, and not on how the observations are distributed among the  components. The distribution p(log(a)Ik, n) is log-concave, so we may efficiently generate  independent samples from this distribution using ARS.  3 The infinite limit  So far, we have considered k to be a fixed finite quantity. In this section we will explore  the limit k --> oc and make the final derivations regarding the conditional posteriors for  the indicators. For all the model variables except the indicators, the conditional posteriors  for the infinite limit is obtained by substituting for k the number of classes that have data  associated with them, krep, in the equations previously derived for the finite model. For the  indicators, letting k --> oc in eq. (13), the conditional prior reaches the following limits:  components where n-i,j > 0: p(ci = jlc-i, a) =  all other compo- p(ci  ci, for all i'  ilc-, a) -  nents combined:  n-i,j  n-l+a  a (16)  n-l+a  This shows that the conditional class prior for components that are associated with other  observations is proportional to the number of such observations; the combined prior for  all other classes depends only on a and n. Notice how the analytical tractability of the  integral in eq. (12) is essential, since it allows us to work directly with the (finite number  of) indicator variables, rather than the (infinite number of) mixing proportions. We may  now combine the likelihood from eq. (1) conditioned on the indicators with the prior from  eq. (16) to obtain the conditional posteriors for the indicators:  components for which rt_i,j > 0: p(ci -- tj, Sj, a) oc (17)  p(ci = jlc-i,a)p(yil/j,sj,c-i) cr n-i, s}/2exp( - sj(yi - /j)2/2),  n-l+a  all other components combined: p(ci  ci, for all i  i'Ic_i, A, r, , w, a) oc  p(ci  ci, for all i  i'fc_i,a ) / p(yi]/j, sj )p(uj, sj[A, r, , w)djdsj.  The likelihood for components with observations other than yi currently associated with  them is Gaussian with component parameters/j and sj. The likelihood pertaining to the  currently unrepresented classes (which have no parameters associated with them) is ob-  tained through integration over the prior distribution for these. Note, that we need not  differentiate between the infinitely many unrepresented classes, since their parameter dis-  tributions are all identical. Unfortunately, this integral is not analytically tractable; I follow  Neal [1998], who suggests to sample from the priors (which are Gaussian and Gamma  shaped) in order to generate a Monte Carlo estimate of the probability of "generating a new  class". Notice, that this approach effectively generates parameters (by sampling from the  prior) for the classes that are unrepresented. Since this Monte Carlo estimate is unbiased,  the resulting chain will sample from exactly the desired distribution, no matter how many  samples are used to approximate the integral; I have found that using a single sample works  fairly well in many applications.  In detail, there are three possibilities when computing conditional posterior class probabil-  ities, depending on the number of observations associated with the class:  558 C. E. Rasmussen  if rbi,j > 0: there are other observations associated with class j, and the posterior class  probability is as given by the top line of eq. (17).  if r-i,j - 0 and ci -- j: observation Yi is currently the only observation associated with  class j; this is an peculiar situation, since there are no other observations associ-  ated with the class, but the class still has parameters. It turns out that this situation  should be handled as an unrepresented class, but rather than sampling for the pa-  rameters, one simply uses the class parameters; consult [Neal 1998] for a detailed  derivation.  unrepresented classes: values for the mixture parameters are picked at random from the  prior for these parameters, which is Gaussian for/z5 and Gamma shaped for s5.  Now that all classes have parameters associated with them, we can easily evaluate their  likelihoods (which are Gaussian) and the priors, which take the form n_i,i/(r - 1 + a)  for components with observations other than Yi associated with them, and a/(r - 1 + c)  for the remaining class. When hitherto unrepresented classes are chosen, a new class is  introduced in the model; classes are removed when they become empty.  4 Inference; the "spirals" example  To illustrate the model, we use the 3 dimensional "spirals" dataset from [Ueda et al, 1998],  containing 800 data point, plotted in figure 1. Five data points are generated from each of  160 isotropic Gaussians, whose means follow a spiral pattern.  3C  1C  16 18 20 22 24  r( resented components  2 4 6  concentration, c  4 5 6 7  shape,  Figure 1: The 800 cases from the three dimensional spirals data. The crosses represent a  single (random) sample from the posterior for the mixture model. The krep -- 20 repre-  sented classes account for r/(r + a) 2 99.6% of the mass. The lines indicate 2 std. dev. in  the Gaussian mixture components; the thickness of the lines represent the mass of the class.  To the right histograms for 100 samples from the posterior for krep, C and fi are shown.  4.1 Multivariate generalisation  The generalisation to multivariate observations is straightforward. The means, /zi, and  precisions, s j, become vectors and matrices respectively, and their prior (and posterior)  The Infinite Gaussian Mixture Model 559  distributions become multivariate Gaussian and Wishart. Similarly, the hyperparameter ,  becomes a vector (multivariate Gaussian prior) and r and w become matrices with Wishart  priors. The/ parameter stays scalar, with the prior on (/ - D + 1) -1 being Gamma with  mean I/D, where D is the dimension of the dataset. All other specifications stay the same.  Setting D -- 1 recovers the scalar case discussed in detail.  4.2 Inference  The mixture model is started with a single component, and a large number of Gibbs sweeps  are performed, updating all parameters and hyperparameters in turn by sampling from the  conditional distributions derived in the previous sections. In figure 2 the auto-covariance  for several quantities is plotted, which reveals a maximum correlation-length of about 270.  Then 30000 iterations are performed for modelling purposes (taking 18 minutes of CPU  time on a Pentium PC): 3000 steps initially for "burn-in", followed by 27000 to generate  100 roughly independent samples from the posterior (spaced evenly 270 apart). In figure  1, the represented components of one sample from the posterior is visualised with the  data. To the right of figure 1 we see that the posterior number of represented classes is  very concentrated around 18 - 20, and the concentration parameter takes values around  a _ 3.5 corresponding to only a/(n + a) _ 0.4% of the mass of the predictive distribution  belonging to unrepresented classes. The shape parameter/ takes values around 5-6, which  gives the "effective number of points" contributed from the prior to the covariance matrices  of the mixture components.  4.3 The predictive distribution  Given a particular state in the Markov Chain, the predictive distribution has two parts: the  represented classes (which are Gaussian) and the unrepresented classes. As when updating  the indicators, we may chose to approximate the unrepresented classes by a finite mixture  of Gaussians, whose parameters are drawn from the prior. The final predictive distribution  is an average over the (eg. 100) samples from the posterior. For the spirals data this density  has roughly 1900 components for the represented classes plus however many are used to  represent the remaining mass. I have not attempted to show this distribution. However, one  can imagine a smoothed version of the single sample shown in figure 1, from averaging  over models with slightly varying numbers of classes and parameters. The (small) mass  from the unrepresented classes spreads diffusely over the entire observation range.  80.6  o   0   0  log(k)  ---1og((:z)  log(j3-2)  -- mean(X)   Iog(det(R))  Iog(det(W))  '0   : .! .- .... : :... =.:-._-.5:.....':=5   E 20 "'.::-'---- _--  <glO .---  20 4dO 6)0 860 1000  01 1000 2000 3000 400 5-00  iteration lag time =u= Monte Carlo iteration  Figure 2: The left plot shows the auto-covariance length for various parameters in the  Markov Chain, based on 105 iterations. Only the number of represented classes, krep, has  a significant correlation; the effective correlation length is approximately 270, computed  as the sum of covariance coefficients between lag -1000 and 1000. The right hand plot  shows the number of represented classes growing during the initial phase of sampling. The  initial 3000 iterations are discarded.  560 C. E. Rasmussen  5 Conclusions  The infinite hierarchical Bayesian mixture model has been reviewed and extended into a  practical method. It has been shown that good performance (without overfitting) can be  achieved on multidimensional data. An efficient and practical MCMC algorithm with no  free parameters has been derived and demonstrated on an example. The model is fully au-  tomatic, without needing specification of parameters of the (vague) prior. This corroborates  the falsity of the common misconception that "the only difference between Bayesian and  non-Bayesian methods is the prior, which is arbitrary anyway ..."  Further tests on a variety of problems reveals that the infinite mixture model produces  densities whose generalisation is highly competitive with other commonly used methods.  Current work is undertaken to explore performance on high dimensional problems, in terms  of computational efficiency and generalisation.  The infinite mixture model has several advantages over its finite counterpart: 1) in many  applications, it may be more appropriate not to limit the number of classes, 2) the number  of represented classes is automatically determined, 3) the use of MCMC effectively avoids  local minima which plague mixtures trained by optimisation based methods, eg. EM [Ueda  et al, 1998] and 4) it is much simpler to handle the infinite limit than to work with finite  models with unknown sizes, as in [Richardson & Green, 1997] or traditional approaches  based on extensive crossvalidation. The Bayesian infinite mixture model solves simultane-  ously several long-standing problems with mixture models for density estimation.  Acknowledgments  Thanks to Radford Neal for helpful comments, and to Naonori Ueda for making the spirals  data available. This work is funded by the Danish Research Councils through the Compu-  tational Neural Network Center (CONNECT) and the THOR Center for Neuroinformatics.  References  Antoniak, C. E. (1974). Mixtures of Dirichlet processes with applications to Bayesian nonparametric  problems. Annals of Statistics 2, 1152-1174.  Ferguson, T. S. (1973). A Bayesian analysis of some nonparametric problems. Annals of Statistics 1,  209-230.  Gilks, W. R. and P. Wild (1992). Adaptive rejection sampling for Gibbs sampling. Applied Statis-  tics 41,337-348.  Neal, R. M. (1996). Bayesian Learning for Neural Networks, Lecture Notes in Statistics No. 118,  New York: Springer-Verlag.  Neal, R. M. (1998). Markov chain sampling methods for Dirichlet process mixture models. Technical  Report 4915, Department of Statistics, University of Toronto.  http: //www. cs. toronto. edu/~radford/mixmc. abs tract. html.  Richardson, S. and R Green (1997). On Bayesian analysis of mixtures with an unknown number of  components. Journal of the Royal Statistical Society, B 59, 731-792.  Ueda, N., R. Nakano, Z. Ghahramani and G. E. Hinton (1998). SMEM Algorithm for Mixture  Models, NIPS 11, MIT Press.  West, M., P. MUller and M.D. Escobar (1994). Hierarchical priors and mixture models with applica-  tions in regression and density estimation. In P. R. Freeman and A. F. M. Smith (editors), Aspects of  Uncertainty, pp. 363-386. John Wiley.  Williams, C. K. I. and C. E. Rasmussen (1996). Gaussian Processes for Regression, in D. S. Touret-  zky, M. C. Mozer and M. E. Hasselmo (editors), NIPS 8, MIT Press.  
Manifold Stochastic Dynamics  for Bayesian Learning  Mark Ziochin  Department of Computer Science  Technion - Israel Institute of Technology  Technion City, Haifa 32000, Israel  zmark @ cs.technion.ac.il  Yoram Baram  Department of Computer Science  Technion - Israel Institute of Technology  Technion City, Haifa 32000, Israel  baram @cs.technion.ac.il  Abstract  We propose a new Markov Chain Monte Carlo algorithm which is a gen-  eralization of the stochastic dynamics method. The algorithm performs  exploration of the state space using its intrinsic geometric structure, facil-  itating efficient sampling of complex distributions. Applied to Bayesian  learning in neural networks, our algorithm was found to perform at least  as well as the best state-of-the-art method while consuming considerably  less time.  1 Introduction  In the Bayesian framework predictions are made by integrating the function of interest  over the posterior parameter distribution, the latter being the normalized product of the  prior distribution and the likelihood. Since in most problems the integrals are too complex  to be calculated analytically, approximations are needed.  Early works in Bayesian learning for nonlinear models [Buntineand Weigend 1991,  MacKay 1992] used Gaussian approximations to the posterior parameter distribution.  However, the Gaussian approximation may be poor, especially for complex models, be-  cause of the multi-modal character of the posterior distribution.  Hybrid Monte Carlo (HMC) [Duane et al. 1987] introduced to the neural network com-  munity by [Neal 1996], deals more successfully with multi-modal distributions but is very  time consuming. One of the main causes of HMC inefficiency is the anisotropic character  of the posterior distribution - the density changes rapidly in some directions while remain-  ing almost constant in others.  We present a novel algorithm which overcomes the above problem by using the intrinsic  geometrical structure of the model space.  2 Hybrid Monte Carlo  Markov Chain Monte Carlo (MCMC) [Gilks et al. 1996] approximates the value  E[a] -' / a(O)Q(O)dO  Manifold Stochastic Dynamics for Bayesian Learning 695  by the mean  N  t--1  where 0 () ,..., 0(r) are successive states of the ergodic Markov chain with invariant dis-  tribution Q(0) .  In addition to ergodicity and invariance of Q(O) another quality we would like the Markov  chain to have is rapid exploration of the state space. While the first two qualities are rather  easily attained, achieving rapid exploration of the state space is often nontrivial.  A state-of-the-art MCMC method, capable of sampling from complex distributions, is Hy-  brid Monte Carlo [Duane et al. 1987].  The algorithm is expressed in terms of sampling from canonical distribution for the state,  q, of a "physical" system, defined in terms of the energy function E(q) '  P(q) oc exp(-E(q)) (1)  To allow the use of dynamical methods, a "momentum" variable, p, is introduced, with the  same dimensionality as q. The canonical distribution over the "phase space" is defined to  be:  P(q,p) oc exp(-H(q,p)) (2)  where H(q, p) = E(q) + K(p) is the "Hamiltonian", which represents the total energy.  K (p) is the "kinetic energy" due to momentum, defined as  K(p) = 2mi (3)  i=1  where p, i: 1,..., n are the momentum components and mi is the "mass" associated  with i'th component, so that different components can be given different weight.  Sampling from the canonical distribution can be done using stochastic dynamics method  [Andersen 1980], in which the task is split into two subtasks - sampling uniformly from  values of q and p with a fixed total energy, H(q, p), and sampling states with different  values of H. The first task is done by simulating the Hamiltonian dynamics of the system:  dqi c9H Pi  dr Opi  dpi OH OE  dr Oqi 69qi  Different energy levels are obtained by occasional stochastic Gibbs sampling  [Geman and Geman 1984] of the momentum. Since q and p are independent, p may be  updated without reference to q by drawing a value with probability density proportional to  exp(-K(p)), which, in the case of (3), can be easily done, since the pi'S have independent  Gaussian distributions.  In practice, Hamiltonian dynamics cannot be simulated exactly, but can be approximated  by some discretization using finite time steps. One common approximation is leapfrog  discretization [Neal 1996].  In the hybrid Monte Carlo method stochastic dynamic transitions are used to generate can-  didate states for the Metropolis algorithm [Metropolis et al. 1953]. This eliminates certain  Note that any probability density that is nowhere zero can be put in this form, by simply defining  E(q) = - log P(q) - log Z, for any convenient Z).  696 M. Zlochin and Y. Baram  drawbacks of the stochastic dynamics such as systematic errors due to leapfrog discretiza-  tion, since Metropolis algorithm ensures that every transition keeps canonical distribution  invariant. However, the empirical comparison between the uncorrected stochastic dynamics  and the HMC in application to Bayesian learning in neural networks [Neal 1996] showed  that with appropriate discretization stepsize there is no notable difference between the two  methods.  A modification proposed in [Horowitz 1991] instead of Gibbs sampling of momentum, is  to replace p each time by p-cos(O) + . sin(O), where 0 is a small angle and  is distributed  according to N(0, I). While keeping canonical distribution invariant, this scheme, called  momentum persistence, improves the rate of exploration.  3 Riemannian geometry  A Riemannian manifold [Amari 1997] is a set 13 _C _R ' equipped with a metric tensor G  which is a positive semidefinite matrix defining the inner product between infinitesimal  increments as:  < dO1, dO2 >-- dO1T  G  dO2  Let us denote entries of G by Gi,j and entries of G- 1 by G i'j . This inner product naturally  gives us the norm  II dO [1-< dO, dO >- d0 r. G-dO.  The Jeffrey prior over 13 is defined by the density function:  I0)l  where [. [denotes determinant.  3.1 Hamiitonian dynamics over a manifold  For Riemannian manifold the dynamics take a more general form than the one described in  section 2.  If the metric tensor is (7 and all masses are set to one then the HamiltonJan is given by:  1T G-1  H(q,p) = E(q) + p  .p  (4)  The dynamics are governed by the following set of differential equations [Chavel 1993]:  OE E i ..  d2 qi  Gi'J Oqj  = -- Fj,k qiqj  dr2 j j,t,  where I'}, k are the Christoffel symbols given by:  OG,,5 OGs,k  = + )   Gi'"( Oq 0% Oq,  dq  and 0 = rr is related to p by 0 = G- lp.  Manifold Stochastic Dynamics for Bayesian Learning 697  3.2 Riemannian geometry of functions  In regression the log-likelihood is proportional to the empirical error, which is simply the  Euclidean distance between the target point, t, and candidate function evaluated over the  sample. Therefore, the most natural distance measure between the models is the Euclidean  seminorm   l  d(01,02) 2 -II fox - fo2 II/-- E(f(xi, 0) - f(xi,O2)) 2 (5)  i=1  The resulting metric tensor is:  l  G -- E{Vof(zi,O). K7of(xi,O) T } - jr. j (6)  i=1  where K70 denotes gradient and /= L 00j j is the Jacobian matrix.  3.3 Bayesian geometry  A Bayesian approach would suggest the inclusion of prior assumptions about the parame-  ters in the manifold geometry.  If, for example, a priori 0 ~ N(O, I/a), then the log-posterior can be written as:  I n  logp(OIx) -/3 E(f(xi, 0,) - t) 2 q-  E(ok - o) 2  i=1 k=l  where/3 is inverse noise variance.  Therefore, the natural metric in the model space is  l  d(01' 02)2 "-' /3 E(f(xi' 01) - f(zi' 02) )2 q- o E(O} -- 0) 2  i=1 k=l  with the metric tensor:  GB =/3' G+ a . I = jT . j (7)  where J is the "extended Jacobian":  005  V/-  6i-t, i > I  (8)  where 6i, is the Kroneker's delta.  Note, that as a --> 0, GB --> /3G, hence as the prior becomes vaguer we approach a non-  Bayesian paradigm. If, on the other hand, a --> x> or/3  G --> 0, the Bayesian geometry  approaches the Euclidean geometry of the parameter space. These are the qualities that we  would like the Bayesian geometry to have - if the prior is "strong" in comparison to the  likelihood, the exact form of (7 should be of little importance.  The definitions above can be applied to any log-concave prior distribution with the inverse  Hessian of the log-prior, (K7V logp(0)) - , replacing I in (7). The framework is not re-  stricted to regression. For a general distribution class it is natural to use Fisher information  matrix, 2Y, as a metric tensor [Amari 1997]. The Bayesian metric tensor then becomes:  GB = Z + (K7V logp(0)) - (9)  698 M. Zlochin and Y. Baram  4 Manifold Stochastic Dynamics  As mentioned before, the energy landscape in many regression problems is anisotropic.  This degrades the performance of HMC in two aspects:  The dynamics may not be optimal for efficient exploration of the posterior distri-  bution as suggested by the studies of Gaussian diffusions [Hwang et al. 1993].  The resulting differential equations are stiff [Gear 1971], leading to large dis-  cretization errors, which in turn necessitates small time steps, implying that the  computational burden is high.  Both of these problems disappear if instead of the Euclidean Hamiltonian dynamics used  in HMC we simulate dynamics over the manifold equipped with the metric tensor GB  proposed in the previous section.  In the context of regression from the definition GB = jT . j, we obtain an alternative  deq  equation for , in a matrix form:  deq _ _G(VE + j rOj '  dr 2 - rrq ) (10)  In the canonical distribution P(q, p) oc exp(-H(q, p)) the conditional distribution of p  given q is a zero-mean Gaussian with the covariance matrix G (q) and the marginal dis-  tribution over q is proportional to exp(-E(q))r(q). This is equivalent to multiplying the  prior by the Jeffrey prior 2.  The sampling from the canonical distribution is two-fold:  Simulate the Hamiltonian dynamics (3.1) for one time-step using leapfrog dis-  cretisation.  Replace p using momentum persistence. Unlike the HMC case, the momentum  perturbation  is distributed according to N(0, G).  The actual weights multiplying the matrices I and G in (7) may be chosen to be different  from the specified a and/3, so as to improve numerical stability.  5 Empirical comparison  5.1 Robot arm problem  We compared the performance of the Manifold Stochastic Dynamics (MSD) algorithm  with the standard HMC. The comparison was carried using MacKay's robot arm problem  which is a common benchmark for Bayesian methods in neural networks [MacKay 1992,  Neal 1996].  The robot arm problem is concerned with the mapping:  Yl -- 2.0 cos 21 q- 1.3 COS(X 1 q- X2) q- el, y2 = 2.0 sin 21 q- 1.3 sin(x1 q- x2) q- e2  where el, e2 are independent Gaussian noise variables of standard deviation 0.05. The  dataset used by Neal and Mackay contained 200 examples in the training set and 400 in the  test set.  2In fact, since the actual prior over the weights is unknown, a truly Bayesian approach would be  to use a non-informative prior such as rr(q). In this paper we kept the modified prior which is the  product of rr(q) and a zero-mean Gaussian.  Manifold Stochastic Dynamics for Bayesian Learning 699  1  0.9  0.8  0,7  0.6  0.5  0.4  o.3!  0.2  o  o  MSD  10 20 30 40 50  1.2  0.8  0.6  0.4  0.2  -0.2  0  10 20 30 40 50  Figure 1' Average (over the 10 runs) autocorrelation of input-to-hidden (left) and hidden-  to-output (right) weights for HMC with 100 and 30 leapfrog steps per iteration and MSD  with single leapfrog step per iteration. The horizontal axis gives the lags, measured in  number of iterations.  We used a neural network with two input units, one hidden layer containing 8 tanh units  and two linear output units.  The hyperparameter/ was set to its correct value of 400 and a was chosen to be 1.  5.2 Algorithms  We compared MSD with two versions of HMC - with 30 and with 100 leapfrog steps per  iteration, henceforth referred to as HMC30 and HMC100. MSD was run with a single  leapfrog step per iteration. In all three algorithms momentum was resampled using persis-  tence with cos(O) -- 0.95.  A single iteration of HMC100 required about 4.8  10  floating point operations (flops),  HMC30 required 1.4  10  flops and MSD required 0.5  10  flops. Hence the computa-  tional load of MSD was about one third of that of HMC30 and 10 times lower than that of  HMC100.  The discretization stepsize for HMC was chosen so as to keep the rejection rate below 5%.  An equivalent criterion of average error in the Hamiltonian around 0.05 was used for the  MSD.  All three sampling algorithms were run 10 times, each time for 3000 iteration with the  first 1000 samples discarded in order to allow the algorithms to reach the regions of high  probability.  5.3 Results  One appropriate measure for the rate of state space exploration is weights autocorrelation  [Neal 1996]. As shown in Figure 1, the behavior of MSD was clearly superior to that of  HMC.  Another value of interest is the total squared error over the test set. The predictions for the  test set were made as follows. A subsample of 100 parameter vectors wag generated by  taking every twentieth sample vector starting from 1001 and on. The predicted value was  700 M. Zlochin and Y. Baram  the average over the empirical function distribution of this subsample.  The total squared errors, normalized with respect to the variance on the test cases, have the  following statistics (over the 10 runs):  average standard deviation  HMC30 1.314 0.074  HMC 100 1.167 0.044  MSD 1.161 0.023  The average error of HMC30 is high, indicating that the algorithm failed to reach the region  of high probability. The errors of HMC100 and MSD are comparable but the standard  deviation for MSD is twice as low as that for HMC 100, meaning that the estimate obtained  using MSD is more reliable.  6 Conclusion  We have described a new algorithm for efficient sampling from complex distributions such  as those appearing in Bayesian learning with non-linear models. The empirical compar-  ison shows that our algorithm achieves results superior to the best achieved by existing  algorithms in considerably smaller computation time.  References  [Amari 1997] Amari S., "Natural Gradient Works Efficiently in Learning", Neural  Computation, vol. 10, pp.251-276.  [Andersen 1980] Andersen H.C., "Molecular dynamics simulations at constant pressure  and/or temperature", Journal of Chemical Physics,vol. 3,pp. 589-603.  [Buntine and Weigend 1991] "Bayesian back-propagation", Complex systems, vol. 5, pp.  603-643.  [Chavel 1993] Chavel I., Riemannian Geometry: A Modern Introduction, University  Press, Cambridge.  [Duane et al. 1987] "Hybrid Monte Carlo", Physics Letters B,vol. 195,pp. 216-222.  [Gear 1971] Gear C.W., Numerical initial value problems in ordinary differential  equations, Prentice Hall.  [Geman and Geman 1984] Geman S.,Geman D., "Stochastic relaxation,Gibbs distribu-  tions and the Bayesian restoration of images", IEEE Trans.,PAMl-  6,721-741.  [Gilks et al. 1996] Gilks W.R., Richardson S. and Spiegelhalter D.J., Markov Chain Monte  Carlo in Practice, Chapman&Hall.  [Hwanget al. 1993] Hwang, C.,-R, Hwang-Ma S.,-Y. and Shen. S.,-J., "Accelerating  Gaussian diffusions", Ann. Appl. Prob., vol. 3,897-913.  [Horowitz 1991] Horowitz A.M., "A generalized guided Monte Carlo algorithm",  Physics Letters B,, vol. 268, pp. 247-252.  [MacKay 1992] MacKay D.J.C., Bayesian Methods for Adaptive Models, Ph.D. thesis,  California Institute of Technology.  [Metropolis et al. 1953] Metropolis N., Rosenbluth A.W., Rosenbluth M.N., Teller A.H.  and Teller E., "Equation of State Calculations by Fast Computing Ma-  chines", Journal of Chemical Physics,vol.21 ,pp. 1087-1092.  [Neal 1996] Neal, R.M., Bayesian Learning for Neural Networks, Springer 1996.  PART V  IMPLEMENTATION  
Building Predictive Models from Fractal  Representations of Symbolic Sequences  Peter Tifio Georg Dorffner  Austrian Research Institute for Artificial Intelligence  Schottengasse 3, A- 1010 Vienna, Austria  {petert, g eorg } @ ai. univie. ac. at  Abstract  We propose a novel approach for building finite memory predictive mod-  els similar in spirit to variable memory length Markov models (VLMMs).  The models are constructed by first transforming the n-block structure of  the training sequence into a spatial structure of points in a unit hypercube,  such that the longer is the common suffix shared by any two n-blocks,  the closer lie their point representations. Such a transformation embodies  a Markov assumption - n-blocks with long common suffixes are likely  to produce similar continuations. Finding a set of prediction contexts is  formulated as a resource allocation problem solved by vector quantizing  the spatial n-block representation. We compare our model with both the  classical and variable memory length Markov models on three data sets  with different memory and stochastic components. Our models have a  superior performance, yet, their construction is fully automatic, which is  shown to be problematic in the case of VLMMs.  1 Introduction  Statistical modeling of complex sequences is a prominent theme in machine learning due  to its wide variety of applications (see e.g. [5]). Classical Markov models (MMs) of finite  order are simple, yet widely used models for sequences generated by stationary sources.  However, MMs can become hard to estimate due to the familiar explosive increase in the  number of free parameters when increasing the model order. Consequently, only low or-  der MMs can be considered in practical applications. Some time ago, Ron, Singer and  Tishby [4] introduced at this conference a Markovian model that could (at least partially)  overcome the curse of dimensionality in classical MMs. The basic idea behind their model  was simple: instead of fixed-order MMs consider variable memory length Markov models  (VLMMs) with a "deep" memory just where it is really needed (see also e.g. [5][7]).  The size of VLMMs is usually controlled by one or two construction parameters. Unfor-  tunately, constructing a series of increasingly complex VLMMs (for example to enter a  model selection phase on a validation set) by varying the construction parameters can be  646 P Tiao and G. Dorffner  a troublesome task [ 1]. Construction often does not work "smoothly" with varying the  parameters. There are large intervals of parameter values yielding unchanged VLMMs in-  terleaved with tiny parameter regions corresponding to a large spectrum of VLMM sizes.  In such cases it is difficult to fully automize the VLMM construction.  To overcome this drawback, we suggest an alternative predictive model similar in spirit  to VLMMs. Searching for the relevant prediction contexts is reformulated as a resource  allocation problem in Euclidean space solved by vector quantization. A potentially pro-  hibitively large set of all length-L blocks is assigned to a much smaller set of prediction  contexts on a suffix basis. To that end, we first transform the set of L-blocks appearing in  the training sequence into a set of points in Euclidean space, such that points corresponding  to blocks sharing a long common suffix are mapped close to each other. Vector quantiza-  tion on such a set partitions the set of L-blocks into several classes dominated by common  suffixes. Quantization centers play the role of predictive contexts. A great advantage of our  model is that vector quantization can be performed on a completely self-organized basis.  We compare our model with both classical MMs and VLMMs on three data sets repre-  senting a wide range of grammatical and statistical structure. First, we train the models on  the Feigenbaum binary sequence with a very strict topological and metric organization of  allowed subsequences. Highly specialized, deep prediction contexts are needed to model  this sequence. Classical Markov models cannot succeed and the full power of admitting a  limited number of variable length contexts can be exploited. The second data set consists of  quantized daily volatility changes of the Dow Jones Industrial Average (DJIA). Predictive  models are used to predict the direction of volatility move for the next day. Financial time  series are known to be highly stochastic with a relatively shallow memory structure. In this  case, it is difficult to beat low-order classical MMs. One can perform better than MMs only  by developing a few deeper specialized contexts, but that, on the other hand, can lead to  overfitting. Finally, we test our model on the experiments of Ron, Singer and Tishby with  language data from the Bible [5]. They trained classical MMs and a VLMM on the books  of the Bible except for the book of Genesis. Then the models were evaluated on the bases  of negative log-likelihood on an unseen text from Genesis. We compare likelihood results  of our model with those of MMs and VLMMs.  2 Predictive models  We consider sequences S = ss2... over a finite alphabet `4 = {1,2, ..., A} generated by  stationary sources. The set of all sequences over `4 with exactly n symbols is denoted by  `4n.  An information source over .4 = {1, 2, ..., A} is defined by a family of consistent prob-  ability measures P on.4 , n = 0, 1,2,..., ,Et P+(ws) = P(w), for all w  (.40 = {A} and P0(A) = 1, A denotes the empty string).  In applications it is useful to consider probability functions P that are easy to handle.  This can be achieved, for example, by assuming a finite source memory of length at most  L, and formulating the conditional measures P(s[w) = P/`+(ws)/P/`(w), w  using a function c: .4/'  C, from L-blocks over .4 to a (presumably small) finite set C of  prediction contexts:  P(slw ) - P(slc(w)). (1)  In Markov models (MMs) of order n _< L, for all L-blocks w 6 .4, c(w) is the length-n  Predictive Models from Fractal Representations of Sequences 647  suffix of w, i.e. c(uv) = v, v  A", u  A t'-".  In variable memory length Markov models (VLMMs), the suffices c(w) of L-blocks w G  A t' can have different lengths, depending on the particular L-block w. For strategies of  selecting and representing the prediction contexts through prediction suffix trees and/or  probabilistic suffix automata see, for example, [4][5]. VLMM construction is controlled by  one, or several parameters regulating selection of candidate contexts and growing/pruning  decisions.  Prediction context function c  A t' --> C in Markov models of order n < L, can be  interpreted as a natural homomorphism c  A t' --> A t' le corresponding to the equivalence  relation  C_ A t. x .A t' on L-blocks over .A: two L-blocks u, v are in the same class, i.e.  (u, v) 6 , if they share the same suffix of length n. The factor set .At. {e = C = .A"  consists of all n-blocks over .A. Classical MMs define the equivalence  on the suffix  bases, but regardless of the suffix structure present in the training data. Our idea is to keep  the Markov-motivated suffix strategy for constructing , but at the same time take into an  account the data suffix structure.  Vector quantization on a set of B points in a Euclidean space positions N < < B codebook  vectors (CVs), each CV representing a subset of points that are closer to it than to any other  CV, so that the overall error of substituting CVs for points they represent is minimal. In  other words, CVs tend to represent points lying close to each other (in a Euclidean metric).  In order to use vector quantization for determining relevant predictive contexts we need to  do two things:  1. Define a suitable metric in the sequence space that would correspond to Markov  assumptions:  (a) two sequences are "close" if they share a common suffix  (b) the longer is the common suffix the closer are the sequences  2. Define a uniformly continuous map from the sequence metric space to the Eu-  clidean space, i.e. sequences that are close in the sequence space (i.e. share a long  common suffix) are mapped close to each other in the Euclidean space.  In [6] we rigorously study a class of such spatial representations of symbolic structures.  Specifically, a family of distances between two L-blocks u = uu2...ut._ut. and v =  vv2...vt._vt. over .A = {1,2, ..., A}, expressed as  t. 1  d(u, v) = (ui, k < 7' (2)  i=1  with $(i, j) = 1 if i = j, and $(i, j) = 0 otherwise, correspond to Markov assumption.  The parameter k influences the rate of "forgetting the past". We construct a map from  the sequence metric space to the Euclidean space as follows: Associate with each symbol  i .A amap  i(z)=kz+(1-k)ti, ti6{O, 1) , z610,1]  (3)  operating on a unit D-dimensional hypercube [0, 1] D. Dimension of the hypercube should  be large enough so that each symbol i is associated with a unique vertex, i.e. D = [log 2 A]  and ti k tj whenever i  j. The map e: .At, ._> [0, 1] D, from L-blocks vv2 ...vt. over .A  to the unit hypercube,  rr(vlv2...VL) = VL(VL-i(...(V2(V(X*)))...)) = (vt. ovL_ o...ov2ov)(X*), (4)  648 P Tiao and G. Dorffner  where z* = {}D is the center of the hypercube, is "uniformly continuous". Indeed,  whenever two sequences u, v share a common suffix of length Q, the Euclidean distance  between their point representations (u) and (v) is less than vkQ. Strictly speaking,  for a mathematically correct treatment of uniform continuity, we would need to consider  infinite sequences. Finite blocks of symbols would then correspond to cylinder sets (see  [6]). For sake of simplicity we only deal with finite sequences.  As with classical Markov models, we define the prediction context function c: `4  C  via an equivalence  on L-blocks over `4: two L-blocks u, v are in the same class if their  images under the map  are represented by the same codebook vector. In this case, the set  of prediction contexts C can be identified with the set of codebook vectors {bl, b2, ..., br},  bi D, i = 1,2,..., N. We refer to predictive models with such a context function as  predictionfractal machines (PFMs). The prediction probabilities (1) are determined by  :v(i, s)  ?(slbi) = Y''aeA N(i,a)' s  `4, (5)  where N (i, a) is the number of (L+ 1)-blocks ua, a  `4;, a  `4, in the training sequence,  such that the point r(u) is allocated to the codebook vector bi.  3 Experiments  1 (see eq. (3))  In all experiments we constructed PFMs using a contraction coefficient k = 5  and K-means as a vector quantization tool.  The first data set is the Feigenbaum sequence over the binary alphabet .4 = { 1,2}. This  sequence is well-studied in symbolic dynamics and has a number of interesting proper-  ties. First, the topological structure of the sequence can only be described using a context  sensitive tool - a restricted indexed context-free grammar. Second, for each block length  n - 1,2, ..., the distribution of n-blocks is either uniform, or has just two probability lev-  els. Third, the n-block distributions are organized in a self-similar fashion (see [2]). The  sequence can be specified by the subsequence composition rule  ao ---- 2, al = 21, an+l -- anan-lan-1.  (6)  We chose to work with the Feigenbaum sequence, because increasingly accurate model-  ing of the sequence with finite memory models requires a selective mechanism for deep  prediction contexts.  We created a large portion of the Feigenbaum sequence and trained a series of classical  MMs, variable memory length MMs (VLMMs), and prediction fractal machines (PFMs)  on the first 260,000 symbols. The following 200,000 symbols formed a test set. Maximum  memory length L for VLMMs and PFMs was set to 30.  As mentioned in the introduction, constructing a series of increasingly complex VLMMs  by varying the construction parameters appeared to be a troublesome task. We spent a fair  amount of time finding "critical" parameter values at which the model size changed. In  contrast, a fully automatic construction of PFMs involved sliding a window of length L =  30 through the training set; for each window position, mapping the L-block u appearing  in the window to the point (u) (eq. (4)), vector-quantizing the resulting set of points (up  to 30 codebook vectors). After the quantization step we computed predictive probabilities  according to eq. (5).  Predictive Models from Fractal Representations of Sequences 649  Table 1: Normalized negative log-likelihoods (NNL) on the Feigenbaum test set.  model # contexts NNL captured block distribution  PFM 2-4 0.6666 1-3  5-7 0.3333 1-6  8-22 0.1666 1-12  23- 0.0833 1-24  VLMM 2-4 0.6666 1-3  5 0.3333 1-6  11 0.1666 1-12  23 0.0833 1-24  MM 2,4,8,16,32 0.6666 1-3  Negative log-likelihoods per symbol (the base of logarithm is always taken to be the number  of symbols in the alphabet) of the test set computed using the fitted models exhibited a step-  like increasing tendency shown in Table 1. We also investigated the ability of the models  to reproduce the n-block distribution found in the training and test sets. This was done by  letting the models generate sequences of length equal to the length of the training sequence  and for each block length n = 1, 2, ..., 30, computing the L distance between the n-block  distribution of the training and model-generated sequences. The n-block distributions on  the test and training sets were virtually the same for n - 1,2, ...30. In Table 1 we show  block lengths for which the L distance does not exceed a small threshold A. We set  A = 0.005, since in this experiment, either the L distance was less 0.005, or exceeded  0.005 by a large amount.  An explanation of the step-like behavior in the log-likelihood and n-block modeling be-  havior of VLMMs and PFMs is out of the scope of this paper. We briefly mention, how-  ever, that by combining the knowledge about the topological and metric structures of the  Feigenbaum sequence (e.g. [2]) with a careful analysis of the models, one can show why  and when an inclusion of a prediction context leads to an abrupt improvement in the mod-  eling performance. In fact, we can show that VLMMs and PFMs constitute increasingly  better approximations to the infinite self-similar Feigenbaum machine known in symbolic  dynamics [2].  The classical MM totally fails in this experiment, since the context length 5 is far too  small to enable the MM to mimic the complicated subsequence structure in the Feigenbaum  sequence. PFMs and VLMMs quickly learn to explore a limited number of deep prediction  contexts and perform comparatively well.  In the second experiment, a time series { z t } of the daily values of the Dow Jones Industrial  Average (DJIA) from Feb. 1 1918 until April 1 1997 was transformed into a time series  of returns rt ---- log zt+ - log zt, and divided into 12 partially overlapping epochs, each  containing about 2300 values (spanning approximately 9 years). We consider the squared  return rt 2 a volatility estimate for day t. Volatility change forecasts (volatility is going to  increase or decrease) based on historical returns can be interpreted as a buying or selling  signal for a straddle (see e.g. [3]). If the volatility decreases we go short (straddle is sold),  if it increases we take a long position (straddle is bought). In this respect, the quality of  a volatility model can be measured by the percentage of correctly predicted directions of  daily volatility differences.  650 P Tiao and G. Dorffner  Table 2: Prediction performance on the DJIA volatility series.  Percent Correct on test set  model '1 2 3 4 5 6  PFM 71.08 70.39 69.70 70.05 72.12" 72.46  VLMM 68.67 68.18 68.79 69.25 69.41 68.29  MM 68.56 69.11 69.78 68.28 69.50 73.13  7 8 9 10 11 12  PFM 74.01 71.77 73.84 73.84 71,77 74.19  VLMM 69.83 67.00 67.96 70.76 69.80 70.25  MM 74.16 71.96 69.95 69.16 71.74 71.07  The series { rt2+ x - rt 2 } of differences between the successive squared returns is transformed  into a sequence { Dt } over 4 symbols by quantizing the series { rt2+x - rt 2 } as follows:  1 (extreme down),  2 (normal down),  3 (normal up),  4 (extreme up),  if rt2+s - rt 2 < 0s < 0  if0 <rt2+s-rt 2 <0  if 0 < rt2_k - rt 2 < 02  if 02-_< r/+ - rt 2,  (7)  where the parameters 0 and 02 correspond to Q percent and (100 - Q) percent sample  quantiles, respectively. So, the upper (lower) Q% of all daily volatility increases (de-  creases) in the sample are considered extremal, and the lower (upper) (50 - Q)% of daily  volatility increases (decreases) are viewed as normal.  Each epoch is partitioned into training, validation and test parts containing 110, 600 and  600 symbols, respectively. Maximum memory length L for VLMMs and PFMs was set to  10 (two weeks). We trained classical MMs, VLMMs and PFMs with various numbers of  prediction contexts (up to 256) and extremal event quantiles Q E {5, 10, 15, ..., 45}. For  each model class, the model size and the quantile Q to be used on the test set were'selected  according to the validation set performance. Performance of the models was quantified as  the percentage of correct guesses of the volatility change direction for the next day. If the  next symbol is 1 or 2 (3 or 4) and the sum of conditional next symbol probabilities for 1  and 2 (3 and 4) given by a model is greater than 0.5, the model guess is considered correct.  Results are shown in Table 2. Paired t-test reveals that PFMs significantly (p < 0.005)  outperform both VLMMs and classical MMs.  Of course, fixed-order MMs are just special cases of VLMMs, so theoretically, VLMMs  cannot perform worse than MMs. We present separate results for MMs and VLMMs to  illustrate practical problems in fitting VLMMs. Besides familiar problems with setting the  construction parameter values, one-parameter-schemes (like that presented in [4] and used  here) operate only on small subsets of potential VLMMs. On data sets with a rather shallow  memory structure, this can have a negative effect.  The third experiment extends the work of Ron, Singer and Tishby [5]. They tested classical  MMs and VLMMs on the Bible. The alphabet is English letters and the blank character (27  symbols). The training set consisted of the Bible except for the book of Genesis. The test  set was a portion of 236 characters from the book of Genesis. They set the maximal mem-  ory depth to L = 30 and constructed a VLMM with about 3000 contexts. Summarizing the  results in [5], classical MMs of order 0, 1, 2 and 3 achieved negative log-likelihoods per  Predictive Models from Fractal Representations of Sequences 651  character (NNL) of 0.853, 0.681, 0.560 and 0.555, respectively. The authors point out a  huge difference between the number of states in MMs of order 2 and 3:27 a- 272 = 18954.  VLMM performed much better and achieved an NNL of 0.456. In our experiments, we  set the maximal memory length to L = 30 (the same maximal memory length was used for  VLMM construction in [5]). PFMs were constructed by vector quantizing a 5-dimensional  (alphabet has 27 symbols) spatial representation of 30-blocks appearing in the training set.  On the test set, PFMs with 100, 500, 1000 and 3000 predictive contexts achieved an NNL  of 0.622, 0.518, 0.510 and 0.435.  4 Conclusion  We presented a novel approach for building finite memory predictive models similar in  spirit to variable memory length Markov models (VLMMs). Constructing a series of  VLMMs is often a troublesome and highly time-consuming task requiring a lot of interac-  tive steps. Our predictive models, prediction fractal machines (PFMs), can be constructed  in a completely automatic and intuitive way - the number of codebook vectors in the vector  quantization PFM construction step corresponds to the number of predictive contexts.  We tested our model on three data sets with different memory and stochastic components.  VLMMs excel over the classical MMs on the Feigenbaum sequence requiring deep pre-  diction contexts. On this sequence, PFMs achieved the same performance as their rivals  - VLMMs. On financial time series, PFMs significantly outperform the purely symbolic  Markov models - MMs and VLMMs. On natural language Bible data, our PFM outper-  forms a VLMM of comparable size.  Acknowledgments  This work was supported by the Austrian Science Fund (FWF) within the research project  "Adaptive Information Systems and Modeling in Economics and Management Science"  (SFB 010) and the Slovak Academy of Sciences grant SAV 2/6018/99. The Austrian Re-  search Institute for Artificial Intelligence is supported by the Austrian Federal Ministry of  Science and Transport.  References  [ 1 ] P. Biihlmann. Model selection for variable length Markov chains and tuning the context algorithm.  Annals of the Institute of Statistical Mathematics, (in press), 1999.  [2] J. Freund, W. Ebeling, and K. Rateitschak. Self-similar sequences and universal scaling of  dynamical entropies. Physical Review E, 54(5), pp. 5561-5566, 1996.  [3] J. Noh, R.F. Engle, and A. Kane. Forecasting volatility and option prices of the s&p 500 index.  Journal of Derivatives, pp. 17-30, 1994.  [4] D. Ron, Y. Singer, and N. Tishby. The power of amnesia. In Advances in Neural Information  Processing Systems 6, pp. 176-183. Morgan Kaufmann, 1994.  [5] D. Ron, Y. Singer, and N. Tishby. The power of amnesia. Machine Learning, 25, 1996.  [6] P. Tifio. Spatial representation of symbolic sequences through iterative function system. IEEE  Transactions on Systems, Man, and Cybernetics Part A: Systems and Humans, 29(4), pp. 386-392,  1999.  [7] M.J. Weinberger, J.J. Rissanen, and M. Feder. A universal finite memory source. IEEE Transac-  tions on Information Theory, 41(3), pp. 643-652, 1995.  
Boosting Algorithms as Gradient Descent  Llew Mason  Research School of Information  Sciences and Engineering  Australian National University  Canberra, ACT, 0200, Australia  Imason @syseng. anu. edu. au  Jonathan Baxter  Research School of Information  Sciences and Engineering  Australian National University  Canberra, ACT, 0200, Australia  Jonathan. Baxter@anu. edu. au  Peter Bartlett  Research School of Information  Sciences and Engineering  Australian National University  Canberra, ACT, 0200, Australia  Peter. Bartlett@anu. edu. au  Marcus Frean  Department of Computer Science  and Electrical Engineering  The University of Queensland  Brisbane, QLD, 4072, Australia  marcusf@elec. uq. edu. au  Abstract  We provide an abstract characterization of boosting algorithms as  gradient decsent on cost-functionals in an inner-product function  space. We prove convergence of these functional-gradient-descent  algorithms under quite weak conditions. Following previous theo-  retical results bounding the generalization performance of convex  combinations of classifiers in terms of general cost functions of the  margin, we present a new algorithm (DOOM II) for performing a  gradient descent optimization of such cost functions. Experiments  on several data sets from the UC Irvine repository demonstrate  that DOOM II generally outperforms AdaBoost, especially in high  noise situations, and that the overfitting behaviour of AdaBoost is  predicted by our cost functions.  I Introduction  There has been considerable interest recently in voting methods for pattern classi-  fication, which predict the label of a particular example using a weighted vote over  a set of base classifiers [10, 2, 6, 9, 16, 5, 3, 19, 12, 17, 7, 11, 8]. Recent theoretical  results suggest that the effectiveness of these algorithms is due to their tendency  to produce large margin classifiers [1, 18]. Loosely speaking, if a combination of  classifiers correctly classifies most of the training data with a large margin, then its  error probability is small.  In [14] we gave improved upper bounds on the misclassification probability of a  combined classifier in terms of the average over the training data of a certain cost  function of the margins. That paper also described DOOM, an algorithm for di-  rectly minimizing the margin cost function by adjusting the weights associated with  Boosting Algorithms as Gradient Descent 513  each base classifier (the base classifiers are suppiled to DOOM). DOOM exhibits  performance improvements over AdaBoost, even when using the same base hypothe-  ses, which provides additional empirical evidence that these margin cost functions  are appropriate quantities to optimize.  In this paper, we present a general class of algorithms (called AnyBoost) which  are gradient descent algorithms for choosing linear combinations of elements of an  inner product function space so as to minimize some cost functional. The normal  operation of a weak learner is shown to be equivalent to maximizing a certain inner  product. We prove convergence of AnyBoost under weak conditions. In Section 3,  we show that this general class of algorithms includes as special cases nearly all  existing voting methods. In Section 5, we present experimental results for a special  case of AnyBoost that minimizes a theoretically-motivated margin cost functional.  The experiments show that the new algorithm typically outperforms AdaBoost, and  that this is especially true with label noise. In addition, the theoretically-motivated  cost functions provide good estimates of the error of AdaBoost, in the sense that  they can be used to predict its overfitting behaviour.  2 AnyBoost  Let (x, y) denote examples from X x Y, where X is the space of measurements  (typically X C_ IR N ) and Y is the space of labels (Y is usually a discrete set or some  subset of IR). Let  denote some class of functions (the base hypotheses) mapping  X - Y, and lin () denote the set of all linear combinations of functions in . Let  l, ) be an inner product on lin (), and  C' lin () -   a cost functional on lin ().  Our aim is to find a function F E lin () minimizing C(F). We will proceed  iteratively via a gradient descent procedure.  Suppose we have some F E lin () and we wish to find a new f   to add to F  so that the cost C(F + el) decreases, for some small value of e. Viewed in function  space terms, we are asking for the "direction" f such that C(F + el) most rapidly  decreases. The desired direction is simply the negative of the functional derivative  of C at F, -VC(F), where:  OC(F + al) ,=o  VC(F)(x) := Oc ' (1)  where 1 is the indicator function of x. Since we are restricted to choosing our new  function f from , in general it will not be possible to choose f = -VC(F), so  instead we search for an f with greatest inner product with -VC(F). That is, we  should choose f to maximize -/VC(F), f). This can be motivated by observing  that, to first order in e, C(F + el) = C(F) + e (VC(F), f) and hence the greatest  reduction in cost will occur for the f maximizing -/VC(F),  For reasons that will become obvious later, an algorithm that chooses f attempting  to maximize -/VC(F), f) will be described as a weak learner.  The preceding discussion motivates Algorithm I (AnyBoost), an iterative algorithm  for finding linear combinations F of base hypotheses in  that minimize the cost  functional C(F). Note that we have allowed the base hypotheses to take values in  an arbitrary set Y, we have not restricted the form of the cost or the inner product,  and we have not specified what the step-sizes should be. Appropriate choices for  514 L. Mason, J. Baxter, P Bartlett andM. Frean  these things will be made when we apply the algorithm to more concrete situations.  Note also that the algorithm terminates when - (VC(Ft), ft+l) _ O, i.e when the  weak learner  returns a base hypothesis ft+l which no longer points in the downhill  direction of the cost function C(F). Thus, the algorithm terminates when, to first  order, a step in function space in the direction of the base hypothesis returned by   would increase the cost.  Algorithm I  AnyBoost  Require:   An inner product space (A', (,)) containing functions mapping from X to  some set Y.   A class of base classifiers  C A'.   A differentiable cost functional U: lin () - 1.   A weak learner (F) that accepts F E lin () and returns f E  with a  large value of - (VC(F), f).  Let Fo(x):= O.  fort:=OtoTdo  Let f/q-1 :----(Ft).  if - (VU(Ft),ft+i) 5 0 then  return Ft.  end if  Choose Wt+l.  Let Ft+l :---- Ft + Wtq-lfiq-1  end for  return FT+i.  3 A gradient descent view of voting methods  We now restrict our attention to base hypotheses f   mapping to Y = (+1),  and the inner product  (F,G) I   := - F(xi)a(xi) (2)  m  i=1  for all F, G  lin (), where S = {Xl, yl),..., (x,, y,)} is a set of training examples  generated according to some unknown distribution 7) on X x Y. Our aim now is to  find F  lin () such that Pr(,u)v sgn (F(x))  y is minimal, where sgn (F(x)) =  -1 if F(x) < 0 and sgn (F(x)) = 1 otherwise. In other words, sgn F should minimize  the misclassification probability.  The margin of F: X - R on example (x, y) is defined as yF(x). Consider margin  cost-functionals defined by  m  1  c(F) := -  m  i=1  where c: R - R is any differentiable real-valued function of the margin. With these  definitions, a quick calculation shows:  ra  1  - (VC(F), f) = rn 2 yYif(xi)c'(yiF(xi)).  i=1  Since positive margins correspond to examples correctly labelled by sgn F and neg-  ative margins to incorrectly labelled examples, any sensible cost function of the  Boosting Algorithms as Gradient Descent 515  Table 1: Existing voting methods viewed as AnyBoost on margin cost functions.  Algorithm Cost function Step size  AdaBoost [9] e -yF() Line search  ARC-X4 [2] (1 - yF(x))  1It  ConfidenceBoost [19] e -yF(z) Line search  LogitBoost [12] ln(1 q- e -yF(z)) Newton-Raphson  margin will be monotonically decreasing. Hence -c'(yiF(xi)) will always be posi-  tive. Dividing through by - Eim__l c'(yiF(xi)), we see that finding an f maximizing  - (VC(F), f) is equivalent to finding an f minimizing the weighted error  y D(i) where D(i):=  i:  c'(yir(xi))  Eim=l c'(yiF(xi))  for i = 1,... ,m.  Many of the most successful voting methods are, for the appropriate choice of margin  cost function c and step-size, specific cases of the AnyBoost algorithm (see Table 3).  A more detailed analysis can be found in the full version of this paper [15].  4 Convergence of AnyBoost  In this section we provide convergence results for the AnyBoost algorithm, under  quite weak conditions on the cost functional C. The prescriptions given for the  step-sizes wt in these results are for convergence guarantees only: in practice they  will almost always be smaller than necessary, hence fixed small steps or some form  of line search should be used.  The following theorem (proof omitted, see [15]) supplies a specific step-size for  AnyBoost and characterizes the limiting behaviour with this step-size.  Theorem 1. Let C: lin () - ll be any lower bounded, Lipschitz differentiable  cost functional (that is, there exists L  0 such that I]VC(F)-VC(F)II  for all F,F  E lin ()). Let Fo, F1,... be the sequence of combined hypotheses  generated by the AnyBoost algorithm, using step-sizes  (VC(Ft),ft+l) (3)  Wtq- 1 := Lllf+111  Then AnyBoost either halts on round T with -(7C(FT), fTq-1) _ O, Or C(Ft)  converges to some finite value C*, in which case limt_, IVC(Ft), ft+i) -- O.  The next theorem (proof omitted, see [15]) shows that if the weak learner can  always find the best weak hypothesis ft E  on each round of AnyBoost, and if  the cost functional C is convex, then any accumulation point F of the sequence  (Ft) generated by AnyBoost with the step sizes (3) is a global minimum of the  cost. For ease of exposition, we have assumed that rather than terminating when  -- (7C(FT), fT+i) _ O, AnyBoost simply continues to return FT for all subsequent  time steps t.  Theorem 2. Let C: lin () - 11 be a convex cost functional with the properties  in Theorem 1, and let (Ft) be the sequence of combined hypotheses generated by  the AnyBoost algorithm with step sizes given by (3). Assume that the weak hypoth-  esis class .T is negation closed (f  .T --4. -f  .T) and that on each round  516 L. Mason, J. Baxter, P Bartlett andM. Frean  the AnyBoost algorithm finds a function ft+ maximizing - (VC(Ft), ft+). Then  any accumulation point F of the sequence (Ft) satisfies supfe.  -(VC(F), f) =  O, and C(F) = infei n(.-) C(G).  5 Experiments  AdaBoost had been perceived to be resistant to overfitting despite the fact that  it can produce combinations involving very large numbers of classifiers. However,  recent studies have shown that this is not the case, even for base classifiers as  simple as decision stumps [13, 5, 17]. This overfitting can be attributed to the use  of exponential margin cost functions (recall Table 3).  The results in in [14] showed that overfitting may be avoided by using margin cost  functionals of a form qualitatively similar to  m  C(F) = I Z 1 tanh(AyiF(xi)) (4)  m  i--1  where  is an adjustable parameter controlling the steepness of the margin cost  function c(z) - 1 -tanh(z). For the theoretical analysis of [14] to apply, F  must be a convex combination of base hypotheses, rather than a general linear  combination. Henceforth (4) will be referred to as the normalized sigmoid cost  functional. AnyBoost with (4) as the cost functional and (2) as the inner product  will be referred to as DOOM II. In our implementation of DOOM II we use a  fixed small step-size e (for all of the experiments e = 0.05). For all details of the  algorithm the reader is referred to the full version of this paper [15].  We compared the performance of DOOM II and AriaBoost on a selection of nine  data sets taken from the UCI machine learning repository [4] to which various levels  of label noise had been applied. To simplify matters, only binary classification  problems were considered. For all of the experiments axis orthogonal hyperplanes  (also known as decision stumps) were used as the weak learner. Full details of  the experimental setup may be found in [15]. A summary of the experimental  results is shown in Figure 1. The improvement in test error exhibited by DOOM  II over AriaBoost is shown for each data set and noise level. DOOM II generally  outperforms AdaBoost and the improvement is more pronounced in the presence of  label noise.  The effect of using the normalized sigmoid cost function rather than the exponential  cost function is best illustrated by comparing the cumulative margin distributions  generated by AdaBoost and DOOM II. Figure 2 shows comparisons for two data  sets with 0% and 15% label noise applied. For a given margin, the value on the  curve corresponds to the proportion of training examples with margin less than or  equal to this value. These curves show that in trying to increase the margins of  negative examples AdaBoost is willing to sacrifice the margin of positive examples  significantly. In contrast, DOOM II 'gives up' on examples with large negative  margin in order to reduce the value of the cost function.  Given that AdaBoost does suffer from overfitting and is guaranteed to minimize an  exponential cost function of the margins, this cost function certainly does not relate  to test error. How does the value of our proposed cost function correlate against  AdaBoost's test error? Figure 3 shows the variation in the normalized sigmoid  cost function, the exponential cost function and the test error for AdaBoost for  two UCI data sets over 10000 rounds. There is a strong correlation between the  normalized sigmoid cost and AdaBoost's test error. In both data sets the minimum  Boosting Algorithms as Gradient Descent 517  3.5  3  2.5  2  1.5  1  0.5  0  -0.5  -1  -1.5  -2  I  sonar cleve ionosphere vote I  credit breazt-cancer  Data set  0% noise  15 ' noise  ama-mdans hypol splice  Figure 1: Summary of test error advantage (with standard error bars) of DOOM II  over AdaBoost with varying levels of noise on nine UCI data sets.  1  0.8  0.6  0.4  0.2  o  -I  breast-cancer-wisconsin  .... :  0% noise - DOOIvl II : , J   - 15% noise- AdaBoost .  ............. 15% noise - DOOM II  ............................ 7"';  -0.5 0 0.5  Margin  1  0.8  0.6  0.4  0.2  o  -1  splice  0 c, IlCilSe - AdaBoost . /  0% noise- DOOM II .' ///  15% noi - Adoost .  //  /'  -0.5 0 0.5  Mgin  Figure 2: Margin distributions for AdaBoost and DOOM II with 0% and 15% label  noise for the breast-cancer and splice data sets.  of AdaBoost's test error and the minimum of the normalized sigmoid cost very  nearly coincide, showing that the sigmoid cost function predicts when AriaBoost  will start to overfit.  References  [1]  [4]  [5]  P. L. Bartlett. The sample complexity of pattern classification with neural networks:  the size of the weights is more important than the size of the network. IEEE Trans-  actions on Information Theory, 44(2):525-536, March 1998.  L. Breiman. Bagging predictors. Machine Learning, 24(2):123-140, 1996.  L. Breiman. Prediction games and arcing algorithms. Technical Report 504, Depart-  ment of Statistics, University of California, Berkeley, 1998.  E. Keogh C. Blake and C. J. Merz. UCI repository of machine learning databases,  1998. http: / /www.ics.uci.edu/-mlearn/MLRepository. html.  T.G. Dietterich. An experimental comparison of three methods for constructing en-  sembles of decision trees: Bagging, boosting, and randomization. Technical report,  Computer Science Department, Oregon State University, 1998.  518 L. Mason, J. Baxter, P Bartlett andM. Frean  30  25  20  15  10  5  0  labor  AdaBoost res! e.,'tor --  Exponential ccst ............  Normalized sigmoid cost .............  ....... ?....\ ,-.  I 1o 100 100o 10000  Rounds  vote 1  ' AriaBoy, st st  Exponential cost .............  Norm',dized sigmoid cost .............  1 10 100 100o 100oo  Rounds  Figure 3: AdaBoost test error, exponential cost and normalized sigmoid cost over  10000 rounds of AdaBoost for the labor and votel data sets. Both costs have been  scaled in each case for easier comparison with test error.  [6] H. Drucker and C. Cortes. Boosting decision trees. In Advances in Neural Information  Processing Systems 8, pages 479-485, 1996.  [7] N. Duffy and D. Helmbold. A geometric approach to leveraging weak learners. In  Computational Learning Theory: dth European Conference, 1999. (to appear).  [8] Y. Freund. An adaptive version of the boost by majority algorithm. In Proceedings of  the Twelfth Annual Conference on Computational Learning Theory, 1999. (to appear).  [9] Y. Freund and R. E. Schapire. Experiments with a new boosting algorithm. In  Machine Learning: Proceedings of the Thirteenth International Conference, pages  148-156, 1996.  [10] Y. Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning  and an application to boosting. Journal of Computer and System Sciences, 55(1):119-  139, August 1997.  [11] J. Friedman. Greedy function approximation: A gradient boosting machine. Tech-  nical report, Stanford University, 1999.  [12] J. Friedman, T. Hastie, and R. Tibshirani. Additive logistic regression: A statistical  view of boosting. Technical report, Stanford University, 1998.  [13] A. Grove and D. Schuurmans. Boosting in the limit: Maximizing the margin of  learned ensembles. In Proceedings of the Fifteenth National Conference on Artificial  Intelligence, pages 692-699, 1998.  [14] L. Mason, P. L. Bartlett, and J. Baxter. Improved generalization through explicit  optimization of margins. Machine Learning, 1999. (to appear).  [15] Llew Mason, Jonathan Baxter, Peter Bartlett, and Marcus Frean. Functional Gradi-  ent Techniques for Combining Hypotheses. In Alex Smola, Peter Bartlett, Bernard  SchSlkopf, and Dale Schurmanns, editors, Large Margin Classifiers. MIT Press, 1999.  To appear.  [16] J. R. Quinlan. Bagging, boosting, and C4.5. In Proceedings of the Thirteenth National  Conference on Artificial Intelligence, pages 725-730, 1996.  [17] G. R/itsch, T. Onoda, and K.-R. M/iller. Soft margins for AdaBoost. Technical Report  NC-TR-1998-021, Department of Computer Science, Royal Holloway, University of  London, Egham, UK, 1998.  [18] R. E. Schapire, Y. Freund, P. L. Bartlett, and W. S. Lee. Boosting the margin  : A new explanation for the effectiveness of voting methods. Annals of Statistics,  26(5):1651-1686, October 1998.  [19] R. E. Schapire and Y. Singer. Improved boosting algorithms using confidence-rated  predictions. In Proceedings of the Eleventh Annual Conference on Computational  Learning Theory, pages 80-91, 1998.  
Modeling High-Dimensional Discrete Data with  Multi-Layer Neural Networks  Yoshua Bengio  Dept. IRO  Universitd de Montrdal  Montreal, Qc, Canada, H3C 3J7  bengioyOiro. umontreal. ca  Abstract  Samy Bengio*  IDIAP  CP 592, rue du Simplon 4,  1920 Martigny, Switzerland  bengioOidiap. ch  The curse of dimensionality is severe when modeling high-dimensional  discrete data: the number of possible combinations of the variables ex-  plodes exponentially. In this paper we propose a new architecture for  modeling high-dimensional data that requires resources (parameters and  computations) that grow only at most as the square of the number of vari-  ables, using a multi-layer neural network to represent the joint distribu-  tion of the variables as the product of conditional distributions. The neu-  ral network can be interpreted as a graphical model without hidden ran-  dom variables, but in which the conditional distributions are tied through  the hidden units. The connectivity of the neural network can be pruned by  using dependency tests between the variables. Experiments on modeling  the distribution of several discrete data sets show statistically significant  improvements over other methods such as naive Bayes and comparable  Bayesian networks, and show that significant improvements can be ob-  tained by pruning the network.  1 Introduction  The curse of dimensionality hits particularly hard on models of high-dimensional discrete  data because there are many more possible combinations of the values of the variables than  can possibly be observed in any data set, even the large data sets now common in data-  mining applications. In this paper we are dealing in particular with multivariate discrete  data, where one tries to build a model of the distribution of the data. This can be used for  example to detect anomalous cases in data-mining applications, or it can be used to model  the class-conditional distribution of some observed variables in order to build a classifier.  A simple multinomial maximum likelihood model would give zero probability to all of  the combinations not encountered in the training set, i.e., it would most likely give zero  probability to most out-of-sample test cases. Smoothing the model by assigning the same  non-zero probability for all the unobserved cases would not be satisfactory either because  it would not provide much generalization from the training set. This could be obtained by  using a multivariate multinomial model whose parameters 0 are estimated by the maximum  a-posteriori (MAP) principle, i.e., those that have the greatest probability, given the training  data D, and using a diffuse prior P(O) (e.g. Dirichlet) on the parameters.  A graphical model or Bayesian network [6, 5] represents the joint distribution of random  variables Z1 ... Zn with  7'  P(Z1. . . Zn) = H P(Zilearentsi)  i=1  Part of this work was done while S.B. was at CIRANO, Montreal, Qc. Canada.  Modeling High-Dimensional Discrete Data with Neural Networks 401  where Parents/ is the set of random variables which are called the parents of variable i  in the graphical model because they directly condition Zi, and an arrow is drawn, in the  graphical model, to Zi, from each of its parents. A fully connected "left-to-right" graphical  model is illustrated in Figure 1 (left), which corresponds to the model  P(ZI ' "Zn) = H P(ZilZl '"Zi-1)' (1)  Figure 1: Left: a fully connected 'qeft-to-right" graphical model.  Right: the architecture of a neural network that simulates a fully connected "left-to-right"  graphical model. The observed values Zi = zi are encoded in the corresponding input  unit group. hi is a group of hidden units. li is a group of output units, which depend  on z ... zi-, representing the parameters of a distribution over Zi. These conditional  probabilities P( ZiIZ ... Zi- ) are multiplied to obtain the joint distribution.  Note that this representation depends on the ordering of the variables (in that all previous  variables in this order are taken as parents). We call each combination of the values of  Parentsi a context. In the "exact" model (with the full table of all possible contexts) all the  orders are equivalent, but if approximations are used, different predictions could be made  by different models assuming different orders.  In graphical models, the curse of dimensionality shows up in the representation of condi-  tional distributions P(ZlParentsi) where Zi has many parents. If Zj E Parents can take  rtj values, there are I-Ij rtj different contexts which can occur in which one would like to  estimate the distribution of Zi. This serious problem has been addressed in the past by two  types of approaches, which are sometimes combined:  1. Not modeling all the dependencies between all the variables: this is the approach mainly  taken with most graphical models or Bayes networks [6, 5]. The set of independencies  can be assumed using a-priori or human expert knowledge or can be learned from data.  See also [2] in which the set Parentsi is restricted to at most one element, which is  chosen to maximize the correlation with  2. Approximating the mathematical form of the joint distribution with a form that takes only  into account dependencies of lower order, or only takes into account some of the possi-  ble dependencies, e.g., with the Rademacher-Walsh expansion or multi-binomial [1, 3],  which is a low-order polynomial approximation of a full joint binomial distribution (and  is used in the experiments reported in this paper).  The approach we are putting forward in this paper is mostly of the second category, al-  though we are using simple non-parametric statistics of the dependency between pairs of  variables to further reduce the number of required parameters.  In the multi-binomial model [3], the joint distribution of a set of binary variables is approx-  imated by a polynomial, Whereas the "exact" representation of P(Z -- z,... Zn = zn)  as a function of z... zn is a polynomial of degree rt, it can be approximated with a lower  402 Y. Bengio and S. Bengio  degree polynomial, and this approximation can be easily computed using the Rademacher-  Walsh expansion [I] (or other similar expansions, such as the Bahadur-Lazarsfeld ex-  pansion [1]). Therefore, instead of having 2 n parameters, the approximated model for  P(Z,... Zn) only requires O(n k) parameters. Typically, order k -- 2 is used. The model  proposed here also requires O(n 2) parameters, but it allows to model dependencies be-  tween tuples of variables, with more than 2 variables at a time.  In previous related work by Frey [4], a fully-connected graphical model is used (see Fig-  ure I, left) but each of the conditional distributions is represented by a logistic, which take  into account only first-order dependency between the variables:  1  P(Zi = 11Z ...Zi_l):  1 + exp(-w0 - Yj<i wjZj)'  In this paper, we basically extend Frey's idea to using a neural network with a hidden  layer, with a particular architecture, allowing multinomial or continuous variables, and we  propose to prune down the network weights. Frey has named his model a Logistic Au-  toregressive Bayesian Network or LARC. He argues that the prior variances on the logistic  weights (which correspond to inverse weight decays) should be chosen inversely propor-  tional to the number of conditioning variables (i.e. the number of inputs to the particular  output neuron). The model was tested on a task of learning to classify digits from 8x8 bi-  nary pixel images. Models with different orderings of the variables were compared and did  not yield significant differences in performance. When averaging the predictive probabili-  ties from 10 different models obtained by considering 10 different random orderings, Frey  obtained small improvements in likelihood but not in classification. The model performed  better or equivalently to other models tested: CART, naive Bayes, K-nearest neighbors, and  various Bayesian models with hidden variables (Helmholtz machines). These results are  impressive, taking into account the simplicity of the LARC model.  2 Proposed Architecture  The proposed architecture is a "neural network" implementation of a graphical model  where all the variables are observed in the training set, with the hidden units playing a sig-  nificant role to share parameters across different conditional distributions. Figure 1 (right)  illustrates the model in the simpler case of a fully connected (left-to-right) graphical model  (Figure 1, left). The neural network represents the parametrized function  fo(zl,... ,Zn) - log(PO(Zl = z,...,Zn -- Zn)) (2)  approximating the joint distribution of the variables, with parameters 0 being the weights of  the neural network. The architecture has three layers, with each layer organized in groups  associated to each of the variables. The above log-probability is computed as the sum of  conditional log-probabilities  n  fo(zl,... ,zn): y log(P(Zi = zi[gi(z1,...,Zi_l)))  where gi(z,..., zi-) is the vector-valued output of the i-th group of output units, and  it gives the value of the parameters of the distribution of Zi when Z = z, Z2 :  z2,..., Zi-1 = zi-. For example, in the ordinary discrete case, gi may be the vector  of probabilities associated with each of the possible values of the multinomial random  variable Zi. In this case, we have  In this example, a softmax output for the i-th group may be used to force these parameters  to be positive and sum to I, i.e.,  egi,i t  gi,i  --  Modeling High-Dimensional Discrete Data with Neural Networks 403  where g,i' are linear combinations of the hidden units outputs, with i  ranging over the  number of elements of the parameter vector associated with the distribution of Zi (for a  fixed value of Z1 ... Zi-). To guarantee that the functions gi(zl,..., zi-) only depend  on zl .. zi-1 and not on any of zi .. zn, the connectivity struture of the hidden units must  be constrained as follows:  m5  j_i j'=l  where the b's arc biases and the w's arc weights of the output layer, and the hj,j, is the  output of the j'-th unit (out of raj such units) in the j~th group of hidden layer nodes. It  may bc computed as follows:  nk  hj,j, : tanh(cj,j, + E E Vj,j',k,k' Zk,,)  k<j k=l  where the c's are biases and the v's are the weights of the hidden layer, and zk,, is k-th  element of the vectorial input representation of the value Z - z. For example, in the  binary case (zi: 0 or 1) we have used only one input node, i.e.,  Zi binomial --> zi,o = zi  and in the multinomial case we use the one-hot encoding,  Zi E {0, 1,... i -- 1} --> Zi,i' --' 6zi,i'  where (i,i' ' i if i -- i  and 0 otherwise. The input layer has n - I groups because  the value Zn = Zn is not used as an input. The hidden layer also has n - I groups  corresponding to the variables j: 2 to n (since P(Zx) is represented unconditionally in  the first output group, its corresponding group does not need any hidden units or inputs, but  just has biases).  2.1 Discussion  The number of free parameters of the model is O(n2H) where H = maxi mj is the maxi-  mum number of hidden units per hidden group (i.e., associated with one of the variables).  This is basically quadratic in the number of variables, like the multi-binomial approxima-  tion that uses a polynomial expansion of the joint distribution. However, as H is increased,  representation theorems for neural networks suggest that we should be able to approximate  with arbitrary precision the true joint distribution. Of course the true limiting factor is the  amount of data, and H should be tuned according to the amount of data. In our experiments  we have used cross-validation to choose a value of mj = H for all the hidden groups. In  this sense, this neural network representation of P(Z1... Zn) is to the polynomial expan-  sions (such as the multi-binomial) what ordinary multilayer neural networks for function  approximation are to polynomial function approximators. It allows to capture high-order  dependencies, but not all of them. It is the number of hidden units that controls "how  many" such dependencies will be captured, and it is the data that "chooses" which of the  actual dependencies are most useful in maximizing the likelihood.  Unlike Bayesian networks with hidden random variables, learning with the proposed archi-  tecture is very simple, even when there are no conditional independencies. To optimize the  parameters we have simply used gradient-based optimization methods, either using con-  jugate or stochastic (on-line) gradient, to maximize the total log-likelihood which is the  sum of values of f (eq. 2) for the training examples. A prior on the parameters can be  incorporated in the cost function and the MAP estimator can be obtained as easily, by max-  imizing the total log-likelihood plus the log-prior on the parameters. In our experiments  we have used a "weight decay" penalty inspired by the analysis of Frey [4], with a penalty  proportional to the number of weights incoming into a neuron.  404 Y. Bengio and S. Bengio  However, it is not so clear how the distribution could be generally marginalized, except  by summing over possibly many combinations of the values of variables to be integrated.  Another related question is whether one could deal with missing values: if the total number  of values that the missing variables can take is reasonably small, then one can sum over  these values in order to obtain a marginal probability and maximize this probability. If  some variables have more systematically missing values, they can be put at the end of the  variable ordering, and in this case it is very easy to compute the marginal distribution (by  taking only the product of the output probabilities up to the missing variables). Similarly,  one can easily compute the predictive distribution of the last variable given the first r - 1  variables.  The framework can be easily extended to hybrid models involving both continuous and  discrete variables. In the case of continuous variables, one has to choose a parametric form  for the distribution of the continuous variable when all its parents (i.e., the conditioning  context) are fixed. For example one could use a normal, log-normal, or mixture of normals.  Instead of having softmax outputs, the i-th output group would compute the parameters  of this continuous distribution (e.g., mean and log-variance). Another type of extension  allows to build a conditional distribution, e.g., to model P(Z... ZnlX... Xm). One  just adds extra input units to represent the values of the conditioning variables X... Xm.  Finally, an architectural extension that we have implemented is to allow direct input-to-  output connections (still following the rules of ordering which allow 7i to depend only on  zi... zi-i). Therefore in the case where the number of hidden units is 0 (H = 0) we obtain  the LARC model proposed by Frey [4].  2.2 Choice of topology  Another type of extension of this model which we have found very useful in our experi-  ments is to allow the user to choose a topology that is not fully connected (left-to-right). In  our experiments we have used non-parametric tests to heuristically eliminate some of the  connections in the network, but one could also use expert or prior knowledge, just as with  regular graphical models, in order to cut down on the number of free parameters.  In our experiments we have used for a pairwise test of statistical dependency the  Kolmogorov-Smirnov statistic (which works both for continuous and discrete variables).  The statistic for variables X and Y is  s = x/sup I/5(X _< xi,Y _< yi) -/5(X _< xi)/5(Y <_ Yi)l  i  where 1 is the number of examples and/5 is the empirical distribution (obtained by counting  over the training data). We have ranked the pairs according to their value of the statistic s,  and we have chosen those pairs for which the value of statistic is above a threshold value  s*, which was chosen by cross-validation. When the pairs {(Zi, Zj)} are chosen to be part  of the model, and assuming without loss of generality that i < j for those pairs, then the  only connections that are kept in the network (in addition to those from the k-th hidden  group to the k-th output group) are those from hidden group i to output group j, and from  input group i to hidden group j, for every such (Z, Zj) pair.  3 Experiments  In the experiments we have compared the following models:   Naive Bayes: the likelihood is obtained as a product of multinomials (one per variable).  Each multinomial is smoothed with a Dirichlet prior.   Multi-Binomial (using Rademacher-Walsh expansion of order 2) [3]. Since this only  handles the case of binary data, it was only applied to the DNA data set.   A simple graphical model with the same pairs of variables and variable ordering as se-  lected for the neural network, but in which each of the conditional distribution is modeled  Modeling High-Dimensional Discrete Data with Neural Networks 405  by a separate multinomial for each of the conditioning context. This works only if the  number of conditioning variables is small so in the Mushroom, Audiology, and Soybean  experiments we had to reduce the number of conditioning variables (following the order  given by the above tests). The multinomials are also smoothed with a Dirichlet prior.   Neural network: the architecture described above, with or without hidden units (i.e.,  LARC), with or without pruning.  5-fold cross-validation was used to select the number of hidden units per hidden group  and the weight decay for the neural network and LARC. Cross-validation was also used  to choose the amount of pruning in the neural network and LARC, and the amount of  smoothing in the Dirichlet priors for the multinomials of the naive Bayes model and the  simple graphical model.  3.1 Results  All four data sets were obtained on the web from the UC! Machine Learning and STATLOG  databases. Most of these are meant to be for classification tasks but we have instead ignored  the classification and used the data to learn a probabilistic model of all the input features.   DNA (from STATLOG): there are 180 binary features. 2000 cases were used for training  and cross-validation, and 1186 for testing.   Mushroom (from UCI): there are 22 discrete features (taking each between 2 and 12  values). 4062 cases were used for training and cross-validation, and 4062 for testing.   Audiology (from UCI): there are 69 discrete features (taking each between 2 and 7 val-  ues). 113 cases are used for training and 113 for testing (the original train-test partition  was 200 + 26 and we concatenated and re-split the data to obtain more significant test  figures).   Soybean (from UCI): there are 35 discrete features (taking each between 2 and 8 values).  307 cases are used for training and 376 for testing.  Table I clearly shows that the proposed model yields promising results since the pruned  neural network was superior to all the other models in all 4 cases, and the pairwise differ-  ences with the other models are statistically significant in all 4 cases (except Audiology,  where the difference with the network without hidden units, LARC, is not significant).  4 Conclusion  In this paper we have proposed a new application of multi-layer neural networks to the mod-  elization of high-dimensional distributions, in particular for discrete data (but the model  could also be applied to continuous or mixed discrete / continuous data). Like the polyno-  mial expansions [3] that have been previously proposed for handling such high-dimensional  distributions, the model approximates the joint distribution with a reasonable (O (r 2)) num-  ber of free parameters but unlike these it allows to capture high-order dependencies even  when the number of parameters is small. The model can also be seen as an extension of  the previously proposed auto-regressive logistic Bayesian network [4], using hidden units  to capture some high-order dependencies.  Experimental results on four data sets with many discrete variables are very encouraging.  The comparisons were made with a naive Bayes model, with a multi-binomial expansion,  with the LARC model and with a simple graphical model, showing that a neural network  did significantly better in terms of out-of-sample log-likelihood in all cases.  The approach to pruning the neural network used in the experiments, based on pairwise  statistical dependency tests, is highly heuristic and better results might be obtained using  approaches that take into account the higher order dependencies when selecting the con-  ditioning variables. Methods based on pruning the fully connected network (e.g., with a  "weight elimination" penalty) should also be tried. Also, we have not tried to optimize  406 Y. Bengio and S. Bengio  DNA Mushroom  mean (stdev) p-value mean (stdev) p-value  naive Bayes 100.4 (. 18) < le-9 47.00 (.29) < 1 e-9  multi-Binomial order 2 117.8 (.01) < le-9  ordinary graph. model 108.1 (.06) <le-9 44.68 (.26) <le-9  LARC 83.2 (.24) 7e-5 42.51 (.16) <le-9  pruned LARC 91.2 (.15) <le-9 43.87 (.13) <le-9  full-conn. neural net. 120.0 (.02) < le-9 33.58 (.01) < le-9  pruned neural network 82.9 (.21) 31.25 (.04)  Audiology Soybean  mean (stdev) p-value mean (stdev) p-value  naive Bayes 36.40 (2.9) < le-9 34.74 (1.0) < le-9  multi-Binomial order 2  ordinary graph. model 16.56 (.48) 6.8e-4 43.65 (.07) < le-9  LARC 17.69 (.65) <le-9 16.95 (.35) 5.5e-4  pruned LARC 16.69 (.41) 0.20 19.06 (.43) < le-9  full-conn. neural net. 17.39 (.58) < le-9 21.65 (.43) < le-9  pruned neural network 16.37 (.45) 16.55 (.27)  Table 1: Average out-of-sample negative log-likelihood obtained with the various models  on four data sets (standard deviations of the average in parenthesis and p-value to test  the null hypotheses that a model has same true generalization error as the pruned neural  network). The pruned neural network was better than all the other models in in all cases,  and the pair-wise difference is always statistically significant (except with respect to the  pruned LARC on Audiology).  the order of the variables, or combine different networks obtained with different orders,  like [4].  References  [ 1 ] R.R. Bahadur. A representation of the joint distribution of responses to n dichotomous  items. In ed. H. Solomon, editor, Studies in Item Analysis and Predictdion, pages  158-168. Stanford University Press, California, 1961.  [2] C.K. Chow. A recognition method using neighbor dependence. IRE Trans. Elec.  Comp., EC- 11:683-690, October 1962.  [3] R.O. Duda and P.E. Hart. Pattern Classification and Scene Analysis. Wiley, New York,  1973.  [4] B. Frey. Graphical models for machine learning and digital communication. MIT  Press, 1998.  [5] Steffen L. Lauritzen. The EM algorithm for graphical association models with missing  data. Computational Statistics and Data Analysis, 19:191-201, 1995.  [6] Judea Pearl. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible  Inference. Morgan Kaufmann, 1988.  
Channel Noise in Excitable Neuronal  Membranes  Amit Manwani;  Peter N. Steinmetz and Christof Koch  Computation and Neural Systems Program, M-S 139-74  California Institute of Technology Pasadena, CA 91125  {quixote, peter, koch} @ klab. caltech. edu  Abstract  Stochastic fluctuations of voltage-gated ion channels generate current  and voltage noise in neuronal membranes. This noise may be a criti-  cal determinant of the efficacy of information processing within neural  systems. Using Monte-Carlo simulations, we carry out a systematic in-  vestigation of the relationship between channel kinetics and the result-  ing membrane voltage noise using a stochastic Markov version of the  Mainen-Sejnowski model of dendritic excitability in cortical neurons.  Our simulations show that kinetic parameters which lead to an increase  in membrane excitability (increasing channel densities, decreasing tem-  perature) also lead to an increase in the magnitude of the sub-threshold  voltage noise. Noise also increases as the membrane is depolarized from  rest towards threshold. This suggests that channel fluctuations may in-  terfere with a neuron's ability to function as an integrator of its synaptic  inputs and may limit the reliability and precision of neural information  processing.  1 Introduction  Voltage-gated ion channels undergo random transitions between different conformational  states due to thermal agitation. Generally, these states differ in their ionic permeabilities  and the stochastic transitions between them give rise to conductance fluctuations which  are a source of membrane noise [1]. In excitable cells, voltage-gated channel noise can  contribute to the generation of spontaneous action potentials [2, 3], and the variability of  spike timing [4]. Channel fluctuations can also give rise to bursting and chaotic spiking  dynamics in neurons [5, 6].  Our interest in studying membrane noise is based on the thesis that noise ultimately limits  the ability of neurons to transmit and process information. To study this problem, we com-  bine methods from information theory, membrane biophysics and compartmental neuronal  modeling to evaluate ability of different biophysical components of a neuron, such as the  synapse, the dendritic tree, the soma and so on, to transmit information [7, 8, 9]. These  neuronal components differ in the type, density, and kinetic properties of their constituent  ion channels. Thus, measuring the impact of these differences on membrane noise rep-  * http://www.klab.caltech.eduFquixote  144 A. Manwani, P. N. Steinmetz and C. Koch  resents a fundamental step in our overall program of evaluating information transmission  within and between neurons.  Although information in the nervous system is mostly communicated in the form of action  potentials, we first direct our attention to the study of sub-threshold voltage fluctuations for  three reasons. Firstly, voltage fluctuations near threshold can cause variability in spike tim-  ing and thus directly influence the reliability and precision of neuronal activity. Secondly,  many computations putatively performed in the dendritic tree (coincidence detection, mul-  tiplication, synaptic integration and so on) occur in the sub-threshold regime and thus are  likely to be influenced by sub-threshold voltage noise. Lastly, several sensory neurons in  vertebrates and invertebrates are non-spiking and an analysis of voltage fluctuations can be  used to study information processing in these systems as well.  Extensive investigations of channel noise were carried out prior to the advent of the patch-  clamp technique in order to provide indirect evidence for the existence of single ion chan-  nels (see [ 1 ] for an excellent review). More recently, theoretical studies have focused on the  effect of random channel fluctuations on spike timing and reliability of individual neurons  [4], as well as their effect on the dynamics of interconnected networks of neurons [5, 6].  In this paper, we determine the effect of varying the kinetic parameters, such as channel  density and the rate of channel transitions, on the magnitude of sub-threshold voltage noise  in an iso-potential membrane patches containing stochastic voltage-gated ion channels us-  ing Monte-Carlo simulations. The simulations are based on the Mainen-Sejnowski (MS)  kinetic model of active channels in the dendrites of cortical pyramidal neurons [10]. By  varying two model parameters (channel densities and temperature), we investigate the rela-  tionship between excitability and noise in neuronal membranes. By linearizing the channel  kinetics, we derive analytical expressions which provide closed-form estimates of noise  magnitudes; we contrast the results of the simulations with the linearized expressions to  determine the parameter range over which they can be used.  2 Monte-Carlo Simulations  Consider an iso-potential membrane patch containing voltage-gated K + and Na+channels  and leak channels,  --7 dVm  dt = gE (V - EE) + gN (V - ENd) + g (V - E) + hn (1)  where C is the membrane capacitance and #K (gNa, gL) and EK (JNa, EL) denote the  K + (Na + , leak) conductance and the K + (Na +, leak) reversal potential respectively. Current  injected into the patch is denoted by Ira j, with the convention that inward current is nega-  tive. The channels which give rise to potassium and sodium conductances switch randomly  between different conformational states with voltage-dependent transition rates. Thus, #K  and #va are voltage-dependent random processes and eq. 1 is a non-linear stochastic differ-  ential equation. Generally, ion channel transitions are assumed to be Markovian [11] and  the stochastic dynamics of eq. 1 can be studied using Monte-Carlo simulations of finite-  state Markov models of channel kinetics.  Earlier studies have carried out simulations of stochastic versions of the classical Hodgkin-  Huxley kinetic model [12] to study the effect of conductance fluctuations on neuronal  spiking [13, 2, 4]. Since we are interested in sub-threshold voltage noise, we consider  a stochastic Markov version of a less excitable kinetic model used to describe dendrites of  cortical neurons [10]. We shall refer to it as the Mainen-Sejnowski (MS) kinetic scheme.  The K+ conductance is modeled by a single activation sub-unit (denoted by n) whereas  the Na+conductance is comprised of three identical activation sub-units (denoted by m)  and one inactivation sub-unit (denoted by/). Thus, the stochastic discrete-state Markov  models of the K+and Na+channel have 2 and 8 states respectively (shown in Fig. 1). The  Channel Noise in Excitable Neural Membranes 145  A  single channel conductances and the densities of the ion channels (K +,Na +) are denoted  by (7K,7Na) and (r/K,r/va) respectively. Thus, gx and gv) are given by the products of  the respective single channel conductances and the corresponding numbers of channels in  the conducting states.  gNa(v't): Na [m3h]  gK(v,t) =K [nl]  Figure 1: Kinetic scheme for the voltage-gated  Mainen-Sejnowski K+(A) and Na+(B) channels.  no and n represent the closed and open states of  K+channel. rn0-2h represent the 3 closed states,  rn0-ah0 the four inactivated states and rnah the  open state of the Na+channel.  were held at the fixed value corresponding to the  for details of this procedure).  -70 -60 V (m,50 -40  m  2  -z  Figure 2: Steady-state I-V curves for different  multiples (2v,) of the nominal MS Na+channel  density. Circles indicate locations of fixed-points  in the absence of current injection.  We performed Monte-Carlo simulations  of the MS kinetic scheme using a fixed  time step of At = 10 ttsec. During each  step, the number of sub-units undergo-  ing transitions between states i and j  was determined by drawing a pseudo-  random binomial deviate (bnldov sub-  routine [14] driven by the ran2 subrou-  tine of the 2 nd edition) with N equal  to the number of sub-units in state i  and p given by the conditional proba-  bility of the transition between i and j.  After updating the number of channels  in each state, eq. 1 was integrated us-  ing fourth order Runge-Kutta integration  with adaptive step size control [ 14]. Dur-  ing each step, the channel conductances  new numbers of open channels. (See [4]  Due to random channel transitions, the  membrane voltage fluctuates around the  steady-state resting membrane voltage  Vrest. By varying the magnitude of  the constant injected current Iinj, the  steady-state voltage can be varied over a  broad range, which depends on the chan-  nel densities. The current required to  maintain the membrane at a holding volt-  age Vhota can be determined from the  steady-state I-V curve of the system, as  shown in Fig. 2. Voltages for which  the slope of the I-V curve is negative  cannot be maintained as steady-states.  By injecting an external current to offset  the total membrane current, a fixed point  in the negative slope region can be ob-  tained but since the fixed point is unsta-  ble, any perturbation, such as a stochastic  ion channel opening or closing, causes  the system to be driven to the closest sta-  ble fixed point. We measured sub-threshold voltage noise only for stable steady-state hold-  ing voltages. A typical voltage trace from our simulations is shown in Fig. 3. To estimate  the standard deviation of the voltage noise accurately, simulations were performed for a  total of 492 seconds, divided into 60 blocks of 8.2 seconds each, for each steady-state  value.  146 A. Manwani, P N. Steinmetz and C. Koch  -64  -65 -  -66   -67   1 O0 200 300 400 500  Time (msec)  Figure 3: Monte-Carlo simulations  of a 1000 /zm 2 membrane patch with  stochastic Na + and deterministic K +  channels with MS kinetics. Bottom  record shows the number of open Na +  channels as a function of time. Top  trace shows the corresponding fluctua-  tions of the membrane voltage. Sum-  mary of nominal MS parameters: G, =  0.75 F/cm 2 r/: = 1.5 channels/m 2  r/N, = 2 channels//.m 2, E: = -90 mV,  EN. = 60 mV, Es = -70 mV, g5 = 0.25  PS//zm 2 , 'YK = 3'Na = 20 pS.  3 Linearized Analysis  The non-linear stochastic differential equation (eq. 1) cannot be solved analytically. How-  ever, one can linearize it by expressing the ionic conductances and the membrane voltage  as small perturbations (6) around their steady-state values:  -C d6V,  dt - (g: + gva + g)6V, + (V  - EK)6gK + (V  - ENa)6gNa (2)  where g: and 9Va denote the values of the ionic conductances at the steady-state voltage  V . G = g + gva + g is the total steady-state patch conductance. Since the leak channel  conductance is constant, 6g = 0. On the other hand, 6gK and 6gNa depend on 6V and t.  It is known that, to first order, the voltage- and time-dependence of active ion channels can  be modeled as phenomenological impedances [15, 16]. Fig. 4 shows the linearized equiv-  alent circuit of a membrane patch, given by the parallel combination of the capacitance C,  the conductance G and three series RL branches representing phenomenological models of  K + activation, Na + activation and Na + inactivation.  = K(EK -- ) + -- ) (3)  represents the current noise due to fluctuations in the channel conductances (denoted by  0K and ONe) at the membrane voltage V (also referred to as holding voltage Vhota).  The details of the linearization are provided [16]. The complex admittance (inverse of the  impedance) of Fig. 4 is given by,  1 1  (f) = G + j2rfC + +  rn + j2xfln rm + j2xflm  The variance of the voltage fluctuations or} can be computed as,  + (4)  rh + j2rfl  (5)  where the power spectral density of In is given by the sum of the individual channel current  noise spectra, Sn(f) = $K(f) + SNa(f).  For the MS scheme, the autocovariance of the K + current noise for patch of area A, clamped  at a voltage V, can be derived using [1, 11],  CK(t) = A rlK ,y, (V  - Ex )2 nm (1 -nm) e -ltl/ (6)  where n and r respectively denote the steady-state probability and time constant of the  K+ activation sub-unit at V. The power special density of the K+cuent noise SzK(f)  can be obtained from the Fourier transform of Cx (t),  SiK(f) : 2 A V  (V - EK)2n r (7)  1 + (2Wfrn) 2  Channel Noise in Excitable Neural Membranes 147  I rn rh rm  In Inn  Figure 4: Linearized circuit of the  membrane patch containing stochastic  voltage-gated ion channels. G denotes  the membrane capacitance, G is the sum  of the steady-state conductances of the  channels and the leak. ri's and/i's de-  note the phenomenological resistances  and inductances due to the voltage- and  time-dependent ionic conductances.  Thus, $zK(f) is a single Lorentzian spectrum with cut-off frequency determined by  Similarly, the auto-covariance of the MS Na+current noise can be written as [1],  where  c, Na(t) = AnNa  m(t) : mm + (1- mm)e -t/Tin, h(t) -- ha + (1- h)e -t/Th (9)  As before, mo (hm) and r, (rh) are the open probability and the time constant of  Na+activation (inactivation) sub-unit. The Na+current noise spectrum SN(f) can be  expressed as a sum of Lorentzian spectra with cut-off frequencies corresponding to the  seven time constants rm, rh, 2 rm, 3 rra, rra + rI, 2 rra + rh and 3 rm + rh. The details of  the derivations can be found in [8].  A B  5  4  2  1  0  +  J + ++  3  -80 -60 -,0 -20 -80 -60  Vhoid (mY) Vhoid (mY)  -20  Figure 5: Standard deviation of the voltage noise av in a 1000/rn 2patch as a function of the  holding voltage Vhota. Circles denote results of the Monte-Carlo simulations for the nominal MS  parameter values (see Fig. 3). The solid curve corresponds to the theoretical expression obtained  by linearizing the channel kinetics. (A) Effect of increasing the sodium channel density by a factor  (compared to the nominal value) of 2 (pluses), 3 (asterisks) and 4 (squares) on the magnitude of  voltage noise. (B) Effect of increasing both the sodium and potassium channel densities by a factor  of two (pluses).  4 Effect of Varying Channel Densities  Fig. 5 shows the voltage noise for a 1000/m 2 patch as a function of the holding voltage  for different values of the channel densities. Noise increases as the membrane is depolar-  ized from rest towards -50 mV and the rate of increase is higher for higher Na+densities.  The range of Vaotd for sub-threshold behavior extends up to -20 mV for nominal densities,  148 A. Manwani, P N. Steinmetz and C. Koch  but does not exceed -60 mV for higher Na+densities. For moderate levels of depolariza-  tion, an increase in the magnitude of the ionic current noise with voltage is the dominant  factor which leads to an increase in voltage noise; for higher voltages phenomenological  impedances are large and shunt away the current noise. Increasing Na+density increases  voltage noise, whereas, increasing K+density causes a decrease in noise magnitude (com-  pare Fig. 5A and 5B). We linearized closed-form expressions provide accurate estimates  of the noise magnitudes when the noise is small (of the order 3 mV).  5 Effect of Varying Temperature  Fig. 6 shows that voltage noise decreases with  temperature. To model the effect of temperature,  transition rates were scaled by a factor Qr/io  (Q10 = 2.3 for K +, Q10 = 3 for Na+). Tem-  perature increases the rates of channel transitions  and thus the bandwidth of the ionic current noise  fluctuations. The magnitude of the current noise,  on the other hand, is independent of temperature.  Since the membrane acts as a low-pass RC fil-  ter (at moderately depolarized voltages, the phe-  nomenological inductances are small), increasing  the bandwidth of the noise results in lower volt-  age noise as the high frequency components are  filtered out.  6 Conclusions  1.5  1  020 25 35  T (Celsius)  Figure 6: av as a function of tem-  perature for a 1000 /m 2 patch with  MS kinetics (Vhotd = -60 mV). Circles  denote Monte-Carlo simulations, solid  curve denotes linearized approximation.  We studied sub-threshold voltage noise due to stochastic ion channel fluctuations in an iso-  potential membrane patch with Mainen-Sejnowski kinetics. For the MS kinetic scheme,  noise increases as the membrane is depolarized from rest, up to the point where the phe-  nomenological impedances due to the voltage- and time-dependence of the ion channels  become large and shunt away the noise. Increasing Na+channel density increases both the  magnitude of the noise and its rate of increase with membrane voltage. On the other hand,  increasing the rates of channel transitions by increasing temperature, leads to a decrease  in noise. It has previously been shown that neural excitability increases with Na+channel  density [17] and decreases with temperature [15]. Thus, our findings suggest that an in-  crease in membrane excitability is inevitably accompanied by an increase in the magnitude  of sub-threshold voltage noise fluctuations. The magnitude and the rapid increase of volt-  age noise with depolarization suggests that channel fluctuations can contribute significantly  to the variability in spike timing [4] and the stochastic nature of ion channels may have a  significant impact on information processing within individual neurons. It also potentially  argues against the conventional role of a neuron as integrator of synaptic inputs [ 18], as  the the slow depolarization associated with integration of small synaptic inputs would be  accompanied by noise, making the membrane voltage a very unreliable indicator of the  integrated inputs. We are actively investigating this issue more carefully.  When the magnitudes of the noise and the phenomenological impedances are small, the  non-linear kinetic schemes are well-modeled by their linearized approximations. We have  found this to be valid for other kinetic schemes as well [19]. These analytical approxi-  mations can be used to study noise in more sophisticated neuronal models incorporating  realistic dendritic geometries, where Monte-Carlo simulations may be too computationally  intensive to use.  Channel Noise in Excitable Neural Membranes 149  Acknowledgments  This work was funded by NSF, NIMH and the Sloan Center for Theoretical Neuroscience.  We thank our collaborators Michael London, Idan Segev and Yosef Yarom for their invalu-  able suggestions.  References  [1] DeFelice L.J. (1981). Introduction to Membrane Noise. Plenum Press: New York, New York.  [2] Strassberg A.F. & DeFelice L.J. (1993). Limitations of the Hodgkin-Huxley formalism: effect  of single channel kinetics on transmembrane voltage dynamics. Neural Computation, 5:843-  855.  [3] Chow C. & White J. (1996). Spontaneous action potentials due to channel fluctuations. Biophy.  J., 71:3013-3021.  [4] Schneidman E., Freedman B. & Segev I. (1998). Ion-channel stochasticity may be critical in  determining the reliability and precision of spike timing. Neural Computation, 10:1679-1703.  [5] DeFelice L.J. & Isaac A. (1992). Chaotic states in a random world. J. Stat. Phys., 70:339-352.  [6] White J.A., Budde T. & Kay A.R. (1995). A bifurcation analysis of neuronal subthreshold  oscillations. Biophy. J., 69:1203-1217.  [7] Manward A. & Koch C. (1998). Synaptic transmission: An information-theoretic perspective.  In: Jordan M., Kearns M.S. & Solla S.A., eds., Advances in Neural Information Processing  Systems 10. pp 201-207. MIT Press: Cambridge, Massachusetts.  [8] Manwani A. & Koch C. (1999). Detecting and estimating signals in noisy cable structures: I.  Neuronal noise sources. Neural Computation. In press.  [9] Manwani A. & Koch C. (1999). Detecting and estimating signals in noisy cable structures: II.  Information-theoretic analysis. Neural Computation. In press.  [10] Mainen Z.E & $ejnowski T.J. (1995). Reliability of spike timing in neocortical neurons. Sci-  ence, 268:1503-1506.  [11] Johnston D. & Wu S.M. (1995). Foundations of Cellular Neurophysiology. MIT Press: Cam-  bridge, Massachusetts.  [12] Hodgkin A.L. & Huxley A.F. (1952). A quantitative description of membrane current and its  application to conduction and excitation in nerve. J. Physiol. (London), 117:500-544.  [13] Skaugen E. & Wallce L. (1979). Firing behavior in a stochastic nerve membrane model based  upon the Hodgkin-Huxley equations. Acta Physiol. Scand., 107:343-363.  [14] Press W.H., Teukolsky S.A., Vetterling W.T. & Flannery B.P. (1992). Numerical Recipes in C:  The Art of Scientific Computing. Cambridge University Press, second edn.  [15] Mauro A., Conti F., Dodge E & Schor R. (1970). Subthreshold behavior and phenomenological  impedance of the squid giant axon. J. Gen. Physiol., 55:497-523.  [16] Koch C. (1984). Cable theory in neurons with active, linearized membranes. Biol. Cybern.,  50:15-33.  [17] Sabah N.H. & Leibovic K.N. (1972). The effect of membrane parameters on the properties of  the nerve impulse. Biophys. J., 12:1132-44.  [18] Shadlen M.N. & Newsome W.T. (1998). The variable discharge of cortical neurons: implica-  tions for connectivity, computation, and information coding. J. Neurosci., 18:3870-3896.  [19] P. N. Steinmetz A. Manwani M.L. & Koch C. (1999). Sub-threshold voltage noise due to  channel fluctuations in active neuronal membranes. In preparation.  
An Analysis of Turbo Decoding  with Gaussian Densities  Paat Rusmevichientong and Benjamin Van Roy  Stanford University  Stanford, CA 94305  ( paatrus, bvr) stanf ord. edu  Abstract  We provide an analysis of the turbo decoding algorithm (TDA)  in a setting involving Gaussian densities. In this context, we are  able to show that the algorithm converges and that - somewhat  surprisingly - though the density generated by the TDA may differ  significantly from the desired posterior density, the means of these  two densities coincide.  I Introduction  In many applications, the state of a system must be inferred from noisy observations.  Examples include digital communications, speech recognition, and control with in-  complete information. Unfortunately, problems of inference are often intractable,  and one must resort to approximation methods. One approximate inference method  that has recently generated spectacular success in certain coding applications is the  turbo decoding algorithm [1, 2], which bears a close resemblance to message-passing  algorithms developed in the coding community a few decades ago [4]. It has been  shown that the TDA is also related to well-understood exact inference algorithms  [5, 6], but its performance on the intractable problems to which it is applied has  not been explained through this connection.  Several other papers have further developed an understanding of the turbo decoding  algorithm. The exact inference algorithms to which turbo decoding has been related  are variants of belief propagation [7]. However, this algorithm is designed for in-  ference problems for which graphical models describing conditional independencies  form trees, whereas graphical models associated with turbo decoding possess many  loops. To understand the behavior of belief propagation in the presence of loops,  Weiss has analyzed the algorithm for cases where only a single loop is present Ill].  Other analyses that have shed significant light on the performance of the TDA in  its original coding context include [8, 9, 10].  In this paper, we develop a new line of analysis for a restrictive setting in which un-  derlying distributions are Gaussian. In this context, inference problems are tractable  and the use of approximation algorithms such as the TDA are unnecessary. How-  ever, studying the TDA in this context enables a streamlined analysis that generates  new insights into its behavior. In particular, we will show that the algorithm con-  verges and that the mean of the resulting distribution coincides with that of the  576 P Rusmevichientong and B. V. Roy  desired posterior distribution.  While preparing this paper, we became aware of two related initiatives, both in-  volving analysis of belief propagation when priors are Gaussian and graphs possess  cycles. Weiss and Freeman [12] were studying the case of graphs possessing only  cliques of size two. Here, they were able to show that, if belief propagation con-  verges, the mean of the resulting approximation coincides with that of the true  posterior distribution. At the same time, Frey [3] studied a case involving graphical  structures that generalize those employed in turbo decoding. He also conducted an  empirical study.  The paper is organized as follows. In Section 2, we provide our working definition  of the TDA. In Section 3, we analyze the case of Gaussian densities. Finally, a  discussion of experimental results and open issues is presented in Section 4.  2 A Definition of Turbo Decoding  Consider a random variable x taking on values in n distributed according to a  density P0. Let Yl and Y2 be two random variables that are conditionally indepen-  dent given x. For example, yl and y2 might represent outcomes of two independent  transmissions of the signal x over a noisy communication channel. If y and y2 are  observed, then one might want to infer a posterior density f for x conditioned on  y and y2. This can be obtained by first computing densities p and p, where the  first is conditioned on y and the second is conditioned on y2. Then,  where ct is a "normalizing operator" defined by  g  .g _-- f  and multiplication/division are carried out pointwise.  Unfortunately, the problem of computing f is generally intractable. The computa-  tional burden associated with storing and manipulating high-dimensional densities  appears to be the primary obstacle. This motivates the idea of limiting attention  to densities that factor. In this context, it is convenient to define an operator r  that generates a density that factors while possessing the same marginals as another  density. In particular, this operator is defined by  n  = H f ^  i=1 ' [ i=ai )  for all densities g and all a  n, where d A di = dl." di-ldi+l" 'dn.  One may then aim at computing f  a proxy for f. Unfortunately, even this  problem is generally intractable. The TDA can be viewed  an iterative algorithm  for approximating f.  Let operators F and F2 be defined by  k p0/  and  An Analysis of Turbo Decoding with Gaussian Densities 577  for any density g. The TDA is applicable in cases where computation of these two  operations is tractable. The algorithm generates sequences q?) and q?) according  to  q?+l)= Flq?) and  initialized with densities q0) and q? that factor. The hope is that a(q?)q?)/po)  converges to an approximation of rf.  3 The Gaussian Case  We will consider a setting in which joint density of x, Yl, and y2, is Gaussian. In this  context, application of the TDA is not warranted - there are tractable algorithms for  computing conditional densities when priors are Gaussian. Our objective, however,  is to provide a setting in which the TDA can be analyzed and new insights can be  generated.  Before proceeding, let us define some notation that will facilitate our exposition.  We will write g - N(g, Eg) to denote a Gaussian density g whose mean vector e/nd  covariance matrix are  and E, respectively. For any matrix A, 5(A) will denote  a diagonal matrix whose entries are given by the diagonal elements of A. For any  diagonal matrices X and Y, we write X _< Y if Xii _< Y/i for all i. For any pair of  nonsingular covariance matrices Eu and E,such that y,l+ EI _ I is nonsingular,  let a matrix A,r,v be defined by  Ar,,r,v  (y,l + EI _ i)-1.  To reduce notation, we will sometimes denote this matrix by Au,  When the random variables x, Yl, and y: are jointly Gaussian, the densities p,  f, and p0 are also Gaussian. We let  and assume that both E1 and E: are symmetric positive definite matrices. We will  also assume that P0 " N(0, I) where I is the identity matrix. It is easy to show  that Ar,,r, is well-defined.  The following lemma provides formulas for the means and covariances that arise  from multiplying and rescaling Gaussian densities. The result follows from simple  algebra, and we state it without proof.  Lemma 1 Let u  N(, E,,) and v ,, N(, E), where E,, and E are positive  definite. If E 1 + E 1 - I is positive definite then  One immediate consequence of this lemma is an expression for the mean of f:  -1   = Az,z (E{11 + E 2).  Let ,5 denote the set of covariance matrices that are diagonal and positive definite.  Let  denote the set of Gaussian densities with covariance matrices in $. We then  have the following result, which we state without proof.  Lemma 2 The set  is closed under F1 and F.  If the TDA is initialized with q0), q(0)  G, this lemma allows us to represent all  iterates using appropriate mean vectors and covariance matrices.  578 P Rusmevichientong and B. V. Roy  3.1 Convergence Analysis  Under suitable technical conditions, it can be shown that the sequence of mean  vectors and covariance matrices generates by the TDA converges. Due to space  limitations, we will only present results pertinent to the convergence of covariance  matrices. Furthermore, we will only present certain central components of the  analyses. For more complete results and detailed analyses, we refer the reader to  our upcoming full-length paper.  Recall that the TDA generates sequences q?) and q?) according to  q(+)= Fq? ) and q?+)- F2q? )  I m   As discussed earlier, if the algorithm is initialized with elements of G, by Lemma 2,  for appropriate sequences of mean vectors and covariance matrices. It turns out  that there are mappings : 3 + 3 and : 3 -+ 3 such that   = T E(2 and = ,  for all k. Let T = T o T. To establish convergence of E? ) and E? ), it suffices to  show that Tn(? )) converges. The following theorem establishes this and further  points out that the limit does not depend on the initial iterates.  Theorem 1 There exists a matrix V*  3 such that  lim Tn(V) = V*,  n-}o<)  for all V  3.  3.1.1 Preliminary Lemmas  Our proof of Theorem 1 relies on a few lemmas that we will present in this section.  We begin with a lemma that captures important abstract properties of the function  7-. Due to space constraints, we omit the proof, even though it is nontrivial.  Lemma 3  (a) There exists a matrix D  3 such that for all D  3, D < T(D) < I.  (b) For all X, Y  3, if X < Y then T(X) < T(Y).  (c) The function T is continuous on 3.  (d) For all   (0, 1) and D  3, (1 + a)T (D) < T (D) for some a > O.  The following lemma establishes convergence when the sequence of covariance ma-  trices is initialized with the identity matrix.  Lemma 4 The sequence T n (I) converges in 3 to a fixed point of T.  Proof: By Lemma 3(a), 7-(1) <_ I, and it follows from monotonicity of 7- (Lemma  3(b)) that <_ 7-n(i) for all n. Since 7-"(I) is bounded below by a matrix  D  3, the sequence converges in 3. The fact that the limit is a fixed point of 7-  follows from the continuity of 7- (Lemma 3(c)).   Let V* = lim__. T  (I). This matrix plays the following special role.  Lemma 5 The matrix V* is the unique fixed point in 3 of T.  An Analysis of Turbo Decoding with Gaussian Densities 579  Proof: Because T n (I) converges to V* and T is monotonic, no matrix V  $ with  V  V* and V* _< V _< I can be a fixed point. Furthermore, by Lemma 3(a), no  matrix V  $ with V _> I and V  I can be a fixed point. For any V  $ with  V < V*, let  = _< v}.  For any V  $with V  V* and V < V*, we havev < 1. For such aV, by  Lemma 3(d), there is an a > 0 such that T(/vV*) > (v + a)V*, and therefore  T(V) : V. The result follows.   3.1.2 Proof of Theorem 1  Proof: For V  $ with V* _< V _< I convergence to V* follows from Lemma 4 and  monotonicity (Lemma 3(b)). For V  $ with V _> I, convergence follows from the  fact that V* _< T(V) < I, which is a consequence of the two previously invoked  lemmas together with Lemma 3(a).  Let us now address the case ofV  $ with V < V*. Let /v be defined as in  the proof of Lemma 5. Then, vV* < T(/vV*). By monotonicity, Tn(vV *) <  Tn+(/vV*) < V* for all n. It follows that Tn(vV *) converges, and since T  is continuous, the limit must be the unique fixed point V*. We have established  convergence for elements V of $ satisfying V _< V* or V _> V*. For other elements  of $, convergence follows from the monotonicity of T.   3.2 Analysis of the Fixed Point  As discussed in the previous section, under suitable conditions, F o F2 and F2 o F  each possess a unique fixed point, and the TDA converges on these fixed points.  Let q ,,, N(pq;, Eq;) and q ,,, N(pq, Eq; ) denote the fixed points of F o F2 and  F2 o F, respectively. Based on Theorem I, Eq; and Eq are in $.  The following lemma provides an equation relating means associated with the fixed  points. It is not hard to show that Aq.. Ar,,r,q; and Ar, q;,r,2, which are used in  l-t2 ' ,  the statement, are well-defined.  Lemma 6  Proof: It follows from the definitions of F and F2 that, if ql -- Fq and q2 2q,  * * * * * *  q q2 P q2 q P2  P0 P0 P0  The result then follows from Lemma i and the fact that r does not alter the mean  of a distribution.   We now prove a central result of this paper: the mean of the density generated by  the TDA coincides with the mean p of the desired posterior density f.  Theorem 2 a(qq/po) N(p, Aq;q)  -1  Proof: By Lemma 1, p = Ar,r (Ep + E p2), while the mean of (qq/Po)  + We wll sow expressions equl.  580 P Rusmevichientong and B. V. Roy  Figure 1' Evolution of errors.  Multiplying the equations from Lemma 6 by appropriate matrices, we obtain  Aqq; A,Eq Aq;q (-,;lq  ;lq )  Aq;q (-11 -[- ;lq ),  and  It follows that  and therefore  Note that A,zq + A - -  Eq;,E2 -- Aq;lq - A,E2' It follows that  -1  --1  = Aq*,,* (ET1/Zl + E 2  1 '2 '  -1  = E/ + E2  4 Discussion and Experimental Results  The limits of convergence q and q of the TDA provide an approximation  a(qq/po) to rf. We have established that the mean of this approximation coin-  cides with that of the desired density. One might further expect that the covariance  matrix of a(q q/po) approximates that of r f, and even more so, that q and q bear  some relation to p and p. Unfortunately, as will be illustrated by experimental  results in this section, such expectations appear to be inaccurate.  We performed experiments involving 20 and 50 dimensional Gaussian densities (i.e.,  x was either 20 or 50 dimensional in each instance). Problem instances were sampled  randomly from a fixed distribution. Due to space limitations, we will not describe  the tedious details of the sampling mechanism.  Figure i illustrates the evolution of certain "errors" during representative runs of  the TDA on 20-dimensional problems. The first graph plots relative errors in means  of densities a(q?)()p0 generated by iterates of the TDA. As indicated by our  (/2 / )  analysis, these errors converge to zero. The second chart plots a measure of relative  error for the covariance of a(q )q?)/P0) versus that of rf for representative runs.  Though these covariances converge, the ultimate errors are far from zero. The two  An Analysis of Turbo Decoding with Gaussian Densities 581  Figure 2: Errors after 50 iterations.  final graphs plot errors between the means of q?*) and q?) and those of p and p,  respectively. Again, though these means converge, the ultimate errors can be large.  Figure 2 provides plots of the same sorts of errors measured on 1000 different in-  stances of 50-dimensional problems after the 50th iteration of the TDA. The hori-  zontal axes are labeled with indices of the problem instances. Note that the errors  in the first graph are all close to zero (the units on the vertical axis must be multi-  plied by l0 -5 and errors are measured in relative terms). On the other hand, errors  in the other graphs vary dramatically.  It is intriguing that - at least in the context of Gaussian densities - the TDA can ef-  fectively compute conditional means without accurately approximating conditional  densities. It is also interesting to note that, in the context of communications, the  objective is to choose a code word  that is comes close to the transmitted code x.  One natural way to do this involves assigning to  the code word that maximizes  the conditional density f, i.e., the one that has the highest chance of being correct.  In the Gaussian case that we have studied, this corresponds to the mean of f - a  quantity that is computed correctly by the TDA! It will be interesting to explore  generalizations of the line of analysis presented in this paper to other classes of  densities.  References  [1] S. Benedetto and G. Montorsi, "Unveiling turbo codes: Some results on parallel concatenated coding  schemes," in IEEE Trans. Inform. Theory, vol. 42, pp. 409-428, Mar. 1996.  [2] G. Berrou, A. Glavieux, and P. Thitimajshima, "Near Shannon limit error-correcting coding: urbo codes,"  in Pvoc. 1993 Int. Conf. Corafaun., Geneva, Switzerland, May 1993, pp. 1064-1070.  [3] B. Frey, "urbo Factor Analysis." To appear in Advances in Neural Information Processing $ysteras 1.  [4] R. G. Callaget, Low-Density Parity-Check Codes. Cambridge, MA: MIT Press, 1963.  [5] F. R. Kschischang and B. J. Frey, "Iterative Decoding of Compound Codes by Probability Propagation in  Graphical Models," in IEEE Journal on Selected Areas in Corafaun., vol. 16, 2, pp. 219-230, Feb. 1998.  [6] R. J. McEliece, D. J. C. MacKay, and J-F. Cheng, "Turbo Decoding as an Instance of Pearl's "Belief  Propagation" Algorithm," in IEEE Journal on Selected Areas in Coramun., vol. 16, 2, pp. 140-152, Feb.  1998.  [7] J. Pearl, Probab*listic Reasoning in Intelligent Systems: Networks of Plausible Inference. San Mateo, CA:  Morgan Kaufmann, 1988.  [8] T. Richardson, "The Geometry of Turbo-Decoding Dynamics," Dec. 1998. To appear in IEEE Trans. Infowra.  Theory.  [9] T. Richardson and R. Urbanke, "The Capacity of Low-Density Parity Check Codes under Message-Passing  Decoding", submitted to the IEEE Trans. on Information Theory.  [10] T. Richardson, A. Shokrollahi, and R. Urbanke, "Design of Provably Good Low-Density Parity Check  Codes," submitted to the IEEE Trans. on Information Theory.  [11] . Weiss, "Belief Propagation and Revision in Networks with Loops," November 1997. Available by ftp to  publications.ai.mit.edu.  [12] . Weiss and W. T. Freeman, "Correctness of belief propagation in Gaussian graphical models of arbitrary  topology." To appear in Advances *n Neural Information Processing Systems 1.  
Broadband Direction-Of-Arrival Estimation  Based On Second Order Statistics  Justinian Rosca Joseph 6 Ruanaidh Alexander Jourjine Scott Rickard  {rosca, oruanaidh, j ourj ine, rickard}Oscr. siemens. com  Siemens Corporate Research, Inc.  755 College Rd E  Princeton, NJ 08540  Abstract  N wideband sources recorded using N closely spaced receivers can  feasibly be separated based only on second order statistics when using  a physical model of the mixing process. In this case we show that the  parameter estimation problem can be essentially reduced to considering  directions of arrival and attenuations of each signal. The paper presents  two demixing methods operating in the time and frequency domain and  experimentally shows that it is always possible to demix signals arriving at  different angles. Moreover, one can use spatial cues to solve the channel  selection problem and a post-processing Wiener filter to ameliorate the  artifacts caused by demixing.  1 Introduction  Blind source separation (BSS) is capable of dramatic results when used to separate mixtures  of independent signals. The method relies on simultaneous recordings of signals from two  or more input sensors and separates the original sources purely on the basis of statistical  independence between them. Unfortunately, BSS literature is primarily concerned with the  idealistic instantaneous mixing model.  In this paper, we formulate a low dimensional and fast solution to the problem of separating  two signals from a mixture recorded using two closely spaced receivers. Using a physical  model of the mixing process reduces the complexity of the model and allows one to identify  and to invert the mixing process using second order statistics only.  We describe the theoretical basis of the new approach, and then focus on two algorithms,  which were implemented and successfully applied to extensive sets of real-world data. In  essence, our separation architecture is a system of adaptive directional receivers designed  using the principles of BSS. The method bears resemblance to methods in beamforming [8]  in that it works by spatial filtering. Array processing techniques [2] reduce noise by  separating signal space from noise space, which necessitates more receivers than emitters.  The main differences are that standard beamforming and array processing techniques [8,  2] are generally strictly concerned with processing directional narrowband signals. The  difference with BSS [7, 6] is that our approach is model-based and therefore the elements  of the mixing matrix are highly constrained: a feature that aids in the robust and reliable  identification of the mixing process.  776 J. Rosca, J.  Ruanaidh, A. Jourjine and S. Rickard  The layout of the paper is as follows. Sections 2 and 3 describe the theoretical foundation of  the separation method that was pursued. Section 4 presents algorithms that were developed  and experimental results. Finally we summarize and conclude this work.  2 Theoretical foundation for the BSS solution  As a first approximation to the general multi-path model, we use the delay-mixing model.  In this model, only direct path signal components are considered. Signal components from  one source arrive with a fractional delay between the time of arrivals at two receivers. By  fractional delays, we mean that delays between receivers are not generally integer multiples  of the sampling period. The delay depends on the position of the source with respect  to the receiver axis and the distance between receivers. Our BSS algorithms demix by  compensating for the fractional delays. This, in effect, is a form of adaptive beamforming  with directional notches being placed in the direction of sources of interference [8]. A more  detailed account of the analytical structure of the solutions can be found in [1].  Below we address the case of two inputs and two outputs but there is no reason why the  discussion cannot be generalized to multiple inputs and multiple outputs. Assume a linear  mixture of two sources, where source amplitude drops off in proportion to distance:  sl(t- )+ s2(t ) (1)  a:i(t)-- til c i2 c  j = 1,2, where c is the speed of wave propagation, and trij indicates the distance from  receiver i to source j. This describes signal propagation through a uniform non-dispersive  medium. In the Fourier domain, Equation 1 results in a mixing matrix A(w) given by:  c Rl2 '-  A((.I.])--' 111  ,]O2 RII  121 e . O2 JR21 _1 e__jOj 22 (2)  c R22 c  It is important to note that the columns can be scaled arbitrarily without affecting separation  of sources because rescaling is absorbed into the sources. This implies that row scaling in  the demixing matrix (the inverse of A(w)) is arbitrary.  Using the Cosine Rule, tij can be expressed in terms of the distance/j of source j to  the midpoint between two receivers, the direction of arrival of source j, and the distance  between receivers, d, as follows:  1  Ris = + + i & cosOs (3)  Expanding the right term above using the binomial expansion and preserving only zeroth  and first order terms, we can express distance from the receivers to the sources as:  RiS--(RS+8-----j)+(-1)i()cosOj (4)  This approximation is valid within a 5% relative error when d _< __z. With the substitution  for RiS and with the redefinition of source j to include the delay due to the term within  brackets in Equation 4 divided by c, Equation 1 becomes:  x(t) = t +(-1 . .cos0j ,i= 1,2  $  In the Fourier domain, equation 5 results in the simplification to the mixing matrix A(w):  ['' ' ]  'eS'*' n= 'd'*2 (6)  Broadband DOA Estimation Based on Second Order Statistics 777  Here phases are functions of the directions of arrival 0j (defined with respect to the midpoint  between receivers), the distance between receivers d, and the speed of propagation c:  6i =  cos Oi, i = 1,2. Rij are unknown, but we can again redefine sources so diagonal  elements are unity:  -jwSl C1.-jw62 ]  A(od) - c2.eJW6l ejw6 2  (7)  where c2, c2 are two positive real numbers. In wireless communications sources are  typically distant compared to antenna distance. For distant sources and a well matched pair  of receivers Cl m c2 m 1. Equation 7 describes the mixing matrix for the delay model in  the frequency domain, in terms of four parameters, 62,62, c2, c2.  The corresponding ideal demixing matrix W(w), for each frequency w, is given by:  W(w) = [A(w)] -2  1 [e/w62 -c2.e-J w*2 ] (8)  det A(w) __C 2.jw61 -jw61  (9)  _Cl-Jw52  The outputs, estimating the sources, are:  Z2(0d) X2(0d) : det A(oa) --C2 jc61  Making the transition back to the time domain results in the following estimate of the  outputs:  Zl(t) ---- h(t 61 62, Cl,C2) Q _C2Xl( t_3_61)_3 _2(t_61)  z2(t) ' '  where  is convolution, and  (10)  1 / eJ,OtH(co, 52,52, Cl, c2)dod  h(t, 52,52, Cl, C2) -- 2'  1 1  H(w, 52,52, c2, c2) = det A(w) - eJ('2-1) - c2 c2 e -j(*2-*l)  (11)  Formulae 9 and 10 form the basis for two algorithms to be described next, in the time  domain and the frequency domains. The algorithms have the role of determining the four  unknown parameters. Note that the filter corresponding to H(w, 62,62, Cl, c2) should be  applied to the output estimates in order to map back to the original inputs.  3 Delay and attenuation compensation algorithms  The estimation of the four unknown parameters 62, 62, Cl, C2 Can be carried out based on  second order criteria that impose the constraint that outputs are decorrelated ([9, 4, 6, 5]).  3.1 Time and frequency domain approaches  The time domain algorithm is based on the idea of imposing the decorrelation constraint  (z2 (t), z2(t)) = 0 between the estimates of the outputs, as a function of the delays D2 and  D2 and scalar coefficients Cl and c2. This is equivalent to the following criterion:  { ]51, ]52, 6, 2 } -- argmin{F(D2,32, Cl, C2))  (12)  where F(.) measures the cross-correlations between the signals given below, representing  filtered versions of the differences of fractionally delayed measurements:  778 J. Rosca, J. ( Ruanaidh, A. Jourjine and S. Rickard  Zl(t ) --' h(t, D1,32, Cl, c2) () (Xl(t --[- 32) -  z2(t) = h(t, Dl, D2, cl, c2)  (c2xi(t + D2) - x2(t))  F(D1,D2, Cl,C2)-- (zi(t),z2(t))  (13)  In the frequency domain, the cross-correlation of the inputs is expressed as follows:  R x (a) = A(a)R s (w)A H (w) (14)  The mixing matrix in the frequency domain has the form given in Equation 7. Inverting  this cross correlation equation yields four equations that are written in matrix form as:  R s(w) = A-'(w)R X (w)A-" (w) (15)  Source orthogonality implies that the off-diagonal terms in the covariance matrix must be  zero:  /f2 (CO)'-'0 (16)  R() = 0  For far field conditions (i.e. the distance between the receivers is much less than the distance  from sources) one obtains the following equations:  a 1 x  b _a x c12  /rl(W ) = Cl--/IX1 (W)- C2/22(W) --abR(w) R(w) = 0  a ;  The terms a = e-J*' and b -- e -j*2 are functions of the time delays. Note that there is  a pair of equations of this kind for each frequency. In practice, the unknowns should be  estimated from data at all available frequencies to obtain a robust estimate.  3.2 Channel selection  Up to this point, there was no guarantee that estimated parameters would ensure source  separation in some specific order. We could not decide a priori whether estimated parameters  for the first output channel correspond to the first or second source. However, the dependence  of the phase delays on the angles of arrival suggests a way to break the permutation symmetry  in source estimation, that is to decide precisely which estimate to present on the first channel  (and henceforth on the second channel as well).  The core idea is that directionality and spatial cues provide the information required to  break the symmetry. The criterion we use is to sort sources in order of increasing delay.  Note that the correspondence between delays and sources is unique when sources are not  symmetrical with respect to the receiver axis. When sources are symmetric there is no way  of distinguishing between their positions because the cosine of the angles of arrival, and  hence the delay, is invariant to the sign of the angle.  4 Experimental results  A robust implementation of criterion 12 averages cross-correlations over a number of  windows, of given size. More precisely F is defined as follows:   I(z,(t), z2(t)>l q (18)  Blocks  Broadband DOA Estimation Based on Second Order Statistics 779  Normally q = 1 to obtain a robust estimate. Ngo and Bhadkamkar [5] suggest a similar  criterion using q - 2 without making use of the determinant of the mixing matrix.  After taking into account all terms from Equation 18, including the determinant of the  mixing matrix A, we obtain the function to be used for parameter estimation in the frequency  domain:  I .a x b 1 q   R,,(w)- R2x2(w)-abR(w) - R(w) (19)  F(Sl, 52): {detA} 2 + q  where r/is a (Wiener Filter-like) constant that helps prevent singularities and q is normally  set to one.  Computing the separated sources using only time differences leads to highpass filtered  outputs. In order to implement exactly the theoretical demixing procedure presented one  has to divide by the determinant of the mixing matrix. Obviously one could filter using the  inverse of the determinant to obtain optimal results. This can be implemented in the form  of a Wiener filter. The Wiener filter requires knowledge both of the signal and noise power  spectral densities. This information is not available to us but a reasonable approximation is  to assume that the (wideband) sources have a flat spectral density and the noise corrupting  the mixtures is white. In this case, the Wiener Filter becomes:  ( {detA(w)}2 ) 1 (20)  H(w) = {d(-- 5  r/ det.(w)  where the parameter r/has been empirically set to the variance of the mixture. Applying  this choice of filter usually dramatically improves the quality of the separated outputs.  The technique of postprocessing using the determinant of the mixing matrix is perfectly  general and applies equally well to demixtures computed using matrices of FIR filters.  The quality of the result depends primarily on the care with which the inverse filter is  implemented. It also depends on the accuracy of the estimate for the mixing parameters.  One should avoid using the Wiener filter for near-degenerate mixtures.  The proof of concept for the theory outlined above was obtained using speech signals which  if anything pose a greater challenge to separation algorithms because of the correlation  structure of speech. Two kinds of data are considered in this paper: synthetic direct  propagation delay data and synthetic multi-path data. Data can be characterized along  two dimensions of difficulty: synthetic vs. real-world, and direct path vs. multi-path.  Combinations along these dimensions represented the main type of data we used.  The value of distance between receivers dictates the order of delays that can appear due  to direct path propagation, which is used by the demixing algorithms. Data was generated  synthetically employing fractional delays corresponding to the various positions of the  sources [3].  We modeled multi-path by taking into account the decay in signal amplitude due to propa-  gation distance as well as the absorption of waves. Only the direct path and one additional  path were considered.  The algorithms developed proved successful for separation of two voices from direct path  mixtures, even where the sources had very similar spectral power characteristics, and for  separation of one source for multi-path mixtures. Moreover, outputs were free from artifacts  and were obtained with modest computational requirements.  Figure 1 presents mean separation results of the first and second channels, which correspond  to the first and second sources, for various synthetic data sets. Separation depends on the  angles of arrival. Plots show no separation in the degenerate case of equal or closeby  angles of arrival, but more than 10dB mean separation in the anechoic case and 5dB in the  multi-path case.  780 d. Rosca, J.  Ruanaidh, A. Jourjine and S. Rickard  Figure 1: Two sources were positioned at a relatively large distance from a pair of closely  spaced receivers. The first source was always placed at zero degrees whilst the second  source was moved uniformly from 30 to 330 degrees in steps of 30 degrees. The above  shows mean separation and standard deviation error bars of first and second sources for six  synthetic delay mixtures or synthetic multi-path data mixtures using the time and frequency  domain algorithms.  5 Conclusions  The present source separation approach is based on minimization of cross-correlations of  the estimated sources, in the time or frequency domains, when using a delay model and  explicitly employing dirrection of arrival. The great advantage of this approach is that it  reduces source separation to a decorrelation problem, which is theoretically solved by a  system of equations. Although the delay model used generates essentially anechoic time  delay algorithms, the results of this work show systematic improvements even when the  algorithms are applied to real multi-path data. In all cases separation improvement is robust  with respect to the power ratios of sources.  Acknowledgments  We thank Radu Balan and Frans Coetzee for useful discussions and proofreading various  versions of this document and our collaborators within Siemens for providing extensive  data for testing.  Broadband DO,,1 Estimation Based on Second Order Statistics 781  References  [1 ] A. Jourjine, S. Rickard, J. 0 Ruanaidh, and J. Rosca. Demixing of anechoic time delay  mixtures using second order statistics. Technical Report SCR-99-TR-657, Siemens  Corporate Research, 755 College Road East, Princeton, New Jersey, 1999.  [2] Hamid Krim and Mats Viberg. Two decades of array signal processing research. IEEE  Signal Processing Magazine, 13(4), 1996.  [3] Tim Laakso, Vesa Valimaki, Matti Karjalainen, and Unto Laine. Splitting the unit delay.  IEEE Signal Processing Magazine, pages 30-60, 1996.  [4] L. Molgedey and H.G. Schuster. Separation of a mixture of independent signals using  time delayed correlations. Phys. Rev. Lett., 72(23):3634-3637, July 1994.  [5] T J. Ngo and N.A. Bhadkamkar. Adaptive blind separation of audio sources by a  physically compact device using second order statistics. In First International Workshop  on ICA and BSS, pages 257-260, Aussois, France, January 1999.  [6] Lucas Parra, Clay Spence, and Bert De Vries. Convolutive blind source separation  based on multiple decorrelation. In NNSP98, 1988.  [7] K. Torkolla. Blind separation for audio signals: Are we there yet? In First International  Workshop on Independent component analysis and blind source separation, pages 239-  244, Aussois, France, January 1999.  [8] V. Van Veen and Kevin M. Buckley. Beamforming: A versatile approach to spatial  filtering. IEEEASSP Magazine, 5(2), 1988.  [9] E. Weinstein, M. Feder, and A. Oppenheim. Multi-channel signal separation by decor-  relation. IEEE Trans. on Speech and Audio Processing, 1(4):405--413, 1993.  
A Winner-Take-All Circuit with  Controllable Soft Max Property  Shih-Chii Liu  Institute for Neuroinformatics, ETH/UNIZ  Winterthurstrasse 190, CH-8057 Zurich  Switzerland  shih@ini.phys.ethz.ch  Abstract  I describe a silicon network consisting of a group of excitatory neu-  rons and a global inhibitory neuron. The output of the inhibitory  neuron is normalized with respect to the input strengths. This out-  put models the normalization property of the wide-field direction-  selective cells in the fly visual system. This normalizing property is  also useful in any system where we wish the output signal to code  only the strength of the inputs, and not be dependent on the num-  ber of inputs. The circuitry in each neuron is equivalent to that in  Lazzaro's winner-take-all (WTA) circuit with one additional tran-  sistor and a voltage reference. Just as in Lazzaro's circuit, the  outputs of the excitatory neurons code the neuron with the largest  input. The difference here is that multiple winners can be chosen.  By varying the voltage reference of the neuron, the network can  transition between a soft-max behavior and a hard WTA behav-  ior. I show results from a fabricated chip of 20 neurons in a 1.2pm  CMOS technology.  1 Introduction  Lazzaro and colleagues (Lazzaro, 1988) were the first to implement a hardware  model of a winner-take-all (WTA) network. This network consists of N excitatory  cells that are inhibited by a global signal. Improvements of this network with ad-  dition of positive feedback and lateral connections have been described (Morris,  1998; Indiveri, 1998). The dynamics and stability properties of networks of cou-  pled excitatory and inhibitory neurons have been analyzed by many (Amari, 1982;  Grossberg, 1988). Grossberg described conditions under which these networks will  exhibit WTA behavior. Lazzaro's network computes a single winner as reflected by  the outputs of the excitatory cells. Several winners can be chosen by using more  localized inhibition.  In this work, I describe two variants of a similar architecture where the outputs of  the excitatory neurons code the relative input strengths as in a soft-max compu-  tation. The relative values of the outputs depend on the number of inputs, their  relative strengths and two parameter settings in the network. The global inhibitory  718 S.-C. Liu  ei_ 1 ei ei+ 1  Yi-1  wi.  Yi+l  Wi+l  Figure 1: Network model of recurrent inhibitory network.  signal can also be used as an output. This output saturates with increasing num-  ber of active inputs, and the saturation level depends on the input strengths and  parameter settings. This normalization property is similar to the normalization be-  havior of the wide-field direction-selective cells in the fly visual system. These cells  code the temporal frequency of the visual inputs and are largely independent of the  stimulation size. The circuitry in each neuron in the silicon network is equivalent  to that in Lazzaro et. al.'s hard WTA network with an additional transistor and  a voltage reference. By varying the voltage reference, the network can transition  between a soft-max computation and a hard WTA computation. In the two vari-  ants, the outputs of the excitatory neurons either code the strength of the inputs  or are normalized with respect to a constant bias current. Results from a fabri-  cated network of 20 neurons in a 1.2/zm AMI CMOS show the different regimes of  operation.  2 Network with Global Inhibition  The generic architecture of a recurrent network with excitatory neurons and a single  inhibitory neuron is shown in Figure 1. The excitatory neurons receive an external  input, and they synapse onto a global inhibitory neuron. The inhibitory neuron, in  turn, inhibits the excitatory neurons. The dynamics of the network is described as  follows:  N  dyi  dt = -Yi + ei - g(E wjyj) (1)  j=l  where wj is the weight of the synapse between the jth excitatory neuron and the  inhibitory neuron, and yj is the state of the jth neuron. Under steady-state condi-  N  tions, Yi = ei -- YT, where YT = g(j= wjyj).  Assume a linear relationship between YT and yj, and letting wj - w,  N -V= 1 ej  YT = w E YJ -- w  l+wN  j=l  As N increases, YT -- --1 e  N . If all inputs have the same level, e, then YT -- e.  A Yrinner-Take-All Circuit with Controllable Soft Max Property 719  Iol  VT  Vr2  Figure 2: First variant of the architecture. Here we show the circuit for two excita-  tory neurons and the global inhibition neuron, M4. The circuit in each excitatory  neuron consists of an input current source, I, and transistors, M to M3. The  inhibitory transistor is a fixed current source, Ib. The inputs to the inhibitory  transistor, Io and I;2 are normalized with respect to Ib.  3 First Variant of Network with Fixed Current Source  In Sections 3 and 4, I describe two variants of the architecture shown in Figure  1. The two variants differ in the way that the inhibition signal is generated. The  first network in Figure 2 shows the circuitry for two excitatory neurons and the  inhibition neuron. Each excitatory neuron is a linear threshold unit and consists of  an input current, I, and transistors, M, M2, and M3. The state of the neuron is  represented by the current, Ir. The diode-connected transistor, M, introduces a  rectifying nonlinearity into the system since It1 cannot be negative. The inhibition  current, IT, is sunk by M, and is determined by the gate voltage, VT. The inhibition  neuron consists of a current source, Ib, and VT is determined by the corresponding  current, I and the corresponding transistor, Ms in each neuron. Notice that IT  cannot be greater than the largest input to the network and the inputs to this  network can only be excitatory. The input currents into the transistor, M4, are  defined as Io and lo2 and are normalized with respect to the current source, Ib. In  the hard WTA condition, the output current of the winning neuron is equal to the  bias current, I,.  This network exhibits either a soft-maximum behavior or a hard WTA behavior  depending on the value of an external bias, Va. The inhibition current, IT, is  derived as:  INIi Nil  - - (2)  + + N  where N is the number of "active" excitatory neurons (that is, neurons whose  Ii > IT), Ii is the same input current to each neuron, and Is - Ioe v/r. In  deriving the above equation, we assumed that  - 1. The inhibition current, IT, is  a linear combination of the states of the neurons because IT = Y. Ii x I/I.  Figure 3(a) shows the response of the common-node voltage, VT, as a function of  the number of inputs for different input values measured from a fabricated silicon  network of 20 neurons. The input current to each neuron is provided by a pFET  transistor that is driven by the gate voltage, V/n. All input currents are equal in  this figure. The saturation behavior of the network as a function of the number  720 S.-C.  o. O.o%  Number of inpu Number of inputs  (a) (b)  Figure 3: (a) Common-node voltage, VT, as a function of the number of input  stimuli. Va = 0.8V. (b) Common-node voltage, VT, as a function of the number  of inputs with an input voltage of 4.3V and Vb - 0.7V. The curves correspond to  different values of Va.  of inputs can be seen in the different traces and the saturation level increases as  n decreases. As seen in Equation 2, the point at which the response saturates is  dependent on the ratio, It,/Ia. In Figure 3(b), I show how the curve saturates at  different points for different values of Va and a fixed Ib and V/n.  In Figure 4, I set all inputs to zero except for two inputs, n and Vin2 that are set  to the same value. I measured Io and Io as a function of Va as shown in Figure  4(a). The four curves correspond to four values of n. Initially both currents Io  and Io2 are equal as is expected in the soft-max condition. As Va increases, the  network starts exhibiting a WTA behavior. One of the output currents finally goes  to zero above a critical value of Va. This critical value increases for higher input  currents because of transistor backgate effects. In Figure 4(b), I show how the  output currents respond as a function of the differential voltage between the two  inputs as shown in Figure 4. Here, I fixed one input at 4.3V and swept the second  input differentially around it. The different curves correspond to different values of  Va. For a low value of Va, the linear differential input range is about 100mV. This  linear range decreases as Va is increased (corresponding to the WTA condition).  4  Second Variant with Diode-Connected Inhibition  Transistor  In the second variant shown in Figure 5, the current source, M4 is replaced by a  diode-connected transistor and the output currents, Ioi, follow the magnitude of  the input currents. The inhibition current, IT, can be expressed as follows:  I T : (IrilIoi) X Ia  (s)  where Ia is defined in Section 3. We sum Equation 3 over all neurons and assuming  equal inputs, we get IT = v/ Iri x Ia. This equation shows that the feedback  signal has a square root dependence on the neuron states. As we will see, this  causes the feedback signal to saturate quickly with the number of inputs.  A I4qnner-Take-All Circuit with Controllable Soft Max Property 721  Vin=4.3V 4 2V4 1 p  : ? 4'5/ l  0.5 0.6 0.7 0.8  Va (v)  2.5  10 -7  Va---O.7V  -0.2  ?;' - Va--O.4V  -0.1 0 0.1 0.2 0.3  Vin2-Vin 1 (V)  (a) (b)  Figure 4: (a) Output currents, Io and Io2, as a function of V*` for a subthreshold  bias current and n - 4.0V to 4.3V. (b) Outputs, Io and Io2, as a function of the  differential input voltage, AV/n, with n = 4.3V.  Irl I  _-  () I2  Ir2  Figure 5: Second variant of network. The schematic shows two excitatory neurons  with diode-connected inhibition transistor.  Substituting Iri = Ii - IT in Equation 3, we solve for IT,  IT = -I*`N + I  N  (I*`N) 2 + 41,, EIi (4)  i  From measurements from a fabricated circuit with 20 neurons, I show the depen-  dence of VT (the natural logarithm of IT) on the number of inputs in Figure 6(a).  The output saturates quickly with the number of inputs and the level of saturation  increases with increased input strengths. All the inputs have the same value.  The network can also act as a WTA by changing V*`. Again, all inputs are set to  zero except for two inputs whose gate voltages are both set at 4.2V. As shown in  Figure 6(b), the output currents, Io and Ion, are initially equal, and as V*` increases  above 0.6V, the output currents split apart and eventually, Io2 - 0A. The final value  of lol depends on the maximum input current. This data shows that the network  acts as a WTA circuit when V*` > 0.73V. If I set V/n2 - 4.25V instead, the output  currents split at a lower value of V*`.  722 S.-C. Liu  0.45  0.4  0.35  0.3  0.25  Vin=3.9V   ..... Vin=4.06V  ';/ Vin--4.3V  i/  //  0'20  z  i 1'0 12  Number of inputs  (a)  .-2  x 10 -?  Vinl=4.2V, Vin2=4.25V  ':  Vin l=Vin2.2V  Va (V)  Figure 6: (a) Common-node voltage, VT, as a function of the number of inputs for  input voltages, 3.9V, 4.06V, and 4.3V for V = 0.4V. (b) Outputs, Ioi and lo2, as  a function of V for hi = 4.2V, n2 - 4.25V for the 2 curves with asterisks and  for Vnl - he -- 4.2V for the 2 curves with circles.  5 Inhibition  The WTA property arises in both variants of this network if the gain parameter,  V, is increased so that the diode-connected transistor, M2, can be ignored. Both  variants then reduce to Lazzaro's network. In the first variant, the feedback current  (IT) is a linear combination of the neuron states. However, when the gain parameter  is increased so that Me can be ignored, the feedback current is now a nonlinear  combination of the input states so the WTA behavior is exhibited by these reduced  networks.  Under hard WTA conditions, if IT is initially smaller than all the input currents, the  capacitances C at the nodes Vrl and Vr2 are charged up by the difference between  the individual input current and IT, i.e., av. _ i-r. Since the inhibition current  dt -- C  is a linear combination of Iri and Iri is exponential in Vr, we can see that I is  a sum of the exponentials of the input currents, I. Hence the feedback current is  nonlinear in the input currents. Another way of viewing this condition in electronic  terms is that in the soft WTA condition, the output node of each neuron is a soft-  impedance node, or a low-gain node. In the hard WTA case, the output node is now  a high-impedance node or a high-gain node. Any input differences are immediately  amplified in the circuit.  6 Discussion  Hahnloser (Hahnloser, 1998) recently implemented a silicon network of linear thresh-  old excitatory neurons that are coupled to a global inhibitory neuron. The inhibitory  signal is a linear combination of the output states of the excitatory neurons. This  network does not exhibit WTA behavior unless the excitatory neurons include a  self-excitatory term. The inhibition current in his network is also generated via a  diode-connected transistor. The circuitry in two variants described here is more  compact than the circuitry in his network.  Recurrent networks with the architecture described in this paper have been proposed  by Reichardt and colleagues (Reichardt, 1983) in modelling the aggregation property  A Vqnner-Take-All Circuit with Controllable Soft Max Property 723  of the wide-field direction-selective cells in flies. The synaptic inputs are inhibited  by a wide-field cell that pools all the synaptic inputs. Similar networks have also  been used to model cortical processing, for example, orientation selectivity (Douglas,  1995).  The network implemented here can model the aggregation property of the direction-  selective cells in the fly. By varying a voltage reference, the network implements  either a soft-max computation or a hard WTA computation. This circuitry will be  useful in hardware models of cortical processing or motion processing in inverte-  brates.  Acknowledgments  I thank Rodney Douglas for supporting this work, and the MOSIS foundation for  fabricating this circuit. I also thank Tobias Delbrfick for proofreading this docu-  ment. This work was supported in part by the Swiss National Foundation Research  SPP grant and the U.S. Office of Naval Research.  References  Amari, S., and Arbib, M. A., "Competition and cooperation in neural networks,"  New York, Springer-Verlag, 1982.  Grossberg, W., "Nonlinear neural networks: Principles, mechanisms, and architec-  tures," Neural Networks, 1, 17-61, 1988.  Hanhloser, R., "About the piecewise analysis of networks of linear threshold neu-  rons," Neural Networks, 11,691-697, 1988.  Hahnloser, R., "Computation in recurrent networks of linear threshold neurons:  Theory, simulation and hardware implementation," Ph.D. Thesis, Swiss Federal  Institute of Technology, 1998.  Lazzaro, J., Ryckebusch, S. Mahowald, M.A., and Mead. C., "Winner-take-all net-  works of 0(n) complexity," In Tourestzky, D. (ed), Advances in Neural Information  Processing Systems 1, San Mateo, CA: Morgan Kaufman Publishers, pp. 703-711,  1988.  Morris, T.G., Horiuchi, T. and Deweerth, S.P., "Object-based selection within an  analog VLSI visual attention system," IEEE Trans. on Circuits and Systems II,  45:12, 1564-1572, 1998.  Indiveri, G., "Winner-take-all networks with lateral excitation," Neuromorphic Sys-  tems Engineering, Editor, Lande, TS., 367-380, Kluwer Academic, Norwell, MA,  1998.  Reichardt, W., Poggio, T., and Hausen, K., "Figure-ground discrimination by rel-  ative movement in the visual system of the fly," Biol. Cybern., 46, 1-30, 1983.  Douglas, RJ., Koch, C., Mahowald, M., Martin, KAC., and Suarez, HH., "Recurrent  excitation in neocortical circuits," Science, 269:5226, 981-985, 1995.  
Regular and Irregular Gallager-type  Error-Correcting Codes  Y. Kabashima and T. Murayama  Dept. of Compt. Intl. & Syst. Sci.  Tokyo Institute of Technology  Yokohama 2268502, Japan  D. Saad and R. Vicente  Neural Computing Research Group  Aston University  Birmingham B4 7ET, UK  Abstract  The performance of regular and irregular Gallager-type error-  correcting code is investigated via methods of statistical physics.  The transmitted codeword comprises products of the original mes-  sage bits selected by two randomly-constructed sparse matrices;  the number of non-zero row/column elements in these matrices  constitutes a family of codes. We show that Shannon's channel  capacity may be saturated in equilibrium for many of the regular  codes while slightly lower performance is obtained for others which  may be of higher practical relevance. Decoding aspects are con-  sidered by employing the TAP approach which is identical to the  commonly used belief-propagation-based decoding. We show that  irregular codes may saturate Shannon's capacity but with improved  dynamical properties.  I Introduction  The ever increasing information transmission in the modern world is based on re-  liably communicating messages through noisy transmission channels; these can be  telephone lines, deep space, magnetic storing media etc. Error-correcting codes play  a significant role in correcting errors incurred during transmission; this is carried out  by encoding the message prior to transmission and decoding the corrupted received  code-word for retrieving the original message.  In his ground breaking papers, Shannon[I] analyzed the capacity of communication  channels, setting an upper bound to the achievable noise-correction capability of  codes, given their code (or symbol) rate, constituted by the ratio between the num-  ber of bits in the original message and the transmitted code-word. Shannon's bound  is non-constructive and does not provide a recipe for devising optimal codes. The  quest for more efficient codes, in the hope of saturating the bound set by Shannon,  has been going on ever since, providing many useful but sub-optimal codes.  One family of codes, presented originally by Gallager[2], attracted significant inter-  est recently as it has been shown to outperform most currently used techniques[3].  Gallager-type codes are characterized by several parameters, the choice of which  defines a particular member of this family of codes. Current theoretical results[3]  Regular and Irregular Gallager-type Error-Correcting Codes 2 73  offer only bounds on the error probability of various architectures, proving the ex-  istence of very good codes under some restrictions; decoding issues are examined  via numerical simulations.  In this paper we analyze the typical performance of Gallager-type codes for several  parameter choices via methods of statistical mechanics. We then validate the an-  alytical solution by comparing the results to those obtained by the TAP approach  and via numerical methods.  2 The general framework  In a general scenario, a message represented by an N dimensional Boolean vector   is encoded to the M dimensional vector jo which is transmitted through a noisy  channel with some flipping probability p per bit (other noise types may also be  studied). The received message J is then decoded to retrieve the original message.  In this paper we analyze a slightly different version of Gallager-type codes termed  the MN code[3] that is based on choosing two randomly-selected sparse matrices A  and B of dimensionality M x N and M x M respectively; these are characterized  by K and L non-zero unit elements per row and C and L per column respectively.  The finite numbers K, C and L define a particular code; both matrices are known  to both sender and receiver. Encoding is carried out by constructing the modulo  2 inverse of B and the matrix B-1A (mod 2); the vector jo_ B-1A  (mod 2,   Boolean vector) constitutes the codeword. Decoding is carried out by taking the  product of the matrix B and the received message J- jo +4 (mod 2), corrupted  by the Boolean noise vector 4, resulting in A+B. The equation  A + B = AS + B' (mod 2) (1)  is solved via the iterative methods of Belief Propagation (BP)[3] to obtain the most  probable Boolean vectors $ and -; BP methods in the context of error-correcting  codes have recently been shown to be identical to a TAP[4] based solution of a  similar physical system[5].  The similarity between error-correcting codes of this type and Ising spin systems  was first pointed out by Sourlas[6], who formulated the mapping of a simpler code,  somewhat similar to the one presented here, onto an Ising spin system Hamiltonian.  We recently extended the work of Sourlas, that focused on extensively connected  systems, to the finite connectivity case[5] as well as to the case of MN codes [7].  To facilitate the current investigation we first map the problem to that of an Ising  model with finite connectivity. We employ the binary representation (:hl) of the  dynamical variables $ and - and of the vectors J and jo rather than the Boolean  (0, 1) one; the vector jo is generated by taking products of the relevant binary  message bits j0 __ I'Iie i, where the indices p - (il,...iK) correspond to the  non-zero elements of B-1A, producing a binary version of jo. As we use statistical  mechanics techniques, we consider the message and codeword dimensionality (N  and M respectively) to be infinite, keeping the ratio between them R - N/M,  which constitutes the code rate, finite. Using the thermodynamic limit is quite  natural as Gallager-type codes are usually used for transmitting long (10 4- 10 5)  messages, where finite size corrections are likely to be negligible. To explore the  system's capabilities we examine the Hamiltonian  274 Y. Kabashima, T. Murayama, D. Saad and R. Ficente  The tensor product 7, where 7 = I'Ie,  I'Ije,, j and a = (j,... j), is  the binary equivalent of A+B, treating both signal (S and index i) d noise  (w and index j) simultaneously. Elements of the sparse connectivity tensor  take the value I if the corresponding indices of both signal and noise are chosen  (i.e., if all corresponding indices of the matrices A and B are 1) d 0 otherwise;  it h C unit elements per /-index d L per j-index representing the system's  degree of connectivity. The 5 function provides I if the selected sites' product  ie Si je rj is in disagreement with the corresponding element , recording  an error, d 0 otherwise. Notice that this term is not frustrated,  there are  M+N degrees of freedom and only M constrnts from Eq.(1), and can therefore  vish at suciently low temperatures. The lt two terms on the right represent  our prior knowledge in the ce of sparse or bied messages F d of the noise  level F and require signing certain values to these additive fields. The choice of     imposes the restriction of Eq.(1), limiting the solutions to those for which  the first term of Eq.(2) vanishes, while the lt two terms, scaled with , survive.  Note that the noise dynic variables r are irrelevant to menuring the retrieval   ) The latter monitors the normized mean  success m =  E i sign<Si)   overlap between the Bayes-optim retrieved message, shown to correspond to the  alignment of <Si) to the nearest binary value[6], d the original message; the  subscript  denotes therm averaging.  Since the first part of Eq.(2) is invariant under the map Si  Sii, wj  wiQ and     ie i je Q = 1, it is useful to decouple the correlation between the  vectors S, w and , . Rewriting Eq.(2) one obtns a similar expression apart from  the lt terms on the right which become Fs/  Sn  and Fr/   .  The random selection of elements in  introduces disorder to the system which  is treated via methods of statistical physics. More specifically, we calculate the  partition function Z(, J) = {S,w} exp[-] averaged over the disorder and the  statistical properties of the message and noise, using the replica method[5, 8, 9].  Taking    gives rise to a set of order parameters  I 1  i=1 tm i=1 lm  (2)  where a, B, .. represem replica indices, and the variables Zi d  come from  enforcing the restriction of C and L connections per index respectively[5]:  5 - C = , (3)  and similarly for the restriction on the j indices.  To proceed with the calculation one has to make  sumption about the order  parameters symmetry. The sumption made here, and validated later on, is that  of replica symmetry in the following representation of the order parameters and the  related conjugate variables  q,a.., = aq / aX x t = a?/  ,  (4)  where 1 is the number of replica indices, a. are normalization coefficients, and  (x), (:), p(y) and () represent probability distributions. Unspecified integrals  Regular and Irregular Gallager-type Error-Correcting Codes 2 75  are over the range [-1, +1]. One then obtains an expression for the free energy  per spin expressed in terms of these probability distributions 1/N (ln Z,,v The  free energy can then be calculated via the saddle point method. Solving the  equations obtained by varying the free energy w.r.t the probability distributions  '(x), (), p(y) and (9), is difficult as they generally comprise both delta peaks  and regular[9] solutions for the ferromagnetic and paramagnetic phases (there is no  spin-glass solution here as the system is not frustrated). The solutions obtained  in the case of unbiased messages (the most interesting case as most messages are  compressed prior to transmission) are for the ferromagnetic phase:  '(x) = 5(x- 1) , () - 5( - 1) , p(y) - 5(y - 1) , (9) = 5(9 - 1) , (5)  and for the paramagnetic phase:  (x) = 5(x), ()=5(), (9)=5(9)  1 + tanh F 1 - tanh F  P(Y) - 2 5(y - tanh F) + 2 5(y + tanh F) . (6)  These solutions obey the saddle point equations. However, it is unclear whether  the contribution of other delta peaks or of an additional continuous solution will be  significant and whether the solutions (5) and (6) are stable or not. In addition, it  is also necessary to validate the replica symmetric ansatz itself. To address these  questions we obtained solutions to the system described by the Hamiltonian (2) via  TAP methods of finitely connected systems[5]; we solved the saddle point equations  derived from the free energy numerically, representing all probability distributions  by up to 10 4 bin models and by carrying out the integrations via Monte-Carlo  methods; finally, to show the consistency between theory and practice we carried  out large scale simulations for several cases, which will be presented elsewhere.  3 Structure of the solutions  The various methods indicate that the solutions may be divided to two different  categories: K- L- 2 and either K _> 3 or L _> 3. We therefore treat them separately.  For unbiased messages and either K _> 3 or L _> 3 we obtain the solutions (5) and  (6) both by applying the TAP approach and by solving the saddle point equations  numerically. The former was carried out at the value of F which corresponds to  the true noise and input bias levels (for unbiased messages Fs = 0) and thus to  Nishimori's condition[10], where no replica symmetry breaking effects are expected.  This is equivalent to having the correct prior within the Bayesian framework[6] and  enables one to obtain analytic expressions for some observables as long as some  gauge requirements are obeyed[10]. Numerical solutions show the emergence of  stable dominant delta peaks, consistent with those of (5) and (6). The question  of longitudinal mode stability (corresponding to the replica symmetric solution)  was addressed by setting initial conditions for the numerical solutions close to the  solutions (5) and (6), showing that they converge back to these solutions which are  therefore stable.  The most interesting quantity to examine is the maximal code rate, for a given  corruption process, for which messages can be perfectly retrieved. This is defined  in the case of K, L >_ 3 by the value of R- K/C = N/M for which the free energy of  the ferromagnetic solution becomes smaller than that of the paramagnetic solution,  constituting a first order phase transition. A schematic description of the solutions  obtained is shown in the inset of Fig.la. The paramagnetic solution (m - 0) has  a lower free energy than the ferromagnetic one (low/high free energies are denoted  276 Y. Kabashima, T. Murayama, D. $aad and R. Vicente  by the thick and thin lines respectively, there are no axis lines at m- 0, 1) for  noise levels p > Pc and vice versa for p_< Pc; both solutions are stable. The critical  code rate is derived by equating the ferromagnetic and paramagnetic free energies  to obtain Rc = 1-H2(p) - l+(plog2p+(1 -p)log(1 -p)) This coincides with  Shannon's capacity. To validate these results we obtained TAP solutions for the  unbiased message case (K=L=3, C=6) as shown in Fig. la (as +) in comparison  to Shannon's capacity (solid line).  Analytical solutions for the saddle point equations cannot be obtained for biased  patterns and we therefore resort to numerical methods arid the TAP approach. The  maximal information rate (i.e., code-rate xH(f = (1 + tanhFs)/2) - the source  redundancy) obtained by the TAP method (O) and numerical solutions of the saddle  point equations ([3), for each noise level, are shown in Fig. la. Numerical results  have been obtained using 10 3-10 4 bin models for each probability distribution and  had been run for 10 5 steps per noise level point. The various results are highly  consistent and practically saturate Shannon's bound for the same noise level.  The MN code for K, L > 3 seems to offer optimal performance. However, the main  drawback is rooted in the co-existence of the stable m = 1 and m - 0 solutions,  shown in Fig.la (inset), which implies that from some initial conditions the system  will converge to the undesired paramagnetic solution. Moreover, studying the fer-  romagnetic solution numerically shows a highly limited basin of attraction, which  becomes smaller as K and L increase, while the paramagnetic solution at m -- 0  always enjoys a wide basin of attraction. Computer simulations (see also [3]) show  that as initial conditions for the decoding process are typically of close-to-zero mag-  netization (almost no prior information about the original message is assumed) it  is likely that the decoding process will converge to the paramagnetic solution.  While all codes with K, L _ 3 saturate Shannon's bound in their equilibrium prop-  erties and are characterized by a first order, paramagnetic to ferromagnetic, phase  transition, codes with K = L = 2 show lower performance and different physical char-  acteristics. The analytical solutions (5) and (6) are unstable at some flip rate levels  and one resorts to solving the saddle point equations numerically and to TAP based  solutions. The picture that emerges is sketched in the inset of Fig. lb: The para-  magnetic solution dominates the high flip rate regime up to the point Pl (denoted  as 1 in the inset) in which a stable, ferromagnetic solution, of higher free energy,  appears (thin lines at m = :t:1). At a lower flip rate value P2 the paramagnetic  solution becomes unstable (dashed line) and is replaced by two stable sub-optimal  ferromagnetic (broken symmetry) solutions which appear as a couple of peaks in  the various probability distributions; typically, these have a lower free energy than  the ferromagnetic solution until P3, after which the ferromagnetic solution becomes  dominant. Still, only once the sub-optimal ferromagnetic solutions disappear, at the  spinodal point Ps, a unique ferromagnetic solution emerges as a single delta peak in  the numerical results (plus a mirror solution). The point in which the sub-optimal  ferromagnetic solutions disappear constitutes the maximal practical flip rate for the  current code-rate and was defined numerically (O) and via TAP solutions (-[-) as  shown in Fig. lb.  Notice that initial conditions for TAP and the numerical solutions were chosen al-  most randomly, with a slight bias of O(10-2), in the initial magnetization. The  TAP dynamical equations are identical to those used for practical BP decoding[5],  and therefore provide equivalent results to computer simulations with the same pa-  rameterization, supporting the analytical results. The excellent convergence results  obtained point out the existence of a unique pair of global solutions to which the  system converges (below Ps) from practically all initial conditions. This observation  and the practical implications of using K--L- 2 code have not been obtained by  Regular and Irregular Gallager-type Error-Correcting Codes 2 77  information theory methods (e.g.[3]); these prove the existence of very good codes  for C- L _> 3, and examine decoding properties only via numerical simulations.  4 Irregular Constructions  Irregular codes with non-uniform number of non-zero elements per column and  uniform number of elements per row were recently introduced [11, 12] and were  found to outperform regular codes. It is relatively straightforward to adapt our  methods to study these particular constructions. The restriction of the number  of connections per index can be replaced by a set of N restrictions of the form  (1), enforcing Ci non-zero elements in the j-th column of the matrix A, and other  M restrictions enforcing Lt non-zero elements in the/-th column of the matrix B.  By construction these restrictions must obey the relations Y7= Ci = MK and  -tM__ Lt = ML. One can assume that a particular set of restrictions is generated  independently by the probability distributions P(C) and P(L). With that we can  compute average properties of irregularly constructed codes generated by arbitrary  distributions.  Proceeding along the same lines to those of the regular case one can find a very  similar expression for the free energy which can be interpreted as a mixture of regular  codes with column weights sampled with probabilities P(C) and P(L). As long as  we choose probability distributions which vanish for C, L - 0 (avoiding trivial  non-invertible matrices) and C, L - I (avoiding single checked bits), the solutions  to the saddle point equations are the same as those obtained in the regular case  (Eqs.5, 6) leading to exactly the same predictions for the maximum performance.  The differences between regular and irregular codes show up in their dynamical  behavior. In the irregular case with K > 2 and for biased messages the basin of  attraction is larger for higher noise levels [13].  5 Conclusion  In this paper we examined the typical performance of Gallager-type codes. We dis-  covered that for a certain choice of parameters, either K >_ 3 or L _> 3, one potentially  obtains optimal performance, saturating Shannon's bound. This comes at the ex-  pense of a decreasing basin of attraction making the decoding process increasingly  impractical. Another code, K--L = 2, shows'close to optimal performance with  a very large basin of attraction, making it highly attractive for practical purposes.  The decoding performance of both code types was examined by employing the TAP  approach, an iterative method identical to the commonly used BP. Both numerical  and TAP solutions agree with the theoretical results. The equilibrium properties of  regular and irregular constructions is shown to be the same. The improved perfor-  mance of irregular codes reported in the literature can be explained as consequence  of dynamical properties. This study examines the typical performance of these in-  creasingly important error-correcting codes, from which optimal parameter choices  can be derived, complementing the bounds and empirical results provided in the  information theory literature . Important aspects that are yet to be investigated  include other noise types, finite size effects and the decoding dynamics itself.  Acknowledgement Support by the JSPS RFTF program (YK), The Royal Society and  EPSRC grant GR/N00562 (DS) is acknowledged.  278 Y. Kabashima, T. Murayama, D. Saad and R. Vicente  0.8  --06  rr 0.4  0.2  0  (a) Ferro  +1 ;  m i  [ Para  , ,1:)  0.1 0.2 0.3 0.4  p  ).5  0.8  0.6  0.4  0.2  0  b) Ferro  +1 i ! .. .....  'x r .... 'i. '! Para  ,' , I   0 0.1 0.2 0.3 0.4 0.5  p  Figure 1: Critical code rate as a function of the flip rate p, obtained from numerical  solutions and the TAP approach (N = 104), and averaged over 10 different initial  conditions with error bars much smaller than the symbols size. (a) Numerical  solutions for K = L = 3, C = 6 and varying input bias fs ([2) and TAP solutions for  both unbiased (+) and biased (O) messages; initial conditions were chosen close to  the analytical ones. The critical rate is multiplied by the source information content  to obtain the maximal information transmission rate, which clearly does not go  beyond R= 3/6 in the case of biased messages; for unbiased patterns H2(fs)= 1.  Inset: The ferromagnetic and paramagnetic solutions as functions of p; thick and  thin lines denote stable solutions of lower and higher free energies respectively. (b)  For the unbiased case of K = L = 2; initial conditions for the TAP (+) and the  numerical solutions (O) are of almost zero magnetization. Inset: The ferromagnetic  (optimal/sub-optimal) and paramagnetic solutions as functions of p; thick and thin  lines are as in (a), dashed lines correspond to unstable solutions.  References  [1]  [2]  [3]  [4]  [6]  [7]  [8]  [9]  [10]  [11]  [12]  C.E. Shannon, Bell $ys. Tech. J., 27, 379 (1948); 27, 623 (1948).  R.G. Gallager, IRE Trans. Info. Theory, IT-8, 21 (1962).  D.J.C. MacKay, IEEE Trans. IT, 45,399 (1999).  D. Thouless, P.W. Anderson and R.G. Palmer, Phil. Mag., 35,593 (1977).  Y. Kabashima and D. Saad, Europhys. Lett., 44 668 (1998) and 45 97 (1999).  N. Sourlas, Nature, 339, 693 (1989) and Euro. Phys. Lett., 25, 159 (1994).  Y. Kabashima, T. Murayama and D. Saad, Phys. Rev. Lett., (1999) in press.  K.Y.M. Wong and D. Sherrington, J. Phys. A, 20, L793 (1987).  C. De Dominicis and P. Mottishaw, J. Phys. A, 20, L1267 (1987).  H. Nishimori, Prog. Theo. Phys., 66, 1169 (1981).  M. Luby et. al, IEEE proceedings of ISIT98 (1998) and Analysis of Low Density  Codes and Improved Designs Using Irregular Graphs, preprint.  D.J.C. MacKay et. al, IEEE Trans. Comm., 47, 1449 (1999).  R. Vicente et. al, http://xxx.lanl.gov/abs/cond-mat/9908358 (1999).  
Approximate Planning in Large POMDPs  via Reusable Trajectories  Michael Kearns  AT&T Labs  mkearns @research.att.com  Yishay Mansour  Tel Aviv University  mansour @ math.tau.ac. il  Andrew Y. Ng  UC Berkeley  ang @ cs.berkeley. edu  Abstract  We consider the problem of reliably choosing a near-best strategy from  a restricted class of strategies II in a partially observable Markov deci-  sion process (POMDP). We assume we are given the ability to simulate  the POMDP, and study what might be called the sample complexity --  that is, the amount of data one must generate in the POMDP in order  to choose a good strategy. We prove upper bounds on the sample com-  plexity showing that, even for infinitely large and arbitrarily complex  POMDPs, the amount of data needed can be finite, and depends only  linearly on the complexity of the restricted strategy class II, and expo-  nentially on the horizon time. This latter dependence can be eased in a  variety of ways, including the application of gradient and local search  algorithms. Our measure of complexity generalizes the classical super-  vised learning notion of VC dimension to the settings of reinforcement  learning and planning.  Introduction  Much recent attention has been focused on partially observable Markov decision processes  (POMDPs) which have exponentially or even infinitely large state spaces. For such do-  mains, a number of interesting basic issues arise. As the state space becomes large, the  classical way of specifying a POMDP by tables of transition probabilities clearly becomes  infeasible. To intelligently discuss the problem of planning that is, computing a good  strategy 1 in a given POMDP -- compact or implicit representations of both POMDPs, and  of strategies in POMDPs, must be developed. Examples include factored next-state dis-  tributions [2, 3, 7], and strategies derived from function approximation schemes [8]. The  trend towards such compact representations, as well as algorithms for planning and learn-  ing using them, is reminiscent of supervised learning, where researchers have long empha-  sized parametric models (such as decision trees and neural networks) that can capture only  limited structure, but which enjoy a number of computational and information-theoretic  benefits.  Motivated by these issues, we consider a setting were we are given a generative model, or  Throughout, we use the word strategy to mean any mapping from observable histories to actions,  which generalizes the notion of policy in a fully observable MDP.  1002 M. Kearns, Y. Mansour and,4. Y. Ng  simulator, for a POMDP, and wish to find a good strategy 7r from some restricted class of  strategies II. A generative model is a "black box" that allows us to generate experience (tra-  jectories) from different states of our choosing. Generative models are an abstract notion of  compact POMDP representations, in the sense that the compact representations typically  considered (such as factored next-state distributions) already provide efficient generative  models. Here we are imagining that the strategy class II is given by some compact repre-  sentation or by some natural limitation on strategies (such as bounded memory). Thus, the  view we are adopting is that even though the world (POMDP) may be extremely complex,  we assume that we can at least simulate or sample experience in the world (via the gener-  ative model), and we try to use this experience to choose a strategy from some "simple"  class II.  We study the following question: How many calls to a generative model are needed to have  enough data to choose a near-best strategy in the given class? This is analogous to the  question of sample complexity in supervised learning -- but harder. The added difficulty  lies in the reuse of data. In supervised learning, every sample (:r, f(:r)) provides feedback  about every hypothesis function h(:r) (namely, how close h(:r) is to f(:r)). If h is restricted  to lie in some hypothesis class 7-/, this reuse permits sample complexity bounds that are far  smaller than the size of 7-/. For instance, only O(log(17-/I) ) samples are needed to choose  a near-best model from a finite class 7-/. If 7-/is infinite, then sample sizes are obtained  that depend only on some measure of the complexity of 7-/(such as VC dimension [9]), but  which have no dependence on the complexity of the target function or the size of the input  domain.  In the POMDP setting, we would like analogous sample complexity bounds in terms of  the "complexity" of the strategy class II -- bounds that have no dependence on the size or  complexity of the POMDP. But unlike the supervised learning setting, experience "reuse"  is not immediate in POMDPs. To see this, consider the "straw man" algorithm that, starting  with some r  II, uses the generative model to generate many trajectories under r, and  thus forms a Monte Carlo estimate of V'(s0). It is not clear that these trajectories under  r are of much use in evaluating a different r   II, since r and r  may quickly disagree  on which actions to take. The naive Monte Carlo method thus gives O(1II]) bounds on the  "sample complexity," rather than O(log(Irtl)), for the finite case.  In this paper, we shall describe the trajectory tree method of generating "reusable" tra-  jectories, which requires generating only a (relatively) small number of trajectories -- a  number that is independent of the state-space size of the POMDP, depends only linearly  on a general measure of the complexity of the strategy class II, and depends exponentially  on the horizon time. This latter dependence can be eased via gradient algorithms such as  Williams' REINFORCE [10] and Baird and Moore's more recent VAPS [1], and by local  search techniques. Our measure of strategy class complexity generalizes the notion of VC  dimension in supervised learning to the settings of reinforcement learning and planning,  and we give bounds that recover for these settings the most powerful analogous results in  supervised learning -- bounds for arbitrary, infinite strategy classes that depend only on  the dimension of the class rather than the size of the state space.  2 Preliminaries  We begin with some standard definitions. A Markov decision process (MDP) is a tuple  (S, so, A, {P(.ls, a)},R), where: $ is a (possibly infinite) state set; so  $ is a start  state; A = {al,..., ak } are actions; P(.[s, a) gives the next-state distribution upon taking  action a from state s; and the reward function R(s, a) gives the corresponding rewards.  We assume for simplicity that rewards are deterministic, and further that they are bounded  Approximate Planning in Large POMDPs via Reusable Trajectories 1003  in absolute value by/max. A partially observable Markov decision process (POMDP)  consists of an underlying MDP and observation distributions Q(ols) for each state s,  where o is the random observation made at s.  We have adopted the common assumption of a fixed start state, 2 because once we limit  the class of strategies we entertain, there may not be a single "best" strategy in the class  different start states may have different best strategies in II. We also assume that we are  given a POMDP M in the form of a generafive model for M that, when given as input any  state-action pair (s, a), will output a state s' drawn according to P(.Is, a), an observation  o drawn according to Q(-[s), and the reward/(s, a). This gives us the ability to sample  the POMDP M in a random-access way. This definition may initially seem unreasonably  generous: the generative model is giving us a fully observable simulation of a partially  observable process. However, the key point is that we must still find a strategy that performs  well in the partially observable setting. As a concrete example, in designing an elevator  control system, we may have access to a simulator that generates random rider arrival times,  and keeps track of the waiting time of each rider, the number of riders waiting at every floor  at every time of day, and so on. However helpful this information might be in designing  the controller, this controller must only use information about which floors currently have  had their call button pushed (the observables). In any case, readers uncomfortable with  the power provided by our generative models are referred to Section 5, where we briefly  describe results requiring only an extremely weak form of partially observable simulation.  At any time t, the agent will have seen some sequence of observations, o0,...,ot,  and will have chosen actions and received rewards for each of the t time  steps prior to the current one. We write its observable history as h -  ( (oo, ao, to),..., (or- , at- , r_ ), (o, _, _)). Such observable histories, also called tra-  jectories, are the inputs to strategies. More formally, a strategy r is any (stochastic) map-  ping from observable histories to actions. (For example, this includes approaches which  use the observable history to track the belief state [5].) A strategy class II is any set of  strategies.  We will restrict our attention to the case of discounted return, 3 and we let 7 E [0, 1) be the  discount factor. We define the e-horizon time to be H - log.(e(1 - 7)/2/rnax). Note  that returns beyond the first H-steps can contribute at most e/2 to the total discounted  return. Also, let Vmax - /max/(1 -- 7) bound the value function. Finally, for a POMDP  M and a strategy class II, we define opt(M, II) = support V(so) to be the best expected  return achievable from so using II.  Our problem is thus the following: Given a generative model for a POMDP M and a  strategy class II, how many calls to the generative model must we make, in order to have  enough data to choose a r E II whose performance V(so) approaches opt(M, II)? Also,  which calls should we make to the generative model to achieve this?  3 The Trajectory Tree Method  We now describe how we can use a generative model to create "reusable" trajectories.  For ease of exposition, we assume there are only two actions a and a2, but our results  generalize easily to any finite number of actions. (See the full paper [6].)  2An equivalent definition is to assume a fixed distribution D over start states, since so can be a  "dummy" state whose next-state distribution under any action is D.  3The results in this paper can be extended without difficulty to the undiscounted finite-horizon  setting [6].  1004 M. Kearns, Y. Mansour and A. Y. Ng  A trajectory tree is a binary tree in which each node is labeled by a state and observation  pair, and has a child for each of the two actions. Additionally, each link to a child is  labeled by a reward, and the tree's depth will be H, so it will have about 2 hr nodes.  (In Section 4, we will discuss settings where this exponential dependence on H can be  eased.) Each trajectory tree is built as follows: The root is labeled by so and the observation  there, o0. Its two children are then created by calling the generative model on (so, al) and  (s0, a2), which gives us the two next-states reached (say s t and s respectively), the two  observations made (say o and o), and the two rewards received (r = R(so,al) and  r = R(so, a2)). Then (st, o) and (s, o) label the root's al-child and a2-child, and the  links to these children are labeled r[ and r. Recursively, we generate two children and  rewards this way for each node down to depth H.  Now for any deterministic strategy r and any trajectory tree T, 7r defines a path through  T: r starts at the root, and inductively, if r is at some internal node in T, then we feed  to 7r the observable history along the path from the root to that node, and r selects and  moves to a child of the current node. This continues until a leaf node is reached, and we  define R(r, T) to be the discounted sum of returns along the path taken. In the case that  7r is stochastic, r defines a distribution on paths in T, and R(r, T) is the expected return  according to this distribution. (We will later also describe another method for treating  stochastic strategies.) Hence, given rn trajectory trees T1,..., T,,, a natural estimate for  m  V(so) is l'(s0) = & Y'i= R(7r, Ti). Note that each tree can be used to evaluate any  strategy, much the way a single labeled example (:c, f(:c)) can be used to evaluate any  hypothesis h(:c) in supervised learning. Thus in this sense, trajectory trees are reusable.  Our goal now is to establish uniform convergence results that bound the error of the es-  timates l'(s0) as a function of the "sample size" (number of trees) m. Section 3.1 first  treats the easier case of deterministic classes II; Section 3.2 extends the result to stochastic  classes.  3.1 The Case of Deterministic II  Let us begin by stating a result for the special case of finite classes of deterministic strate-  gies, which will serve to demonstrate the kind of bound we seek.  Theorem 3.1 Let II be any finite class of deterministic strategies for an arbitrary two-  action POMDP M. Let rn trajectory trees be created using a generative model for M, and  l ( so ) be the resulting estimates. If m = O ((Vmax/e)2(log(1III) + log(I/6))), then with  probability 1 - 6, IVY(so) - Q'"(so)l _< e holds simultaneously for all r  I-I.  Due to space limitations, detailed proofs of the results of this section are left to the full  paper [6], but we will try to convey the intuition behind the ideas. Observe that for any  fixed deterministic -, the estimates R(-, Ti) that are generated by the rrt different trajectory  trees Ti are independent. Moreover, each R(r, Ti) is an unbiased estimate of the expected  discounted H-step return of -, which is in turn e/2-close to V ' (so). These observations,  combined with a simple Chernoff and union bound argument, are sufficient to establish  Theorem 3.1. Rather than developing this argument here, we instead move straight on to  the harder case of infinite II.  When addressing sample complexity in supervised learning, perhaps the most important  insight is that even though a class  may be infinite, the number of possible behaviors of   on a finite set of points is often not exhaustive. More precisely, for boolean functions,  we say that the set :Cl,..., :ca is shattered by  if every of the 2 '/possible labelings of  Approximate Planning in Large POMDPs via Reusable Trajectories 1005  these points is realized by some h  7-{. The VC dimension of '1-{ is then defined as the  size of the largest shattered set [9]. It is known that if the VC dimension of'I-{ is d, then the  number a (m) of possible labelings induced by '1-{ on a set of m points is at most (era/d)a,  which is much less than 2 " for d << m. This fact provides the key leverage exploited by  the classical VC dimension results, and we will concentrate on replicating this leverage in  our setting.  If 11 is a (possibly infinite) set of deterministic strategies, then each strategy r  II is simply  a deterministic function mapping from the set of observable histories to the set {a, a2 },  and is thus a boolean function on observable histories. We can therefore write VC(II) to  denote the familiar VC dimension of the set of binary functions II. For example, if II is  the set of all thresholded linear functions of the current vector of observations (a particular  type of memoryless strategy), then VC(11) simply equals the number of parameters. We  now show intuitively why a class II of bounded VC dimension d cannot induce exhaustive  behavior on a set T,..., T,, of trajectory trees for m >> d. Note that ifr, r2  II are such  that their "reward labelings" (R(r, T),..., R(r, T,)) and (R(r2, T),..., R(r2,  differ, then R(rl,Ti)  R(r2,Ti) for some 1 _< i _< m. But if 71' 1 and r2 give different  returns on Ti, then they must choose different actions at some node in Ti. In other words,  every different reward labeling of the set of m trees yields a different (binary) labeling of  the set of m  2 r observable histories in the trees. So, the number of different tree reward  labelings can be at most a(m  2 r) < (era  2r/d) a. By developing this argument  carefully and applying classical uniform convergence techniques, we obtain the following  theorem. (Full proof in [6].)  Theorem 3.2 Let II be any class of deterministic strategies for an arbitrary two-action  POMDP M, and let VC (11) denote its VC dimension. Let m trajectory trees be created  using a generative model for M, and d( so ) be the resulting estimates. If  m = 0 ((Vmax/e)2(HVC(II)log(Vmax/e) + log(I/5)))  (1)  then with probability 1 - 5, IVY(s0) - l(s0)l <_ e holds simultaneously for all r e II.  3.2 The Case of Stochastic II  We now address the case of stochastic strategy classes. We describe an approach where  we transform stochastic strategies into "equivalent" deterministic ones and operate on the  deterministic versions, reducing the problem to the one handled in the previous section. The  transformation is as follows: Given a class of stochastic strategies II, each with domain X  (where X is the set of all observable histories), we first extend the domain to be X x [0, 1].  Now for each stochastic strategy r 6 II, define a corresponding deterministic transformed  strategy r' with domain X x [0,1], given by: r'(h,r) = al if r <_ Pr[r(h) = al],  and 7r'(h, r) = a2 otherwise (for any h  X, r  [0, 1]). Let II' be the collection of  these transformed deterministic strategies r'. Since II' is just a set of deterministic boolean  functions, its VC dimension is well-defined. We then define the pseudo-dimension of the  original set of stochastic strategies II to be pVC (II) = VC (11'). 4  Having transformed the strategy class, we also need to transform the POMDP, by augment-  ing the state space S to be S x [0, 1]. Informally, the transitions and rewards remain the  same, except that after each state transition, we draw a new random variable r uniformly in  [0, 1], and independently of all previous events. States are now of the form (s, r), and we let  r be an observed variable. Whenever in the original POMDP a stochastic strategy r would  4This is equivalent to the conventional definition of the pseudo-dimension of rl [4], when it is  viewed as a set of maps into real-valued action-probabilities.  1006 M. Kearns, Y. Mansour and A. Y. Ng  have been given a history h, in the transformed POMDP the corresponding deterministic  transformed strategy 7r' is given (h, r), where r is the [0, 1]-random variable at the current  state. By the definition of 7r t, it is easy to see that 7r  and 7r have exactly the same chance  of choosing each action at any node (randomization over r).  We are now back in the deterministic case, so Theorem 3.2 applies, with VC (II) replaced  by pVC (II) = VC (lit), and we again have the desired uniform convergence result.  4 Algorithms for Approximate Planning  Given a generative model for a POMDP, the preceding section's results immediately sug-  gest a class of approximate planning algorithms: generate rn trajectory trees Ti,..., Tin,  and search for a 7r  II that maximizes /'(so) - (I/m) 5-R(7r, Ti). The following  corollary to the uniform convergence results establishes the soundness of this approach.  Corollary 4.1 Let II be a class of strategies in a POMDP M, and let the number rn of  trajectory trees be as given in Theorem 3.2. Let/r - argmax,En{l'(so)} be the policy  in II with the highest empirical return on the rn trees. Then with probability 1 - (5, ' is  near-optimal within II:  Vn(so) _> ovt(M, II) - 2e. (2)  If the suggested maximization is computationally infeasible, one can search for a local  maximum 7r instead, and uniform convergence again assures us that /' (so) is a trusted es-  timate of our true performance. Of course, even finding a local maximum can be expensive,  since each trajectory tree is of size exponential in H.  However, in practice it may be possible to significantly reduce the cost of the search. Sup-  pose we are using a class of (possibly transformed) deterministic strategies, and we perform  ^  a greedy local search over II to optimize V  (so). Then at any time in the search, to evalu-  ate the policy we are currently considering, we really need to look at only a single path of  length H in each tree, corresponding to the path taken by the strategy being considered.  Thus, we should build the trajectory trees lazily  that is, incrementally build each node  of each tree only as it is needed to evaluate/(Tr, Ti) for the current strategy 7r. If there are  parts of a tree that are reached only by poor policies, then a good search algorithm may  never even build these parts of the tree. In any case, for a fixed number of trees, each step  of the local search now takes time only linear in H.5  There is a different approach that works directly on stochastic strategies (that is, without re-  quiring the transformation to deterministic strategies). In this case each stochastic strategy  7r defines a distribution over all the paths in a trajectory tree, and thus calculating/(Tr, T)  may in general require examining complete trees. However, we can view each trajectory  tree as a small, deterministic POMDP by itself, with the children of each node in the tree  being its successor nodes. So if II - {Tr0: 0  I a } is a smoothly parameterized family  of stochastic strategies, then algorithms such as William's REINFORCE [10] can be used  to find an unbiased estimate of the gradient (d/dO)l TM (so), which in turn can be used to  5See also (Ng and Jordan, in preparation) which, by assuming a much stronger model of a POMDP  (a deterministic function f such that f(s, a, r) is distributed according to P(.Is, a) when r is dis-  tributed Uniform[0,1]), gives an algorithm that enjoys uniform convergence bounds similar to those  presented here, but with only a polynomial rather than exponential dependence on H. The algorithm  samples a number of vectors r (i) E [0, 1] H , each of which, with f, defines an H-step Monte Carlo  evaluation trial for any policy r. The bound is on the number of such random vectors needed (rather  than on the total number of calls to f).  Approximate Planning in Large POMDPs via Reusable Trajectories 1007  perform stochastic gradient ascent to maximize 1) TM (so). Moreover, for a fixed number of  trees, these algorithms need only O (H) time per gradient estimate; so combined with lazy  tree construction, we again have a practical algorithm whose per-step complexity is only  linear in the horizon time. This line of thought is further developed in the long version of  the papertl  5 The Random Trajectory Method  Using a fully observable generative model of a POMDP, we have shown that the trajectory  tree method gives uniformly good value estimates, with an amount of experience linear in  VC(II), and exponential in H. It turns out we can significantly weaken the generative  model, yet still obtain essentially the same theoretical results. In this harder case, we as-  sume a generative model that provides only partially observable histories generated by a  truly random strategy (which takes each action with equal probability at every step, regard-  less of the history so far). Furthermore, these trajectories always begin at the designated  start state, so there is no ability provided to "reset" the POMDP to any state other than so.  (Indeed, underlying states may never be observed.)  Our method for this harder case is called the Random Trajectory method. It seems to lead  less readily to practical algorithms than the trajectory tree method, and its formal descrip-  tion and analysis, which is more difficult than for trajectory trees, are given in the long  version of this paper [6]. As in Theorem 3.2, we prove that the amount of data needed is  linear in VC (II), and exponential in the horizon time -- that is, by averaging appropriately  over the resulting ensemble of trajectories generated, this amount of data is sufficient to  yield uniformly good estimates of the values for all strategies in II.  References  [1] L. Baird and A. W. Moore. Gradient descent for general Reinforcement Learning. In Advances  in Neural Information Processing Systems 11, 1999.  [2] C. Boutilier, T. Dean, and S. Hanks. Decision theoretic planning: Structural assumptions and  computational leverage. Journal of Artificial Intelligence Research, 1999.  [3] X. Boyen and D. Koller. Tractable inference for complex stochastic processes. In Proc. UAI,  pages 33-42, 1998.  [4] David Haussler. Decision theoretic generalizations of the PAC model for neural net and oter  learning applications. Information and Computation, 100:78-150, 1992.  [5] L.P. Kaelbling, M. L. Littman, and A. R. Cassandra. Planning and acting in partially observable  stochastic domains. Artificial Intelligence, 101, 1998.  [6] M. Keams, Y. Mansour, and A. Y. Ng. Approximate planning in large POMDPs via reusable  trajectories. (long version), 1999.  [7] D. Koller and R. Parr. Computing factored value functions for policies in structured MDPs. In  Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence, 1999.  [8] R. S. Sutton and A. G. Barto. Reinforcement Learning. MIT Press, 1998.  [9] V.N. Vapnik. Estimation of Dependences Based on Empirical Data. Springer-Verlag, 1982.  [10] R.J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement  learning. Machine Learning, 8:229-256, 1992.  6In the full paper, we also show how these algorithms can be extended to find in expected O(H)  time an unbiased estimate of the gradient of the true value V TM (so) for discounted infinite hori-  zon problems (whereas most current algorithms either only converge asymptotically to an unbiased  estimate of this gradient, or need an absorbing state and "proper" strategies).  
Constructing Heterogeneous Committees  Using Input Feature Grouping:  Application to Economic Forecasting  Yuansong Liao and John Moody  Department of Computer Science, Oregon Graduate Institute,  P.O.Box 91000, Portland, OR 97291-1000  Abstract  The committee approach has been proposed for reducing model  uncertainty and improving generalization performance. The ad-  vantage of committees depends on (1) the performance of individ-  ual members and (2) the correlational structure of errors between  members. This paper presents an input grouping technique for de-  signing a heterogeneous committee. With this technique, all input  variables are first grouped based on their mutual information. Sta-  tistically similar variables are assigned to the same group. Each  member's input set is then formed by input variables extracted  from different groups. Our designed committees have less error cor-  relation between its members, since each member observes different  input variable combinations. The individual member's feature sets  contain less redundant information, because highly correlated vari-  ables will not be combined together. The member feature sets con-  tain almost complete information, since each set contains a feature  from each information group. An empirical study for a noisy and  nonstationary economic forecasting problem shows that commit-  tees constructed by our proposed technique outperform committees  formed using several existing techniques.  I Introduction  The committee approach has been widely used to reduce model uncertainty and  improve generalization performance. Developing methods for generating candidate  committee members is a very important direction of committee research. Good  candidate members of a committee should have (1) good (not necessarily excellent)  individual performance and (2) small residual error correlations with other members.  Many techniques have been proposed to reduce residual correlations between mem-  bers. These include resampling the training and validation data [3], adding ran-  domness to data [7], and decorrelation training [8]. These approaches are only  effective for certain models and problems. Genetic algorithms have also been used  to generate good and diverse members [6].  Input feature selection is one of the most important stages of the model learning  process. It has a crucial impact both on the learning complexity and the general-  922 Y. Liao andJ. Moody  ization performance. It is essential that a feature vector gives sufficient information  for estimation. However, too many redundant input features not only burden the  whole learning process, but also degrade the achievable generalization performance.  Input feature selection for individual estimators has received a lot of attention  because of its importance. However, there has not been much research on feature  selection for estimators in the context of committees. Previous research found  that giving committee members different input features is very useful for improving  committee performance [4], but is difficult to implement [9]. The feature selection  problem for committee members is conceptually different than for single estimators.  When using committees for estimation, as we stated previously, committee members  not only need to have reasonable performance themselves, but should also make  decisions independently.  When all committee members are trained to model the same underlying function,  it is difficult for committee members to optimize both criteria at the same time. In  order to generate members that provide a good balance between the two criteria, we  propose a feature selection approach, called input feature grouping, for commit-  tee members. The idea is to give each member estimator of a committee a rich but  distinct feature sets, in the hope that each member will generalize independently  with reduced error correlations.  The proposed method first groups input features using a hierarchical clustering  algorithm based on their mutual information, such that features in different groups  are less related to each other and features within a group are statistically similar  to each other. Then the feature set for each committee member is formed by  selecting a feature from each group. Our empirical results demonstrate that forming  a heterogeneous committee using input feature grouping is a promising approach.  2 Committee Performance Analysis  There are many ways to construct a committee. In this paper, we are mainly  interested in heterogeneous committees whose members have different input feature  sets. Committee members are given different subsets of the available feature set.  They are trained independently, and the committee output is either a weighted or  unweighted combination of individual members' outputs.  In the following, we analyze the relationship between committee errors and average  member errors from the regression point of view and discuss how the residual cor-  relations between members affect the committee error. We define the training data  7) = {(XZ, YZ);/3 = 1,2,...N} and the test data 7- = {(X,Y);/ = 1,2,...o},  where both are assumed to be generated by the model: Y = t(X) + e , e  A/'(0, rr 2) . The data 7) and 7- are independent, and inputs are drawn from an  unknown distribution. Assume that a committee has K members. Denote the  available input features as X = [Xl,X2,... ,Xm], the feature sets for the i th and  jth members as Xi = [Xil,Xi2,...,Xm] and Xj = [Xjl,Xj2,...,Xmj] respectively,  where Xi  X, Xj  X and Xi  Xj, and the mapping function of the i th and jta  member models trained on data from 7) as fi(Xi) and fj(Xj). Define the model  t,=t t,_fi(X,) for a11/-1,2,3, . oandi=l,2, K.  efTof'e i , .. , ...,  Constructing Heterogeneous Committees for Economic Forecasting 923  The MSE of a committee is  and the average MSE made by the committee members acting individually is  K  Eave = (2)  i----1  where [.] denotes the expectation over all test data T. Using Jensen's inequality,  we get Ec _ Eave, which indicates that the performance of a committee is always  equal to or better than the average performance of its members.  We define the average model error correlation as C = 1 K [e i ej ] , and  K(K--I) Eij 1 I I  then have  Ec = Eave + C = ( + q)Eave , (3)  c We consider the following four cases of q:  where q = Eave .  Case 1:  < q < 0. In this case, the model errors between members  K--1 --  are anti-correlated, which might be achieved through decorrelation training.  Case 2: q = 0. In this case, the model errors between members are  __ I Eave. That is to say, a committee can  uncorrelated, and we have: Ec -   do much better than the average performance of its members.  Case 3:0 < q < 1. If Eave is bounded above, when the committee size  K > o, we have Ec -- qEave  This gives the asymptotic limit of a  committee's performance. As the size of a committee goes to infinity, the  committee error is equal to the average model error correlation C. The  difference between Ec and Eave is determined by the ratio q.  Case 4: q = 1. In this case, Ec is equal to Eave. This happens only when  i -- e j, for all i,j = 1,..., K. It is obvious that there is no advantage to  combining a set of models that act identically.  It is clear from the analyses above that a committee shows its advantage when  the ratio q is less than one. The smaller the ratio q is, the better the committee  performs compared to the average performance of its members. For the commit-  tee to achieve substantial improvement over a single model, committee members  not only should have small errors individually, but also should have small residual  correlations between each other.  3 Input Feature Grouping  One way to construct a feature subset for a committee member is by randomly  picking a certain number of features from the original feature set. The advantage  of this method is that it is simple. However, we have no control on each member's  performance or on the residual correlation between members by randomly selecting  subsets.  924 Y. Liao andJ. Moody  Instead of randomly picking a subset of features for a member, we propose an input  feature grouping method for forming committee member feature sets. The input  grouping method first groups features based on a relevance measure in a way such  that features between different groups are less related to one another and features  within a group are more related to one another.  After grouping, there are two ways to form member feature sets. One method  is to construct the feature set for each member by selecting a feature from' each  group. Forming a member's feature set in this way, each member will have enough  information to make decision, and its feature set has less redundancy. This is the  method we use in this paper.  Another way is to use each group as the feature set for a committee member. In this  method each member will only have partial information. This is likely to hurt in-  dividual member's performance. However, because the input features for different  members are less dependent, these members tend to make decisions more inde-  pendently. There is always a trade-off between increasing members' independence  and hurting individual members' performance. If there is no redundancy among  input feature representations, removing several features may hurt individual mem-  bers' performance badly, and the overall committee performance will be hurt even  though members make decisions independently. This method is currently under  investigation.  The mutual information I(xi; xj) between two input variables xi and xj is used as  the relevance measure to group inputs. The mutual information I(x; xj), which is  defined in equation 4, measures the dependence between the two random variables.  I(xi;xj) = H(xi) - H(xilxj) -  p(xi,yi)log  p(xi,xj)  p(xi)p(xj)  (4)  If features xi and xj are highly dependent, I(Xi; Xj) will be large. Because the  mutual information measures arbitrary dependencies between random variables, it  has been effectively used for feature selections in complex prediction tasks [1], where  methods bases on linear relations like the correlation are likely to make mistakes.  The fact that the mutual information is independent of the coordinates chosen  permits a robust estimation.  4 Empirical Studies  We apply the input grouping method to predict the one-month rate of change of the  Index of Industrial Production (IP), one of the key measures of economic activity.  It is computed and published monthly. Figure 4 plots monthly IP data from 1967  to 1993.  Nine macroeconomic time series, whose names are given in Table 1, are used  for forecasting IP. Macroeconomic forecasting is a difficult task because data are  usually limited, and these series are intrinsically very noise and nonstationary.  These series are preprocessed before they are applied to the forecasting mod-  els. The representation used for input series is the first difference on one month  time scales of the logged series. For example, the notation IP.L.D1 represents  IP.L.D1 =/n(IP(t)) -/n(IP(t-1)). The target series is IP.L.FD1, which is defined  as IP.L.FD1 -- /n(IP(t+l)) -/n(IP(t)). The data set has been one of our bench-  marks for various studies [5, 10].  Constructing Heterogeneous Committees for Economic Forecasting 925  Index of Industrial Production: 1967 - 1993  1970 1980 1990  Year  Figure 1: U.S. Index of Industrial Production (IP) for the period 1967 to 1993. Shaded  regions denote official recessions, while unshaded regions denote official expansions. The  boundaries for recessions and expansions are determined by the National Bureau of Eco-  nomic Research based on several macroeconomic series. As is evident for IP, business  cycles are irregular in magnitude, duration, and structure, making prediction of IP an  interesting challenge.  Series Description  IP Index of Industrial Production  SP Standard &: Poor's 500  DL Index of Leading Indicators  M2 Money Supply  CP Consumer Price Index  CB Moody's Aaa Bond Yield  HS Housing Starts  TB3 3-month Treasury Bill Yield  Tr Yield Curve Slope: (10-Year Bond Composite)-(3-Month Treasury Bill)  Table 1: Input data series. Data are taken from the Citibase database.  During the grouping procedure, measures of mutual information between all pairs  of input variables are computed first. A simple histogram method is used to calcu-  late these estimates. Then a hierarchical clustering algorithm [2] is applied to these  values to group inputs. Hierarchical clustering proceeds by a series of successive  fusions of the nine input variables into groups. At any particular stage, the pro-  cess fuses variables or groups of variables which are closest, base on their mutual  information estimates. The distance between two groups is defined as the average  of the distances between all pairs of individuals in the two groups. The result is  presented by a tree which illustrates the fusions made at each successive level (see  Figure 2). From the clustering tree, it is clear that we can break the input variables  into four groups: (IP.L.D1, DL.L.D1) measure recent economic changes, (SP.L.D1)  reflects recent stock market momentum, (CB.D1, TB3.D1, Tr.D1) give interest rate  information, and (M2.L.D1, CP.L.D1, HS.L.D1) provide inflation information. The  grouping algorithm meaningfully clusters the nine input series.  926 Y. Lao andJ. Moody  Figure 2: Variable grouping based on mutual information. Y label is the distance.  Eighteen different subsets of features can be generated from the four groups by  selecting a feature from each group. Each subset is given to a committee mem-  ber. For example, the subsets (IP.L.D1, SP.L.D1, CB.D1, M2.L.D1) and (DL.L.D1,  SP.L.D1, TB3.D1, M2.L.D1) are used as feature sets for different committee mem-  bers. A committee has totally eighteen members. Data from Jan. 1950 to Dec.  1979 is used for training and validation, and from Jan. 1980 to Dec. 1989 is used for  testing. Each member is a linear model that is trained using neural net techniques.  We compare the input grouping method with three other committee member gen-  erating methods: baseline, random selection, and bootstrapping. The baseline  method is to train a committee member using all the input variables. Members  are only different in their initial weights. The bootstrapping method also trains  a member using all the input features, but each member has different bootstrap  replicates of the original training data as its training and validation sets. The ran-  dom selection method constructs a feature set for a member by randomly picking a  subset from the available'features. For comparison with the grouping method, each  committee generated by these three methods has 18 members.  Twenty runs are performed for each of the four methods in order to get reliable per-  formance measures. Figure 3 shows the boxplots of normalized MSE for the four  methods. The grouping method gives the best result, and the performance improve-  ment is significant compared to other methods. The grouping method outperforms  the random selection method by meaningfully grouping of input .features. It is in-  teresting to note that the heterogeneous committee methods, grouping and random  selection, perform better than homogeneous methods for this data set. One of the  reasons for this is that giving different members different input sets increases their  model independence. Another reason could be that the problem becomes easier to  model because of smaller feature sets.  5 Conclusions  The performance of a committee depends on both the performance of individual  members and the correlational structure of errors between members. An empirical  study for a noisy and nonstationary economic forecasting problem has demonstrated  that committees constructed by input variable grouping outperform committees  formed by randomly selecting member input variables. They also outperform com-  mittees without any input variable manipulation.  Constructing Heterogeneous Committees for Economic Forecasting 927  0.82  0.8  0.78  0.76  0.74  Committee Psdonanc$ Comparison (20 runs)  1 2 3 4  1:Grouping. 2:Random selection. 3.Bas$1in$. 4.Bootstrapping  Figure 3: Comparison between four different committee member generating methods.  The proposed grouping method gives the best result, and the performance improvement  is significant compared to the other three methods.  References  [1] R. Battiti. Using mutual information for selecting features in supervised neural net  learning. IEEE Trans. on Neural Networks, 5(4), July 1994.  [2] B.Everitt. Cluster Analysis. Heinemann Educational Books, 1974.  [3] L. Breimam. Bagging predictors. Machine Learning, 24(2):123-40, 1996.  [4]  K.J. Cherkauer. Human expert-level performance on a scientific image analysis task  by a system using combined artifical neural networks. In P. Chan, editor, Working  Notes of the AAAI Workshop on Integrating Multiple Learned Models, pages 15-21.  1996.  [5]  J. Moody, U. Levin, and S. Rehfuss. Predicting the U.S. index of industrial pro-  duction. In proceedings of the 1993 Parallel Applications in Statistics and Economics  Conference, Zeist, The Netherlands. Special issue of Neural Network World, 3(6):791-  794, 1993.  [6]  D. Opitz and J. Shavlik. Generating accurate nd diverse members of a neural-  network ensemble. In D. Touretzky, M. Mozer, and M. Hasselmo, editors, Advances  in Neural Information Processing Systems 8. MIT Press, Cambridge, MA, 1996.  [7] Y. Raviv and N. Intrator. Bootstrapping with noise: An effective regularization  technique. Connection Science, 8(3-4):355-72, 1996.  [8] B. E. Rosen. Ensemble learning using decorrelated neural networks. Connection  Science, 8(3-4):373-83, 1996.  [9] K. Tumer and J. Ghosh. Error correlation and error reduction in ensemble classifiers.  Connection Science, 8(3-4):385-404, December 1996.  [10] L. Wu and J. Moody. A smoothing regularizer for feedforward and recurrent neural  networks. Neural Computation, 8.3:463-491, 1996.  
A Multi-class Linear Learning Algorithm  Related to Winnow  Chris Mesterharm*  Rutgers Computer Science Department  110 Frelinghuysen Road  Piscataway, NJ 08854  mesterha@ paul .rutgers.edu  Abstract  In this paper, we present Committee, a new multi-class learning algo-  rithm related to the Winnow family of algorithms. Committee is an al-  gorithm for combining the predictions of a set of sub-experts in the on-  line mistake-bounded model of learning. A sub-expert is a special type of  attribute that predicts with a distribution over a finite number of classes.  Committee learns a linear function of sub-experts and uses this function  to make class predictions. We provide bounds for Committee that show  it performs well when the target can be represented by a few relevant  sub-experts. We also show how Committee can be used to solve more  traditional problems composed of attributes. This leads to a natural ex-  tension that learns on multi-class problems that contain both traditional  attributes and sub-experts.  1 Introduction  In this paper, we present a new multi-class learning algorithm called Committee. Committee  learns a k class target function by combining information from a large set of sub-experts.  A sub-expert is a special type of attribute that predicts with a distribution over the target  classes. The target space of functions are linear-max functions. We define these as functions  that take a linear combination of sub-expert predictions and return the class with maximum  value. It may be useful to think of the sub-experts as individual classifying functions that  are attempting to predict the target function. Even though the individual sub-experts may  not be perfect, Committee attempts to learn a linear-max function that represents the target  function. In truth, this picture is not quite accurate. The reason we call them sub-experts  and not experts is because even though a individual sub-expert might be poor at prediction,  it may be useful when used in a linear-max function. For example, some sub-experts might  be used to add constant weights to the linear-max function.  The algorithm is analyzed for the on-line mistake-bounded model of learning [Lit89]. This  is a useful model for a type of incremental learning where an algorithm can use feedback  about its current hypothesis to improve its performance. In this model, the algorithm goes  through a series of learning trials. A trial is composed of three steps. First, the algorithm  *Part of this work was supported by NEC Research Institute, Princeton, NJ.  520 C. Mesterharm  receives an instance, in this case, the predictions of the sub-experts. Second, the algorithm  predicts a label for the instance; this is the global prediction of Committee. And last, the  algorithm receives the true label of the instance; Committee uses this information to up-  date its estimate of the target. The goal of the algorithm is to minimize the total number of  prediction mistakes the algorithm makes while learning the target.  The analysis and performance of Committee is similar to another learning algorithm, Win-  now [Lit89]. Winnow is an algorithm for learning a linear-threshold function that maps  attributes in [0, 1] to a binary target. It is an algorithm that is effective when the concept  can be represented with a few relevant attributes, irrespective of the behavior of the other  attributes. Committee is similar but deals with learning a target that contains only a few  relevant sub-experts. While learning with sub-experts is interesting in it's own right, it  turns out the distinction between the two tasks is not significant. We will show in section 5  how to transform attributes from [0, 1] into sub-experts. Using particular transformations,  Committee is identical to the Winnow algorithms, Balanced and WMA [Lit89]. Further-  more, we can generalize these transformations to handle attribute problems with multi-class  targets. These transformations naturally lead to a hybrid algorithm that allows a combina-  tion of sub-experts and attributes for multi-class learning problems. This opens up a range of  new practical problems that did not easily fit into the previous framework of [0, 1] attributes  and binary classification.  2 Previous work  Many people have successfully tried the Winnow algorithms on real-world tasks. In the  course of their work, they have made modifications to the algorithms to fit certain aspects  of their problem. These modifications include multi-class extensions.  For example, [DKR97] use Winnow algorithms on text classification problems. This multi-  class problem has a special form; a document can belong to more than one class. Because  of this property, it makes sense to learn a different binary classifier for each class. The linear  functions are allowed, even desired, to overlap. However, this paper is concerned with cases  where this is not possible. For example, in [GR96] the correct spelling of a word must be  selected from a set of many possibilities. In this setting, it is more desirable to have the  algorithm select a single word.  The work in [GR96] presents many interesting ideas and modifications of the Winnow al-  gorithms. At a minimum, these modification are useful for improving the performance of  Winnow on those particular problems. Part of that work also extends the Winnow algorithm  to general multi-class problems. While the results are favorable, the contribution of this pa-  per is to give a different algorithm that has a stronger theoretical foundation for customizing  a particular multi-class problem.  Blum also works with multi-class Winnow algorithms on the calendar scheduling problem  of [MCF+94]. In [Blu95], a modified Winnow is given with theoretical arguments for good  performance on certain types of multi-class disjunctions. In this paper, these results are ex-  tended, with the new algorithm Committee, to cover a wider range of multi-class linear func-  tions.  Other related theoretical work on multi-class problems includes the regression algorithm  EG +-. In [KW97], Kivinen and Warmuth introduce EG +-, an algorithm related to Winnow  but used on regression problems. In general, while regression is a useful framework for  many multi-class problems, it is not straightforward how to extend regression to the con-  cepts learned by Committee. A particular problem is the inability of current regression tech-  niques to handle 0-1 loss.  A Multi-class Linear Learning Algorithm Related to Winnow 521  3 Algorithm  This section of the paper describes the details of Committee.  we will give a formal statement of the algorithm.  Near the end of the section,  3.1 Prediction scheme  Assume there are n sub-experts. Each sub-expert has a positive weight that is used to vote  for k different classes; let wi be the weight of sub-expert i. A sub-expert can vote for sev-  eral classes by spreading its weight with a prediction distribution. For example, if k - 3, a  sub-expert may give 3/5 of its weight to class 1, 1/5 of its weight to class 2, and 1/5 of its  weight to class 3. Let xi represent this prediction distribution, where x is the fraction of the  n J Committee predicts  weight sub-expert i gives to class j. The vote for class j is 5-= 1 wix.  the class that has the highest vote. (On ties, the algorithm picks one of the classes involved  in the tie.) We call the function computed by this prediction scheme a linear-max function,  since it is the maximum class value taken from a linear combination of the sub-expert pre-  dictions.  3.2 Target function  The goal of Committee is to minimize the number of mistakes by quickly learning  sub-expert weights that correctly classify the target function. Assume there exists it, a vec-  tor of nonnegative weights that correctly classifies the target. Notice that tz can be multiplied  by any constant without changing the target. To remove this confusion, we will normalize  the weights to sum to 1, i.e., n  Y']4= tzi = 1. Let (j) be the target's vote for class j.  = bti i  Part of the difficulty of the learning problem is hidden in the target weights. Intuitively, a  target function will be more difficult to learn if there is a small difference between the  votes  of the correct and incorrect classes. We measure this difficulty by looking at the minimum  difference, over all trials, of the vote of the correct label and the vote of the other labels.  Assume for trial t that Pt is the correct label.  5= min  tETrials  Because these are the weights of the target, and the target always makes the correct predic-  tion, 5 > 0.  One problem with the above assumptions is that they do not allow noise (cases where 5 _ 0).  However, there are variations of the analysis that allow for limited amounts of noise [Lit89,  Lit91]. Also experimental work [Lit9$, LM] shows the family of Winnow algorithms to be  much more robust to noise than the theory would predict. Based on the similarity of the  algorithm and analysis, and some preliminary experiments, Committee should be able to  tolerate some noise.  3.3 Updates  Committee only updates on mistakes using multiplicative updates. The algorithm starts by  initializing all weights to 1In. During the trials, let p be the correct label and  be the pre-  dicted label of Committee. When   p the weight of each sub-expert i is multiplied by  ct:-x This corresponds to increasing the weights of the sub-experts who predicted the  522 C. Mesterharm  correct label instead of the label Committee predicted. The value of c is initialized at the  start of the algorithm. The optimal value of c for the bounds depends on 6. Often 6 is not  known in advance, but experiments on Winnow algorithms suggest that these algorithms  are more flexible, often performing well with a wider range of c values [LM]. Last, the  weights are renormalize to sum to 1. While this is not strictly necessary, normalizing has  several advantages including reducing the likelyhood of underflow/overflow errors.  3.4 Committee code  Initialization  Vi E {1,...,n} wi := 1/n.  Set c > 1.  Trials  Instance sub-experts (Xl,..., xn).  Prediction , is the first class c such that for all  E=i _> E=i   Update Let p be the coect label. If mistake (X  p)  for i:=l to n  Wi :=  i--x Wi '  Nomalize weights, i=1 w = I  other classes j,  3.5 Mistake bound  We do not have the space to give the proof for the mistake bound of Committee, but the  technique is similar to the proof of the Winnow algorithm, Balanced, given in [Lit89]. For  the complete proof, the reader can refer to [Mes99].  Theorem 1 Committee makes at most 2 in (n)/62mistakes when the target conditions in  section 3.2 are satisfied and c is set to (1 -  Surprisingly, this bound does not refer to the number of classes. The effects of larger values  of k show up indirectly in the 6 value.  While it is not obvious, this bound shows that Committee performs well when the target  can be represented by a small fraction of the sub-experts. Call the sub-experts in the target  the relevant sub-experts. Since 6 is a function of the target, 6 only depends on the relevant  sub-experts. On the other hand, the remaining sub-experts have a small effect on the bound  since they are only represented in the ln(n) factor. This means that the mistake bound of  Committee is fairly stable even when adding a large number of additional sub-experts. In  truth, this doesn't mean that the algorithm will have a good bound when there are few rele-  vant sub-experts. In some cases, a small number of sub-experts can give an arbitrarily small  6 value. (This is a general problem with all the Winnow algorithms.) What it does mean is  that, given any problem, increasing the number of irrelevant sub-experts will only have a  logarithmic effect on the mistake bound.  4 Attributes to sub-experts  Often there are no obvious sub-experts to use in solving a learning problem. Many times the  only information available is a set of attributes. For attributes in [0, 1], we will show how to  use Committee to learn a natural kind of k class target function, a linear machine. To learn  this target, we will transform each attribute into k separate sub-experts. We will use some  of the same notion as Committee to help understand the transformation.  A Multi-class Linear Learning Algorithm Related to Winnow 523  4.1 Attribute target (linear machine)  A linear machine [DH73] is a prediction function that divides the feature space into disjoint  convex regions where each class corresponds to one region. The predictions are made by  a comparing the value of k different linear functions where each function corresponds to a  class.  More formally, assume there are m - 1 attributes and k classes. Let zi E [0, 1] be attribute  i. Assume the target function is represented using k linear functions of the attributes. Let  (J) Ei=i j  _- m tz i zi be the linear function for class j where tz is the weight of attribute i in  class j. Notice that we have added one extra attribute. This attribute is set to 1 and is needed  for the constant portion of the linear functions. The target function labels an instance with  the class of the largest  function. (Ties are not defined.) Therefore, (j) is similar to the  voting function for class j used in Committee.  4.2 Transforming the target  One difficulty with these linear functions is that they may have negative weights. Since  Committee only allows targets with nonnegative weights, we need transform to an equiv-  alent problem that has nonnegative weights. This is not difficult. Since we are only con-  cerned with the relative difference between the ( functions, we are allowed to add any func-  tion to the  functions as long as we add it to all  functions. This gives us a simple procedure  to remove negative weights. For example, if  (1) = 3z - 2z2 + lz3 - 4, we can add 2z2 + 4  to every  function to remove the negative weights from  (1). It is straightforward to extend  this and remove all negative weights.  We also need to normalize the weights. Again, since only the relative difference between  the  functions matter, we can divide all the  functions by any constant. We normalize  k n j  the weights to sum to 1, i.e., Y'j=Y'4=/ = 1. At this point, without loss of generality,  assume that the original  functions are nonnegative and normalized.  The last step is to identify a 6 value. We use the same definition of 6 as Committee substi-  tuting the corresponding  functions of the linear machine. Assume for trial t that pt is the  correct label.  5= min (i((pt)-(j)))  tETmals  4.3 Transforming the attributes  The transformation works as follows: convert attribute zi into k sub-experts. Each  sub-expert will always vote for one of the k classes with value zi. The target weight for  each of these sub-experts is the corresponding target weight of the attribute, label pair in  the  functions. Do this for every attribute.  0 zi k '  ;. . + o +... +    zi  Notice that we are not using distributions for the sub-expert predictions. A sub-expert's  prediction can be converted to a distribution by adding a constant amount to each class pre-  diction. For example, a sub-expert that predicts Zl = .7, z2 = 0, z3 = 0 can be changed  to zl = .8, z2 = .1, z3 -- .1 by adding .1 to each class. This conversion does not affect  the predicting or updating of Committee.  524 C. Mesterharm  Theorem 2 Committee makes at most 2 ln(mk)/62mistakes on a linear machine, as de-  fined in this section, when c is set to (1 - 6) -1/2  Proof: The above target transformation creates mk normalized target sub-experts that vote  with the same  functions as the linear machine. Therefore, this set of sub-experts has the  same 6 value. Plugging these values into the bound for Committee gives the result.  This transformation provides a simple procedure for solving linear machine problems.  While the details of the transformation may look cumbersome, the actual implementa-  tion of the algorithm is relatively simple. There is no need to explicitly keep track of the  sub-experts. Instead, the algorithm can use a linear machine type representation. Each class  keeps a vector of weights, one weight for each attribute. During an update, only the correct  class weights and the predicted class weights are changed. The correct class weights are  multiplied by cZ; the predicted class weights are multiplied by a -z.  The above procedure is very similar to the Balanced algorithm from [Lit89], in fact, for k =  2, it is identical. A similar transformation duplicates the behavior of the linear-threshold  learning version of WMA as given in [Lit89].  ( )  zi . 1 - zi  While this transformation shows some advantages for k = 2, more research is needed to  determine the proper way to generalize to the multi-class case. For both of these transfor-  mations, the bounds given in this paper are equivalent (except for a superficial adjustment  in the 6 notation of WMA) to the original bounds given in [Lit89].  4.4 Combining attributes and sub-experts  These transformations suggest the proper way to do a hybrid algorithm that combines  sub-experts and attributes: use the transformations to create new sub-experts from the at-  tributes and combine them with the original sub-experts when running Committee. It may  even be desirable to break original sub-experts into attributes and use both in the algorithm  because some sub-experts may perform better on certain classes. For example, if it is felt  that a sub-expert is particularly good at class 1, we can perform the following transforma-  tion.  x2 ;- x2 0  3:3 3:3 0 3:1  Now, instead of using one weight for the whole sub-expert, Committee can also learn based  on the sub-expert's performance for the first class. Even if a good target is representable  only with the original sub-experts, these additional sub-experts will not have a large effect  because of the logarithmic bound. In the same vein, it may be useful to add constant at-  tributes to a set of sub-experts. These add only k extra sub-experts, but allow the algorithm  to represent a larger set of target functions.  5 Conclusion  In this paper, we have introduced Committee, a multi-class learning algorithm. We feel that  this algorithm will be important in practice, extending the range of problems that can be han-  dled by the Winnow family of algorithms. With a solid theoretical foundation, researchers  can customize Winnow algorithms to handle various multi-class problems.  A Multi-class Linear Learning Algorithm Related to Winnow 525  Part of this customization includes feature transformations. We show how Committee can  handle general linear machine problems by transforming attributes into sub-experts. This  suggests a way to do a hybrid learning algorithm that allows a combination of sub-experts  and attributes. This same techniques can also be used to add to the representational power  on a standard sub-expert problem.  In the future, we plan to empirically test Committee and the feature transformations on real  world problems. Part of this testing will include modifying the algorithm to use extra infor-  mation, that is related to the proof technique JMes99], in an attempt to lower the number of  mistakes. We speculate that adjusting the multiplier to increase the change in progress per  trial will be useful for certain types of multi-class problems.  Acknowledgments  We thank Nick Littlestone for stimulating this work by suggesting techniques for converting  the Balanced algorithm to multi-class targets. Also we thank Haym Hirsh, Nick Littlestone  and Warren Smith for providing valuable comments and corrections.  References  [Blu95]  [DH73]  [DKR97]  [GR96]  [KW97]  [Lit89]  [Lit91]  [Lit95]  [LM]  [MCF+94]  [Mes99]  Avrim Blum. Empirical support for winnow and weighted-majority algorithms:  results on a calendar scheduling domain. In ML-95, pages 64-72, 1995.  R. O. Duda and P. Hart. Pattern Classification and Scene Analysis. Wiley, New  York, 1973.  I. Dagan, Y. Karov, and D. Roth. Mistake-driven learning in text categorization.  In EMNLP-97, pages 55-63, 1997.  A. R. Golding and D. Roth. Applying winnow to context-sensitive spelling  correction. In ML-96, 1996.  Jyrki Kivinen and Manfred K. Warmuth. Additive versus exponentiated gradi-  ent updates for linear prediction. Information and Computation, 132( 1 ): 1-64,  1997.  Nick Littlestone. Mistake bounds and linear-threshoM learning algorithms.  PhD thesis, University of California, Santa Cruz, 1989. Technical Report  UCSC-CRL-89-11.  Nick Littlestone. Redundant noisy attributes, attribute errors, and linear-  threshold learning using winnow. In COLT-91, pages 147-156, 1991.  Nick Littlestone. Comparing several linear-threshold learning algorithms on  tasks involving superfluous attributes. In ML-95, pages 353-361, 1995.  Nick Littlestone and Chris Mesterharm. A simulation study of winnow and  related algorithms. Work in progress.  T Mitchell, R. Camana, D. Freitag, J. McDermott, and D. Zabowski. Experi-  ence with a personal learning assistant. CA CM, 37(7):81-91, 1994.  Chris Mesterharm. A multi-class linear learning algorithm related to winnow  with proof. Technical report, Rutgers University, 1999.  
Semiparametric Approach to Multichannel  Blind Deconvolution  of Nonminimum Phase Systems  L.-Q. Zhang, S. Amari and A. Cichocki  Brain-style Information Systems Research Group, BSI  The Institute of Physical and Chemical Research  Wako shi, Saitama 351-0198, JAPAN  zha@open.brain.riken.go.jp  { amari,cia } @brain.riken.go.jp  Abstract  In this paper we discuss the semiparametric statistical model for blind  deconvolution. First we introduce a Lie Group to the manifold of non-  causal FIR filters. Then blind deconvolution problem is formulated in  the framework of a semiparametric model, and a family of estimating  functions is derived for blind deconvolution. A natural gradient learn-  ing algorithm is developed for training noncausal filters. Stability of the  natural gradient algorithm is also analyzed in this framework.  1 Introduction  Recently blind separation/deconvolution has been recognized as an increasing important  research area due to its rapidly growing applications in various fields, such as telecom-  munication systems, image enhancement and biomedical signal processing. Refer to re-  view papers [7] and [13] for details. A semiparametric statistical model treats a family  of probability distributions specified by a finite-dimensional parameter of interest and an  infinite-dimensional nuisance parameter [12]. Amari and Kumon [10] have proposed an  approach to semiparametric statistical models in terms of estimating functions and eluci-  dated their geometric structures and efficiencies by information geometry [ 1 ]. Blind source  separation can be formulated in the framework of semiparametric statistical models. Amari  and Cardoso [5] applied information geometry of estimating functions to blind source sep-  aration and derived an admissible class of estimating functions which includes efficient  estimators. They showed that the manifold of mixtures is m-curvature free, so that we  can design algorithms of blind separation without taking much care of misspecification of  source probability functions.  The theory of estimating functions has also been applied to the case of instantaneous mix-  tures, where independent source signals have unknown temporal correlations [3]. It is also  applied to derive efficiency and superefficiency of demixing learning algorithms [4].  Most of these theories treat only blind source separation of instantaneous mixtures. It is  only recently that the natural gradient approach has been proposed for multichannel blind  364 L.-Q. Zhang, S. Amari and A. Cichocki  deconvolution [8], [18]. The present paper extends the geometrical theory of estimating  functions to the semiparametric model of multichannel blind deconvolution. For the limited  space, the detailed derivations and proofs are left to a full paper.  2 Blind Deconvolution Problem  In this paper, as a convolutive mixing model, we consider a multichannel linear time-  invariant (LTI) systems, with no poles on the unit circle, of the form  x(k): E Hls(k- p)' (1)  p:--oO  where s(k) is an n-dimensional vector of source signals which are spatially mutu-  ally independent and temporarily identically independently distributed, and x(k) is an  n-dimensional sensor vector at time k, k = 1, 2,.  .. We denote the unknown mixing filter  by H(z) = Y''lo=-oo Hp z-p. The goal of multichannel blind deconvolution is to retrieve  source signals s(k) only using sensor signals x(k), k - 1, 2,..., and certain knowledge  of the source signal distributions and statistics. We carry out blind deconvolution by using  another multichannel LTI system of the form  y(k) = W(z)x(k), (2)  where W(z) = y''pN___ N Wpz -p, N is the length of FIR filter W(z), y(k) =  [y (k),..., yn(k)] r is an n-dimensional vector of the outputs, which is used to estimate  the source signals.  When we apply W(z) to the sensor signal x(k), the global transfer function from s(k)  to y(k) is defined by G(z) = W(z)H(z). The goal of the blind deconvolution task is  to find W(z) such that G(z) = PAD(z), where P E R nx" is a permutation matrix,  D(z) = diag{z-d, --. , z -d' }, and A E R "x' is a nonsingular diagonal scaling matrix.  3 Lie Group on .M(N, N)  In this section, we introduce a Lie group to the manifold of noncausal FIR filters. The Lie  group operations play a crucial role in the following discussion. The set of all the noncausal  FIR filters W(z) of length N, having the constraint that VV is nonsingular, is denoted by  4(N,N) = {W(z)  I W(z) = Wpz-, dot(w) # 0 , (3)  p=-N  where W is an N x N block matrix,  W0  W  WN-1  W-1 ''' W-N+I ]  W 0 ... W_N+ 2  WN-2 ''' W0  (4)  A//(N, N) is a manifold of dimension n2(2N + 1). In general, multiplication of two filters  in A//(N, N) will enlarge the filter length and the result does belong to A//(N, N) anymore.  This makes it difficult to introduce the Riemannian structure to the manifold of noncausal  FIR filters. In order to explore possible geometrical structures of A//(N, N) which will  lead to effective learning algorithms for W(z), we define algebraic operations of filters in  the Lie group framework. First, we introduce a novel filter decomposition of noncausal  filters in A//(N, N) into a product of two one-sided FIR filters [ 19], which is illustrated in  Fig. 1.  Blind Deconvolution of Nonminimum Phase Systems 365  Unknown  n(z)  Mixing model  R(z")  Demixing model  y(k)  Figure 1: Illustration of decomposition of noncausal filters in ./4 (N, N)  Lemma I 119] If the matrix 14] is nonsingular, any noncausal filter W(z) in JI(N,N)  has the decomposition W(z) = R(z)L(z -x) where R(z) v  , -- -p=oRpz-p, L(z -1) --  N  Y]p=O LpzP are one-sided FIR filters.  In the manifold J4 (N, N), Lie operations, multiplication  and inverse t, are defined  as follows: For B(z), C(z) e .A4(N, N),  B(z) * C(z) '- [B(z)C(Z)]N, B'(z) '- L  (z-1)l])'(z),  (5)  where [B(z)]v is the truncating operator that any terms with orders higher than N in the  polynomial B(z) are truncated, and the inverse of one-side FIR filters is recurrently defined  byP R0-, Rp p t --1  ---- '-- -- Zq=i Rp_qRqR , p = 1,..., N. Refer to [18] for the detailed  derivation. With these operations, both B(z)  C(z) and B t (z) still remain in the manifold  .M (N, N). It is easy to verify that the manifold .M (N, N) with the above operations forms  a Lie Group. The identity element is E(z) = I.  4 Semiparametric Approach to Blind Deconvolution  We first introduce the basic theory of semiparametric models, and formulate blind decon-  volution problem in the framework of the semiparametric models.  4.1 Semiparametric model  Consider a general statistical model {p(x; O, )}, where x is a random variable whose  probability density function is specified by two parameters, 0 and , 0 being the param-  eter of interest, and  being the nuisance parameter. When the nuisance parameter is of  infinite dimensions or of functional degrees of freedom, the statistical model is called  a semiparametric model [12]. The gradient vectors of the log likelihood u(a:; O, ) =  O og v(a:;O,)  0 log p(X;O,) V(X; O, ) O are called the score functions of the parameter  O0 ' -- '  of interest or shortly O-score and the nuisance score or shortly -score, respectively.  In the semiparametric model, it is difficult to estimate both the parameters of interest and  nuisance parameters at the same time, since the nuisance parameter  is of infinite degrees  of freedom. The semiparametric approach suggests to use an estimating function to es-  timate the parameters of interest, regardless of the nuisance parameters. The estimating  function is a vector function z(x, 0), independent of nuisance parameters , satisfying the  following conditions  1) Eo,[z(x,O)]=O, (6)  _0z(x,  2) det(lC)  O, where/C = Eo,f[ _0).]. (7)  366 L.-Q. Zhang, S. Amari and A. Cichocki  a) r(x, 0)] <  (8)  for all 0 and . Generally speaking, it is difficult to find an estimating function. Amari  and Kawanabe [9] studied the information geometry of estimating functions and provided  a novel approach to find all the estimating functions. In this paper, we follow the approach  to find a family of estimating functions for bind deconvolution.  4.2 Semiparametric Formulation for Blind Deconvolution  Now we turn to formulate the blind deconvolution problem in the framework of semipara-  metric models. From the statistical point of view, the blind deconvolution problem is to  estimate H(z) or H-(z) from the observed data DL = {x(k), k = 1,2,...}. The es-  timate includes two unknowns: One is the mixing filter H(z) which is the parameter of  interest, and the other is the probability density function p(s) of sources, which is the nui-  sance parameter in the present case. Fo blind deconvolution problem, we usually assume  that source signals are zero-mean, E[$il '- 0, for i = 1,..., n. In addition, we generally  impose constraints on the recovered signals to remove the indeterminacy,  E[ki(si)] = 0, for i = 1,...,n. (9)  4 _ 1. Since the source signals are spatially  A typical example of the constraint is ki ($i) '- $i  mutually independent and temporally iid, the pdf r(s) can be factorized into a product form  /'(S) : I-Iin=i I'($i). The purpose of this paper is to find a family of estimating functions  for blind deconvolution. Remarkable progress has been made recently in the theory of the  semiparametric approach [9],[12]. It has been shown that the efficient score itself is an  estimating function for blind separation.  5 Estimating Functions  In this section, we give an explicit form of the score function matrix of interest and the  nuisance tangent space, by using a local nonholonomic reparameterization. We then derive  a family of estimating functions from it.  5.1 Score function matrix and its representation  Since the mixing model is a matrix filter, we write an estimating function in the same matrix  filter format  N  F(x;H(z))-- E Flvx;U(z))z-P' (10)  p=--N  where Fp(x; H(z)) are n x n-matrices. In order to derive the explicit form of the H-score,  we reparameterize the filter in a small neighborhood of H (z) by using a new variable matrix  filter as H(z)  (I - X(z)), where I is the identity element of the manifold JM(N, N).  The variation X(z) represents a local coordinate system at the neighborhood ./Vii of H(z)  on the manifold JM(N, N). The variation dH(z) of H(z) is represented as dH(z) =  -H(z)  dX(z). Letting W(z) = Ht(z), we have  = 0arV(z)  wt(z), (11)  which is a nonholonomic differential variable [6] since (11) is not integrable. With this  representation of the parameters, we can obtain learning algorithms having the equivariant  property [14] since the deviation dX(z) is independent of a specific H(z). The relative or  the natural gradient of a cost function on the manifold can be automatically derived from  this representation [2], [ 14], [ 18].  Blind Deconvolution of Nonminimum Phase Systems 367  u i  Figure 2: Illustration of orthogonal decomposition of score functions  The derivative of any cost function/(H(z)) with respect to a noncausal filter X(z) =  v X z-V is defined by  p=--N P  ot(}I(z)) N Ot(H(z))  0x() -  z -p (12)  Now we can easily calculate the score function matrix of noncausal filter X(z),  Ologp(x;H(z),r) v  OX(z) = E qo(y)yT(k-- P)z-P'  (13)  where p(y) = (qoi(ys),-.-, qon(Yn)) T, Ti(Yi) '- dlogr,(y0 and y -- Ht(z)x.  dy '  5.2 Efficient scores  The efficient scores, denoted by UZ(s; H(z), r), can be obtained by projecting the score  function to the space orthogonal to the nuisance tangent space v  T(z),r, which is illustrated  in figure 2. In this section, we give an explicit form of the efficient scores for blind decon-  volution.  Lemma 2 [5] The tangent nuisance space N  7"I'(z),r is a linear space spanned by the nui-  sance score functions, denoted by N  Tl(z),r = {-'=l cii(si)}, where ci are coefficients,  and as ( ss ) are arbitrary functions, satisfying the following conditions  [,s(ss) ] < oo, [a,,(ss)] = o, [(s),(s)] = o. (14)  We rewrite the score function (13) into the form U(s; H(z), r) N  = Y.p=-v Up z-p, where  Up - (T(ss(k))sj(k - P))nxn.  Lemma 3 The off-diagonal elements uo,o (s; H(z), r), i  j, and the delay elements  Up,O(s; H(z), r), p  O, of the score functions are orthogonal to the nuisance tangent  space r  Ti(:),,..  Lemma 4 The projection of uo,si to the space orthogonal to the nuisance tangent space  N  7'(z),r is ofthe form w(si) '- co + clss + c2k(si), where cs are any constants.  368 L.-Q. Zhang, S. Amari and A. Cichocki  In summary we have the following theorem  Theorem 1 The efficient score, U m (s; H(z), r) N  : p=-l Up Ez-p, is given by  U = o(s)sr(k-p), forp0; (15)  U0  = { qo(s)s r, for off diagonal elements,  co + Cl S + c2 k ( s ) , for diagonal elements. (16)  For the instantaneous mixture case, it has been proven [9] that the semiparametric model  for blind separation is information m-curvature free. This is also true in the multichannel  blind deconvolution case. As a result, the efficient score function is an estimating function  for blind deconvolution. Using this result, we easily derive a family of estimating functions  for blind deconvolution  N  F(x(k);W(z)) =  qo(y(k))y(k - p)Tg-p _ I, (17)  p:-N  where y(k) = W(z)x(k), and qo is a given function vector. The estimating function is the  efficient score function, when co = Cl = 0, c2 = I and ki(si) = i(si)si - 1.  6 Natural Gradient Learning and its Stability  Ordinary stochastic gradient methods for parameterized systems suffer from slow conver-  gence due to the statistical correlations of the processes signals. While quasi-Newton and  related methods can be used to improve convergence, they also suffer from the mass com-  putation and numerical instability, as well as local convergence.  The natural gradient approach was developed to overcome the drawback of the ordinary  gradient algorithm in the Riemannian spaces [2, 8, 15]. It has been proven that the natural  gradient algorithm is an efficient algorithm in blind separation and blind deconvolution [2].  The efficient score function ( the estimating function ) gives an efficient search direction  for updating filter X(z). Therefore, the updating rule for X(z) is described by  Xk+l(Z) - Xk(z) - r/F(x(k), We(z)), (18)  where r/is a learning rate. Since the new parameterization X(z) is defined by a nonholo-  nomic transformation dX(z) = dW(z)  W t (z), the deviation of W(z) is given by  AW(z) -- AX(z)  W(z). (19)  Hence, the natural gradient learning algorithm for W(z) is described as  Wk+ 1 (Z) -- Wk(z ) --]F(x(k), Wk (z)), Wk(Z), (20)  where F(x, W(z)) is an estimating function in the form (17). The stability of the algorithm  (20) is equivalent to the one of algorithm (18). Consider the averaged version of algorithm  (18)  /xx(z) = -vE[r(x(k), (21)  Analyzing the variational equation of the above equation and using the mutual indepen-  dence and i.i.d. properties of source signals, we derive the stability conditions of learning  algorithm (21) at vicinity of the true solution  2 2  mi q- 1 > O, ni > O, ija i aj > 1, (22)  , E ' 2  fori,j = 1,...,n, wheremi: E(qo'(yi(k))y(k)] ti = [i(Yi)], ai = E[Iy12].  Therefore, we have the following theorem:  Theorem 2 If the conditions (22) are satisfied, then the natural gradient learning algo-  rithm (20) is locally stable.  Blind Deconvolution of Nonminimum Phase Systems 369  References  [11  [21  [31  [41  [51  [6]  [71  [8]  [91  [10]  [11]  [12]  [13]  [14]  [15]  [16]  [17]  [18]  [19]  S. Amari. Differential-geometrical methods in statistics, Lecture Notes in Statistics,  volume 28. Springer, Berlin, 1985.  S. Amari. Natural gradient works efficiently in learning. Neural Computation,  10:251-276, 1998.  S. Amari. ICA of temporally correlated signals - Learning algorithm. In Proceeding  of 1st Inter. Workshop on Independent Component Analysis and Signal Separation,  pages 37-42, Aussois, France, January, 11-15 1999.  S. Amari. Superefficiency in blind source separation. IEEE Trans. on Signal Process-  ing, 47(4):936-944, April 1999.  S. Amari and J.-F. Cardoso. Blind source separation- semiparametric statistical ap-  proach. IEEE Trans. Signal Processing, 45:2692-2700, Nov. 1997.  S. Amari, T. Chen, and A. Cichocki. Nonholonomic orthogonal constraints in blind  source separation. Neural Cornput., to be published.  S. Amari and A. Cichocki. Adaptive blind signal processing- neural network ap-  proaches. Proceedings of the IEEE, 86(10):2026-2048, 1998.  S. Amari, S. Douglas, A. Cichocki, and H. Yang. Multichannel blind deconvolution  and equalization using the natural gradient. In Proc. IEEE Workshop on Signal Pro-  cessing Adv. in Wireless Communications, pages 101-104, Paris, France, April 1997.  S. Amari and M. Kawanabe. Estimating functions in semiparametric statistical mod-  els. In I. V. Basawa, V.P. Godambe, and R.L. Taylor, editors, Estimating Functions,  volume 32 of Monograph Series, pages 65-81. IMS, 1998.  S. Amari and M. Kumon. Estimation in the presence of infinitely many nuisance  parameters in semiparametric statistical models. Ann. Statistics, 16:1044-1068, 1988.  A.J. Bell and T.J. Sejnowski. An information maximization approach to blind sepa-  ration and blind deconvolution. Neural Computation, 7:1129-1159, 1995.  P. Bickel, C. Klaassen, Y. Ritov, and J. Wellner. Efficient and Adaptive Estimation  for Semiparametric Models. The Johns Hopkins Univ. Press, Baltimore and London,  1993.  J.-F Cardoso. Blind signal separation: Statistical principles. Proceedings of the IEEE,  86(10):2009-2025, 1998.  J.-F. Cardoso and B. Laheld. Equivariant adaptive source separation. IEEE Trans.  Signal Processing, SP-43:3017-3029, Dec 1996.  A. Cichocki and R. Unbehauen. Robust neural networks with on-line learning for  blind identification and blind separation of sources. IEEE Trans Circuits and Systems  I: Fundamentals Theory and Applications, 43(11): 894-906, 1996.  L. Tong, R.W. Liu, V.C. Soon, and Y.F. Huang. Indeterminacy and identifiability of  blind identification. IEEE Trans. Circuits, Syst., 38(5):499-509, May 1991.  H. Yang and S. Amari. Adaptive on-line learning algorithms for blind separation:  Maximum entropy and minimal mutual information. Neural Cornput., 9:1457-1482,  1997.  L. Zhang, A. Cichocki, and S. Amari. Geometrical structures of FIR manifold and  their application to multichannel blind deconvolution. In Proceeding of NNSP'99,  pages 303-312, Madison, Wisconsin, August 23-25 1999.  L. Zhang, A. Cichocki, and S. Amari. Multichannel blind deconvolution of non-  minimum phase systems using information backpropagation. In Proceedings of the  Fifth International Conference on Neural Information Processing(ICONIP'99), page  210-216, Perth, Australia, Nov. 16-20 1999.  
A Neuromorphic VLSI System for Modeling  the Neural Control of Axial Locomotion  Girish N. Patel  giri sh @ ece. gatech.edu  Edgar A. Brown  ebrown @ ece. gatech.edu  Stephen P. DeWeerth  steved@ece.gatech.edu  School of Electrical and Computer Engineering  Georgia Institute of Technology  Atlanta, Ga. 30332-0250  Abstract  We have developed and tested an analog/digital VLSI system that mod-  els the coordination of biological segmental oscillators underlying axial  locomotion in animals such as leeches and lampreys. In its current form  the system consists of a chain of twelve pattern generating circuits that  are capable of arbitrary contralateral inhibitory synaptic coupling. Each  pattern generating circuit is implemented with two independent silicon  Morris-Lecar neurons with a total of 32 programmable (floating-gate  based) inhibitory synapses, and an asynchronous address-event inter-  connection element that provides synaptic connectivity and implements  axonal delay. We describe and analyze the data from a set of experi-  ments exploring the system behavior in terms of synaptic coupling.  1 Introduction  In recent years, neuroscientists and modelers have made great strides towards illuminat-  ing structure and computational properties in biological motor systems. For example,  much progress has been made toward understanding the neural networks that elicit rhyth-  mic motor behaviors, including leech heartbeat, crustacean stomatogastric mill and lam-  prey swimming (a good review on these is in [1] and [2]). It is thought that these same  mechanisms form the basis for more complex motor behaviors. The neural substrate for  these control mechanisms are called central pattern generators (CPG). In the case of loco-  motion these circuits are distributed along the body (in the spinal cord of vertebrates or in  the ganglia of invertebrates) and are richly interactive with sensory input and descending  connections from the brain, giving rise to a highly distributed system as shown in  Figure 1. In cases in which axial locomotion is involved, such as leech and lamprey  swimming, synaptic interconnection patterns among autonomous segmental oscillators  along the animal's axis produce coordinated motor patterns. These intersegmental coordi-  nation architectures have been well studied through both physiological experimentation  and mathematical modeling. In addition, undulatory gaits in snakes have also been stud-  ied from a robotics perspective [3]. However, a thorough understanding of the computa-  tional principles in these systems is still lacking.  A Neuromorphic System for Modeling Axial Locomotion 725  Muscle  Muscle  T  I  I  I  __  Figure 1: Neuroanatomy of segmented animals.  In order to better understand the computational paradigms that mediate intersegmental  coordination and the resulting neural control of axial locomotion (and other motor pat-  terns), we are using neuromorphic very large-scale integrated (VLSI) circuits to develop  models of these biological systems. The goals in our research are (i) to study how the  properties of individual neurons in a network affect the overall system behavior; (ii) to  facilitate the validation of the principles underlying intersegmental coordination; and (iii)  to develop a real-time, low power, motion control system. We want to exploit these prin-  ciples and architectures both to improve our understanding of the biology and to design  artificial systems that perform autonomously in various environments.  Parameter Input  GUI ]  Address-Event Communication Network  Event x /  Output 12 $eg*ment$  Figure 2: Block-level diagram of the implemented system. The intersegmental  communications network facilitates communication among the intrasegmental units with  pipelined stages.  In this paper, we present a VLSI model of intersegmental coordination as shown in  Figure 2. Each segment in our system is implemented with a custom IC containing a CPG  consisting of two silicon model neurons, each one with 16 inhibitory synapses whose val-  ues are stored on chip and are continuously variable; an asynchronous address event com-  munications IC that implements the queuing and delaying of events providing synaptic  connectivity and thus simulating axonal properties; and a microcontroller (with internal  A/D converter and timer) that facilitates the modification of individual parameters  through a serial bus. The entire system consists of twelve such segments linked to a com-  puter on which a graphical user interface (GUI) is implemented. By using the GUI, we  are able to control all of the synaptic connections in the system and to measure the result-  726 G. N. Patel, E. A. Brown and $. P DeWeerth  ing neural outputs. We present the system model, and we investigate the role of synaptic  coupling in the establishment of phase lags along this chain of neural oscillators.  2 Pattern generating circuits  The smallest neural system capable of generating the basic alternating activity that char-  acterizes the swimming CPGs is the half-center oscillator, essentially two bursting neu-  rons with reciprocally inhibitory connections [1] as shown in Figure 3a. In biological  systems, the associated neurons have both slow and fast time constants to facilitate the  fast spiking (action potentials) and the slower bursting oscillations that control the elic-  ited movements as shown in Figure 3b. To simplify the parameter space of our system,  we use reduced two-state silicon neurons [4]. The output of each silicon neuron is an  oscillation that represents the envelope of the bursting activity (i.e. the spiking activity  and corresponding fast time constants are eliminated) as shown in Figure 3c. Each neu-  ron also has 16 analog synapses that receive off-chip input. The synaptic parameters are  stored in an array of floating-gate transistors [5] that provide nonvolatile analog memory.  CPG  B  c  1  Figure 3: Half-center oscillator and the generation of events in spiking and nonspiking  silicon neurons. Events are generated by detecting rapid rises in the membrane potential of  spiking neurons or by detecting rapid rises and falls in nonspiking neurons.  3 Intersegmental communication  Our segmented system consists of an array of CPG circuits interconnected via an commu-  nication network that implements an asynchronous, address-event protocol [6][7]. Each  CPG is connected to one node of this address-event intersegmental communication sys-  tem as illustrated in Figure 2. This application-specific architecture uses a pipelined  broadcast scheme that is based upon its biological counterpart. The principal advantage of  using this custom scheme is that requisite addresses and delays are generated implicitly  based upon the system architecture. In particular the system implements distance-depen-  dent delays and relative addressing. The delays, which are thought to be integral to the  network computation, replicate the axonal delays that result as action potentials propa-  gate down an animal's body [2]. The relative addressing greatly simplifies the implemen-  tation of synaptic spread [8], the hypothesized translational invariance in the  intersegmental connectivity in biological axial locomotion systems. Thus, we can set the  synaptic parameters identically at every segment, greatly reducing system complexity.  In this architecture (which is described in more depth in [4]), each event is passed from  segment to neighboring segment bidirectionally down the length of the one-dimensional  A Neuromorphic System for Modeling Axial Locomotion 72 7  communications network. By delaying each event at every segment, the pipeline architec-  ture facilitates the creation of distance-dependent delays. The other primary advantage of  this architecture is that it can easily generate a relative addressing scheme. Figure 4 illus-  trates the event-passing architecture with respect to the relative addressing and distance-  dependent delays. Each event, generated at a particular node (the center node, in this  example), is transmitted bidirectionally down the length of the network. It is delayed by  time AT at each segment, not including the initiating segment.  t = t0+2AT t = t0+AT t = t0+AT t = t0+2AT  Figure 4: Relative addressing and distance-dependent delays.  The events are generated by the neurons in each segment. Because these are not spiking  neurons, we could not use the typical scheme of generating one event per action poten-  tial. Instead, we generate one event at the beginning and end of each burst (as illustrated  in Figure 3) and designate the individual events as rising or falling. In each segment the  events are stored in a queue (Figure 5), which implements delay based upon uniform con-  duction velocities. As an event arrives at each new segment, it is time stamped, its rela-  tive address is incremented (or decremented), and then it is stored in the queue for the  A T interval. As the event exits the queue, its data is decoded by the intrasegmental units,  and synaptic inputs are applied to the appropriate intrasegmental neurons.  ovonts from ovonts from  rostral segment intrasegmental unit  (closer to head) [ l tlror   (event storage stral and  -- and processing) caudal segments  events from and intrasegmental  caudal segment  (closer to tail) unit  Figure 5: Block-level diagram of a communications node illustrating how events enter  and exit each stage of the pipeline.  4 Experiments and Discussion  We have implemented the complete system shown in Figure 2, and have performed a  number of experiments on the system. In Figure 6, we show the behaviors the system  exhibits when it is configured with asymmetrical nearest-neighbor connections. The sys-  tem displays traveling waves whose directions depend on the direction of the dominant  coupling. Note that the intersegmental phase lags vary for different swim frequencies.  One important set of experiments focussed on the role of long-distance connections on  the system behaviors. In these experiments, we configured the system with strong  descending (towards the tail) connections such that robust rearward traveling waves (for-  ward swimming) are observed. The long-distance connections are weak enough to avoid  any bifurcations in behavior (different type of behavior). Thus, the traveling wave solu-  tion resulting from the nearest-neighbor connections persists as we progressively add  long-distance connections. In Figure 7 we show the dependency of the swim frequency  and the total phase lag (summation of the normalized intersegmental phase lags, where  1 = 360  ) on the extent of the connections. The results show a clear difference in behav-  728 G. N. Patel, E. A. Brown and S. P. DeWeerth  stronger ascending coupling  05 -   ra  0-' / \'" ' , '2'  -0.2 -0.1 0 0.1 0.2  time (see)  stronger descending coupling  -0.2 -0.1 0 0.1 0.2  time (see)  , .'.  -0.05 0 0.05 -0.05 0 0.05  time (see) time (see)  Figure 6: Traveling waves in the system with asymmetrical, nearest-neighbor  connections. Plots are cross-correlations between rising edge events generated by a neuron  in segment six and events generated by homolog neurons in each segment. Stronger  ascending connections (A & B) produce forward traveling waves (backward swimming)  and stronger descending connections (C & D) produce rearward traveling waves (forward  swimming). An externally applied current (Iext) controls the swim frequency. At small  values of Iex t (6.7 nA) the periods of the swim cycles are approximately 0.180 ms and  0.150 ms for A & C, respectively; for large values of Iex t (32.8 nA), the periods of the  swim cycles are approximately 36 ms and 33 ms for B & D, respectively.  iors between the lowest tonic drive (Icx t = 21.9 nA) and the two higher tonic drives. (By  tonic drive, we mean a constant dc current is applied to all neurons.) In the former, the  sensitivity of long-distance connections on frequency and intersegmental phase lags is  considerably greater than in the latter. The demarcation in behavior may be attributed to  different behaviors at different tonic drives. For lower tonic drive, the long-distance con-  nections tend to synchronize the system (decrease the intersegmental phase lags). At the  higher tonic drives, long-distance connections do not affect the system considerably. For  Iex t = 32.8 nA, connections that span up to four segments aid in producing uniformity in  the intersegmental phase lags. Although this does not hold for Icx t = 48.1 nA, long-dis-  tance connections play a more significant role in preserving the total phase difference. At  Iex t - 32.8 nA and Iex t = 48.1 nA, the system with short-distance connections produces a  total phase difference of 1.19 and 1.33, respectively. In contrast, for Icx t = 32.8 nA and  Iex t = 48.1 nA, the system with long-distance connections that span up to seven segments  produces a total phase difference of 1.20 and 1.25, respectively.  In the above experiments, we have demonstrated that, in a specific parameter regime,  weak long-distance connections can affect the intersegmental phase lags. However, these  weight profiles should not be construed as a possible explanation on what the weight pro-  files in a biological system might be. The parameter regime in which we observed this  behavior is small; at moderate strengths of coupling, the traveling wave solutions disap-  pear and move towards synchronous behavior. Recent experiments done on spinalized  lampreys reveal that long-distance connections are moderately strong [10]. Thus, our cur-  rent model is unable to replicate this aspect of intersegmental coordination. There are  several explanations that may account for this discrepancy.  A Neuromorphic System for Modeling Axial Locomotion 729  A 40 B 1.5  30  -, 20  10  0 2 4 6 8 0 2 4 6 8  extent extent  Figure 7: Effects of weak long-distance connections on swimming frequency (A), on the  total phase difference (summation of the normalized intersegmental phase lags) (B), and  on the standard deviation of the intersegmental phase lags (C). 5 < = denote Iex t = 48.1 nA,  32.8 nA, and 21 nA, respectively.  In the segmental CPG network of the animal, there are many classes of neurons that send  projections to many other classes of neurons. The phase a connection imposes is deter-  mined by which neuron class connects with which other neuron class. In our system, the  segmental CPG network has only a single class of neurons upon which the long-distance  connections can impose their phase. Depending on where in parameter space we operate  our system, the long-distance connections have too little or too great an effect on the  behavior of the system. At high tonic drives, the sensitivity of the weak long-distance  connections on the intersegmental phase lags is small, whereas for small tonic drives, the  long-distance connections have a great effect on the intersegmental phase lags.  It has been shown that if the waveform of the oscillators is sinusoidal (i.e., the time scales  of the two state variables are not too different), traveling wave solutions exist and have a  large basin of attraction [11]. However, as the disparity between the two time scales is  made larger (i.e., the neurons are stiff and the waveform of the oscillations appears  square-wave like), the system will move towards synchrony. In our implementation, to  facilitate accurate communication of events, we bias the neurons with relatively large dif-  ferences in the time scales. Thus, this restriction reduces the parameter regime in which  we can observe stable traveling waves.  Another factor that determines the range of parameters in which stable traveling waves  are observed is the slope of our synaptic coupling function. When the slope of the cou-  pling function is steep, the total synaptic current over a cycle can increase significantly,  causing weak connections to appear strong. This has an overall effect of synchronizing  the network [ 11]. For coupling functions whose slopes are shallow, the total synaptic cur-  rent over a cycle is reduced; therefore, the connections appear weak and larger interseg-  mental phase lags are possible. Thus, the sharp synaptic coupling function in our  implementation, which is necessary for communication, is another factor that diminishes  the parameter regime in which we can observe stable traveling waves.  The above factors limit the parameter range in which we observe traveling waves. How-  ever, all of these issues can be addressed by improving our CPG network. The first issue  can be addressed by increasing the number of neuron classes or adding more segments.  The second and third issues can be addressed by adding spiking neurons in our CPG net-  work so that the form of the oscillations can be coded in the spike train and the synaptic  coupling functions can be implemented on the receiving side of the CPG chip. The fourth  730 G. N. Patel, E. A. Brown and S. P. DeWeerth  issue can be addressed by designing self-adapting neurons that tune their internal parame-  ters so that their waveforms and intrinsic frequencies are matched. Although weak cou-  pling may not be biologically plausible, producing traveling waves based on phase  oscillators would be an interesting research direction.  5 Conclusions and Future Work  In this paper, we described a functional, neuromorphic VLSI system that implements an  array of neural oscillators interconnected by an address-event communication network.  This system represents our most ambitious neuromorphic VLSI effort to date, combining  24 custom ICs, a special-purpose asynchronous communication architecture designed  analogously to its biological counterpart, large-scale synaptic interconnectivity with  parameters stored using floating-gate devices, and a computer interface for setting the  parameters and for measuring the neural activity. The working system represents the cul-  mination of a four-year effort, and now provides a testbed for exploring a variety of bio-  logical hypotheses and theoretical predictions.  Our future directions in the development of this system are threefold. First, we will con-  tinue to explore, in depth, the operation of the present system, comparing it to theoretical  predictions and biological hypotheses. Second, we are implementing a segmented  mechanical system that will provide a moving output and will facilitate the implementa-  tion of sensory feedback. Third, we are developing new CPG model centered around sen-  sory feedback and motor learning. The modular design of the system, which puts all of  the neural and synaptic specificity on the CPG IC, allows us to design a completely new  CPG and to replace it in the system without changing the communication architecture.  References  [1]  [21  [3]  [4]  [5]  [61  [7]  [8]  [9]  [lO]  [11]  [12]  E. Marder & R.L. Calabrese. Principles of rhythmic motor pattern generation. Physiological  Reviews 76 (3): 687-717, 1996.  A.H. Cohen, G.B. Ermentrout, T Kiemel, N. Kopell, K.A. Sigvardt, & T.L. Williams. Modeling  of intersegmental coordination in the lamprey central pattern generator for locomotion. TINS  15:434-438, 1992.  S. Hirose. Biologically Inspired Robots: Snake-like Locomotors and Manipulators. Oxford  University Press, 1993.  S. DeWeerth, G. Patel, D. Schimmel, M. Simoni, & R.L. Calabrese. A VLSI Architecture for  Modeling Intersegmental Coordination. In Proceedings of the Seventeenth Conference on  Advanced Research in VLSI, R.B. Brown and A.T. Ishii (eds), Los Alamitos, CA: IEEE  Computer Society, 182-200, 1997.  P. Hasler, B.A. Minch, and C. Diorio. Adaptive circuits using pFet floating-gate devices. In Scott  Wills and Stephen DeWeerth editors, 20th Conference of Advanced Research in VLSI, pages  215-230, Los Alamitos, California, CA: IEEE Computer Society, 1999.  M.A. Mahowald. VLSI Analogs of Neuronal Visual Processing: A Synthesis of Form and  Function. Ph.D. Thesis, California Institute of Technology, Pasadena, CA, 1992.  K.A. Boahen. Communicating Neuronal Ensembles between Neuromorphic Chips. Analog  Integrated Circuits and Signal Processing, 1997.  T Willams. Phase Coupling and Synaptic Spread in Chains of Coupled Neuronal Oscillators.  Science, vol. 258, 1992.  G. Patel. A Neuromorphic Architecture for Modeling Intersegmental Coordination. Ph.D.  Thesis, Georgia Institute of Technology, Atlanta, GA, 1999.  A. H. Cohen. Personal communication.  D. Somers & N. Kopell. Waves and synchrony in networks of oscillators of relaxation and non-  relaxation type. Phyica D, 89:169-183, 1995.  N. Kopell & G.B. Ermentrout. Coupled oscillators and the design of central pattern generators.  Mathematical Biosciences, 90:87-109, 1988.  
Emergence of Topography and Complex  Cell Properties from Natural Images  using Extensions of ICA  Aapo HyvSrinen and Patrik Hoyer  Neural Networks Research Center  Helsinki University of Technology  P.O. Box 5400, FIN-02015 HUT, Finland  aapo. hyvarinenhut. f i, patrik. hoyerhut. f i  http://www. cis. hut. f i/proj ects/ica/  Abstract  Independent component analysis of natural images leads to emer-  gence of simple cell properties, i.e. linear filters that resemble  wavelets or Gabor functions. In this paper, we extend ICA to  explain further properties of V1 cells. First, we decompose natural  images into independent subspaces instead of scalar components.  This model leads to emergence of phase and shift invariant fea-  tures, similar to those in V1 complex cells. Second, we define a  topography between the linear components obtained by ICA. The  topographic distance between two components is defined by their  higher-order correlations, so that two components are close to each  other in the topography if they are strongly dependent on each  other. This leads to simultaneous emergence of both topography  and invariances similar to complex cell properties.  I Introduction  A fundamental approach in signal processing is to design a statistical generative  model of the observed signals. Such an approach is also useful for modeling the  properties of neurons in primary sensory areas. The basic models that we consider  here express a static monochrome image I(x, y) as a linear superposition of some  features or basis functions bi(x, y):  I(x,y) -- E bi(x,y)si (1)  i----1  where the si are stochastic coefficients, different for each image I(x, y). Est'unation  of the model in Eq. (1) consists of determining the values of si and bi(x, y) for all i  and (x, y), given a sufficient number of observations of images, or in practice, image  patches I(x, y). We restrict ourselves here to the basic case where the bi (x, y) form  an invertible linear system. Then we can invert si = wi, I  where the wi denote  the inverse filters, and  wi, I = --x,y wi(x,y)I(x,y) denotes the dot-product.  828 A. Hyviirinen and P. Hoyer  The wi(x, y) can then be identified as the receptive fields of the model simple cells,  and the si are their activities when presented with a given image patch I(x, y).  In the basic case, we assume that the si are nongaussian, and mutually independent.  This type of decomposition is called independent component analysis (ICA) [3, 9,  1, 8], or sparse coding [13]. Olshausen and Field [13] showed that when this model  is estimated with input data consisting of patches of natural scenes, the obtained  filters Wi(x,y) have the three principal properties of simple cells in VI: they are  localized, oriented, and bandpass (selective to scale/frequency). Van Hateren and  van der Schaaf [15] compared quantitatively the obtained filters wi(x, y) with those  measured by single-cell recordings of the macaque cortex, and found a good match  for most of the parameters.  We show in this paper that simple extensions of the basic ICA model explain emer-  gence of further properties of V1 cells: topography and the invariances of complex  cells. Due to space limitations, we can only give the basic ideas in this paper. More  details can be found in [6, 5, 7].  First, using the method of feature subspaces [11], we model the response of a com-  plex cell as the norm of the projection of the input vector (image patch) onto a  linear subspace, which is equivalent to the classical energy models. Then we maxi-  mize the independence between the norms of such projections, or energies. Thus we  obtain features that are localized in space, oriented, and bandpass, like those given  by simple cells, or Gabor analysis. In contrast to simple linear filters, however, the  obtained feature subspaces also show emergence of phase invariance and (limited)  shift or translation invariance. Maximizing the independence, or equivalently, the  sparseness of the norms of the projections to feature subspaces thus allows for the  emergence of exactly those invariances that are encountered in complex cells.  Second, we extend this model of independent subspaces so that we have overlapping  subspaces, and every subspace corresponds to a neighborhood on a topographic grid.  This is called topographic ICA, since it defines a topographic organization between  components. Components that are far from each other on the grid are independent,  like in ICA. In contrast, components that are near to each other are not independent:  they have strong higher-order correlations. This model shows emergence of both  complex cell properties and topography from image data.  2 Independent subspaces as complex cells  In addition to the simple cells that can be modelled by basic ICA, another important  class of cells in V1 is complex cells. The two principal properties that distinguish  complex cells from simple cells are phase invariance and (limited) shift invariance.  The purpose of the first model in this paper is to explain the emergence of such  phase and shift invariant features using a modification of the ICA model. The  modification is based on combining the principle of invariant-feature subspaces [11]  and the model of multidimensional independent component analysis [2].  Invariant feature subspaces. The principle of invariant-feature subspaces  states that one may consider an invariant feature as a linear subspace in a feature  space. The value of the invariant, higher-order feature is given by (the square of) the  norm of the projection of the given data point on that subspace, which is typically  spanned by lower-order features. A feature subspace, as any linear subspace, can  always be represented by a set of orthogonal basis vectors, say wi(x, y), i = 1, ..., m,  where ra is the dimension of the subspace. Then the value F(I) of the feature F  with input vector I(x, y) is given by F(I) m  -- i=1 < Wi' I , where a square root  Emergence of 1 properties using Extensions of lCA 829  might be taken. In fact, this is equivalent to computing the distance between the  input vector I(x, y) and a general linear combination of the basis vectors (filters)  wi(x, y) of the feature subspace [11]. In [11], it was shown that this principle, when  combined with competitive learning techniques, can lead to emergence of invariant  image features.  Multidimensional independent component analysis. In multidimensional  independent component analysis [2] (see also [12]), a linear generative model as in  Eq. (1) is assumed. In contrast to ordinary ICA, however, the components (re-  sponses) si are not assumed to be all mutually independent. Instead, it is assumed  that the si can be divided into couples, triplets or in general m-tuples, such that  the si inside a given m-tuple may be dependent on each other, but dependencies  between different m-tuples are not allowed. Every m-tuple of si corresponds to m  basis vectors bi(x, y). The m-dimensional probability densities inside the m-tuples  of si is not specified in advance in the general definition of multidimensional ICA [2l.  In the following, let us denote by J the number of independent feature subspaces,  and by $j,j = 1, ..., J the set of the indices of the si belonging to the subspace of  index j.  Independent feature subspaces. Invariant-feature subspaces can be embedded  in multidimensional independent component analysis by considering probability dis-  tributions for the m-tuples of si that are spherically symmetric, i.e. depend only  on the norm. In other words, the probability density pj(.) of the m-tuple with  index j E { 1,..., J}, can be expressed as a function of the sum of the squares of the  si, i  $j only. For simplicity, we assume further that the pj(.) are equal for all j,  i.e. for all subspaces.  Assume that the data consists of K observed image patches Ik(x,y), k = 1, ..., K.  Then the logarithm of the likelihood L of the data given the model can be expressed  K J  logL(wi(x,y),i = 1...n) = E E 1gp(E < wi,Ik >2) + Klog i det W] (2)  k----1 j----1 iSj  where P(..ie$j s) = pj(si,i  Sj) gives the probability density inside the j-th  m-tuple of si, and W is a matrix containing the filters wi(x, y) as its columns.  As in basic ICA, prewhitening of the data allows us to consider the wi(x, y) to be  orthonormal, and this implies that log[detW[ is zero [6]. Thus we see that the  likelihood in Eq. (2) is a function of the norms of the projections of I(x, y) on  the subspaces indexed by j, which are spanned by the orthonormal basis sets given  by wi(x,y),i  $j. Since the norm of the projection of visual data on practically  any subspace has a supergaussian distribution, we need to choose the probability  density p in the model to be sparse [13], i.e. supergaussian [8]. For example, we  could use the following probability distribution  logp( E s)=-ale 8] 1/2 -}-/, (3)  is is  which could be considered a multi-dimensional version of the exponential distribu-  tion. Now we see that the estimation of the model consists of finding subspaces  such that the norms of the projections of the (whitened) data on those subspaces  have maximally sparse distributions.  The introduced "independent (feature) subspace analysis" is a natural generalization  of ordinary ICA. In fact, if the projections on the subspaces are reduced to dot-  products, i.e. projections on 1-D subspaces, the model reduces to ordinary ICA  830 A. Hyvarinen and P Hoyer  (provided that, in addition, the independent components are assumed to have non-  skewed distributions). It is to be expected that the norms of the projections on  the subspaces represent some higher-order, invariant features. The exact nature of  the invariances has not been specified in the model but will emerge from the input  data, using only the prior information on their independence.  When independent subspace analysis is applied to natural image data, we can iden-  tify the norms of the projections (Y-ies s/) 1/2 as the responses of the complex  cells. If the individual filter vectors wi(x, y) are identified with the receptive fields  of simple cells, this can be interpreted as a hierarchical model where the complex  cell response is computed from simple cell responses si, in a manner similar to the  classical energy models for complex cells. Experiments (see below and [6]) show  that the model does lead to emergence of those invariances that are encountered in  complex cells.  3 Topographic ICA  The independent subspace analysis model introduces a certain dependence structure  for the components si. Let us assume that the distribution in the subspace is sparse,  which means that the norm of the projection is most of the time very near to zero.  This is the case, for example, if the densities inside the subspaces are specified as  in (3). Then the model implies that two components si and sj that belong to the  same subspace tend to be nonzero simultaneously. In other words, s and s are  positively correlated. This seems to be a preponderant structure of dependency in  most natural data. For image data, this has also been noted by Simoncelli [14].  Now we generalize the model defined by (2) so that it models this kind of depen-  dence not only inside the m-tuples, but among all 'Neighboring" components. A  neighborhood relation defines a topographic order [10]. (A different generalization  based on an explicit generative model is given in [51. ) We define the model by the  following likelihood:  logL(wi(x,y),i = 1,...,n) =  E G(Z h(i,j) < wi,I >2) + KlogldetWl (4)  k=l j=l i----1  Here, h(i, j) is a neighborhood function, which expresses the strength of the con-  nection between the i-th and j-th units. The neighborhood function can be defined  in the same way as with the self-organizing map [10]. Neighborhoods can thus be  defined as one-dimensional or two-dimensional; 2-D neighborhoods can be square  or hexagonal. A simple example is to define a 1-D neighborhood relation by  1, if[i-jl_m  h(i,j)= 0, otherwise. (5)  The constant m defines here the width of the neighborhood.  The function G has a similar role as the log-density of the independent components  in classic ICA. For image data, or other data with a sparse structure, G should be  chosen as in independent subspace analysis, see Eq. (3).  Properties of the topographic ICA model. Here, we consider for simplicity  only the case of sparse data. The first basic property is that all the components si are  uncorrelated, as can be easily proven by symmetry arguments [5]. Moreover, their  variances can be defined to be equal to unity, as in classic ICA. Second, components  si and sj that are near to each other, i.e. such that h(i, j) is significantly non-zero,  Emergence of V1 properties using Extensions of lCA 831  2  tend to be active (non-zero) at the same time. In other words, their energies s i  2  and si are positively correlated. Third, latent variables that are far from each  other re practically independent. Higher-order correlation decreases as a function  of distance, assuming that the neighborhood is defined in a way similar to that in  (5). For details, see [5].  Let us note that our definition of topography by higher-order correlations is very  different from the one used in practically all existing topographic mapping methods.  Usually, the distance is defined by basic geometrical relations like Euclidean distance  or correlation. Interestingly, our principle makes it possible to define a topography  even among a set of orthogonal vectors whose Euclidean distances are all equal.  Such orthogonal vectors are actually encountered in ICA, where the basis vectors  and filters can be constrained to be orthogonal in the whitened space.  4 Experiments with natural image data  We applied our methods on natural image data. The data was obtained by taking  16 x 16 pixel image patches at random locations from monochrome photographs  depicting wild-life scenes (animals, meadows, forests, etc.). Preprocessing consisted  of removing the DC component and reducing the dimension of the data to 160 by  PCA. For details on the experiments, see [6, 5].  Fig. I shows the basis vectors of the 40 feature subspaces (complex cells), when  subspace dimension was chosen to be 4. It can be seen that the basis vectors  associated with a single complex cell all have approximately the same orientation  and frequency. Their locations are not identical, but close to each other. The phases  differ considerably. Every feature subspace can thus be considered a generalization  of a quadrature-phase filter pair as found in the classical energy models, enabling  the cell to be selective to some given orientation and frequency, but invariant to  phase and somewhat invariant to shifts. Using 4 dimensions instead of 2 greatly  enhances the shift invariance of the feature subspace.  In topographic ICA, the neighborhood function was defined so that every neighbor-  hood consisted of a 3 x 3 square of 9 units on a 2-D torus lattice [10]. The obtained  basis vectors, are shown in Fig. 2. The basis vectors are similar to those obtained  by ordinary ICA of image data [13, 1]. In addition, they have a clear topographic  organization. In addition, the connection to independent subspace analysis is clear  from Fig. 2. Two neighboring basis vectors in Fig. 2 tend to be of the same orienta-  tion and frequency. Their locations are near to each other as well. In contrast, their  phases are very different. This means that a neighborhood of such basis vectors, i.e.  simple cells, is similar to an independent subspace. Thus it functions as a complex  cell. This was demonstrated in detail in [5].  5 Discussion  We introduced here two extensions of ICA that are especially useful for image  modelling. The first model uses a subspace representation to model invariant fea-  tures. It turns out that the independent subspaces of natural images are similar  to complex cells. The second model is a further extension of the independent sub-  space model. This topographic ICA model is a generafive model that combines  topographic mapping with ICA. As in all topographic mappings, the distance in  the representation space (on the topographic "grid") is related to some measure of  distance between represented components. In topographic ICA, the distance be-  tween represented components is defined by higher-order correlations, which gives  832 A. Hyviirinen and P Hoyer  the natural distance measure in the context of ICA.  An approach closely related to ours is given by Kohonen's Adaptive Subspace Self-  Organizing Map [11]. However, the emergence of shift invariance in [11] was condi-  tional to restricting consecutive patches to come from nearby locations in the image,  giving the input data a temporal structure like in a smoothly changing image se-  quence. Similar developments were given by FSldigtk [4]. In contrast to these two  theories, we formulated an explicit image model. This independent subspace analy-  sis model shows that emergence of complex cell properties is possible using patches  at random, independently selected locations, which proves that there is enough in-  formation in static images to explain the properties of complex cells. Moreover, by  extending this subspace model to model topography, we showed that the emergence  of both topography and complex cell properties can be explained by a single principle:  neighboring cells should have strong higher-order correlations.  References  [1] A.J. Bell and T.J. Sejnowski. The 'independent components' of natural scenes are  edge filters. Vision Research, 37:3327-3338, 1997.  [2] J.-F. Cardoso. Multidimensional independent component analysis. In Proc. IEEE Int.  Conf. on Acoustics, Speech and Signal Processing (ICASSP'98), Seattle, WA, 1998.  [3] P. Comon. Independent component analysis - a new concept? Signal Processing,  36:287-314, 1994.  [4] P. FSldiak. Learning invariance from transformation sequences. Neural Computation,  3:194-200, 1991.  [5] A. Hyv'inen and P. O. Hoyer. Topographic independent component analysis. 1999.  Submitted, available at http://www.cis.hut.fi/-aapo/.  [6] A. Hyv'inen and P.O. Hoyer. Emergence of phase and shift invariant features by  decomposition of natural images into independent feature subspaces. Neural Compu-  tation, 2000. (in press).  [7] A. Hyv'inen, P.O. Hoyer, and M. Inki. The independence assumption: Analyzing  the independence of the components by topography. In M. Girolami, editor, Advances  in Independent Component Analysis. Springer-Verlag, 2000. in press.  [8] A. Hyv'inen and E. Oja. A fast fixed-point algorithm for independent component  analysis. Neural Computation, 9(7):1483-1492, 1997.  [9] C. Jutten and J. Herault. Blind separation of sources, part I: An adaptive algorithm  based on neuromimetic architecture. Signal Processing, 24:1-10, 1991.  [10] T. Kohonen. Self-Organizing Maps. Springer-Verlag, Berlin, Heidelberg, New York,  1995.  [11] T. Kohonen. Emergence of invariant-feature detectors in the adaptive-subspace self-  organizing map. Biological Cybernetics, 75:281-291, 1996.  [12] J. K. Lin. Factorizing multivariate function classes. In Advances in Neural Information  Processing Systems, volume 10, pages 563-569. The MIT Press, 1998.  [13] B. A. Olshausen and D. J. Field. Emergence of simple-cell receptive field properties  by learning a sparse code for natural images. Nature, 381:607-609, 1996.  [14] E. P. $imoncelli and O. Schwartz. Modeling surround suppression in V1 neurons  with a statistically-derived normalization model. In Advances in Neural Information  Processing Systems 11, pages 153-159. MIT Press, 1999.  [15] J. H. van Hateren and A. van der SchaM. Independent component filters of natural  images compared with simple cells in primary visual cortex. Proc. Royal Society  set. B, 265:359-366, 1998.  Emergence of V! properties using Extensions of lCA 833  Bl/l  Figure 1: Independent subspaces of natural image data. The model gives Gabor-  like basis vectors for image windows. Every group of four basis vectors corresponds  to one independent feature subspace, or complex cell. Basis vectors in a subspace  are similar in orientation, location and frequency. In contrast, their phases are very  different.  Figure 2: Topographic ICA of natural image data. This gives Gabor-like basis vec-  tors as well. Basis vectors that are similar in orientation, location and/or frequency  are close to each other. The phases of nearby basis vectors are very different, giving  each neighborhood properties similar to a complex cell.  
Leveraged Vector Machines  Yoram Singer  Hebrew University  singer@cs.huj i. ac. il  Abstract  We describe an iterative algorithm for building vector machines used in  classification tasks. The algorithm builds on ideas from support vector  machines, boosting, and generalized additive models. The algorithm can  be used with various continuously differential functions that bound the  discrete (0-1) classification loss and is very simple to implement. We test  the proposed algorithm with two different loss functions on synthetic and  natural data. We also describe a norm-penalized version of the algorithm  for the exponential loss function used in AdaBoost. The performance of  the algorithm on natural data is comparable to support vector machines  while typically its running time is shorter than of SVM.  1 Introduction  Support vector machines (SVM) [1, 13] and boosting [10, 3, 4, 1 l] are highly popular and  effective methods for constructing linear classifiers. The theoretical basis for SVMs stems  from Vapnik's seminal on learning and generalization [12] and has proved to be of great  practical usage. The first boosting algorithms [10, 3], on the other hand, were developed  to answer certain fundamental questions about PAC-learnability [6]. While mathemati-  cally beautiful, these algorithms were rather impractical. Later, Freund and Schapire [4]  developed the AriaBoost algorithm, which proved to be a practically useful meta-learning  algorithm. AriaBoost works by making repeated calls to a weak learner. On each call the  weak learner generates a single weak hypothesis, and these weak hypotheses are combined  into an ensemble called strong hypothesis. Recently, Schapire and Singer [11] studied a  simple generalization of AriaBoost in which a weak-hypothesis can assign a real-valued  confidence to each prediction. Even more recently, Friedman, Hastie, and Tibshirani [5]  presented an alternative view of boosting from a statistical point of view and also described  a new family of algorithms for constructing generalized additive models of base learners  in a similar fashion to AriaBoost. The work of Friedman, Hastie, and Tibshirani generated  lots of attention and motivated research in classification algorithms that employ various  loss functions [8, 7].  In this work we combine ideas from the research mentioned above and devise an alternative  approach to construct vector machines for classification. As in SVM, the base predictors  that we use are Mercer kernels. The value of a kernel evaluated at an input pattern, i.e.,  the dot-product between two instances embedded in a high-dimensional space, is viewed  as a real-valued prediction. We describe a simple extension to additive models in which the  prediction of a base-learner is a linear transformation of a given kernel. We then describe  an iterative algorithm that greedily adds kernels. We derive our algorithm using the expo-  nential loss function used in AriaBoost and the loss function used by Friedman, Hastie, and  Tibshirani [5] in "LogitBoost". For brevity we call the resulting classifiers boosted vector  machines (BVM) and logistic vector machines (LVM). We would like to note in passing  Leveraged Vector Machines 611  that the resulting algorithms are not boosting algorithms in the PAC sense. For instance,  the weak-learnability assumption that the weak-learner can always find a weak-hypothesis  is violated. We therefore adopt the terminology used in [2] and call the resulting classifiers  leveraged vector machines.  The leveraging procedure we give adopts the chunking technique from SVM. After present-  ing the basic leveraging algorithms we compare their performance with SVM on synthetic  data. The experimental results show that the leveraged vector machines achieve similar  performance to SVM and often the resulting vector machines are smaller than the ones  obtained by SVM. The experiments also demonstrate that BVM is especially sensitive to  (malicious) label noise while LVM seems to be more insensitve. We also describe a simple  norm-penalized extension of BVM that provides a partial solution to overfitting in the p-  resence of noise. Finally, we give results of experiments performed with natural data from  the UCI repository and conclude.  2 Preliminaries  Let $ = ((Xl, y),..., (Xm, Ym)) be a sequence of training examples where each instance  xi belongs to a domain or instance space A', and each label Yi is in {-1, +1}. (The  methods described in this paper to build vector machines and SVMs can be extended to  solve multiclass problems using, for instance, error correcting output coding. Such methods  are beyond the scope of this paper and will be discussed elsewhere). For convenience, we  will use i to denote (Yi + 1)/2 E {0, 1}.  As is boosting, we assume access to a weak or base learning algorithm which accepts as  input a weighted sequence of training examples $. Given such input, the weak learner  computes a weak (or base) hypothesis h. In general, h has the form h: X - 11. We  interpret the sign of h(x) as the predicted label (-1 or +1) to be assigned to instance x,  and the magnitude Ih(x) l as the "confidence" in this prediction.  To build vector machines we use the notion of confidence-rated predictions, take for base  hypotheses sample-based Mercer kernels [13], and define the confidence (i.e., the magni-  tude of prediction) of a base learner to be the value of its dot-product with another instance.  The sign of the prediction is set to be the label of the corresponding instance. Formally,  for each base hypothesis h there exist (xj,yj)  S such that h(x) = yjK(xj,x) and  K(u,v) defines an inner product in a feature space: K(u,v) = E=l akJk(tt)bk(V).  We denote the function induced by an instance label pair (x j, yj) with a kernel K by  ckj (x) = yjK(xj, x). Our goal is to find a classifier f(x), called a strong hypothesis in the  context of boosting algorithms, of the form f(x) = y'tT=l atht (x) + , such that the signs  of the predictions of the classifier should agree, as much as possible, with the labels of the  training instances.  The leverage algorithm we describe maintains a distribution D over {1,..., m}, i.e., over  the indices of $. This distribution is simply a vector of non-negative weights, one weight  per example and is an exponential function of the classifier f which is built incrementally,  1 m  D(i) = exp(-yif(xi)) where Z= Zexp(-yif(xi)). (1)  i----1  For a random function g of the input instances and the labels, we denote the sample ex-  pectation of g according to D by ED(g) - Eim=l D(i)g(xi, Yi). We also use this notation  to denote the expectation of matrices of random functions. We will convert a confidence-  rated classifier f into a randomized predictor by using the soft-max function and denote it  by P(xi) where  exp(f(xi)) 1  P(xi) = exp(f(xi)) +exp(-f(xi)) - 1 +exp(-2f(xi)) (2)  612 Y. Sinr  3 The leveraging algorithm  The basic procedure to construct leveraged vector machines builds on ideas from [ 11, $] by  extending the prediction to be a linear function of the base classifiers. The algorithm works  in rounds, constructing a new classifier ft from the previous one ft-1 by adding a new  base hypothesis ht to the current classifier, ft. Denoting by Dt and Pt+l the distribution  and probability given by Eqn. (1) and Eqn. (2) using ft and ft+l, the algorithm attempts to  minimize either the exponential function that arise in AdaBoost:  exp(-yift(xi)) =  exp(-yi(ft-l(Xi) + atht(xi) +/t))  i----1  Dt(i) exp (-yi(atht(xi) +/t)),  m  or the logistic loss  (3)  function:  L - -'log(1 +exp(-2yift(xi))) (4)  i----1  m  Z log (1 + exp (-2yi(ft- (xi) + atht(xi) +/t)))  i=1  m  -- Z (i log(Pt+1(xi)) + (1 - i)log(1 - Pt+l(Xi)))   i----1  and for L  We initialize fo(x) to be zero everywhere and run the procedure for a predefined num-  ber of rounds T. The final classifier is therefore fT(x) = E=l(atht(x) +/t) =  / + E=i atht(x) where / = Et/t  We would like to note parenthetically that it  is possible to use other loss functions that bound the 0-1 (classification) loss (see for in-  stance [8]). Here we focus on the above loss functions, L and Z. Fixing ft- and ht, these  functions are convex in at and/t which guarantees, under mild conditions (details omitted  due to lack of space), the uniqueness of at and fit.  On each round we look for the current base hypothesis ht that will reduce the loss function  (Z or L) the most. As discussed before, each input instance xj defines a function j (x)  and is a candidate for ht (x). In general, there is no close form solution for Eqn. (3) and (5)  and finding a and/ for each possible input instance is time consuming. We therefore use a  quadratic approximation for the loss functions. Using the quadratic approximation, for each  j we can find a and/ analytically and calculate the reduction in the loss function. Let  VZ (oz oz r  - - (-'d, oZ ) be the column vectors of the partial derivatives  o,o) andVL o: o: r  of Z and L w.r.t a and/ (fixing ft-1 and ht). Similarly, let V2Z and 72L be the 2 x 2  matrices of second order derivatives of Z and L with respect to a and/. Then, quadratic  approximation yields that (o 6/)T = (V2Z)- V'Z and (o 6/)T = (V2L)-I VL. On  each round t we maintain a distribution Dt which is defined from ft as given by Eqn. (1)  and conditional class probability estimates Pt (xi) as given by Eqn. (2). Solving the linear  equation above for a and/ for each possible instance is done by setting ht (x) - c)j (x),  we get for Z  (Otj [ ( 32' J )]-IED,[y( J )] , (6)  ---- [ED, [P(1-P)( g} g}; )]]-1 ED,[(-P)( J )]  (7)  = (5)  Leveraged Vector Machines 613  Figure 1: Comparison of the test error as  a function of number of leveraging rounds  when using full numerical search for c and  r, a "one-step" numerical search based on a  quadratic approximation of the loss function,  and a one-step search with chunking of the  instances.  Note that the equations above share much in common and require, after pre-computing  ? (zi), the same amount of computation time.  After calculating the value of c and  for each instance (arj, /j), we simply evaluate the  corresponding value of the loss function, choose the instance (a:j,, /j.) that attains the  minimal loss, and set ht = b.. We then numerically search for the optimal value of c  and  by iterating Eqn. (6) or Eqn. (7) and summing the values into ct and t. We would  like to note that typically two or three iterations suffice and we can save time by using the  value of c and  found using the quadratic approximation without a full numerical search  for the optimal value of c and . (See also Fig. 1.) We repeat this process for T rounds  or until no instance can serve as a base hypothesis. We note that the same instance can be  chosen more than once, although not in consecutive iterations, and typically only a small  fraction of the instances is actually used in building f. Roughly speaking, these instances  are the "support patterns" of the leveraged machines although they are not necessarily the  geometric support patterns.  As in SVMs, in order to make the search for a base hypothesis efficient we pre-compute and  store K(z, c ) for all pairs a:  a:  from S. Storing these values require IS[ 2 space, which  might be prohibited in large problems. To save space, we employ the idea of chunking  used in SVM. We partition $ into r blocks $1, $2,..-, $,- of about the same size. We  divide the iterations into sub-groups such that all iterations belonging to the ith sub-group  use and evaluate kernels based on instances from the ith block only. When switching to a  new block k we need to compute the values K(z, c ) for a: E $ and a:   Sk. This division  into blocks might be more expensive since we typically use each block of instances more  than once. However, the storage of the kernel values can be done in place and we thus save  a factor of r in memory requirements. In practice we found that chunking does not hurt  the performance. In Fig. 1 we show the test error as a function of number of rounds when  using (a) full numerical search to determine c and  on each round, (b) using the quadratic  approximation ("one-step") to find c and , and (c) using quadratic approximation with  chunking. The number of instances in the experiment is 1000, each block for chunking is  of size 100, and we switch to a different block every 100 iterations. (Further description of  the data is given in the next section.) In this example, after 10 iterations, there is virtually  no difference in the performance of the different schemes.  4 Experiments with synthetic data  In this section we describe experiments with synthetic data comparing different aspects  of leveraged vector machines to SVMs. The original instance space is two dimensional  where the positive class includes all points inside a circle of radius R, i.e., an instance  (Ul,U2) ( ]i 2 is labeled +1 iff u 2 + u <_ R. The instances were picked at random  according to a zero mean unit variance normal distribution and R was set such exactly half  of the instances belong to the positive class. In all the experiments described in this section  we generated 10 groups of training and test sets each of which includes 1000 train and test  examples. Overall, there are 10,000 training examples and 10,000 test examples. The  614 Y. Singer  Figure 2: Performance comparison of SVM and BVM as a function of the training data  size (left), the dimension of the kernels (middle), and the number of redundant features.  Figure 3: Train and test er-  rors for SVM, LVM, and  BVM as a function of the  label noise.  average variance of the estimates of the empirical errors across experiments is about 0.2%.  For SVM we set the regularization parameter, C', to 100 and used 500 iterations to build  leveraged machines. In all the experiments without noise the results for BVM and LVM  were practically the same. We therefore only compare BVM to SVM in Fig. 2. Unless said  otherwise we used polynomials of degree two as kernels: K(a:,' z) = (z  z ' + 1) 2. Hence,  the data is separable in the absence of noise.  In the first experiment we tested the sensitivity to the number of training examples by omit-  ting examples from the training data (without any modification to the test sets). On the left  part of Fig. 2 we plot the test error as a function of the number of training examples. The  test error of BVM is almost indistinguishable from the error of SVM and performance of  both methods improves very fast as a function of training examples. Next, we compared  the performance as a function of the dimension of polynomial constituting the kernel. We  ran the algorithms with kernels of the form K(:r,':r) = (:r. z' + 1) a for d = 2,..., 8.  The results are depicted in the middle plots of Fig. 2. Again, the performance of BVM and  SVM is very close (note the small scale of the y axis for the test error in this experimen-  t). To conclude the experiments with clean, realizable, data we checked the sensitivity to  irrelevant features of the input. Each input instance (Ul, u2) was augmented with random  elements us,.. , ut to form an input vector of dimension I. The right hand side graphs of  Fig. 2 shows the test error as a function of I for I = 2,..., 12. Once more we see that the  performance of both algorithms is very similar.  We next compared the performance of the algorithms in the presence of noise. We used ker-  nels of dimension two and instances without redundant features. The label of each instance  was flipped with probability e. We ran 15 sets of experiments, for e = 0.01,..., 0.15. As  before, each set included 10 runs each of which used 1000 training examples and 1000 test  examples. In Fig. 3 we show the average training error (left), and the average test error  (right), for each of the algorithms. It is apparent from the graphs that BVMs built based  on the exponential loss are much more sensitive to noise than SVMs and LVMs, and their  generalization error degrades significantly, even for low noise rates. The generalization er-  ror of LVMs is, on the other hand, only slightly worse than the that of SVMs, although the  Leveraged Vector Machines 615  t  Figure 4: The training error, test error, and the cumulative L1 norm (Zt,=l la[) as a  function of the number of leveraging iterations for LVM,BVM, and PBVM.  only algorithmic difference in constructing BVMs and LVMs is in the loss function. The  fact that LVMs exhibit performance similar to SVM can be partially attributed to the fact  that the asymptotic behavior of their loss functions is the same.  5 A norm-penalized version  One of the problems with boosting and the corresponding leveraging algorithm with the  exponential loss described here, is that it might increase the confidence on a few instances  while misclassifying many other instances, albeit with a small confidence. This often hap-  pens on late rounds, during which the distribution Dt (i) is concentrated on a few examples,  and the leveraging algorithm typically assigns a large weight to a weak hypothesis that does  not effect most of the instances. It is therefore desired to control the complexity of the lever-  aged classifiers by limiting the magnitude of base hypotheses' weights. Several methods  have been proposed to limit the confidence of AdaBoost, using, for instance, regulariza-  tion (e.g., [9]) or "smoothing" the predictions [11]. Here we propose a norm-penalized  method for BVM that is very simple to implement and maintains the convexity properties  of the objective function. Following the idea Cortes and Vapnik's of SVMs in the non-  separable case [1] we add the following penalization term: 7o exp (y'tT= lat[V). Simple  algebric manipulation implies that the objective function at the tth round for BVMs with  the penalization term above is,  m  2 = y. Dt(i) exp (-yi(atht(xi) + Bt)) + 7t exp(latlP)   i=1  (8)  It is also easy to show that the penalty parameter should be updated after each round is:  7t = 7t- exp(lat-llv)/Zt-1. Since Zt < 1, unless there is no kernel function better  than random, 7t typically increases as a function of t, forcing more and more the new  weights to be small. Note that Eqn. (8) implies that the search for a base predictor ht  and weights at, fit on each round can still be done independently of previous rounds by  maintaining the distribution Dt and a single regularization value 7t. The penalty term for  p - 1 and p -- 2 simply adds a diagonal term to the matrix of second order derivatives  (Eqn. (6)) and the algorithm follows the same line (details omitted). For brevity we call  the norm-penalized leveraging procedure PBVM. In Fig. 4 we plot the test error (right),  training error (middle), and Y't latl as functions of number of rounds for LVM, BVM,  and PBVM with p = i 7o = 0.01. The training set in this example was made small on  purpose (200 examples) and was contaminated with 5% label noise. In this very small  example both LVM and BVM overfit while PBVM stops increasing the weights and finds  a reasonably good classifier. The plots demonstrate that the norm-penalized version can  safeguard against overfitting by preventing the weights from growing arbitrarily large, and  that the effect of the penalized version is very similar to early stopping. We would like  616 Y. Singer  SVM LVM BVM RBVM SVM LVM BVM PBVM  #Example  Data Set & Size Size Size Size Error Error Error Error  (Source) #Feature  labor (uci) 57: 16 12.5 13.7 16.1 13.6 6.0 14.0 14.0 12.0  echocard. (uci) 74: 12 7.8 13.0 12.6 12.4 8.6 5.7 10.0 10.0  bridges (uci) 102: 7 27.2 20.2 18.5 17.9 15.0 15.0 23.0 14.0  hepatitis (uci) 155: 19 41.2 13.5 17.4 14.0 21.3 22.0 22.7 22.0  horse-colic (uci) 300: 23 122.0 13.0 13.0 13.0 14.7 14.7 14.7 13.2  liver (uci) 345: 6 228.6 11.3 12.8 10.7 33.8 35.6 33.5 35.6  ionosphere (uci) 351: 34 63.4 58.9 67.9 59.1 13.7 13.1 16.9 13.7  vote (uci) 435: 16 37.0 37.0 41.0 37.0 4.4 5.2 5.9 5.2  ticketl (att) 556: 78 48.1 84.6 89.3 82.3 8.4 3.3 11.5 5.1  ticket2 (att) 556: 53 52.6 77.1 75.4 74.0 6.6 6.4 8.0 6.4  ticket3 (att) 556: 61 46.1 76.2 77.8 73.3 6.9 4.9 7.6 6.7  bands (uci) 690: 39 265.5 78.2 76.4 75.6 32.8 33.2 34.3 33.3  bmast-wisc (uci) 699: 9 49.3 26.5 24.4 24.0 3.5 3.6 4.1 4.1  pinna (uci) 768: 8 360.7 47.7 30.3 22.8 23.0 22.6 23.2 22.1  german (uci) 1000: 10 485.2 89.8 96.5 87.0 23.5 24.0 23.8 24.1  weather (uci) 1000: 35 562.0 52.0 52.0 52.0 25.9 25.4 25.4 25.4  network (art) 2600: 35 1031.0 42.0 43.0 42.0 24.8 21.2 23.5 21.2  splice (uci) 3190: 60 318,0 153.0 156.0 153.0 8.0 8.4 8.4 8.4  boa (att) 5000: 68 637.0 183.0 178.0 160.0 41.5 40.8 40.8 41.0  Table 1: Summary of results for a collection of binary classification problems.  to note that we found experimentally that the norm-penalized version does compensate for  incorrect estimates of c and/9 due to malicious label noise. The experimental results given  in the next section show, however, that it does indeed help in preventing overfitting when  the training set is small.  6 Experiments with natural data  We compared the practical performance of leveraged vector machines with SVMs on a  collection of nineteen dataset from the UCI machine learning repository and AT&T net-  working and marketing data. For SVM we set (7 = 100. We built each of the leveraged  vector machines using 500 rounds. For PBVM we used again p -- 1 and 7o = 0.01. We  used chunking in building the leveraged vector machines, dividing each training set into 10  blocks. For all the datasets, with the exception of "boa", we used 10-fold cross validation  to calculate the test error. (The dataset "boa" has 5000 training examples and 6000 test  examples.) The performance of SVM, LVM, and PBVM seem comparable. In fact, with  the exception of a very few datasets the differences in error rates are not statistically signif-  icant. Of the three methods (SVM, PBVM, and LVM), LVM is the simplest to implement  the time required to build an LVM is typically much shorter than that of an SVM. It is  also worth noting that the size of leveraged machines is often smaller than the size of the  corresponding SVM. Finally, it apparent that PBVMs frequently yield better results than  BVMs, especially for small and medium size datasets.  References  [1] Corinna Cortes and Vladimir Vapnik. Support-vector networks. Machine Learning, 20(3):273-297, September 1995.  [2] N. Duffy and D. Helmbold. A geometric approach to leveraging weak learners. EuroCOLT '99.  [3] Yoav Freund. Boosting a weak learning algorithm by majority. Information and Computation, 121(2):256-285, 1995.  [4] Yoav Freund and Robert E. Schapire. A decision-theoretic generalization of on-line learning and an application to boosting.  Journal of Computer and System Sciences, 55(1 ): 119-139, August 1997.  [5] J. Friedman, T. Hastie, and R. Tibshirani. Additive logistic regression: a statistical view of boosting. Tech. Report, 1998.  [6] Michael Kearns and Leslie G. Valiant. Cryptographic limitations on learning Boolean formulae and finite automata. Journal  of the Association for Computing Machiner)', 41(1):67-95, January 1994.  [7] John D. Lafferty. Additive models, boosting and inference for generalized divergences. In Proceedings of the Twelfth Annual  Conference on Computational Learning Theor)', 1999.  [8] L. Mason, J. Baxter, P. Bartlett, and M. Fman. Doom II. Technical report, Depa. of Sys. Eng. ANU 1999.  [9] G. Ratsch, TOnoda, and K.-R. Mfiller. Regularizing adaboost. In Advances in Neuraiinfo. Processing Systems 12, 1998.  [10] Robert E. Schapire. The strength of weak learnability. Machine Learning, 5(2): 197-227, 1990.  [11] Robert E. Schapire and Yoram Singer. Improved boosting algorithms using confidence-rated predictions. COLT'98.  [ 12] V.N. Vapnik. Estimation of Dependences Based on Empirical Data. Springer-Verlag, 1982.  [13] Vladimir N. Vapnik. The Nature of Statistical Learning Theor),. Springer, 1995.  
Optimal sizes of dendritic and axonal arbors  Dmitri B. Chklovskii  Sloan Center for Theoretical Neurobiology  The Salk Institute, La Jolla, CA 92037  tnitya @ salk. edu  Abstract  I consider a topographic projection between two neuronal layers with dif-  ferent densities of neurons. Given the number of output neurons con-  nected to each input neuron (divergence or fan-out) and the number of  input neurons synapsing on each output neuron (convergence or fan-in) I  determine the widths of axonal and dendritic arbors which minimize the  total volume of axons and dendrites. My analytical results can be sum-  marized qualitatively in the following rule: neurons of the sparser layer  should have arbors wider than those of the denser layer. This agrees with  the anatomical data from retinal and cerebellar neurons whose morphol-  ogy and connectivity are known. The rule may be used to infer connec-  tivity of neurons from their morphology.  1 Introduction  Understanding brain function requires knowing connections between neurons. However,  experimental studies of inter-neuronal connectivity are difficult and the connectivity data  is scarce. At the same time neuroanatomists possess much data on cellular morphology  and have powerful techniques to image neuronal shapes. This suggests using morphologi-  cal data to infer inter-neuronal connections. Such inference must rely on rules which relate  shapes of neurons to their connectivity.  The purpose of this paper is to derive such rule for a frequently encountered feature in the  brain organization: a topographic projection. Two layers of neurons are said to form a topo-  graphic projection if adjacent neurons of the input layer connect to adjacent neurons of the  output layer, Figure 1. As a result, output neurons form an orderly map of the input layer.  I characterize inter-neuronal connectivity for a topographic projection by divergence and  convergence factors defined as follows, Figure 1. Divergence, D, of the projection is the  number of output neurons which receive connections from an input neuron. Convergence,  IY, of the projection is the number of input neurons which connect with an output neuron. I  assume that these numbers are the same for each neuron in a given layer. Furthermore, each  neuron makes the required connections with the nearest neurons of the other layer. In most  cases, this completely specifies the wiring diagram.  A typical topographic wiring diagram shown in Figure 1 misses an important biological de-  tail. In real brains, connections between cell bodies are implemented by neuronal processes:  axons which carry nerve pulses away from the cell bodies and dendrites which carry signals  Optimal Sizes of Dendritic and Axonal Arbors 109  Figure 1: Wiring diagram of a topographic projection between input (circles) and output  (squares) layers of neurons. Divergence, D, is the number of outgoing connections (here,  D = 2) from an input neuron (wavey lines). Convergence, C', is the number of connections  incoming (here, C' = 4) to an output neuron (bold lines). Arrow shows the direction of  signal propagation.  wiring diagram  ...999999999999.-.  Type II  Figure 2: Two different arrangements implement the same wiring diagram. (a) Topographic  wiring diagram with C' = 6 and D = 1. (b) Arrangement with wide dendritic arbors and  no axonal arbors (Type I) (c) Arrangement with wide axonal arbors and no dendritic arbors  (Type II). Because convergence exceeds divergence type I has shorter wiring than type II.  towards cell bodies.[ 1] Therefore each connection is interrupted by a synapse which sepa-  rates an axon of one neuron from a dendrite of another. Both axons and dendrites branch  away from cell bodies forming arbors.  In general, a topographic projection with given divergence and convergence may be imple-  mented by axonal and dendritic arbors of different sizes, which depend on the locations of  synapses. For example, consider a wiring diagram with D = 1 and C' = 6, Figure 2a. Nar-  row axonal arbors may synapse onto wide dendritic arbors, Figure 2b or wide axonal arbors  may synapse onto narrow dendritic arbors, Figure 2c. I call these arrangements type I and  type II, correspondingly. The question is: which arbor sizes are preferred?  I propose a rule which specifies the sizes of axonal arbors of input neurons and dendritic  arbors of output neurons in a topographic projection: High divergence/convergence ratio  favors wide axonal and narrow dendritic arbors while low divergence/convergence ratio  favors narrow axonal arbors and wide dendritic arbors. Alternatively, this rule may be for-  mulated in terms of neuronal densities in the two layers: Sparser layer has wider arbors.  In the above example, divergence/convergence (and neuronal density) ratio is 1/6 and, ac-  cording to the rule, type I arrangement, Figure 2b, is preferred.  In this paper I derive a quantitative version of this rule from the principle of wiring economy  which can be summarized as follows. [2, 3, 4, 5, 6] Space constraints require keeping the  brain volume to a minimum. Because wiring (axons and dendrites) takes up a significant  fraction of the volume, evolution has probably designed axonal and dendritic arbors in a  way that minimizes their total volume. Therefore we may understand the existing arbor  sizes as a result of wiring optimization.  110 D. B. Chklovsla'i  To obtain the rule I formulate and solve a wiring optimization problem. The goal is to find  the sizes of axons and dendrites which minimize the total volume of wiring in a topographic  wiring diagram for fixed locations of neurons. I specify the wiring diagram with divergence  and convergence factors. Throughout most of the paper I assume that the cross-sectional  area of dendrites and axons are constant and equal. Therefore, the problem reduces to the  wire length minimization. Extension to unequal fiber diameters is given below.  2 Topographic projection in two dimensions  Consider two parallel layers of neurons with densities n and n2. The topographic wiring  diagram has divergence and convergence factors, D and 6', requiring each input neuron to  connect with D nearest output neurons and each output neuron with 6' nearest input neurons.  Again, the problem is to find the arrangement of arbors which minimizes the total length of  axons and dendrites. For different arrangements I compare the wirelength per unit area, L. I  assume that the two layers are close to each other and include only those parts of the wiring  which are parallel to the layers.  I start with a special case where each input neuron connects with only one output neuron  (D = 1). Consider an example with 6' = 16 and neurons arranged on a square grid in  each layer, Figure 3a. Two extreme arrangements satisfy the wiring diagram: type I has  wide dendritic arbors and no axonal arbors, Figure 3b; type II has wide axonal arbors and  no dendritic arbors, Figure 3c. I take the branching angles equal to 120 , an optimal value  for constant crossectional area.[4] Assuming "point" neurons the ratio of wirelength for type  I and type II arrangements:  L   -. 0.57. (1)  Lii  Thus, the type I arrangement with wide dendritic arbors has shorter wire length. This con-  clusion holds for other convergence values much greater than one, provided D -- 1. How-  ever, there are other arrangements with non-zero axonal arbors that give the same wire  length. One of them is shown in Figure 3d. Degenerate arrangements have axonal arbor  width 0 < sa < 1/f' where the upper bound is given by the approximate inter-neuronal  distance. This means that the optimal arbor size ratio for D = 1  8d 12  -- > (2)  8a  By using the symmetry in respect to the direction of signal propagation I adapt this result  for the 6' = 1 case. For D > 1, arrangements with wide axonal arbors and narrow dendritic  arbors (0 < sa < 1/x/h--) have minimal wirelength. The arbor size ratio is  8d 21  -- < . (3)  8a  Next, I consider the case when both divergence and convergence are greater than one.  Due to complexity of the problem I study the limit of large divergence and convergence  (D, 6' >> 1). I find analytically the optimal layout which minimizes the total length of ax-  ons and dendrites.  Notice that two neurons may form a synapse only if the axonal arbor of the input neuron  overlaps with the dendritic arbor of the output neuron in a two-dimensional projection, Fig-  ure 4. Thus the goal is to design optimal dendritic and axonal arbors so that each dendritic  arbor intersects 6' axonal arbors and each axonal arbor intersects D dendritic arbors.  To be specific, I consider a wiring diagram with convergence exceeding divergence, 6' > D  (the argument can be readily adapted for the opposite case). I make an assumption, to be  Optimal Sizes of Dendritic and,4xona1.4 rbors 111  a) b)  wiring diagram Type I  c) )  Type II Type I'  Figure 3: Different arrangements implement the same wiring diagram in two dimensions.  (a) Topographic wiring diagram with D = 1 and C = 16. (b) Arrangement with wide  dendritic arbors and no axonal arbors, Type I. (c) Arrangement with wide axonal arbors and  no dendritic arbors, Type II. Because convergence exceeds divergence type I has shorter  wiring than type II. (d) Intermediate arrangement which has the same wire length as type I.  $d  Figure 4: Topographic projection between the layers of input (circles) and output (squares)  neurons. For clarity, out of the many input and output neurons with overlapping arbors only  few are shown. The number of input neurons is greater than the number of output neurons  (CID > 1). Input neurons have narrow axonal arbors of width sa connected to the wide but  sparse dendritic arbors of width sa. Sparseness of the dendritic arbor is given by sa because  all the input neurons spanned by the dendritic arbor have to be connected.  112 D. B. Chklovski  verified later, that dendritic arbor diameter sa is greater than axonal one, sa. In this regime  each output neuron's dendritic arbor forms a sparse mesh covering the area from which sig-  nals are collected, Figure 4. Each axonal arbor in that area must intersect the dendritic arbor  mesh to satisfy the wiring diagram. This requires setting mesh size equal to the axonal arbor  diameter.  By using this requirement I express the total length of axonal and dendritic arbors as a func-  tion of only the axonal arbor size, sa. Then I find the axonal arbor size which minimizes the  total wirelength. Details of the calculation will be published elsewhere. Here, I give an in-  tuitive argument for why in the optimal layout both axonal and dendritic size are non-zero.  Consider two extreme layouts. In the first one, dendritic arbors have zero width, type II. In  this arrangement axons have to reach out to every output neuron. For large convergence,  C >> 1, this is a redundant arrangement because of the many parallel axonal wires whose  signals are eventually merged. In the second layout, axonal arbors are absent and dendrites  have to reach out to every input neuron. Again, because each input neuron connects to many  output neurons (large divergence, D >> 1) many dendrites run in parallel inefficiently car-  rying the same signal. A non-zero axonal arbor rectifies this inefficiency by carrying signals  to several dendrites along one wire.  I find that the optimal ratio of dendritic and axonal arbor diameters equals to the square  root of the convergence/divergence ratio, or, alternatively, to the square root of the neuronal  density ratio:  -- = = (4)  8a  Since I considered the case with C > D this result also justifies the assumption about axonal  arbors being smaller than dendritic ones.  For arbitrary axonal and dendritic cross-sectional areas, he and ha, expressions of this Sec-  tion are modified. The wiring economy principle requires minimizing the total volume oc-  cupied by axons and dendrites resulting in the following relation for the optimal arrange-  ment:  8d _  C  12f- (5)  - V = v  Notice that in the optimal arrangement the total axonal volume of input neurons is equal to  the total dendritic volume of the output neurons.  3 Discussion  3.1 Comparison of the theory with anatomical data  This theory predicts a relationship between the con-/divergence ratio and the sizes of axonal  and dendritic arbors. I test these predictions on several cases of topographic projection in  two dimensions. The predictions depend on whether divergence and convergence are both  greater than one or not. Therefore, I consider the two regimes separately.  First, I focus on topographic projections of retinal neurons whose divergence factor is equal  or close to one. Because retinal neurons use mostly graded potentials the difference between  axons and dendrites is small and I assume that their cross-sectional areas are equal. The  theory predicts that the ratio of dendritic and axonal arbor sizes must be greater than the  square root of the input/output neuronal density ratio, Sd/Sa > (n In2) /2 (Eq.2).  I represent the data on the plot of the relative arbor diameter, Sd/Sa, vs. the square root  of the relative densities, (n/n2) /, (Figure 5). Because neurons located in the same layer  may belong to different classes, each having different arbor size and connectivity, I plot data  Optimal Sizes of Dendritic and Axona1.4 rbors 113  D=-I  v  & o  0.02 C--]  Figure 5: Anatomical data for several pairs of retinal cell classes which form topographic  projections with D = 1. All the data points fall in the triangle above the sd/sa =  (n/n2) /2 line in agreement with the theoretical prediction, Eq.2. The following data has  been used: o - midget bipolar-} midget ganglion,J7, 8, 11]; LI -diffuse bipolar-} parasol  ganglion,J7, 9]; 7 - rods -} rod bipolar, J10]; A - cones -} HI horizontals, J12]; o - rods -}  telodendritic arbors of HI horizontals,[ 13].  from different classes separately. All the data points lie above the Sd/Sa = (n/n) / line  in agreement with the prediction.  Second, I apply the theory to cerebellar neurons whose divergence and convergence are both  greater than one. I consider a projection from granule cell axons (parallel fibers) onto Purk-  inje cells. Ratio of granule cells to Purkinje cells is 3300,[14], indicating a high conver-  gence/divergence ratio. This predicts a ratio of dendritic and axonal arbor sizes of 58. This  is qualitatively in agreement with wide dendritic arbors of Purkinje cells and no axonal ar-  bors on parallel fibers.  Quantitative comparison is complicated because the projection is not strictly two-  dimensional: Purkinje dendrites stacked next to each other add up to a significant third di-  mension. Naively, given that the dendritic arbor size is about 400/m Eq.4 predicts axonal  arbor of about 7/m. This is close to the distance between two adjacent Purkinje cell arbors  of about 9/m. Because the length of parallel fibers is greater than 7ttm absence of axonal  arbors comes as no surprise.  3.2 Other factors affecting arbor sizes  One may argue that dendrites and axons have functions other than linking cell bodies to  synapses and, therefore, the size of the arbors may be dictated by other considerations. Al-  though I can not rule out this possibility, the pr/mary function of axons and dendrites is to  connect cell bodies to synapses in order to conduct nerve pulses between them. Indeed, if  neurons were not connected more sophisticated effects such as non-linear interactions be-  tween different dendritic inputs could not take place. Hence the most basic parameters of  axonal and dendritic arbors such as their size should follow from considerations of connec-  tivity.  Another possibility is that the size of dendritic arbors is dictated by the surface area needed  114 D. B. Chklovsk'i  to arrange all the synapses. This argument does not specify the arbor size, however: a com-  pact dendrite of elaborate shape can have the same surface area as a wide dendritic arbor.  Finally, agreement of the predictions with the existing anatomical data suggests that the rule  is based on correct principles. Further extensive testing of the rule is desirable. Violation  of the rule in some system would suggest the presence of other overriding considerations in  the design of that system, which is also interesting.  Acknowledgements  I benefited from helpful discussions with E.M. Callaway, E.J. Chichilnisky, H.J. Karten,  C.F. Stevens and T.J. Sejnowski and especially with A.A. Koulakov. I thank G.D. Brown  for suggesting that the size of axonal and dendritic arbors may be related to con-/divergence.  References  [17]  [1] Cajal, S.R.y. (1995a). Histology of the nervous system p.95 (Oxford University Press, New-  York).  [2] Cajal, S.R.y. ibid. p. 116.  [3] Mitchison, G. (1991). Neuronal branching patterns and the economy of cortical wiring. Proc R  Soc Lond B Biol Sci 245, 151-8.  [4] Cherniak, C. (1992). Local optimization of neuron arbors, Biol Cybern 66, 503-510.  [5] Young, M.P. (1992). Objective analysis of the topological organization of the primate cortical  visual system Nature 358, 152-5.  [6] Chklovskii, D.B. & Stevens, C.F. (1999). Wiring the brain optimally, submitted Nature Neuro-  science.  [7] Watanabe, M. & Rodieck, R.W. (1989). Parasol and midget ganglion cells of the primate retina.  J Comp Neurol 289, 434-54.  [8] Milam, A.H., Dacey, D.M. & Dizhoor, A.M. (1993). Recoverin immunoreactivity in mam-  malian cone bipolar cells. Vis Neurosci 10, 1-12.  [9] Gmnert, U., Martin, P.R. & Wassic H. (1994). Immunocytochemical analysis of bipolar cells in  the macaque monkey retina. J Comp Neurol 348, 607-27.  [10] Gmnert, U. & Martin, P.R. (1991). Rod bipolar cells in the macaque monkey retina: immunore-  activity and connectivity. J Neurosci 11, 2742-58.  [11] Dacey, D.M. (1993). The mosaic of midget ganglion cells in the human retina. J Neurosci 13,  5334-55.  [12] Wassic, H., Boycott, B.B. & Rohrenbeck, J. (1989). Horizontal cells in the monkey retina: cone  connections and dendritic network. Eur J Neurosci 1, 421-435.  [13] Rodieck, R.W. (1989) The First Steps in Seeing (Sinauer Associates, Sunderland, MA).  [14] Andersen, B.B., Korbo, L. & Pakkenberg, B. (1992). A quantitative study of the human cere-  bellum with unbiased stereological techniques. J Comp Neurol 326, 549-60.  [15] Peters A., Payne B.R. & Budd, J. (1994). A numerical analysis of the geniculocortical input to  striate cortex in the monkey. Cereb Cortex 4, 215-229.  [16] B!asdel, G.G. & Lund, J.S. (1983) Termination of afferent axons in macaque striate cortex. J  Neurosci 3, 1389-1413.  Wiser, A.K. & Callaway, E.M. (1996). Contributions of individual layer 6 pyramidal neurons  to local circuitry in macaque primary visual cortex. J Neurosci 16, 2724-2739.  
Invariant Feature Extraction and  Classification in Kernel Spaces  Sebastian Mika , Gunnar Ritsch  , Jason Weston 2,  Bernhard Sch51kopf 3, Alex Smola 4, and Klaus-Robert Miiller   1 GMD FIRST, Kekulstr. 7, 12489 Berlin, Germany  2 Barnhill BioInformatics, 6709 Waters Av., Savannah, GR 31406, USA  3 Microsoft Research Ltd., 1 Guildhall Street, Cambridge CB2 3NH, UK  4 Australian National University, Canberra, 0200 ACT, Australia  {mika, raetsch, klaus}@first.gmd.de, jasonw@dcs.rhbnc.ac.uk  bsc@microsoft.com, Alex. Smola.anu.edu.au  Abstract  We incorporate prior knowledge to construct nonlinear algorithms  for invariant feature extraction and discrimination. Employing a  unified framework in terms of a nonlinear variant of the Rayleigh  coefficient, we propose non-linear generalizations of Fisher's dis-  criminant and oriented PCA using Support Vector kernel functions.  Extensive simulations show the utility of our approach.  I Introduction  It is common practice to preprocess data by extracting linear or nonlinear features.  The most well-known feature extraction technique is principal component analysis  PCA (e.g. [3]). It aims to find an orthonormal, ordered basis such that the i-th  direction describes as much variance as possible while maintaining orthogonality to  all other directions. However, since PCA is a linear technique, it is too limited to  capture interesting nonlinear structure in a data set and nonlinear generalizations  have been proposed, among them Kernel PCA [14], which computes the principal  components of the data set mapped nonlinearly into some high dimensional feature  space r.  Often one has prior information, for instance, we might know that the sample is  corrupted by noise or that there are invariances under which a classification should  not change. For feature extraction, the concepts of known noise or transformation  invariance are to a certain degree equivalent, i.e. they can both be interpreted as  causing a change in the feature which ought to be minimized. Clearly, invariance  alone is not a sufficient condition for a good feature, as we could simply take the  constant function. What one would like to obtain is a feature which is as invariant  as possible while still covering as much of the information necessary for describing  the particular data. Considering only one (linear) feature vector w and restricting  to first and second order statistics of the data one arrives at a maximization of the  so called Rayleigh coefficient  wTsw  J(w) = wVSvw , (1)  Invariant Feature Extraction and Classification in Kernel Spaces 52 7  where w is the feature vector and $I, $v are matrices describing the desired and  undesired properties of the feature, respectively (e.g. information and noise). If $I  is the data covariance and Sv the noise covariance, we obtain oriented PCA [3].  If we leave the field of data description to perform supervised classification, it is  common to choose $ as the separability of class centers (between class variance)  and $v to be the within class variance. In that case, we recover the well known  Fisher Discriminant [7]. The ratio in (1) is maximized when we cover much of  the information coded by $ while avoiding the one coded by $v. The problem is  known to be solved, in analogy to PCA, by a generalized symmetric eigenproblem  Sw = Svw [3], where  C ll is the corresponding (biggest) eigenvalue.  In this paper we generalize this setting to a nonlinear one. In analogy to [8, 14]  we first map the data via some nonlinear mapping  to some high-dimensional fea-  ture space r and then optimize (1) in r. To avoid working with the mapped data  explicitly (which might be impossible if r is infinite dimensional) we introduce sup-  port vector kernel functions [11], the well-known kernel trick. These kernel functions  k(x, y) compute a dot product in some feature space r, i.e. k(x, y) - ((x). (y)).  Formulating the algorithms in r using  only in dot products, we can replace any  occurrence of a dot product by the kernel function k. Possible choices for k which  have proven useful e.g. in Support Vector Machines [2] or Kernel PCA [14] are Gaus-  sian RBF, k(x,y): exp(-llx- yll2/c), or polynomial kernels, k(x,y) = (x. y)a,  for some positive constants c C ll and d  N, respectively.  The remainder of this paper is organized as follows: The next section shows how to  formulate the optimization problem induced by (1) in feature space. Section 3 con-  siders various ways to find Fisher's Discriminant in r; we conclude with extensive  experiments in section 4 and a discussion of our findings.  2 Kernelizing the Rayleigh Coefficient  To optimize (1) in some kernel feature space r we need to find a formulation which  uses only dot products of -images. As numerator and denominator are both scalars  this can be done independently. Furthermore, the matrices $ and $v are basically  covariances and thus the sum over outer products of -images. Therefore, and due  to the linear nature of (1) every solution w  r can be written as an expansion in  terms of mapped training data , i.e.  i=1  (2)  To define some common choices in r let X - {x,... , x} be our training sample  and, where appropriate, X U X2 = X, X Cl X2 = , two subclasses (with Ixil = i).  We get the full covariance of X by  I 1  C =  Z (I,(x) - m)(I,(x) - m) n- with m = j Z (x),  (3)  SB and Sw are operators on a (finite-dimensional) subspace spanned by the I,(xi) (in  a possibly infinite space). Let w -- v + v2, where v E Span((xi): i = 1,... ,f) and  v2 _L Span((xi): i = 1,... , f). Then for $ -- $w or $ = $B (which are both symmetric)  o,so) =  =  =  As Vl lies in the span of the (I)(:Ci) and S only operates on this subspace there exist an  expansion of w which maximizes J(w).  528 S. Mika, G. Riitsch, J. Weston, B. Sch6lkopf A. J. Smola and K.-R. Miiller  which could be used as St in oriented Kernel PCA. For SN we could use an estimate  of the noise covariance, analogous to the definition of C but over mapped patterns  sampled from the assumed noise distribution. The standard formulation of the  Fisher discriminant in O r, yielding the Kernel Fisher Discriminant (KFD) [8] is  given by  SW : Z Z ((x) - mi)((x) -- mi) y and SB -- (m2 -- m)(m2 -- m) -r,  i----1,2 x Afl  the within-class scatter Sw (as SN), and the between class scatter Sis (as St). Here  mi is the sample mean for patterns from class i.  To incorporate a known invariance e.g. in oriented Kernel PCA, one could use the  tangent covariance matrix [12],  1  T = t---  Z (I,(x) - (,x))((x) - (,x)) q- for some small t > 0. (4)  Here t is a local 1-parameter transformation. T is a finite difference approximation  t of the covariance of the tangent of t at point (x) (details e.g. in [12]). Using  St - C and $N -- T in oriented Kernel PCA, we impose invariance under the local  transformation t. Crucially, this matrix is not only constructed from the training  patterns X. Therefore, the argument used to find the expansion (2) is slightly  incorrect. Neverthless, we can assume that (2) is a reasonable approximation for  describing the variance induced by T.  Multiplying either of these matrices from the left and right with the expansion (2),  we can find a formulation which uses only dot products. For the sake of brevity, we  only give the explicit formulation of (1) in Or for KFD (cf. [8] for details). Defining  1  (i)j: -. Ea:exi k(xj,x) we can write (1) for KFD as  = (ru)2  a--ffNa = dNa , (5)  where N = KK n-- y.i=,2eilil, / = /2 -/1, M - //'1-, and Kij = k(xi, xj).  The results for other choices of $I and SN in Or as for the cases of oriented kernel  PCA or transformation invariance can be obtained along the same lines. Note that  we still have to maximize a Rayleigh coefficient. However, now it is a quotient in  terms of expansion coefficients a, and not in terms of w C Or which is a potentially  infinite-dimensionai space. Furthermore, it is well known that the solution for this  special eigenproblem is in the direction of N - (/2 -/) [7], which can be solved  using e.g. a Cholesky factorization of N. The projection of a new pattern x onto  w in Or can then be computed by  (w . I,(x)) = Z ai k(xi, x). (6)  i=1  3 Algorithms  Estimating a covariance matrix with rank up to/? from  samples is ill-posed. Fur-  thermore, by performing an explicit centering in Or each covariance matrix loses one  more dimension, i.e. it has only rank g- I (even worse, for KFD the matrix N has  rank - 2). Thus the ratio in (1) is not well defined anymore, as the denomina-  tor might become zero. In the following we will propose several ways to deal with  this problem in KFD. Furthermore we will tackle the question how to solve the  optimization problem of KFD more efficiently. So far, we have an eigenproblem of  size  x g. If g becomes large this is numerically demanding. Reformulations of the  original problem allow to overcome some of these limitations. Finally, we describe  the connection between KFD and RBF networks.  Invariant Feature Extraction and Classification in Kernel Spaces 529  3.1 Regularization and Solution on a Subspace  As noted before, the matrix N has only rank/ - 2. Besides numerical problems  which can cause the matrix N to be not even positive, we could think of imposing  some regularization to control capacity in r. To this end, we simply add a multiple  of the identity matrix to N, i.e. replace N by Nu where  N := N + . (7)  This can be viewed in different ways: (i) for t > 0 it makes the problem feasible  and numerically more stable as N u becomes positive; (ii) it can be seen as decreas-  ing the bias in sample based estimation of eigenvalues (cf. [6]); (iii) it imposes a  regularization on lieill 2, favoring solutions with small expansion coefficients. Fur-  thermore, one could use other regularization type additives to N, e.g. penalizing  IIw]] 2 in analogy to SVM (by adding the kernel matrix Kij = k(xi, xj)).  To optimize (5) we need to solve an  x  eigenproblem, which might be intractable  for large . As the solutions are not sparse one can not directly use efficient algo-  rithms like chunking for Support Vector Machines (cf. [13]). To this end, we might  restrict the solution to lie in a subspace, i.e. instead of expanding w by (2) we write  m  = (8)  i=1  with m < l. The patterns zi could either be a subset of the training patterns X  or e.g. be estimated by some clustering algorithm. The derivation of (5) does not  change, only K is now m x  and we end up with m x m matrices N and M. Another  advantage is, that it increases the rank of N (relative to its size) although there  still might be some need for regularization.  3.2 Quadratic optimization and Sparsification  Even if N has full rank, maximizing (5) is underdetermined: if c is optimal, then so  is any multiple thereof. Since c-rMc = (cn-tz) 2, M has rank one. Thus we can seek  for a vector c, such that anna is minimal for fixed c-rtz (e.g. to 1). The solution  is unique and we can find the optimal c by solving the quadratic optimization  problem:  min anna subject to cn-tz = 1. (9)  Although the quadratic optimization problem is not easier to solve than the eigen-  problem, it has an appealing interpretation. The constraint cn-/z - I ensures, that  the average class distance, projected onto the direction of discrimination, is con-  stant, while the intra class variance is minimized, i.e. we maximize the average  margin. Contrarily, the SVM approach [2] optimizes for a large minimal margin.  Considering (9) we are able to overcome another shortcoming of KFD. The solu-  tions c are not sparse and thus evaluating (6) is expensive. To solve this we can  add an/-regularizer AIIc]l to the objective function, where A is a regularization  parameter allowing us to adjust the degree of sparseness.  3.3 Connection to RBF Networks  Interestingly, there exists a close connection between RBF networks (e.g. [9, 1]) and  KFD. If we add no regularization and expand in all training patterns, we find that  an optimal c is given by c - K-y, where K is the symmetric, positive matrix of  all kernel elements k(xi, xj) and y the :t:1 label vector 2. A RBF-network with the  2To see this, note that N can be written as N = KDK where D - I-yy-y2y  has  rank f - 2, while Yi is the vector of 1/v/'s for patterns from class i and zero otherwise.  530 S. Mika, G. Rtitsch, d. Weston, B. Sch61kopf A. d. Smola andK.-R. Miiller  Banana  B.Cancer  Diabetes  German  Heart  Image  Ringnorm  F.Sonar  Splice  Thyroid  Titanic  Twonorm  Waveform  RBF  10.8+-0.06  27.6+.0.47  24.3+.0.19  24.7+.0.24  17.6+.0.33  3.3+.0.06  1.7+.0.02  34.4+.0.20  10.0-2:0.10  4.5+0.21  23.3+.0.13  2.9+.0.03  10.7+0.11  AB  12.3:l:0.07  30.4+.0.47  26.5+.0.23  27.54-0.25  20.3+0.34  2.7+.0.07  1.9+.0.03  35.7+.0.18  22.6+.0.12  3.04-0.03  10.8+0.06  ABR  9+o. oj  26.5+.0.45  23.8+0.18  24.3+0.21  16.5+.0.35  2.7'+.0.06  1.6+. O. 01  34.2+.0.22  9.5+0.0?  4.6+.0.22  22.6+.0.12  2.7+.0.02  9.8+0.08  SVM  11.5+.0.07  2 6. 0::t: 0.  ?  23.5+0.17  23.6+0.21  16.0+0.33  3. O::t: O. 06  1.7+.0.01  32.4+.0.18  10.9+0.07  4.8+.0.22  22.4+.0.10  3.0+.0.02  9.9+0.0  KFD  10.8+.0.05 Table 1: Com-  25.8+0.46 parison between  23.2+.0.16 KFD, single  23.7+.0.22 RBF classifier,  16.1+.0.3 AdaBoost (AB)  4.8+.0.06 '  1.5+.0.01 regul. Ada-  33.2+.0.17 Boost (ABR)  10.5+.0.06 and SVMs (see  4.2+0.21 text). Best re-  23.2+0.20 suit in bold face,  2.6+0.02 second best in  9.9+.0.0J italics.  same kernel at each sample and fixed kernel width gives the same solution, if the  mean squared error between labels and output is minimized. Also for the case of  restricted expansions (8) there exists a connection to RBF networks with a smaller  number of centers (cf. [4]).  4 Experiments  Kernel Fisher Discriminant Figure I shows an illustrative comparison of the  features found by KFD, and Kernel PCA. The KFD feature discriminates the two  classes, the first Kernel PCA feature picks up the important nonlinear structure.  To evaluate the performance of the KFD on real data sets we performed an extensive  comparison to other state-of-the-art classifiers, whose details are reported in [8]. 3  We compared the Kernel Fisher Discriminant and Support Vector Machines, both  with Gaussian kernel, to AdaBoost [5], and regularized AdaBoost [10] (cf. table 1).  For KFD we used the regularized within-class scatter (7) and computed projections  onto the optimal direction w 6 9 r by means of (6). To use w for classification we  have to estimate a threshold. This can be done by e.g. trying all thresholds between  two outputs on the training set and selecting the median of those with the smallest  empirical error, or (as we did here) by computing the threshold which maximizes the  margin on the outputs in analogy to a Support Vector Machine, where we deal with  errors on the trainig set by using the SVM soft margin approach. A disadvantage  of this is, however, that we have to control the regularization constant for the slack  variables. The results in table I show the average test error and the standard  If K has full rank, the null space of D, which is spanned by Yl and Y2, is the null space  of N. For & ---- K-ly we get &-N& -- 0 and &-/ : 0. As we are free to fix the constraint  c-/ to any positive constant (not just 1), & is also feasible.  SThe breast cancer domain was obtained from the University Medical Center,  Inst. of Oncology, Ljubljana, Yugoslavia. Thanks to M. Zwitter and M. Sok-  lic for the data. All data sets used in the experiments can be obtained via  http://www. first. gmd. de/~raetsch/.  Figure 1: Comparison of feature  found by KFD (left) and first  Kernel PCA feature (right). De-  picted are two classes (informa-  tion only used by KFD) as dots "  and crosses and levels of same  feature value. Both with polyno-  mial kernel of degree two, KFD  with the regularized within class  scatter (7) (/-- 10-3). '-"  Invariant Feature Extraction and Classification in Kernel Spaces 531  deviation of the averages' estimation, over 100 runs with different realizations of  the datasets. To estimate the necessary parameters, we ran 5-fold cross validation  on the first five realizations of the training sets and took the model parameters to  be the median over the five estimates (see [10] for details of the experimental setup).  Using prior knowledge. A toy example (figure 2) shows a comparison of Ker-  nel PCA and oriented Kernel PCA, which used $ as the full covariance (3) and  as noise matrix Sv the tangent covariance (4) of (i) rotated patterns and (ii) along  the x-axis translated patterns. The toy example shows how imposing the desired  invariance yields meaningful invariant features.  In another experiment we incorporated prior knowledge in KFD. We used the USPS  database of handwritten digits, which consists of 7291 training and 2007 test pat-  terns, each 256 dimensional gray scale images of the digits 0...9. We used the  regularized within-class scatter (7) (/ = 10- ) as Sv and added to it an multiple A  of the tangent covariance (4), i.e. Sv = Nu + AT. As invariance transformations we  have chosen horizontal and vertical translation, rotation, and thickening (cf. [12]),  where we simply averaged the matrices corresponding to each transformation. The  feature was extracted by using the restricted expansion (8), where the patterns zi  were the first 3000 training samples. As kernel we have chosen a Gaussian of width  0.3. 256, which is optimal for SVMs [12]. For each class we trained one KFD which  classified this class against the rest and computed the 10-class error by the winner-  takes-all scheme. The threshold was estimated by minimizing the empirical risk on  the normalized outputs of KFD.  Without invariances, i.e. A = 0, we achieved a test error of 3.7%, slightly better than  a plain SVM with the same kernel (4.2%) [12]. For A = 10 -3, using the tangent  covariance matrix led to a very slight improvement to 3.6%. That the result was not  significantly better than the corresponding one for KFD (3.7%) can be attributed  to the fact that we used the same expansion coefficients in both cases. The tangent  covariance matrix, however, lives in a slightly different subspace. And indeed, a  subsequent experiment where we used vectors which were obtained by clustering a  larger dataset, including virtual examples generated by the appropriate invariance  transformation, led to 3.1%, comparable to an SVM using prior knowledge (e.g. [12];  best SVM result 2.9% with local kernel and virtual support vectors).  5 Conclusion  In the task of learning from data it is equivalent to have prior knowledge about  e.g. invariances or about specific sources of noise. In the case of feature extraction,  we seek features which are sufficiently (noise-) invariant while still describing in-  teresting structure. Oriented PCA and, closely related, Fisher's Discriminant, use  particularly simple features, since they only consider first and second order statis-  tics for maximizing the Rayleigh coefficient (1). Since linear methods can be too  restricted in many real-world applications, we used Support Vector Kernel functions  to obtain nonlinear versions of these algorithms, namely oriented Kernel PCA and  Kernel Fisher Discriminant analysis.  Our experiments show that the Kernel Fisher Discriminant is competitive or in  Figure 2: Comparison of first  features found by Kernel PCA  and oriented Kernel PCA (see '  text); from left to right: KPCA,  OKPCA with rotation and  translation invariance; all with  Gaussian kernel.  532 S. Mika, G. Riitsch, J. Weston, B. SchOlkopf A. J. Smola and K.-R. Mailer  some cases even superior to the other state-of-the-art algorithms tested. Interest-  ingly, both SVM and KFD construct a hyperplane in r which is in some sense  optimal. In many cases, the one given by the solution w of KFD is superior to  the one of SVMs. Encouraged by the preliminary results for digit recognition, we  believe that the reported results can be improved, by incorporating different invari-  ances and using e.g. local kernels [12].  Future research will focus on further improvements on the algorithmic complexity  of our new algorithms, which is so far larger than the one of the SVM algorithm,  and on the connection between KFD and Support Vector Machines (cf. [16, 15]).  Acknowledgments This work was partially supported by grants of the DFG (JA  379/5-2,7-1,9-1) and the EC STORM project number 25387 and carried out while  BS and AS were with GMD First.  References  [1] C.M. Bishop. Neural Networks for Pattern Recognition. Oxford Univ. Press, 1995.  [2] B. Boser, I. Guyon, and V.N. Vapnik. A training algorithm for optimal margin  classifiers. In D. Haussler, editor, Proc. COLT, pages 144-152. ACM Press, 1992.  [3] K.I. Diamantaras and S.Y. Kung. Principal Component Neural Networks. Wiley, New  York, 1996.  [4] B.Q. Fang and A.P. Dawid. Comparison of full bayes and bayes-least squares criteria  for normal discrimination. Chinese Journal of Applied Probability and Statistics,  12:401-410, 1996.  [5] Y. Freund and R.E. Schapire. A decision-theoretic generalization of on-line learning  and an application to boosting. In EuroCOLT 93. LNCS, 1994.  [6] J.H. Friedman. Regularized discriminant analysis. Journal of the American Statistical  Association, 84(405):165-175, 1989.  [7] K. Fukunaga. Introduction to Statistical Pattern Recognition. Academic Press, San  Diego, 2nd edition, 1990.  [8] S. Mika, G. Riitsch, J. Weston, B. SchSlkopf, and K.-R. Mfiller. Fisher discriminant  analysis with kernels. In Y.-H. Hu, J. Larsen, E. Wilson, and S. Douglas, editors,  Neural Networks for Signal Processing IX, pages 41-48. IEEE, 1999.  [9] J. Moody and C. Darken. Fast learning in networks of locally-tuned processing units.  Neural Computation, 1(2):281-294, 1989.  [10] G. Riitsch, T. Onoda, and K.-R. Mfiller. Soft margins for adaboost. Technical Report  NC-TR-1998-021, Royal Holloway College, University of London, UK, 1998.  [11] S. Saitoh. Theory of Reproducing Kernels and its Applications. Longman Scientific  & Technical, Harlow, England, 1988.  [12] B. SchSlkopf. Support vector learning. Oldenbourg Verlag, 1997.  [13] B. SchSlkopf, C.J.C. Burges, and A.J. Smola, editors. Advances in Kernel Methods -  Support Vector Learning. MIT Press, 1999.  [14] B. SchSlkopf, A.J. Smola, and K.-R. Mfiller. Nonlinear component analysis as a kernel  eigenvalue problem. Neural Computation, 10:1299-1319, 1998.  [15] A. Shashua. On the relationship between the support vector machine for classification  and sparsifted fisher's linear discriminant. Neural Processing Letters, 9(2):129-139,  April 1999.  [16] S. Tong and D. Koller. Bayes optimal hyperplanes - maximal margin hy-  perplanes. Submitted to IJCAI'99 Workshop on Support Vector Machines  (robotics. stanford. edu/~koller/), 1999.  
Perceptual Organization Based on  Temporal Dynamics  Xiuwen Liu and DeLiang L. Wang  Department of Computer and Information Science  Center for Cognitive Science  The Ohio State University, Columbus, OH 43210-1277  Email: (liux, dwang)@cis.ohio-state.edu  Abstract  A figure-ground segregation network is proposed based on a novel  boundary pair representation. Nodes in the network are bound-  ary segments obtained through local grouping. Each node is ex-  citatorily coupled with the neighboring nodes that belong to the  same region, and inhibitorily coupled with the corresponding paired  node. Gestalt grouping rules are incorporated by modulating con-  nections. The status of a node represents its probability being  figural and is updated according to a differential equation. The  system solves the figure-ground segregation problem through tem-  poral evolution. Different perceptual phenomena, such as modal  and amodal completion, virtual contours, grouping and shape de-  composition are then explained through local diffusion. The system  eliminates combinatorial optimization and accounts for many psy-  chophysical results with a fixed set of parameters.  i Introduction  Perceptual organization refers to the ability of grouping similar features in sensory  data. This, at a minimum, includes the operations of grouping and figure-ground  segregation, which refers to the process of determining relative depths of adjacent  regions in input data and thus proper occlusion hierarchy. Perceptual organization  has been studied extensively and many of the existing approaches [5] [4] [8] [10] [3]  start from detecting discontinuities, i.e. edges in the input; one or several configura-  tions are then selected according to certain criteria, for example, non-accidentalness  [5]. Those approaches have several disadvantages for perceptual organization. Edges  should be localized between regions and an additional ambiguity, the ownership of  a boundary segment, is introduced, which is equivalent to figure-ground segrega-  tion [7]. Due to that, regional attributions cannot be associated with boundary  segments. Furthermore, because each boundary segment can belong to different  regions, the potential search space is combinatorial.  To overcome some of the problems, we propose a laterally-coupled network based on  a boundary-pair representation to resolve figure-ground segregation. An occluding  boundary is represented by a pair of boundaries of the two associated regions, and  Perceptual Organization Based on Temporal Dynamics 39  (a) (b) (c) (d) (e)  Figure 1: On- and off-center cell responses. (a) On- and off-center cells. (b) Input  image. (c) On-center cell responses. (d) Off-center cell responses (e) Binarized  on- and off-center cell responses, where white regions represent on-center response  regions and black off-center regions.  initiates a competition between the regions. Each node in the network represents a  boundary segment. Regions compete to be figural through boundary-pair competi-  tion and figure-ground segregation is resolved through temporal evolution. Gestalt  grouping rules are incorporated by modulating coupling strengths between different  nodes within a region, which influences the temporal dynamics and determines the  percept of the system. Shape decomposition and grouping are then implemented  through local diffusion using the results from figure-ground segregation.  2 Figure-Ground Segregation Network  The central problem in perceptual organization is to determine relative depths  among regions. As figure reversal occurs in certain circumstances, figure-ground  segregation cannot be resolved only based on local attributes.  2.1 The Network Architecture  The boundary-pair representation is motivated by on- and off-center cells, shown  in Fig. l(a). Fig. l(b) shows an input image and Fig. l(c) and (d) show the  on- and off-center responses. Without zero-crossing, we naturally obtain double  responses for each occluding boundary, as shown in Fig. l(e). In our boundary-pair  representation, each boundary is uniquely associated with a region.  In this paper, we obtain closed region boundaries from segmentation and form  boundary segments using corners and junctions, which are detected through local  corner and junction detectors. A node i in the figure-ground segregation network  represents a boundary segment, and Pi represents its probability being figural, which  is set to 0.5 initially. Each node is laterally coupled with neighboring nodes on the  closed boundary. The connection weight from node i to j, wig, is 1 and can be  modified by T-junctions and local shape information. Each occluding boundary is  represented by a pair of boundary segments of the involved regions. For example,  in Fig. 2(a), nodes 1 and 5 form a boundary pair, where node 1 belongs to the  white region and node 5 belongs to the black region. Node i updates its status by:  dPi  ' d---=Iz  wki(Pk -- Pi) +/j(1 - Pi)  H(Qii) +/s(1 - Pi) exp(--) (1)  Here N(i) is the set of neighboring nodes of i, and/, /g, and/B are parameters  to determine the influences from lateral connections, junctions, and bias. J(i) is  40 X.. Liu and D. L. Wang  (b)  Figure 2: (a) The figure-ground segregation network for Fig. l(b). Nodes 1, 2, 3  and 4 belong to the white region; nodes 5, 6, 7, and 8 belong to the black region;  and nodes 9 and 10, and nodes 11 and 12 belong to the left and right gray regions  respectively. Solid lines represent excitatory coupling while dashed lines represent  inhibitory connections. (b) Result after surface completion. Left and right gray  regions are grouped together.  the set of junctions that are associated with i and Qli is the junction strength of  node i of junction 1. H(x) is given by H(x) = tanh(3(x - 8j)), where 3 controls  the steepness and 0j is a threshold.  In (1), the first term on the right reflects the lateral influences. When nodes are  strongly coupled, they are more likely to be in the same status, either figure or  background. The second term incorporates junction information. In other words,  at a T-junction, segments that vary more smoothly are more likely to be figural. The  third term is a bias, where Bi is the bias introduced to simulate human perception.  The competition between paired nodes i and j is through normalization based on  the assumption that only one of the paired nodes should be figural at a given time:  p/(t+) _ Piti(Pit + p) and pt+l) __ p/(pi t q_ p).  2.2 Incorporation of Gestalt Rules  To generate behavior that is consistent with human perception, we incorporate  grouping cues and some Gestalt grouping principles. As the network provides a  generic model, additional grouping rules can also be incorporated.  T-junctions T-junctions provide important cues for determining relative depths  [7] [10]. In Williams and Hanson's model [10], T-junctions are imposed as  topological constraints. Given a T-junction l, the initial strength for node i that is  associated with I is:  Qli = exp(-o( i,c( i) ) / KT )  1/2 YkN3(l) exp(--a(k,())/Kr) '  where Kr is a parameter, Ng(1) is a set of all the nodes associated with junction l,  c(i) is the other node in Na(1) that belongs to the same region as node i, and o(ij)  is the angle between segments i and j.  Non-accidentalness Non-accidentalness tries to capture the intrinsic relation-  ships among segments [5]. In our system, an additional connection is introduced to  node i if it is aligned well with a node j from the same region and j ' N(i) initially.  The connection weight wig is a function of distance and angle between the involved  ending points. This can be viewed as virtual junctions, resulting in virtual contours  and conversion of a corner into a T-junction if involved nodes become figural. This  corresponds to an organization criterion proposed by Geiger et al [3].  Perceptual Organization Based on Temporal Dynamics 41  Time  8  10  12  Time  Time  Figure 3: Temporal behavior of each node in the network shown in Fig. 2(a). Each  plot shows the status of the corresponding node with respect to time. The dashed  line is 0.5.  Shape information Shape information plays a central role in Gestalt principles  and is incorporated through enhancing lateral connections. In this paper, we  consider local symmetry. Let j and k be two neighboring nodes of i:  wij = 1 + C exp(-lai j - otkil/K,)  exp(-(Lj/Lk + L/Lj - 2)/KL)),  where C, K,, and KL are parameters and Lj is the length of segment j. Essentially  the lateral connections are strengthened when two neighboring segments of i are  symmetric.  Preferences Human perceptual systems often prefer some organizations over oth-  ers. Here we incorporated a well-known figure-ground segregation principle, called  closeness. In other words, the system prefers filled regions over holes. In current  implementation, we set Bi -- 1.0 if node i is part of a hole and otherwise Bi - O.  2.3 Temporal Properties of the Network  After we construct the figure-ground segregation network, each node is updated  according to (1). Fig. 3 shows the temporal behavior of the network shown in Fig.  2(a). The system approaches to a stable solution. For figure-ground segregation, we  can binarize the status of each node using threshold 0.5. Thus the system generates  the desired percept in a few iterations. The black region occludes other regions  while gray regions occlude the white region. For example, Ps is close to I and thus  segment 5 is figural, and P is close to 0 and thus segment I is in the background.  2.4 Surface Completion  After figure-ground segregation is resolved, surface completion and shape decompo-  sition are implemented through diffusion [3]. Each boundary segment is associated  with regional attributes such as the average intensity value because its ownership  is known. Boundary segments are then grouped into diffusion groups based on sim-  ilarities of their regional attributes and if they are occluded by common regions.  In Fig. l(b), three diffusion groups are formed, namely, the black region, two gray  regions, and the white region. Segments in one diffusion group are diffused simulta-  neously. For a figural segment, a buffer with a given radius is generated. Within the  buffer, the values are fixed to I for pixels belonging to the region and 0 otherwise.  Now the problem becomes a well-defined mathematical problem. We need to solve  42 X. Liu and D. L. Wang  (a) (b) (c)  Figure 4: Images with virtual contours. In each column, the top shows the input  image and the bottom the surface completion result, where completed surfaces are  shown according to their relative depths and the bottom one is the projection of all  the completed surfaces. (a) Alternate pacman. (b) Reverse-contrast pacman. (c)  Kanizsa triangle. (d) Woven square. (e) Double pacman.  the heat equation with given boundary conditions. Currently, the heat equation is  solved through local diffusion. The results from diffusion are then binarized using  threshold 0.5. Fig. 2(b) shows the results for Fig. l(b) after surface completion.  Here the two gray regions are grouped together through surface completion because  occluded boundaries allow diffusion. The white region becomes the background,  which is the entire image.  3 Experimental Results  Given an image, the system automatically constructs the network and establishes  the connections based on the rules discussed in Section 2.2. For all the experiments  shown here, a fixed set of parameters is used.  3.1 Modal and Amodal Completion  We first demonstrate that the system can simulate virtual contours and modal  completion. Fig. 4 shows the input images and surface completion results. The  system correctly solves figure-ground segregation problem and generates the most  probable percept. Fig. 4 (a) and (b) show two variations of pacman images [9]  [4]. Even though the edges have opposite contrast, the virtual rectangle is vivid.  Through boundary-pair representation, our system can handle both cases using the  same network. Fig. 4(c) shows a typical virtual image [6] and the system correctly  simulates the percept. In Fig. 4(d) [6], the rectangular-like frame is tilted, making  the order between the frame and virtual square not well-defined. Our system handles  that in the temporal domain. At any given time, the system outputs one of the  Perceptual Organization Based on Temporal Dynamics 43  (b) (c) (d) (e)  - :i:-. :  (f)  Figure 5: Surface completion results. (a) and (b) Bregman figures [1]. (c) and (d)  Surface completion results for (a) and (b). (e) and (f) An image of some groceries  and surface completion result.  completed surfaces. Due to this, the system can also handle the case in Fig. 4(e)  [2], where the percept is bistable, as the order between the two virtual squares is  not well defined.  Fig. 5(a) and (b) show the well-known Bregman figures [1]. In Fig. 5(a), there is no  perceptual grouping and parts of B's remain fragmented. However, when occlusion  is introduced as in Fig. 5(b), perceptual grouping is evident and fragments of B's  are grouped together. Our results, shown in Fig. 5 (c) and (d), are consistent with  the percepts. Fig. 5(e) shows an image of groceries, which is used extensively in [8].  Even though the T-junction at the bottom is locally confusing, our system gives the  most plausible result through lateral influences of the other two strong T-junctions.  Without search and parameter tuning, our system gives the optimal solution shown  in Fig. 5(f).  3.2 Comparison with Existing Approaches  As mentioned earlier, at the minimum, figure-groud segregation and grouping need  to be addresssed for perceptual organization. Edge-based approaches [4] [10] at-  tempt to solve both problems simultaneously by prefering some configurations over  combinatorially many ones according to certain crete fla. There are several diffi-  culties common to those approaches. First it cannot account for different human  percepts of cases where edge elements are similar. Fig. 5 (a) and (b) are well-  known examples in this regard. Another example is that the edge-only version of  Fig. 4(c) does not give rise to a vivid virtual contour as in Fig. 4(c) [6]. To reduce  the potential search space, often contrast signs of edges are used as additional con-  traints [10]. However, both Fig. 4 (a) and (b) give rise to virtual contours despite  the opposite edge contrast signs. Essentially based on Fig. 4(b), Grossberg and  Mingolla [4] claimed that illusory contours can join edges with different directions  of contrast, which does not hold in general. As demonstrated through experiments,  our approach does offer a common principle underlying these examples.  Our approach shares some similarities with the one by Geiger et al [3]. In both  approaches, perceptual organization is solved in two steps. In [3], figure-ground  segregation is encoded implicitly in hypotheses which are defined at junction points.  Because potential hypotheses are combinatorial, only a few manually chosen ones  are tested in their experiments, which is not sufficient for a general computational  44 X. Liu and D. L. Wang  model. In our approach, by resolving figure-ground segregation, there is no need to  define hypotheses explicitly. In both methods, grouping is implemented through  diffusion. In [3], "heat" sources for diffusion are given manually for each hy-  pothesis whereas our approach generates "heat" sources automatically using the  figure-ground segregation results. Finally, in our approach, local ambiguities can  be resolved through lateral connections using temporal dynamics, resulting in ro-  bust behavior. To obtain good results for Fig. 5(e), Nitzberg et al [8] need to  tune parameters and increase their search space substantially due to the misleading  T-junction at the bottom of Fig. 5(e).  4 Conclusion  In this paper we have proposed a network for perceptual organization using tem-  poral dynamics. The pair-wise boundary representation resolves the ownership  ambiguity inherent in an edge-based representation and is equivalent to a surface  representation through diffusion, providing a unified edge- and surface-based rep-  resentation. Through temporal dynamics, our model allows for interactions among  different modules and top-down influences can be incorporated.  Acknowledgments  Authors would like to thank S. C. Zhu and M. Wu for their valuable discussions.  This research is partially supported by an NSF grant (IRI-9423312) and an ONR  Young Investigator Award (N00014-96-1-0676) to DLW.  References  [1] A. S. Bregman, "Asking the 'What for' question in auditory perception," In Perceptual  Organization, M. Kubovy and J R. Pomerantz, eds., Lawrence Erlbaum Associates,  Publishers, Hillsdale, New Jersey, pp. 99-118, 1981.  [2] M. Fahle and G. Palm, "Perceptual rivalry between illusory and real contours," Bio-  logical Cybernetics, vol. 66, pp. 1-8, 1991.  [3] D. Geiger, H. Pao, and N. Rubin, "Salient and multiple illusory surfaces," In Pro-  ceedings of IEEE Computer Society Conference on Computer Vision and Pattern  Recognition, pp. 118-124, 1998.  [4] S. Grossberg and E. Mingolla, "Neural dynamics of perceptual grouping: textures,  boundaries, and emergent segmentations," Perception FJ Psychophysics, vol. 38, pp.  141-170, 1985.  [5] D. G. Lowe, Perceptual Organization and Visual Recognition, Kluwer Academic Pub-  lishers, Boston, 1985.  [6] G. Kanizsa, Organization in Vision, Praeger, New York, 1979.  [7] K. Nakayama, Z. J. He, and S. Shimojo, "Visual surface representation: a critical link  between lower-level and higher-level vision," In Visual Cognition, S. M. Kosslyn and  D. N. Osherson, eds., The MIT Press, Cambridge, Massachusetts, vol. 2, pp. 1-70,  1995.  [8] M. Nitzberg, D. Mumford, and T. Shiota, Filtering, Segmentation and Depth,  Springer-Verlag, New York, 1993.  [9] R. Shapley and J. Gordon, "The existence of interpolated illusory contours depends  on contrast and spatial separation," In The Perception of Illusory Contours, S. Petry  and G. E. Meyer, eds., Springer-Verlag, New York, pp. 109-115, 1987.  [10] L. R. Williams and A. R. Hanson, "Perceptual Completion of Occluded Surfaces,"  Computer Vision and Image Understanding, vol. 64, pp. 1-20, 1996.  
Bifurcation Analysis of a Silicon Neuron  Girish N. Patel l, Gennady S. Cymbalyuk 2'3,  Ronald L. Calabrese 2, and Stephen P. DeWeerth 1  1School of Electrical and Computer Engineering  Georgia Institute of Technology  Atlanta, Ga. 30332-0250  { girish.patel, steve.deweerth } @ ece.gatech.edu  2Department of Biology  Emory University  1510 Clifton Road, Atlanta, GA 30322  { gcym, rcalabre } @biology. emory.edu  3Institute of Mathematical Problems in Biology RAS  Pushchino, Moscow Region, Russia 142292 (on leave)  Abstract  We have developed a VLSI silicon neuron and a corresponding mathe-  matical model that is a two state-variable system. We describe the cir-  cuit implementation and compare the behaviors observed in the silicon  neuron and the mathematical model. We also perform bifurcation analy-  sis of the mathematical model by varying the externally applied current  and show that the behaviors exhibited by the silicon neuron under corre-  sponding conditions are in good agreement to those predicted by the  bifurcation analysis.  1 Introduction  The use of hardware models to understand dynamical behaviors in biological systems is  an approach that has a long and fruitful history [1][2]. The implementation in silicon of  oscillatory neural networks that model rhythmic motor-pattern generation in animals is  one recent addition to these modeling efforts [3][4]. The oscillatory patterns generated by  these systems result from intrinsic membrane properties of individual neurons and their  synaptic interactions within the network [5]. As the complexity of these oscillatory sili-  con systems increases, effective mathematical analysis becomes increasingly more impor-  tant to our understanding their behavior. However, the nonlinear dynamical behaviors of  the model neurons and the large-scale interconnectivity among these neurons makes it  very difficult to analyze theoretically the behavior of the resulting very large-scale inte-  grated (VLSI) systems. Thus, it is important to first identify methods for modeling the  model neurons that underlie these oscillatory systems.  Several simplified neuronal models have been used in the mathematical simulations of  pattern generating networks [6][7][8]. In this paper, we describe the implementation of a  732 G. N. Patel, G. S. Cymbalyuk, R. L. Calabrese and S. P. De Weerth  two-state-variable silicon neuron that has been used effectively to develop oscillatory net-  works [9][10]. We then derive a mathematical model of this implementation and analyze  the neuron and the model using nonlinear dynamical techniques including bifurcation  analysis [11 ]. Finally, we compare the experimental data derived from the silicon neuron  to that obtained from the mathematical model.  2 The silicon model neuron  The schematic for our silicon model neuron is shown in Figure 1. This silicon neuron is  inspired by the two-state, Morris-Lecar neuron model [12][13]. Transistor M1, analo-  gous to the voltage-gated calcium channel in the Morris-Lecar model, provides an instan-  taneous inward current that raises the membrane potential towards VHig h when the  membrane is depolarized. Transistor M 2 , analogous to the voltage-gated potassium chan-  nel in the Morris-Lecar model, provides a delayed outward current that lowers the mem-  brane potential toward VLo w when the membrane is depolarized. V H and VL are  analogous to the half-activation voltages for the inward and outward currents, respec-  tively. The voltages across C 1 and C 2 are the state variables representing the membrane  potential, V, and the slow "activation" variable of the outward current, W, respectively.  The W-nullcline represents its steady-state activation curve. Unlike the Morris-Lecar  model, our silicon neuron model does not possess a leak current.  Using current conservation at node V, the net current charging C 1 is given by  C 1 $2 = Iext0[ P + iH0[p -- iL0[N  (1)  where iH and i are the output currents of a differential pair circuit, and c% and c N  describe the ohmic effects of transistors M 1 and M2, respectively. The net current into C 2  is given by  C 2W = iXPN (2)  where i x is the output current of the OTA, and [3p and 13 N account for ohmic effects of  the pull-up and the pull-down transistors inside the OTA.  VHigh"  lpi M 1 {Xpiex t  )-w  VLow  W  \ [3p[3N i X  :v  Figure 1: Circuit diagram of the silicon neuron. The circuit incorporates analog building  blocks including two differential pair circuits composed of a bias current, IBH, and  transistors M4-M 5, and a bias current, IBL, and transistors M6-M 7, and a single follower-  integrator circuit composed of an operational transconductance amplifier (OTA), X 1 in the  configuration shown and a load capacitor, C 2. The response of the follower-integrator  circuit is similar to a first-order low-pass filter.  Bifurcation Analysis of a Silicon Neuron 733  The output currents of the differential-pair and an OTA circuits, derived by using sub-  threshold transistor equations [2], are a Fermi function and a hyperbolic-tangent func-  tion, respectively [2]. Substituting these functions for i a , i n , and i x in (1) and (2) yields  Cliff = Iext(Zp+IBH  K(V- VH)/U T K(W- VL)/U T  e e  K(V_ VH)/UTCILp -- IBL K(W _ VL)/UTCILN  1 +e 1 +e (3)  c2W= Ixtanh(K V) p N  where  V - VHigh/U T VLo w - V/U T  (Zp = 1-e (Z N = 1-e  W - Vdd/UT -W/U T  [3p = 1-e [3 N = 1-e  (4)  U T is the thermal voltage, Vdd is the supply voltage, and K is a fabrication dependent  parameter. The terms (Zp and (Z N limit the range of V to within VHig h and VLo w, and the  terms Op and ON limit the range of I/V to within the supply rails (Vdd and Gnd).  In order to compare the model to the experimental results, we needed to determine val-  ues for all of the model parameters. Vli_ h , VLo w , V H , V L , and Vdd were directly mea-  sured in experiments. The parameters fia and IB L were measured by voltage-clamp  experiments performed on the silicon neuron. At room temperature, U T = 0.025 volts.  The value of K = 0.65 was estimated by measuring the slope of the steady-state activa-  tion curve of inward current. Because W was implemented as an inaccessible node, I x  could only be estimated. Based on the circuit design, we can assume that the bias cur-  rents I x and IBH are of the same order of magnitude. We choose I T = 2.2 nA to fit the  bifurcation diagram (see Figure 3). C1 and C2, which are assumed to be identical accord-  ing to the physical design, are time scaling parameters in the model. We choose their val-  ues (C1 = C2 = 28 pF) to fit frequency dependence on Iex t (see Figure 4).  3 Bifurcation analysis  The silicon neuron and the mathematical model 1 described by (3) demonstrate various  dynamical behaviors under different parametric conditions. In particular, stable oscilla-  tions and steady-state equilibria are observed for different values of the externally applied  current, /ext' We focused our analysis on the influence of lext on the neuron behavior for  two reasons: (i) it provides insight about effects of synaptic currents, and (ii) it allows  comparison with neurophysiological experiments in which polarizing current is used as a  primary control parameter. The main results of this work are presented as the comparison  between the mathematical models and the experimental data represented as bifurcation  diagrams and frequency dependencies.  The nullclines described by (3) and for Iex t = 32 nA are shown in Figure 2A. In the  regime that we operate the circuit, the W-nullcline is an almost-linear curve and the V-  nullcline is an N-shaped curve. From (3), it can be seen that when IBu + Iex t > IB L the  nullclines cross at (V, W)= (VHigh, VHigh) and the system has high voltage (about 5  volts) steady-state equilibrium. Similarly, for Iex t close to zero, the system has one stable  equilibrium point close to (V, W) = (VLo w, Vow).  1The parameters used throughout the analyses of the model are VLo w = 0 V,  VHig h = 5 V, V L = V H = 2.5 V, IB8 = 6.5 nA, /BL = 42 nA, I T = 2.2 nA,  Vdd = 5 V, U t = 0.025 mY, and K = 0.65.  734 G. iV. Patel, G. S. Cymbalyuk, R. L. Calabrese andS. P. DeWeerth  A  2.85  2.8  2.75  --. 2.7   2.65  2.6  2.55  2.5  2.45  I W-nullcline  o  o  o  V-nullcline  I  I  I.  I  !  I I I I I  0 1 2 3 4 5  V(volts)  B  3.2  2.8  0 2.6  2.4  2.2  0 5 10 15 20 25 30 35  time (msec)  Figure 2: Nullclines and trajectories in the model of the silicon neuron for Iex t = 32 nA.  The system exhibits a stable limit-cycle (filled circles), an unstable limit-cycle (untilled  circles), and stable equilibrium point. Unstable limit-cycle separates the basins of  attraction of the stable limit-cycle and stable equilibrium point. Thus, trajectories initiated  within the area bounded by the unstable limit-cycle approach the stable equilibrium point  (solid line in A's inset, and "x's" in B). Trajectories initiated outside the unstable limit-  cycle approach the stable limit-cycle. In A, the inset shows an expansion at the  intersection of the V- and W-nullclines.  Bifurcation Analysis of a Silicon Neuron 735  A  Experimental data  B  5  4  o  X  o  5  4  3  >2  o   x      x    xxxx  x xxx xx    10 2o 3o  le (nAmps)  Modeling data  40  50  I I I I I  0 10 20 30 40 50  lex t (nAmps)  Figure 3: Bifurcation diagrams of the hardware implementation (A) and of the  mathematical model (B) under variation of the externally applied current. In A, the steady-  state equilibrium potential of V is denoted by "x"s. The maximum and minimum values  of V during stable oscillations are denoted by the filled circles. In B, the stable and  unstable equilibrium points are denoted by the solid and dashed curve, respectively, and  the minimum and maximum values of the stable and unstable oscillations are denoted by  the filled and untilled circles, respectively. In B, limit-cycle oscillations appear and  disappear via sub-critical Andronov-Hopf bifurcations. The bifurcation diagram (B) was  computed with the LOCBIF program [14].  736 G. N. Patel, G. S. Cymbalyuk, R. L. Calabrese and S. P. DeWeerth  A Experimental data B Modeling data  100  100     80    . '  .,,'"'--"'-.,...' .   40  ; ''  40  " 20 " 20  0 0 ' ' '  0 10 20 30 0 10 20 30  lex t (nAmps) lex t (nAmps)  Figure 4: Frequency dependence of the silicon neuron (A) and the mathematical model  (B) on the externally applied current.  For moderate values of Iex t ([ 1 nA,34 nA]), the stable and unstable equilibrium points are  close to (V, W)--(V m VL) (Figure 3). In experiments in which Iex t was varied, we  observed a hard loss of the stability of the steady-state equilibrium and a transition into  oscillations at Iex t - 7.2 nA (Iex t -- 27.5 nA ). In the mathematical model, at the criti-  cal value of Iex t - 7.7 nA (Iex t - 27.8 nA ), an unstable limit cycle appears via a sub-  critical Andronov-Hopf bifurcation. This unstable limit cycle merges with the stable limit  cycle at the fold bifurcation at Iex t = 3.4 nA (Iex t '- 32.1 nA). Similarly, in the experi-  ments, we observed hard loss of stability of oscillations at Iex t = 2.0nA  (Iex t -32.8 nA). Thus, the system demonstrates hysteresis. For example, when  Iex t -- 20 nA the silicon neuron has only one stable regime, namely, stable oscillations.  Then if external current is slowly increased to Iex t - 32.8 nA, the form of oscillations  changes. At this critical value of the current, the oscillations suddenly lose stability, and  only steady-state equilibrium is stable. Now, when the external current is reduced, the  steady-state equilibrium is observed at the values of the current where oscillations were  previously exhibited. Thus, within the ranges of externally applied currents (2.0,7.2) and  (27.5,32.8), oscillations and a steady-state equilibrium are stable regimes as shown in  Figure 2.  4 Discussion  We have developed a two-state silicon neuron and a mathematical model that describes the  behavior of this neuron. We have shown experimentally and verified mathematically that  this silicon neuron has three regions of operation under the variation of its external current  (one of its parameters). We also perform bifurcation analysis of the mathematical model  by varying the externally applied current and show that the behaviors exhibited by the sili-  con neuron under corresponding conditions are in good agreement to those predicted by  the bifurcation analysis.  This analysis and comparison to experiment is an important step toward our understand-  ing of a variety of oscillatory hardware networks that we and others are developing. The  Bifurcation Analysis of a Silicon Neuron 737  model facilitates an understanding of the neurons that the hardware alone does not pro-  vide. In particular for this neuron, the model allows us to determine the location of the  unstable fixed points and the types of bifurcations that are exhibited. In higher-order sys-  tems, we expect that the model will provide us insight about observed behaviors and  complex bifurcations in the phase space. The good matching between the model and the  experimental data described in this paper gives us some confidence that future analysis  efforts will prove fruitful.  Acknowledgments  S. DeWeerth and G. Patel are funded by NSF grant IBN-9511721, G.S. Cymbalyuk is  supported by Russian Foundation of Fundamental Research grant 99-04-49112, R.L. Cal-  abrese and G.S. Cymbalyuk are supported by NIH grants NS24072 and NS34975.  References  [91  [IO]  [11]  [12]  [131  [14]  [1] Van Der Pol, B (1939) Biological rhythms considered as relaxation oscillations In H.  Bremmer and C.J. Bouwkamp (eds) Selected Scientific Papers, Vol 2, North Holland  Pub. Co., 1960.  [2] Mead, C.A. Analog VLSI and Neural Systems. Addison-Wesley, Reading, MA, 1989.  [3] Simoni, M.F., Patel, G.N., DeWeerth, S.P., & Calabrese, R.L. Analog VLSI model of  the leech heartbeat elemental oscillator. Sixth Annual Computational Neuroscience  Meeting, 1997. in Big Sky, Montana.  [4] DeWeerth, S., Patel, G., Schimmel, D., Simoni, M. and Calabrese, R. (1997). In  Proceedings of the Seventeenth Conference on Advanced Research in VLSI, R.B.  Brown and A.T. Ishii (eds), Los Alamitos, CA: IEEE Computer Society, 182-200.  [5] Marder, E. & Calabrese, R.L. (1996) Principles of rhythmic motor pattern generation.  Physiological Reviews 76 (3): 687-717.  [6] Kopell, N. & Ermentrout, B. (1988) Coupled oscillators and the design of central  pattern generators. Mathematical biosciences 90: 87-109.  [7] Skinner, F.K., Turrigiano, G.G., & Marder, E. (1993) Frequency and burst duration in  oscillating neurons and two-cell networks. Biological Cybernetics 69: 375-383.  [8] Skinner, F.K., Gramoll, S., Calabrese, R.L., Kopell, N. & Marder, E. (1994) Frequency  control in biological half-center oscillators. In F.H. Eeckman (ed.), Computation in  neurons and neural systems, pp. 223-228, Boston: Kluwer Academic Publishers.  Patel, G. Holleman, J., DeWeerth, S. Analog VLSI model of intersegmental  coordination with nearest-neighbor coupling. In, 1997.  Patel, G. A neuromorphic architecture for modelling intersegmental coordination.  Ph.D. dissertation, Georgia Institute of Technology, 1999.  J. Guckenheimer and P. Holmes. Nonlinear Oscillations, Dynamical Systems, and  Bifurcation of Vector Fields. Applied Mathematical Sciences, 42. Springer-Verlag,  New York, New York, Heidelberg, Berlin, 1983.  Morris, C. and Lecar, H. (1981) Voltage oscillations in the barnacle giant muscle fiber.  Biophys. J, 35: 193-213.  Rinzel, J. & Ermentrout, G.B. (1989) Analysis of Neural Excitability and Oscillations.  In C. Koch and I. $egev (eds) Methods in Neuronal Modeling from Synapses to  Networks. MIT press, Cambridge, MA.  Khibnik, A.I., Kuznetsov, Yu.A., Levitin, V.V., Nikolaev, E.V. (1993) Continuation  techniques and interactive software for bifurcation analysis of ODEs and iterated  maps. Physica D 62 (1-4): 360-367.  
Audio-Vision:  Using Audio-Visual Synchrony to Locate  Sounds  John Hershey *  j hersheycogsci. ucsd. edu  Department of Cognitive Science  University of California, San Diego  La Jolla, CA 92093-0515  Javier Movellan  movellancogsci. ucsd. edu  Department of Cognitive Science  University of California, San Diego  La Jolla, CA 92093-0515  Abstract  Psychophysical and physiological evidence shows that sound local-  ization of acoustic signals is strongly influenced by their synchrony  with visual signals. This effect, known as ventriloquism, is at work  when sound coming from the side of a TV set feels as if it were  coming from the mouth of the actors. The ventriloquism effect  suggests that there is important information about sound location  encoded in the synchrony between the audio and video signals. In  spite of this evidence, audiovisual synchrony is rarely used as a  source of information in computer vision tasks. In this paper we  explore the use of audio visual synchrony to locate sound sources.  We developed a system that searches for regions of the visual land-  scape that correlate highly with the acoustic signals and tags them  as likely to contain an acoustic source. We discuss our experience  implementing the system, present results on a speaker localization  task and discuss potential applications of the approach.  Introduction  We present a method for locating sound sources by sampling regions of an im-  age that correlate in time with the auditory signal. Our approach is inspired by  psychophysical and physiological evidence suggesting that audio-visual contingen-  cies play an important role in the localization of sound sources: sounds seem to  emanate from visual stimuli that are synchronized with the sound. This effect be-  comes particularly noticeable when the perceived source of the sound is known to  be false, as in the case of a ventriloquist's dummy, or a television screen. This  phenomenon is known in the psychophysical community as the ventriloquism effect,  defined as a mislocation of sounds toward their apparent visual source. The effect is  robust in a wide variety of conditions, and has been found to be strongly dependent  on the degree of "synchrony" between the auditory and visual signals (Driver, 1996;  Bertelson, Vroomen, Wiegeraad & de Gelder, 1994).  To whom correspondence should be addressed.  814 J. Hershey and J. R. Movellan  The ventriloquism effect is in fact less speech-specific than first thought. For exam-  ple the effect is not disrupted by an upside-down lip signal (Bertelson, Vroomen,  Wiegeraad & de Gelder, 1994) and is just as strong when the lip signals are re-  placed by light flashes that are synchronized with amplitude peaks in the audio  signal (Radeau & Bertelson, 1977). The crucial aspect here is correlation between  visual and auditory intensity over time. When the light flashes are not synchronized  the effect disappears.  The ventriloquism effect is strong enough to produce an enduring localization bias,  known as the ventriloquism aftereffect. Over time, experience with spatially offset  auditory-visual stimuli causes a persistent shift in subsequent auditory localization.  Exposure to audio-visual stimuli offset from each other by only 8 degrees of azimuth  for 20-30 minutes is sufficient to shift auditory localization by the same amount. A  corresponding shift in neural processing has been detected in macaque monkeys as  early as primary auditory cortex(Recanzone, 1998). In barn owls a misalignment  of visual and auditory stimuli during development causes the realignment of the  auditory and visual maps in the optic tectum(Zheng & Knudsen, 1999; Stryker,  1999; Feldman & Knudsen, 1997).  The strength of the psychophysical and physiological evidence suggests that audio-  visual contingency may be used as an important source of information that is cur-  rently underutilized in computer vision tasks. Visual and auditory sensor systems  carry information about the same events in the world, and this information must  be combined correctly in order for a useful interaction of the two modalities. Au-  diovisual contingency can be exploited to help determine which signals in different  modalities share a common origin. The benefits are two-fold: the two signals can  help localize each other, and once paired can help interpret each other. To this  effect we developed a system to localize speakers using input from a camera and  a single microphone. The approach is based on searching for regions of the image  which are "synchronized" with the acoustic signal.  Measuring Synchrony  The concept of audio-visual synchrony is not well formalized in the psychophysical  literature, so for a working definition we interpret synchrony as the degree of mutual  information between audio and spatially localized video signals. Ultimately it is a  causal relationship that we are often interested in, but causes 'can only be inferred  from effects such as synchrony. Let a(t)  11 ' be a vector describing the acoustic  signal at time t. The components of a(t) could be cepstral coefficients, pitch mea-  surements, or the outputs of a filter bank. Let v(x, y, t)  ]m be a vector describing  the visual signal at time t, pixel (x, y). The components of v(x, y, t) could represent  Gabor energy coefficients, RGB color values, etc.  Consider now a set of s audio and visual vectors $ = (a(tt), v(x, y, tt))t=k-s-1,... ,k  sampled at times tk-s-1,"' ,tk and at spatial coordinates (x,y). Given this set  of vectors our goal is to provide a number that describes the temporal contingency  between audio and video at time t. The approach we take is to consider each  vector in $ as an independent sample from a joint multivariate Gaussian process  (A(t), V(x, y, t)) and define audio-visual synchrony at time t as the estimate of  the mutual information between the audio and visual components of the process.  Let A(tk) .. A/'n(laA(t),EA(t)), and V(x,y,t) .. A/'m(lav(x,y,t),Ev(x,y,t)),  where/ represents means and E covariance matrices. Let A(tk) and V(x,y,t)  be jointly Gaussian, i.e., (A(t), V(x,y,t)) .. Af,+m(!A,V(X,y,t),EA,v(X,y, tk) ).  Audio 14sion: Using Audio-14sual Synchrony to Locate Sounds 815  The mutual information between A(x, y, tk) and V(tk) can be shown to be as follows  I(A(t); V(x,y,t))  + - V(x, y,  1  1 log(2re)lEA(t) ] +  log(2rre)mlEv(x,y,t) I  __1 log(2re)+mlEA,v(x, y, t ) ]  2  1 ]EA(tk)llEv(x,y,t)[  1og  (1)  (2)  In the special case that n - m - 1, then  1 log(1 - p2(x,y, tk)),  I(A(t); V(x,y, tk)) = -  (4)  where p(x, y, t) is the Pearson correlation coefficient between A(t) and V(x, y, t).  For each triple (x,y, t) we estimate the mutual information between A(tk) and  V(x,y,t) by considering each element of $ as an independent sample from the  random vector (A(t),V(x,y,t)). This amounts to computing estimates of the  joint covariance matrix EA,v(X, y, t). For example the estimate of the covariance  between the i th audio component and the jth video component would be as follows  SA,,vj(x,y, tk) = --  s--1  s--1  y(ai(tk-l) -- i(tk))(Vj(X,y, tk--l) -- Oj(x,y,t)),  /=0  (5)  where  s--1  I  ai(tk_l) '  /----0  s--1  I  vj(x,y,t_,).  =  /=0  (6)  (7)  (8)  These simple covariance estimates can be computed recursively in constant time  with respect to the number of timepoints. The independent treatment of pixels  would lend well to a parallel implementation.  To measure performance, a secondary system produces a single estimate of the  auditory location, for use with a database of labeled solitary audiovisual sources.  Unfortunately there are many ways of producing such estimates so it becomes dif-  ficult to separate performance of the measure from the underlying system. The  model used here is a centroid computation on the mutual information estimates,  with some enhancements to aid tracking and reduce background noise.  Implementation Issues  A real time system was prototyped using a QuickCam on the Linux operating  system and then ported to NT as a DirectShow filter. This platform provides input  from real-time audio and video capture hardware as well as from static movie files.  The video output could also be rendered live or compressed and saved in a movie  file. The implementation was challenging in that it turns out to be rather difficult  816 J. Hershey andJ. R. Movellan  (a) M is talking.  (b) J is talking.  Figure 1: Normalized audio and visual intensity across sequences of frames in which  a sequence of four numbers is spoken. The top trace is the contour of the acoustic  energy from one of two speakers, M or J, and the bottom trace is the contour of  intensity values for a single pixel, (147,100), near the mouth of J.  to process precisely time-synchronized audio and video on a serial machine in real  time. Multiple threads are required to read from the peripheral audio and visual  devices. By the time the audio and visual streams reach the AV filter module, they  are quite separate and asynchronous. The separately threaded auditory and visual  packet streams must be synchronized, buffered, and finally matched and aligned by  time-stamps before they can finally be processed. It is interesting that successful  biologial audiovisual systems employ a parallel architecture and thus avoid this  problem.  Results  To obtain a performance baseline we first tried the simplest possible approach:  A single audio and visual feature per location: n - ra = 1, v(x, y, t)  ]R is the  intensity of pixel (x, y) at time t, and a(t)  ]R is the average acoustic energy over the  interval [t- At, t], where At = 1/30 msec, the sampling period for the NTSC video  signal. Figure I illustrates the time course of these signals for a non-synchronous  and a synchronous pair of acoustic energy and pixel intensity. Notice in particular  that in the synchonous pair, l(b), where the sound and pixel values come from the  same speaker, the relationship between the signals changes over time. There are  regions of positive and negative covariance strung together in succession. Clearly  the relationship over the entire sequence is far from linear. However over shorter  time periods a linear relationship looks like a better approximation. Our window  size of 16 samples (i.e., s = 16 in 5 coincides approximately with this time-scale.  Perhaps by averaging over many small windows we can capture on a larger scale  what would be lost to the same method applied with a larger window. Of course  there is a trade-off in the time-scale between sensitivity to spurious transients, and  the response time of the system.  We applied this mutual information measure to all the pixels in a movie, in the  spirit of the perceptual maps of the brain. The result is a changing topographic  map of audiovisual mutual information. Figure 2 illustrates two snapshots in which  Audio I/ision: Using Audio-Iqsual Synchrony to Locate Sounds 817  .]  (a) Frame 206: M (at left) is talking.  (b) Frame 104: J (at right) is talking.  Figure 2: Estimated mutual information between pixel intensity and audio intensity  (bright areas indicate greater mutual information) overlaid on stills from the video  where one person is in mid-utterance.  different parts of the face are synchronous (possibly with different sign) with the  sound they take part in producing. It is interesting that the synchrony is shared  by some parts, such as the eyes, that do not directly contribute to the sound, but  contribute to the communication nonetheless. f  To estimate the position of the speaker we computed a centroid were each point was  weighted by the estimated mutual information between the correpsonding pixel and  the audio signal. At each time step the mutual information was estimated using 16  past frames (i.e., s = 16) In order to reduce the intrusion of spurious correlations  from competing targets, once a target has been found, we employ a Gaussian influ-  ence function. (Goodall, 1983) The influence function reduces the weight given to  mutual information from locations far from the current centroid when computing  the next centroid. To allow for the speedy disengagement from a dwindling source  of mutual information we set a threshold on the mutual information. Measurements  under the threshold are treated as zero. This threshold also reduces the effects of  unwanted background noise, such as camera and microphone jitter.  &(t) = y }-"y x O(log(1 - 152(x,y,t)))b(x,&(t - 1))  }-. }-.y O(log(1 - 152(x,y,t)))b(x,&(t - 1))  (9)  where (t) represents the estimate of the x coordinate for the position of the  speaker at time t. 0(.) is the thresholding function, and b(x, x(t- 1)) is the  influence function, which depends upon the'position x of the pixel being sampled  and the prior estimate x(t- 1). /52(x, y, t) is the estimate of the correlation between  the intensity in pixel (x, y) and the acoustic enery, when using the 16 past video   log(1 -/52 (x, y, t)) is the corresponding estimate of mutual information  frames. -   i cancels out in the quotient after adjusting the threshold function  (the factor, -  accordingly.)  We tried the approach on a movie of two people (M and J) taking turns while saying  random digits. Figure 3 shows the estimates of the actual positions of the speaker  818 Y. Hershey andY. R. Movellan  as a function of time. The estimates clearly provide information that could be used  to localize the speaker, especially in combination with other approaches (e.g., flesh  detection).  u3 120  o  '7 100   80   so  I,U  4O  20   '    0 100 200 300 400 500 600 700  Frame Number  Figure 3: Estimated and actual position of speaker at each frame for six hundred  flames. The sources, M and J, took turns uttering a series of four digits, for three  turns each. The actual positions and alternation times were measured by hand from  the video recording  Conclusions  We have presented exploratory work on a system for localizing sound sources on  a video signal by tagging regions of the image that are correlated in time with  the auditory signal. The approach was motivated by the wealth of evidence in  the psychophysical and physiological literature showing that sound localization is  strongly influenced by synchrony with the visual signal. We presented a measure  of local synchrony based on modeling the audio-visual signal as a non-stationary  Gaussian process. We developed a general software tool that accepts as inputs all  major video and audio file formats as well as direct input from a video camera. We  tested the tool on a speaker localization task with very encouraging results. The  approach could have practical applications for localizing sound sources in situations  where where acoustic stereo cues are inexistent or unreliable. For example the  approach could be used to help localize the actor talking in a video scene and put  closed-captioned text near the audio source. The approach could also be used to  guide a camera in teleconferencing applications.  While the results reported here are very encouraging, more work needs to be done  before practical applications are developed. For example we need to investigate  more sophisticated methods for processing the audio and video signals. At this  point we use average energy to represent the video and thus changes in the fun-  damental frequency that do not affect the average energy would not be captured  by our model. Similarly local video decompositions, like spatio-temporal Gabor  filtering, or approaches designed to enhance the lip regions may be helpful. The  Audio Vision: UsingAudio-Visual Synchrony to Locate Sounds 819  changing symmetry observed between audio and video signals might be addressed  rectifying or squaring the normalized signals and derivatives. Finally, relaxing the  Gaussian constraints in our measure of audio-visual contingency may help improve  performance. While the work shown here is exploratory at this point, the approach  is very promising: It emphasizes the idea of machine perception as a multimodal  process it is backed by psychophysical evidence, and when combined with other ap-  proaches it may help improve robustness in tasks such as localization and separation  of sound sources.  References  Bertelson, P., Vroomen, J., Wiegeraad, G., & de Gelder, B. (1994). Exploring the  relation between McGurk interference and ventriloquism. In Proceedings of  the 1995 International Conference on Spoken Language Processing, volume 2,  pages 559-562.  Driver, J. (1996). Enhancement of selective listening by illusory mislocation of  speech sounds due to lip-reading. Nature, 381, 66-68.  Feldman, D. E. & Knudsen, E. I. (1997). An anatomical basis for visual calibra-  tion of the auditiory space map in the barn owl's midbrain. The Journal of  Neuroscience, 17(17), 6820-6837.  Goodall, C. (1983). M-Estimators of Location: an outline of the theory. Wiley series  in probability and mathematical statistics. Applied probability and statistics.  Radeau, M. & Bertelson, P. (1977). Adaptation to auditory-visual discordance and  ventriloquism in semi-realistic situations. Perception and Psychophysics, 22,  137-146.  Recanzone, G. H. (1998). Rapidly induced auditory plasticity: The ventriloquism  aftereffect. Proceedings of the National Academy of Sciences, USA, 95, 869-875.  Stryker, M.P. (1999). Sensory Maps on the Move. Science, 925-926.  Zheng, W. & Knudsen, E. I. (1999). Functional Selection of Adaptive Auditory  Space Map by GABAA-Mediated Inhibition, 962-965.  
Correctness of belief propagation in Gaussian  graphical models of arbitrary topology  Yair Weiss  Computer Science Division  UC Berkeley, 485 Soda Hall  Berkeley, CA 94720-1776  Phone: 510-642-5029  yweiss @ cs. berkeley. ed u  William T. Freeman  Mitsubishi Electric Research Lab  201 Broadway  Cambridge, MA 02139  Phone: 617-621-7527  freeman @ merl.com  Abstract  Local "belief propagation" rules of the sort proposed by Pearl [15] are  guaranteed to converge to the correct posterior probabilities in singly  connected graphical models. Recently, a number of researchers have em-  pirically demonstrated good performance of "1oopy belief propagation"-  using these same rules on graphs with loops. Perhaps the most dramatic  instance is the near Shannon-limit performance of "Turbo codes", whose  decoding algorithm is equivalent to loopy belief propagation.  Except for the case of graphs with a single loop, there has been little theo-  retical understanding of the performance of 1oopy propagation. Here we  analyze belief propagation in networks with arbitrary topologies when  the nodes in the graph describe jointly Gaussian random variables. We  give an analytical formula relating the true posterior probabilities with  those calculated using loopy propagation. We give sufficient conditions  for convergence and show that when belief propagation converges it gives  the correct posterior means for all graph topologies, not just networks  with a single loop.  The related "max-product" belief propagation algorithm finds the max-  imum posterior probability estimate for singly connected networks. We  show that, even for non-Gaussian probability distributions, the conver-  gence points of the max-product algorithm in loopy networks are max-  ima over a particular large local neighborhood of the posterior proba-  bility. These results help clarify the empirical performance results and  motivate using the powerful belief propagation algorithm in a broader  class of networks.  Problems involving probabilistic belief propagation arise in a wide variety of applications,  including error correcting codes, speech recognition and medical diagnosis. If the graph  is singly connected, there exist local message-passing schemes to calculate the posterior  probability of an unobserved variable given the observed variables. Pearl [15] derived such  a scheme for singly connected Bayesian networks and showed that this "belief propagation"  algorithm is guaranteed to converge to the correct posterior probabilities (or "beliefs").  Several groups have recently reported excellent experimental results by running algorithms  674 Y. Weiss and W T. Freeman  equivalent to Pearl's algorithm on networks with loops [8, 13, 6]. Perhaps the most dramatic  instance of this performance is for "Turbo code" [2] error correcting codes. These codes  have been described as "the most exciting and potentially important development in coding  theory in many years" [12] and have recently been shown [10, 11] to utilize an algorithm  equivalent to belief propagation in a network with loops.  Progress in the analysis of loopy belief propagation has been made for the case of networks  with a single loop [17, 18, 4, 1]. For these networks, it can be shown that (1) unless  all the compatabilities are deterministic, 1oopy belief propagation will converge. (2) The  difference between the loopy beliefs and the true beliefs is related to the convergence rate  of the messages -- the faster the convergence the more exact the approximation and (3) If  the hidden nodes are binary, then the loopy beliefs and the true beliefs are both maximized  by the same assignments, although the confidence in that assignment is wrong for the loopy  beliefs.  In this paper we analyze belief propagation in graphs of arbitrary topology, for nodes de-  scribing jointly Gaussian random variables. We give an exact formula relating the correct  marginal posterior probabilities with the ones calculated using 1oopy belief propagation.  We show that if belief propagation converges, then it will give the correct posterior means  for all graph topologies, not just networks with a single loop. We show that the covari-  ance estimates will generally be incorrect but present a relationship between the error in  the covariance estimates and the convergence speed. For Gaussian or non-Gaussian vari-  ables, we show that the "max-product" algorithm, which calculates the MAP estimate in  singly connected networks, only converges to points that are maxima over a particular large  neighborhood of the posterior probability of loopy networks.  1 Analysis  To simplify the notation, we assume the graphical model has been preprocessed into an  undirected graphical model with pairwise potentials. Any graphical model can be con-  verted into this form, and running belief propagation on the pairwise graph is equivalent  to running belief propagation on the original graph [ 18]. We assume each node zi has a  local observation .i. In each iteration of belief propagation, each node zi sends a message  to each neighboring zj that is based on the messages it received from the other neighbors,  its local observation y and the pairwise potentials ij (xi, x j) and  ii (xi, yi). We assume  the message-passing occurs in parallel.  The idea behind the analysis is to build an unwrapped tree. The unwrapped tree is the  graphical model which belief propagation is solving exactly when one applies the belief  propagation rules in a loopy network [9, 20, 18]. It is constructed by maintaining the same  local neighborhood structure as the 1oopy network but nodes are replicated so there are no  loops. The potentials and the observations are replicated from the loopy graph. Figure 1 (a)  shows an unwrapped tree for the diamond shaped graph in (b). By construction, the belief  at the root node a is identical to that at node zx in the loopy graph after four iterations of  belief propagation. Each node has a shaded observed node attached to it, omitted here for  clarity.  Because the original network represents jointly Gaussian variables, so will the unwrapped  tree. Since it is a tree, belief propagation is guaranteed to give the correct answer for the  unwrapped graph. We can thus use Gaussian marginalization formulae to calculate the  true mean and variances in both the original and the unwrapped networks. In this way, we  calculate the accuracy of belief propagation for Gaussian networks of arbitrary topology.  We assume that the joint mean is zero (the means can be added-in later). The joint distri-  Correctness of Belief Propagation 675  xl  x2  x5  Figure 1: Left: A Markov network with multiple loops. Right: The unwrapped network  corresponding to this structure.  bution of z = is given by P(z) = ae- where V = . It  y ' vvv  is straightforward to construct the inverse covariance matrix V of the joint Gaussian that  describes a given Gaussian graphical model [3].  Writing out the exponent of the joint and completing the square shows that the mean t of  x, given the observations y, is given by:  = - Vvy , (1)  and the covariance matrix C'i v of x given y is: C=i v: g x. We will denote by C,iy the  ith row of Clv so the marginal posterior variance of zi given the data is a 2 (i) = C, Iv (i).  We will use ~ for unwrapped quantities. We scan the tree in breadth first order and denote by  . the vector of values in the hidden nodes of the tree when so scanned. Simlarly, we denote  by 0 the observed nodes scanned in the same order and P, Pv the inverse covariance  matrices. Since we are scanning in breadth first order the last nodes are the leaf nodes and  we denote by L the number of leaf nodes. By the nature of unwrapping, (1) is the mean  of the belief at node zx after t iterations of belief propagation, where t is the number of  unwrappings. Similarly 2(1) -- 'xlv(1) is the variance of the belief at node x after t  iterations.  Because the data is replicated we can write 0 = Oy where O(i, j) = 1 if 0i is a replica ofyj  and 0 otherwise. Since the potentials I'(zi, Yi) are replicated, we can write PvO = OVer.  Since the I, (zi, z j) are also replicated and all non-leaf :i have the same connectivity as  the corresponding zi, we can write lzO = OVzz + E where E is zero in all but the last  L rows. When these relationships between the loopy and unwrapped inverse covariance  matrices are substituted into the loopy and unwrapped versions of equation 1, one obtains  the following expression, true for any iteration [ 19]:  (1) =/(1) + zlve  (2)  where e is a vector that is zero everywhere but the last L components (corresponding to the  leaf nodes). Our choice of the node for the root of the tree is arbitrary, so this applies to  all nodes of the 1oopy network. This formula relates, for any node of a network with loops,  the means calculated at each iteration by belief propagation with the true posterior means.  Similarly when the relationship between the loopy and unwrapped inverse covariance ma-  trices is substituted into the loopy and unwrapped definitions of Ci v we can relate the  676 Y. Weiss and W. T. Freeman  0.5  0.4  0.3  0.2  0.1  0  --0.1  --0.2  0 40 60 80  node  oo  Figure 2: The conditional correlation between the root node and all other nodes in the  unwrapped tree of Fig. I after eight iterations. Potentials were chosen randomly. Nodes  are presented in breadth first order so the last elements are the correlations between the root  node and the leaf nodes. We show that if this correlation goes to zero, belief propagation  converges and the loopy means are exact. Symbols plotted with a star denote correlations  with nodes that correspond to the node :r in the loopy graph. The sum of these correlations  gives the correct variance of node :ca while loopy propagation uses only the first correlation.  marginalized covariances calculated by belief propagation to the true ones [ 19]:  2(1) = a2(1) + '=,lvel - '=,lve9. (3)  where el is a vector that is zero everywhere but the last L components while e2 is equal  to I for all nodes in the unwrapped tree that are replicas of :ca except for Ea. All other  components of e2 are zero  Figure 2 shows 'x Iv for the diamond network in Fig. 1. We generated random potential  functions and observations and calculated the conditional correlations in the unwrapped  tree. Note that the conditional correlation decreases with distance in the tree m we are  scanning in breadth first order so the last L components correspond to the leaf nodes.  As the number of iterations of loopy propagation is increased the size of the unwrapped  tree increases and the conditional correlation between the leaf nodes and the root node  decreases.  From equations 2-3 it is clear that if the conditional correlation between the leaf nodes and  the root nodes are zero for all sufficiently large unwrappings then (1) belief propagation  converges (2) the means are exact and (3) the variances may be incorrect. In practice the  conditional correlations will not actually be equal to zero for any finite unwrapping. In [ 19]  we give a more precise statement: if the conditional correlation of the root node and the  leaf nodes decreases rapidly enough then (1) belief propagation converges (2) the means  are exact and (3) the variances may be incorrect. We also show sufficient conditions on the  potentials (xi, xj) for the correlation to decrease rapidly enough: the rate at which the  correlation decreases is determined by the ratio of off-diagonal and diagonal components  in the quadratic form defining the potentials [19].  How wrong will the variances be? The term Ox lye2 in equation 3 is simply the sum of  many components of O-lv- Figure 2 shows these components. The correct variance is  the sum of all the components while the belief propagation variance approximates this sum  with the first (and dominant) term. Whenever there is a positive correlation between the  root node and other replicas of z the loopy variance is strictly less than the true variance  m the 1oopy estimate is overconfident.  Correctness of Belief Propagation 677  (a)  0  o  0.  0.5   0.4  :o  0.1  0 10 0 50 60  R  30  iterations  (b)  Figure 3: (a) 25 x 25 graphical model for simulation. The unobserved nodes (untilled) were  connected to their four nearest neighbors and to an observation node (filled). (b) The error  of the estimates of loopy propagation and successive over-relaxation (SOR) as a function  of iteration. Note that belief propagation converges much faster than SOR.  Note that when the conditional correlation decreases rapidly to zero two things happen.  First, the convergence is faster (because 'x Ite approaches zero faster). Second, the ap-  proximation error of the variances is smaller (because 'x ite2 is smaller). Thus we have  shown, as in the single loop case, quick convergence is correlated with good approximation.  2 Simulations  We ran belief propagation on the 25 x 25 2D grid of Fig. 3 a. The joint probability was:  P(x, y) = exp(- E wij(xi - xj)  - E wii(xi - yi) 2)  j   (4)  where wij = 0 if nodes zi, zj are not neighbors and 0.01 otherwise and wii was randomly  selected to be 0 or I for all i with probability of 1 set to 0.2. The observations yi were  chosen randomly. This problem corresponds to an approximation problem from sparse  data where only 20% of the points are visible.  We found the exact posterior by solving equation 1. We also ran belief propagation and  found that when it converged, the calculated means were identical to the true means up  to machine precision. Also, as predicted by the theory, the calculated variances were too  small -- the belief propagation estimate was overconfident.  In many applications, the solution of equation I by matrix inversion is intractable and iter-  ative methods are used. Figure 3 compares the error in the means as a function of iterations  for 1oopy propagation and successive-over-relaxation (SOR), considered one of the best  relaxation methods [ 16]. Note that after essentially five iterations loopy propagation gives  the right answer while SOR requires many more. As expected by the fast convergence, the  approximation error in the variances was quite small. The median error was 0.018. For  comparison the true variances ranged from 0.01 to 0.94 with a mean of 0.322. Also, the  nodes for which the approximation error was worse were indeed the nodes that converged  slower.  678 Y Weiss and W. T. Freeman  3 Discussion  Independently, two other groups have recently analyzed special cases of Gaussian graphical  models. Frey [7] analyzed the graphical model corresponding to factor analysis and gave  conditions for the existence of a stable fixed-point. Rusmevichientong and Van Roy [14]  analyzed a graphical model with the topology of turbo decoding but a Gaussian joint den-  sity. For this specific graph they gave sufficient conditions for convergence and showed  that the means are exact.  Our main interest in the Gaussian case is to understand the performance of belief propaga-  tion in general networks with multiple loops. We are struck by the similarity of our results  for Gaussians in arbitrary networks and the results for single loops of arbitrary distribu-  tions [ 18]. First, in single loop networks with binary nodes, loopy belief at a node and the  true belief at a node are maximized by the same assignment while the confidence in that  assignment is incorrect. In Gaussian networks with multiple loops, the mean at each node  is correct but the confidence around that mean may be incorrect. Second, for both single-  loop and Gaussian networks, fast belief propagation convergence correlates with accurate  beliefs. Third, in both Gaussians and discrete valued single loop networks, the statistical  dependence between root and leaf nodes governs the convergence rate and accuracy.  The two models are quite different. Mean field approximations are exact for Gaussian  MRFs while they work poorly in sparsely connected discrete networks with a single loop.  The results for the Gaussian and single-loop cases lead us to believe that similar results  may hold for a larger class of networks.  Can our analysis be extended to non-Gaussian distributions? The basic idea applies to  arbitrary graphs and arbitrary potentials: belief propagation is performing exact inference  on a tree that has the same local neighbor structure as the 1oopy graph. However, the linear  algebra that we used to calculate exact expressions for the error in belief propagation at any  iteration holds only for Gaussian variables.  We have used a similar approach to analyze the related "max-product" belief propagation  algorithm on arbitrary graphs with arbitrary distributions [5] (both discrete and continuous  valued nodes). We show that if the max-product algorithm converges, the max-product  assignment has greater posterior probability then any assignment in a particular large region  around that assignment. While this is a weaker condition than a global maximum, it is much  stronger than a simple local maximum of the posterior probability.  The sum-product and max-product belief propagation algorithms are fast and paralleliz-  able. Due to the well known hardness of probabilistic inference in graphical models, belief  propagation will obviously not work for arbitrary networks and distributions. Nevertheless,  a growing body of empirical evidence shows its success in many networks with loops. Our  results justify applying belief propagation in certain networks with multiple loops. This  may enable fast, approximate probabilistic inference in a range of new applications.  References  [1] S.M. Aji, G.B. Horn, and R.J. McEliece. On the convergence of iterative decoding on graphs  with a single cycle. In Proc. 1998 ISIT, 1998.  [2] C. Berrou, A. Glavieux, and P. Thitimajshima. Near Shannon limit error-correcting coding and  decoding: Turbo codes. In Proc. IEEE International Communications Conference '93, 1993.  [3] R. Cowell. Advanced inference in Bayesian networks. In M.I. Jordan, editor, Learning in  Graphical Models. MIT Press, 1998.  [4] G.D. Fomey, ER. Kschischang, and B. Marcus. Iterative decoding of tail-biting trellisses.  preprint presented at 1998 Information Theory Workshop in San Diego, 1998.  Correctness of Belief Propagation 679  [9]  [101  [11]  [12]  [13]  [14]  [15]  [16]  [171  [18]  [5] W. T Freeman and Y. Weiss. On the fixed points of the max-product algorithm. Technical  Report 99-39, MERL, 201 Broadway, Cambridge, MA 02139, 1999.  [6] W.T Freeman and E.C. Pasztor. Learning to estimate scenes from images. In M.S. Kearns,  S.A. Solla, and D.A. Cohn, editors, Adv. Neural Information Processing Systems 1 I. MIT Press,  1999.  [7] B.J. Frey. Turbo factor analysis. In Adv. Neural Information Processing Systems 12. 2000. to  appear.  [8] Brendan J. Frey. Bayesian Networks for Pattern Classification, Data Compression and Channel  Coding. MIT Press, 1998.  R.G. Gallager. Low Density Parity Check Codes. MIT Press, 1963.  F. R. Kschischang and B. J. Frey. Iterative decoding of compound codes by probability propaga-  tion in graphical models. IEEE Journal on Selected Areas in Communication, 16(2):219-230,  1998.  R.J. McEliece, D.J.C. MackKay, and J.F. Cheng. Turbo decoding as as an instance of Pearl's  'belief propagation' algorithm. IEEE Journal on Selected Areas in Communication, 16(2): 140-  152, 1998.  R.J. McEliece, E. Rodereich, and J.F. Cheng. The Turbo decision algorithm. In Proc. 33rd  Allerton Conference on Communications, Control and Computing, pages 366-379, Monticello,  IL, 1995.  K.P. Murphy, Y. Weiss, and MI. Jordan. Loopy belief propagation for approximate inference:  an empirical study. In Proceedings of Uncertainty in AI, 1999.  Rusmevichientong P. and Van Roy B. An analysis of Turbo decoding with Gaussian densities.  In Adv. Neural Information Processing Systems 12. 2000. to appear.  Judea Pearl. Probabilistic Reasoning in Intelligent Systems: Networks of.Plausible Inference.  Morgan Kaufmann, 1988.  Gilbert Strang. Introduction to Applied Mathematics. Wellesley-Cambridge, 1986.  Y. Weiss. Belief propagation and revision in networks with loops. Technical Report 1616, MIT  AI lab, 1997.  Y. Weiss. Correctness of local probability propagation in graphical models with loops. Neural  Computation, to appear, 2000.  [19] Y. Weiss and W. T. Freeman. Loopy propagation gives the correct posterior means for  Gaussians. Technical Report UCB.CSD-99-1046, Berkeley Computer Science Dept., 1999.  www.cs.berkeley.edu yweiss/.  [20] N. Wiberg. Codes and decoding on general graphs. PhD thesis, Department of Electrical  Engineering, U. Linkoping, Sweden, 1996.  
Gaussian Fields for Approximate Inference  in Layered Sigmoid Belief Networks  David Barber*  Stichting Neurale Netwerken  Medical Physics and Biophysics  Nijmegen University, The Netherlands  barberdaston. ac. uk  Peter Sollich  Department of Mathematics  King's College, University of London  London WC2R 2LS, U.K.  peter. sollichkcl. ac. uk  Abstract  Layered Sigmoid Belief Networks are directed graphical models  in which the local conditional probabilities are parameterised by  weighted sums of parental states. Learning and inference in such  networks are generally intractable, and approximations need to be  considered. Progress in learning these networks has been made by  using variational procedures. We demonstrate, however, that vari-  ational procedures can be inappropriate for the equally important  issue of inference - that is, calculating marginMs of the network.  We introduce an alternative procedure, based on assuming that the  weighted input to a node is approximately Gaussian distributed.  Our approach goes beyond previous Gaussian field assumptions in  that we take into account correlations between parents of nodes.  This procedure is specialized for calculating marginals and is sig-  nificantly faster and simpler than the variational procedure.  I Introduction  Layered Sigmoid Belief Networks [1] are directed graphical models [2] in which  the local conditional probabilities are parameterised by weighted sums of parental  states, see fig(l). This is a graphical representation of a distribution over a set of  binary variables si 6 {0, 1}. Typically, one supposes that the states of the nodes  at the bottom of the network are generated by states in previous layers. Whilst, in  principle, there is no restriction on the number of nodes in any layer, typically, one  considers structures similar to the "fan out" in fig(l) in which higher level layers  provide an "explanation" for patterns generated in lower layers. Such graphical  models are attractive since they correspond to layers of information processors, of  potentially increasing complexity. Unfortunately, learning and inference in such net-  works is generally intractable, and approximations need to be considered. Progress  in learning has been made by using variational procedures [3, 4, 5]. However, an-  other crucial aspect remains inference [2]. That is, given some evidence (or none),  calculate the marginal of a variable, conditional on this evidence. This assumes  that we have found a suitable network from some learning procedure, and now wish  *Present Address: NCRG, Aston University, Birmingham B4 7ET, U.K.  394 D. Barber and P. Sollich  to query this network. Whilst the variational procedure is attractive for learning,  since it generally provides a bound on the likelihood of the visible units, we demon-  strate that it may not always be equally appropriate for the inference problem.  A directed graphical model defines a distribution over  a set of variables s = (sx ...sn) that factorises into  the local conditional distributions,  p(sx...s,) = l'Ip(silri) (1)  i:1  where ri denotes the parent nodes of node i. In a  layered network, these are the nodes in the proceed-  ing layer that feed into node i. In a sigmoid belief  network the local probabilities are defined as  p(si--ll7ri)--r (Ewijsj-Oi)-o'(hi)  J  Figure 1: A Layered Sig-  moid Belief Network  (2)  where the "field" at node/is defined as hi = Y'.j w,jsj +Oi and (r(h) = 1/(1 +e-n).  wij is the strength of the connection between node i and its parent node j; if j is  not a parent of i we set wlj = O. Oi is a bias term that gives a parent-independent  bias to the state of node i.  We are interested in inference - in particular, calculating marginals of the network  for cases with and without evidential nodes. In section (2) we describe how to  approximate the quantities p(si -- 1) and discuss in section (2.1) why our method  can improve on the standard variational mean field theory. Conditional marginals,  such as p(si = 11s j = 1, sk = 0) are considered in section (3).  2 Gaussian Field Distributions  Under the 0/1 coding for the variables si, the mean of a variable, mi is given by the  probability that it is in state 1. Using the fact from (2) that the local conditional  distribution of node i is dependent on its parents only through its field hi, we have  rni =p(si: 1)= f p, = l[hi)p(hi)dhi _= (o'(hi))p(h,) (3)  where we use the notation ((.))p to denote an average with respect to the distri-  bution p. If there are many parents of node i, a reasonable assumption is that the  distribution of the field hi will be Gaussian, p(hi) -, N (lui, efT). Under this Gaus-  sian Field (GF) assumption, we need to work out the mean and variance, which are  given by  = = + o, = + O, (4)  J J  (5)  j,k  where Rjk = (AsjAs). We use the notation A (.) -- (.) - ((.)).  The diagonal terms of the node covariance matrix are Rii - mi (1 - mi). In contrast  to previous studies, we include off diagonal terms in the calculation of R [4]. From  Gaussian Fields for Approximate Inference 395  (5) we only need to find correlations between parents i and j of a node. These are  easy to calculate in the layered networks that we are considering, because neither i  nor j is a descendant of the other:  Rid --p(si -- 1, sj -- 1) - mimj  = fp(, = = 11hj)p(hi,hj)dh-mimj  = {(r (hi) (r (hj))p(n,,ni) - mimj  Assuming that the joint distribution p(hi, hj) is Gaussian,  and covariance, given by  l  (6)  (7)  (8)  we again need its mean  E,j = {AhiAhj) = Z w,kwj, {AskAs,) = Z wiwjtR, (10)  kl kl  Under this scheme, we have a closed set of equations, (4,5,8,10) for the means  mi and covariance matrix tij which can be solved by forward propagation of the  equations. That is, we start from nodes without parents, and then consider the  next layer of nodes, repeating the procedure until a full sweep through the network  has been completed. The one and two dimensional field averages, equations (3)  and (8), are computed using Gaussian Quadrature. This results in an extremely  fast procedure for approximating the marginals mi, requiring only a single sweep  through the network.  Our approach is related to that of [6] by the common motivating assumption that  each node has a large number of parents. This is used in [6] to obtain actual  bounds on quantities of interest such as joint marginals. Our approach does not  give bounds. Its advantage, however, is that it allows fluctuations in the fields hi,  which are effectively excluded in [6] by the assumed scaling of the weights wij with  the number of parents per node.  2.1 Relation to Variational Mean Field Theory  In the variational approach, one fits a tractable approximating distribution Q to  the SBN. Taking Q factorised, Q(s) - 1-Ii rn'(1 - rni) x-' we have the bound  lnp ( sx . . . sn ) _> E {-mi lnmi - (1 - mi ) In (1 - mi ) }  i  +Z{Zmiwijmj+Oimi_(ln(l+eh'))Q} (11)  i j  The final term in (11) causes some difficulty even in the case in which Q is a fac-  torised model. Formally, this is because this term does not have the same graphical  structure as the tractable model Q. One way around around this difficulty is to em-  ploy a further bound, with associated variational parameters [7]. Another approach  is to make the Gaussian assumption for the field hi as in section (2). Because Q is  factorised, corresponding to a diagonal correlation matrix R, this gives [4]  (ln (1 + e n') )Q 0 (ln (1 + e'))N(,,a) (12)  396 D. Barber and P. Sollich  where/ui - -.j wijmj q- Oi and eri 2 -- -]j wi2jmj(1 - mj). Note that this is a one  dimensional integral of a smooth function. In contrast to [4] we therefore evaluate  this quantity using Gaussian Quadrature. This has the advantage that no extra  variational parameters need to be introduced. Technically, the assumption of a  Gaussian field distribution means that (11) is no longer a bound. Nevertheless, in  practice it is found that this has little effect on the quality of the resulting solution.  In our implementation of the variational approach, we find the optimal parameters  mi by maximising the above equation for each component mi separately, cycling  through the nodes until the parameters mi do not change by more than 10 -.  This is repeated 5 times, and the solution with the highest bound score is chosen.  Note that these equations cannot be solved by forward propagation alone since the  final term contains contributions from all the nodes in the network. This is in  contrast to the GF approach of section (2). Finding appropriate parameters mi by  the variational approach is therefore rather slower than using the GF method.  In arriving at the above equations, we have made two assumptions. The first is  that the intractable distribution is well approximated by a factorised model. The  second is that the field distribution is Gaussian. The first step is necessary in  order to obtain a bound on the likelihood of the model (although this is slightly  compromised by the Gaussian fielc[ assumption). In the GF approach we dispense  with this assumption of an effectively factorised network (partially because if we  are only interested in inference, a bound on the model likelihood is less relevant).  The GF method may therefore prove useful for a broader class of networks than the  variational approach.  2.2 Results for unconditional marginals  We compared three procedures for estimating the conditional values p(si = 1) for  all the nodes in the network, namely the variational theory, as described in section  (2.1), the diagonal Gaussian field theory, and the non-diagonal Gaussian field theory  which includes correlation effects between parents. Results for small weight values  wij are shown in fig(2). In this case, all three methods perform reasonably well,  although there is a significant improvement in using the GF methods over the  variational procedure; parental correlations are not important (compare figs(2b)  and (2c)). In fig(3) the weights and biases are chosen such that the exact mean  variables mi are roughly 0.5 with non-trivial correlation effects between parents.  Note that the variational mean field theory now provides a poor solution, whereas  the GF methods are relatively accurate. The effect of using the non-diagonal R  terms is beneficial, although not dramatically so.  3 Calculating Conditional Marginals  We consider now how to calculate conditional marginals, given some evidential  nodes. (In contrast to [6], any set of nodes in the network, not just output nodes,  can be considered evidential.) We write the evidence in the following manner  The quantities that we are interested in are conditional marginals which, from Bayes  rule are related to the joint distribution by  p(s = lIE)= p(s = 1, E)  p(si = O,E) +p(s = 1, E) (13)  That is, provided that we have a procedure for estimating joint marginals, we can  obtain conditional marginals too. Without loss of generality, we therefore consider  Gaussian Fields for Approximate Inference 397  2O  15  10  Erro using factodeed model fit  0 002 004 006  Error using Gaussian Field. Dagonal covariance  0.002 0 004 0 006 0.008 0.01  Error using Gaussian Field, Non Diagonal covariance  (a) Mean error = 0.0377  (b) Mean error = 0.0018  (c) Mean error = 0.0017  Figure 2: Error in approximating p(si = 1) for the network in fig(l), averaged over  all the nodes in the network. In each of 100 trials, weights were drawn from a  zero mean, unit variance Gaussian; biases were set to 0. Note the different scale  in (b) and (c). In (a) we use the variational procedure with a factorised Q, as  in section (2.1). In (b) we use the Gaussian field equations, assuming a diagonal  covariance matrix R. This procedure was repeated in (c) including correlations  between parents.  E + = E U {si = 1}, which then contains n + 1 "evidential" variables. That is, the  desired marginal variable is absorbed into the evidence set. For convenience, we  then split the nodes into two sets, those containing the evidential or "clamped"  nodes, C, and the remaining "free" nodes F. The joint evidence is then given by  p(E +) = Ep(Ec,,...Ec+,,sf,,...sf.) (14)   7r* 71'* *  =  , (15)  v]ues  specified [ +.  he sgmo[d belief ewo  =  (2S - 1) ws + Ok ' si = otherwise  is therefore determined by the distribution of the field h; =  wks +0.  Examining (15), we see that the product over the "free" nodes defines a SBN in  which the local probability distributions are given by those of the original network,  but with any evidential parental nodes clamped to their evidence values. Therefore,  Consistent with our previous assumptions, we assume that the distribution of the  ( ')  fields h* - h...hc+ is jointly Gaussian. We can then find the mean and  covariance matrix for the distribution of h* by repeating the calculation of section  (2) in which evidential nodes have been clamped to their evidence values. Once this  Gaussian has been determined, it can be used in (17) to determine p(E +). Gaussian  averages of products of sigmoids are calculated by drawing 1000 samples from the  Gaussian over which we wish to integrate x. Note that if there are evidential nodes  In one and two dimensions (n = 0, 1), or n = 1, we use Gaussian Quadrature.  398 D. Barber and P Sollich  Error using factorised model fit Error using Gaussian Field, Diagonal covariance Error using Gaussian Field, Non Diagonal covariance  30  20 10 1011_  o o., o o o. o o. o o o. o. o. o. o o o, o' o. o. o. .  (a) Mean error = 0.4188  (b) Mean error = 0.0253  (c) Mean error = 0.0198  Figure 3: All weights are set to uniformly from 0 to 50. Biases are set to -0.5 of  the summed parental weights plus a uniform random number from -2.5 to 2.5. The  root node is set to be 1 with probability 0.5. This has the effect of making all the  nodes in the exact network roughly 0.5 in mean, with non-negligible correlations  between parental nodes. 160 simulations were made.  in different layers, we require the correlations between their fields h to evaluate (17).  Such 'inter-layer' correlations were not required in section (2), and to be able to use  the same calculational scheme we simply neglect them. (We leave a study of the  effects of this assumption for future work.) The average in (17) then factors into  groups, where each group contains evidential terms in a particular layer.  The conditional marginal for node i is obtained from repeating the above procedure  in which the desired marginal node is clamped to its opposite value, and then using  these results in (13). The above procedure is repeated for each conditional marginal  that we are interested in. Although this may seem computationally expensive, the  marginal for each node is computed quickly, since the equations are solved by one  forward propagation sweep only.  6O  50  40  30  2O  10  0  0  Error using factorised model fit  Error using Gaussian Field, Diagonal covariance  0.1 0.2 03 0.4 0.5 0.6 0 0.1 0.2 0.3 0.4 0.5 06  Error using Gaussian Field, Non Diagonal covariance  ) 01 02 0.3 04 0.5 06  (a) Mean error - 0.1534  (b) Mean error = 0.0931  (c) Mean error = 0.0865  Figure 4: Estimating the conditional marginal of the top node being in state 1,  given that the four bottom nodes are in state 1. Weights were drawn from a zero  mean Gaussian with variance 5, with biases set to -0.5 the summed parental weights  plus a uniform random number from -2.5 to 2.5. Results of 160 simulations.  3.1 Results for conditional marginals  We used the same structure as in the previous experiments, as shown in fig(l). We  are interested here in calculating the probability that the top node is in state 1,  Gaussian Fields for Approximate Inference 399  given that the four bottom nodes are in state 1. Weights were chosen from a zero  mean Gaussian with variance 5. Biases were set to negative half Of the summed  parent weights, plus a uniform random value from -2.5 to 2.5. Correlation effects  in these networks are not as.strong as in the experiments in section (2.2), although  the improvement of the GF theory over the variational theory seen in fig(4) remains  clear. The improvement from the off diagonal terms in R is minimal.  4 Conclusion  Despite their appropriateness for learning, variational methods may not be equally  suited to inference, making more tailored methods attractive. We have considered  an approximation procedure that is based on assuming that the distribution of the  weighted input to a node is approximately Gaussian. Correlation effects between  parents of a node were taken into account to improve the Gaussian theory, although  in our examples this gave only relatively modest improvements.  The variational mean field theory performs poorly in networks with strong cor-  relation effects between nodes. On the other hand, one may conjecture that the  Gaussian Field approach will not generally perform catastrophically worse than the  factorised variational mean field theory. One advantage of the variational theory  is the presence of an objective function against which competing solutions can be  compared. However, finding an optimum solution for the mean parameters mi from  this function is numerically complex. Since the Gaussian Field theory is extremely  fast to solve, an interesting compromise might be to prime the variational solution  with the results from the Gaussian Field theory.  Acknowledgments  DB would like to thank Bert Kappen and Wim Wiegerinck for stimulating and  helpful discussions. PS thanks the Royal Society for financial support.  References [1] R. Neal. Connectionist learning of Belief Networks. Artificial Intelligence, 56:71-113,  1992.  [2] E. Castillo, J. M. Gutierrez, and A. S. Hadi. Expert Systems and Probabilistic Network  Models. Springer, 1997.  [3] M. I. Jordan, Z. Gharamani, T. S. Jaakola, and L. K. Saul. An Introduction to Vari-  ational Methods for Graphical Models. In M. I. Jordan, editor, Learning in Graphical  Models, pages 105-161. Kluwer, 1998.  [4] L. Saul and M. I. Jordan. A mean field learning algorithm for unsupervised neural  networks. In M. I. Jordan, editor, Learning in Graphical Models, 1998.  [5] D. Barber and W Wiegerinck. Tractable variational structures for approximating  graphical models. In M.S. Kearns, S.A. Solla, and D.A. Cohn, editors, Advances in  Neural Information Processing Systems NIPS 11. MIT Press, 1999.  [6] M. Kearns and L. Saul. Inference in Multilayer Networks via Large Deviation Bounds.  In Advances in Neural Information Processing Systems NIPS 11, 1999.  [7] L. K. Saul, T. Jaakkola, and M. I. Jordan. Mean Field Theory for Sigmoid Belief  Networks. Journal of Artificial Intelligence Research, 4:61-76, 1996.  
Search for Information Bearing  Components in Speech  Howard Hua Yang and Hynek Hermansky  Department of Electrical and Computer Engineering  Oregon Graduate Institute of Science and Technology  20000 NW, Walker Rd., Beaverton, OR97006, USA  {hyang,hynek)@ece.ogi.edu, FAX:503 7481406  Abstract  In this paper, we use mutual information to characterize the dis-  tributions of phonetic and speaker/channel information in a time-  frequency space. The mutual information (MI) between the pho-  netic label and one feature, and the joint mutual information (JMI)  between the phonetic label and two or three features are estimated.  The Miller's bias formulas for entropy and mutual information es-  timates are extended to include higher order terms. The MI and  the JMI for speaker/channel recognition are also estimated. The  results are complementary to those for phonetic classification. Our  results show how the phonetic information is locally spread and  how the speaker/channel information is globally spread in time  and frequency.  I Introduction  Speech signals typically carry information about number of target sources such  as linguistic message, speaker identity, and environment in which the speech was  produced. In most realistic applications of speech technology, only one or a few in-  formation targets are important. For example, one may be interested in identifying  the message in the signal regardless of the speaker or the environments in which  the speech was produced, or the identification of the speaker is needed regardless  of the words the targeted speaker is saying. Thus, not all components of the signal  may be equally relevant for a decoding of the targeted information in the signal.  The speech research community has at its disposal rather large speech databases  which are mainly used for training and testing automatic speech recognition (ASR)  systems. There have been relatively few efforts to date to use such databases  for deriving reusable knowledge about speech and speech communication processes  which could be used for improvements of ASR technology. In this paper we apply  information-theoretic approaches to study a large hand-labeled data set of fluent  speech to learn about the information structure of the speech signal including the  distribution of speech information in frequency and in time.  Based on the labeled data set, we analyze the relevancy of the features for phonetic  804 H. H. Yang and H. Hermansky  classifications and speaker/channel variability. The features in this data set are  labeled with respect to underlying phonetic classes and files from which the features  come from. The phoneme labels relate to the linguistic message in the signal, and  the file labels carry the information about speakers and communication channels  (each file contains speech of a single speaker transmitted through one telephone  channel). Thus, phoneme and file labels are two target variables for statistical  inference. The phoneme labels take 19 different values corresponding to 19 broad  phoneme categories in the OGI Stories database [2]. The file labels take different  values representing different speakers in the OGI Stories database.  The relevancy of a set of features is measured by the joint mutual information (JMI)  between the features and a target variable. The phoneme target variable represents  in our case the linguistic message. The file target variable represents both different  speakers and different telephone channels. The joint mutual information between  a target variable and the features quantifies the relevancy of the features for that  target variable.  Mutual information measure the statistical dependence between random variables.  Morris et al (1993) used mutual information to find the critical points of informa-  tion for classifying French Vowel-Plosive-Vowel utterances. Bilmes(1998) showed  recently that the information appears to be spread over relatively long tempo-  ral spans. While Bilmes used mutual information between two variables on non-  labeled data to reveal the mutual dependencies between the components of the  spectral energies in time and frequency, we focused on joint mutual information  between the phoneme labels or file labels and one, two or three feature variables  in the time-frequency planet7, 6] and used this concept to gain insight into how  information about phonemes and speaker/channel variability is distributed in the  time-frequency plane.  2 Data Set and Preprocessing  The data set used in this paper is 3-hour phonetically labeled telephone speech, a  subset of the English portion (Stories) of the OGI multi-lingual database [2] contain-  ing approximately 50 seconds of extemporaneous speech from each of 210 different  speakers. The speech data is labeled by a variable Y taking 19 values represent-  ing 19 most often occurring phoneme categories. The average phoneme duration is  about 65 ms and the number of phoneme instances is 65421.  Acoustic features X(fk, t) for the experiments are derived from a short-time anal-  ysis of the speech signal with a 20 ms analysis window (Hamming) at the frame t  advanced in 10 ms steps. The logarithmic energy at a frequency fk is computed  from the squared magnitude FFT using a critical-band spaced (log-like in the fre-  quency variable) weighting function in a manner similar to that of the computation  of Perceptual Linear Prediction coefficients [3]. In particular, the 5-th, 8-th and 12-  th bands are centered around 0.5, i and 2 kHz respectively. Each feature X(f, t) is  labeled by a phoneme label YP(t) and a file label Yf(t). We use mutual information  to measure the relevancy of X(f, t - d) across all frequencies f and in a context  window -D _< d _< +D for the phoneme classification and the speaker/channel  identification.  3 Estimation of MI and Bias Correction  In this paper, we only consider the mutual information (MI) between discrete ran-  dom variables. The phoneme label and the file label are discrete random variables.  Search for Information Bearing Components in Speech 805  However, the feature variables are bounded continuous variables. To obtain the  quantized features, we divide the maximum range of the observed features into  cells of equal volume so that we can use histogram to estimate mutual information  defined by  p(x,y)  I(X;Y)- Ep(x,y) log2 p(x)p(y)'   In(1 p2  If X and Y are jointly Gaussian, then I(X;Y) = -3 - ) where p is the  correlation coefficient between X and Y. However, for speech data the feature vari-  ables are generally non-Gaussian and target variables are categorical type variables.  Correlations involving a categorical variable are meaningless.  The MI can also be written as  z(x; v) = n(x) + n(v) - n(x, v)  = n(v)- n(VlX) = n(x)- n(XlV) (1)  where H(YIX) is a conditional entropy defined by  H(YIX) = - p(x) p(yl x) log2(ylx ).  x y  The two equations in (1) mean that the MI is the uncertainty reduction about Y  give X or the uncertainty reduction about X give Y.  Based on the histogram, H(X) is estimated by  f ( E ni ni  X) = - -- 1og --  i  where ni is the number of data points in the i-th cell and n is the data size. And  I(X; Y) is estimated by  i(x; Y) = (x) +/(Y)- (x, ).  Miller(1954)[4] has shown that //(X) is an underestimate of H(X) and/(X; Y) is  an overestimate of I(X; Y). The biases are  r-1 1  E[(X)]- n(X) = j( + 0(7)  (2)  E[i(X;Y)]- I(X;Y) = (r- 1)(c- 1) O(nfi__/) (3)  2 ln(2)n +  where r and c are the number of cells for X and Y respectively.  Interestingly, the first order terms in (2) and (3) do not depend on the probability  distribution. After using these formulas to correct the'estimates, the new estimates  have the same variances as the old estimates but with reduced biases. However,  these formulas break down when r and n are of the same order. Extending Miller's  approach, we find a high order correction for the bias. Let {pi} be the probability  distribution of X, then  r- 1 1 (S({pi })- 3r + 2)  E[//(X)]- H(X) = 21n(2)n + 61n(2)n 2  i (S({pi}) - 1) + O(4) (4)  4n a  where  806 H. H. Yang and H. Hermansky  The last two terms in the bias (4) depend on the unknown probabilities (pi}. In  practice they are approximated by the relative frequency estimates.  Similarly, we can find the bias formulas of the high order terms O() and O(  for the MI estimate.  When X is evenly distributed, pi = l/r, so S({pi}) = r 2 and  E[/(X)]- H(X) =  r-1 1  2 ln(2)n + 6 ln(2)n 2 (r - 3r + 2) - --  i 1  4n a (r 2 - 1) + 0(-).  Theoretically S({pi}) has no upper bound when one of the probabilities is close to  zero. However, in practice it is hard to collect a sample to estimate a very small  probability. For this reason, we assume that pi is either zero or greater than  where e > 0 is a small constant does not depend on n or r. Under this assumption  1 [/.2/  S({pi}) _< r2/e and the amplitude of the last term in (4)is less than 4x / - 1).  4 MI in Speech for Phonetic Classification  The three hour telephone speech in the OGI database gives us a sample size greater  than i million, n = 1050000. To estimate the mutual information between three  features and a target variable, we need to estimate the entropy H(X1, X2, X3, Y).  Take B = 20 as the number of bins for each feature variable and C = 19 is the  number of phoneme categories. Then the total number of cells is r - B 3  C. After  a constant adjustment, assuming e - 1, the bias is  1  O() = 6 ln(2)n 2 (r2 - 3r + 2) = 0.005(bits).  It is shown in Fig. l(a) that X(f4,t) and X(f,t) are most relevant features for  phonetic classification. From Fig. l(b), at 5 Bark the MI spread around the current  frame is 200 ms.  Given one feature X1, the information gain due to the second feature is the difference  - r(xx; lXx)  where I(X2; Y]X) is called the information gain of X2 given X. It is a conditional  mutual information defined by  z(x;YIX) - p(xl) yp(x2, ylx)log2  Xl 2,Y  p(x,ylx)  p(x2lxx)p(ylxx)  It is shown in rig. l(c)-(d) that given X(f,t) across different bands the maxi-  mum information gain is achieved by X(f9,t), and within 5 Bark band the max-  imum information gain is achieved by X(fs,t - 5). The mutual informations  I(X(f4,t),X(fk,t + d);Y) for k = 1,..., 15, k  4, and d- +i,-..,+10, the infor-  mation gain from the second feature in the vicinity of the first one, are shown in  Fig. 2. The asymmetric distribution of the MI around the neighborhood (rs, d = 0)  indicates that the phonetic information is spread asymmetrically through time but  localized in about 200 ms around the current frame.  Based on our data set, we have//(Y) - 3.96 (bits). The JMI for three frequency  features and three temporal features are shown in Fig. l(e)-(f). Based on these  estimates, the three frequency features give 28% reduction in uncertainty about Y  while the three temporal features give 19% reduction.  Search for Information Bearing Components in Speech 807  0.55  0.5  0.45  0.4  0.3  0.25  0.2  0.15  0  0.9  0.85  0.8  0.75  0.7  0.65  0.8  0.55  0.5  0  3 6 9 12  Bark  3 8 9 12 15  Bark  (c)  0.5  0.4  .. 0.3  0.2  0  ( 200  --400 --200  time shift in ms  0.9  0.85  0.8  0.75  0 7  0.65  0.6  0.55  0.5  --400  1.1  0.8  1.1  . 0.9  0.8  0.7  0.7  (b)  400  --200 0 200 400  time shift in ms  (d)     ' 0.8  '  0'60 3 8 9 I 2 I 5 --400 --200 0 200 400  Bark time shift in ms  (e) (f)  Figure 1: (a) MIs of individual features in different bands. (b) MIs of individual  feature at 5 Bark with different 10ms-frame shifts. (c) JMIs of two features: at  5 Bark and in other bands. (d) JMIs of two features: current frame and shifted  frames, both at 5 Bark. (e) JMIs of three features: at 5 Bark, 9 Bark and in other  bands. The dashed line is the JMI level achieved by the two features X(fs,t) and  X(f9,t). (f) Jmls of three features: current frame, 5th frame before current frame,  and other shifted frames, all at 5 Bark. The dashed line is the JMI level achieved  by X(fs,t) and X(fs,t-5).  The size of our data set is n = 1050000. Therefore, we can reliably estimate the joint  808 H. H. Yang and H. Hermansky  MI between three features and the phoneme label. However, to estimate the JMI  for more than 3 features we have the problem of curse of dimensionality since for k  features, r = B k  C is exponential increasing. For example, when k = 4, B = 20,  and C = 19, the second order bias is O(1/n 2) = 2.02 (bits) which is too high to be  ignored. To extend our approach beyond the current three-feature level, we need  either to enlarge our data set or to find an alternative to the histogram based MI  estimation.  0.95.. -'  0.9.  0.85.  0.8.  0.6.  0.55.  0.5.  15  . - '' ' - 50  5 ' ' ',.. 0  Ba 0 -1 O0  frame shift in ms  Figure 2: The 3-D plot of joint mutual information around X(f4, t). An asymmetric  distribution is apparent especially around 4 Bark and 5 Bark.  1.3  0.9  3  Bark  (a)  1.5  1.4 --  -- -- I 5--th band_  0.6   -' %o -,oo o too  (b)  200  Figure 3: (a) The MI between one frequency feature and the file label. (b) The JMI  between two features and the file identity labels.  5 MI in Speech for Speaker/Channel Recognition  The linguistic variability expressed by phoneme labels is not the only variability  present in speech. We use the mutual information to evaluate relevance to other  Search for Information Bearing Components in Speech 809  sources of variabilities such as speaker/channel variability. Taking the file label as  a target variable, we estimated the mutual information for one and two features.  It is shown in Fig. 3(a) that the most relevant features are in the very low frequency  channels, which in our case of telephone speech carry only very little speech infor-  mation. Fig. 3(b) shows that the second most relevant feature for speaker/channel  recognition is at least 150 ms apart from the first most relevant feature. These re-  sults suggest that the information about the speaker and the communication channel  is not localized in time. These results are complementary to the results for phonetic  classification shown in rig. l(a) and (d).  6 CONCLUSIONS  Our results have shown that the information theoretic analysis of labeled speech  data is feasible and useful for obtaining reusable knowledge about speech/channel  variabilities. The joint mutual information of two features for phonetic classification  is asymmetric around the current frame. We also estimated the joint mutual infor-  mation between the phoneme labels and three feature variables. The uncertainty  about the phonetic classification is reduced by adding more features. The maximum  uncertainty reductions due to three frequency features and three temporal features  are 28% and 19% respectively.  The mutual informations of one and two features for speaker/channel recognition  are estimated. The results show that the most relevant features are in the very low  frequency bands. At i Bark and 5 Bark, the second most relevant temporal feature  for speaker/channel recognition is at least 150 ms apart from the first most relevant  feature. These results suggest that the information about the speaker and the  communication channel is not localized in time. These results are complementary  to the results for phonetic classification for which the mutual information is generally  localized with some time spread.  References  [1] J. A. Bilmes. Maximum mutual information based reduction strategies for cross-  correlation based joint distribution modeling. In ICASSP98 , pages 469-472,  April 1998.  [2] R. Cole, M. Fanty, M. Noel, and T. Lander. Telephone speech corpus develop-  ment at CSLU. In ICSLP, pages 1815-1818, Yokohama, Sept. 1994.  [3] H. Hermansky. Perceptual linear predictive (PLP) analysis of speech. J. Acoust.  Soc. Am., 87(4):1738-1752, April 1990.  [4] G. A. Miller. Note on the bias of information estimates. In H. Quastler, editor,  Information Theory and Psychology , pages 95-100. The Free Press, Illinois,  1954.  [5] Andrew Morris, Jean-Luc Schwartz, and Pierre Escudier. An information the-  oretical investigation into the distribution of phonetic information across the  auditory spectogram. Computer Speech  Language, 7(2):121-136, April 1993.  [6] H. H. Yang, S. Van Vuuren, , S. Sharma, and H. Hermansky. Relevancy of time-  frequency features for phonetic classification and speaker-channel recognition.  Accepted by Speech Communication, 1999.  [7] H. H. Yang, S. Van Vuuren, and H. Hermansky. Relevancy of time-frequency fea-  tures for phonetic classification measured by mutual information. In ICASSP99,  pages I:225-228, Phoenix, March 1999.  PART VII  VISUAL PROCESSING  
Information Factorization in  Connectionist Models of Perception  Javier It. Movellan  Department of Cognitive Science  Institute for Neural Computation  University of California San Diego  James L. McClelland  Center for the Neural Bases of Cognition  Department of Psychology  Carnegie Mellon University  Abstract  We examine a psychophysical law that describes the influence of  stimulus and context on perception. According to this law choice  probability ratios factorize into components independently con-  trolled by stimulus and context. It has been argued that this pat-  tern of results is incompatible with feedback models of perception.  In this paper we examine this claim using neural network models  defined via stochastic differential equations. We show that the law  is related to a condition named channel separability and has little  to do with the existence of feedback connections. In essence, chan-  nels are separable if they converge into the response units without  direct lateral connections to other channels and if their sensors are  not directly contaminated by external inputs to the other chan-  nels. Implications of the analysis for cognitive and computational  neurosicence are discussed.  1 Introduction  We examine a psychophysical law, named the Morton-Massaro law, and its implica-  tions to connectionist models of perception and neural information processing. For  an example of the type of experiments covered by the Morton-Massaro law consider  an experiment by Massaro and Cohen (1983) in which subjects had to identify syn-  thetic consonant sounds presented in the context of other phonemes. There were  two response alternatives, seven stimulus conditions, and four context conditions.  The response alternatives were/1/and/r/, the stimuli were synthetic sounds gen-  erated by varying the onset frequency of the third formant, followed by the vowel  /i/. Each of the 7 stimuli was placed after each of four different context consonants,  /v/, /s/, /p/, and/t/. Morton (1969) and Massaro independently showed that in a  remarkable range of experiments of this type, the influence of stimulus and context  on response probabilities can be accounted for with a factorized version of Luce's  strength model (Luce, 1959)  !s(i, k) !c(j, k)  P(R = k lS = i,C = j) = Evs(i, 1)vc(j, 1)' for (i,j,k) e $ x  x 7. (1)  Here $, C and R are random variables representing the stimulus, context and the  subject's response, $,  and 7 are the set of stimulus, context and response al-  46 J.. R. Movellan and J.. L. McClelland  ternatives, $(i, k) > 0 represents the support of stimulus i for response k, and  r/c(j, k) > 0 the support of context j for response k. Assuming no strength param-  eter is exactly zero, (1) is equivalent to  P(R=kIS=i,C=j)  P(R-11$ -i,C -j)  = \ n(i, t) ] j  , for all (i, j, k)  $ x  x 7.  (2)  This says that response probability ratios factorize into two components, one which  is affected by the stimulus but unaffected by the context and one affected by the  context but unaffected by the stimulus.  2 Diffusion Models of Perception  Massaro (1989) conjectured that the Morton-Massaro law may be incompatible  with feedback models of perception. This conjecture was based on the idea that in  networks with feedback connections the stimulus can have an effect on the context  units and the context can have an effect on the stimulus units making it impossible  to factorize the influence of information sources. In this paper we analyze such  a conjecture and show that, surprisin. gly, the Morton-Massaro law has little to do  with the existence of feedback and lateral connections. We ground our analysis  on continuous stochastic versions of recurrent neural networks . We call these  models diffusion (neural) networks for they are stochastic diffusion processes defined  by adding Brownian motion to the standard recurrent neural network dynamics.  Diffusion networks are defined by the following stochastic differential equation  dYi(t) - !i(Y(t),X) dt + rr dBi(t) for i  {1,..-,n}, (3)  where Y/(t) is a random variable representing the internal potential at time t of the  i th unit, Y(t) = (Y (t),-.. , Yn(t))', X represents the external input, which consists  of stimulus and context, and Hi is Brownian motion, which acts as a stochastic  driving term. The constant rr > 0, known as the dispersion, controls the amount  of noise injected onto each unit. The function/i, known as the dr/f, determines  the average instantaneous change of activation and is borrowed from the standard  recurrent neural network literature: this change is modulated by a matrix w of  connections between units, and a matrix v that controls the influence of the external  inputs onto each unit.  1  ti(ri(t),X) -- i(ri(t) ) (i(t) - ri(t)), for all/ {1,.--,n}, (4)  where 1/;i is a positive function, named the capacitance, controlling the speed of  processing and  ?i(t)=Ewi,jZj(t)+Evi,kXk, for all i e {1,-.- ,n}, (5)  Zj(t) = 9i(Yj(t)) = 9(ci Yj(t)) = 1/(1 + e -a' (t)). (6)  Here wi,j, an element of the connection matrix w, is the weight from unit j to unit i,  vi,& is an element of the matrix v, 9 is the logistic activation function and the ci > 0  terms are gain parameters, that control the sharpness of the activation functions.  For large values of ci the activation function of unit i converges to a step function.  The variable Zj (t) represents a short-time mean firing rate (the activation) of unit  1For an analysis grounded on discrete time networks with binary states see McClelland  Information Factorization 47  j scaled in the (0, 1) range. Intuition for equation (4) can be achieved by thinking  of it as a the limit of a discrete time difference equation, in such case  Y(t + At) = Y/(t) + !i(Yi(t),X)At + av/Ni(t),  (7)  where the Ni(t) are independent standard Gaussian random variables. For a fixed  state at time t there are two forces controlling the change in activation: the drift,  which is deterministic, and the dispersion which is stochastic. This results in a  distribution of states at time t + At. As At goes to zero, the solution to the  difference equation (7) converges to the diffusion process defined in (4). In this  paper we focus on the behavior of diffusion networks at stochastic equilibrium, i.e.,  we assume the network is given enough time to approximate stochastic equilibrium  before its response is sampled.  3 Channel Separability  In this section we show that the Morton-Massaro is related to an architectural con-  straint named channel separability, which has nothing to do with the existence of  feedback connections. In order to define channel separability it is useful to char-  acterize the function of different units using the following categories: 1) Response  specification units: A unit is a response specification unit, if, when the state of all  the other units in the network is fixed, changing the state of this unit affects the  probability distribution of overt responses. 2) Stimulus units: A unit belongs to  the stimulus channel if: a) it is not a response unit, and b) when the state of the  response units is fixed, the probability distribution of the activations of this unit is  affected by the stimulus. 3) Context units: A unit belongs to the context channel if:  a) it is not a response unit, and b) when the states of the response units are fixed,  the probability distribution of the activations of this unit can be affected by the  context. Given the above definitions, we say that a network has separable stimulus  and context channels if the stimulus and context units are disjoint: no unit simul-  taneously belongs to the stimulus and context channels. In essence, channels are  structurally separable if they converge into the response units without direct lateral  connections to other channels and if their sensors are not directly contaminated by  external inputs to the other channels (see Figure 1).  In the rest of the paper we show that if a diffusion network is structurally separable  the Morton-Massaro law can be approximated with arbitrary precision regardless of  the existence of feedback connections. For simplicity we examine the case in which  the weight matrix is symmetric. In such case, each state has an associated goodness  function that greatly simplifies the analysis. In a later section we discuss how the  results generalize to the non-symmetric case.  Let y  11 ' represent the internal potential of a diffusion network. Let zi = 99(otiYi)  for i = 1,..-,n represent the firing rates corresponding to y. Let z s, z c and  z r represent the components of z for the units in the stimulus channel, context  channel and response specification module. Let x be a vector representing an input  and let x s, x c be the components of x for the external stimulus and context. Let  ot = (Oil,''' ,Otn) be a fixed gain vector and Za(t) a random vector representing  the firing rates at time t of a network with gain vector c. Let Z a = limt-oo Z a (t),  represent the firing rates at stochastic equilibrium. In Movellan (1998) it is shown  that if the weights are symmetric i.e., w = w' and 1/ni(x) = d99i(x)/dx then the  equilibrium probability density of Z a is as follows  s z r 1 exp((2/cr 2) Ga(zS,zrlxs xc)) (S)  PzIx(z ,z c, I x*'xC) = K(x,,xc) ' '  48 J. R. Movellan and J. L. McClelland  Rqonse  Spicatlon  Units  Stimulus Context  !iays Rdays  Stimulus Context  Smsors Sensors  $timuim /  Input  Context  Figure 1: A network with separable context and stimulus processing channels. The  stimulus sensor and stimulus relay units make up the stimulus channel units, and  the context sensor and context channel units make up the context channel units.  Note that any of the modules can be empty except the response module.  where  Ka(xs,xc) = f exp((2/a 2) Ga(z I xs,xc)) dz, (9)  n  o( Ix) = H(z Ix) -  S, (z), (10)  i=1  H(z Ix) = z' w z/2 + z' v x, (11)  Sc,,(zi)= ai(log(zi)+log(1 zi))+l(  - zlog(z) + (1 - zi)log(1 - z) . (12)  Without loss of generality hereafter we set a 2 = 2. When there are no direct con-  nections between the stimulus and context units there are no terms in the goodness  function in which x  or z  occur jointly with x c or z c. Because of this, the goodness  can be separated into three additive terms, that depend on x , x c and a third term  which depends on the response units:  Z r  o,(z',z c, I x',x ) = a(z',zr l x') + a2(zr, zC l x) + a,(z) ,  where  8 f' I 8  a(, I x ) = (s)%,,/2 + (z)',r + (s)%,,x  + () w,x -  s(4),  i  (14)  c c I  c(z, I x ') = (C)'w,c/2 + () w,z + ()'v,x c + (r)'w,x  -  s),  i  (15)  C,(z") = (z[')'w,.,,.z"/2- E S(z[') . (16)  i  Inf ormat'on Facto rization 49  where ws,r is a submatrix of w connecting the stimulus and response units. Similar  notation is used for the other submatrices of w and v. It follows that we can write  the ratio of the joint probability density' of two states z and 2 as follows:  Pz"lx(zS'zC'zr l xS'xC) = exp(G2(zS'z* l x*) + G2(zC'z* l c) + G2(z*)) (17)  pzlx(2,2c, 2 l x,x*) exp(G(:,2r l x ) + G(2,2 l x ) + G(2)) '  whh factorizes  desired. To get probability densities for the response units, we  integrate over the states of l the other units  Pz:Ix( zr I xS, xc) = PzIx(Z*, zc, I x c) dz s dz c (18)  d after rerging terms  i +  pz:lx(z l x*,x ) = K(x,,x )  (1)  which also factorizes. All is left is mapping continuous states of the response units  to discrete externM responses. To do so we ptition the space of the response  specification units into screte regions. The probability of a response becomes the  integral of the probability density over the region corresponding to that response.  The problem is that the integral of probability densities does not necessarily fac-  torize even though the densities factorize at every point.  Fortunately there are two importt ces for which the law holds, at let  a  good approximation. The first ce is when the response reons e small and thus  we c approximate the inteM over that re,on by the density at a point times the  volume of the re,on. In such a ce the ratio of the integrMs can be appromated  by the ratio of the probability densities of those individuM states. The second ce  applies to models, like McClelld and Rumelhm's (1981) interactive activation  model, in which each response is sociated with a distinct response unit. These  models typicMly have negative connections mongst the response units so that at  equilibrium one unit tends to be active while the others e inactive. In such a  ce a common response policy picks the response corresponding to the tive unit.  We now show that such a policy can approximate the Morton-Mso law to an  bitrary level of precision  the gain pmeter of the response units is incre,ed.  Let z represent the joint state of a network d let the first r components of z  be the states of the response specification units. Let z (1) = (1, 0, 0,..., 0) , z (2) =  (0, 1,0,... ,0)  be two r-dimensionM vectors representing states of the response  specification units. For i  {1, 2} and A  (0, 1) let  ,) = (1 - ,('))5 + ('))(1 - 5), (20)  2) = { e :  e ((1 - 5),J'),5 + (1 - 5),J')), for  = 1,..-,}. (21)  The sets R? and R?  external responses. We  these two responses as  corners of [0, 1] .  lim P(Z  R? I X = x)  o P(Z  R(2) I x = )  lim A'Pzlx(z() l x)  ,x-o ZXpz:lx(z(2) lx)  are regions of the [0, 1] r space mapping into two distinct  now investigate the convergence of the probability ratio of  we let A -+ 0, i.e., as the response regions collapse into  = lim fR? pz:lx(ulx)du _  /-o fR(2) pzxlx(u l x)du -  = lim f f er;((2)''zl)dzS dzc  -o f f er((2),o,l)dzS dz c'  (22)  (23)  50 J. R. Movellan and J. L. McClelland  Table 1: Predictions by the Morton-Massaro law (left side) versus diffusion network  (square brackets) for subject 7 of Massaro and Cohen (1983) Experiment 2. Each  prediction of the diffusion network is based on 100 random samples.  Context  Stimulus V S P T  0 0.0017 [0.01] 0.0000 [0.00] 0.0152 [0.03] 0.9000 [0.91]  i 0.0126 [0.00] 0.0000 [0.00] 0.1008 [0.10] 0.9849 [0.97]  2 0.1105 [0.19] 0.0008 [0.00] 0.5208 [0.45] 0.9984 [1.00]  3 0.5463 [0.54] 0.0079 [0.00] 0.9133 [0.91 0.9998 '1.00]  4 0.9827 [1.00] 0.2756 [0.30] 0.9980 [1.00 0.9999 1.00]  5 0.9999 [1.00] 0.9924 [0.99] 0.9999 [1.00] 1.0000 [1.00]  6 0.9999 [1.00] 0.9924 [1.00] 0.9999 [1.00] 1.0000 [1.00]  Now note that  =  i=1 i j  (24)  and since 2=1Soi (Z(Aii) ---- 2ir=1Soi (Z(ff!i), it follows that  lim P(Z  R() ] X = x)  f f eH(Z),z,z c I x)-E, $,(zi)-Ej $j( j)dz s dz c  f f eH(Z(2),z,zc I x)-E, S,(zl)-Ei Ss( j)dz s dz c  (25)  It is easy to show that this ratio factorizes. Moreover, for all A > 0 if we let  o/1 = ''' = 0/r -- 0/, where 0/> 0 then  lim P(Za r  [A, 1- A] r) =O,  (26)  since as the gain of the response units increases Sot decreases very fast at the corners  of (0, 1) r. Thus as 0/- cx> the random variable Za r converges in distribution to a  discrete random variable with mass at the corner of the [0, 1] r hypercube and with  factorized probability ratios as expressed on (25). Since the indexing of the response  units is arbitrary the argument applies to all the responses.  4 Discussion  Our analysis establishes that in diffusion networks the Morton-Massaro law is not  incompatible with the presence of feedback and lateral connections. Surprisingly,  even though in diffusion networks with feedback connections stimulus and context  units are interdependent, it is still possible to factorize the effect of stimulus and  context on response probabilities.  The analysis shows that the Morton-Massaro can be arbitrarily approximated as  the sharpness of the response units is increased. In practice we have found very  good approximations with relatively small values of the sharpness parameter (see  Table i for an example). The analysis assumed that the weights were symmetric.  Mathematical analysis of the general case with non-symmetric weights is difficult.  Information Factorization 51  However useful approximations exist (Movellan & McClelland, 1995) showing that  if the noise parameter a is relatively small or if the activation function T is approx-  imately linear, symmetric weights are not needed to exhibit the Morton-Massaro  law.  The analysis presented here has potential applications to investigate models of per-  ception and the functional architecture of the brain. For example the interactive  activation model of word perception has a separable architecture and thus, diffusion  versions of it adhere to the Morton Massaro law. The analysis also points to po-  tential applications in computational neuroscience. It would be of interest to study  whether the Morton-Massaro holds at the level of neural responses. For example,  we may excite a neuron with two different sources of information and observe its  short term average response to combination of stimuli. If the observed distribution  of responses exhibits the Morton-Massaro law, this would be consistent with the  existence of separable channels converging into that neuron. Otherwise, it would  indicate that the channels from the two input areas to the response may not be  structurally separable.  References  Luce, R. D. (1959). Individual choice behavior. New York: Wiley.  Massaro, D. W. (1989). Testing between the TRACE Model and the fuzzy logical  model of speech perception. Cognitive Psychology, 21, 398-421.  Massaro, D. W. (1998). Perceiving Talking Faces. Cambridge, Massachusetts: MIT  Press.  Massaro, D. W. & Cohen, M. M. (1983a). Phonological constraints in speech per-  ception. Perception and Psychophysics, 36, 338-348.  McClelland, J. L. (1991). Stochastic interactive activation and the effect of context  on perception. Cognitive Psychology, 23, 1-44.  Morton, J. (1969). The interaction of information in word recognition. Psychological  Review, 76, 165-178.  Movellan, J. R. (1998). A Learning Theorem for Networks at Detailed Stochastic  Equilibrium. Neural Computation, i0(5), 1157-1178.  Movellan, J. R. & McClelland, J. L. (1995). Stochastic interactive processing, chan-  nel separability and optimal perceptual inference: an examination of Mor-  ton's law. Technical Report PDP. CNS.95.4, Available at http://cnbc.cmu.edu,  Carnegie Mellon University.  
Better Generative Models for Sequential  Data Problems: Bidirectional Recurrent  Mixture Density Networks  Mike Schuster  ATR Interpreting Telecommunications Research Laboratories  2-2 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-02, JAPAN  gustl@itl. atr. co.jp  Abstract  This paper describes bidirectional recurrent mixture density net-  works, which can model multi-modal distributions of the type  P(xtly T) and P(xtlxl,x2,...,xt_,y T) without any explicit as-  sumptions about the use of context. These expressions occur fre-  quently in pattern recognition problems with sequential data, for  example in speech recognition. Experiments show that the pro-  posed generative models give a higher likelihood on test data com-  pared to a traditional modeling approach, indicating that they can  summarize the statistical properties of the data better.  I Introduction  Many problems of engineering interest can be formulated as sequential data prob-  lems in an abstract sense as supervised learning from sequential data, where an  input vector (dimensionality D) sequence X = x T = {Xl,X2,...,XT_i,XT} liv-  ing in space A' has to be mapped to an output vector (dimensionality K) target  sequence T = t T = {tl,t2,...,tT-,tT} in space  y, that often embodies cor-  relations between neighboring vectors xt,xt+l and tt,tt+l. In general there are  a number of training data sequence pairs (input and target), which are used to  estimate the parameters of a given model structure, whose performance can then  be evaluated on another set of test data pairs. For many applications the problem  becomes to predict the best sequence Y given an arbitrary input sequence X, with  'best' meaning the sequence that minimizes an error using a suitable metric that is  yet to be defined. Making use of the theory of pattern recognition [2] this problem  is often simplified by treating any sequence as one pattern. This makes it possi-  ble to express the objective of sequence prediction with the well known expression  Y = arg maxy P(YIX), with X being the input sequence, Y being any valid out-  put sequence and Y being the predicted sequence with the highest probability 2  I  a sample sequence of the training target data is denoted as T, while an output sequence  in general is denoted as Y, both live in the output space y  2to simplify notation, random variables and their values, are not denoted as different  symbols. This means, P(x)- P(X- x).  590 M. Schuster  among all possible sequences.  Training of a sequence prediction system corresponds to estimating the distribution  3 p(yix) from a number of samples which includes (a) defining an appropriate  model representing this distribution and (b) estimating its pa.rameters such that  P(Y]X) for the training data is maximized. In practice the model consists of  several modules with each of them being responsible for a different part of P(YIX).  Testing (usage) of the trained system or recognition for a given input sequence  X corresponds principally to the evaluation of P(Y[X) for all possible output se-  quences to find the best one Y*. This procedure is called the search and its efficient  implementation is important for many applications.  In order to build a model to predict sequences it is necessary to decompose the  sequences such that modules responsible for smaller parts can be build. An often  used approach is the decomposition into a generative and prior model part, using  P(BIA) = P(AIB)P(B)/P(A ) and P(A,B)= P(A)P(BIA), as:  Y* = arg maxP(Y[X)= arg maxP(XIY)P(Y)  y y  -- argnax [HP(xlx'x""'xt-'Yr)]  t=l t=l  generative part prior part  For many applications (1) is approximated by simpler expressions, for example as  a first order Markov Model  T T  Y*  argnax [H P(xtlYt)] [H P(YtlYt-1)]  t--1 t=l  making some simplifying approximations. These are for this example:  (2)   Every output Yt depends only on the previous output yt_ and not on all  previous outputs:  P(YtlYl,Y,..-,Yt-1)  P(YtlYt-1) (3)   The inputs are assumed to be statistically independent in time:  P(xtlx,x,...,xt-,y T) :: P(xtly) (4)   The likelihood of an input vector xt given the complete output sequence y  is assumed to depend only on the output found at t and not on any other  ones:  P(xt[y) :: P(xt[yt) (5)  Assuming that the output sequences are categorical sequences (consisting of sym-  bols), approximation (2) and derived expressions are the basis for many applications.  For example, using Gaussian mixture distributions to model P(xt[yt) -- P(x) V K  occuring symbols, approach (2) is used in a more sophisticated form in most state-  of-the-art speech recognition systems.  Focus of this paper is to present some models for the generative part of (1) which  need less assumptions. Ideally this means to be able to model directly expressions  of the form P(xtlx , x,..., xt_, yT), the possibly (multi-modal) distribution of a  vector conditioned on previous x vectors xt, xt_ ,..., Xl and a complete sequence  yT, as shown in the next section.  athere is no distinction made between probability mass and density, usually denoted  as P and p, respectively. If the quantity to model is categorical, a probability mass is  assumed, if it is continuous, a probability density is assumed.  Bidireca'onal Recurrent Mixture Density Networks 591  2 Mixture density recurrent neural networks  Assume we want to model a continuous vector sequence, conditioned on a sequence  of categorical variables as shown in Figure 1. One approach is to assume that  the vector sequence can be modeled by a uni-modal Gaussian distribution with  a constant variance, making it a uni-modal regression problem. There are many  practical examples where this assumption doesn't hold, requiring a more complex  output distribution to model multi-modal data. One example is the attempt to  model the sounds of phoneroes based on data from multiple speakers. A certain  phoneme will sound completely different depending on its phonetic environment or  on the speaker, and using a single Gaussian with a constant variance would lead to  a crude averaging of all examples.  The traditional approach is to build generative models for each symbol separately, as  suggested by (2). If conventional Gaussian mixtures are used to model the observed  input vectors, then the parameters of the distribution (means, covariances, mixture  weights) in general do not change with the temporal position of the vector to model  within a given state segment of that symbol. This can be a bad representation  for the data in some areas (shown are here the means of a very bi-modal looking  distribution), as indicated by the two shown variances for the state 'E'. When used  to model speech, a procedure often used to cope with this problem is to increase  the number of symbols by grouping often appearing symbol sub-strings into a new  symbol and by subdividing each original symbol into a number of states.  KKKEEEEEEEEEEIIIIIIIIIIIKKKOOOOOOOOO O  KKKEEEEEEEEEEIIIIIIIIIIIKKKOOOOOOOOOO  Figure 1: Conventional Gaussian mixtures (left) and mixture density BRNNs (right)  for multi-modal regression  Another alternative is explored here, where all parameters of a Gaussian mixture dis-  tribution modeling the continuous targets are predicted by one bidirectional recur-  rent neural network, extended to model mixture densities conditioned on a complete  vector sequence, as shown on the right side of Figure 1. Another extension (sec-  tion 2.1) to the architecture allows the estimation of time varying mixture densities  conditioned on a hypothesized output sequence and a continuous vector sequence  to model exactly the generative term in (1) without any explicit approximations  about the use of context.  Basics of non-recurrent mixture density networks (MLP type) can be found in [1112 ].  The extension from uni-modal to multi-modal regression is somewhat involved but  straightforward for the two interesting cases of having a radial covariance matrix or a  diagonal covariance matrix per mixture component. They are trained with gradient-  descent procedures as regular uni-modal regression NNs. Suitable equations to  calculate the error that is back-propagated can be found in [6] for the two cases  mentioned, a derivation for the simple case in [1][2].  Conventional recurrent neural networks (RNNs) can model expressions of the form  P(xtlyl, Y2,... ,Yt), the distribution of a vector given an input vector plus its past  input vectors. Bidirectional recurrent neural networks (BRNNs) [5][6] are a simple  592 M. Schuster  extension of conventional RNNs. The extension allows one to model expressions of  the form P(xtly) , the distribution of a vector given an input vector plus its past  and following input vectors.  2.1 Mixture density extension for BRNNs  liere two types of extensions of BRNNs to mixture density networks are considered:  I) An extension to model expressions of the type P(xtly), a multi-modal  distribution of a continuous vector conditioned on a vector sequence yT,  here labeled as mixture density BRNN of Type I.  II) An extension to model expressions of the type P(xtlx,x2,... ,xt_,y),  a probability distribution of a continuous vector conditioned on a vector  sequence y and on its previous context in time Xl,Xe,...,xt-. This  architecture is labeled as mixture density BRNN of Type II.  The first extension of conventional uni-modal regression BRNNs to mixture density  networks is not particularly difficult compared to the non-recurrent implementation,  because the changes to model multi-modal distributions are completely independent  of the structural changes that have to be made to form a BRNN.  The second extension involves a structural change to the basic BRNN structure  to incorporate the x,x2,... ,x-I as additional inputs, as shown in Figure 2. For  any t the neighboring xt-, xt-2,... are incorporated by adding an additional set  of weights to feed the hidden forward states with the extended inputs (the tar-  gets for the outputs) from the time step before. This includes xt_ directly and  xt-2, xt-a,... x indirectly through the hidden forward neurons. This architecture  allows one to estimate the generative term in (1) without making the explicit as-  sumptions (4) and (5), since all the information xt is conditioned on, is theoretically  available.  FORWARD  STATES  BACKWARD  STATES  t-I t t+l  Figure 2: BRNN mixture density extension (Type II) (inputs: striped, outputs:  black, hidden neurons: grey, additional inputs: dark grey). Note that without the  backward states and the additional inputs this structure is a conventional RNN,  unfolded in time.  Different from non-recurrent mixture density networks, the extended BRNNs can  predict the parameters of a Gaussian mixture distribution conditioned on a vector  sequence rather than a single vector, that is, at each (time) position t one parameter  set (means, variances (actually standard variations), mixture weights) conditioned  on yT for the BRNN of type I and on x, x2,..., xt_ 1, yT for the BRNN of type II.  Bidirectional Recurrent Mixture Density Networks 593  3 Experiments and Results  The goal of the experiments is to show that the proposed models are more suit-  able to model speech data than traditional approaches, because they rely on fewer  assumptions. The speech data used here has observation vector sequences repre-  senting the original waveform in a compressed form, where each vector is mapped to  exactly one out of K phonemes. Here three approaches are compared, which allow  the estimation of the likelihood P(XlY ) with various degrees of approximations:  Conventional Gaussian mixture model, P(XIY )  1-ltT=l P(xtlyt):  According to (2) the likelihood of a phoneme class vector is approximated by a  conventional Gaussian mixture distribution, that is, a separate mixture model is  built to estimate P(xly ) - P(x) for each of the possible K categorical states in  y. In this case the two assumptions (4) and (5) are necessary. For the variance  a radial covariance matrix (diagonal single variance for all vector components) is  chosen to match it to the conditions for the BRNN cases below. The number of  parameters for the complete model is KM(D + 2) for M > 1. Several models of  different complexity were trained (Table 1).  Mixture density BRNN I, P(X[Y)  HT__lP(Xt[y): One mixture density  BRNN of type I, with the same number of mixture components and a radial co-  variance matrix for its output distribution as in the approach above, is trained  by presenting complete sample sequences to it. Note that for type I all possible  context-dependencies (assumption (5)) are automatically taken care of, because the  probability is conditioned on complete sequences y. The sequence y contains for  any t not only the information about neighboring phonemes, but also the position of  a frame within a phoneme. In conventional systems this can only be modeled crudely  by introducing a certain number of states per phoneme. The number of outputs  for the network depends on the number of mixture components and is M(D + 2).  The total number of parameters can be adjusted by changing the number of hidden  forward and backward state neurons, and was set here to 64 each.  Mixture density BRNN II, P(XIY ) = H= P(x]x,x,... ,x_,y):  One mixture density BRNN of type II, again with the same number of mixture  components and a radial covariance matrix, is trained under the same conditions as  above. Note that in this case both assumptions (4) and (5) are taken care of, be-  cause exactly expressions of the required form can be modeled by a mixture density  BRNN of type II.  3.1 Experiments  The recommended training and test data of the TIMIT speech database [3] was  used for the experiments. The TIMIT database comes with hand-aligned phonetic  transcriptions for all utterances, which were transformed to sequences of categorical  class numbers (training: 702438, test = 256617 vec.). The number of possible  categorical classes is the number of phonemes, K = 61. The categorical data  (input data for the BRNNs) is represented as K-dimensional vectors with the kth  component being one and all others zero. The feature extraction for the waveforms,  which resulted in the vector sequences xl T to model, was done as in most speech  recognition systems [7]. The variances were normalized with respect to all training  data, such that a radial variance for each mixture component in the model is a  reasonable choice.  594 M. Schuster  All three model types were trained with M = 1, 2, 3, 4, the conventional Gaussian  mixture model also with M = 8, 16 mixture components. The number of resulting  parameters, used as a rough complexity measure for the models, is shown in Table 1.  The states of the triphone models were not clustered.  Table 1: Number of parameters for different types of models  mixture mono61 mono61 tri571 BRNN I BRNN II  components 1-state 3-state 3-state  1 1952 5856 54816 20256 22176  2 3904 11712 109632 24384 26304  3 5856 17568 164448 28512 30432  4 7808 23424 219264 32640 34560  8 15616 46848 438528 - -  16 31232 93696 877056 - -  Training for the conventional approach using M mixtures of Gaussians was done  using the EM algorithm. For some classes with only a few samples M had to be  reduced to reach a stationary point of the likelihood. Training of the BRNNs of both  types must be done using a gradient descent algorithm. Here a modified version of  RPROP [4] was used, which is in more detail described in [6].  The measure used in comparing the tested approaches is the log-likelihood of train-  ing and test data given the models built on the training data. In absence of a search  algorithm to perform recognition this is a valid measure to evaluate the models since  maximizing log-likelihood on the training data is the objective for all model types.  Note that the given alignment of vectors to phoneme classes for the test data is  used in calculating the log-likelihood on the test data - theie is no search for the  best alignment.  3.2 Results  Figure 3 shows the average log-likelihoods depending on the number of mixture  components for all tested approaches on training (upper line) and test data (lower  line). The baseline 1-state monophones give the lowest likelihood. The 3-state  monophones are slightly better, but have a larger gap between training and test  data likelihood. For comparison on the training data a system with 571 distinct  triphones with 3 states each was trained also. Note that this system has a lot more  parameters than the BRNN systems (see Table 1) it was compared to. The results  for the traditional Gaussian mixture systems show how the models become better  by building more detailed models for different (phonetic) context, i.e., by using more  states and more context classes.  The mixture density BRNN of type I gives a higher likelihood than the traditional  Gaussian mixture models. This was expected because the BRNN type I models  are, in contrast to the traditional Gaussian mixture models, able to include all  possible phonetic context effects by removing assumption (5) - i.e. a frame of a  certain phoneme surrounded by frames of any other phonemes with theoretically no  restriction about the range of the contextual influence.  The mixture density BRNN of type II, which in addition removes the independence  assumption (4), gives a significant higher likelihood than all other models. Note  that the difference in likelihood on training and test data for this model is very  small, indicating a useful model for the underlying distribution of the data.  Bidirectional Recurrent Mixture Density Networks 595  -19  -20  -21  -22  -23  -24  -25  -26  -27  -28  -29  -30  0  'mono. 1 state.train' --  'mono.lstate.test' --  'mono.3state.traJn' - ....  'mono.3state.test' - ....  tri.571.3state.train ......  'BRNN I.traJn'  'BRNI I,test  'BRNN IT. train' -- ~  'BRNlll.test' -.- -  I I I I I I I  2 4 6 8 10 12 14 16  NUMBER OF GAUSSIAN MIXTURE COMPONENTS  Figure 3: Mixture density BRNNs for multi-modal regression: Results  4 Conclusions  The mixture density BRNNs allow one to model probabilistic expressions frequently  occurring in sequence processing problems, with less assumptions than traditionally  necessary. Here it was shown that they can model the statistical properties of  speech data better than the traditional approach using Gaussian mixture models,  making mixture density BRNNs and approximations to them potential candidates  for improved speech recognition, coding and synthesis.  Many issues couldn't be covered in this paper because of space limitations. A more  detailed description of these models can be found in [6].  References  [1] C. M. Bishop. Mixture density networks. Technical Report NCRG/94/004, Neural  Computing Research Group, Aston University, Birmingham, England, 1994.  [2] C. M. Bishop. Neural Networks .for Pattern Recognition. Clarendon Press, Oxford,  England, 1995.  [3] Linguistic Data Consortium. TIMIT Acoustic-Phonetic Continuous Speech Corpus,  1993. (http://morph.ldc.upenn.edu/Catalog/LDC93S1.html).  [4] M. Riedmiller and H. Braun. A direct adaptive method for faster back-propagation  learning: The RPROP algorithm. In Proceedings o] the IEEE International ConIerence  on Neural Networks, pages 586-591, 1993.  [5] M. Schuster and K. K. Paliwal. Bidirectional recurrent neural networks. IEEE Trans-  actions on Neural Networks, 45(11):2673-2681, 1997.  [6] M. Schuster. On supervised learning ]tom sequential data with applications ]or  speech recognition. PhD thesis, Nara Institute of Science and Technology, Nara,  JAPAN, 1999. (http://isw3.aist-nara.ac.jp/IS/Shikano-lab/database/library/paper/  DT9661205.ps.gz).  [7] S. Young. A review of large vocabulary speech recognition. IEEE Signal Processing  Magazine, 15(5):45-57, 1996.  
Actor-Critic Algorithms  Vijay R. Konda John N. Tsitsiklis  Laboratory for Information and Decision Systems,  Massachusetts Institute of Technology,  Cambridge, MA, 02139.  konda@mit. edu, jnt@mit. edu  Abstract  We propose and analyze a class of actor-critic algorithms for  simulation-based optimization of a Markov decision process over  a parameterized family of randomized stationary policies. These  are two-time-scale algorithms in which the critic uses TD learning  with a linear approximation architecture and the actor is updated  in an approximate gradient direction based on information pro-  vided by the critic. We show that the features for the critic should  span a subspace prescribed by the choice of parameterization of the  actor. We conclude by discussing convergence properties and some  open problems.  1 Introduction  The vast majority of Reinforcement Learning (RL) [9] and Neuro-Dynamic Pro-  gramming (NDP) [1] methods fall into one of the following two categories:  (a)  (b)  Actor-only methods work with a parameterized family of policies. The gra-  dient of the performance, with respect to the actor parameters, is directly  estimated by simulation, and the parameters are updated in a direction of  improvement [4, 5, 8, 13]. A possible drawback of such methods is that the  gradient estimators may have a large variance. Furthermore, as the pol-  icy changes, a new gradient is estimated independently of past estimates.  Hence, there is no "learning," in the sense of accumulation and consolida-  tion of older information.  Critic-only methods rely exclusively on value function approximation and  aim at learning an approximate solution to the Bellman equation, which will  then hopefully prescribe a near-optimal policy. Such methods are indirect  in the sense that they do not try to optimize directly over a policy space. A  method of this type may succeed in constructing a "good" approximation of  the value function, yet lack reliable guarantees in terms of near-optimality  of the resulting policy.  Actor-critic methods aim at combining the strong points of actor-only and critic-  only methods. The critic uses an approximation architecture and simulation to  learn a value function, which is then used to update the actor's policy parameters  Actor-Critic Algorithms 1009  in a direction of performance improvement. Such methods, as long as they are  gradient-based, may have desirable convergence properties, in contrast to critic-  only methods for which convergence is guaranteed in very limited settings. They  hold the promise of delivering faster convergence (due to variance reduction), when  compared to actor-only methods. On the other hand, theoretical understanding of  actor-critic methods has been limited to the case of lookup table representations of  policies [6].  In this paper, we propose some actor-critic algorithms and provide an overview of  a convergence proof. The algorithms are based on an important observation. Since  the number of parameters that the actor has to update is relatively small (compared  to the number of states), the critic need not attempt to compute or approximate  the exact value function, which is a high-dimensional object. In fact, we show that  the critic should ideally compute a certain "projection" of the value function onto a  low-dimensional subspace spanned by a set of "basis functions," that are completely  determined by the parameterization of the actor. Finally, as the analysis in [11]  suggests for TD algorithms, our algorithms can be extended to the case of arbitrary  state and action spaces as long as certain ergodicity assumptions are satisfied.  We close this section by noting that ideas similar to ours have been presented in  the simultaneous and independent work of Sutton et al. [10].  2  Markov decision processes and parameterized family of  RSP's  Consider a Markov decision process with finite state space $, and finite action space  A. Let g: $ x A -> I be a given cost function. A randomized stationary policy (RSP)  is a mapping/ that assigns to each state x a probability distribution over the action  space A. We consider a set of randomized stationary policies IP - {/;8  In),  parameterized in terms of a vector 8. For each pair (x,u)  5' x A, lU(x,u) denotes  the probability of taking action u when the state x is encountered, under the policy  corresponding to 8. Let pxy (u) denote the probability that the next state is y, given  that the current state is x and the current action is u. Note that under any RSP, the  sequence of states {Xn) and of state-action pairs {Xn, Un) of the Markov decision  process form Markov chains with state spaces 5' and 5' x A, respectively. We make  the following assumptions about the family of policies IP.  (A1)  (A2)  For all x  5' and u  A the map 8 -> /(x,u) is twice differentiable  with bounded first, second derivatives. Furthermore, there exists a ll  valued function  (x, u) such that V/ (x, u) =/ (x, u) (x, u) where the  mapping 8  (x, u) is bounded and has first bounded derivatives for any  fixed x and u.  For each 8  I n , the Markov chains {Xn) and {Xn, Un) are irreducible and  aperiodic, with stationary probabilities r (x) and r/ (x, u) = r (x)/ (x, u),  respectively, under the RSP/.  In reference to Assumption (A1), note that whenever/(x,u) is nonzero we have  )(x,u) = /(x, u) = Vin/(x,u).  Consider the average cost function ,X: I '  I, given by  = g(x,  xS,uA  1010 V. R. Konda and J. N. Tsitsiklis  We are interested in minimizing A(9) over all 8. For each 9  ll n, let Vs ' $  lt  be the "differential" cost function, defined as solution of Poisson equation:  A(8) + Vs(x)= E ps(x,u) g(x,u)+ Epxv(u)Vs(y)].  uA y  Intuitively, Vs (x) can be viewed as the "disadvantage" of state x: it is the expected  excess cost - on top of the average cost - incurred if we start at state x. It plays  role similar to that played by the more familiar value function that arises in total  or discounted cost Markov decision problems. Finally, for every O  ll n, we define  the q-function qs ' $ x A - ll, by  qs(x,u) = g(x,u) - A(O) + Epxv(u)Vs(y).  y  We recall the following result, as stated in [8]. (Different versions of this result have  been established in [3, 4, 5].)  Theorem 1.  t9 () _ E ris(x,u)qs(x,u)$(x,u ) (1)  where $(x, u) stands for the ith component of  In [8], the quantity qs(x,u) in the above formula is interpreted as the expected  excess cost incurred over a certain renewal period of the Markov chain Xn,  under the RSP Ps, and is then estimated by means of simulation, leading to actor-  only algorithms. Here, we provide an alternative interpretation of the formula in  Theorem 1, as an inner product, and thus derive a different set of algorithms, which  readily generalize to the case of an infinite space as well.  For any   ll n , we define the inner product (., ')s of two real valued functions ql, q2  on S x A, viewed as vectors in lll$1lAI, by  (ql, q2)s = E rio(x, u)ql (x, u)q2(x, u).  XU  With this notation we can rewrite the formula (1) as  (9OiA(O) = (qs,)s, i- 1,...,n.  Let ]]. Ils denote the norm induced by this inner product on lllSllAI. For each O  ll n  let s denote the span of the vectors $; i _( i _(n) in lllslll (This is same as  the set of all functions f on S x A of the form f(x,u) - 54 i$(x,u), for some  scalars O 1 ,... , On. )  Note that although the gradient of A depends on the q-function, which is a vector  in a possibly very high dimensional space ltlSllAI, the dependence is only through  its inner products with vectors in s. Thus, instead of "learning" the function  it would suffice to learn the projection of qs on the subspace  Indeed, let IIs ll [$t11  s be the projection operator defined by  Since  Ilsq = arg min Ilq- Oils.  it is enough to compute the projection of qs onto  Actor-Critic Algorithms 1 O11  3 Actor-critic algorithms  We view actor critic-algorithms as stochastic gradient algorithms on the parameter  space of the actor. When the actor parameter vector is ), the job of the critic is  to compute an approximation of the projection IIq of q onto . The actor uses  this approximation to update its policy in an approximate gradient direction. The  analysis in [11, 12] shows that this is precisely what TD algorithms try to do, i.e.,  to compute the projection of an exact value function onto a subspace spanned by  feature vectors. This allows us to implement the critic by using a TD algorithm.  (Note, however, that other types of critics are possible, e.g., based on batch solution  of least squares problems, as long as they aim at computing the same projection.)  We note some minor differences with the common usage of TD. In our context,  we need the projection of q-functions, rather than value functions. But this is  easily achieved by replacing the Markov chain (xt) in [11, 12] by the Markov chain  (Xn, Un). A further difference is that [11, 12] assume that the control policy and  the feature vectors are fixed. In our algorithms, the control policy as well as the  features need to change as the actor updates its parameters. As shown in [6, 2],  this need not pose any problems, as long as the actor parameters are updated on a  slower time scale.  We are now ready to describe two actor-critic algorithms, which differ only as far  as the critic updates are concerned. In both variants, the critic is a TD algorithm  with a linearly parameterized approximation architecture for the q-function, of the  form  m  j=l  where r = (r,...,r m)  m denotes the parameter vector of the critic. The  features b, j = 1,... ,m, used by the critic are dependent on the actor parameter  vector ) and are chosen such that their span in ]$11AI, denoted by , contains   . Note that the formula (2) still holds if II is redefined as projection onto   as long as  contains . The most straightforward choice would be to let m = n  and b -  for each i. Nevertheless, we allow the possibility that m > n and   properly contains , so that the critic uses more features than that are actually  necessary. This added flexibility may turn out to be useful in a number of ways:  1. It is possible for certain values of 8, the features  are either close to  zero or are almost linearly dependent. For these values of ), the operator  II becomes ill-conditioned and the algorithms can become unstable. This  might be avoided by using richer set of features .  2. For the second algorithm that we propose (TD() e  1) critic can only  compute approximate - rather than exact - projection. The use of additional  features can result in a reduction of the approximation error.  Along with the parameter vector r, the critic stores some auxiliary parameters: these  are a (scalar) estimate , of the average cost, and an m-vector z which represents  Sutton's eligibility trace [1, 9]. The actor and critic updates take place in the course  of a simulation of a single sample path of the controlled Markov chain. Let rk, zk,   be the parameters of the critic, and let 9 be the parameter vectpr of the actor, at  time k. Let (X, U) be the state-action pair at that time. Let Xk+ be the new  state, obtained after action U is applied. A new action U+I is generated according  to the RSP corresponding to the actor parameter vector 9. The critic carries out  an update similar to the average cost temporal-difference method of [12]:  = + -  1012 V. R. Konda andJ. N. TsitsiMis  (Here, 7k is a positive stepsize parameter.) The two variants of the critic use  different ways of updating zk'  TD(1) Critic:  Let x* be a state in $.  Zk_{_ 1 ---- Z k '- O0(Xk_{_l,Uk_{_l),  =  if X+  x*  otherwise.  TD(a) Critic, 0 _  < 1:  Zk_{_ 1 -- OtZk -{- o (Xk_l_l , Uk_{_l ).  Actor:  Finally, the actor updates its parameter vector by letting  Ok-l-1 -- Ok -- /k F(rk)Qr ( Xk+ l , Uk + l )0 ( Xk + l , Uk+ l ).  Here,/ is a positive stepsize and F(rk) > 0 is a normalization factor satisfying:  (A3) F(.) is Lipschitz continuous.  (A4) There exists C > 0 such that  c  r(r) <_  The above presented algorithms are only two out of many variations. For instance,  one could also consider "episodic" problems in which one starts from a given initial  state and runs the process until a random termination time (at which time the  process is reinitialized at x*), with the objective of minimizing the expected cost  until termination. In this setting, the average cost estimate A is unnecessary  and is removed from the critic update formula. If the critic parameter r were to  be reinitialized each time that x* is entered, one would obtain a method closely  related to Williams' REINFORCE algorithm [13]. Such a method does not involve  any value function learning, because the observations during one episode do not  affect the critic parameter r during another episode. In contrast, in our approach,  the observations from all past episodes affect current critic parameter r, and in  this sense critic is "learning". This can be advantageous because, as long as 0 is  slowly changing, the observations from recent episodes carry useful information on  the q-function under the current policy.  4 Convergence of actor-critic algorithms  Since our actor-critic algorithms are gradient-based, one cannot expect to prove  convergence to a globally optimal policy (within the given class of RSP's). The  best that one could hope for is the convergence of VA(0) to zero; in practical terms,  this will usually translate to convergence to a local minimum of (0). Actually,  because the TD(a) critic will generally converge to an approximation of the desired  projection of the value function, the corresponding convergence result is necessarily  weaker, only guaranteeing that VA(O) becomes small (infinitely often). Let us now  introduce some further assumptions.  Actor-Critic Algorithms 1013  (A5) For each 0  n, we define an m x m matrix G(O) by  G(o) = r.  We assume that G(O) is uniformly positive definite, that is, there exists  some q > 0 such that for all r  11 m and 0  1t '  rrG(O)r > qllrll 2  (A6) We assume that the stepsize sequences {7k}, {/k} are positive, nonincreas-  ing, and satisfy  6k >0, V, y.6k = ec, y.6 <  k k  where 5k stands for either/k or ffk. We also assume that  ---0.  Note that the last assumption requires that the actor parameters be updated at a  time scale slower than that of critic.  Theorem 2. In an actor-critic algorithm with a TD(1) critic,  lim inf IIV (0k)11 - 0  k  Furthermore, if {0k} is bounded w.p. I then  lim IIVX(0k)]l- 0  k  Theorem 3. For every e > O, there exists  liminfk llVX(0k)11 <_  .p. 1.  w.p. 1.  w.p. 1.  sufficiently close to  1, such that  Note that the theoretical guarantees appear to be stronger in the case of the TD(1)  critic. However, we expect that TD(a) will perform better in practice because of  much smaller variance for the parameter rk. (Similar issues arise when considering  actor-only algorithms. The experiments reported in [7] indicate that introducing a  forgetting factor a < i can result in much faster convergence, with very little loss of  performance.) We now provide an overview of the proofs of these theorems. Since  /k/ffk -* 0, the size of the actor updates becomes negligible compared to the size  of the critic updates. Therefore the actor looks stationary, as far as the critic is  concerned. Thus, the analysis in [1] for the TD(1) critic and the analysis in [12]  for the TD(a) critic (with a < 1) can be used, with appropriate modifications, to  conclude that the critic's approximation of H0 qo will be "asymptotically correct".  If r(O) denotes the value to which the critic converges when the actor parameters  are fixed at 0, then the update for the actor can be rewritten as  = - +/kek,  where ek is an error that becomes asymptotically negligible. At this point, standard  proof techniques for stochastic approximation algorithms can be used to complete  the proof.  15 Conclusions  The key observation in this paper is that in actor-critic methods, the actor pa-  rameterization and the critic parameterization need not, and should not be chosen  1014 V. R. Konda andJ. N. Tsitsiklis  independently. Rather, an appropriate approximation architecture for the critic is  directly prescribed by the parameterization used in actor.  Capitalizing on the above observation, we have presented a class of actor-critic algo-  rithms, aimed at combining the advantages of actor-only and critic-only methods. In  contrast to existing actor-critic methods, our algorithms apply to high-dimensional  problems (they do not rely on lookup table representations), and are mathematically  sound in the sense that they possess certain convergence properties.  Acknowledgments: This research was partially supported by the NSF under  grant ECS-9873451, and by the AFOSR under grant F49620-99-1-0320.  References  [1] D. P. Bertsekas and J. N. Tsitsiklis. Neurodynamic Programming. Athena  Scientific, Belmont, MA, 1996.  [2] V. S. Borkar. Stochastic approximation with two time scales. Systems and  Control Letters, 29:291-294, 1996.  [3] X. R. Cao and H. F. Chen. Perturbation realization, potentials, and sensitiv-  ity analysis of Markov processes. IEEE Transactions on Automatic Control,  42:1382-1393, 1997.  [4] P. W. Glynn. Stochastic approximation for monte carlo optimization. In Pro-  ceedings of the 1986 Winter Simulation Conference, pages 285-289, 1986.  [5] T. Jaakola, S. P. Singh, and M. I. Jordan. Reinforcement learning algorithms  for partially observable Markov decision problems. In Advances in Neural In-  formation Processing Systems, volume 7, pages 345-352, San Francisco, CA,  1995. Morgan Kaufman.  [6] V. R. Konda and V. S. Borkar. Actor-critic like learning algorithms for Markov  decision processes. SIAM Journal on Control and Optimization, 38(1):94-123,  1999.  [7] P. Marbach. Simulation based optimization of Markov reward processes. PhD  thesis, Massachusetts Institute of Technology, 1998.  [8] P. Marbach and J. N. Tsitsiklis. Simulation-based optimization of Markov  reward processes. Submitted to IEEE Transactions on Automatic Control.  [9] R. Sutton and A. Barto. Reinforcement Learning: An Introduction. MIT Press,  Cambridge, MA, 1995.  [10] R. S. Sutton, D. McAllester, S. Singh, and Y. Mansour. Policy gradient meth-  ods for reinforcement learning with function approximation. In this proceedings.  [11] J. N. Tsitsiklis and B. Van Roy.  ing with function approximation.  42(5):674-690, 1997.  [12] J. N. Tsitsiklis and B. Van Roy.  [13]  An analysis of temporal-difference learn-  IEEE Transactions on Automatic Control,  Average cost temporal-difference learning.  Automatica, 35(11):1799-1808, 1999.  R. Williams. Simple statistical gradient following algorithms for connectionist  reinforcement learning. Machine Learning, 8:229-256, 1992.  
An Information-Theoretic Framework for  Understanding Saccadic Eye Movements  Tai Sing Lee *  Department of Computer Science  Carnegie Mellon University  Pittsburgh, PA 15213  tai@cs. cmu. edu  Stella X. Yu  Pobotics Institute  Carnegie Mellon University  Pittsburgh, PA 15213  stella@cnbc. cmu. edu  Abstract  In this paper, we propose that information maximization can pro-  vide a unified framework for understanding saccadic eye move-  ments. In this framework, the mutual information among the cor-  tical representations of the retinal image, the priors constructed  from our long term visual experience, and a dynamic short-term  internal representation constructed from recent saccades provides  a map for guiding eye navigation. By directing the eyes to loca-  tions of maximum complexity in neuronal ensemble responses at  each step, the automatic saccadic eye movement system greedily  collects information about the external world, while modifying the  neural representations in the process. This framework attempts  to connect several psychological phenomena, such as pop-out and  inhibition of return, to long term visual experience and short term  working memory. It also provides an interesting perspective on  contextual computation and formation of neural representation in  the visual system.  1 Introduction  When we look at a painting or a visual scene, our eyes move around rapidly and  constantly to look at different parts of the scene. Are there rules and principles that  govern where the eyes are going to look next at each moment? In this paper, we  sketch a theoretical framework based on information maximization to reason about  the organization of saccadic eye movements.  *Both authors are members of the Center for the Neural Basis of Cognition - a joint  center between University of Pittsburgh and Carnegie Mellon University. Address: Rm  115, Mellon Institute, Carnegie Mellon University, Pittsburgh, PA 15213.  Information-Theoretic Framework for Understanding Saccadic Behaviors 835  Vision is fundamentally a Bayesian inference process. Given the measurement by  the retinas, the brain's memory of eye positions and its prior knowledge of the  world, our brain has to make an inference about what is where in the visual scene.  The retina, unlike a camera, has a peculiar design. It has a small foveal region  dedicated to high-resolution analysis and a large low-resolution peripheral region  for monitoring the rest of the visual field. At about 2.5  visual angle away from  the center of the fovea, visual acuity is already reduced by a half. When we 'look'  (foveate) at a certain location in the visual scene, we direct our high-resolution  fovea to analyze information in that location, taking a snap shot of the scene using  our retina. Figure 1A-C illustrate what a retina would see at each fixation. It  is immediately obvious that our retinal image is severely limited - it is clear only  in the fovea and is very blurry in the surround, posing a severe constraint on the  information available to our inference system. Yet, in our subjective experience, the  world seems to be stable, coherent and complete in front of us. This is a paradox  that have engaged philosophical and scientific debates for ages. To overcome the  constraint of the retinal image, during perception, the brain actively moves the eyes  around to (1) gather information to construct a mental image of the world, and (2)  to make inference about the world based on this mental image. Understanding the  forces that drive saccadic eye movements is important to elucidating the principles  of active perception.  A B C D  Figure 1. A-C: retinal images in three separate fixations. D: a mental mosaic created by  integrating the retinal images from these three and other three fixations.  It is intuitive to think that eye movements are used to gather information. Eye  movements have been suggested to provide a means for measuring the allocation  of attention or the values of each kind of information in a particular context [16].  The basic assumption of our theory is that we move our eyes around to maximize  our information intake from the world, for constructing the mental image and for  making inference of the scene. Therefore, the system should always look for and  attentively fixate at a location in the retinal image that is the most unusual or the  most unexplained - and hence carries the maximum amount of information.  2 Perceptual Representation  How can the brain decide which part of the retinal image is more unusual? First of  all, we know the responses of V1 simple cells, modeled well by the Gabor wavelet  pyramid [3,7], can be used to reconstruct completely the retinal image. It is also  well established that the receptive fields of these neurons developed in such a way  as to provide a compact code for natural images [8,9,13,14]. The idea of compact  code or sparse code, originally proposed by Barlow [2], is that early visual neurons  capture the statistical correlations in natural scenes so that only a small number  836 T. S. Lee and S. X. Yu  of cells out of a large set will be activated to represent a particular scene at each  moment. Extending this logic, we suggest that the complexity or the entropy of  the neuronal ensemble response of a hypercolumn in V1 is therefore closely related  to the strangeness of the image features being analyzed by the machinery in that  hypercolumn. A frequent event will have a more compact representation in the  neuronal ensemble response. Entropy is an information measure that captures the  complexity or the variability of signals. The entropy of a neuronal ensemble in a  hypercolumn can therefore be used to quantify the strangeness of a particular event.  A hypercolumn in the visual cortex contains roughly 200,000 neurons, dedicated  to analyzing different aspects of the image in its 'visual window'. These cells are  tuned to different spatial positions, orientations, spatial frequency, color disparity  and other cues. There might also be a certain degree of redundancy, i.e. a number  of neurons are tuned to the same feature. Thus a hypercolumn forms the funda-  mental computational unit for image analysis within a particular window in visual  space. Each hypercolumn contains cells with receptive fields of different sizes, many  significantly smaller than the aggregated 'visual window' of the hypercolumn. The  entropy of a hypercolumn's ensemble response at a certain time t is the sum of  entropies of all the channels, given by,  H(u(R, t)) = - y. y.p(u(R, v, , O, t)) log2p(u(R, v, , O, t))  where u(Re, t) denotes [he responses of all complex cell channels inside the visual  window R5 of a hypercolumn at time t, computed within a 20 msec time window.  u(, , 0, t) is the response of a V1 complex cell channel of a particular scale  and  orientation  at spatial location  at t. p(u(R, v, , O, t)) is the probability of cells  in that channel within the visual window R2 of the hypercolumn firing v number  of spikes. v can be computed as  cell channels, modeled by Gabor  probability p(u(Re, v, r, O, t)) can  the variations in spatial position  the power modulus of the corresponding simple  wavelets [see 7]. -p(u(_R2, v,r,O,t))-1. The  be computed at each moment in time because of  of the receptive fields of similar cell within the  hypercolumn - hence the 'same' cells in the hypercolumn are analyzing different  image patches, and also because of the redundancy of cells coding similar features.  The neurons' responses in a hypercolumn are subject to contextual modulation from  other hypercolumns, partly in the form of lateral inhibition from cells with similar  tunings. The net observed effect is that the later part of V1 neurons' response,  starting at about 80 msec, exhibits differential suppression depending on the spatial  extent and the nature of the surround stimulus. The more similar the surround  stimulus is to the center stimuli, and the larger the spatial extent of the 'similar  surround', the stronger is the suppressive effect [e.g. 6]. Simoncelli and Schwartz  [15] have proposed that the steady state responses of the cells can be modeled by  dividing the response of the cell (i.e. modeled by the wavelet coefficient or its power  modulus) by a weighted combination of the responses of its spatial neighbors in order  to remove the statistical dependencies between the responses of spatial neighbors.  These weights are found by minimizing a predictive error between the center signal  from the surround signals. In our context, this idea of predictive coding [see also 14]  is captured by the concept of mutual information between the ensemble responses  of the different hypercolumns as given below,  I(u(R,t);u(gi,t-dt)) - H(u(R,t))- H(u(R,t)lu(gi,t-dt) )  = y. y. (u(R,vt,a,O,t),u(gi,vr,a,O,t))  0'0 VR,V  log2 p(u(R,vR, a,O,t),u(,vn,a,O,t))  O, t) ), O, t) )]'  Information-Theoretic Framework for Understanding $accadic Behaviors 837  where u(Re, t) is the ensemble response of the hypercolumn in question, and u(f2e, t)  is the ensemble response of the surrounding hypercolumns. p(u(Re, vR, r, O, t)) is the  probability that cells of a channel in the center hypercolumn assumes the response  value vR and p(u(f2e, v, r, O, t)) the probability that cells of a similar channel in the  surrounding hypercolumns assuming the response value vr. tx i the delay by which  the surround information exerts its effect on the center hypercolumn. The mutual  information I can be computed from the joint probability of ensemble responses of  the center and the surround.  The steady state responses of the V1 neurons, as a result of this contextual modula-  tion, are said to be more correlated to perceptual pop-out than the neurons' initial  responses [5,6]. The complexity of the steady state response in the early visual  cortex is described by the following conditional entropy,  H(u(R2, t)lu(g2,t- 6t)) = H(u(R2,t)) - I(u(R2,t); u(g22, t- 6tl)).  However, the computation in V1 is not limited to the creation of compact repre-  sentation through surround inhibition. In fact, we have suggested that V1 plays  an active role in scene interpretation particularly when such inference involves high  resolution details [6]. Visual tasks such as the inference of contour and surface likely  involve V1 heavily. These computations could further modify the steady state re-  sponses of V1, and hence the control of saccadic eye movements.  3 Mental Mosaic Representation  The perceptual representation provides the basic force for the brain to steer the high  resolution fovea to locations of maximum uncertainty or maximum signal complex-  ity. Foveation captures the maximum amount of available information in a location.  Once a location is examined by the fovea, its information uncertainty is greatly re-  duced. The eyes should move on and not to return to the same spot within a certain  period of time. This is called the 'inhibition of return'.  How can we model this reduction of interest? We propose that the mind creates  a mental mosaic of the scene in order to keep track of the information that have  been gathered. By mosaic, we mean that the brain can assemble successive reti-  nal images obtained from multiple fixations into a coherent mental picture of the  scene. Figure 1D provides an example of a mental mosaic created by combining  information from the retinal images from 6 fixations. Whether the brain actually  keeps such a mental mosaic of the scene is currently under debate. McConkie and  Raynet [10] had suggested the idea of an integrafive visual buffer to integrate in-  formation across multiple saccades. However, numerous experiments demonstrated  we actually remember relatively little across saccades [4]. This lead to the idea  that brain may not need an explicit internal representation of the world. Since the  world is always out there, the brain can access whatever information it needs at the  appropriate details by moving the eyes to the appropriate place at the appropriate  time. The subjective feeling of a coherent and a complete world in front of us is a  mere illusion [e.g. 1].  The mental mosaic represented in Figure 1D might resemble McConkie and Rayner's  theory superficially. But the existence of such a detailed high-resolution buffer with  a large spatial support in the brain is rather biologically implausible. Rather, we  think that the information corresponding to the mental mosaic is stored in an in-  terpreted and semantic form in a mesh of Bayesian belief networks in the brain  (e.g. involving PO, IT and area 46). This distributed semantic representation of  838 T. S. Lee and S. X.. Yu  the mental mosaic, however, is capable of generating detailed (sometimes false) im-  agery in early visual cortex using the massive recurrent convergent feedback from  the higher areas to V1. However, because of the limited support provided by V1  machinery, the instantiation of mental imagery in V1 has to be done sequentially  one 'retinal image' frame at a time, presumably in conjunction with eye movement,  even when the eyes are closed. This might explain why vivid visual dream is always  accompanied by rapid eye movement in REM sleep. The mental mosaic accumu-  lates information from the retinal images up to the last fixation and can provide  prediction on what the retina will see in the current fixation. For each u(, , O)  cell, there is a corresponding effective prediction signal m(, , O) fed back from the  mental mosaic.  This prediction signal can reduce the conditional entropy or complexity of the en-  semble response in the perceptual representation by discounting the mutual infor-  mation between the ensemble response to the retinal image and the mental mosaic  prediction as follow,  H(u(Re, t)lm(Re,t--St2)) = I(u(R2, t),m(R2, t-St2))  where 5t2 is the transmission delay from the mental mosaic back to V1.  At places where the fovea has visited, the mental mosaic representation has high  resolution information and m(, r, O, t - 5re) can explain u(, r, 0, t) fully. Hence,  the mutual information is high at those hypercolumns and the conditional entropy  H(u(Re, t)lm(Re, t- 5))is low, with two consequences: (1) the system will not  get the eyes stuck at a particular location; once the information at  is updated  to the mental mosaic, the system will lose interest and move on; (2) the system  will exhibit 'inhibition of return' as the information in the visited locations are  fully predicted by the mental mosaic. Also, from this standpoint, the 'habituation  dynamics' often observed in visual neurons when the same stimulus is presented  multiple times might not be simply due to neuro-chemical fatigue, but might be  understood in terms of mental mosaic being updated and then fed back to explain  the perceptual representation in V1. The mental mosaic is in effect our short-term  memory of the scene. It has a forgetting dynamics, and needs to be periodically  updated. Otherwise, it will rapidly fade away.  4 Overall Reactive Saccadic Behaviors  Now, we can combine the influence of the two predictive processes to arrive at a  discounted complexity measure of the hypercolumn's ensemble response:  H(u(R2, t)lu(2, t-6tl),m(R2, t-6t2)) = H(u(Rs,t))  --'(u(., t); u(2, t -- 5tl))  -I(u(R2, t); m(R2, t - 6t2) )  +I(u(2, t - 6tx); m(R2, t - 6t2) )  If we can assume the long range surround priors and the mental mosaic short term  memory are independent processes, we can leave out the last term, I(u(s,t-  5t); m(Rs, t - 5t)), of the equation.  The system, after each saccade, will evaluate the new retinal scene and select the  location where the perceptual representation has the maximum conditional entropy.  To maximize the information gain, the system must constantly search for and make  a saccade to the locations of maximum uncertainty (or complexity) computed from  Information- Theoretic Framework for Understanding Saccadic Behaviors 839  the hypercolumn ensemble responses in V1 at each fixation. Unless the number of  saccades is severely limited, this locally greedy algorithm, coupled the inhibition  of return mechanism, will likely steer the system to a relatively optimal global  sampling of the world - in the sense that the average information gain per saccade  is maximized, and the mental mosaic's dissonance with the world is minimized.  5 Task-dependent schema Representation  However, human eye movements are not simply controlled by the generic informa-  tion in a bottom-up fashion. Yarbus [16] has shown that, when staring at a face,  subjects' eyes tend to go back to the same locations (eyes, mouth) over and over  again. Further, he showed that when asked different questions, subjects exhibited  different kinds of scan-paths when looking at the same picture. Norton and Stark  [12] also showed that eye movements are not random, but often exhibit repetitive  or even idiosyncratic path patterns.  To capture these ideas, we propose a third representation, called task schema, to  provide the necessary top-down information to bias the eye movement control. It  specifies the learned or habitual scan-paths for a particular task in a particular  context or assigns weights to different types of information. Given that we arenot  mostly unconscious of the scan-path patterns we are making, these task-sensitive  or context-sensitive habitual scan-patterns might be encoded at the levels of motor  programs, and be downloaded when needed without our conscious control. These  motor programs for scan-paths can be trained from reinforcement learning. For  example, since the eyes and the mouths convey most of the emotional content of  a facial expression, a successful interpretation of another person's emotion could  provide the reward signal to reinforce the motor programs just executed or the fixa-  tions to certain facial features. These unconscious scan-path motor programs could  provide the additional modulation to automatic saccadic eye movement generation.  6 Discussion  In this paper, we propose that information maximization might provide a theoretical  framework to understand the automatic saccadic eye movement behaviors in human.  In this proposal, each hypercolumn in V1 is considered a fundamental computational  unit. The relative complexity or entropy of the neuronal ensemble response in the  V1 hypercolumns, discounted by the predictive effect of the surround, higher order  representations and working memory, creates a force field to guide eye navigation.  The framework we sketched here bridge natural scene statistics to eye movement  control via the more established ideas of sparse coding and predictive coding in neu-  ral representation. Information maximization has been suggested to be a possible  explanation for shaping the receptive fields in the early visual cortex according to  the statistics of natural images [8,9,13,14] to create a minimum-entropy code [2,3].  As a result, a frequent event is represented efficiently with the response of a few  neurons in a large set, resulting in a lower hypercolumn ensemble entropy, while  unusual events provoke ensemble responses of higher complexity. We suggest that  higher complexity in ensemble responses will arouse attention and draw scrutiny by  the eyes, forcing the neural representation to continue adapting to the statistics of  the natural scenes. The formulation here also suggests that information maximiza-  tion might provide an explanation for the formation of horizontal predictive network  in V1 as well as higher order internal representations, consistent with the ideas of  predictive coding [11, 14, 15]. Our theory hence predicts that the adaptation of the  840 T. S. Lee and S. X. Yu  neural representations to the statistics of natural scenes will lead to the adaptation  of, saccadic eye movement behaviors.  Acknowledgement s  The authors have been supported by a grant from the McDonnell Foundation and  a NSF grant (LIS 9720350). Yu is also being supported in part by a grant to Takeo  Kanade.  References  [1] Ballard, D. Hayhoe, M.M. Pook, P.K. & Rao, R.P.N. (1997). Deictic codes for the  embodiment of cognition. Behavioral and Brain Science, 20:4, December, 723-767.  [2] Barlow, H.B. (1989). Unsupervised learning. Neural Computation, 1, 295-311.  [3] Daugman, J.G. (1989). Entropy reduction and decorrelation in visual coding by ori-  ented neural receptive fields. IEEE Transactions on Biomedical Engineering 36:, 107-114.  [4] Irwin, D. E, 1991. Information Integration across Saccadic Eye Movements. Cognitive  Psychology, 23(3):420-56.  [5] Knierim, J. & Van Essen, D.C. Neural response to static texture patterns in area V1  of macaque monkey. J. Neurophysiology, 67: 961-980.  [6] Lee, T.S., Mumford, D., Romero R. & Lamme, V.A.F. (1998). The role of primary  visual cortex in higher level vision. Vision Research 38, 2429-2454.  [7] Lee, T.S. (1996). Image representation using 2D Gabor wavelets. IEEE Transaction  of Pattern Analysis and Machine Intelligence. 18:10, 959-971.  [8] Lewicki, M. & Olshausen, B. (1998). Inferring sparse, overcomplete image codes using  an efficient coding framework. In Advances in Neural Information Processing System 10,  M. Jordan, M. Kearns and S. Solla (eds). MIT Press.  [9] Linsker, R. (1989). How to generate ordered maps by maximizing the mutual informa-  tion between input and output signals. Neural Computation, 1: 402-411.o  [10] McConkie, G.W. & Rayner, K. (1976). Identifying the span of effective stimulus in  reading. Literature review and theories of reading. In H. Singer and R.B. Ruddell (Eds),  Theoretical models and processes of reading, 137-162. Newark, D.E.: International Reading  Association.  [11] Mumford, D. (1992). On the computational architecture of the neocortex II. Biological  cybernetics, 66, 241-251.  [12] Norton, D. and Stark, L. (1971) Eye movements and visual perception. Scientific  American, 224, 34-43.  [13] Olshausen, B.A., & Field, D.J. (1996), Emergence of simple cell receptive field prop-  erties by learning a sparse code for natural images. Nature, 381: 607-609.  [14] Rao R., & Ballard, D.H. (1999). Predictive coding in the visual cortex: a functional  interpretation of some extra-classical receptive field effects. Nature Neuroscience, 2:1 79-  87.  [15] Simoncelli, E.P. & Schwartz, O. (1999). Modeling surround suppression in V1 neu-  rons with a statistically-derived normalization model. In Advances in Neural Information  Processing Systems 11, . M.S. Kearns, S.A. Solla, and D.A. Cohn (eds). MIT Press.  [16] Yarbus, A.L. (1967). Eye movements and vision. Plenum Press.  
Model selection in clustering by uniform  convergence bounds*  Joachim M. Buhmann nd Marcus Held  Institut fiir Informatik III,  RSmerstrafie 164, D-53117 Bonn, Germany  {jb,held)@cs.uni-bonn.de  Abstract  Unsupervised learning algorithms are designed to extract struc-  ture from data samples. Reliable and robust inference requires a  guarantee that extracted structures are typical for the data source,  i.e., similar structures have to be inferred from a second sample  set of the same data source. The overfitting phenomenon in max-  imum entropy based annealing algorithms is exemplarily studied  for a class of histogram clustering models. Bernstein's inequality  for large deviations is used to determine the maximally achievable  approximation quality parameterized by a minimal temperature.  Monte Carlo simulations support the proposed model selection cri-  terion by finite temperature annealing.  I Introduction  Learning algorithms are designed to extract structure from data. Two classes of  algorithms have been widely discussed in the literature - supervised and unsuper-  vised learning. The distinction between the two classes depends on supervision or  teacher information which is either available to the learning algorithm or missing.  This paper applies statistical learning theory to the problem of unsupervised learn-  ing. In particular, error bounds as a protection against over fitting are derived for  the recently developed Asymmetric Clustering Model (ACM) for co-occurrence  data [6]. These theoretical results show that the continuation method "determin-  istic annealing" yields robustness of the learning results in the sense of statistical  learning theory. The computational temperature of annealing algorithms plays the  role of a control parameter which regulates the complexity of the learning machine.  Let us assume that a hypothesis class 7/ of loss functions h(x; c) is given. These  loss functions measure the quality of structures in data. The complexity of 7/ is  controlled by coarsening, i.e., we define a 'y-cover of 7/. Informally, the inference  principle advocated by us performs learning by two inference steps: (i) determine  the optimal approximation level 7 for consistent learning (in terms of large risk devi-  ations); (ii) given the optimal approximation level 7, average over all hypotheses in  an appropriate neighborhood of the empirical minimizer. The result of the inference  *This work has been supported by the German Israel Foundation for Science and Re-  search Development (GIF) under grant #1-0403-001.06/95.  Model Selection in Clustering by Uniform Convergence Bounds 217  procedure is not a single hypothesis but a set of hypotheses. This set is represented  either by an average of loss functions or, alternatively, by a typical member of this  set. This induction approach is named Empirical Risk Approximation (ERA) [2].  The reader should note that the learning algorithm has to return an average struc-  ture which is typical in a -,-cover sense but it is not supposed to return the hypothesis  with minimal empirical risk as in Vapnik's "Empirical Risk Minimization" (ERM)  induction principle for classification and regression [9]. The loss function with mini-  mal empirical risk is usually a structure with maximal complexity, e.g., in clustering  the ERM principle will necessarily yield a solution with the maximal number of clus-  ters. The ERM principle, therefore, is not suitable as a model selection principle to  determine the number of clusters which are stable under sample fluctuations. The  ERA principle with its approximation accuracy q, solves this problem by controlling  the effective complexity of the hypothesis class.  In spirit, this approach is similar to the Gibbs-algorithm presented for example in  [3]. The Gibbs-algorithm samples a random hypothesis from the version space to  predict the label of the l + lth data point xt+. The version space is defined as  the set of hypotheses which are consistent with the first l given data points. In our  approach we use an alternative definition of consistency, where all hypothesis in an  appropriate neighborhood of the empirical minimizer define the version space (see  also [4]). Averaging over this neighborhood yields a structure with risk equivalent to  the expected risk obtained by random sampling from this set of hypotheses. There  exists also a tight methodological relationship to [7] and [4] where learning curves  for the learning of two class classifiers are derived using techniques from statistical  mechanics.  2 The Empirical Risk Approximation Principle  The data samples Z = (zr  f, 1 _ r _ l) which have to be analyzed by the unsu-  pervised learning algorithm are elements of a suitable object (resp. feature) space  f. The samples are distributed according to a measure/z which is not assumed to  be known for the analysis.   A mathematically precise statement of the ERA principle requires several defini-  tions which formalize the notion of searching for structure in the data. The qual-  ity of structures extracted from the data set Z is evaluated by the empirical risk  1  7(c; Z) :-- ? '-r=l h(zr; c) of a structure a given the training set Z. The function  h(z; a) is known as loss function in statistics. It measures the costs for processing a  generic datum z with model a. Each value a  A parameterizes an individual loss  function with A denoting the set of possible parameters. The loss function which  minimizes the empirical risk is denoted by d  := argminae^ 7(a; Z).  The relevant quality measure for learning is the expected risk T(c) :=  fa h(z; a)d/z(z). The optimal structure to be inferred from the data is a  :-  argminae^ T(a). The distribution /z is assumed to decay sufficiently fast with  bounded rth moments E, (lb(z; a) - T(a)l < r!w-2V. (h(z; a)), a e A  (r  2). E, (.) and V, (.) denote expectation and variance of a random vari-  able, respectively. - is a distribution dependent constant.  ERA requires the learning algorithm to determine a set hypotheses on the basis  of the finest consistently learnable cover of the hypothesis class. Given a learning  accuracy 7 a subset of parameters A v = (el,... ,alAl-1) U (&) can be defined  such that the hypothesis class 7/ is covered by the function balls with index sets  By(a) := {a'' fa Ih(z;a')- h(z;a)[ dp(z) _< 7}, i.e. A C [JaeA B(a). The em-   Knowledge of covering numbers is required in the following analysis which is a weaker  type of information than complete knowledge of the probability measure/ (see also [5]).  218 J. M. Buhmann andM. HeM  pirical minimizer & has been added to the cover to simplify bounding arguments.  Large deviation theory is used to determine the approximation accuracy "/for learn-  ing a hypothesis from the hypothesis class 7/. The expected risk of the empirical  minimizer exceeds the global minimum of the expected risk T(c ) by ca* with a  probability bounded by Bernstein's inequality [8]  P {T(& ) -T(a ) >  < P(sup17(a)-T(a)I> 1  _ 2[A[exp-8+4,r(e_7/a.r)  The complexity lay[ of the coarsened hypothesis class has to be small enough to  guarantee with high confidence small e-deviations? This large deviation inequality  weighs two competing effects in the learning problem, i.e. the probability of a  large deviation exponentially decreases with growing sample size l, whereas a large  deviation becomes increasingly likely with growing cardinality of the "/-cover of the  hypothesis class. According to (1) the sample complexity l0 (% e, 5) is defined by  t0 2  lg IAvl- 8 + 4r(e-f/a  2  +1og =0. (2)  With probability 1 - 5 the deviation of the empirical risk from the expected risk is  bounded by  (ePt7 T -- "/) --: "/'PP. Averaging over a set of functions which exceed  the empirical minimizer by no more than 2"/'pp in empirical risk yields an average  hypothesis corresponding to the statistically significant structure in the data, i.e.,  7(a) - 7(& ) <_ T(a) +"/'P - (7(&) -"/') _< 2"/'p since T(a ) < T(& ) by  definition. The key task in the following remains to calculate the minimal precision  e("/) as a function of the approximation "/and to bound from above the cardinality  [Av[ of the "/-cover for specific learning problems.  3 Asymmetric clustering model  The asymmetric clustering model was developed for the analysis resp. grouping of  objects characterized by co-occurrence of objects and certain feature values [6].  Application domains for this explorerive data analysis approach are for example  texture segmentation, statistical language modeling or document retrieval.  Denote by f - A' x y the product space of objects xi  A',I _< i _< n and  features yj  y, 1 _< j _< f. The xi  3:' are characterized by observations  Z = {zr} = {(xi(0,yj(0),r = 1,... ,l}. The sufficient statistics of how often  the object-feature pair (xi, yj) occurs in the data set Z is measured by the set  of frequencies {rli j  number of observations (xi, y j)/total number of observations}.  Derived measurements are the frequency of observing object xi, i.e. r/i = Y]-=I r/ij  and the frequency of observing feature yj given object xi, i.e. r/li = rli/rli. The  asymmetric clustering model defines a generarive model of a finite mixture of com-  ponent probability distributions in feature space with cluster-conditional distri-  butions q = (qjlv), 1 _< j _< f, 1 _<  _< k (see [6]). We introduce indicator  variables Mir  {0, 1} for the membership of object xi in cluster   {1,..., k}.  Y].,x Mi = I i  i _< i <_ n enforces the uniqueness constraint for assignments.  2The maximal standard deviation a* := supe^ /V {h(z; a)} defines the scale to  measure deviations of the empirical risk from the expected risk (see [2]).  Model Selection in Clustering by Uniform Convergence Bounds 219  Using these variables the observed data Z are distributed according to the genera-  tive model over A' x y:  i k  P {xi'YjlM'q) - n .=x Mi"qjl"' (3)  For the analysis of the unknown data source -- characterized (at least approxima-  tively) by the empirical data Z -- a structure a - (M, q) with M  {0, 1) xk has  to be inferred. The aim of an ACM analysis is to group the objects xi as coded by  the unknown indicator variables Uiv and to estimate for each cluster  a prototyp-  ical feature distribution qjlv  k  Using the loss function h(xi,yj; a) = logn - '-v=x Mir logqjlv the maximiza-  tion of the likelihood can be formulated as minimization of the empirical risk:  7(a; Z) = y.in=x y.= rlijh(xi, yj; a), where the essential quantity to be minimized  is the expected risk: 7(a) = in y.= ptrue {xi,Yj} h(xi,Yj; c 0. Using the max-  imum entropy principle the following annealing equations are derived [6]:  ZiLl (Miv)r]iJ __ Z____ 1 (Miv)r]i  qjlv = Y.in__x {Mir) Y.=, {May) rljli' (4)  exp [ '.f=l ljli log qjlv ]  (M/v) = (5)  The critical temperature: Due to the limited precision of the observed data it  is natural to study histogram clustering as a learning problem with the hypothesis   2  1)A  class 7/= {-Y.v M,v logqjlv 'M,v  {0,1} A Y.vM,v = 1A qYlv  {?, 7," ,  Y-y qlv = 1}. The limited number of observations results in a limited precision of  the frequencies jli. The value qJl- = 0 h been excluded since it causes infinite  expected risk for pte {yj ix i } > 0. The size of the regulized hypothesis class  can be upper bounded by the cardinality of the complete hypothesis cls divided  by the minimal cardinality of a 7-function ball centered at a function of the 7-cover  A,, i.e. IA, I < I1/me  The cdinity of a hnction ball with radius ff can be appromated by adopting  techniques from asymptotic anysis [1] (0 (x) = (o  I(a)l : o .- (yy Ix) log (6)   {ql} i,j n j[(i)  = ' dql"  dO.  and the entropy $ is given by  3(q,Q,&) = 7&-ZvvZ,q, lv1)+  i Zi log Z. exp  k  The auxiliary variables Q = (Q.).= are Lagrge pameters to enforce the nor-  malizations j qJl. = 1. Choosing qjla = qj[(i)(i)  a, we obtain  approxi-  mation of the inteal. The reader should note that a saddlepoint appromation in  220 J. M. Buhmann and M. HeM  the usual sense is only applicable for the parameter & but will fail for the q, Q param-  eters since the integrand is maximal at the non-differentiability point of the absolute  value function. We, therefore, expand q (q, Q, &) up to linear terms (.2 (q - ) and  integrate piece-wise.  I l the following saddle  Using the abbreviation niv := j ptrue {yj [xi } log Os(,)  point approximation for the integral over  is obtained:  1 Z- Z k _ exp(-ia)  7 =  _ ,=lPi"l" with Pi - ,exp(_iu). (8)  The entropy  evaluated at q = q yields in combination with the Laplace approxi-  mation [1] an estimate for the cdinality of the 7-cover  1  logjam[ = n(logk-)+Zi,pnipPip(ZPini-nip) (9)  where the second term results from the second order term of the Taylor expansion  ound the saddle point. serting this complexity in equation (2) yields an equation  which determines the required number of samples l0 for a fixed precision e d  confidence . This equation defines & function relationship between the precision  e and the approximation quity 7 for fixed sample size l0 and confidence . Under  this sumption the precision e depends on 7 in a non-monotone fhion, i.e.  e = a  + 2loC + + rC , (10)  using the abbreviation C = log Avl + log . The minimum of the function e (7)  defines a compromise between uncertainty originating from empirical fluctuations  and the loss of precision due to the approximation by a 7-cover. Differentiating  with respect to 7 d setting the result to zero (de(f)/d 7 = 0) yields  upper  bound for the inverse temperature:  1 /0 ( lo+Cr2 ) -   -< a* 2n r + x/21oC + r2C 2 (11)  Analogous to estimates of k-means, phase-transitions occur in ACM while lowering  the temperature. The mixture model for the data at hand can be partitioned  into more and more components, revealing finer and finer details of the generation  process. The critical &opt defines the resolution limit below which details can not  be resolved in a reliable fashion on the basis of the sample size 10.  Given the inverse temperature & the effective cardinality of the hypothesis class  can be upper bounded via the solution of the fix point equation (8). On the other  hand this cardinality defines with (11) and the sample size 10 an upper bound on &.  Iterating these two steps we finally obtain an upper bound for the critical inverse  temperature given a sample size 10.  Empirical Results:  For the evaluation of the derived theoretical result a series of Monte-Carlo exper-  iments on artificial data has been performed for the asymmetric clustering model.  Given the number of objects n = 30, the number of groups k = 5 and the size of the  histograms f = 15 the generative model for this experiments was created randomly  and is summarized in fig. 1. From this generative model sample sets of arbitrary  size can be generated and the true distributions ptrue {yjlxi } can be calculated.  In figure 2a,b the predicted temperatures are compared to the empirically observed  critical temperatures, which have been estimated on the basis of 2000 different sam-  ples of randomly generated co-occurrence data for each 10. The expected risk (solid)  Model Selection in Clustering by Uniform Convergence Bounds 221  ' qJl'  I {0.11, 0.01, 0.11, 0.07, 0.08, 0.04, 0.06, O, 0.13, 0.07, 0.08, 0.1, O, 0.11, 0.03}  2 {0.18, 0.1, 0.09, 0.02, 0.05, 0.09, 0.08, 0.03, 0.06, 0.07, 0.03, 0.02, 0.07, 0.06, 0.05}  3 {0.17, 0.05, 0.05, 0.06, 0.06, 0.05, 0.03, 0.11, 0.09, 0, 0.02, 0.1, 0.03, 0.07, 0.11}  {0.15, 0.07, 0.1, 0.03, 0.09, 0.03, 0.04, 0.05, 0.06, 0.05, 0.08, 0.04, 0.08, 0.09, 0.04}  5 {0.09, 0.09, 0.07, 0.1, 0.07, 0.06, 0.06, 0.11, 0.07, 0.07, 0.1, 0.02, 0.07, 0.02, 0)  re(i) ---- (5,3,2,5,2,2,5,4,2,2,2,4,1,5,3,5,3,4,1,2,2,3,1,1,2,5,5,2,2,1)  Figure 1: Generatire ACM model for the Monte-Carlo experiments.  and empirical risk (dashed) of these 2000 inferred models are averaged. Overfitting  sets in when the expected risk rises as a function of the inverse temperature :.  Figure 2c indicates that on average the minimal expected risk is assumed when the  effective number is smaller than or equal 5, i.e. the number of clusters of the true  generatire model. Predicting the right computational temperature, therefore, also  enables the data analyst to solve the cluster validation problem for the asymmetric  clustering model. Especially for 10 = 800 the sample fluctuations do not permit the  estimate of five clusters and the minimal computational temperature prevents such  an inference result. On the other hand for l0 = 1600 and 10 = 2000 the minimal  temperature prevents the algorithm to infer too many clusters, which would be an  instance of overfitting.  As an interesting point one should note that for an infinite number of observations  the critical inverse temperature reaches a finite positive value and not more than  the five effective clusters are extracted. At this point we conclude, that for the case  of histogram clustering the Empirical Risk Approximation solves for realizable rules  the problem of model validation, i.e. choosing the right number of clusters.  Figure 2d summarizes predictions of the critical temperature on the basis of the  empirical distribution ij rather than the true distribution ptrue (xi, yj). The em-  pirical distribution has been generated by a training sample set with  of eq. (11)  being used as a plug-in estimator. The histogram depicts the predicted inverse  temperature for 10 = 1200. The average of these plug-in estimators is equal to  the predicted temperature for the true distribution. The estimates of & are biased  towards too small inverse temperatures due to correlations between the parameter  estimates and the stopping criterion. It is still an open question and focus of ongo-  ing work to rigorously bound the variance of this plug-in estimator.  Empirically we observe a reduction of the variance of the expected risk occurring  at the predicted temperature for higher sample sizes 10.  4 Conclusions  The two conditions that the empirical risk has to uniformly converge towards the  expected risk and that all loss functions within an 27PP-range of the global empirical  risk minimum have to be considered in the inference process limits the complexity  of the underlying hypothesis class for a given number of samples. The maximum  entropy method which has been widely employed in deterministic annealing proce-  dures for optimization problems is substantiated by our analysis. Solutions with  too many clusters clearly overfit the data and do not generalize. The condition that  the hypothesis class should only be divided in function balls of size q, forces us to  stop the stochastic search at the lower bound of the computational temperature.  Another important result of this investigation is the fact that choosing the right  stopping temperature for the annealing process not only avoids over fitting but also  solves the cluster validation problem in the realizable case of ACM. A possible  inference of too many clusters using the empirical risk functional is suppressed.  222 J. M. Buhmann and M. Held  8O  72  a) ' ,' 1:800exp  .. Io800emp  / .... Io1200emP i  78 // Io1200emp i  76 -- ? /  %  70  68 I I I I I  5 10 15 20 26 30 35  inverse temperature [  80 b) ' ' I !  Io1600exp' '  I I -- Io16P  I I Io2000emp  78 io2OOOem p .  72  68   0 5 10 15 20 25 30  inverse temperature [  10 i ...... .---  786  9 :: , ,? ,.-   i  i ,-' 7648   7 i  ," .'  . i ' ..  ' 6 ! . : 7436  ,i/.'z _ o=SOO  0 6 12 18 24 30  inveme temperature   35  :).25  :).2  :).15  :).05  [1] N. G. De Bruijn. Asymptotic Methods in Analysis. North-Holland Publishing Co.,  (repr. Dover), Amsterdam, 1958, (1981).  [2] J. M. Buhmann. Empirical risk approximation. Technical Report IAI-TR 98-3, Institut  fiir Informatik III, Universitit Bonn, 1998.  [3] D. Haussler, M. Kearns, and R. Schapire. Bounds on the sample complexity of Bayesian  learning using information theory and the VC dimension. Machine Learning, 14(1):83-  113, 1994.  [4] D. Haussler, M. Kearns, H.S. Seung, and N. Tishby. Rigorous learning curve bounds  from statistical mechanics. Machine Learning, 25:195-236, 1997.  [5] D. Haussler and M. Opper. Mutual information, metric entropy and cumulative relative  entropy risk. Annals of Statistics, December 1996.  [6] T. Hofmann, J. Puzicha, and M.I. Jordan. Learning from dyadic data. In M. S. Kearns,  S. A. Solla, and D. A. Cobh, editors, Advances in Neural Information Processing Sys-  tems 11. MIT Press, 1999. to appear.  [7] H. S. Seung, H. Sompolinsky, and N. Tishby. Statistical mechanics of learning from  examples. Physical Review A, 45(8):6056-6091, April 1992.  [8] A. W. van der Vaart and J. A. Wellner. Weak Convergence and Empirical Processes.  Springer-Verlag, New York, Berlin, Heidelberg, 1996.  [9] V. N. Vapnik. Statistical Learning Theory. Wiley-Interscience, New York, 1998.  References  Figure 2: Comparison between the theoretically derived upper bound on  and  the observed critical temperatures (minimum of the expected risk rs.  curve).  Depicted are the plots for 10 = 800, 1200, 1600, 2000. Vertical lines indicate the  predicted critical temperatures. The average effective number of clusters is drawn  in part c. In part d the distribution of the plug-in estimates is shown for 10 -- 1200.  
Effective Learning Requires Neuronal  Remodeling of Hebbian Synapses  Gal Chechik Isaac Meilijson Eytan Ruppin  School of Mathematical Sciences  Tel-Aviv University Tel Aviv, Israel  ggal@math.tau.ac.il isaco@math.tau.ac.il ruppin@math.tau.ac.il  Abstract  This paper revisits the classical neuroscience paradigm of Hebbian  learning. We find that a necessary requirement for effective as-  sociative memory learning is that the efcacies of the incoming  synapses should be uncorrelated. This requirement is difficult to  achieve in a robust manner by Hebbian synaptic learning, since it  depends on network level information. Effective learning can yet be  obtained by a neuronal process that maintains a zero sum of the in-  coming synaptic efcacies. This normalization drastically improves  the memory capacity of associative networks, from an essentially  bounded capacity to one that linearly scales with the network's size.  It also enables the effective storage of patterns with heterogeneous  coding levels in a single network. Such neuronal normalization can  be successfully carried out by activity-dependent homeostasis of the  neuron's synaptic efcacies, which was recently observed in cortical  tissue. Thus, our findings strongly suggest that effective associa-  tive learning with Hebbian synapses alone is biologically implausi-  ble and that Hebbian synapses must be continuously remodeled by  neuronally-driven regulatory processes in the brain.  I Introduction  Synapse-specific changes in synaptic efcacies, carried out by long-term potentiation  (LTP) and depression (LTD) are thought to underlie cortical self-organization and  learning in the brain. In accordance with the Hebbian paradigm, LTP and LTD  modify synaptic efcacies as a function of the firing of pre and post synaptic neurons.  This paper revisits the Hebbian paradigm showing that synaptic learning alone  cannot provide effective associative learning in a biologically plausible  manner, and must be complemented with neuronally-driven synaptic  remodeling.  The importance of neuronally driven normalization processes has already been  demonstrated in the context of self-organization of cortical maps [1, 2] and in con-  tinuous unsupervised learning as in principal-component-analysis networks [3]. In  these scenarios normalization is necessary to prevent the excessive growth of synap-  Effective Learning Requires Neuronal Remodeling of Hebbian Synapses 97  tic efficacies that occurs when learning and neuronal activity are strongly coupled.  In contradistinction, this paper focuses on associative memory learning where this  excessive synaptic runaway growth is mild [4], and shows that even in this simple  learning paradigm, normalization processes are essential. Moreover, while numer-  ous normalization procedures can prevent synaptic runaway, our analysis shows that  only a specific neuronally-driven correction procedure that preserves the total sum  of synaptic efiqcacies leads to effective associative memory storage.  2 Effective Synaptic Learning rules  We study the computational aspects of associative learning in a model of a low-  activity associative memory network with binary firing {0, 1} neurons. M uncorre-  t M  lated memory patterns { }u= with coding level p (fraction of firing neurons) are  stored in an N neurons network. The ith neuron updates its state X/t at time t by  xt+l _ O(f[) f 1 V 1 + sign(f) (1)  , - , = w, - r, o(f) = 2 '  j=l  where fi is its input field (postsynaptic potential) and T is its firing thresh-  old. The synaptic weight Wij between the jth (presynaptic) and ith (postsy-  naptic) neurons is determined by a general additive synaptic learning rule de-  pending on the neurons' activity in each of the M stored memory patterns "  presynaptic (j)  Wij = Z A(?,?) , A(i,j) = postsynaptic(i) 1 c  , (2)  where the synaptic learning matrix A(/", ?) governs the incremental modifications  to a synapse as a function of the firing of the presynaptic (column) and postsynaptic  (row) neurons. In conventional biological terms, c denotes an increment following a  long-term potentiation (LTP) event,  denotes heterosynaptic long-term depression  (LTD), and ' a homosynaptic LTD event.  The parameters a, , % 5 define a four dimensional space in which all linear additive  Hebbian learning rules reside. However, in order to visualize this space, one may  represent these Hebbian learning rules in a reduced, two-dimensional space utilizing  a scaling invariance constraint and the requirement that the synaptic matrix should  have a zero mean (otherwise the synaptic values diverge, the noise overshadows the  signal term and no retrieval is possible [5]).  Figure 1A plots the memory capacity of the network as a function of the two free  parameters a and . It reveals that considerable memory storage may be obtained  only along an essentially one dimensional curve, naturally raising the possibility  of identifying an additional constraint on the relations between (a, , % 5). Such  a constraint is revealed by a signal-to-noise analysis of the neuronal input field fi  during retrieval  Signal  Noise  E(fi[i = 1) - E(fi[i = O)  v/Var(fi) v/Var [Wij] + NpCOV [Wij, Wik]  + VpOV '  (a)  98 G. Chechik, I. Meilijson and E. Ruppin  where averages are taken over the ensemble of stored memory patterns.  A. So  500  400 ----.  ':300  400 " ';" ::   Ee 200 [  200 ' ' lO E  o ha   - _; ' ' o.o'  o o beta  beta  Figure 1: A. Memory capacity of a 1000-neurons network with p = 0.05 for different  values of a and/3 as obtained in computer simulations. Capacity is defined as the  maximal number of memories that can be retrieved with overlap bigger than 0.95  when presented with a degraded input cue with overlap 0.8. The overlap serves  N   to measure retrieval acuity and is defined as mn 1 -d=l() -p)Xj B.  p(1-p)N '  Memory capacity of the effective learning rules: The peak values on the ridge of  Figure A, are displayed by tracing their projection on the 3 coordinate. The optimal  learning rule A(i,j) = (i -P)(j -P) [5], marked with an arrow, performs only  slightly better than other effective learning rules.  As evident from equation (3) and already pointed out by [6], when the postsynap-  tic covariance COV [A(i, j), A(i, )] (determining the covariance between the  incoming synapses of the postsynaptic neuron) is positive, the network's memory  capacity is bounded, i.e., it does not scale with the network size. As the postsy-  naptic covariance is non negative, effective learning rules that obtain linear scaling  of memory capacity as a function of the network's size require a vanishing post-  synaptic covariance. Intuitively, when the synaptic weights are correlated, adding  any new synapse contributes only little new information, thus limiting the number  of beneficial synapses that help the neuron estimate whether it should fire or not.  Figure lB depicts the memory capacity of the effective synaptic learning rules which  lie on the essentially one-dimensional ridge observed in Figure 1A. It shows that all  these effective rules are only slightly inferior to the optimal synaptic learning rule  calculated previously by [5, 6], which maximizes memory capacity.  The vanishing covariance constraint on effective learning rules implies a new re-  quirement concerning the balance between synaptic depression and facilitation:  /3 = -_--p a. Thus, effective memory storage requires a delicate balance between  LTP (xp) and heterosynaptic depression (/3), and is strongly dependent on the cod-  ing level p which is a global property of the network. It is thus difficult to see how  effective rules can be implemented at the synaptic level. Moreover, as shown in  Figure 1A, Hebbian learning rules lack robustness as small perturbations from the  effective rules may result in large decrease in memory capacity.  Effective Learning Requires Neuronal Remodeling of Hebbian Synapses 99  Furthermore, these problems cannot be circumvented by introducing a nonlinear  Hebbian learning rule of the form Wij = g '"n(i, ?) as even for a nonlinear  function g the covariance Cov [g('"nA(, ?)), g('"nA(, ))] remains positive  if Cov(A(i,j), A(i,))is positive. These observations show that effective  associative learning with Hebbian rules alone is implausible from a bio-  logical standpoint requiring locality of information.  3 Effective Learning via Neuronal Weight Correction  The above results show that in order to obtain effective memory storage, the post-  synaptic covariance must be kept negligible. How then may effective storage  take place in the brain with Hebbian learning? We now proceed to show that  a neuronally-driven procedure (essentially similar to that assumed by [2, 1] to occur  during self-organization) can maintain a vanishing covariance and turn ineffective  Hebbian synapses into effective ones. This enables the brain to utilize ineglcient  learning rules which use local information only, but still attain effective learning  capabilities.  The solution emerges when rewriting the signal-to-noise equation (Eq. 3) as  Noise  Signal N  c (4)  /NVar[Wij] (1 - p) + pVar(.jN__l Wij)  showing that the post synaptic covariance can be greatly diminished when the  variance of the sum of incoming synapses is vanishing. We thus propose the following  neuronal weight correction procedure: During learning, whenever a synapse is  modified, its postsynaptic neuron additively modifies all its synapses to maintain  the sum of their eglcacies at a baseline zero level.  N  Wij .'. Wij N Z Wij ; Vj = 1..N (5)  j----1  As this neuronal weight correction is additive, it can be performed either after  several memories have been stored (as done in prescriptive learning), or during the  storge of each memory pattern (as in developmental learning models).  Interestingly, the joint operation of weight correction over a linear Hebbian learning  rule is equivalent to the storage of the same set of memory patterns with another  Hebbian learning rule. We prove that this new rule has a zero-covariance learning  matrix  1 I o I  (a-)(1-p) I (a - )(O - p) I  (7-5)(1 -p) (7-5)(0-p)  It should be reemphasized that the matrix on the right is not applied at the synaptic  level but is the emergent result of the operation of the neuronal mechanism on the  matrix on the left, and is used here as a mathematical tool to analyze network's  performance. Thus, using a neuronal mechanism that maintains the sum of incom-  ing synapses fixed enables the same level of effective performance as would have  been achieved by using a zero-covariance Hebbian learning rule, but without the  need to know the memories' coding level.  100 G. Chechik, I. Meilijson and E. Ruppin  To demonstrate the beneficiary effects of neuronal weight correction we have first  applied it to a common realization of the Hebb rule with inhibition added to obtain a  zero-mean input field (otherwise the capacity vanishes) yielding A(i, j) =  [7]. Even though this learning rule has a zero mean synaptic matrix, its postsynaptic  covariance is non-zero and is thus still an ineffective rule. Applying neuronal weight  correction after learning with the above rule, results in a synaptic matrix which is  identical to the one generated by the rule A(i, j) = i(j -P) without neuronal  weight correction, which has both zero mean and zero postsynaptic covariance.  Figure 2A plots the memory capacity obtained with the zero mean Hebb rule,  before and after neuronal weight correction, as a function of the network's size.  After applying neuronal weight correction the originally bounded capacity turns to  scale linearly with the network's size.  A. Ineffective learning rule  B. Variable coding level  1500  03 1000  o  E  ) 500  E  1500  SO0 1000 1500 2000  network's size (N)  500  optimal learning rule  + neuronal correction  i-1  0 0 500 1000 1500 2000  network's size (N)  Figure 2: Network memory capacity as a function of network's size. A. While  the original zero-mean learning rule has bounded memory capacity, the capacity  becomes linear in the network's size when the same learning rule is coupled with  neuronal-weight-correction. The lines plot analytical results and the squares desig-  nate simulation results (p -- 0.05). B. Even the optimal learning rule becomes inef-  fective when the stored patterns have variable coding levels (coding levels are nor-  mally distributed N(0.1, 0.022), but neuronal-weight-correction provides successful  memory storage of such patterns. Results were obtained in a computer simulations.  As the effectiveness of the learning rule depends on the coding level of the stored  patterns, all learning rules turn ineffective when the coding levels of the stored  patterns are heterogeneous. Figure 2B compares the memory capacity of a network  that uses the optimal learning rule (A(i, j) - (i -p)(j -p)) for a coding level of  p - 0.1 but actually stores memory patterns with coding levels that are normally  distributed around 0.1. Only the application of neuronal weight correction provides  effective storage of such patterns while the optimal learning rule does not.  4 Neuronal Regulation Implements Weight Correction  Like previous normalization procedures, the proposed neuronal algorithm relies on  the availability of explicit information about the total sum of synaptic efficacies at  the neuronal level. However, as explicit information on the synaptic sum may not  be available, several mechanisms for conservation of the total synaptic strength have  been proposed (see [8] for a review). Here we focus on one such mechanism, Neu-  tonal Regulation (NR), where the total synaptic sum is regulated indirectly  by estimating the neuron's average postsynaptic potential. NR is a slow  process, continuously modifying synaptic efficacies to maintain the homeostasis of  Effective Learning Requires Neuronal Remodeling of Hebbian Synapses 1 O1  neuronal activity. Such activity-dependent scaling of excitatory synapses, which  acts to maintain the homeostasis of neuronal firing, has already been observed in  cortical tissues by [9].  We have studied the operation of NR-driven correction compared with additive  neuronal weight correction in an excitatory-inhibitory network. Figure 3 plots the  memory capacity of networks storing memories according to the Hebb rule, show-  ing how NR approximates the additive neuronal weight correction and succeeds in  obtaining a linear growth of memory capacity.  1200  >,1000  . 800  , 600  o  l::: 400  200  e--eOriginal Hebb rule .=   = = Neuronal Weight Correction J :  500 1000 1500 2000 2500 3000  network size  Figure 3. Applying NR achieves a linear  scaling of memory capacity with a slightly  inferior capacity compared with that ob-  tained with neuronal weight correction.  Memory patterns were stored according to  the Hebb rule Wij -M= , ,  5 Summary  In this paper we have analyzed Hebbian learning rules in associative memory net-  work models, and identified an essential requirement for effective memory storage:  a vanishing postsynaptic covariance. We show that this constraint depends on the  coding level of the stored memory patterns, thus requiring the use of network level  information at the synaptic level. Moreover, when the stored memory patterns are  heterogeneous, there is no single learning rule that can effectively store all patterns.  We further show that applying a neuronally driven mechanism that preserves the  total synaptic sum zeroes the catastrophic covariance and provides effective learn-  ing even for ineffective synaptic learning rules. The resulting improvement in  memory capacity is drastic: learning rules yielding bounded capacity  are transformed into learning rules yielding linear memory capacity as  a function of the network's size. Finally, the normalization mechanism can  be carried out by neuronal regulation (NR), a mechanism recently identified in  mammalian cortical cultures.  The characterization of effective synaptic learning rules reopens the discussion of  the computational role of heterosynaptic and homosynaptic depression. Previous  studies have shown that long-term synaptic depression is necessary to prevent sat-  uration of synaptic values [10], and to maintain zero mean synaptic efflcacies [11].  Our study shows that effective learning requires proper heterosynaptic depression,  but can be obtained regardless of the homosynaptic depression magnitude. The  terms potentiation/depression used in the above context should be cautiously in-  terpreted, as the apparent changes in synaptic efficacies measured in LTD/LTP  experiments may involve two kinds of processes: Synaptic-driven processes, chang-  ing synapses according to the covariance between pre and post synaptic neurons,  and neuronally-driven processes, operating to zero the covariance between incom-  ing synapses of the neuron. These processes may be experimentally segregated as  they operate on different time scales ([12, 9]), and their relative weights can be  experimentally tested.  102 G. Chechik, I. Meilijson and E. Ruppin  While several forms of synaptic constraints were suggested to improve the stability  of Hebbian learning [2, 3], our analysis shows that effective memory storage requires  that the sum of synaptic strengths which must be preserved, thus predicting that  it is this specific form of normalization that occurs in the brain. The utilization of  the simple McCullough-Pitts model studied here has enabled us to gain analytical  insight to the phenomena in hand. Recent findings of neuronal weight normalization  in spiking models [13], lead us to believe that these results will also extent to spiking  neurons' networks.  Neuronal weight correction qualitatively improves the ability of a neuron to cor-  rectly discriminate between a large number of input patterns. It thus enhances  the computational power of the single neuron and is likely to play a fundamental  computational role in a variety of brain functions such as perceptual processing and  associative learning.  References  [10]  [11]  [12]  [1] K.D. Miller and D.J.C MacKay. The role of constraints in Hebbian learning.  Neural Computation, 6(1):100-126, 1994.  [2] C. von der Malsburg. Self organization of orientation sensitive cells in the  striate cortex. Kybernetik, 14:85-100, 1973.  [3] Erkki Oja. A simplified neuron model as a principal component analyzer.  Journal of Mathematical Biology, 15:267-273, 1982.  [4] A. Grinstein Massica and E. Ruppin. Synaptic runaway in associative networks  and the pathogenesis of schizophrenia. Neural Computation, 10:451-465, 1998.  [5] P. Dayan and D.J. Willshaw. Optimizing synaptic learning rules in linear  associative memories. Biol. Cyber., 65:253, 1991.  [6] G. Palm and F. Sommer. Associative data storage and retrielval in neural net-  works. In E. Domani, J.L. vanHemmen, and eds. K. Schulten, editors, Models  of Neural Networks III. Association, Generalization and Represantation, pages  79-118. Springer, 1996.  [7] M.V. Tsodyks. Associative memory in neural networks with Hebbian learning  rule. Modern Physics letters, 3(7):555-560, 1989.  [8] K.D. Miller. Synaptic economics: Competition and cooperation in synaptic  plasticity. Neuron, 17:371-374, 1996.  [9] G.G. Turrigano, K. Leslie, N. Desai, and S.B. Nelson. Activity depen-  dent scaling of quantal amplitude in neocoritcal pyramidal neurons. Nature,  391 (6670):892-896, 1998.  T.J. Sejnowski. Statistical constraints on synaptic plasticity. J. Theo. Biol.,  69:385-389, 1977.  D.J. Willshaw and P. Dayan. Optimal plasticity from matrix memories: What  goes up must come down. Neural Computation, 2(1):85-93, 1990.  M.F. Bear and W.C. Abraham. Long term depression in hippocampus. Annu.  Rev. Neurosci., 19:437-462, 1996.  R. Kempter, W. Gerstner, and J.L. van Hemmen. Hebbian learning and spiking  neurons. Phys. Rev. E., 59(4), 1999.  
Robust Learning of Chaotic Attractors  Rembrandt Bakker*  Chemical Reactor Engineering  Delft Univ. of Technology  r. bakke r @ strn. tude l fi. nl  Jaap C. Schouten  Chemical Reactor Engineering  Eindhoven Univ. of Technology  J. C. Schouten @ rue. nl  Marc-Olivier Coppens  Chemical Reactor Engineering  Delft Univ. of Technology  coppens @ stm. tudelfi. nl  Floris Takens  Dept. Mathematics  University of Groningen  F. Takens @ math. rug. nl  C. Lee Giles  NEC Research Institute  Princeton NJ  giles @research. nj. nec. corn  Cor M. van den B!eek  Chemical Reactor Engineering  Delft Univ. of Technology  vdbleek@ stm. tudelfi. nl  Abstract  A fundamental problem with the modeling of chaotic time series data is that  minimizing short-term prediction errors does not guarantee a match  between the reconstructed attractors of model and experiments. We  introduce a modeling paradigm that simultaneously learns to short-term  predict and to locate the outlines of the attractor by a new way of nonlinear  principal component analysis. Closed-loop predictions are constrained to  stay within these outlines, to prevent divergence from the attractor. Learning  is exceptionally fast: parameter estimation for the 1000 sample laser data  from the 1991 Santa Fe time series competition took less than a minute on  a 166 MHz Pentium PC.  1 Introduction  We focus on the following objective: given a set of experimental data and the assumption that  it was produced by a deterministic chaotic system, find a set of model equations that will  produce a time-series with identical chaotic characteristics, having the same chaotic attractor.  The common approach consists of two steps: (1) identify a model that makes accurate short-  term predictions; and (2) generate a long time-series with the model and compare the  nonlinear-dynamic characteristics of this time-series with the original, measured time-series.  Principe et al. [1] found that in many cases the model can make good short-term predictions  but does not team the chaotic attractor. The method would be greatly improved if we could  minimize directly the difference between the reconstructed attractors of the model-generated  and measured data, instead of minimizing prediction errors. However, we cannot reconstruct  the attractor without first having a prediction model. Until now research has focused on how  to optimize both step 1 and step 2. For example, it is important to optimize the prediction  horizon of the model [2] and to reduce complexity as much as possible. This way it was  possible to team the attractor of the benchmark laser time series data from the 1991 Santa Fe  *DelftChemTech, Chemical Reactor Engineering Lab, Julianalaan 136, 2628 BL, Delft, The  Netherlands; http://www.cpt. stm.tudelft.nl/cpt/cre/research/bakker/.  880 R. Bakker, d.C. Schouten, M.-O. Coppens, E Takens, C. L. Giles and C. M. v. d. Bleek  time series competition. While training a neural network for this problem, we noticed [3] that  the attractor of the model fluctuated from a good match to a complete mismatch from one  iteration to another. We were able to circumvent this problem by selecting exactly that model  that matches the attractor. However, after carrying out more simulations we found that what  we neglected as an unfortunate phenomenon [3] is really a fundamental limitation of current  approaches.  An important development is the work of Principe et al. [4] who use Kohonen Self Organizing  Maps (SOMs) to create a discrete representation of the state space of the system. This creates  a partitioning of the input space that becomes an infrastructure for local (linear) model  construction. This partitioning enables to verify if the model input is near the original data (i.e.,  detect if the model is not extrapolating) without keeping the training data set with the model.  We propose a different partitioning of the input space that can be used to (i) learn the outlines  of the chaotic attractor by means of a new way of nonlinear Principal Component Analysis  (PCA), and (ii) enforce the model never to predict outside these outlines. The nonlinear PCA  algorithm is inspired by the work of Kambhatla and Leen [5] on local PCA: they partition the  input space and perform local PCA in each region. Unfortunately, this introduces  discontinuities between neighboring regions. We resolve them by introducing a hierarchical  partitioning algorithm that uses fuzzy boundaries between the regions. This partitioning closely  resembles the hierarchical mixtures of experts of Jordan and Jacobs [6].  In Sec. 2 we put forward the fundamental problem that arises when trying to learn a chaotic  attractor by creating a short-term prediction model. In Sec. 3 we describe the proposed  partitioning algorithm. In Sec. 4 it is outlined how this partitioning can be used to learn the  outline of the attractor by defining a potential that measures the distance to the attractor. In Sec.  5 we show modeling results on a toy example, the logistic map, and on a more serious  problem, the laser data from the 1991 Santa Fe time series competition. Section 6 concludes.  2 The attractor learning dilemma  Imagine an experimental system with a chaotic attractor, and a time-series of noise-free  measurements taken from this system. The data is used to fit the parameters of the model  fit.  = F(fft, 't- ,'-', t-m) where F is a nonlinear function,  contains its adjustable parameters  and m is a positive constant. What happens if we fit the parameters  by nonlinear least  squares regression? Will the model be stable, i.e., will the closed-loop long term prediction  converge to the same attractor as the one represented by the measurements?  Figure 1 shows the result of a test by Diks et al. [7] that compares the difference between the  model and measured attractor. The figure shows that while the neural network is trained to  predict chaotic data, the model quickly  converges to the measured attractor  (S=0), but once in a while, from one  iteration to another, the match between  the attractors is lost.  To understand what causes this  instability, imagine that we try to fit the  parameters of a model t. = d + B 't  while the real system has a point  attractor, = , where  is the state of  the system and  its attracting value.  Clearly, measurements taken from this  system contain no information to  20'  ?'n  0 8oo0  training progress [co Rerati0ns]  Figure 1: Diks test monitoring curve for a neural  network model trained on data from an  experimental chaotic pendulum [3].  Robust Learning of Chaotic Attractors 881  estimate both d and B. If we fit the model parameters with non-robust linear least squares, B  may be assigned any value and if its largest eigenvalue happens to be greater than zero, the  model will be unstable!  For the linear model this problem has been solved a long time ago with the introduction of  singular value decomposition. There still is a need for a nonlinear counterpart of this technique,  in particular since we have to work with very flexible models that are designed to fit a wide  variety of nonlinear shapes, see for example the early work of Lapedes and Farber [8]. It is  akeady common practice to control the complexity of nonlinear models by pruning or  regularization. Unfortunately, these methods do not always solve the attractor learning  problem, since there is a good chance that a nonlinear term explains a lot of variance in one  part of the state space, while it causes instability of the attractor (without affecting the one-step-  ahead prediction accuracy) elsewhere. In Secs. 3 and 4 we will introduce a new method for  nonlinear principal component analysis that will detect and prevent unstable behavior.  3. The split and fit algorithm  The nonlinear regression procedure of this section will form the basis of the nonlinear principal  component algorithm in Sec. 4. It consists of (i) a partitioning of the input space, (ii) a local  linear model for each region, and (iii) fuzzy boundaries between regions to ensure global  smoothness. The partitioning scheme is outlined in Procedure 1:  Procedure 1: Partitioning the input space  1) Start with the entire set Z of input data  2) Determine the direction of largest variance of Z: perform a singular value  decomposition of Z into the product UZ-'V r and take the eigenvector (column  of V) with the largest singular value (on the diagonal of Z-').  3) Split the data in two subsets (to be called: clusters) by creating a plane  perpendicular to the direction of largest variance, through the center of  gravity of Z.  4) Next, select the cluster with the largest sum squared error to be split next,  and recursively apply 2-4 until a stopping criteria is met.  Figures 2 and 3 show examples of the partitioning. The disadvantage of dividing regression  problems into localized subproblems was pointed out by Jordan and Jacobs [6]: the spread of  the data in each region will be much smaller than the spread of the data as a whole, and this  will increase the variance of the model parameters. Since we always split perpendicular to the  direction of maximum variance, this problem is minimized.  The partitioning can be written as a binary tree, with each non-terminal node being a split and  each terminal node a cluster. Procedure 2 creates fuzzy boundaries between the clusters.  Procedure 2. Creating fuzzy boundaries  1) An input ' enters the tree at the top of the partfioning tree.  2) The Euclidean distance to the splitting hyperplane is divided by the  bandwidth fl of the split, and passed through a sigmoidal function with range  [0,1]. This results in 's share erin the subset on 's side of the splitting  plane. The share in the other subset is 1-a.  3) The previous step is carded out for all non-terminal nodes of the tree.  882 R. Bakker, J. C. Schouten, M.-O. Coppens, E Takens, C. L. Giles and C. M. v. d. Bleek  4) The membership #c of ff to subset (terminal node) c is computed by  taking the product of all previously computed shares o along the path from  the terminal node to the top of the tree.  If we would make all parameters adjustable, that is (i) the orientation of the splitting  hyperplanes, (ii) the bandwidths r, and (iii) the local linear model parameters, the above model  structure would be identical to the hierarchical mixtures of experts of Jordan and Jacobs [6].  However, we akeady fixed the hyperplanes and use Procedure 3 to compute the bandwidths:  Procedure 3. Computing the Bandwidths  1) The bandwidths of the terminal nodes are taken to be a constant (we use 1.65,  the 90% confidence limit of a normal distribution) times the variance of the  subset before it was last split, in the direction of the eigenvector of that last split.  2) The other bandwidths do depend on the input . They are computed by  climbing upward in the tree. The bandwidth of node n is computed as a  weighted sum between the rs of its right and left child, by the implicit formula  fin=or. fit. % fiR, in which at. and o R depend on fin- Starting from initial guess  fin=fit. if %>0.5, or else fin=fiR, the formula is solved in a few iterations.  This procedure is designed to create large overlap between neighboring regions and almost no  overlap between non-neighboring regions. What remains to be fitted is the set of the local  linear models. Thej-th output of the split&fit model for a given input ', is computed:  c  C C C  jo =  [tp {ajzp +bj }, where Cand b c contain the linear model parameters of subset c,  c=l  and C is the number of clusters. We can determine the parameters of all local linear models in  one global fit that is linear in the parameters. However, we prefer to locally optimize the  parameters for two reasons: (i) it makes it possible to locally control the stability of the  attractor and do the principal component analysis of Sec. 4; and (ii) the computing time for a  linear regression problem with r regressors scales -O(r3). If we would adopt global fitting, r  would scale linearly with C and, while growing the model, the regression problem would  quickly become intractable. We use the following iterative local fitting procedure instead.  Procedure 4. Iterative Local Fitting  1) Initialize a J by N matrix of residuals R to zero, J being the number of  outputs and N the number of data.  2) For cluster c, if an estimate for its linear model parameters akeady exists,  for each input vector z add gjo to the matrix of residuals, otherwise add  gy,,to R, yj.p being thej-th element of the desired output vector for sample  p.  3) Least squares fit the linear model parameters of cluster c to predict the  current residuals R, and subtract the (new) estimate, g,,,, from R.  4) Do 2-4 for each cluster and repeat the fitting several times (default: 3).  From simulations we found that the above fast optimization method converges to the global  minimum if it is repeated many times. Just as with neural network training, it is often better to  use early stopping when the prediction error on an independent test set starts to increase.  Robust Learning of Chaotic Attractors 883  4. Nonlinear Principal Component Analysis  To learn a chaotic attractor from a single experimental time-series we use the method of delays:  the state E consists of m delays taken from the time series. The embedding dimension m must  be chosen large enough to ensure that it contains sufficient information for faithful  reconstruction of the chaotic attractor, see Takens [9]. Typically, this results in an m-  dimensional state space with all the measurents covering only a much lower dimensional, but  non-linearly shaped, subspace. This creates the danger pointed out in Sec. 2: the stability of the  model in directions perpendicular to this low dimensional subspace cannot be guaranteed.  With the split & fit algorithm from Sec. 3 we can learn the non-linear shape of the low  dimensional subspace, and, if the state of the system escapes from this subspace, we use the  algorithm to redirect the state to the nearest point on the subspace. See Malthouse [10] for  limitations of existing nonlinear PCA approaches. To obtain the low dimensional subspace, we  proceed according to Procedure 5.  Procedure 5. Learning the Low-dimensional Subspace  1) Augment the output of the model with the m-dimensional state ': the  model will learn to predict its own input.  2) In each cluster c, perform a singular value decomposition to create a set of  m principal directions, sorted in order of decreasing explained variance. The  result of this decomposition is also used in step 3 of Procedure 4.  3) Allow the local linear model of each cluster to use no more than mre a of  these principal directions.  4) Define a potential P to be the squared Euclidian distance between the  state Z and its prediction by the model.  The potential P implicitly defines the  lower dimensional subspace: if a state   is on the subspace, P will be zero.  P will increase with the distance of   from the subspace. The model has  learned to predict its own input with  small error, meaning that it has tried  to reduce P as much as possible at  exactly those points in state space  where the training data was sampled.  In other words, P will be low if the  input  is close to one of the original  points in the training data set. From  the split&fit algorithm we can  analytically compute the gradient  dP/d. Since the evaluation of the  split&fit model involves a backward  (computing the bandwidths) and  forward pass (computing  memberships), the gradient algorithm  involves a forward and backward  pass through the tree. The gradient is  used to project states that are off the  nonlinear subspace onto the subspace  -1  o o o  -2 -1 0 I 2  Figure 2. Projecting two-dimensional data on a one-  dimensional self-intersecting subspace. The  colorscale represents the potential P, white indicates  884 R. Bakker, J. C. Schouten, M.-O. Coppens, E Takens, C. L. Giles and C. M. v. d. Bleek  in one or a few Newton-Rhapson iterations.  Figure 2 illustrates the algorithm for the  problem of creating a one-dimensional  representation of the number '8'. The  training set consists of 136 clean samples,  and Fig. 2 shows how a set of 272 noisy  inputs is projected by a 48 subset split&fit  model onto the one-dimensional subspace.  Note that the center of the '8' cannot be well  represented by a one4imensional space. We  leave development of an algorithm that  automatically detects the optimum local  subspace dimension for future research.  5. Application Examples  -1  -1 o 1  X  Figure 3. Learning the attractor of the two-  input logistic map. The order of creation of the  splits is indicated. The colorscale represents the  potential P, white indicates P>0.05.  First we show the nonlinear principal  component analysis result for a toy  example, the logistic map zt. 1 =4Zt(1-Zt). If we use a model zt. 1 =F(zt), where the  prediction only depends on one previous output, there is no lower dimensional space to which  the attractor is confined. However, if we allow the output to depend on more than a single  delay, we create a possibility for unstable behavior. Figure 3 shows how well the split&fit  algorithm learns the one-dimensional shape of the attractor after creating only five regions. The  parabola is slightly deformed (seen from the white lines perpendicular to the attractor), but this  may be solved by increasing the number of splits.  Next we look at the laser data. The complex behavior of chaotic systems is caused by an  interplay of destabilizing and stabilizing forces: the destabilizing forces make nearby points in  state space diverge, while the stabilizing forces keep the state of the system bounded. This  process, known as 'stretching and folding', results in the attractor of the system: the set of  points that the state of the system will visit after all transients have died out. In the case of the  laser data this behavior is clear cut: destabilizing forces make the signal grow exponentially  until the increasing amplitude triggers a collapse that reinitiates the sequence. We have seen  in neural network based models [3] and in this study that it is very hard for the models to cope  with the sudden collapses. Without the nonlinear subspace correction of Sec. 4, most of the  0.4  o  1o  o 3000  1000 3000  time  Figure 4. Laser data from the Santa Fe time series competition. The 1000 sample train  data set is followed by iterated prediction of the model (a). After every prediction a  correction is made to keep P (see Sec. 4) small. Plot (b) shows P before this correction.  Robust Learning of Chaotic Attractors 885  models we tested grow without bounds after one or more rise and collapse sequences. That is  not very surprising - the training data set contains only three examples of a collapse. Figure 4  shows how this is solved with the subspace correction: every time the model is about to grow  to infinity, a high potential P is detected (depicted in Fig. 3b) and the state of the system is  directed to the nearest point on the subspace as learned from the nonlinear principal component  analysis. After some trial and error, we selected an embedding dimension m of 12 and a  reduced dimension mrea of 4. The split&fit model starts with a single dataset, and was grown  until 48 subsets. At that point, the error on the 1000 sample train set was still decreasing  rapidly but the error on an independent 1000 sample test set increased. We compared the  reconstructed attractors of the model and measurements, using 9000 samples of closed-loop  generated and 9000 samples of measured data. No significant difference between the two  could be detected by the Diks test [7].  6. Conclusions  We present an algorithm that robustly models chaotic attractors. It simultaneously learns (1)  to make accurate short term predictions; and (2) the outlines of the attractor. In closed-loop  prediction mode, the state of the system is corrected after every prediction, to stay within these  outlines. The algorithm is very fast, since the main computation is to least squares fit a set of  local linear models. In our implementation the largest matrix to be stored is N by C, N being  the number of data and C the number of clusters. We see many applications other than attractor  learning: the split&fit algorithm can be used as a fast learning alternative to neural networks  and the new form of nonlinear PCA will be useful for data reduction and object recognition.  We envisage to apply the technique to a wide range of applications, from the control and  modeling of chaos in fluid dynamics to problems in finance and biology to fluid dynamics.  Acknowledgements  This work is supported by the Netherlands Foundation for Chemical Research (SON) with financial  aid from the Netherlands Organization for Scientific Research (NWO).  References  [1] J.C. Principe, A. Rathie, and J.M. Kuo, "Prediction of Chaotic Time Series with Neural Networks  and the Issue of Dynamic Modeling", Int. J. Bifurcation and Chaos, 2, 1992, p 989.  [2] J.M. Kuo, and J.C. Principe, "Reconstructed Dynamics and Chaotic Signal Modeling", In Proc.  IEEE Int'l Conf. Neural Networks, 5, 1994, p 3131.  [3] R. Bakker, J.C. Schouten, C.L. Giles, F. Takens, C.M. van den Bleek, "Learning Chaotic  Attractors by Neural Networks", submitted.  [4] J.C. Principe, L. Wang, M.A. Motter, "Local Dynamic Modeling with Self-Organizing Maps and  Applications to Nonlinear System Identification and Control",Proc. IEEE, 86(11), 1998.  [5] N. Kambhatla, T.K. Leen, "Dimension Reduction by Local PCA", Neural Computation, 9, 1997,  p. 1493  [6] M.I. Jordan, R.A. Jacobs, "Hierarchical Mixtures of Experts and the EM Algorithm", Neural  Compution, 6, 1994, p. 181.  [7] C. Diks, W.R. van Zwet, F. Takens, and J. de Goede, "Detecting differences between delay vector  distributions", Physical Review E, 53, 1996, p. 2169.  [8] A. La Pedes, R. Farber, "Nonlinear Signal Processing Using Neural Networks: Prediction and  System Modelling", Los Alamos Technical Report LA-UR-87-2662.  [9] F. Takens, "Detecting strange attractors in turbulence", Lecture notes in Mathematics,  898, 1981, p. 365.  [10] E.C. Malthouse, "Limitations of Nonlinear PCA as performed with Generic Neural Networks,  IEEE Trans. Neural Networks, 9(1), 1998, p. 165.  
An Improved Decomposition Algorithm  for Regression Support Vector Machines  Pavel Laskov  Department of Computer and Information Sciences  University of Delaware  Newark, DE 19718  laskovasel. udel. edu  Abstract  A new decomposition algorithm for training regression Support  Vector Machines (SVM) is presented. The algorithm builds on  the basic principles of decomposition proposed by Osuna et. al.,  and addresses the issue of optimal working set selection. The new  criteria for testing optimality of a working set are derived. Based  on these criteria, the principle of "maximal inconsistency" is pro-  posed to form (approximately) optimal working sets. Experimental  results show superior performance of the new algorithm in compar-  ison with traditional training of regression SVM without decompo-  sition. Similar results have been previously reported on decomposi-  tion algorithms for pattern recognition SVM. The new algorithm is  also applicable to advanced SVM formulations based on regression,  such as density estimation and integral equation SVM.  I Introduction  The increasing interest in applications of Support Vector Machines (SVM) to large-  scale problems ushers in new requirements for computational complexity of their  training algorithms. Requests have been recently made for algorithms capable of  handling problems containing 10 5 - 10 6 examples [1]. Training an SVM constitutes  a quadratic programming problem, and a typical SVM package uses an off-the-shelf  optimization software to obtain a solution to it. The number of variables in the  optimization problem is equal to the number of training data points (for the pattern  recognition SVM) or twice that number (for the regression SVM). The speed of  general-purpose optimization methods is insufficient for problems containling more  than a few thousand examples. This has motivated a quest for special-purpose  training algorithms to take advantage of the particular structure of SVM training  problems.  The main avenue of research in SVM training algorithms is decomposition. The key  idea of decomposition, due to Osuna et. al. [2], is to freeze all but a small number of  optimization variables, and to solve a sequence of small fixed-size problems. The set  of variables whose values are optimized at a current iteration is called the working  set. Complexity of re-optimizing the working set is assumed to be constant-time.  An Improved Decomposition Algorithm for Regression Support Vector Machines 485  In order for a decomposition algorithm to be successful, the working set must be  selected in a smart way. The fastest known decomposition algorithm is due to  Joachims [3]. It is based on Zoutendijk's method of feasible directions proposed in  the optimization community in the early 1960's. However Joachims' algorithm is  limited to pattern recognition SVM because it makes use of labels being 4-1. The  current article presents a similar algorithm for the regression SVM.  The new algorithm utilizes a slightly different background from optimization the-  ory. The Karush-Kuhn-Tucker Theorem is used to derive conditions for determining  whether or not a given working set is optimal. These conditions become the algo-  rithm's termination criteria, as an alternative to Osuna's criteria (also used by  Joachims without modification) which used conditions for individual points. The  advantage of the new conditions is that knowledge of the hyperplane's constant  factor b, which in some cases is difficult to compute, is not required. Further inves-  tigation of the new termination conditions allows to form the strategy for selecting  an optimal working set. The new algorithm is applicable to the pattern recognition  SVM, and is provably equivalent to Joachims' algorithm. One can also interpret  the new algorithm in the sense of the method of feasible directions. Experimental  results presented in the last section demonstrate superior performance of the new  method in comparison with traditional training of regression SVM.  2 General Principles of Regression SVM Decomposition  The original decomposition algorithm proposed for the pattern recognition SVM in  [2] has been extended to the regression SVM in [4]. For the sake of completeness  I will repeat the main steps of this extension with the aim of providing terse and  streamlined notation to lay the ground for working set selection.  Given the training data of size l, training of the regression SVM amounts to solving  the following quadratic programming problem in 21 variables:  Maximize W(&) = T&_ I&TD&  subject to: c T& = 0 (1)  &-C1 < 0  & > 0  where  c ' = -y el ' D= -K K ' c= -1  The basic idea of decomposition is to split the variable vector  into the working set  B of fixed size q and the non-working set N containing the rest of the variables.  The corresponding parts of vectors c and r will also bear subscripts N and B. The  matrix D is partitioned into DBB, DBN = DB and DvN. A further requirement  is that, for the i-th element of the training data, both ci and c i are either included  in or omitted from the working set. 1 The values of the variables in the non-working  set are frozen for the iteration, and optimization is only performed with respect to  the variables in the working set.  Optimization of the working set is also a quadratic program. This can be seen  by re-arranging the terms of the objective function and the equality constraint in  This rule facilitates formulation of sub-problems to be solved at each iteration.  486 P. Laskov  (1) and dropping the terms independent of &B from the objective.  quadratic program (sub-problem) is formulated as follows:  The resulting  I T  Maximize WB(&B) : ( -- &vDNB)&B -- &BDBB&B  subject to: c&B +c&N = 0 (2)  &s - C1 < 0  &  0  The basic decomposition algorithm chooses the first working set at random, and  proceeds iteratively by selecting sub-optimal working sets and re-optimizing them,  by solving quadratic program (2), until all subsets of size q are optimal. The precise  formulation of termination conditions will be developed in the following section.  3 Optimality of a Working Set  In order to maintain strict improvement of the objective function, the working  set must be sub-optimal before re-optimization. The classical Karush-Kuhn-Tucker  (KKT) conditions are necessary and sufficient for optimality of a quadratic program.  I will use these conditions applied to the standard form of a quadratic program, as  described in [5], p. 36.  The standard form of a quadratic program requires that all constraints are of equal-  ity type except for non-negativity constraints. To cast the reression SVM quadratic  program (1) into the standard form, the slack variables s ' - (s,... ,s2t) corre-  sponding to the box constraints, and the following matrices are introduced:  I = z= f=  E= 0 ' ' '  (s)  where I is a vector of length l, C is a vector of length 21. The zero element in vector  z reflects the fact that a slack variable for the equality constraint must be zero. In  the matrix notation all constraints of problem (1) can be compactly expressed as:  Tz : f  z > 0 (4)  In this notation the Karush-Kuhn-Tucker Theorem can be stated as follows:  Theorem 1 (Karush-Kuhn-Tucker Theorem) The primal vector z solves the  quadratic problem (1) if and only if it satisfies () and there exists a dual vector  ur= (Ii r w r) = (Ii r (p r)) sch that:  II = D&+Ew- _> 0 (5)  T _> 0 (6)  urz = 0 (7)  It follows from the Karush-Kuhn-Tucker Theorem that if for all u satisfying con-  ditions (6) - (7) the system of inequalities (5) is inconsistent then the solution of  problem (1) is not optimal. Since the objective function of sub-problem (2) was  obtained by merely re-arranging terms in the objective function of the initial prob-  lem (1), the same conditions guarantee that the sub-problem (2) is not optimal.  Thus, the main strategy for identifying sub-optimal working sets will be to enforce  inconsistency of the system (5) while satisfying conditions (6) - (7).  An Improved Decomposition Algorithm for Regression Support Vector Machines 487  Let us further analyze inequalities in (5). Each inequality has one of the following  forms:  where   = -c+e+v+t _Y 0 (8)  7r = ci + e - vi - la _ 0 (9)  l  cfii = Yi - E(ctj -ctj)Kij  j=l  Consider the values cti can possible take:  ctl = 0. In this case si = C, and, by complementarity condition (7), vi = O.  Then inequality (8) becomes:  r  = - c  + e + t _ 0 = ta _ c i - e  2. cti = C. By complementarity condition (7), ri = 0. Then inequality (8)  becomes:  -c)i + e + tz + v = 0  tz _< c)i - e  3. 0 < cti < C. By complementarity condition (7), vi = 0, *ri = 0.  inequality (8) becomes:  Then  Similar reasoning for ct**. and inequality (9) yields the following results:  1. ct i = 0. Then  2. ct i = C. Then  3. 0<ct i <C. Then  tz = c)i +e  As one can see, the only free variable in system (5) is/. Each inequality restricts  / to a certain interval on a real line. Such intervals will be denoted as la-sets in  the rest of the exposition. Any subset of inequalities in (5) is inconsistent if the  intersection of the corresponding/-sets is empty. This provides a lucid rule for  determining optimality of any working set: it is sub-optimal if the intersection of  /-sets of all its points is empty. A sub-optimal working set will also be denoted as  "inconsistent". The following summarizes the rules for calculation of/-sets, taking  into account that for regression SVM cic i = 0:  .A/li --  [i -- , i q- ],  +  ifcti=0, ct i =0  if0<cti<C, ct i =0  ifcti=C, ai-0  if cti = 0, 0 < ct i < C  ifcti=0, ct i =C  (lo)  488 P. Laskov  4 Maximal Inconsistency Algorithm  While inconsistency of the working set at each iteration guarantees convergence of  decomposition, the rate of convergence is quite slow if arbitrary inconsistent working  sets are chosen. A natural heuristic is to select "maximally inconsistent" working  sets, in a hope that such choice would provide the greatest improvement of the  objective function. The notion of "maximal inconsistency" is easy to define: let it  be the gap between the smallest right boundary and the largest left boundary of  /-sets of elements in the training set:  G=L-R  L = max Iti,  0<i<l  R = min /z'  0<i</  where/ti,/' are the left and the right boundaries respectively (possibly minus or  plus infinity) of the/-set li. It is convenient to require that the largest possible  inconsistency gap be maintained between all pairs of points comprising the working  set. The obvious implementation of such strategy is to select q/2 elements with the  largest values of/t and q/2 elements with the smallest values of/r. The maximal  inconsistency strategy is summarized in Algorithm 1.  Algorithm 1 Maximal inconsistency SVM decomposition algorithm.  Let $ be the list of all samples.  while (L > R)   compute li according to the rules (10) for all elements in $   select q/2 elements with the largest values of/t (,,left pass")   select q/2 elements with the smallest values of/r ("right pass")   re-optimize the working set  Although the motivation provided for the maximal inconsistency algorithm is purely  heuristic, the algorithm can be rigorously derived, in a similar fashion as Joachims'  algorithm, from Zoutendijk's feasible direction problem. Details of such derivation  cannot be presented here due to space constraints. Because of this relationship I  will further refer to both algorithms as "feasible direction" algorithms.  5 Experimental Results  Experimental evaluation of the new algorithm was performed on the mod-  ified KDD Cup 1998 data set. The original data set is available under  http://www. ics.uci. edu/-kdd/databases/kddcup98/kddcup98. html. The following  modifications were made to obtain a pure regression problem:   All 75 character fields were eliminated.   Numeric fields CONTROLN, ODATEDW, TCODE and DOB were elimi-  tated.  The remaining 400 features and the labels were scaled between 0 and 1. Initial  subsets of the training database of different sizes were selected for evaluation of the  scaling properties of the new algorithm. The training times of the algorithms, with  and without decomposition, the numbers of support vectors, including bounded  support vectors, and the experimental scaling factors, are displayed in Table 1.  An Improved Decomposition Algorithm for Regression Support Vector Machines 489  Table 1: Training time (sec) and number of SVs for the KDD Cup problem  Examples no dcmp dcmp total SV BSV  500 39 10 274 0  1000 226 41 518 3  2000 1490 158 970 5  3000 5744 397 1429 7  5000 27052 1252 2349 15  scaling factor:  SV-scaling factor:  2.84 2.08  3.06 2.24  Table 2: Training time (sec) and number of SVs for the KDD Cup problem, reduced  Examples no dcmp dcmp total SV BSV  feature space.  500 56 18 170 30  1000 346 44 374 62  2000 1768 198 510 144  3000 4789 366 729 222  5000 22115 863 1139 354  scaling factor: 2.55 1.72  SV-scaling factor: 3.55 2.35  The experimental scaling factors are obtained by fitting lines to log-log plots of the  running times against sample sizes, in the number of examples and the number  of unbounded support vectors respectively. Experiments were run on SGI Octane  with 195MHz clock and 256M RAM. RBF kernel with "/= 10, C = 1, termination  accuracy 0.001, working set size of 20, and cache size of 5000 samples were used.  A similar experiment was performed on a reduced feature set consisting of the first  50 features selected from the full-size data set. This experiment illustrates the  behavior of the algorithms when the large number of support vectors are bounded.  The results are presented in Table 2.  6 Discussion  It comes at no surprise that the decomposition algorithm outperforms the conven-  tional training algorithm by an order of magnitude. Similar results have been well  established for pattern recognition SVM. Remarkable is the co-incidence of scaling  factors of the maximal inconsistency algorithm and Joachims' algorithm: his scaling  factors range from 1.7 to 2.1 [3]. I believe however, that a more important perfor-  mance measure is SV-scaling factor, and the results above suggest that this factor  is consistent even for problems with significantly different compositions of support  vectors. Further experiments should investigate properties of this measure.  Finally, I would like to mention other methods proposed in order to speed-up train-  ing of SVM, although no experimental results have been reported for these methods  with regard to training of the regression SVM. Chunking [6], p. 366, iterates through  490 P Laskov  the training data accumulating support vectors and adding a "chunk" of new data  until no more changes to a solution occur. The main problem with this method is  that when the percentage of support vectors is high it essentially solves the problem  of almost the same size more than once. Sequential Minimal Optimization (SMO),  proposed by Platt [7] and easily extendable to the regression SVM [1], employs an  idea similar to decomposition but always uses the working set of size 2. For such  a working set, a solution can be calculated "by hand" without numerical optimiza-  tion. A number of heuristics is applied in order to choose a good working set. It  is difficult to draw a comparison between the working set selection mechanisms of  SMO and the feasible direction algorithms but experimental results of Joachims [3]  suggest that SMO is slower. Another advantage of feasible direction algorithms is  that the size of the working set is not limited to 2, as in SMO. Practical experience  shows that the optimal size of the working set is between 10 and 100. Lastly, tradi-  tional optimization methods, such as Newton's or conjugate gradient methods, can  be modified to yield the complexity of O(s3), where s is the number of detected  support vectors [8]. This can be a considerable improvement over the methods that  have complexity of O(/3), where 1 is the total number of training samples.  The real challenge lies in attaining sub-O(s 3) complexity. While the experimen-  tal results suggest that feasible direction algorithms might attain such complexity,  their complexity is not fully understood from the theoretical point of view. More  specifically, the convergence rate, and its dependence on the number of support  vectors, needs to be analyzed. This will be the main direction of the future research  in feasible direction SVM training algorithms.  References  [1] Smola, A., Sch51kopf, B. (1998) A Tutorial on Support Vector Regression.  NeuroCOLT2 Technical Report NC2-TR-1998-030.  [2] Osuna, E., Freund, R., Girosi, F. (1997) An Improved Training Algorithm for  Support Vector Machines. Proceedings of IEEE NNSP'97. Amelia Island FL.  [3] Joachims, T. (1998) Making Large-Scale SVM Learning Practical. Advances in  Kernel Methods - Support Vector Learning. B. SchSlkopf, C. Burges, A. Smola,  (eds.) MIT-Press.  [4] Osuna, E. (1998) Support Vector Machines: Training and Applications. Ph.D.  Dissertation. Operations Research Center, MIT.  [5] Boot, J. (1964) Quadratic Programming. Algorithms - Anomalies - Applica-  tions. North Holland Publishing Company, Amsterdam.  [6] Vapnik, V. (1982) Estimation of Dependencies Based on Empirical Data.  Springer-Verlag.  [7] Platt, J. (1998) Fast Training of Support Vector Machines Using Sequential  Minimal Optimization. Advances in Kernel Methods - Support Vector Learning.  B. SchSlkopf, C. Burges, A. Smola, (eds.) MIT-Press.  [8] Kaufman, L. (1998) Solving the Quadratic Programming Problem Arising in  SupportVector Classification. Advances in Kernel Methods - Support Vector  Learning. B. SchSlkopf, C. Burges, A. Smola, (eds.) MIT-Press.  
Boosting with Multi-Way Branching in  Decision Trees  Yishay Mansour  David McAllester  AT&T Labs-Research  180 Park Ave  Florham Park NJ 07932  {mansour, dmac}@research.att.com  Abstract  It is known that decision tree learning can be viewed as a form  of boosting. However, existing boosting theorems for decision tree  learning allow only binary-branching trees and the generalization to  multi-branching trees is not immediate. Practical decision tree al-  gorithms, such as CART and C4.5, implement a trade-off between  the number of branches and the improvement in tree quality as  measured by an index function. Here we give a boosting justifica-  tion for a particular quantitative trade-off curve. Our main theorem  states, in essence, that if we require an improvement proportional  to the log of the number of branches then top-down greedy con-  struction of decision trees remains an effective boosting algorithm.  I Introduction  Decision trees have been proved to be a very popular tool in experimental machine  learning. Their popularity stems from two basic features -- they can be constructed  quickly and they seem to achieve low error rates in practice. In some cases the  time required for tree growth scales linearly with the sample size. Efficient tree  construction allows for very large data sets. On the other hand, although there  are known theoretical handicaps of the decision tree representations, it seem that  in practice they achieve accuracy which is comparable to other learning paradigms  such as neural networks.  While decision tree learning algorithms are popular in practice it seems hard to  quantify their success in a theoretical model. It is fairly easy to see that even  if the target function can be described using a small decision tree, tree learning  algorithms may fail to find a good approximation. Kearns and. Mansour [6] used  the weak learning hypothesis to show that standard tree learning algorithms perform  boosting. This provides a theoretical justification for decision tree learning similar  Boosting with Multi-Way Branching in Decision Trees 301  to justifications that have been given for various other boosting algorithms, such as  AdaBoost [4].  Most decision tree learning algorithms use a top-down growth process. Given a  current tree the algorithm selects some leaf node and extends it to an internal node  by assigning to it some "branching function" and adding a leaf to each possible  output value of this branching function. The set of branching functions may differ  from one algorithm to another, but most algorithms used in practice try to keep the  set of branching functions fairly simple. For example, in C4.5 [7], each branching  function depends on a single attribute. For categorical attributes, the branching  is according to the attribute's value, while for continuous attributes it performs a  comparison of the attribute with some constant.  Of course such top-down tree growth can over-fit the data -- it is easy to construct  a (large) tree whose error rate on the training data is zero. However, if the class of  splitting functions has finite VC dimension then it is possible to prove that, with  high confidence of the choice of the training data, for all trees T the true error rate  of T is bounded by (T) -{- O (I) where (T) is the error rate of T on the  training sample, ITI is the number of leaves of T, and rn is the size of the training  sample. Over-fitting can be avoided by requiring that top-down tree growth produce  a small tree. In practice this is usually done by constructing a large tree and then  pruning away some of its nodes. Here we take a slightly different approach. We  assume a given target tree size s and consider the problem of constructing a tree T  with ITI = s and (T) as small as possible. We can avoid over-fitting by selecting a  small target value for the tree size.  A fundamental question in top-down tree growth is how to select the branching  function when growing a given leaf. We can think of the target size as a "budget".  A four-way branch spends more of the tree size budget than does a two-way branch  -- a four-way branch increases the tree size by roughly the same amount as two two-  way branches. A sufficiently large branch would spend the entire tree size budget in  a single step. Branches that spend more of the tree size budget should be required to  achieve more progress than branches spending less of the budget. Naively, one would  expect that the improvement should be required to be roughly linear in the number  of new leaves introduced -- one should get a return proportional to the expense.  However, a weak learning assumption and a target tree size define a nontrivial  game between the learner and an adversary. The learner makes moves by selecting  branching functions and the adversary makes moves by presenting options consistent  with the weak learning hypothesis. We prove here that the learner achieve a better  value in this game by selecting branches that get a return considerably smaller than  the naive linear return. Our main theorem states, in essence, that the return need  only be proportional to the log of the number of branches.  2 Preliminaries  We assume a set Y of instances and an unknown target function f mapping Y  to {0, 1}. We assume a given "training set" S which is a set of pairs of the form  (x, f(x)). We let 7/be a set of potential branching functions where each h 6 7/is  a function from A' to a finite set Rh -- we allow different functions in 7/ to have  different ranges. We require that for any h 6 7/ we have I Rhl >_ 2. An 7/-tree is  302 Y. Mansour and D. McAllester  a tree where each internal node is labeled with an branching function h 6 7/ and  has children corresponding to the elements of the set Rh. We define [T[ to be the  number of leaf nodes of T. We let L(T) be the set of leaf nodes of T. For a given tree  T, leaf node  of T and sample $ we write St to denote the subset of the sample $  reaching leaf . For  6 T we define/dr to be the fraction of the sample reaching leaf  , i.e., [St[/[S[. We define t to be the fraction of the pairs (x, fix)) in St for which  f(x) = 1. The training error of T, denoted e(T), is -teL(,)ldt min(t, 1 - t).  3 The Weak Learning Hypothesis and Boosting  Here, as in [6], we view top-down decision tree learning as a form of Boosting [8, 3].  Boosting describes a general class of iterative algorithms based on a weak learning  hypothesis. The classical weak learning hypothesis applies to classes of Boolean  functions. Let 7/2 be the subset of branching functions h 6 7/ with [Rh[ = 2. For  d > 0 the classical d-weak learning hypothesis for 7/2 states that for any distribution  on A' there exists an h 6 7/2 with Prt(h(x)k fix)) _ 1/2-d. Algorithms designed  to exploit this particular hypothesis for classes of Boolean functions have proved to  be quite useful in practice [5].  Kearns and Mansour show [6] that the key to using the weak learning hypothesis  for decision tree learning is the use of an index function I: [0, 1] -+ [0, 1] where  I(q) <_ 1, I(q) _> min(q,(1- q)) and where I(T)is defined to be  Note that these conditions imply that g(T) <_ I(T). For any sample W let qw be  the fraction of pairs (x, f(x))  W such that f(x) = 1. For any h 6 7/ let Ta be  the decision tree consisting of a single internal node with branching function h plus  a leaf for each member of IRal. Let Iw(Ta) denote the value of I(Ta) as measured  with respect to the sample W. Let A(W, h) denote I(qw)- Iw(Th). The quantity  A(W, h) is the reduction in the index for sample W achieved by introducing a single  branch. Also note that/dtA($t, h) is the reduction in I(T) when the leaf is replaced  by the branch h. Kearns and Mansour [6] prove the following lemma.  Lemma 3.1 (Kearns & Mansour) Assuming the d-weak learning hypothesis for  7t2, and taking I(q) to be 2v/q(1 - q), we have that for any sample W there exists  an h  7t2 such that A (W, h) _> Y6 I(qw).  This lemma motivates the following definition.  Definition I We say that 7t2 and I satisfies the y-weak tree-growth hypothesis if  for any sample W from 2( there exists an h  7t2 such that A(W, h) >_ 7I(qw).  Lemma 3.1 states, in essence, that the classical weak learning hypothesis implies the  weak tree growth hypothesis for the index function I(q) = 2v/q(1 - q). Empirically,  however, the weak tree growth hypothesis seems to hold for a variety of index  functions that were already used for tree growth prior to the work of Kearns and  Mansour. The Ginni index I(q) = 4q(1- q) is used in CART [1] and the entropy  I(q) = -qlog q-(1- q)log(1- q)is used in C4.5 [7]. It has long been empirically  observed that it is possible to make steady progress in reducing I(T) for these  choices of I while it is difficult to make steady progress in reducing g(T).  We now define a simple binary branching procedure. For a given training set $  and target tree size s this algorithm grows a tree with ITI = s. In the algorithm  Boosting with Multi-Way Branching in Decision Trees 303  0 denotes the trivial tree whose root is a leaf node and Tt,h denotes the result of  replacing the leaf  with the branching function h and a new leaf for each element  of Rh.  WHILE (ITI < s) DO   - argmax ]()  h - argmax7t  A($, h)  T - T,;  END-WHILE  n-1 n-1 _ff  We now define e(n) to be the quantity ]-Ii= (1- ). Note that e(n) <_ I-Ii= e =  e-'7 Zia__"l 1 1/i < e_, 7 Inn ____ n-3' '  Theorem 3.2 (Kearns & Mansour) If Yt2 and I satisfy the y-weak tree growth  hypothesis then the binary branching procedure produces a tree T with g(T) _ I(T) _  e(lTI) _<  Proof: The proof is by induction on the number of iterations of the procedure.  We have that I(0) _ 1 = e(1) so the initial tree immediately satisfies the condi-  tion. We now assume that the condition is satisfied by T at the begining of an  iteration and prove that it remains satisfied by Tt,a at the end of the iteration.  Since I(T) = "]-tT 15I() we have that the leaf  selected by the procedure is  such that StI(qt) _ !. By the y-weak tree growth assumption the function  h selected by the procedure has the property that A(St, h) _ yI(t). We now  have that I(T) - I(Tt,a) = ItA(St, h) _ btyI(qt) _ y--. This implies that  - = - - )e(ITI) = e(ll + =  4 Statement of the Main Theorem  We now construct a tree-growth algorithm that selects multi-way branching func-  tions. As with many weak learning hypotheses, the y-weak tree-growth hypothesis  can be viewed as defining a game between the learner and an adversary. Given a  tree T the adversary selects a set of branching functions allowed at each leaf of the  tree subject to the constraint that at each leaf  the adversary must provide a binary  branching function h with A(S/, h) _ yI({t). The learner then selects a leaf  and  a branching function h and replaces T by Tt,n. The adversary then again selects  a new set of options for each leaf subject to the y-weak tree growth hypothesis.  The proof of theorem 3.2 implies that even when the adversary can reassign all op-  tions at every move there exists a learner strategy, the binary branching procedure,  guaranteed to achieves a final error rate of ITI -.  Of course the optimal play for the adversary in this game is to only provide a single  binary option at each leaf. However, in practice the "adversary" will make mistakes  and provide options to the learner which can be exploited to achieve even lower  error rates. Our objective now is to construct a strategy for the learner which can  exploit multi-way branches provided by the adversary.  We first say that a branching function h is acceptable for tree T and target size  304 Y. Mansour and D. McAllester  s if either IRal = 2 or ITI < e(IRaDs7/(21Ra D. We also define g(k) to be the  quantity (1- e(k))/7. It should be noted that g(2) = 1. It should also be noted  that e(k) ~ e -'nk and hence for 71nk small we have e(k) ~ 1 - 71nk and hence  g(k) ~ In k. We now define the following multi-branch tree growth procedure.  T=0  WHILE (IT] < s) DO   4- argmax t /3t I (l)  h 4- argmaxa/' h acceptable for T and s  T 4- Tt,a;  END-WHILE  A run of the multi-branch tree growth procedure will be called 7-boosting if at each  iteration the branching function h selected has the property that A(St, h)/g(IRal) >  7I(qt). The 7-weak tree growth hypothesis implies that a(&,h)/g(lahl) >  7I(qt)/g(2) = 7I(t). Therefore, the 7-weak tree growth hypothesis implies that  every run of the multi-branch growth procedure is ?-bootsing. But a run can be  ?-bootsing by exploiting mutli-way branches even when the ?-weak tree growth  hypothesis fails. The following is the main theorem of this paper.  Theorem 4.1 If T is produced by a 7-boosting run of the multi-branch tree-growth  procedure then I(T) <_ e(lT D < IT[ -'.  5 Proof of Theorem 4.1  To prove the main theorem we need the concept of a visited weighted tree, or VW-  tree for short. A VW-tree is a tree in which each node rn is assigned both a rational  weight w, 6 [0, 1] and an integer visitation count v, > 1. We now define the  following VW tree growth procedure. In the procedure T,o is the tree consisting of  a single root node with weight w and visitation count 1. The tree Tt,,o,...,,ok is the  result of inserting k new leaves below the leaf  where the ith new leaf has weight  wi and new leaves have visitation count 1.  w 4- any rational number in [0, 1]  T 4- T,  FOR ANY NUMBER OF STEPS REPEAT THE FOLLOWING   4- argmaxt e(v),o  vl 4- vl q- 1  OPTIONALLY T 4- Tt,,o,...,,ov WITH w +... wv < e(vt)wt  We first prove an analog of theorem 3.2 for the above procedure. For a VW-tree T  we define ITI to be Y]lsi;(a, vt and we define I(T) to be Y]tsi;(a, e(vt)wt.  Lemma 5.1 The VW procedure maintains the invariant that I(T) < e(ITI).  Proof.' The proof is by induction on the number of iterations of the algorithm.  The result is immediate for the initial tree since e(1) = 1. We now assume that  < e(ITI) at the start of an iteration and show that this remains true at the  end of the iteration.  Boosting with Multi-Way Branching in Decision Trees 305  We can associate each leaf with vt "subleaves" each of weight e(vt)wt/vt. We have  that ITI is the total number of these subleaves and I(T) is the total weight of these  subleaves. Therefore there must exist a subleaf whose weight is at least I(T)/IT I.  Hence there must exist a leaf  satisfying e(vt)wt/vt _ I(T)/ITI. Therefore this  relation must hold of the leaf  selected by the procedure.  Let T t be the tree resulting from incrementing vt. We now have I(T) - I(T t) =  e(vt)wt-e(vt+ 1)wt = e(vt)wt-(1-)e(vt)wt = e(vt)wt _ 7!. So we have  _< _< - )e(ITI) =  Finally, if the procedure grows new leaves we have that the I(T) does not increase  and that IT I remains the same and hence the invariant is maintained. []  For any internal node m in a tree T let C(m) denote the set of nodes which are  children of m. A VW-tree will be called locally-well-formed if for every internal  node m we have that v, = IC(m)l, that Y'.,c(,,)w, _ e(IC(m)])w,,. A VW-tree  will be called globally-safe if maxt(T) e(vt)wt/vt _ minmV(T) e(vt -- 1)wt/(vt-- 1)  where N(T) denotes the set of internal nodes of T.  Lemma 5.2 If T is a locally well-formed and globally safe VW-tree, then T is a  possible output of the VW growth procedure and therefore I(T) _ e(lTI).  Proof: Since T is locally well formed we can use T as a "template" for making  nondeterministic choices in the VW growth procedure. This process is guaranteed  to produce T provided that the growth procedure is never forced to visit a node  corresponding to a leaf of T. But the global safety condition guarantees that any  unfinished internal node of T has a weight as least as large as any leaf node of T.  We now give a way of mapping ']/-trees into VW-trees. More specifically, for any  7/-tree T we define VW(T) to be the result of assigning each node m in T the weight  1m I(qm), each internal node a visitation count equal to its number of children, and  each leaf node a visitation count equal to 1. We now have the following lemmas.  Lemma 5.3 If T is grown by a ?-boosting run of the multi-branch procedure then  VW(T) is locally well-formed.  Proof: Note that the children of an internal node m are derived by selecting  a branching function h for the node m. Since the run is y-boosting we have  /x($t,h)/g(IRhl) >_ ?I(qt). Therefore A($t,h)= (I(t)- Is(Th)) _ I(t)(1-  c(IRl)). This implies that Is(Th) _ Multiplying by/t and trans-  forming the result into weights in the tree VW(T) gives the desired result. []  The following lemma now suffices for theorem 4.1.  Lemma 5.4 If T is grown by a 7-boosting run of the multi-branch procedure then  VW(T) is globally safe.  Proof: First note that the following is an invariant of a v-boosting run of the  multi-branch procedure.  max wt _ min wt  t(vw()) ,,v(vw())  306 E Mansour and D. McAllester  The proof is a simple induction on ?-boosting tree growth using the fact that the  procedure always expands a leaf node of maximal weight.  We must now show that for every internal node m and every leaf  we have that  wl _< e(k- 1)w,/(k- 1) where k is the number of children of to. Note that if k = 2  then this reduces to wl <_ w, which follows from the above invariant. So we can  assume without loss of generality that k  2. Also, since e(k)/k  e(k - 1) / (k - 1),  it suffices to show that wt _< e(k)w,/k.  Let m be an internal node with k  2 children and let T  be the tree at the time  rn was selected for expansion. Let wt be the maximum weight of a leaf in the final  tree T. By the definition of the acceptability condition, in the last s/2 iterations  we are performing only binary branching. Each binary expansion reduces the index  by at least ? times the weight of the selected node. Since the sequence of nodes  selected in the multi-branch procedure has non-increasing weights, we have that in  any iteration the weight of the selected node is at least wt. Since there are at least  s/2 binary expansions after the expansion of m, each of which reduces I by at least  ?w, we have that s?wl/2 _< I(T') so w <_ 2I(T')/(?s). The acceptability condition  can be written as 2/(?s) _< e(k)/(kIT'l) which now yields wl _< I(T)e(k)/(klT'l).  But we have that I(T)/IT'I <_ w, which now yields wl <_ e(k)w,/k as desired. []  References  [1] Leo Breiman, Jerome H. Friedman, Richard A. Olshen, and Charles J. Stone.  Classification and Regression Trees. Wadsworth International Group, 1984.  [2]  Tom Dietterich, Michael Kearns and Yishay Mansour.  Learning Framework to understand and improve C4.5.  Learning, 96-104, 1996.  Applying the Weak  In Proc. of Machine  [3] Yoav Freund. Boosting a weak learning algorithm by majority. Information and  Computation, 121(2):256-285, 1995.  [4]  Yoav Freund and Robert E. Schapire. A decision-theoretic generalization of  on-line learning and an application to boosting. In Computational Learning  Theory: Second European Conference, EuroCOLT '95, pages 23-37. Springer-  Verlag, 1995.  [5]  Yoav Freund and Robert E. Schapire. Experiments with a new boosting al-  gorithm. In Machine Learning: Proceedings of the Thirteenth International  Conference, pages 148-156, 1996.  [6]  Michael Kearns and Yishay Mansour. On the boosting ability of top-down  decision tree learning. In Proceedings of the Twenty-Eighth A CM Symposium  on the Theory of Computing, pages 459-468, 1996.  [7] J. Ross Quinlan. C.5: Programs for Machine Learning. Morgan Kaufmann,  1993.  [8] Robert E. Schapire. The strength of weak learnability. Machine Learning,  5(2):197-227, 1990.  
Neural Representation of Multi-Dimensional  Stimuli  Christian W. Eurich, Stefan D. Wilke and Helmut Schwegler  Institut ffir Theoretische Physik  Universitit Bremen, Germany  (eurich, swilke, schwegler) @physik. uni-bremen.de  Abstract  The encoding accuracy of a population of stochastically spiking neurons  is studied for different distributions of their tuning widths. The situation  of identical radially symmetric receptive fields for all neurons, which  is usually considered in the literature, turns out to be disadvantageous  from an information-theoretic point of view. Both a variability of tun-  ing widths and a fragmentation of the neural population into specialized  subpopulations improve the encoding accuracy.  1 Introduction  The topic of neuronal tuning properties and their functional significance has focused much  attention in the last decades. However, neither empirical findings nor theoretical consider-  ations have yielded a unified picture of optimal neural encoding strategies given a sensory  or motor task. More specifically, the question as to whether narrow tuning or broad tuning  is advantageous for the representation of a set of stimulus features is still being discussed.  Empirically, both situations are encountered: small receptive fields whose diameter is less  than one degree can, for example, be found in the human retina [7], and large receptive  fields up to 180  in diameter occur in the visual system of tongue-projecting salamanders  [10]. On the theoretical side, arguments have been put forward for small [8] as well as for  large [5, 1, 9, 3, 13] receptive fields.  In the last years, several approaches have been made to calculate the encoding accuracy  of a neural population as a function of receptive field size [5, 1, 9, 3, 13]. It has turned  out that for a firing rate coding, large receptive fields are advantageous provided that D >  3 stimulus features are encoded [9, 13]. For binary neurons, large receptive fields are  advantageous also for D = 2 [5, 3].  However, so far only radially symmetric tuning curves have been considered. For neural  populations which lack this symmetry, the situation may be very different. Here we study  the encoding accuracy of a population of stochastically spiking neurons. A Fisher infor-  mation analysis performed on different distributions of tunings widths will indeed reveal a  much more detailed picture of neural encoding strategies.  116 C. W. Eurich, S. D. Willre and H. Schwegler  2 Model  Consider a D-dimensional stimulus space, X. A stimulus is characterized by a position  x = (zi,..., ZD)  X, where the value of feature i, zi (i = 1,..., D), is measured  relative to the total range of values in the i-th dimension such that it is dimensionless.  Information about the stimulus is encoded by a population of N stochastically spiking  neurons. They are assumed to have independent spike generation mechanisms such that the  joint probability distribution for observing n = (nO),..., n(k),..., n(iV)) spikes within a  time interval r, Ps (n; x), can be written in the form  N  ?s(n;x) = II ??)(.(*);x),  k=l  where P?) (n ; x) is the single-neuron probability distribution of the number of observed  spikes given the stimulus at position x. Note that (1) does not exclude a correlation of the  neural firing rates, i.e., the neurons may have common input or even share the same tuning  function.  The firing rates depend on the stimulus via the local values of the tuning functions, such that  P?) (n(k); x) can be written in the form P?)(n(); x) = S (n  , f()(x), r), where the  tuning function of neuron k, f(k) (x), gives its mean firing rate in response to the stimulus  at position x. We assume here a form of the tuning function that is not necessarily radially  symmetric,  where c  = (c?),... ,c?) is the center of the tuning curve of neuron k, a? ) is its  F()2 (tO,2. ()2  tuning width in the i-th dimension, - :- (zi for i - 1, D, and  -ci ) IO'i ...,  (t)2 := ?)2 + ... + )2. F > 0 denotes the maximal firing rate of the neurons, which  requires that maxz>0 b(z) = 1.  We assume that the tuning widths o'? ) a ) of each neuron k are drawn from a distri-  bution P, (Crl,..., CrD). For a population of tuning functions with centers c(1),..., c( , a  N  density r/(x) is introduced according to r/(x) := Ek--1 (x - c ).  The encoding accuracy can be quantified by the Fisher information matrix, J, which is  defined as  Jij(x):=E[(xilnP(n;x)) (lnP(n;x))], (3)  where E[...] denotes the expectation value over the probability distribution P(n; x) [2].  The Fisher information yields a lower bound on the expected error of an unbiased estimator  that retrieves the stimulus x from the noisy neural activity (Cram6r-Rao inequality) [2]. The  minimal estimation error for the i-th feature xi, i,min, is given by e. 2  : (j-1)ii which  ,mln  reduces to e.2 . = 1/Jii(x) if J is diagonal.  ,mln  We shall now derive a general expression for the population Fisher information. In the  next chapter, several cases and their consequences for neural encoding strategies will be  discussed.  For model neuron (k), the Fisher information (3) reduces to  (4)  Neural Representation of Multi-Dimensional Stimuli 117  where the dependence on the tuning widths is indicated by the list of arguments. The  function AO depends on the shape of the tuning function and is given in [13]. The in-  dependence assumption (1) implies that the population Fisher information is the sum of  the contributions of the individual neurons, -N j)(x; a? ) , , a (k)) We now define  k=l _ ' '.' .D ' . .  a population Fisher information which is averaged over the distribution of tuning widths  P,,(a,...  Introducing the density of tuning curves, r/(x), into (5) and assuming a constant distri-  bution, r/(x)   _-- const., one obtains the result that the population Fisher information  becomes independent of x and that the off-diagonal elements of J vanish [ 13]. The average  population Fisher information then becomes  (Jij) =DK(F,r,D) ( Htl at )  2  60, (6)  where K depends on the geometry of the tuning curves and is defined in [ 13].  3 Results  In this section, we consider different distributions of tuning widths in (6) and discuss ad-  vantageous and disadvantageous strategies for obtaining a high representational accuracy  in the neural population.  Radially symmetric tuning curves. For radially symmetric tuning curves of width ,  the tuning-width distribution reads  D  ,0) = II -  i=1  see Fig. 1 a for a schematic visualization of the arrangement of the tuning widths for the  case D = 2. The average population Fisher information (6) for i = j becomes  (Jii) = riDK(F, r, D)D-2, (7)  a result already obtained by Zhang and Sejnowski [ 13]. Equation (7) basically shows that  the minimal estimation error increases with  for D - 1, that it does not depend on  for  D - 2, and that it decreases as  increases for D _> 3. We shall discuss the relevance of  this case below.  Identical tuning curves without radial symmetry. Next we discuss tuning curves which  are identical but not radially symmetric; the tuning-width distribution for this case is  D  = II -  i=1  where  denotes the fixed width in dimension i. For i = j, the average population Fisher  information (6) reduces to [ 11, 4]  -2 (8)  o' i  118 C. W. Eurich, S. D. Vqlke and H. Schwegler  (c)  (a)  ,/  b 1  (b)  (d)  IJ 1 IJ 1  Figure 1: Visualization of different distributions of  tuning widths for D - 2. (a) Radially symmetric tun-  ing curves. The dot indicates a fixed , while the diag-  onal line symbolizes a variation in  discussed in [13].  (b) Identical tuning curves which are not radially sym-  metric. (c) Tuning widths uniformly distributed within  a small rectangle. (d) Two subpopulations each of  which is narrowly tuned in one dimension and broadly  tuned in the other direction.  Equation (8) contains (7) as a special case. From (8) it becomes immediately clear that the  expected minimal square encoding error for the i-th stimulus feature, 2  i,min- 1/(Jii(x))a,  depends on i, i.e., the population specializes in certain features. The error obtained in  dimension i thereby depends on the tuning widths in all dimensions.  Which encoding strategy is optimal for a population whose task it is to encode a single  feature, say feature i, with high accuracy while not caring about the other dimensions? In  order to answer this question, we re-write (8) in terms of receptive field overlap.  For the tuning functions f (k) (x) encountered empirically, large values of the single-neuron  Fisher information (4) are typically restricted to a region around the center of the tuning  function, c . The fraction p(fl) of the Fisher information that falls into a region ED '  V/(k) 2 _< fl around c  is given by  f dZx E/= JJ/)(x)  p() := Er>  f dVx =t J/)(x)  x  f d D+I Acp(2, F T)  0  f d t>+t A(2, F, r)  0  (9)  where the index (k) was dropped because the tuning curves are assumed to have iden-  tical shapes. Equation (9) allows the definition of an effective receptive field, RFr ),  inside of which neuron k conveys a major fraction P0 of Fisher information, RF? :=  {xl V/- ()2 _< 0 }, where 0 is chosen such that P(fio): Po. The Fisher information a  n,()  neuron k carries is small unless x E *- eft  This has the consequence that a fixed stimulus  x is actually encoded only by a subpopulation of neurons. The point x in stimulus space is  covered by  D  2rt>/2()t> H J (10)  Ncode :=. Dr(D/2)  receptive fields. With the help of (10), the average population Fisher information (8) can  be re-written as  D2r( D /2) %ode  (Jii)a = 27rD/2(o) D Kq(F,r,D) -2 (11)  ai  Equation (11) can be interpreted as follows: We assume that the population of neurons  encodes stimulus dimension i accurately, while all other dimensions are of secondary im-  portance. The average population Fisher information for dimension i, (Jii) , is determined  by the tuning width in dimension i, i, and by the size of the active subpopulation, -/code.  There is a tradeoff between these quantities. On the one hand, the encoding error can be  decreased by decreasing i, which enhances the Fisher information carried by each single  Neural Representation of Multi-Dimensional Stimuli 119  neuron. Decreasing i, on the other hand, will also shrink the active subpopulation via  (10). This impairs the encoding accuracy, because the stimulus position is evaluated from  the activity of fewer neurons. If (11) is valid due to a sufficient receptive field overlap,  Ncode can be increased by increasing the tuning widths, j, in all other dimensions j  i.  This effect is illustrated in Fig. 2 for D = 2.  X 2  X2, s  I   Xl,s X 1  X 2  X2, s  Xl,s X 1  Figure 2: Encoding strategy for a stimulus characterized by parameters arl,s and ar2,s. Fea-  ture ar is to be encoded accurately. Effective receptive field shapes are indicated for both  populations. If neurons are narrowly tuned in ar2 (left), the active population (solid) is  small (here: Ncode = 3). Broadly tuned receptive fields for are (right) yield a much larger  population (here: Ncode = 27) thus increasing the encoding accuracy.  It shall be noted that although a narrow tuning width i is advantageous, the limit i > 0  yields a bad representation. For narrowly tuned cells, gaps appear between the receptive  fields: The condition r/(x) _= const. breaks down, and (6) is no longer valid. A more  detailed calculation shows that the encoding error diverges as i > 0 [4]. The fact that  the encoding error decreases for both narrow tuning and broad tuning - due to (11) - proves  the existence of an optimal tuning width. An example is given in Fig. 3a.  0.8  0.6  "0.4  v  0.2  0 0 0.5 1 1.5 2  a [a]  Figure 3: (a) Example for the encoding behavior with narrow tuning curves arranged on  a regular lattice of dimension D = 1 (grid spacing A). Tuning curves are Gaussian, and  neural firing is modeled as a Poisson process. Dots indicate the minimal square encoding  error averaged over a uniform distribution of stimuli, 2  (min), as a function of . The mini-  mum is clearly visible. The dotted line shows the corresponding approximation according  to (8). The inset shows Gaussian tuning curves of optimal width, -opt m 0.4&. (b) gD (A)  as a function of A for different values of D.  120 C. W. Eurich, S. D. Wilke and H. Schwegler  Narrow distribution of tuning curves. In order to study the effects of encoding the  stimulus with distributed tuning widths instead of identical tuning widths as in the previous  cases, we now consider the distribution  Pa(O'l,... ,O'D) ---  (12)  where O denotes the Heaviside step function. Equation (12) describes a uniform distri-  bution in a D-dimensional cuboid of size bl,..., bD around (l,... D); cf. Fig. lc. A  straightforward calculation shows that in this case, the average population Fisher informa-  + co . (13)  tion (6) for i = j becomes  = {  -2 1 +  A comparison with (8) yields the astonishing result that an increase in bi results in an  increase in the i-th diagonal element of the average population Fisher information matrix  and thus in an improvement in the encoding of the i-th stimulus feature, while the encoding  in dimensions j  i is not affected. Correspondingly, the total encoding error can be  decreased by increasing an arbitrary number of edge lengths of the cube. The encoding by  a population with a variability in the tuning curve geometries as described is more precise  than that by a uniform population. This is true for arbitrary D. Zhang and Sejnowski [13]  consider the more artificial situation of a correlated variability of the tuning widths: tuning  curves are always assumed to be radially symmetric. This is indicated by the diagonal  line in Fig. 1 a. A distribution of tuning widths restricted to this subset yields an average  population Fisher information ec (v-2) and does not improve the encoding for D = 2 or  D=3.  Fragmentation into D subpopulations. Finally, we study a family of distributions of  tuning widths which also yields a lower minimal encoding error than the uniform popula-  tion. Let the density of tuning curves be given by  D  i=1  H 6(crj -), (14)  where ,X > 0. For A = 1, the population is uniform as in (7). For A  1, the population  is split up into D subpopulations; in subpopulation i, cr i is modified while crj =  for  j  i. See Fig. ld for an example. The diagonal elements of the average population Fisher  information are  , (15)  where the term in brackets will be  this case because of the symmetry  case (7) differ by gD (/) which will  values of D. For ,X = 1, gD (/)  abbreviated as gi9(,). (Y) does not depend on i in  in the subpopulations. Equation (15) and the uniform  now be discussed. Figure 3b shows gD (/) for different  -- 1 and (7) is recovered as expected. gv() = 1  also holds for/ = 1/(D - 1) < 1: narrowing one tuning width in each subpopulation  will at first decrease the resolution provided D > 3; this is due to the fact that ]Vcode is  decreased. For , < 1/(D - 1), however, gD(,) > 1, and the resolution exceeds (Jii) in  (7) because each neuron in the i-th subpopulation carries a high Fisher information in the  i-th dimension. D - 2 is a special case where no impairment of encoding occurs because  the effect of a decrease of ]Vcode is less pronounced. Interestingly, an increase in , also  yields an improvement in the encoding accuracy. This is a combined effect resulting from  an increase in Node on the one hand and the existence of D subpopulations, D - 1 of  Neural Representation of Multi-Dimensional Stimuli 121  which maintain their tuning widths in each dimension on the other hand. The discussion  of /D (,X) leads to the following encoding strategy. For small ,X, (Jii) increases rapidly,  which suggests a fragmentation of the population into D subpopulations each of which  encodes one feature with high accuracy, i.e., one tuning width in each subpopulation is  small whereas the remaining tuning widths are broad. Like in the case discussed above, the  theoretical limit of this method is a breakdown of the approximation of r/= const. and the  validity of (6) due to insufficient receptive field overlap.  4 Discussion and Outlook  We have discussed the effects of a variation of the tuning widths on the encoding accuracy  obtained by a population of stochastically spiking neurons. The question of an optimal  tuning strategy has turned out to be more complicated than previously assumed. More  specifically, the case which focused most attention in the literature - radially symmetric  receptive fields [5, 1, 9, 3, 13] - yields a worse encoding accuracy than most other cases we  have studied: uniform populations with tuning curves which are not radially symmetric;  distributions of tuning curves around some symmetric or non-symmetric tuning curve; and  the fragmentation of the population into D subpopulations each of which is specialized in  one stimulus feature.  In a next step, the theoretical results will be compared to empirical data on encoding prop-  erties of neural populations. One aspect is the existence of sensory maps which consist  of neural subpopulations with characteristic tuning properties for the features which are  represented. For example, receptive fields of auditory neurons in the midbrain of the barn  owl have elongated shapes [6]. A second aspect concerns the short-term dynamics of re-  ceptive fields. Using single-unit recordings in anaesthetized cats, Wfrgftter et al. [ 12]  observed changes in receptive field size taking place in 50-100ms. Our findings suggest  that these dynamics alter the resolution obtained for the corresponding stimulus features.  The observed effect may therefore realize a mechanism of an adaptable selective signal  processing.  References  [9]  [10]  [11]  [12]  [13]  [1] Baldi, P. & Heiligenberg, W. (1988) Biol. Cybern. 59:313-318.  [2] Deco, G. & Obradovic, D. (1997) An Information-Theoretic Approach to Neural Computing.  New York: Springer.  [3] Eurich, C. W. & Schwegler, H. (1997) Biol. Cybern. 76: 357-363.  [4] Eurich, C. W. & Wilke, S. D. (2000) Neural Comp. (in press).  [5] Hinton, G. E., McClelland, J. L. & Rumelhart, D. E (1986) In Rumelhart, D. E. & McClelland,  J. L. (eds.), Parallel Distributed Processing, Vol. 1, pp. 77-109. Cambridge MA: MIT Press.  [6] Knudsen, E. I. & Konishi, M. (1978) Science 200:795-797.  [7] Kuffier, S. W. (1953) J. Neurophysiol. 16:37-68.  [8] Lettvin, J. Y., Maturana, H. R., McCulloch, W. S. & Pitts, W. H. (1959) Proc. Inst. Radio Eng.  NY 47:1940-1951.  Snippe, H. P. & Koenderink, J. J. (1992) Biol. Cybern. 66:543-551.  Wiggers, W., Roth, G., Eurich, C. W. & Straub, A. (1995) J. Comp. Physiol. A 176:365-377.  Wilke, S. D. & Eurich, C. W. (1999) In Verleysen, M. (ed.), ESANN 99, European Symposium  on Artificial Neural Networks, pp. 435--440. Brussels: D-Facto.  WiSrgiStter, F., Suder, K., Zhao, Y., Kerscher, N., Eysel, U. T. & Funke, K. (1998) Nature  396:165-168.  Zhang, K. & Sejnowski, T. J. (1999) Neural Comp. 11:75-84.  
Probabilistic methods for Support Vector  Machines  Peter Sollich  Department of Mathematics, King's College London  Strand, London WC2R 2LS, U.K. Email: peter.sollich@kcl.ac.uk  Abstract  I describe a framework for interpreting Support Vector Machines  (SVMs) as maximum a posterJori (MAP) solutions to inference  problems with Gaussian Process priors. This can provide intuitive  guidelines for choosing a 'good' SVM kernel. It can also assign  (by evidence maximization) optimal values to parameters such as  the noise level C which cannot be determined unambiguously from  properties of the MAP solution alone (such as cross-validation er-  ror). I illustrate this using a simple approximate expression for the  SVM evidence. Once C has been determined, error bars on SVM  predictions can also be obtained.  I Support Vector Machines: A probabilistic flamework  Support Vector Machines (SVMs) have recently been the subject of intense re-  search activity within the neural networks community; for tutorial introductions  and overviews of recent developments see [1, 2, 3]. One of the open questions that  remains is how to set the 'tunable' parameters of an SVM algorithm: While meth-  ods for choosing the width of the kernel function and the noise parameter C (which  controls how closely the training data are fitted) have been proposed [4, 5] (see  also, very recently, [6]), the effect of the overall shape of the kernel function remains  imperfectly understood [1]. Error bars (class probabilities) for SVM predictions --  important for safety-critical applications, for example -- are also difficult to obtain.  In this paper I suggest that a probabilistic interpretation of SVMs could be used to  tackle these problems. It shows that the SVM kernel defines a prior over functions  on the input space, avoiding the need to think in terms of high-dimensional feature  spaces. It also allows one to define quantities such as the evidence (likelihood) for a  set of hyperparameters (C, kernel amplitude K0 etc). I give a simple approximation  to the evidence which can then be maximized to set such hyperparameters. The  evidence is sensitive to the values of C and K0 individually, in contrast to properties  (such as cross-validation error) of the deterministic solution, which only depends  on the product CKo. It can therefore be used to assign an unambiguous value to  C, from which error bars can be derived.  350 P Sollich  I focus on two-class classification problems. Suppose we are given a set D of n  training examples (xi,yi) with binary outputs Yi = q-1 corresponding to the two  classes. The basic SVM idea is to map the inputs x onto vectors 4(x) in some  high-dimensional feature space; ideally, in this feature space, the problem should be  linearly separable. Suppose first that this is true. Among all decision hyperplanes  w.qb(x) q- b = 0 which separate the training examples (i.e. which obey yi(w.c(xi) q-  b) ) 0 for all xi  Dx, Dx being the set of training inputs), the SVM solution is  chosen as the one with the largest margin, i.e. the largest minimal distance from  any of the training examples. Equivalently, one specifies the margin to be one and  minimizes the squared length of the weight vector ]lw] ]' [1], subject to the constraint  that yi(w.c(xi) + b) _> I for all i. If the problem is not linearly separable, 'slack  variables' i _> 0 are introduced which measure how much the margin constraints  are violated; one writes yi(w'c(xi) + b) > I - i. To control the amount of slack  1  allowed, a penalty term Cii is then added to the objective function ]]w] ,  with a penalty coefficient C. Training examples with yi(w. c(xi) + b) _> I (and  hence i = 0) incur no penalty; all others contribute C[1 -yi(w.c(xi) q- b)] each.  This gives the SVM optimization problem: Find w and b to minimize  -I1,,,11 + c +  2  (1)  where l(z) is the (shifted) 'hinge loss', l(z) = (1 - z)O(1 - z).  To interpret SVMs probabilistically, one can regard (1) as defining a (negative)  log-posterior probability for the parameters w and b of the SVM, given a training  set D. The first term gives the prior Q(w,b) - exp(-llwl[ ' 1 2 --2  -b B ). This  is a Gaussian prior on w; the components of w are uncorrelated with each other  and have unit variance. I have chosen a Gaussian prior on b with variance B';  the fiat prior implied by (1) can be recovered I by letting B - oe. Because only  the 'latent variable' values 0(x) = w.qb(x) + b -- rather than w and b individually  -- appear in the second, data dependent term of (1), it makes sense to express  the prior directly as a distribution over these. The 0(x) have a joint Gaussian  distribution because the components of w do, with covariances given by (O(x)O(x))  = ((qb(x).w)(w-qb(x'))) q- B ' = c(x).c(x') + B '. The SVM prior is therefore simply  a Gaussian process (GP) over the functions 0, with covariance function K(x, x ) =  qb(x) .b(x') q- B ' (and zero mean). This correspondence between SVMs and GPs  has been noted by a number of authors, e.g. [6, 7, 8, 9, 10].  The second term in (1) becomes a (negative) log-likelihood if we define the proba-  bility of obtaining output y for a given x (and 0) as  Q(y-rkllx, O) - n(C) exp[-Cl(yO(x))]  (2)  We set n(C) = 1/[1 q- exp(-2C)] to ensure that the probabilities for y - :t:1  never add up to a value larger than one. The likelihood for the complete data set  is then Q(D[O) = rli Q(yi[xi,O)Q(xi), with some input distribution Q(I) which  remains essentially arbitrary at this point. However, this likelihood function is not  normalized, because  v(O(x)) = Q(llx, O ) + Q(-l[x,O) = n(C){exp[-Cl(O(x))] + exp[-Cl(-O(x))]} < 1  In the probabilistic setting, it actually makes more sense to keep B finite (and small);  for B --> o, only training sets with all yi equal have nonzero probability.  Probabilistic Methods for Support Vector Machines 351  except when 10(x)] = 1. To remedy this, I write the actual probability model as  P(D,O) = Q(DIO)Q(O)/Af(D ).  (3)  Its posterior probability P(OID) Q(DIO)Q(O) is independent 9f the normalization  factor Af(D); by construction, the MAP value of 0 is therefore the SVM solution.  The simplest choice of Af(D) which normalizes P(D, O) is D-independent:  Af = N = fdOO(O)N"(O), N(O) = fdxO(x)v(O(x)).  (4)  Conceptually, this corresponds to the following procedure of sampling from P(D, 0):  First, sample 0 from the GP prior Q(O). Then, for each data point, sample x from  Q(x). Assign outputs y = +1 with probability Q(y[x,O), respectively; with the  remaining probability 1- v(O(x)) (the 'don't know' class probability in [11]), restart  the whole process by sampling a new 0. Because v(O(x)) is smallest 2 inside the 'gap'  [0(x)[ < 1, functions 0 with many values in this gap are less likely to 'survive' until  a dataset of the required size n is built up. This is reflected in an n-dependent  factor in the (effective) prior, which follows from (3,4) as P(O) O(O)N"(O).  Correspondingly, in the likelihood  P(ylx, O) = O(ylx, O)/v(O(x)), P(xlO )  O(x)v(O(x))  (5)  (which now is normalized over y = +1), the input density is influenced by the  function 0 itself; it is reduced in the 'uncertainty gaps' [O(x)l < 1.  To summarize, eqs. (2-5) define a probabilistic data generation model whose MAP  solution 0* = argmax P(OID ) for a given data set D is identical to a standard  SVM. The effective prior P(O) is a GP prior modified by a data set size-dependent  factor; the likelihood (5) defines not just a conditional output distribution, but also  an input distribution (relative to some arbitrary Q(x)). All relevant properties of  the feature space are encoded in the underlying GP prior Q(0), with covariance  matrix equal to the kernel K(x, x). The log-posterior of the model  1 fdxdx, O(x)K-l(x,x,)O(x,)_Cil(yiO(xi))+const (6)  lnP(O[O) = -  is just a transformation of (1) from w and b to 0. By differentiating w.r.t. the  O(x) for non-training inputs, one sees that its maximum is of the standard form  O*(X) -- 5-]iotiYiK(x, xi); for yiO*(Xi) > 1, < 1, and = 1 one has ci = 0, ci = C and  ci E [0, C] respectively. I will call the training inputs xi in the last group marginal;  they form a subset of all support vectors (the xi with ci > 0). The sparseness of  the SVM solution (often the number of support vectors is << n) comes from the  fact that the hinge loss l(z) is constant for z > 1. This contrasts with other uses  of GP models for classification (see e.g. [12]), where instead of the likelihood (2)  a sigmoidal (often logistic) 'transfer function' with nonzero gradient everywhere is  used. Moreover, in the noise free limit, the sigmoidal transfer function becomes a  step function, and the MAP values 0* will tend to the trivial solution O*(x) = O.  This illuminates from an alternative point of view why the margin (the 'shift' in  the hinge loss) is imtortant for SVMs.  Within the probabilistic framework, the main effect of the kernel in SVM classi-  fication is to change the properties of the underlying GP prior Q(O) in P(O)  2This is true for C > In 2. For smaller C, v(O(x)) is actually higher in the gap, and the  model makes less intuitive sense.  352 P. Sollich  (b)  c)  (d)  (g)  (e)  (h)  i)  Figure 1: Samples from SVM priors; the input space is the unit square [0, 1] 2.  3d plots are samples O(x) from the underlying Gaussian process prior Q(O). 2d  greyscale plots represent the output distributions obtained when O(x) is used in the  likelihood model (5) with C = 2; the greyscale indicates the probability of y = 1  (black: 0, white: 1). (a,b) Exponential (Ornstein-Uhlenbeck) kernel/covariance  function K0 exp(-]x -x'l/1), giving rough O(x) and decision boundaries. Length  scale 1 = 0.1, K0 = 10. (c) Same with K0 = 1, i.e. with a reduced amplitude of 0(x);  note how, in a sample from the prior corresponding to this new kernel, the grey  'uncertainty gaps' (given roughly by IO(x)l < 1) between regions of definite outputs  (black/white) have widened. (d,e) As first row, but with squared exponential (RBF)  kernel K0 exp[-(x- x/)2/(212)], yielding smooth O(x) and decision boundaries. (f)  Changing 1 to 0.05 (while holding K0 fixed at 10) and taking a new sample shows how  this parameter sets the typical length scale for decision regions. (g,h) Polynomial  kernel (1 + x.x') p, with p = 5; (i) p = 10. The absence of a clear length scale and  the widely differing magnitudes of O(x) in the bottom left (x = [0, 0]) and top right  (x = [1, 1]) corners of the square make this kernel less plausible from a probabilistic  point of view.  Probabilistic Methods for Support Vector Machines 353  Q(O)N'(O). Fig. 1 illustrates this with samples from Q(O) for three different types  of kernels. The effect of the kernel on smoothness of decision boundaries, and typ-  ical sizes of decision regions and 'uncertainty gaps' between them, can clearly be  seen. When prior knowledge about these properties of the target is available, the  probabilistic framework can therefore provide intuition for a suitable choice of ker-  nel. Note that the samples in Fig. I are from Q(O), rather than from the effective  prior P(O). One finds, however, that the n-dependent factor Nn(O) does not change  the properties of the prior qualitatively 3.  2 Evidence and error bars  Beyond providing intuition about SVM kernels, the probabilistic framework dis-  cussed above also makes it possible to apply Bayesian methods to SVMs. For ex-  ample, one can define the evidence, i.e. the likelihood of the data D, given the model  as specified by the hyperparameters C and (some parameters defining) K(x, x'). It  follows from (3) as  P(D) = Q(D)/N., Q(D) = fdOQ(DIO)Q(O).  (7)  The factor Q(D) is the 'naive' evidence derived from the unnormalized likelihood  model; the correction factor N n ensures that P(D) is normalized over all data  sets. This is crucial in order to guarantee that optimization of the (log) evidence  gives optimal hyperparameter values at least on average (M Opper, private com-  munication). Clearly, P(D) will in general depend on C and K(x,x t) separately.  The actual $VM solution, on the other hand, i.e. the MAP values 0', can be seen  from (6) to depend on the product CK(x, x') only. Properties of the deterministi-  cally trained SVM alone (such as test or cross-validation error) cannot therefore be  used to determine C and the resulting class probabilities (5) unambiguously.  I now outline how a simple approximation to the naive evidence can be derived.  Q(D) is given by an integral over all 0(x), with the log integrand being (6) up to an  additive constant. After integrating out the Gaussian distributed 0(x) with x ( Dx,  an intractable integral over the O(xi) remains. However, progress can be made by  expanding the log integrand around its maximum O*(xi). For all non-marginal  training inputs this is equivalent to Laplace's approximation: the first terms in  the expansion are quadratic in the deviations from the maximum and give simple  Gaussian integrals. For the remaining O(xi), the leading terms in the log integrand  vary linearly near the maximum. Couplings between these O(xi) only appear at the  next (quadratic) order; discarding these terms as subleading, the integral factorizes  over the O(xi) and can be evaluated. The end result of this calculation is:  _1 1  In Q(D)   --i yiciO*(xi) - C --i l(yiO*(xi) ) - n ln(1 + e -2c) -  In det(LmKra)  (s)  The first three terms represent the maximum of the log integrand, ln Q(DlO*);  the last one comes from the integration over the fluctuations of the O(x). Note  that it only contains information about the marginal training inputs: Era is the  corresponding submatrix of K(x,x), and Lra is a diagonal matrix with entries  aQuantitative changes arise because function values with [O(x)l < 1 are 'discouraged'  for large n; this tends to increase the size of the decision regions and narrow the uncertainty  gaps. I have verified this by comparing samples from Q(O) and P(O).  354 P. $ollich  O(x)  0  0 0.2  -1  -2  0.4. x 0.6 0.8  o  -o.1  -0.2  -0.3  -0.4  -0.5  1  0.8  0.6  0.4  0.2  o  :P(y= 1Ix)  0 0.2  1 2 c 3 4  0.4 x 0.6 0.8 1  Figure 2: Toy example of evidence maximization. Left: Target 'latent' function O(x)  (solid line). A SVM with RBF kernel K(x, x ) - K0 exp[-(x- x)2/(2/2)], l -- 0.05,  CKo - 2.5 was trained (dashed line) on n = 50 training examples (circles). Keeping  CKo constant, the evidence P(D) (top right) was then evaluated as a function  of C using (7,8). Note how the normalization factor N n shifts the maximum of  P(D) towards larger values of C than in the naive evidence ((D). Bottom right:  Class probability P(y = 1]x) for the target (solid), and prediction at the evidence  maximum C  1.8 (dashed). The target was generated from (3) with C-2.  27r[ai (C- ai)/C] 2. Given the sparseness of the SVM solution, these matrices should  be reasonably small, making their determinants amenable to numerical computation  or estimation [12]. Eq. (8) diverges when ai  0 or -+ C for one of the marginal  training inputs; the approximation of retaining only linear terms in the log integrand  then breaks down. I therefore adopt the simple heuristic of replacing det(LmKm)  by det(I + LmKm), which prevents these spurious singularities (I is the identity  matrix). This choice also keeps the evidence continuous when training inputs move  in or out of the set of marginal inputs as hyperparameters are varied.  Fig. 2 shows a simple application of the evidence estimate (8). For a given data set,  the evidence P(D) was evaluated 4 as a function of C. The kernel amplitude K0 was  varied simultaneously such that CKo and hence the SVM solution itself remained  unchanged. Because the data set was generated artificially from the probability  model (3), the 'true' value of C = 2 was known; in spite of the rather crude  approximation for ((D), the maximum of the full evidence P(D) identifies C   1.8 quite close to the truth. The approximate class probability prediction P(y =  11x , D) for this value of C is also plotted in Fig. 2; it overestimates the noise in the  target somewhat. Note that P(y]x, D) was obtained simply by inserting the MAP  values O*(x) into (5). In a proper Bayesian treatment, an average over the posterior  distribution P(OID ) should of course be taken; I leave this for future work.  4The normalization factor N s was estimated, for the assumed uniform input density  Qlx) of the example, by sampling from the GP prior Q(O). If Q(x) is unknown, the  empirical training input distribution can be used as a proxy, and one samples instead from  a multivariate Gaussian for the O(xi) with covariance matrix K(xi,xj). This gave very  similar values of In N s in the example, even when only a subset of 30 training inputs was  used.  Probabilistic Methods for Support Vector Machines 355  In summary, I have described a probabilistic framework for SVM classification. It  gives an intuitive understanding of the effect of the kernel, which determines a  Gaussian process prior. More importantly, it also allows a properly normalized  evidence to be defined; from this, optimal values of hyperparameters such as the  noise parameter C, and corresponding error bars, can be derived. Future work  will have to include more comprehensive experimental tests of the simple Laplace-  type estimate of the (naive) evidence Q(D) that I have given, and comparison wi'th  other approaches. These include variational methods; very recent experiments with  a Gaussian approximation for the posterior P(OID), for example, seem promis-  ing [6]. Further improvement should be possible by dropping the restriction to a  'factor-analysed' covariance form [6]. (One easily shows that the optimal Gaussian  covariance matrix is (D + K-) - , parameterized only by a diagonal matrix D.) It  will also be interesting to compare the Laplace and Gaussian variational results for  the evidence with those from the 'cavity field' approach of [10].  Acknowledgements  It .is a pleasure to thank Tommi Jaakkola, Manfred Opper, Matthias Seeger, Chris  Williams and Ole Winther for interesting comments and discussions, and the Royal  Society for financial support through a Dorothy Hodgkin Research Fellowship.  References  [1] C J C Burges. A tutorial on support vector machines for pattern recognition. Data  Mining and Knowledge Discovery, 2:121-167, 1998.  [2] A J Smola and B Sch51kopf. A tutorial on support vector regression. 1998. Neuro  COLT Technical Report TR-1998-030; available from http://svm.first.gmd.de/.  [3] B Sch51kopf, C Burges, and A J Smola. Advances in Kernel Methods: Support Vector  Machines. MIT Press, Cambridge, MA, 1998.  [4] B Sch51kopf, P Bartlett, A Smola, and R Williamson. Shrinking the tube: a new  support vector regression algorithm. In NIPS 11.  [5] N Cristianini, C Campbell, and J Shawe-Taylor. Dynamically adapting kernels in  support vector machines. In NIPS 11.  [6] M Seeger. Bayesian model selection for Support Vector machines, Gaussian processes  and other kernel classifiers. Submitted to NIPS 12.  [7] G Wahba. Support vector machines, reproducing kernel Hilbert spaces and the ran-  domized GACV. Technical Report 984, University of Wisconsin, 1997.  [8] T S Jaakkola and D Haussler. Probabilistic kernel regression models. In Proceedings of  The 7th International Workshop on Artificial Intelligence and Statistics. To appear.  [9] A J Smola, B SchSlkopf, and K R Miiller. The connection between regularization  operators and support vector kernels. Neural Networks, 11:637-649, 1998.  [10] M Opper and O Winther. Gaussian process classification and SVM: Mean field results  and leave-one-out estimator. In Advances in Large Margin Classifiers. MIT Press. To  appear.  [11] P Sollich. Probabilistic interpretation and Bayesian methods for Support Vector  Machines. Submitted to ICANN 99.  [12]  C K I Williams. Prediction with Gaussian processes: From linear regression to linear  prediction and beyond. In M I Jordan, editor, Learning and Inference in Graphical  Models, pages 599-621. Kluwer Academic, 1998.  
Data Visualization and Feature Selection:  New Algorithms for Nongaussian Data  Howard Hua Yang and John Moody  Oregon Graduate Institute of Science and Technology  20000 NW, Walker Rd., Beaverton, OR97006, USA  hyang@ece.ogi.edu, moody@cse.ogi.edu, FAX:503 7481406  Abstract  Data visualization and feature selection methods are proposed  based on the joint mutual information and ICA. The visualization  methods can find many good 2-D projections for high dimensional  data interpretation, which cannot be easily found by the other ex-  isting methods. The new variable selection method is found to be  better in eliminating redundancy in the inputs than other methods  based on simple mutual information. The efficacy of the methods  is illustrated on a radar signal analysis problem to find 2-D viewing  coordinates for data visualization and to select inputs for a neural  network classifier.  Keywords: feature selection, joint mutual information, ICA, vi-  sualiz ation, classification.  1 INTRODUCTION  Visualization of input data and feature selection are intimately related. A good  feature selection algorithm can identify meaningful coordinate projections for low  dimensional data visualization. Conversely, a good visualization technique can sug-  gest meaningful features to include in a model.  Input variable selection is the most important step in the model selection process.  Given a target variable, a set of input variables can be selected as explanatory  variables by some prior knowledge. However, many irrelevant input variables cannot  be ruled out by the prior knowledge. Too many input variables irrelevant to the  target variable will not only severely complicate the model selection/estimation  process but also damage the performance of the final model.  Selecting input variables after model specification is a model-dependent approach[6].  However, these methods can be very slow if the model space is large. To reduce the  computational burden in the estimation and selection processes, we need model-  independent approaches to select input variables before model specification. One  such approach is 5-Test [7]. Other approaches are based on the mutual information  (MI) [2, 3, 4] which is very effective in evaluating the relevance of each input variable,  but it fails to eliminate redundant variables.  In this paper, we focus on the model-independent approach for input variable selec-  688 H. H. Yang and d.. Moody  tion based on joint mutual information (JMI). The increment from MI to JMI is the  conditional MI. Although the conditional MI was used in [4] to show the monotonic  property of the MI, it was not used for input selection.  Data visualization is very important for human to understand the structural re-  lations among variables in a system. It is also a critical step to eliminate some  unrealistic models. We give two methods for data visualization. One is based on  the JMI and another is based on Independent Gomponent Analysis (IGA). Both  methods perform better than some existing methods such as the methods based on  PGA and canonical correlation analysis (GGA) for nongaussian data.  2 Joint mutual information for input/feature selection  Let Y be a target variable and Xi's are inputs. The relevance of a single input is  measured by the MI  Y) =  where K(pllq) is the Kullback-Leibler divergence of two probability functions p and  q defined by K (p(x)llq(x)) = - p(x) log q).  The relevance of a set of inputs is defined by the joint mutual information  I (Xg,   , Xk; Y) - g (p(zg,   , zk, y)IlP(zi,   , zn)p(y) ).  Given two selected inputs zj and z,, the conditional MI is defined by  I(Xi ; SlXj,X,) -  p(xj, x,)K (p(xi, ylx, x,)llp(xilx, x,)p(ylx, x,) ).  j,33k  Similarly define I(X;Y]Xj,...,X,) conditioned on more than two variables.  The conditional MI is always non-negative since it is a weighted average of the  Kullback-Leibler divergence. It has the following property  .l'(X1,'",Xn-l,Xn;V)- .l'(X1,...,Xn_l;V) : .l'(Xn;VlX1,"',Xn-1 )  O.  Therefore, I(Xx,...,X,_,X,;Y) >_ I(Xx,...,X,_x;Y), i.e., adding the variable  X, will always increase the mutual information. The information gained by adding  a variable is measured by the conditional MI.  When X, and Y are conditionally independent given Xx,  ., X_ x, the conditional  MI between X and Y is  =0,  so X, provides no extra information about Y when X,..., X,_ are known. In  particular, when X, is a function of X,..-, X,_x, the equality (1) holds. This is  the reason why the joint MI can be used to eliminate redundant inputs.  The conditional MI is useful when the input variables cannot be distinguished by  the mutual information I(X;Y). For example, assume I(Xt;Y) = I(X2;Y) =  I(Xa;Y), and the problem is to select (zx,z2),(zx,za) or (z2,za). Since  s) - r(x,x3; s) = slx) - r(x3; YIX),  we should choose (zi,z2) rather than (zi,za) if I(X2;Y]Xi) > I(Xa;YIXi). Oth-  erwise, we should choose (zl, za). All possible comparisons are represented by a  binary tree in Figure 1.  To estimate I(X,...,X&;Y), we need to estimate the joint probability  p(zi,...,z,, y). This suffers from the curse of dimensionality when k is large.  Data Visualization and Feature Selection 689  Sometimes, we may not be able to estimate high dimensional MI due to the sample  shortage. Further work is needed to estimate high dimensional joint MI based on  parametric and non-parametric density estimations, when the sample size is not  large enough.  In some real world problems such as mining large data bases and radar pulse classi-  fication, the sample size is large. Since the parametric densities for the underlying  distributions are unknown, it is better to use non-parametric methods such as his-  tograms to estimate the joint probability and the joint MI to avoid the risk of  specifying a wrong or too complicated model for the true density function.  I(X2;YIXI)>-- 3(X ;YIXI;YIXI)  (xl, x2)  I(X I,YIX2)>=IOC3;YIX2) I;YIX2)<I(X3;YIX2)  (xl ,x2) (x2,x3)  (xl,x3)  l(X 1 ;YIX3)>=I(X2;Y[X3/YIX3)<IOC2;Y[X3)  (xl,x3) (x2,x3)  Figure 1: Input selection based on the conditional MI.  In this paper, we use the joint mutual information I(Xi,Xj;Y) instead of the  mutual information I(Xi; Y) to select inputs for a neural network classifier. Another  application is to select two inputs most relevant to the target variable for data  visualization.  3 Data visualization methods  We present supervised data visualization methods based on joint MI and discuss  unsupervised methods based on ICA.  The most natural way to visualize high-dimensional input patterns is to dis-  play them using two of the existing coordinates, where each coordinate corre-  sponds to one input variable. Those inputs which are most relevant to the tar-  get variable corresponds the best coordinates for data visualization. Let (i*, j*) =  arg max(i,j)I(Xi , Xj; Y). Then, the coordinate axes (xi., xj.) should be used for  visualizing the input patterns since the corresponding inputs achieve the maximum  joint MI. To find the maximum I(Xi., Xj. ]Y), we need to evaluate every joint MI  I(Xi,Xj;Y) for i < j. The number of evaluations is O(n2).  Noticing that I(Xi,Xj;Y) = I(Xi;Y) + I(Xj;YIXi), we can first maximize the  MI I(Xi; Y), then maximize the conditional MI. This algorithm is suboptimal, but  only requires n- 1 evaluations of the joint MIs. Sometimes, this is equivalent to  exhaustive search. One such example is given in next section.  Some existing methods to visualize high-dimensional patterns are based on dimen-  sionality reduction methods such as PCA and CCA to find the new coordinates to  display the data. The new coordinates found by PCA and CCA are orthogonal in  Euclidean space and the space with Mahalanobis inner product, respectively. How-  ever, these two methods are not suitable for visualizing nongaussian data because  the projections on the PCA or CCA coordinates are not statistically independent  for nongaussian vectors. Since the JMI method is model-independent, it is better  for analyzing nongaussian data.  690 H. H. Yang and J. Moody  Both CCA and maximum joint MI are supervised methods while the PCA method  is unsupervised. An alternative to these methods is ICA for visualizing clusters [5].  The ICA is a technique to transform a set of variables into a new set of variables,  so that statistical dependency among the transformed variables is minimized. The  version of ICA that we use here is based on the algorithms in [1, 8]. It discovers  a non-orthogonal basis that minimizes mutual information between projections on  basis vectors. We shall compare these methods in a real world application.  4 Application to Signal Visualization and Classification  4.1 Joint mutual information and visualization of radar pulse patterns  Our goal is to design a classifier for radar pulse recognition. Each radar pulse  pattern is a 15-dimensional vector. We first compute the joint MIs, then use them  to select inputs for the visualization and classification of radar pulse patterns.  A set of radar pulse patterns is denoted by D = {(a: i, yi) : i = 1,.-., N} which  consists of patterns in three different classes. Here, each a: i E -R 5 and each yi   {1,2,3}.  1  0.4  0.  0  ' 't:::' , .  0  10 15 I 2 3 4 5 6 7 8 9 10 11 12 13 14 15  bundle numbel  (b)  Figure 2: (a) MI vs conditional MI for the radar pulse data; maximizing the MI then  the conditional MI with O(n) evaluations gives I(Xi, Xj ;Y) - 1.201 bits. (b) The  joint MI for the radar pulse data; maximizing the joint MI gives I(X., Xj.;Y) =  1.201 bits with O(n 2) evaluations of the joint MI. (i,j) = (i', j') in this case.  Let i = arg maxiI(Xi;Y ) and j = arg maxjviI(Xj;YlXil). From Figure 2(a),  we obtain (i,j) = (2,9) and I(Xi,Xj;Y) = I(X,;Y) + I(Xj;YIXi) =  1.201 bits. If the number of total inputs is n, then the number of evaluations for  computing the mutual information (Xi; Y) and the conditional mutual information  I(X;YIXi) is O(n).  To find the maximum I(Xi.,X.;Y), we evaluate every I(Xi,X;Y) for i < j.  These MIs are shown by the bars in Figure 2(b), where the i-th bundle displays the  MIs I(Xi,Xj;Y)for j =i+ 1,...,15.  In order to compute the joint MIs, the MI and the conditional MI is evaluated O(n)  and O(n 2) times respectively. The maximumjoint MI is I(X,., Xj. ;Y) = 1.201 bits.  Generally, we only know I(X, X;Y) _< I(gi., Xj.;Y). But in this particular  Data Iqsualization and Feature Selection 691  application, the equality holds. This suggests that sometimes we can use an efficient  algorithm with only linear complexity to find the optimal coordinate axis view  (xi., xj.). The joint MI also gives other good sets of coordinate axis views with  high joint MI values.  3 1 1  1  -1'0 0 10 20  first pdnopal component  111  1 ! 11 1  1  11  1 1 0 TM 11 1  1 1 11' ; 1 1  3  33 3  3 3 3   3 3  2  2 2  2  Fimt LD  3 3 33  -20 0 20  X2  (b)  25  05[ 1 1 1  3 3 3 3 3 1  0 33 33 33 33 3 3  3 233 33 33323 222 2  q)5- 2 2 3 3  2  / 2 2 2 2 3 2  2 2  -1 t 2 2 " 3 2 2 3 2 3 33 2 2:2 2 2 2 2  -151 223 22 22 2  -2 ' 22 2  (c) (d)  Figure 3: (a) Data visualization by two principal components; the spatial relation  between patterns is not clear. (b) Use the optimal coordinate axis view (zi., xj.)  found via joint MI to project the radar pulse data; the patterns are well spread  to give a better view on the spatial relation between patterns and the boundary  between classes. (c) The CCA method. (d) The ICA method.  Each bar in Figure 2(b) is associated with a pair of inputs. Those pairs with high  joint MI give good coordinate axis view for data visualization. Figure 3 shows that  the data visualizations by the maximum JMI and the ICA is better than those by  the PCA and the CCA because the data is nongaussian.  4.2 Radar pulse classification  Now we train a two layer feed-forward network to classify the radar pulse patterns.  Figure 3 shows that it is very difficult to separate the patterns by using just two  inputs. We shall use all inputs or four selected inputs. The data set D is divided  692 H. H. Yang and.J. Moody  into a training set Dx and a test set D2 consisting of 20 percent patterns in D. The  network trained on the data set Dx using all input variables is denoted by  Y = f(Xx,...,X,;Wx,W2,0)  where Wx and W2 are weight matrices and 0 is a vector of thresholds for the hidden  layer.  From the data set D, we estimate the mutual information I(Xi; Y) and select  ix = arg maxl(Xi; Y). Given Xi, we estimate the conditional mutual information  I(Xd;Y[Xi,) for j : ix. Ghoose three inputs Xi2,Xi3 and Xi, with the largest  conditional MI. We found a quartet (ix, i2, ia, i4) = (1, 2, 3, 9). The two-layer feed-  forward network trained on Dx with four selected inputs is denoted by   = g(X, X, Xa, X9; W, W, 0').  There are 1365 choices to select 4 input variables out of 15. To set a reference perfor-  mance for network with four inputs for comparison. Choose 20 quartets from the set  Q = {(jx,j2,j3, j4) ' 1 _ j < j2  j3  j4 _ 15). For each quartet (jx,j2,j3, j4), a  two-layer feed-forward network is trained using inputs (Xj, Xj2, Xj3, Xj4 ). These  networks are denoted by  Y = hi(Xj,Xj,Xj,Xj,; W t, Wt,O"), i = 1,2,...,20.  06  05  0.45  o  03  025  02  0.15  01  0.4  ] --- avenge lesta ER wllh 2D quMeL  leslblg ER: will14 letected bfiuls Xl, X2, X3. altd X9  ] - - - lra',tngER wilh4salecled.0uisXl. X2. X3. and) O InuningER'wMinpuls  (a) (b)  Figure 4: (a) The error rates of the network with four inputs (Xx,X2,X3,Xg)  selected by the joint MI are well below the average error rates (with error bars  attached) of the 20 networks with different input quartets randomly selected; this  shows that the input quartet (Xx,X,Xa, Xa) is rare but informative. (b) The  network with the inputs (Xx, X2, Xa, Xa) converges faster than the network with  all inputs. The former uses 65% fewer parameters (weights and thresholds) and  73% fewer inputs than the latter. The classifier with the four best inputs is less  expensive to construct and use, in terms of data acquisition costs, training time,  and computing costs for real-time application.  The mean and the variance of the error rates of the 20 networks are then computed.  All networks have seven hidden units. The training and testing error rates of the  networks at each epoch are shown in Figure 4, where we see that the network  with four inputs selected by the joint MI performs better than the networks with  randomly selected input quartets and converges faster than the network with all  inputs. The network with fewer inputs is not only faster in computing but also less  expensive in data collection.  Data Visualization and Feature Selection 693  5 CONCLUSIONS  We have proposed data visualization and feature selection methods based on the  joint mutual information and ICA.  The maximum JMI method can find many good 2-D projections for visualizing high  dimensional data which cannot be easily found by the other existing methods. Both  the maximum JMI method and the ICA method are very effective for visualizing  nongaussian data.  The variable selection method based on the JMI is found to be better in eliminating  redundancy in the inputs than other methods based on simple mutual information.  Input selection methods based on mutual information (MI) have been useful in many  applications, but they have two disadvantages. First, they cannot distinguish inputs  when all of them have the same MI. Second, they cannot eliminate the redundancy  in the inputs when one input is a function of other inputs. In contrast, our new  input selection method based on the joint MI offers significant advantages in these  two aspects.  We have successfully applied these methods to visualize radar patterns and to select  inputs for a neural network classifier to recognize radar pulses. We found a smaller  yet more robust neural network for radar signal analysis using the JMI.  Acknowledgement: This research was supported by grant ONR N00014-96-1-  0476.  References  [1] S. Amari, A. Cichocki, and H. H. Yang. A new learning algorithm for blind  signal separation. In Advances in Neural Information Processing Systems, 8,  eds. David S. Touretzky, Michael C. Mozer and Michael E. Hasselmo, MIT  Press: Cambridge, MA., pages 757-763, 1996.  [2] G. Barrows and J. Sciortino. A mutual information measure for feature selection  with application to pulse classification. In IEEE Intern. Symposium on Time-  Frequency and Time-Scale Analysis , pages 249-253, 1996.  [3] R. Battiti. Using mutual information for selecting features in supervised neural  net learning. IEEE Trans. on Neural Networks, 5(4):537-550, July 1994.  [4] B. Bonnlander. Nonparametric selection of input variables for connectionist  learning. Technical report, PhD Thesis. University of Colorado, 1996.  [5] C. Jutten and J. Herault. Blind separation of sources, part i: An adaptive  algorithm based on neuromimetic architecture. Signal Processing, 24:1-10, 1991.  [6] J. Moody. Prediction risk and architecture selection for neural network. In  V. Cherkassky, J.H. Friedman, and H. Wechsler, editors, From Statistics to  Neural Networks: Theory and Pattern Recognition Applications. NATO ASI  Series F, Springer-Verlag, 1994.  [7] H. Pi and C. Peterson. Finding the embedding dimension and variable depen-  dencies in time series. Neural Computation, 6:509-520, 1994.  [8] H. H. Yang and S. Amari. Adaptive on-line learning algorithms for blind sep-  aration: Maximum entropy and minimum mutual information. Neural Compu-  tation, 9(7):1457-1482, 1997.  
Mixture Density Estimation  Jonathan Q. Li  Department of Statistics  Yale University  P.O. Box 208290  New Haven, CT 06520  Qiang. Li@aya. yale. edu  Andrew R. Barron  Department of Statistics  Yale University  P.O. Box 208290  New Haven, CT 06520  Andrew. Barron@yale. edu  Abstract  Gaussian mixtures (or so-called radial basis function networks) for  density estimation provide a natural counterpart to sigmoidal neu-  ral networks for function fitting and approximation. In both cases,  it is possible to give simple expressions for the iterative improve-  ment of performance as components of the network are introduced  one at a time. In particular, for mixture density estimation we show  that a k-component mixture estimated by maximum likelihood (or  by an iterative likelihood improvement that we introduce) achieves  log-likelihood within order 1/k of the log-likelihood achievable by  any convex combination. Consequences for approximation and es-  timation using Kullback-Leibler risk are also given. A Minimum  Description Length principle selects the optimal number of compo-  nents k that minimizes the risk bound.  I Introduction  In density estimation, Gaussian mixtures provide flexible-basis representations for  densities that can be used to model heterogeneous data in high dimensions. We  introduce ar index of regularity cy of density functions f with respect to mixtures  of densities from a given family. Mixture models with k components are shown to  achieve Kullback-Leibler approximation error bounded by c/k for every k. Thus  in a manner analogous to the treatment of sinusoidal and sigrnoidal networks in  Barron [1],[2], we find classes of density functions f such that reasonable size net-  works (not exponentially large as function of the input dimension) achieve suitable  approximation and estimation error.  Consider a parametric family G = (b0(x), x E 3J C R d' :0 E O C R d) of probability  density functions parameterized by 0 & O. Then consider the class C = CONV(G')  of density functions for which there is a mixture representation of the form  f(x) = fo 0)P(dO) (1)  where 0 () are density functions from G and P is a probability measure on .  The main theme of the paper is to give approximation and estimation bounds of  arbitrary densities by finite mixture densities. We focus our attention on densities  280 J. Q. Li and A. R. Barron  inside C first and give an approximation error bound by finite mixtures for arbi-  trary f E C. The approximation error is measured by Kullback-Leibler divergence  between two densities, defined as  D(fllg ) = f f(x) log[f(x)/g(x)]dx. (2)  In density estimation, D is more natural to use than the L 2 distance often seen in  the function fitting literature. Indeed, D is invariant under scale transformations  (and other 1-1 transformation of the variables) and it has an intrinsic connection  with Maximum Likelihood, one of the most useful methods in the mixture density  estimation. The following result quantifies the approximation error.  THEOREM 1 Let G = {qb0(x)  0 e O} and C= CONV(G). Let f(x) =  f c)o(x)P(dO)  . There exists fk, a k-component mixture of trio, such that  (3)  O(fllfk)-< k  In the bound, we have  f f  f o(x)p(do)  and ' = 411og(3v) + a], where  dx, (4)  log (x)  a= sup (5)  01,02,X  Here, a characterizes an upper bound of the log ratio of the densities in G, when  the parameters are restricted to O and the variable to  Note that the rate of convergence, l/k, is not related to the dimensions of  The behavior of the constants, though, depends on the choices of G and the target  f.  For example we may take G to be the Gaussian location family, which we restrict  to a set r which is a cube of side-length A. Likewise we restrict the parameters to  be in the same cube. Then,  dA :  a < (6)  In this case, a is linear in dimension.  The value of c depends on the target density f. Suppose f is a finite mixture with  M components, then  < (7)  with equality if and only if those M components are disjoint. Indeed, suppose  f(x) M  -- i=lPiqbOi(X), then piqbo,(X)/iM=l piqboi(X) _ 1 and hence  M  c}: f '4M--(P'bO' (x))bO' (x)dx < f E(1)cfio,(x)dx: M. (8)  M --  --i=1PiO0, (X) i=1  Genovese and Wasserman [3] deal with a similar setting. A Kullback-Leibler ap-  proximation bound of order 1/Vr for one-dimensional mixtures of Gaussians is  given by them.  In the more general case that f is not necessarily in , we have a competitive  optimality result. Our density approximation is nearly at least as good as any gp  in .  Mixture Density Estimation 281  THEOREM 2 For every gp(x) - f  2  O(fllf) 5 O(fllgP) q-  (9)  Here,  2 f fck(x)P(dO) f(x)dx. (10)  In particular, we can take infimum over all gr E C, and still obtain a bound.  Let D(flIC) = infgec D(fllg). A theory of information projection shows that if  there exists a sequence of fk such that D(fllfk ) -+ D(fll), then f converges to  a function f*, which achieves D(fll ). Note that f* is not necessarily an element  in . This is developed in Li[4] building on the work of Bell and Cover[5]. As a  consequence of Theorem 2 we have  2  O(fllf) 5 O(fllf*) q-  (11)  where c s is the smallest limit of , for sequences of P achieving D(fllg) that  f,*  approaches the infimum D(fllC ).  We prove Theorem 1 by induction in the following section. An appealing feature  of such an approach is that it provides an iterative estimation procedure which  allows us to estimate one component at a time. This greedy procedure is shown  to perform almost as well as the full-mixture procedures, while the computational  task of estimating one component is considerably easier than estimating the full  mixtures.  Section 2 gives the iterative construction of a suitable approximation, while Section  3 shows how such mixtures may be estimated from data. Risk bounds are stated in  Section 4.  2 An iterative construction of the approximation  We provide an iterative construction of f's in the following fashion. Suppose  during our discussion of approximation that f is given. We seek a k-component  mixture f close to f. Initialize fx by choosing a single component from G to  minimize D(fllfx ) = D(fllq5o). Now suppose we have fk-l(X). Then let f(x) =  (1 - a)f-l(X) + acfo(x) where a and 0 are chosen to minimize D(fllf ). More  generally let f be any sequence of k-component mixtures, for k = 1, 2, ... such  that D(fllfk) <_ mina,0 D(fll(1 - a)fk-x q- aO). We prove that such sequences fk  achieve the error bounds in Theorem 1 and Theorem 2.  Those familiar with the iterative Hilbert space approximation results of Jones[6],  Barron[I], and Lee, Bartlett and Williamson[7], will see that we follow a similar  strategy. The use of L2 distance measures for density approximation involves L2  norms of component densities that are exponentially large with dimension. Naive  Taylor expansion of the Kullback-Leibler divergence leads to an L2 norm approxi-  mation (weighted by the reciprocal of the density) for which the difficulty remains  (Zeevi gc Meir[8], Li[9]). The challenge for us was to adapt iterative approximation  to the use of Kullback-Leibler divergence in a manner that permits the constant  a in the bound to involve the logarithm of the density ratio (rather than the ratio  itself) to allow more manageable constants.  282 J. Q. Li and A. R. Barron  The proof establishes the inductive relationship  Dk _< (1 -o)Dk-1 + o?B,  where B is bounded and D = D(fllf). By choosing a = 1,a2  thereafter a = 2/k, it's easy to see by induction that D _< 4B/k.  (12)  = 1/2 and  To get (12) we establish a quadratic upper bound for -log/x -  ' f --  -log ((1-a)f_+a0) Three key analytic inequalities regarding to the logarithm  will be handy for us,  forr_>ro >0,  and  - log(to) + ro - 1]( r _ 1) 2  -log(r) _< -(r- 1) +[ (to - 1) 2  (13)  2[-log(r) +r- 1] < logr, (14)  r-1 -  - log(r) + r - 1  (r- 1) 2  _< 1/2 + log-(r)  (15)  Note that in our application, ro is a ratio of densities in . Thus we obtain an upper  bound for log-(ro) involving a. Indeed we find that (1/2 + log-(ro)) _< 7/4 where  7 is as defined in the theorem.  In the case that f is in , we take g = f. Then taking the expectation with respect  to f of both sides of (16), we acquire a quadratic upper bound for Dk, noting that  r = L Also note that D is a function of 0. The greedy algorithm chooses 0 to  minimize D(O). Therefore  Plugging the upper bound (16) for Dk(O) into (17), we have  _ __ + .2 (7/4) + - log(ro)]f(x)dxP(dO). (18)  g g  (16)  Now apply (14) and (15) respectively. We get  log(r) < log(to) aqb a2 (1/2 log-  .... + + (to)) + - log(ro).  - g -/  where log-(.) is the negative part of the logarithm. The proof of of inequality (13)  -- 1og(r)+r--1  is done by verifying that (r_1)2 is monotone decreasing in r. Inequalities (14)  and (15) are shown by separately considering the cases that r < I and r > i (as  well as the limit as r -+ 1). To get the inequalities one multiplies through by (r- 1)  or (r - 1) 2, respectively, and then takes derivatives to obtain suitable monotonicity  in r as one moves away from r = 1.  (x-a)f_+ao and ro = (1-a)f/_t where  Now apply the inequality (13) with r = /7 /7 ,  g is an arbitrary density in  with g = f coP(dO). Note that r >_ ro in this case  because ao > 0. Plug in r = ro + a  at the right side of (13) and expand the  square. Then we get  aqb - log(to) + ro aqb)] 2  -log(r) < -(to+---1)+[ -!][(ro-1)+(  - g (ro - 1) 2 g  log(to) + a 2 -[- log(to) + ro- 1] log(to) + ro- 1]  g (to 1) 2 + 2a-[- '  - g ro- 1  Mxture Density Estimation 283  where ro = (1- a)fk-x (x)/g(x) and P is chosen to satisfy f0 c)o(x)P(dO) = g(x).  Thus  z>k < (1 - f  -  f(x)dx(7/4) + a log(1 - a) - a - log(1 - a).  (19)  It can be shown that a log(1 - a) - a - log(1 - a) _< O. Thus we have the desired  inductive relationship,  D < (1- a)Dk-1 + a2c,e7/4. (20)  Therefore, D _< k'  In the case that f does not have a mixture representation of the form f boP(dO),  i.e. f is outside the convex hull , we take D to be f f(x) log gr-eiZ2dx for any given  fk(x)  g.,:,(x) = f c)o(x)P(dO). The above analysis then yields D - D(fllf)- D(fllg1) _<  Cp  ,,, as desired. That completes the proof of Theorems I and 2.  3 A greedy estimation procedure  The connection between the K-L divergence and the MLE helps to motivate the  following estimation procedure for fk if we have data Xx, ...,X, sampled from f.  The iterative construction of f can be turned into a sequential maximum likeli-  hood estimation by changing minD(flirt, ) to max ElL1 log ft,(Xi) at each step. A  surprising result is that the resulting estimator f has a log likelihood almost at  least as high as log likelihood achieved by any density gp in C with a difference of  order 1/k. We formally state it as  - ^ - 'P (21)  1 logfk(Xi) >_ 1 logg1:,(Xi) - 7 k  n n  i--1 i--1  gp E . Here Fn is the empirical distribution,  for all  (I/n) -4__1 2 where  Xl ,P  f  cx,r = (f .  for which c.,p =  (22)  The proof of this result (21) follows as in the proof in the last section, except that  now we take D = EF, logg.,:,(X)/f(X) to be the expectation with respect to F  instead of with respect to the density f.  Let's look at the computation at each step to see the benefits this new greedy  procedure can bring for us. We have f(x) = (1- a)f-l(/) + acfo(x) with 0 and  a chosen to maximize  n  ' log[(1 - a)fk-1 (Xi) + ac)o(Xi)] (23)  i----1  which is a simple two component mixture problem, with one of the two components,  f-x(x), fixed. To achieve the bound in (21), a can either be chosen by this iterative  maximum likelihood or it can be held fixed at each step to equal a (which as  before is a = 2/k for k > 2). Thus one may replace the MLE-computation of a k-  component mixture by successive MLE-computations of two-component mixtures.  The resulting estimate is guaranteed to have almost at least as high a likelihood as  is achieved by any mixture density.  284 J. Q. Li and A. R. Barron  A disadvantage of the greedy procedure is that it may take a number of steps to  adequately downweight poor initial choices. Thus it is advisable at each step to re-  tune the weights of convex combinations of previous components (and even perhaps  to adjust the locations of these components), in which case, the result from the  previous iterations (with k - I components) provide natural initialization for the  search at step k. The good news is that as long as for each k, given ]k-l, the ]k is  chosen among k component mixtures to achieve likelihood at least as large as the  choice achieving max0 in__x log[(1 - ak)fk-l(Xi) + akqb0(Xi)], that is, we require  that  ElogA(Xi) >_ moaxElog[(1-ok)f_x(Xi)+okqbo(Xi)], (24)  i=1 i=1  then the conclusion (21) will follow.  In particular, our likelihood results and risk bound results apply both to the case  that ]k is taken to be global maximizer of the likelihood over k-component mixtures  as well as to the case that ]k is the result of the greedy procedure.  4 Risk bounds for the MLE and the iterative MLE  The metric entropy of the family G is controlled to obtain the risk bound and  to determine the precisions with which the coordinates of the parameter space are  allowed to be represented. Specifically, the following Lipschitz condition is assumed:  for 0 E 0 C R d and x E X C R d,  d  sup I logo(x) -logr;bo, (x)l _< B '- IO -0. I  x:' j=l  (25)  where 0j is the j-th coordinate of the parameter vector. Note that such a condition  is satisfied by a Gaussian family with x restricted to a cube with sidelength A  and has a location parameter 0 that is also prescribed to be in the same cube. In  particular, if we let the variance be a 2, we may set B - 2A/a 2.  Now we can state the bound on the K-L risk of fk.  THEOREM 3 Assume the condition (5). Also assume 0 to be a cube with side-  length A. Let fk(x) be either the maximizer of the likelihood over k-component  mixtures or more generally any sequence of density estimates fk satisfying (24).  We have   c 2 2kd  E(D(fllfk)) - D(fllC) < . log(nnBe). (26)  - k n  From the bound on risk, a best choice of k would be of order roughly v/ leading  to a bound on ED(f[[fk) - D(f[[) of order 1/v/ to within logarithmic factors.  However the best such bound occurs with k - 7cf,.x//x/2dlog(nABe) which is  not available when the value of cL, is unknown. More importantly, k should not be  chosen merely to optimize an upper bound on risk, but rather to balance whatever  approximation and estimation sources of error actually occur. Toward this end we  optimize a penalized likelihood criterion related to the minimum description length  principle, following Barron and Cover [10].  Let l(k) be a function of k that satisfies E3__l e -l(k) _< 1, such as l(k) = 2 log(k+ 1).  Mixture Density Estimation 285  A penalized MLE (or MDL) procedure picks k by minimizing  1  log 1 2kdlOg(nABe)  - + +  n i--1 fk(Xi) n  (27)  Then we have  i,. 2kd  E(D(fllfi))- D(fll) < nn{72 c2  _ ----' + 7 n  log(.4s) + (2s)  A proof of these risk bounds is given in Li[4]. It builds on general results for  maximum likelihood and penalized maximum likelihood procedures.  Recently, Dasgupta [11] has established a randomized algorithm for estimating mix-  tures of Gaussians, in the case that data are drawn from a finite mixture of suffi-  ciently separated Gaussian components with common covariance, that runs in time  linear in the dimension and quadratic in the sample size. However, present forms  of his algorithm require impractically large sample sizes to get reasonably accurate  estimates of the density. It is not yet known how his techniques will work for more  general mixtures. Here we see that iterative likelihood maximization provides a  better relationship between accuracy, sample size and number of components.  References  [1] Barron, Andrew (1993) Universal Approximation Bounds for Superpositions of a  Sigmoidal Function. IEEE Transactions on Information Theory 39, No. 3:930-945  [2] Barron, Andrew (1994) Approximation and Estimation Bounds for Artificial  Neural Networks. Machine Learning 14: 115-133.  [3] Genovese, Chris and Wasserman, Larry (1998) Rates of Convergence for the  Gaussian Mixture Seive. Manuscript.  [4] Li, Jonathan Q. (1999) Estimation of Mixture Models. Ph.D Dissertation. The  Department of Statistics. Yale University.  [5] Bell, Robert and Cover, Thomas (1988) Game-theoretic optimal portfolios. Man-  agement Science 34: 724-733.  [6] Jones, Lee (1992) A simple lemma on greedy approximation in Hilbert space  and convergence rates for projection pursuit regression and neural network training.  Annals of Statistics 20: 608-613.  [7] Lee, W.S., Bartlett, P.L. and Williamson R.C. (1996) Efficient Agnostic Learn-  ing of Neural Networks with Bounded Fan-in. IEEE Transactions on Information  Theory 42, No. 6: 2118-2132.  [8] Zeevi, Assaf and Meir Ronny (1997) Density Estimation Through Convex Com-  binations of Densities: Approximation and Estimation Bounds. NeUral Networks  10, No.1: 99-109.  [9] Li, Jonathan Q. (1997) Iterative Estimation of Mixture Models. Ph.D. Prospec-  tus. The Department of Statistics. Yale University.  [10] Barron, Andrew and Cover, Thomas (1991) Minimum Complexity Density  Estimation. IEEE Transactions on Information Theory 37: 1034-1054.  [11] Dasgupta, Sanjoy (1999) Learning Mixtures of Gaussians. Proc. IEEE Conf.  on Foundations of Computer Science, 634-644.  
An Oculo-Motor System with Multi-Chip  Neuromorphic Analog VLSI Control  Oliver Landolt*  CSEM SA  2007 Neuchtel / Switzerland  E-mail: landolt@caltech.edu  Stive Gyger  CSEM SA  2007 Neuchatel / Switzerland  E-mail: steve.gyger @csem.ch  Abstract  A system emulating the functionality of a moving eye hence the name  oculo-motor system--has been built and successfully tested. It is made  of an optical device for shifting the field of view of an image sensor by up  to 45 o in any direction, four neuromorphic analog VLSI circuits imple-  menting an oculo-motor control loop, and some off-the-shelf electronics.  The custom integrated circuits communicate with each other primarily by  non-arbitrated address-event buses. The system implements the behav-  iors of saliency-based saccadic exploration, and smooth pursuit of light  spots. The duration of saccades ranges from 45 ms to 100 ms, which is  comparable to human eye performance. Smooth pursuit operates on light  sources moving at up to 50 /s in the visual field.  I INTRODUCTION  Inspiration from biology has been recognized as a seminal approach to address some en-  gineering challenges, particularly in the computational domain [1]. Researchers have bor-  rowed architectures, operating principles and even micro-circuits from various biological  neural structures and turned them into analog VLSI circuits [2]. Neuromorphic approaches  are often considered to be particularly suited for machine vision, because even simple  animals are fitted with neural systems that can easily outperform most sequential digital  computers in visual processing tasks. It has long been recognized that the level of visual  processing capability needed for practical applications would require more circuit area than  can be fitted on a single chip. This observation has triggered the development of inter-chip  communication schemes suitable for neuromorphic analog VLSI circuits [3]-[4], enabling  the combination of several chips into a system capable of addressing tasks of higher com-  plexity. Despite the availability of these communication protocols, only few successful  implementations of multi-chip neuromorphic systems have been reported so far (see [5] for  a review). The present contribution reports the completion of a fully functional multi-chip  system emulating the functionality of a moving eye, hence the denomination oculo-motor  system. It is made of two 2D VLSI retina chips, two custom analog VLSI control chips,  dedicated optical and mechanical devices and off-the-shelf electronic components. The  four neuromorphic chips communicate mostly by pulse streams mediated by non-arbitrated  address-event buses [4]. In its current version, the system can generate saccades (quick eye  * Now with Koch Lab, Division of Biology 139-74, Caltech, Pasadena, CA 91125, USA  An Oculo-Motor System with Multi-Chip Neuromorphic Analog VLSI Control 711  movements) toward salient points of the visual scene, and track moving light spots. The  purpose of the saccadic operating mode is to explore the visual scene efficiently by allo-  cating processing time proportionally to significance. The purpose of tracking (also called  smooth pursuit) is to slow down or suppress the retina image slip of moving objects in order  to leave visual circuitry more time for processing. The two modes--saccadic exploration  and smooth pursuit operate concurrently and interact with each other. The development  of this oculo-motor system was meant as a framework in which some general issues per-  tinent to neuromorphic engineering could be addressed. In this respect, it complements  Horiuchi's pioneering work [6]-[7], which consisted of developing a 1D model of the pri-  mate oculo-motor system with a focus on automatic on-chip learning of the correct control  function. The new system addresses different issues, notably 2D operation and the problem  of strongly non-linear mapping between 2D visual and motor spaces.  2 SYSTEM DESCRIPTION  The oculo-motor system is made of three modules (Fig. 1). The moving eye module con-  tains a 35 by 35 pixels electronic retina [8] fitted with a light deflection device driven by two  motors. This device can shift the field of view of the retina by up to 45 o in any direction.  The optics are designed to cover only a narrow field of view of about 12 o. Thereby, the  retina serves as a high-resolution "spotlight" gathering details of interesting areas of the  visual scene, similarly to the fovea of animals. Two position control loops implemented  by off-the-shelf components keep the optical elements in the position specified by input  signals applied to this module. The other modules control the moving eye in two types  of behavior, namely saccadic exploration and smooth pursuit. They are implemented as  physically distinct printed circuit boards which can be enabled or disabled independently.  wide-angle retina  '- saccadic ] .........     . / saaencymsmouuon l"----.   ' [ consol  t lllll X   : I , chp /  ::  ., moor  / poiio  , . .. . :   : incremenui I- spot spot reda  position location  age   m,  chp  Figure 1: Oculo-motor system architecture  The light deflection device is made of two transparent and flat disks with a micro-prism  grating on one side, mounted perpendicularly to the optical axis of a lens. Each disk can  rotate without restriction around this axis, independently from the other. As a whole, each  micro-prism grating acts on light essentially like a single large prism, except that it takes  much less space (Fig. 2). Although a single fixed prism cannot have an adjustable de-  flection angle, with two mobile prisms, any magnitude and direction of deflection within  some boundary can be selected, because the two contributions may combine either con-  712 O. Landolt and S. Gyger  structively or destructively depending on the relative prism orientations. The relationship  between prism orientations and deflection angle has been derived in [9]. The advantage of  this system over many other designs is that only two small passive optical elements have  to move whereas most of the components are fixed, which enables fast movements and  avoids electrical connections to moving parts. The drawback of this principle is that optical  aberrations introduced by the prisms degrade image quality. However, when the device is  used in conjunction with a typical electronic retina, this degradation is not limiting because  these image sensors are characterized by a modest resolution due to focal-plane electronic  processing.  lens  retina  Go  micro-prism  gratings   :.* '1  Figure 2: A. Light deflection device principle. B. Replacement of conventional prisms by  micro-prism gratings. C. Photograph of the prototype with motors and orientation sensors.  The saccadic exploration module (Fig. 1) consists of an additional retina fitted with a  fixed wide-angle lens, and a neuromorphic saccadic control chip. The retina gathers low-  resolution information from the whole visual scene accessible to the moving eye, deter-  mines the degree of interest or saliency [10] of every region and transmits the resulting  saliency distribution to the saccadic control chip. In the current version of the system, the  distribution of saliency is just the raw output image of the retina, whereby saliency is deter-  mined by the brightness of visual scene locations. By inserting additional visual processing  hardware between the retina and the saccadic control chip, it would be possible to generate  interest for more sophisticated cues like edges, motion or specific shapes or patterns. The  saccadic control chip (Fig. 3) determines the sequence and timing of an endless succes-  sion of quick jumps or saccades to be executed by the moving eye, in such a way that  salient locations are attended longer and more frequently than less significant locations.  The chip contains a 2D array of about 900 cells, which is called visual map because its  organization matches the topology of the visual field accessible by the moving eye. The  chip also contains two 1D arrays of 64 cells called motor maps, which encode micro-prism  orientations in the light deflection device. Each cell of the visual map is externally stim-  ulated by a stream of brief pulses, the frequency of which encodes saliency. The cells  integrate incoming pulses over time on a capacitor, thereby building up an internal voltage  at a rate proportional to pulse frequency. A global comparison circuit called winner-  take-all--selects the cell with the highest internal voltage. In the winning cell, a leakage  mechanism slowly decrease the internal voltage over time, thereby eventually leading an-  other cell to win. With this principle, any cell stimulated to some degree wins from time  to time. The frequency of winning and the time ellapsed until another cell wins increases  with saliency. The visual map and the two motor maps are interconnected by a so-called  network of links [9], which embodies the mapping between visual and motor spaces. This  network consists of a pair of wires running from each visual cell to one cell in each of the  two motor maps. Thereby, the winning cell in the visual map stimulates exactly one cell in  An Oculo-Motor System with Multi-Chip Neuromorphic Analog VLSI Control 713  each motor map. The location of the active cell in a motor map encodes the orientation of  a micro-prism grating, therefore this representation convention is called place coding [9].  The addresses of the active cells on the motor maps are transmitted to the moving eye,  which triggers micro-prism displacements toward the specified orientations.  motor maps  visual map  saliency  distribution  5-- orientations  ,1  " .. (address-  (adress-[-  event)  event)   network of links  Figure 3: Schematic of the saccadic control chip  The smooth pursuit module consists of an EPROM chip and a neuromorphic incremental  control chip (Fig. 1). The address-event stream delivered by the narrow-field retina is  applied to the EPROM. The field of view of this retina has been divided up into eight  angular sectors and a center region (Fig. 4A). The EPROM maps the addresses of pixels  located in the same sector onto a common output address, thereby summing their spiking  frequencies. The resulting address-event stream is applied to a topological map of eight  cells constituting one of the inputs of the neuromorphic incremental control chip. If a  single bright spot is focused on the retina away from the center, a large sum is produced in  one or two neighboring cells of this map, whereas the other cells receive only background  stimulation levels close to zero. Thereby, the angular position of the light spot is encoded by  the location of the spot of activity on the mapwin other words place coding. Other objects  than light spots could be processed similarly after insertion of relevant detection hardware  between the retina and the EPROM. The incremental control chip has two additional input  maps representing the current orientations of the two prisms (Fig. 4B). These maps are  connected to position sensors incorporated into the moving eye module (Fig. 1). These  additional inputs are necessary because the control actions depends not only on the location  of the target on the retina, but also on the current prism orientations [9]. The control actions  are computed by three networks of links relating the primary inputs maps to the final output  map via an intermediate layer. The purpose of this intermediate stage is to break down the  control function of three variables into three functions of only two variables, which can  be implemented by a lower number of links [11]. As in the saccadic control chip, the  mapping between the input and output spaces has been calculated numerically prior to chip  fabrication, then hardwired as electrical connections. The final outputs of the chip are pulse  streams encoding the direction and rate at which each micro-prism grating must rotate in  order to shift the target toward the center of the retina. These pulses incrementally update  prism orientations settings at the input of the moving eye module (Fig. 1).  Since two different modules control the same moving eye, it is necessary to coordinate  them in order to avoid conflicts. Saccadic module interventions occur whenever a saccade  is generated, namely every 200-500 ms in typical operating conditions. At the instant a  saccade is requested, the smooth pursuit module is shut off in order to prevent it from  reacting against the saccade. A similar mechanism called saccadic suppression exists in  biology. When the eye reaches the target location, control is left entirely to the smooth  pursuit module until the next saccade is generated. Reciprocally, if an object tracked by  714 O. Landolt and S. Gyger  mo  0%  80% 20%  0% 0% o%  0%  go  spot  location  current  prism  positions  input maps intermediate output maps  m .  Mj52x/ pn.'sm    /5 .orientation  lncrem, ents  networks of links  Figure 4: A. Place-coded spot location obtained by summing the outputs of pixels belong-  ing to the same sector. B. Architecture of the incremental control chip  the smooth pursuit module reaches the boundary of the global visual field, the incremental  control chip sends a signal triggering a saccade back toward the center of the visual field--  which is called nystagmus in biology. The reason for splitting control into two modules is  that visuo-motor coordinate mappings are very different for saccadic exploration and for  smooth pursuit [9]. In the former case, visual input is related to the global field of view  covered by the fixed wide-angle retina, and outputs are absolute micro-prism orientations.  Saccade targets need not be initially visible to the moving eye. Since saccades are executed  without permanent visual feedback, their accuracy is limited by the mapping hardwired in  the control chip. Inversely, smooth pursuit is based on information extracted directly from  the retina image of the moving eye. The output of the incremental control chip are small  changes in micro-prism orientations instead of absolute positions. Thereby, the smooth  pursuit module operates under closed-loop visual feedback, which confers it high accuracy.  However, operation under visual feedback is slower than open-loop saccadic movements,  and smooth pursuit inherently applies only to a single target. Thus, the two control modules  are very complementary in purpose and performance.  3 EXPERIMENTAL RESULTS  The present section reports both qualitative observations and quantitative measurements  made on the oculo-motor system, because the complexity of its behavior is difficult to  convey by just a few numbers. The measurement setup consisted of a black board on  which high efficiency white light emitting diodes were mounted, the intensity of which  could be set individually. The visual scene was placed about 70 cm away from the moving  eye. The axes of the two retinas were parallel at a distance of 6.5 cm. It was necessary  to take this spacing into account for the visuo-motor coordinate mapping. The saliency  distribution produced by the visual scene was measured by analyzing the output image of  the wide-angle retina chip (Fig. 1).  When a single torchlight was waved in front of the moving eye, it was found that the  smooth pursuit system indeed keeps the center of gravity of the light source image at the  center of the narrow field of view. The maximum tracking velocity depends on the intensity  ratio contrastwbetween the light spot and the background. This behavior was expected  because by construction, the incremental control chip generates correction pulses at a rate  proportional to the magnitude of its input signals. At the highest contrast, we were able to  achieve a maximum tracking speed of 50 /s. For comparison, smooth pursuit in humans  can in principle reach up to 180 /s, but tracking is accurate only up to about 30 /s [7].  When shown two fixed light spots, the moving eye jumps from one to the other periodically.  An Oculo-Motor System with Multi-Chip Neuromorphic Analog VLSI Control 715  The relative time spent on each light source depends on their intensity ratio. The duty cycle  has been measured for ratios ranging from 0.1 to 10 (Fig. 5A). It is close to 50% for equal  saliency, and tends toward a ratio of 10 to 1 in favor of the brightest spot at the extremities of  the range. The delay between onset of a saccade and stabilization on the target ranges from  45 ms to lOOms. The delay is not constant because it depends to some extent on saccade  magnitude, and because of occasional mechanical slipping at the onset. In humans, the  duration of saccades tends to be proportional to their amplitude, and ranges between 25 ms  and 200 ms.  mo  lOO  80  60  40  20  o  o.1  saccades duty cycle  1  saliency ratio  10  go  . 60   50  E 40  "' 30  = 20  0 10   0   0.1  background observation time  10  100  spot intensity / total background intensity [%]  Figure 5' Measured data plots. A. Gaze time sharing between two salient spots versus  saliency ratio. B. Gaze time on background versus spot-to-background intensity ratio.  When more than two spots are turned on, the saccadic exploration is not obviously peri-  odic anymore, but the eye keeps spending most time on the light spots, with a noticeable  preference for larger intensities. This behavior is consistent with measurements previously  made on the saccadic control chip alone under electrical stimulation [9]. Saccades towards  locations in the background are rare and brief if the intensity ratio between the light sources  and the background is high enough. This phenomenon has been studied quantitatively by  measuring the fraction of time spent on background locations for different light source in-  tensities (Fig. 5B). The quantity on the horizontal axis of the plot is the ratio between the  total intensity in light spots and the total background intensity. These two quantities are  measured by summing the outputs of wide-angle retina pixels belonging to the light spot  images and to the background respectively. It can be seen that if this ratio is above 1, less  than 10% of the time is spent scanning the background.  Open-loop saccade accuracy has been evaluated by switching off the smooth pursuit mod-  ule, and measuring the error vector between the center of gravity of the light spot and the  center of the narrow-field retina after each saccade, for six different light spots spread over  the field of view. The error vectors were found to be always less than 2 o in magnitude, with  different orientations in each case. Whenever the moving eye returned to a same light spot,  the error vector was the same. This shows that the residual error is not due to random noise,  but to the limited accuracy of visuo-motor mapping within the saccadic control chip. The  magnitude of the error is always low enough that the target light spot is completely visible  by the moving eye, thereby ensuring that the smooth pursuit module can indeed correct the  error when enabled.  4 CONCLUSION  The oculo-motor system described herein performs as intended, thereby demonstrating the  value of a neuromorphic engineering approach in the case of a relatively complex task  involving mechanical and optical components. This system provides an experimental plat-  form for studying active vision, whereby a visual system acts on itself in order to facilitate  perception of its surroundings. Besides saccadic exploration and smooth pursuit, a mov-  716 O. Landolt and S. Gyger  ing eye can be exploited to improve vision in many other ways. For instance, resolution  shortcomings in retinas incorporating only a modest number of pixels can be overcome  by continuously sweeping the field of view back and forth, thereby providing continuous  information in space--although not simultaneously in time. In binocular vision, 3D infor-  mation perception by stereopsis is also made easier if the fields of view can be aligned by  vergence control [12]. Besides active vision, the oculo-motor system also lends itself as  a framework for testing and demonstrating other analog VLSI vision circuits. As already  mentioned, due to its modular architecture, it is possible to insert additional visual pro-  cessing chips either in the saccadic exploration module, or in the smooth pursuit module,  in order to make the current light-source oriented system suitable for operation in natural  visual environments.  Acknowledgments  The authors wish to express their gratitude to all their colleagues at CSEM who contributed  to this work. Special thanks are due to Patrick Debergh for the micro-prism light deflec-  tion concept, to Friedrich Heitger for designing and building the mechanical device, and  to Edoardo Franzi for designing and building the related electronic interface. Thanks are  also due to Arnaud Tisserand, Friedrich Heitger, Eric Vittoz, Reid Harrison, Theron Stan-  ford, and Edoardo Franzi for helpful comments on the manuscript. Mr. Roland Lagger,  from Portescap, La Chaux-de-Fonds, Switzerland, provided friendly assistance in a critical  mechanical assembly step.  References  [1] C. Mead. Analog VLSI and Neural Systems. Addison Wesley, 1989.  [2] T.S. Lande, editor. Neuromorphic Systems Engineering. Kluwer Academic Publishers, Dor-  drecht, 1998.  [3] K. Boahen. Retinomorphic vision systems II: Communication channel design. In IEEE lnt.  Syrup. Circuits and Systems (ISCAS'96), Atlanta, May 1996.  [4] A. Mortara, E. Vittoz, and P. Venier. A communication scheme for analog VLSI perceptive  systems. IEEE Journal of Solid-State Circuits, 30, June 1995.  [5] C.M. Higgins. Multi-chip neuromorphic motion processing. In Conference on Advanced Re-  search in VLSI, Atlanta, March 1999.  [6] T.K. Horiuchi, B. Bishofberger, and C. Koch. An analog VLSI saccadic eye movement system.  In Advances in Neural Processing Systems 6, 1994.  [7] T.K. Horiuchi. Analog VLSI-Based, Neuromorphic Sensorimotor Systems: Modeling the Pri-  mate Oculomotor System. PhD thesis, Caltech, Pasadena, 1997.  [8] P. Venier. A constrast sensitive silicon retina based on conductance modulation in a diffu-  sion network. In 6th lnt. Conf. Microelectronics for Neural Networks and Fuzzy Systems (Mi-  croNeuro'97), Dresden, Sept 1997.  [9] O. Landolt. Place Coding in Analog VLSI - A Neuromorphic Approach to Computation. Kluwer  Academic Publishers, Dordrecht, 1998.  [10] TG. Morris and S.P. DeWeerth. Analog VLSI excitatory feedback circuits for attentional shifts  and tracking. Analog Integrated Circuits and Signal Processing, 13, May-June 1997.  [11] O. Landok. Place coding in analog VLSI and its application to the control of a light deflection  system. In MicroNeuro'97, Dresden, Sept 1997.  [12] M. Mahowald. An Analog VLSI System for Stereoscopic Vision. Kluwer Academic Publishers,  Boston, 1994.  
A recurrent model of the interaction between  Prefrontal and Inferotemporal cortex in delay  tasks  ALFONSO RENART, NISTOR PARGA  Departamento de F(sica Te6rica  Universidad Aut6noma de Madrid  Canto Blanco, 28049 Madrid, Spain  http://www. ft.uam.es/neurocienciaJGRUPO/grupo_english.html  and  EDMUND T. ROLLS  Oxford University  Department of Experimental Psychology  South Parks Road, Oxford OX1 3 UD, England  Abstract  A very simple model of two reciprocally connected attractor neural net-  works is studied analytically in situations similar to those encountered  in delay match-to-sample tasks with intervening stimuli and in tasks of  memory guided attention. The model qualitatively reproduces many of  the experimental data on these types of tasks and provides a framework  for the understanding of the experimental observations in the context of  the attractor neural network scenario.  1 Introduction  Working memory is usually defined as the capability to actively hold information in mem-  ory for short periods of time. In primates, visual working memory is usually studied in  experiments in which, after the presentation of a given visual stimulus, the monkey has  to withhold its response during a certain delay period in which no specific visual stimulus  is shown. After the delay, another stimulus is presented and the monkey has to make a  response which depends on the interaction between the two stimuli. In order to bridge the  temporal gap between the stimuli, the first one has to be held in memory during the delay.  Electrophysiological recordings in primates during the performance of this type of tasks  has revealed that some populations of neurons in different brain areas such as prefrontal  (PF), inferotemporal (IT) or posterior parietal (PP) cortex, maintain approximately con-  stant firing rates during the delay periods (for a review see [ 1]) and this delay activity states  have been postulated as the internal representations of the stimuli provoking them [2]. Al-  though up to now most of the modeling effort regarding the operation of networks able to  support stable delay activity states has been put in the study of uni-modular (homogeneous)  networks, there is evidence that in order for the monkey to solve the tasks satisfactorily, the  interaction of several different neural structures is needed. A number of studies of delay  match-to-sample tasks with intervening stimuli in primates performed by Desimone and  172 A. Renart, N. Parga and E. T. Rolls  colleagues has revealed that although IT cortex supports delay activity states and shows  memory related effects (differential responses to the same, fixed stimulus depending on its  status on the trial, e.g. whether it matches or not the sample), it cannot, by itself, provide  the information necessary to solve the task, as the delay activity states elicited by each of  the stimuli in a sequence are disrupted by the input information associated with each new  stimulus presented [3, 4, 5]. Another structure is therefore needed to store the information  for the whole duration of the trial. PF cortex is a candidate, since it shows selective delay  activity maintained through entire trials even with intervening stimuli [6]. A series of par-  allel experiments by the same group on memory guided attention [7, 8] have also shown  differential firing of IT neurons in response to the same visual stimulus shown after a delay  (an array of figures), depending on previous information shown before the delay (one of  the figures in the array working as a target stimulus). This evidence suggests a distributed  memory system as the proper scenario to study working memory tasks as those described  above. Taking into account that both IT and PF cortex are known to be able to support  delay activity states, and that they are bi-directionally connected, in this paper we propose  a simple model consisting of two reciprocally connected attractor neural networks to be  identified with IT and PF cortex. Despite its simplicity, the model is able to qualitatively  reproduce the behavior of IT and PF cortex during delay match-to-sample tasks with in-  tervening stimuli, the behavior of IT cells during memory guided attention tasks, and to  provide an unified picture of these experimental data in the context of associative memory  and attractor neural networks.  2 Model and dynamics  The model network consists of a large number of (excitatory) neurons arranged in two  modules. Following [9, 10], each neuron is assumed to be a dynamical element which  transforms an incoming afferent current into an output spike rate according to a given  transduction function. A given afferent current Iai to neuron i (i = 1,..., N) in module a  (a - IT, PF) decays with a characteristic time constant T but increases proportionally to  the spike rates vbj of the rest of the neurons in the network (both from inside and outside  its module) connected to it, the contribution of each presynaptic neuron, e.g. neuron j from  module b, being proportional to the synaptic efficacy Jf between the two. This can be  expressed through the following equation  /rai(t) ri(t)  dt T  h(eXt)  An external current --ai from outside the network, representing the stimuli, can also  be imposed on every neuron. Selective stimuli are modeled as proportional to the stored  patterns, i.e. hU? 't) - h u where h, is the intensity of the external current to module a.  '-a -- aai'  The transduction function of the neurons transforming currents into rates has been chosen  as a threshold hyperbolic tangent of gain G and threshold 0.  The synaptic efficacies between the neurons of each module and between the neurons in  different modules are respectively [ 1 l, 12]  P  Jo E(r/a u/-f) (r/aUJ-f) ij ; a=IT, PF  j;,a): f(1- f)Nt =l  (2)  P   E (TIa5 -- f) (TIj -- f) V i, j ; a  b  Jf') = f(1 - f)Nt/--1  (3)  Recurrent Model of lT-PF Interactions in Delay Tasks 173  The intra-modular connections express the learning of P binary patterns {r/,U/= 0, 1, tt =  1,..., P} by each module, each of them signaling which neurons are active in each of  the sustained activity configurations. Each variable r/,Ui is supposed to take the values 1  and 0 with probabilities f and (1 - f) respectively, independently across neurons and  across patterns. The inter-modular connections reflect the temporal associations between  the sustained activity states of each module. In this way, every stored pattern tt in the IT  module has an associated pattern in the PF module which is labelled by the same index.  The normalization constant Nt = N(Jo q- g) has been chosen so that the sum of the  magnitudes of the inter- and the intra-modular connections remains constant and equal to  1 while their relative values are varied. When this constraint is imposed the strength of  the connections can be expressed in terms of a single independent parameter g measuring  the relative intensity of the inter- vs. the intra-modular connections (Jo can be set equal  to I everywhere). We will limit our study to the case where the number of stored patterns  per module P does not increase proportionally to the size of the modules N since a large  number of stored patterns does not seem necessary to describe the phenomenology of the  delay match-to-sample experiments.  Since the number of neurons in a typical network one may be interested in is very large,  e.g.  10 5 - 10 6, the analytical treatment of the set of coupled differential equations (1)  becomes intractable. On the other hand, when the number of neurons is large, a reliable de-  scription of the asymptotic solutions of these equations can be found using the techniques  of statistical mechanics [13, 9]. In this framework, instead of characterizing the states  of the system by the state of every neuron, this characterization is performed in terms of  macroscopic quantities called order parameters which measure and quantify some global  properties of the network as a whole. The relevant order parameters appearing in the de-  scription of our system are the overlaps of the state of each module with each of the stored  patterns m, defined as:  1  = << f).i >>. , (4)  i  where the symbol <<... >>v stands for an average over the stored patterns.  Using the free energy per neuron of the system at zero temperature r (which we do not  write explicitly to reduce the technicalities to a minimum) we have modeled the experi-  ments by giving the order parameters the following dynamics:  TOrn.U = OY (5)  Ot Ore.   This dynamics ensures that the stationary solutions, corresponding to the values of the  order parameters at the attractors, correspond also to minima of the free energy, and that,  as the system evolves, the free energy is always minimized through its gradient. The time  constant of the macroscopic dynamics is a free parameter which has been chosen equal to  the time constant of the individual neurons, reflecting the assumption that neurons operate  in parallel. Its value has been set to 7- = 10 ms. Equations (5) have been solved by a  simple discretizing procedure (first order Runge-Kutta method).  Since not all neurons in the network receive the same inputs, not all of them behave in  the same way, i.e. have the same firing rates. In fact, the neurons in each of the module  can be split into different sub-populations according to their state of activity in each of  the stored patterns. The mean firing rate of the neurons in each sub-population depends  on the particular state realized by the network (characterized by the values of the order  parameters). Associated to each pattern there are two larger sub-populations, to be denoted  as foreground (all active neurons) and background (all inactive neurons) of that pattern.  174 A. Renart, N. Parga and E. T. Rolls  The overlap with a given pattern can be expressed as the difference between the mean firing  rate of the neurons in its foreground and its background. The average is performed over all  other sub-populations to which each neuron in the foreground (background) may belong  to, where the probability of a given sub-population is equal to the fraction of neurons in  the module belonging to it (determined by the probability distribution of the stored patterns  as given above). This partition of the neurons into sub-populations is appealing since, in  experiments, cells are usually classified in terms of their response properties to a set of  fixed stimuli, i.e. whether each stimulus is effective or ineffective in driving their response.  The modeling of the different experiments proceeded according to the macroscopic dynam-  ics (5), where each stimulus was implemented as an extra current for a desired period of  time.  3 Sequence with intervening stimuli  In order to study delay match-to-sample tasks with intervening stimuli [5, 6], the module  to be identified with IT was sequentially stimulated with external currents proportional to  some of the stored patterns with a delay between them. To take into account the large  fraction of PF neurons with non-selective responses to the visual stimuli (which may be  involved in other aspects of the task different from the identification of the stimuli), and  since the neurons in our modules are, by definition, stimulus selective (although they are  probably connected to the non-selective neurons) a constant, non-selective current of the  same intensity as the selective input to the IT module was applied (during the same time)  equally to all sub-populations of the PF module. The external current to the IT module was  stimulus selective because the fraction of IT neurons with non-selective responses to the  visual stimuli is very small [6]. The results can be seen in Figure 1 where the sequence  ABA with A as the sample stimulus and B as a non-matching stimulus has been studied.  The values of the model parameters are listed in the caption. In Figure 1 a, the mean firing  rates of the foreground populations of patterns AIT and BIT of the IT module have been  plotted as a function of time. The main result is that, as observed in the experiments, the  delay activity in the IT module is determined by the last stimulus presented. The delay  activity provoked by a given stimulus is disrupted by the next, unless it corresponds to the  same stimulus, in which case the effect of the stimulus is to increase the firing rate of the  neurons in its foreground. We have checked that no noticeable effects occur if more non-  matching stimuli are presented (they are all equivalent with respect to the sample) or if a  non-match stimulus is repeated.  If the coupling g between the modules is weak enough [12] the behavior in the PF module  is different. This can be seen in Figure lb, where the time evolution of the mean firing rates  of the foreground of the two associated patterns ApF and BpF stored in the PF module are  shown. In agreement with the findings of Desimone and colleagues, the neurons in the  PF module remain correlated with the sample for the whole trial, despite the non-selective  signal received by all PF neurons (not only those in the foreground of the sample) and the  fact that the selective current from the IT module tends to activate the pattern associated  with the current stimulus.  Desimone and colleagues [5, 6] report that the response of some neurons (not necessarily  those with sample selective delay activity or with stimulus selective responses) in both IT  and PF cortex to some stimuli, is larger if those stimuli are matches in their trials than if  the same stimuli are non-matches. This has been denoted as match enhancement. In the  present scenario the explanation is straightforward: when a stimulus is a non-match, IT and  PF are in different states and therefore send inconsistent signals to each other. The firing  rate of the neurons of each module is maintained in that case solely by the contribution  to the total current coming from the recurrent collaterals. On the other hand, when the  stimulus is the match, both modules find themselves in states associated in the synapses  Recurrent Model of lT-PF Interactions in Delay Tasks 175  0.8  0.7  0.6  0.5  0,4.  0.3  0.2  0.1  0 1 2 3 4 5 6 7 8  Time (s)  0.7  0.6  0.5  0.4.  0.3  0.2  0.1  L ...............  ',... ............... j ",._. ..............  0 1 2 3 4 5 6 7 8 9  Time (s)  Figure 1: (a) Mean rams in the foreground of patterns AIT (solid line) and BIT (dashed  line) in the IT module as a function of time. (b) Same but for patterns ApF and BpF of the  PF module. Model parameters are G = 1.3, 0 = 10 -3, f = 0.2, g = 10 -2, h = 0.13.  Stimuli are presented during 500 ms at seconds 0, 3, and 6 following the sequence ABA.  between the neurons connecting them, PF because it has remained that way the whole trial,  and IT because it is driven by the current stimulus. When this happens, the contribution  to the total current from the recurrent collaterals and from the long range afferents add up  consistently, and the firing rate increases. In order for this explanation to hold there should  be a correlation between the top-down input from PF and the sensory bottom-up signal to  IT. Indeed, experimental evidence for such a correlation has very recently been found [ 14].  This is an important experimental finding which supports our theory.  Looking at Figure 1, one sees that the effect is not evident in the model during the time  of stimulus presentation, which is the period where it has been reported. The effect is, in  fact, present, although its magnitude is too small to be noticeable in the figure. We would  argue, however, that this quantitative difference is an artifact of the model. This is because  the enhancement effect is very noticeable on the delay periods, where essentially the same  neurons are active as during the stimulus presentations (i.e., where the same correlations  between the top-down and bottom-up signals exist) but with lower firing rates. During  stimulus presentations the firing rates are closer to the saturation regime, and therefore the  dynamical response range of the neurons is largely reduced.  4 Memory guided attention  To test the differential response of cells as a function of the contents of memory, we have  followed [7, 8] and studied a sub-population of IT cells which are simultaneously in the  foreground of one of the patterns (AIT) and in the background of another (BIT) in the same  conditions as the previous section (same model parameters). In Figure 2a the response  of this sub-population as a function of time has been plotted in two different situations.  In the first one, the effective stimulus AIT was shown first (throughout this section non  selective stimulation of PF proceeded as in the last section) and after a delay, a stimulus  array equal to the sum of AIT and BIT was presented. The second situation is exactly  equal, except for the fact that the cue stimulus shown first was the ineffective stimulus BIT.  The response of the same sub-population to the same stimulus array is totally different and  determined by the cue stimulus: If the sub-population is in the background of the cue, its  response is null during the trial except for the initial period of the presentation of the array.  In accordance with the experimental observations [7, 8], its response grows initially (as  one would expect, since during the array presentation time, stimulation is symmetric with  176 A. Renart, N. Parga and E. T. Rolls  respect of A and B) but is later suppressed by the top-down signal being sent by the PF  module. This suppression provides a clear example of a situation in which the contents  of memory (in the form of an active PF activity state) are explicitly gating the access of  sensory information to IT, implementing a non-spatial attentional mechanism.  0.8  0.7  0,6  0.5  0.4  0.3  0.2  0.1  0  -0.1  0.7  0.6  0.5  0.4  0.3  0.2  0.1  1 2 3 4 5 6 0 I 2 3  me (s) me (s)  4 5 6  Figure 2: (a) Mean rates as a function of time in IT neurons which are both in the fore-  ground AIT and in the background of BIT when the cue stimulus is AIT (solid line) or BIT  (dashed line). (b) Mean rates of the same neurons when CiT is the cue stimulus and the  array is AIT alone (long dashed line), BiT alone (short dashed line) or the sum of AiT and  BT (solid line). Cue present until 500 ms. Array present from 3000 ms to 3500 ms.  Model parameters as in Figure 1  In the model, the PF module remains in a state correlated with the cue during the whole  trial (to our knowledge there are no measurements of PF activity during memory guided  attention tasks) and therefore provides a persistent signal 'in the direction' of the cue which  biases the competition between AIT and BIT established at the onset of the array. This  is how the gating mechanism is implemented. The competitive interactions between the  stimuli in the array are studied in Figure 2b, which is an emulation of the target-absent trials  of [8]. In this figure, the same sub-population is studied under situations in which the cue  stimulus is not present in the array (another one of the stored patterns, i.e. CIT) The three  curves correspond to different arrays: The effective stimulus alone, the ineffective stimulus  alone, and a sum of the two as in the previous experiment. In all three, the PF module  remains in a sustained activity state correlated with CT the whole trial and therefore, since  the patterns are independent, the signal it sends to IT is symmetric with respect of A and  B. Thus, the response of the sub-population during the array is in this case unbiased, and  the effect of the competitive interactions can be isolated. The result is that, as observed  experimentally, the response to the complex array is intermediate between the one to the  effective stimulus alone and the one to the ineffective stimulus alone. The nature of the  competition in an attractor network like the one under study here is based on the fact that  complex stimulus combinations are not stored in the recurrent collaterals of each module.  These connections tend to stabilize the individual patterns which, being independent, tend  to cancel each other when presented together. After the array is presented, the state of the  IT module, which is correlated with CiT in the initial delay, becomes correlated with AiT  or BiT if they are presented alone. When the array contains both of them in a symmetric  fashion, since the sum of the patterns is not a stored pattern itself, the IT module remains  correlated with pattern CIT due to the signal from the PF module.  Recurrent Model of lT-PF Interactions in Delay Tasks 177  5 Discussion  We have proposed a toy model consisting of two reciprocally connected attractor mod-  ules which reproduces nicely experimental observations regarding intra-trial data in delay  match-to-sample and memory guided attention experiments in which the interaction be-  tween IT and PF cortex is relevant. Several important issues are taken into account in the  model: a complex interaction between the PF and IT modules resultant from the associa-  tion of frequent patterns of activity in both modules, delay activity states in each module  which exert mutually modulatory influences on each other, and a common substrate (we  emphasize that the results on Sections 3 and 4 where obtained with exactly the same model  parameters, just by changing the type of task) for the explanation of apparently diverse  phenomena. Perception is clearly an active process which results from the complex in-  teractions between past experience and incoming sensory information. The main goal of  this model was to show that a very simple associational (Hebbian) pattern of connectivity  between a perceptual module and a 'working memory' module can provide the basic in-  gredients needed to explain coherently different experimentally found neural mechanisms  related to this process. The model has clear limitations in terms of 'biological realism'  which will have to be improved in order to use it to make quantitative predictions and com-  parisons, and does not provide a complete an exhaustive account of the very complex and  diverse phenomena in which temporo-frontal interactions are relevant (there is, for exam-  ple, the issue of how to reset PF activity in between trials [15]). However, it is precisely the  simplicity of the mechanism it provides and the fact that it captures the essential features of  the experiments, despite being so simple, what makes it likely that it will remain relevant  after being refined.  Acknowledgements  This work was funded by a Spanish grant PB96-0047. We acknowledge the Max Planck  Institute for Physics of Complex Systems in Dresden, Germany, for the hospitality received  by A.R. and N.P. during the meeting held there from March 1 to 26, 1999.  References  [11 J.M.  [2] D.J.  [3] G.C.  [41 E.K.  [5] E.  [6] E.  [71 L.  [8] L.  [91  Fuster. Memory in the cerebral cortex. Cambridge, MA: MIT Press (1995)  Amit. Behavioral and Brain Sciences 18, 617-657 (1995)  Baylis & E. T. Rolls. Exp. Brain Res. 65, 614-622 (1987)  Miller, L. Li & R. Desimone. J. Neurosci. 13, 1460-1478 (1993)  K. Miller & R. Desimone. Science 263, 520-522 (1994)  K. Miller, C. A. Erickson & R. Desimone. J. Neurosci. 16, 5154-5167 (1996)  Chelazzi, E. K. Miller, J. Duncan & R. Desimone. Nature 363, 345-347 (1993)  Chelazzi, J. Duncan, E. K. Miller & R. Desireone. J. Neurophysiol. 80, 2918-2940 (1998)  R. Kuhn. In Statistical Mechanics of Neural Networks. (ed. L. Garrido), 19-32. Berlin:  Springer-Verlag (1990)  [10] D.J. Amit & M. V. Tsodyks. Network 2, 259-273 (1991)  [11] A. Renart, N. Parga & E. T. Rolls. Neural Computation 11, 1349-1388 (1999).  [12] A. Renart, N. Parga & E. T. Rolls. Network 10, 237-255 (1999).  [13] M. Mezard, G. Parisi & M. Virasoro. Spin glass theory and beyond. Singapore: World Scientific  (1987)  [14] H. Tomita, M Ohbayashi, K. Nakahara, I. Hasegawa & Y. Miyashita. Nature 401, 699-703  (1999)  [15] D. Durstewitz, M. Kelc & O. Giinttirktin. J. Neurosci. 19, 2807-2822 (1999)  
Uniqueness of the SVM Solution  Christopher J.C. Burges  Advanced Technologies,  Bell Laboratories,  Lucent Technologies  Holmdel, New Jersey  burges lucent. corn  David J. Crisp  Centre for Sensor Signal and  Information Processing,  Deptartment of Electrical Engineering,  University of Adelaide, South Australia  dcrisp eleceng. adelaide. edu. au  Abstract  We give necessary and sufficient conditions for uniqueness of the  support vector solution for the problems of pattern recognition and  regression estimation, for a general class of cost functions. We show  that if the solution is not unique, all support vectors are necessarily  at bound, and we give some simple examples of non-unique solu-  tions. We note that uniqueness of the primal (dual) solution does  not necessarily imply uniqueness of the dual (primal) solution. We  show how to compute the threshold b when the solution is unique,  but when all support vectors are at bound, in which case the usual  method for determining b does not work.  1 Introduction  Support vector machines (SVMs) have attracted wide interest as a means to imple-  ment structural risk minimization for the problems of classification and regression  estimation. The fact that training an SVM amounts to solving a convex quadratic  programming problem means that the solution found is global, and that if it is not  unique, then the set of global solutions is itself convex; furthermore, if the objec-  tive function is strictly convex, the solution is guaranteed to be unique [1] x. For  quadratic programming problems, convexity of the objective function is equivalent  to positive semi-definiteness of the Hessian, and strict convexity, to positive definite-  ness [1]. For reference, we summarize the basic uniqueness result in the following  theorem, the proof of which can be found in [1]:  Theorem 1: The solution to a convex programming problem, for which the ob-  jective function is strictly convex, is unique. Positive definiteness of the Hessian  implies strict convexity of the objective function.  Note that in general strict convexity of the objective function does not neccesarily  imply positive definiteness of the Hessian. Furthermore, the solution can still be  unique, even if the objective function is loosely convex (we will use the term "loosely  convex" to mean convex but not strictly convex). Thus the question of uniqueness  XThis is in contrast with the case of neural nets, where local minima of the objective  function can occur.  224 C. J. C. Burges and D. J. Crisp  for a convex programming problem for which the objective function is loosely convex  is one that must be examined on a case by case basis. In this paper we will give  necessary and sufficient conditions for the support vector solution to be unique,  even when the objective function is loosely convex, for both the clasification and  regression cases, and for a general class of cost function.  One of the central features of the support vector method is the implicit mapping   of the data z  Rn to some feature space 5, which is accomplished by replacing  dot products between data points zi, zj, wherever they occur in the train and test  algorithms, with a symmetric function K(zi, zj), which is itself an inner product in   [2]: K(zi, zj) -- ((I)(zi), (I)(zj)) -- (xi,xj), where we denote the mapped points in   by x - (z). In order for this to hold the kernel function K must satisfy Mercer's  positivity condition [3]. The algorithms then amount to constructing an optimal  separating hyperplane in 5, in the pattern recognition case, or fitting the data to a  linear regression tube (with a suitable choice of loss function [4]) in the regression  estimation case. Below, without loss of generality, we will work in the space 5,  whose dimension we denote by dr. The conditions we will find for non-uniqueness  of the solution will not depend explicitly on  or .  Most approaches to solving the support vector training problem employ the Wolfe  dual, which we describe below. By uniqueness of the primal (dual) solution, we  mean uniqueness of the set of primal (dual) variables at the solution. Notice that  strict convexity of the primal objective function does not imply strict convexity of  the dual objective function. For example, for the optimal hyperplane problem (the  problem of finding the maximal separating hyperplane in input space, for the case  of separable data), the primal objective function is strictly convex, but the dual  objective function will be loosely convex whenever the number of training points  exceeds the dimension of the data in input space. In that case, the dual Hessian  H will necessarily be positive semidefinite, since H (or a submatrix of H, for the  cases in which the cost function also contributes to the (block-diagonal) Hessian) is a  Gram matrix of the training data, and some rows of the matrix will then necessarily  be linearly dependent [5] 2 . In the cases of support vector pattern recognition and  regression estimation studied below, one of four cases can occur: (1) both primal  and dual solutions are unique; (2) the primal solution is unique while the dual  solution is not; (3) the dual is unique but the primal is not; (4) both solutions  are not unique. Case (2) occurs when the unique primal solution has more than  one expansion in terms of the dual variables. We will give an example of case (3)  below. It is easy to construct trivial examples where case (1) holds, and based on  the discussion below, it will be clear how to construct examples of (4). However,  since the geometrical motivation and interpretation of SVMs rests on the primal  variables, the theorems given below address uniqueness of the primal solution s .  2 The Case of Pattern Recognition  We consider a slightly generalized form of the problem given in [6], namely to  minimize the objective function  F = (1/2)[[w[[ 2 +  Cif (1)  i  aRecall that a Gram matrix is a matrix whose ij'th element has the form {xi,xj} for  some inner product {, ), where xi is an element of a vector space, and that the rank of a  Gram matrix is the maximum number of linearly independent vectors xi that appear in it  [6].  aDue to space constraints some proofs and other details will be omitted. Complete  details will be given elsewhere.  Uniqueness of the SVM Solution 225  with constants p  [1, cw), Ci > 0, subject to constraints:  yi(w ' xi + b) _ 1 - i, i = 1,...,l (2)  i k 0, i=l,---,l (3)  where w is the vector of weights, b a scalar threshold, i are positive slack variables  which are introduced to handle the case of nonseparable data, the Yi are the polar-  ities of the training samples (Yi 6 (+1)), xi are the images of training samples in  the space  by the mapping , the Ci determine how much errors are penalized  (here we have allowed each pattern to have its own penalty), and the index i labels  the I training patterns. The goal is then to find the values of the primal variables  (w, b, i) that solve this problem. Most workers choose p = 1, since this results in  a particularly simple dual formulation, but the problem is convex for any p _ 1.  We will not go into further details on support vector classification algorithms them-  selves here, but refer the interested reader to [3], [7]. Note that, at the solution, b  is determined from w and i by the Karush Kuhn Tucker (KKT) conditions (see  below), but we include it in the definition of a solution for convenience.  Note that Theorem i gives an immediate proof that the solution to the optimal  hyperplane problem is unique, since there the objective function is just (1/2)l[w[I 2,  which is strictly convex, and the constraints (Eq. (2) with the  variables removed)  are linear inequality constraints which therefore define a convex set 4.  For the discussion below we will need the dual formulation of this problem, for the  case p = 1. It takes the following form: minimize  Eij OtiOtjYiYj{Xi,Xj) -- -].i Oti  subject to constraints:  _Y 0, alP_0 (4)  Ci = -i + m (5)  ) = o (6)  i  and where the solution takes the form w = -']-i otiYiXi, and the KKT conditions,  which are satisfied at the solution, are rlii = O, oq(yi(w.xi + b) - 1 + i) = 0, where  r/i are Lagrange multipliers to enforce positivity of the i, and ci are Lagrange  multipliers to enforce the constraint (2). The r/i can be implicitly encapsulated  in the condition 0 _ oi _ Ci, but we retain them to emphasize that the above  equations imply that whenever i  0, we must have ci - Ci. Note that, for a  given solution, a support vector is defined to be any point xi for which ci > 0. Now  suppose we have some solution to the problem (1), (2), (3). Let Af denote the set  {i: Yi = 1, w 'xi + b < 1}, Af2 the set {i: Yi = -1, w 'xi + b > -1}, JV'a the set  (i: Yi = 1, w.xi + b = 1), Af4 the set (i: Yi = -1, w. xi + b = -1), JV'5 the set  (i: yi = 1, w.xi +b  1), and Af6 the set (i: yi = -1, w.xi + b (-1). Then we  have the following theorem:  Theorem 2: The solution to the soft-margin problem, (1), (2) and (3), is unique  for p > 1. For p = 1, the solution is not unique if and only if at least one of the  following two conditions holds:  c, = (7)  Furthermore, whenever the solution is not unique, all solutions share the same w,  and any support vector xi has Lagrange multiplier satisfying cq = Ci, and when (7)  4This is of course not a new result: see for example [3].  226 C. J. C. Burges and D. J.. Crisp  holds, then Afa contains no support vectors, and when (8) holds, then iV'4 contains  no support vectors.  Proof: For the case p > 1, the objective function F is strictly convex, since a  sum of strictly convex functions is a strictly convex function, and since the function  g(v) - v p, v  + is strictly convex for p > 1. Furthermore the constraints define  a convex set, since any set of simultaneous linear inequality constraints defines a  convex set. Hence by Theorem i the solution is unique.  For the case p = 1, define z to be that dr +/-component vector with zi - wi, i -  1,..., dF, and zi - i, i - d, + 1,...,d, + I. In terms of the variables z, the  problem is still a convex programming problem, and hence has the property that  any solution is a global solution. Suppose that we have two solutions, z and  z2. Then we can form the family of solutions zt, where zt -- (1 - t)zx + tz2, and  since the solutions are global, we have F(z) = F(z2) = F(zt). By expanding  F(zt) - F(z) = 0 in terms of z and z2 and differentiating twice with respect to t  we find that wx = w2. Now given w and b, the i are completely determined by the  KKT conditions. Thus the solution is not unique if and only if b is not unique.  Define 5  min {minieAr  i, minieAr6 (-1 - w.xi - b)}, and suppose that condition  (7) holds. Then a different solution {w', b','} is given by w' = w, b' = b +  and i = i - 5, Vi  Aft, i = i + 5, Vi  iV'2 uAf4, all other i = 0, since by  construction F then remains the same, and the constraints (2), (3) are satisfied  by the primed variables. Similarly, suppose that condition (8) holds. Define 5 --  min{minief 2 i,minie5(w .xi + b- 1)}. Then a different solution {w',b','} is  given byw' = w, b' = b-5, and i =  all other i = 0, since again by construction F is unchanged and the constraints  are still met. Thus the given conditions are sufficient for the solution to be non-  unique. To show necessity, assume that the solution is not unique: then by the  above argument, the solutions must differ by their values of b. Given a particular  solution b, suppose that b + 5, 5 > 0 is also a solution. Since the set of solutions is  itself convex, then b + 5  will also correspond to a solution for all 5 : 0 _< 5  _< 5.  Given some b  = b + 5 , we can use the KKT conditions to compute all the i, and  we can choose 5' sufficiently small so that no i, i 6 iV'6 that was previously zero  becomes nonzero. Then we find that in order that F remain the same, condition  (7) must hold. If b - 5, 5 > 0 is a solution, similar reasoning shows that condition  (8) must hold. To show the final statement of the theorem, we use the equality  constraint (6), together with the fact that, from the KKT conditions, all support  vectors xi with indices in Af UJV'2 satisfy ai = Ci. Substituting (6) in (7) then gives  Y-a ai + Y-4 (Ci - ai) = 0 which implies the result, since all ai are non-negative.  Similarly, substituting (6) in (8) gives Y-fa (el - cq) + y,f4 ci = 0 which again  implies the result. Cl  Corollary: For any solution which is not unique, letting $ denote the set of indices  of the corresponding set of support vectors, then we must have Y-i$ CiYi - O.  Furthermore, if the number of data points is finite, then for at least one of the  family of solutions, all support vectors have corresponding i  0.  Note that it follows from the corollary that if the Ci are chosen such that there  exists no subset 7' of the train data such that Y-iT CiYi -' O, then the solution is  guaranteed to be unique, even ifp = 1. Furthermore this can be done by choosing all  the Ci very close to some central value C, although the resulting solution can depend  sensitively on the values chosen (see the example immediately below). Finally, note  that if all Ci are equal, the theorem shows that a necessary condition for the solution  to be non-unique is that the negative and positive polarity support vectors be equal  in number.  Uniqueness of the SVM Solution 22 7  A simple example of a non-unique solution, for the case p = 1, is given by a train set  in one dimension with just two examples, (xl = 1, yx = 1} and (x2 = -1,y2 = -1,  with C1 = C  C. It is straightforward to show analytically that for C _ 5,  the solution is unique, with w = 1, ix =  = b = 0, and margin 5 equal to 2,   there is a family of solutions, with -1 + 2C < b < i - 2C and  while for C < 5 _ _  x corresponds to   =1-b-2C, = l+b-2C, and margin l/C. ThecaseC<   Case (3) in Section (1) (dual unique but primal not), since the dual variables are  uniquely specified by c - C. Note also that this family of solutions also satisfies  the condition that any solution is smoothly deformable into another solution [7].  If Cx  C, the solution becomes unique, and is quite different from the unique  solution found when C2  C. When the C's are not equal, one can interpret  what happens in terms of the mechanical analogy [8], with the central separating  hyperplane sliding away from the point that exerts the higher force, until that point  lies on the edge of the margin region.  Note that if the solution is not unique, the possible values of b fall on an interval  of the real line: in this case a suitable choice would be one that minimizes an  estimate of the Bayes error, where the SVM output densities are modeled using a  validation set 6. Alternatively, requiring continuity with the cases p  1, so that one  would choose that value of b that would result by considering the family of solutions  generated by different choices of p, and taking the limit from above of p - 1, would  again result in a unique solution.  3 The Case of Regression Estimation 7  Here one has a set of I pairs (xl,y),(x,y),...,(x,y), (xi  .T',yi  , and  the goal is to estimate the unknown functional dependence ] of the y on the x,  where the function f is assumed to be related to the measurements {xi,Yi} by  Yi ---- ](Xi)+hi, and where ni represents noise. For details we refer the reader to [3],  [9]. Again we generalize the original formulation [10], as follows: for some choice of  positive error penalties el, and for positive el, minimize  i .  F- Ilwll + +c; (9)  with constant p  [1, ), subject to constrnts  yi-w'xi-b  ei+i (10)  w'xi+b-yi 5 ei+ (11)  *)  0 (12)  where we have adopted the notation *)  (i, ) [9]. This formulation results in  an % insensitive" loss function, that is, there is no penalty (*) = 0) sociated  with point xi if [Yi - w 'xi - bl  el. Now let fl, fl* be the Lagrange multipliers  introduced to enforce the constraints (10), (11). The dual then gives  = o 5 C,, o 5 5 (13)  i i  5The margin is defined to be the distance between the two hyperplanes corresponding  to equality in Eq. (2), namely 2/11wll , and the margin region is defined to be the set of  points between the two hyperplanes.  6This method was used to estimate b under similar circumstances in [8].  The notation in this section only coincides with that used in section 2 where convenient.  228 C. ,J.. C. Burges and D. d. Crisp  which we will need below. For this formulation, we have the following  Theorem 3: For a given solution, define f(Xi, Yi) ---- Yi -- W ' Xi -- b, and define  to be the set of indices {i: f(xi, Yi) > el}, iV'2 the set {i: f(xi, Yi) -- el}, JV'a the set  {i: f(xi,Yi) = -el}, and JV'a the set {i: f(xi,Yi) < -el}. Then the solution to (9)  - (12) is unique for p > 1, and for p = I it is not unique if and only if at least one  of the following two conditions holds:  ieu  (14)  (15)  Furthermore, whenever the solution is not unique, all solutions share the same w,  and all support vectors are at bound (that is s, either/i = Ci or/ = C), and  when (14) holds, then JV'a contains no support vectors, and when (15) holds, then  iV'2 contains no support vectors.  The theorem shows that in the non-unique case one will only be able to move the  tube (and get another solution) if one does not change its normal w. A trivial  example of a non-unique solution is when all the data fits inside the e-tube with  room to spare, in which case for all the solutions, the normal to the e-tubes always  lies along the y direction. Another example is when all Ci are equal, all data falls  outside the tube, and there are the same number of points above the tube as below  it.  4 Computing b when all SVs are at Bound  The threshold b in Eqs. (2), (10) and (11) is usually determined from that sub-  set of the constraint equations which become equalities at the solution and for  which the corresponding Lagrange multipliers are not at bound. However, it may  be that at the solution, this subset is empty. In this section we consider the sit-  uation where the solution is unique, where we have solved the optimization prob-  lem and therefore know the values of all Lagrange multipliers, and hence know  also w, and where we wish to find the unique value of b for this solution. Since  the *) are known once b is fixed, we can find b by finding that value which  both minimizes the cost term in the primal Lagrangian, and which satisfies all  the constraint equations. Let us consider the pattern recognition case first. Let  $+ (S_) denote the set of indices of positive (negative) polarity support vectors.  Also let V+ (V_) denote the set of indices of positive (negative) vectors which are  not support vectors. It is straightforward to show that if '-i$_ Ci > Y-ie$+ Ci,  then b = max{maxims_ (-1 -w.xi), maxiv+(1 -w.xi)}, while if Y-i$_ Ci <  Y-i$+ Ci, then b = min{mini$+(1-w.xi), miniv_(-1-w.xi)}. Further-  more, if Y-i$_ Ci = Y-i$+ Ci, and if the solution is unique, then these two values  coincide.  In the regression case, let us denote by $ the set of indices of all sup-  port vectors,  its complement, S the set of indices for which /i = Ci,  and S2 the set of indices for which / = Ct, so that S = S U S2 (note  S1 r"l S 2 -- 0). Then if Y-ies2 Ct > Y'-ies C, the desired value of b is  b = max {maxies(Yi - w. xi + el), maxie$(yi - w- xi - el)} while if Y-ies2 C <  Y-i& Ci, then b = min{mini$(yi-w.xi-ei), miniB(yi-w.xi +ei)}.  SRecall that if ei > O, then 3i/[ = O.  Uniqueness of the SVM Solution 229  Again, if the solution is unique, and if also '-ies2 C = '-ies Ci, then these  two values coincide.  5 Discussion  We have shown that non-uniqueness of the SVM solution will be the exception rather  than the rule: it will occur only when one can rigidly parallel transport the margin  region without changing the total cost. If non-unique solutions are encountered,  other techniques for finding the threshold, such as minimizing the Bayes error arising  from a model of the SVM posteriors [8], will be needed. The method of proof in the  above theorems is straightforward, and should be extendable to similar algorithms,  for example Mangasarian's Generalized SVM [11]. In fact one can extend this result  to any problem whose objective function consists of a sum of strictly convex and  loosely convex functions: for example, it follows immediately that for the case of the  v-SVM pattern recognition and regression estimation algorithms [12], with arbitrary  convex costs, the value of the normal w will always be unique.  Acknowledgments  C. Burges wishes to thank W. Keasler, V. Lawrence and C. Nohl of Lucent Tech-  nologies for their support.  References  [1] R. Fletcher. Practical Methods of Optimization. John Wiley and Sons, Inc., 2nd  edition, 1987.  [2] B. E. Boser, I. M. Guyon, and V .Vapnik. A training algorithm for optimal margin  classifiers. In Fifth Annual Workshop on Computational Learning Theory, Pittsburgh,  1992. ACM.  [3] V. Vapnik. Statistical Learning Theory. John Wiley and Sons, Inc., New York, 1998.  [4] A.J. Smola and B. SchSlkopf. On a kernel-based method for pattern recognition,  regression, approximation and operator inversion. Algorithmica, 22:211 - 231, 1998.  [5] Roger A. Horn and Charles R. Johnson. Matrix Analysis. Cambridge University Press,  1985.  [6] C. Cortes and V. Vapnik. Support vector networks. Machine Learning, 20:273-297,  1995.  [7] C.J.C. Burges. A tutorial on support vector machines for pattern recognition. Data  Mining and Knowledge Discovery, 2(2):121-167, 1998.  [8] C. J. C. Burges and B. SchSlkopf. Improving the accuracy and speed of support vector  learning machines. In M. Mozer, M. Jordan, and T. Petsche, editors, Advances in Neural  Information Processing Systems 9, pages 375-381, Cambridge, MA, 1997. MIT Press.  [9] A. Smola and B. SchSlkopf. A tutorial on support vector regression. Statistics and  Computing, 1998. In press: also, COLT Technical Report TR-1998-030.  [10] V. Vapnik, S. Golowich, and A. Smola. Support vector method for function approx-  imation, regression estimation, and signal processing. Advances in Neural Information  Processing Systems, 9:281-287, 1996.  [11] O.L. Mangarasian. Generalized support vector machines, mathematical programming  technical report 98-14. Technical report, University of Wisconsin, October 1998.  [12] B. SchSlkopf, A. Smola, R. Williamson and P. Bartlett, New Support Vector Algo-  rithms, NeuroCOLT2 NC2-Tl-1998-031, 1998.  
A Geometric Interpretation of v-SVM  Classifiers  David J. Crisp  Centre for Sensor Signal and  Information Processing,  Deptartment of Electrical Engineering,  University of Adelaide, South Australia  dcrisp @eleceng. adelaide. edu. au  Christopher J.C. Burges  Advanced Technologies,  Bell Laboratories,  Lucent Technologies  Holmdel, New Jersey  burges@lucent. com  Abstract  We show that the recently proposed variant of the Support Vector  machine (SVM) algorithm, known as y-SVM, can be interpreted  as a maximal separation between subsets of the convex hulls of the  data, which we call soft convex hulls. The soft convex hulls are  controlled by choice of the parameter y. If the intersection of the  convex hulls is empty, the hyperplane is positioned halfway between  them such that the distance between convex hulls, measured along  the normal, is maximized; and if it is not, the hyperplane's normal  is similarly determined by the soft convex hulls, but its position  (perpendicular distance from the origin) is adjusted to minimize  the error sum. The proposed geometric interpretation of y-SVM  also leads to necessary and sufficient conditions for the existence of  a choice of y for which the y-SVM solution is nontrivial.  I Introduction  Recently, Sch51kopf et al. [1] introduced a new class of SVM algorithms, called  y-SVM, for both regression estimation and pattern recognition. The basic idea is to  remove the user-chosen error penalty factor C that appears in SVM algorithms by  introducing a new variable p which, in the pattern recognition case, adds another  degree of freedom to the margin. For a given normal to the separating hyperplane,  the size of the margin increases linearly with p. It turns out that by adding p to  the primal objective function with coefficient -y, y _ 0, the variable C can be  absorbed, and the behaviour of the resulting SVM - the number of margin errors  and number of support vectors - can to some extent be controlled by setting y.  Moreover, the decision function produced by y-SVM can also be produced by the  original SVM algorithm with a suitable choice of C.  In this paper we show that y-SVM, for the pattern recognition case, has a clear  geometric interpretation, which also leads to necessary and sufficient conditions for  the existence of a nontrivial solution to the y-SVM problem. All our considerations  apply to feature space, after the mapping of the data induced by some kernel. We  adopt the usual notation: w is the normal to the separating hyperplane, the mapped  Geometric Interpretation of u-SVM Classifiers 24.5  data is denoted by xi 6 N, i = 1,.-., l, with corresponding labels yi  {+l}, b, p  are scalars, and Ei, i = 1,-- -, l are positive scalar slack variables.  2 v-SVM Classifiers  The v-SVM formulation, as given in [1], is as follows: minimize  F' 1 1 ,  - 11w'11  - p' + ?  E,  with respect to w', b', p', E'i, subject to:  (1)  y(w'.x + b') _ p' - El, El _ 0, p' _ 0. (.)  Here v is a user-chosen parameter between 0 and 1. The decision function (whose  sign determines the label given to a test point x) is then:  /'(x) = o'.  + b'. (s)  1  The Wolfe dual of this problem is: maximize F b = - ]ij aiajyiyxi  x5 subject  to  O_oti _ 1, _otiYi=O, Eoti_Y  i i  (4)  with w' given by w' = '-i aiyixi. Sch51kopf et al. [1] show that v is an upper  bound on the fraction of margin errors x, a lower bound on the fraction of support  vectors, and that both of these quantities approach v asymptotically.  Note that the point w' = b' = p = E = 0 is feasible, and that at this point, F' = 0.  Thus any solution of interest must have F' _ 0. Furthermore, if vp' = 0, the  optimal solution is at w' = b' = p = E = 02. Thus we can assume that vp' > 0 (and  therefore u > 0) always. Given this, the constraint p' _ 0 is in fact redundant: a  negative value of p' cannot appear in a solution (to the problem with this constraint  removed) since the above (feasible) solution (with p' = 0) gives a lower value for  F'. Thus below we replace the constraints (2) by  y(w'.x + ') _ p' - El, El _ o.  (5)  2.1 A Reparameterization of v-SVM  We reparameterize the primal problem by dividing the objective function F' by  v2/2, the constraints (5) by , and by making the following substitutions:  2 w' ' p' El  ,= , = -, o=-, p=-, E =-. (6)  XA margin error xi is defined to be any point for which i > 0 (see [1]).  Sin fact we can prove that, even if the optimal solution is not unique, the global  solutions still all have w - 0: see Burges and Crisp, "Uniqueness of the SVM Solution" in  this volume.  246 D. J. Crisp and C. J. C. Burges  This gives the equivalent formulation: minimize  F = Ilwll 2 - 2p +   e,  i  with respect to w, b, p, i, subject to:  (7)  y(w.x + b) _> p- ,  ) 0. (8)  If we use as decision function f(x) -- f(X)/l, the formulation is exactly equivalent,  although both primal and dual appear different. The dual problem is now: minimize  1  FD =  Z aiajyiyjxi . xj (9)  i,j  with respect to the ai, subject to:  Z ciYi = O' Z ai = 2' O _< ai <_ l a (10)  i i  with w given by w =  Y]-i iYiXi. In the following, we will refer to the reparam-  eterized version of ,-SVM given above as p-SVM, although we emphasize that it  describes the same problem.  3 A Geometric Interpretation of .SVM  In the separable case, it is clear that the optimal separating hyperplane is just that  hyperplane which bisects the shortest vector joining the convex hulls of the positive  and negative polarity points a. We now show that this geometric interpretation can  be extended to the case of ,-SVM for both separable and nonseparable cases.  3.1 The Separable Case  We start by giving the analysis for the separable case. The convex hulls of the two  classes are  and  H+ = {  aixi  i: yl --'- I  i:yi=--I  Z ai=l, ai_>0} (11)  i:yi=+l  Z ai=l, ai>_0}. (12)  i:yl =-- 1  Finding the two closest points can be written as the following optimization problem:  m2n E tixi-- Z li3i (13)  i: Yl -----]- 1 i: Yl ---- -- 1  3See, for example, K. Bennett, 1997, in http://www.rpi.edu/Dennek/svmtalk.ps (also,  to appear).  A Geometric Interpretation ofu-SVM Classifiers 247  subject to:   ai=l, ' ai=l, a>_O (14)  i:yi=+l i:yl------1  Taking the decision boundary f(x) = w. x +  = 0 to be the perpendicular bisector  of the line segment joining the two closest points means that at the solution,  and  = -w.p, where  1  w-- ( Z oqxi- Z aixi) (15)  i:yi----+l i:yi------1  Thus w lies along the line segment (and is half its size) and p is the midpoint of the  line segment. By rescaling the objective function and using the class labels yi -- +1  we can rewrite this as4:  subject to  1  mjn [[w[[ 2 =  ZaiaJYiYJXi .xj (17)  ij  Y]. aiyi = 0, E ai = 2, ai _> 0.  i i  1  The associated decision function is f(x) = w. x +  where w =  Y']-i otiYiXi,  I 1  P --  Ei Otiggi and  = -w.p = - ']4j otiYiOtj ggi ' ggJ'  (18)  3.2 The Connection with v-SVM  Consider now the two sets of points defined by:  H+,={ Z aixi Z ai=l,  i:yi=+l i:yi=+l  0 <_ ai <_ } (19)  and  H-,={ Z aixi  i:yi=--I  Z ai = 1,  i:yl------1  o < < (20)  We have the following simple proposition:  Proposition 1: H+. C H+ and H_. C H_, and H+. and H_. are both convex  sets. Furthermore, the positions of the points H+. and H_. with respect to the xi  do not depend on the choice of origin.  Proof.' Clearly, since the ai defined in H+. is a subset of the ai defined in H+,  H+. C H+, similarly for H_. Now consider two points in H+. defined by ax,  Then all points on the line joining these two points can be written as Y]-i:u,=+x ((1 -  ,)Otli -{- ,Ot2i)ggi, 0 _  _ 1. Since ai and a2i both satisfy 0 _ ai _ p, so does  (1-A)axi+Aa2i, and since also '.i:y=+x (1-A)axi+Aa2i = 1, the set H+. is convex.  4That one can rescale the objective function without changing the constraints follows  from uniqueness of the solution. See also Burges and Crisp, "Uniqueness of the SVM  Solution" in this volume.  248 D. . Crisp and C. J. C. Burges  The argument for H_ is similar. Finally, suppose that every xi is translated by  xo, i.e. xi -- xi + xo Vi. Then since -'-i:y,--+ ai - 1, every point in H+ is also  translated by the same amount, similarly for H_. I2  The problem of finding the optimal separating hyperplane between the convex sets  H+ and H_ then becomes:  subject to  1  main Hw[[ 2 -   oqajyiyjxi .xj (21)  ij  (22)  i i  Since Eqs. (21) and (22) are identical to (9) and (10), we see that the ,-SVM  algorithm is in fact finding the optimal separating hyperplane between the convex  sets H+ and H_. We note that the convex sets H+ and H_ are not simply  uniformly scaled versions of H+ and H_. An example is shown in Figure 1.  1/3  xl  x3  !  p.=!/3  5112  !/6 '  x2  xl  !/3 !  x3  !  --5/! 2  xl  1/6 5/12 !  x3  1/2 !  x2  !  /4  !/4  xl  x3  !/4 3/4 !  Figure 1: The soft convex hull for the vertices of a right isosceles triangle, for  various p. Note how the shape changes as the set grows and is constrained by the  boundaries of the encapsulating convex hull. For/z  , the set is empty.  Below, we will refer to the formulation given in this section as the soft convex hull  formulation, and the sets of points defined in Eqs. (19) and (20) as soft convex  hulls.  3.3 Comparing the Offsets and Margin Widths  The natural value of the offset  in the soft convex hull approach,  = -w .p, arose  by asking that the separating hyperplane lie halfway between the closest extremities  of the two soft convex hulls. Different choices of b just amount to hyperplanes with  the same normal but at different perpendicular distances from the origin. This  value of b will not in general be the same as that for which the cost term in Eq. (7)  is minimized. We can compare the two values as follows. The KKT conditions for  the p-SVM formulation are  - = 0  a(y(w . + b) - p + = 0  Multiplying (24) by Yi, summing over i and using (23) gives  (23)  (24)  A Geometn'c Interpretation ofu-SVM Classifiers 249  (25)  Thus the separating hyperplane found in the/z-SVM algorithm sits a perpendicular  distance 121111 >-]4 yiil away from that found in the soft convex hull formulation.  For the given w, this choice of b results in the lowest value of the cost,/z ]-i i-  The soft convex hull approach suggests taking 5 = w  w, since this is the value  Ill takes at the points >-.y,=+x aixi and >-.y,=_x aixi. Again, we can use the KKT  conditions to compare this with p. Summing (24) over i and using (23) gives  p = + f,. (26)  i  Since 5 = w- w, this again shows that if p = 0 then w = i = O, and, by (25), b - O.  3.4 The Primal for the Soft Convex Hull Formulation  By substituting (25) and (26) into the/z-SVM primal formulation (7) and (8) we  obtain the primal formulation for the soft convex hull problem: minimize  P- I1112-  with respect to w, , 5, , subject to:  (27)  yi(w ' xi + ) > 5- i + I z E I + YiYj j,  - 2  o. (28)  It is straightforward to check that the dual is exactly (9) and (10). Moreover, by  summing the relevant KKT conditions, as above, we see that  - -w.p and 5 - w.w.  Note that in this formulation the variables i retain their meaning according to (8).  4 Choosing v  In this section we establish some results on the choices for v, using the /z-SVM  formulation. First, note that --i aiyi = 0 and '-i ci = 2 implies -]4:y,=+ ci -  -]-i:v,=- ci = 1. Then ci _> 0 gives ci <_ 1, Vi. Thus choosing/z  1, which  corresponds to choosing v ( 2/l, results in the same solution of the dual (and hence  the same normal w) as choosing/z = 1. (Note that different values of/z  I can  still result in different values of the other primal variables, e.g. b).  The equalities 5-]-i:v,=+ ci = 5-]-i:w=- ci = 1 also show that if/z < 2/l then the  feasible region for the dual is empty and hence the problem is insoluble. This  corresponds to the requirement y < 1. However, we can improve upon this. Let l+  (l_) be the number of positive (negative) polarity points, so that l+ + l_ = l. Let  lmin ---- min{/+, l_ }. Then the minimal value of/z which still results in a nonempty  feasible region is IZmin -- 1/lmin. This gives the condition v <_ 21min/l.  We define a "nontrivial" solution of the problem to be any solution with w  0.  The following proposition gives conditions for the existence of nontrivial solutions.  250 D. J. Crisp and C. J. C. Burges  Proposition 2: A value of v exists which will result in a nontrivial solution to  the v-SVM classification problem if and only if {H+: p = P,in} r3 {H_: t =  min } = 0.  Proof: Suppose that {H+: p = P,i} rn {H_: t = P,i}  0. Then for all  allowable values of p (and hence u), the two convex hulls will intersect, since {H+:  t = t,} C {H+: t _> t,i} and {H_: t = t,n} C {H_: t _> t,i}. If  the two convex hulls intersect, then the solution is trivial, since by definition there  then exist feasible points z such that z = Y'i:ui=+ OtiXi and z = i:i=_ oti2i ,  and hence 2w = EiotiYiXi = Ei:yi=+l tixi- Ei:yi=-i otixi = 0 (cf. (21), (22).  Now suppose that {H+: p = P,i} r3 {H_: / = P,i} = 0. Then clearly a  nontrivial solution exists, since the shortest distance between the two convex sets  (H+: p = P,i) and (H_: p = P,in) is not zero, hence the corresponding  w0. D  Note that when l+ = l_, the condition amounts to the requirement that the centroid  of the positive examples does not coincide with that of the negative examples. Note  also that this shows that, given a data set, one can find a lower bound on u, by  finding the largest p that satisfies H_u Cl H+u = 0.  5 Discussion  The soft convex hull interpretation suggests that an appropriate way to penalize  positive polarity errors differently from negative is to replace the sum p -i i in (7)  with p+ i:=+ i + P- -].i:=_, i. In fact one can go further and introduce a p  for every train point. The p-SVM formulation makes this possibility explicit, which  it is not in original u-SVM formulation.  Note also that the fact that ,-SVM leads to values of b which differ from that which  would place the optimal hyperplane halfway between the soft convex hulls suggests  that there may be principled methods for choosing the best b for a given problem,  other than that dictated by minimizing the sum of the i's. Indeed, originally, the  sum of i's term arose in an attempt to approximate the number of errors on the  train set [2]. The above reasoning in a sense separates the justification for w from  that for b. For example, given w, a simple line search could be used to find that  value of b which actually does minimize the number of errors on the train set. Other  methods (for example, minimizing the estimated Bayes error [3]) may also prove  useful.  Acknowledgments  C. Burges wishes to thank W. Keasler, V. Lawrence and C. Nohl of Lucent Tech-  nologies for their support.  References  [1] B. Sch51kopf and A. Smola and R. Williamson and P. Bartlett. New support vector  algorithms, neurocolt2 nc2-tr-1998-031. Technical report, GMD First and Australian  National University, 1998.  [2] C. Cortes and V. Vapnik. Support vector networks. Machine Learning, 20:273-297,  1995.  [3] C. J. C. Burges and B. Sch51kopf. Improving the accuracy and speed of support vector  learning machines. In M. Mozer, M. Jordan, and T. Petsche, editors, Advances in Neural  Information Processing Systems 9, pages 375-381, Cambridge, MA, 1997. MIT Press.  
Neural System Model of Human Sound  Localization  Craig T. Jin  Department of Physiology and  Department of Electrical Engineering,  Univ. of Sydney, NSW 2006, Australia  Simon Carlfie  Department of Physiology  and Institute of Biomedical Research,  Univ. of Sydney, NSW 2006, Australia  Abstract  This paper examines the role of biological constraints in the human audi-  tory localization process. A psychophysical and neural system modeling  approach was undertaken in which performance comparisons between  competing models and a human subject explore the relevant biologi-  cally plausible "realism constraints". The directional acoustical cues,  upon which sound localization is based, were derived from the human  subject's head-related transfer functions (HRTFs). Sound stimuli were  generated by convolving bandpass noise with the HRTFs and were pre-  sented to both the subject and the model. The input stimuli to the model  was processed using the Auditory Image Model of cochlear processing.  The cochlear data was then analyzed by a time-delay neural network  which integrated temporal and spectral information to determine the spa-  tial location of the sound source. The combined cochlear model and  neural network provided a system model of the sound localization pro-  cess. Human-like localization performance was qualitatively achieved  for broadband and bandpass stimuli when the model architecture incor-  porated frequency division (or tonotopicity), and was trained using vari-  able bandwidth and center-frequency sounds.  1 Introduction  The ability to accurately estimate the location of a sound source has obvious evolutionary  advantages in terms of avoiding predators and finding prey. Indeed, humans are very accu-  rate in their ability to localize broadband sounds. There has been a considerable amount of  psychoacoustical research into the auditory processes involved in human sound localization  (recent review [ 1 ]). Furthermore, numerous models of the human and animal sound local-  ization process have been proposed (recent reviews [2, 3]). However, there still remains a  large gap between the psychophysical and the model explanations. Principal congruence  between the two approaches exists for localization performance under restricted conditions,  such as for narrowband sounds where spectral integration is not required, or for restricted  regions of space. Unfortunately, there is no existing computational model that accounts  well for human sound localization performance for a wide-range of sounds (e.g., vary-  ing in bandwidth and center-frequency). Furthermore, the biological consmints pertinent  to sound localization have generally not been explored by these models. These include  the spectral resolution of the auditory system in terms of the number and bandwidth of  762 C. T. Jin and S. Carlile  frequency channels and the role of tonotopic processing. In addition, the performance re-  quirements of such a system are substantial and involve, for example, the accomodation of  spectrally complex sounds, the robustness to irregularity in the sound source spectrum, and  the channel based structure of spatial coding as evidenced by auditory spatial after-effects  [4]. The crux of the matter is the notion that "biologically-likely realism", if built into a  model, provides for a better understanding of the underlying processes.  This work attempts to bridge part of this gap between the modeling and psychophysics. It  describes the development and use (for the first time, to the authors' knowledge) of a time-  delay neural network model that integrates both spectral and temporal cues for auditory  sound localization and compares the performance of such a model with the corresponding  human psychophysical evidence.  2 Sound Localization  The sound localization performance of a normal hearing human subject was tested using  stimuli consisting of three different band-passed sounds: (1) a low-passed sound (300 -  2000 Hz) (2) a high-passed sound (2000 - 14000 Hz) and (3) a broadband sound (300 -  14000 Hz). These frequency bands respectively cover conditions in which either temporal  cues, spectral cues, or both dominate the localization process (see [ 1 ]). The subject per-  formed five localization trials for each sound condition, each with 76 test locations evenly  distributed about the subject's head. The detailed methods used in free-field sound local-  ization can be found in [5]. A short summary is presented below.  2.1 Sound Localization Task  Human sound localization experiments were carried out in a darkened anechoic cham-  ber. Free-field sound stimuli were presented from a loudspeaker carried on a semicircular  robotic arm. These stimuli consisted of "fresh" white Gaussian noise appropriately band-  passed for each trial. The robotic ann allowed for placement of the speaker at almost any  location on the surface of an imaginary sphere, one meter in radius, centered on the sub-  ject's head. The subject indicated the location of the sound source by pointing his nose in  the perceived direction of the sound. The subject's head orientation was monitored using  an electromagnetic sensor system (Polhemus, Inc.).  2.2 Measurement and Validation of Outer Ear Acoustical Filtering  The cues for sound localization depend not only upon the spectral and temporal proper-  ties of the sound stimulus, but also on the acoustical properties of the individual's outer  ears. It is generally accepted that the relevant acoustical cues (i.e., the interaural time dif-  ference, ITD; interaural level difference, ILD; and spectral cues) to a sound's location in  the free-field are described by the head-related transfer function (HRTF) which is typically  represented by a finite-length impulse response (FIR) filter [1]. Sounds filtered with the  HRTF should be localizable when played over ear-phones which bypass the acoustical fil-  tering of the outer ear. The illusion of free-field sounds using head-phones is known as  virtual auditory space (VAS).  Thus in order to incorporate outer ear filtering into the modelling process, measurements  of the subject's HRTFs were carried out in the anechoic chamber. The measurements were  made for both ears simultaneously using a "blocked ear" technique [ 1 ]. 393 measurements  were made at locations evenly distributed on the sphere. In order to establish that the  HRTFs appropriately indicated the direction of a sound source the subject repeated the  localization task as above with the stimulus presented in VAS.  Neural System Model of Human Sound Localization 763  2.3 Human Sound Localization Performance  The sound localization performance of the human subject in three different stimulus con-  ditions (broadband, high-pass, low-pass) was examined in both the free-field and in vir-  tual auditory space. Comparisons between the two (using correlational statistics, data not  shown, but see [3]) across all sound conditions demonstrated their equivalence. Thus the  measured HRTFs were highly effective.  Localization data across all three sound conditions (single trial VAS data shown in Fig. la)  shows that the subject performed well in both the broadband and high-pass sound condi-  tions and rather poorly in the low-pass condition, which is consistent with other studies [6].  The data is illustrated using spherical localization plots which well demonstrates the global  distribution of localization responses. Given the large qualitative differences in the data  sets presented below, this visual method of analysis was sufficient for evaluating the com-  peting models. For each condition, the target and response locations are shown for both the  left (L) and right (R) hemispheres of space. It is clear that in the low-pass condition, the  subject demonstrated gross mislocalizations with the responses clustering toward the lower  and frontal hemispheres. The gross mislocalizations correspond mainly to the traditional  cone of confusion errors [6].  3 Localization Model  The sound localization model consisted of two basic system components: (1) a modi-  fied version of the physiological Auditory Image Model [7] which simulates the spectro-  temporal characteristics of peripheral auditory processing, and (2) the computational archi-  tecture of a time-delay neural network. The sounds presented to the model were filtered  using the subject's HRTFs in exactly the same manner as was used in producing VAS.  Therefore, the modeling results can be compared with human localization performance on  an individual basis.  The modeling process can be broken down into four stages. In the first stage a sound  stimulus was generated with specific band-pass characteristics. The sound stimulus was  then filtered with the subject's right and left ear HRTFs to render an auditory stimulus  originating from a particular location in space. The auditory stimulus was then processed  by the Auditory Image Model (AIM) to generate a neural activity profile that simulates  the output of the inner hair cells in the organ of Corti and indicates the spiking probability  of auditory nerve fibers. Finally, in the fourth and last stage, a time-delay neural network  (TDNN) computed the spatial direction of the sound input based on the distribution of  neural activity calculated by AIM.  A detailed presentation of the modeling process can be found in [3], although a brief sum-  mary is presented here. The distribution of cochlear filters across frequency in AIM was  chosen such that the minimum center frequency was 300 Hz and the maximum center fre-  quency was 14 kHz with 31 filters essentially equally spaced on a logarithmic scale. In  order to fully describe a computational layer of the TDNN, four characteristic numbers  must be specified: (1) the number of neurons; (2) the kernel length, a number which de-  termines the size of the current layer's time-window in terms of the number of time-steps  of the previous layer; (3) the kernel width, a number which specifies how many neurons  in the previous layer with which there are actual connections; and (4) the undersampling  factor, a number describing the multiplicative factor by which the current layer's time-step  interval is increased from the previous layer's. Using this nomenclature, the architecture  of the different layers of one TDNN is summarized in Table 1, with the smaIlest time-step  being 0.15 ms. The exact connection arrangement of the network is described in the next  section.  764 C. T. Jin and S. Carlde  Layer  Table 1: The Architecture of the TDNN.  Neurons Kernel Length Kernel Width Undersampling  Input 62 -- m __  Hidden 1 50 15 6 2  Hidden 2 28 10 4,5,6 2  Output 393 4 28 I  The spatial location of a sound source was encoded by the network as a distributed response  with the peak occurring at the output neuron representing the target location of the input  sound. The output response would then decay away in the form of a two-dimensional  Gaussian as one moves to neurons fiarther away from the target location. This derives from  the well-established paradigm that the nervous system uses overlapping receptive fields to  encode properties of the physical world.  3.1 Networks with Frequency Division and Tonotopicity  The major auditory brainstem nuclei demonstrate substantial frequency division within  their structure. The tonotopic organization of the primary auditory nerve fibers that in-  nervate the cochlea carries forward to the brainstem's auditory nuclei. This arrangement  is described as a tonotopic organization. Despite this fact and to our knowledge, no pre-  vious network model for sound localization incorporates such frequency division within  its architecture. Typically (e.g., [8]) all of the neurons in the first computational layer are  fully connected to all of the input cochlear frequency channels. In this work, different ar-  chitectures were examined with varying amounts of frequency division imposed upon the  network structure. The network with the architecture described above had its network con-  nections constrained by frequency in a tonotopic like arrangement. The 31 input cochlear  frequency channels for each ear were split into ten overlapping groups consisting generally  of six contiguous frequency channels. There were five neurons in the first hidden layer for  each group of input channels. The kernel widths of these neurons were set, not to the total  number of frequency channels in the input layer, but only to the six contiguous frequency  channels defining the group. Information across the different groups of frequency channels  was progressively integrated in the higher layers of the network.  3.2 Network Training  Sounds with different center-frequency and bandwidth were used for training the networks.  In one particular training paradigm, the center-frequency and bandwidth of the noise were  chosen randomly. The center-frequency was chosen using a uniform probability distri-  bution on a logarithmic scale that was similar to the physiological distribution of output  frequency channels from AIM. In this manner, each frequency region was trained equally  based on the density of neurons in that frequency region. During training, the error back-  propagation algorithm was used with a summed squared error measure. It is a natural  feature of the learning rule that a given neuron's weights are only updated when there is  activity in its respective cochlear channels. So, for example, a training sound containing  only low frequencies will not train the high-frequency neurons and vice versa. All model-  ing results correspond with a single tonotopically organized TDNN trained using random  sounds (unless explicitly stated otherwise).  Neural System Model of Human Sound Localization 765  4 Localization Performance of a Tonotopic Network  Experimentation with the different network architectures clearly demonstrated that a net-  work with frequency division vastly improved the localization performance of the TDNNs  (Figure 1). In this case, frequency division was essential to producing a reasonable neural  system model that would localize similarly to the human subject across all of the different  band-pass conditions. For any single band-pass condition, it was found that the TDNN  did not require frequency division within its architecture to produce quality solutions when  trained only on these band-passed sounds.  As mentioned above it was observed that a tonotopic network, one that divides the input fre-  quency channels into different groups and then progressively interconnects the neurons in  the higher layers across frequency, was more robust in its localization performance across  sounds with variable center-frequency and bandwidth than a simple fully connected net-  work. There are two likely explanations for this observation. One line of reasoning argues  that it was easier for the tonotopic network to prevent a narrow band of frequency chan-  nels from dominating the localization computation across the entire set of sound stimuli.  Or expressed slightly differently, it may have been easier for it to incorporate the relevant  information across the different frequency channels. A second line of reasoning argues that  the tonotopic network structure (along with the training with variable sounds) encouraged  the network to develop meaningful connections for all frequencies.  (a) SUBJECT VAS  Broadband  Low-Pass  L R  (b) TONOTOPIC  NETWORK  (c) NETWORK without  FREQUENCY DIVISION  .......  Dot Response location  4,:-"  '. ,.:, ' :' "',  L R L R  Figure 1: Comparison of the subject's VAS localization performance and the model's lo-  calization performance both with and without frequency division. The viewpoint is from  an outside observer, with the target location shown by a cross and the response location  shown by a black dot.  766 C. T. din and S. Carlile  5 Matched Filtering and Sound Localization  A number of previous sound localization models have used a relatively straight-forward  matched filter or template matching analysis [9]. In such cases, the ITD and spectrum  of a given input sound is commonly cross-correlated with the ITD and spectrum of an  entire database of sounds for which the location is known. The location with the highest  correlation is then chosen as the optimal source location.  Matched filtering analysis is compared with the localization performance of both the hu-  man subject and the neural system model using a bandpass sound with restricted high-  frequencies (Figure 2). The matched filtering localizes the sounds much better than the sub-  ject or the TDNN model. The matched filtering model used the same number of cochlear  channels as the TDNNs and therefore contained the same inherent spectral resolution. This  spectral resolution (31 cochlear channels) is certainly less than the spectral resolution of the  human cochlea. This shows that although there was sufficient information to localize the  sounds from the point of view of matched filtering, neither the human nor TDNN demon-  strated such ability in their performance. In order for the TDNN to localize similarly to  the matched filtering model, the network weights corresponding to a given location need  to assume the form of the filter template for that location. As all of the training sounds  were flat-spectrum, the TDNN received no ambiguity as far as the source spectrum was  concerned. Thus it is likely that the difference in the distribution of localization responses  in Figure 2b, as compared with that in Figure 2c, has been encouraged by using training  sounds with random center-frequency and bandwidth, providing a partial explanation as to  why the human localization performance is not optimal from a matched filtering standpoint.  Band-pass (7.6 - 13.3 kHz)  (a) Subiect  (c) Matched Filter Model  L  Figure 2: Comparison of the localization performances of the subject, the TDNN model,  and a matched filtering model. Details as in Fig. 1.  6 Varying Sound Levels and the ILD Cue  The training of the TDNNs was performed in such a fashion, that for any particular location  in space, the sound level (67 dB SPL) did not vary by more than 1 dB SPL during repeated  presentations of the sound. The localization performance of the neural system model was  then examined, using a broadband sound source, across a range of sound levels varying  from 60 dB SPL to 80 dB SPL. The spherical correlation coefficient between the target  and response locations ([ 10], values above 0.8 indicate "high" correlation) remained above  0.8 between 60 and 75 dB SPL demonstrating that there was a graceful degradation in  localization performance over a range in sound level of 15 dB.  The network was also tested on broadband sounds, 10 dB louder in one ear than the other.  The results of these tests are shown in Figure 3 and clearly illustrate that the localization  responses were pulled toward the side with the louder sound. While the magnitude of this  effect is certainly not human-like, such behaviour suggests that interaural level difference  Neural System Model of Human Sound Localization 767  cues were a prominent and constant feature of the data that conferred a measure of robust-  ness to sound level variations.  (a) Left Ear:  10 dB Level In  L  ,ease  (b) Right Ear:  10 dB Level Increase  L R  (c) Both Ears:  10 dB Level In  L  Figure 3: Model's localization performance with a 10 dB increase in sound level:  (a,b) monaurally, (c) binaurally.  7 Conclusions  A neural system model was developed in which physiological constraints were imposed  upon the modeling process: (1) a TDNN model was used to incorporate the important  role of spectral-temporal processing in the auditory nervous system, (2) a tonotopic struc-  ture was added to the network, (3) the training sounds contained randomly varying center-  frequencies and bandwidths. This biologically plausible model provided increased under-  standing of the role that these constraints play in determining localization performance.  Acknowledgments  The authors thank Markus Schenkel and Andr6 van Schaik for valuable comments. This  research was supported by the NHMRC, ARC, and a Dora Lush Scholarship to CJ.  References  [l]  [2]  [3]  [4]  [5]  [6]  [7]  [8]  [9]  [10]  S. Carlile, Virtual auditory space: Generation and applications. New York: Chapman and  Hall, 1996.  R. H. Gilkey and T. R. Anderson, Binaural and Spatial Hearing in real and virtual environ-  ments. Mahwah, New Jersey: Lawrence Erlbaum Associates, Publishers, 1997.  C. Jin, M. Schenkel, and S. Carlile, "Neural system identification model of human sound local-  isation," (Submitted to d.. Acoust. Soc. Am.), 1999.  S. Hyams and S. Carlile, "After-effects in auditory localization: evidence for channel based  processing," Submitted to the J.. Acoust. Soc. Am., 2000.  S. Carlile, P. Leong, and S. Hyams, "The nature and distribution of errors in the localization of  sounds by humans," Hearing Research, vol. 114, pp. 179-196, 1997.  S. Carlile, S. Delaney, and A. Corderoy, "The localization of spectrally restricted sounds by  human listeners," Hearing Research, vol. 128, pp. 175-189, 1999.  C. Gigure and P. C. Woodland, "A computational model of the auditory periphery for speech  and hearing research. i. ascending path," J. Acoust. Soc. Am., vol. 95, pp. 331-342, 1994.  C. Neti, E. Young, and M. Schneider, "Neural network models of sound localization based on  directional filtering by the pinna," d.. Acoust. Soc. Am., vol. 92, no. 6, pp. 3140-3156, 1992.  J. Middlebrooks, "Narrow-band sound localization related to external ear acoustics," J.. Acoust.  Soc. Am., vol. 92, no. 5, pp. 2607-2624, 1992.  N. Fisher, I, T. Lewis, and B. J. J. Embleton, Statistical analysis of spherical data. Cambridge:  Cambridge University Press, 1987.  
Information Capacity and Robustness of  Stochastic Neuron Models  Elad Schneidman Idan Segev Naftali Tishby  Institute of Computer Science,  Department of Neurobiology and  Center for Neural Computation,  Hebrew University  Jerusalem 91904, Israel   elads, tishby) @cs.huji. ac. il, idan@lobster. ls.huji. ac.il  Abstract  The reliability and accuracy of spike trains have been shown to  depend on the nature of the stimulus that the neuron encodes.  Adding ion channel stochasticity to neuronal models results in a  macroscopic behavior that replicates the input-dependent reliabili-  ty and precision of real neurons. We calculate the amount of infor-  mation that an ion channel based stochastic Hodgkin-Huxley (HH)  neuron model can encode about a wide set of stimuli. We show that  both the information rate and the information per spike of the s-  tochastic model are similar to the values reported experimentally.  Moreover, the amount of information that the neuron encodes is  correlated with the amplitude of fluctuations in the input, and less  so with the average firing rate of the neuron. We also show that for  the HH ion channel density, the information capacity is robust to  changes in the density of ion channels in the membrane, whereas  changing the ratio between the Na + and K + ion channels has a  considerable effect on the information that the neuron can encode.  Finally, we suggest that neurons may maximize their information  capacity by appropriately balancing the density of the different ion  channels that underlie neuronal excitability.  I Introduction  The capacity of neurons to encode information is directly connected to the nature  of spike trains as a code. Namely, whether the fine temporal structure of the spike  train carries information or whether the fine structure of the train is mainly noise  (see e.g. [1, 2]). Experimental studies show that neurons in vitro [3, 4] and in vivo  [5, 6, 7], respond to fluctuating inputs with repeatable and accurate spike trains,  whereas slowly varying inputs result in lower repeatability and 'jitter' in the spike  timing. Hence, it seems that the nature of the code utilized by the neuron depends  on the input that it encodes [3, 6].  Recently, we suggested that the biophysical origin of this behavior is the stochas-  Capacity and Robustness of Stochastic Neuron Models 179  ticity of single ion channels. Replacing the average conductance dynamics in the  Hodgkin-Huxley (HH) model [8], with a stochastic channel population dynamics  [9, 10, 11], yields a stochastic neuron model which replicates rather well the spike  trains' reliability and precision of real neurons [12]. The stochastic model also shows  subthreshold oscillations, spontaneous and missing spikes, all observed experimen-  tally. Direct measurement of membranal noise has also been replicated successfully  by such stochastic models [13]. Neurons use many tens of thousands of ion channels  to encode the synaptic current that reaches the soma into trains of spikes [14]. The  number of ion channels that underlies the spike generation mechanism, and their  types, depend on the activity of the neuron [15, 16]. It is yet unclear how such  changes may affect the amount and nature of the information that neurons encode.  Here we ask what is the information encoding capacity of the stochastic HH mod-  el neuron and how does this capacity depend on the densities of different of ion  channel types in the membrane. We show that both the information rate and the  information per spike of the stochastic HH model are similar to the values reported  experimentally and that neurons encode more information about highly fluctuat-  ing inputs. The information encoding capacity is rather robust to changes in the  channel densities of the HH model. Interestingly, we show that there is an optimal  channel population size, around the natural channel density of the HH model. The  encoding capacity is rather sensitive to changes in the distribution of channel types,  suggesting that changes in the population ratios and adaptation through channel  inactivation may change the information content of neurons.  2 The Stochastic HH Model  The stochastic HH (SHH) model expands the classic HH model [8], by incorporating  the stochastic nature of single ion channels [9, 17]. Specifically, the membrane  voltage dynamics is given by the HH description, namely,  dV  m dt -- --]L(V-- VL) -]K(V,t)(V- VK) -]Na(V,t)(V- VNa) q-I (1)  where V is the membrane potential, VL, VK and V/v. are the reversal potentials of  the leakage, potassium and sodium currents, respectively, gL, gK (V, t) and gN.(V, t)  are the corresponding ion conductances, Cm is the membrane capacitance and I is  the injected current. The ion channel stochasticity is introduced by replacing the  equations describing the ion channel conductances with explicit voltage-dependent  Markovian kinetic models for single ion channels [9, 10]. Based on the activation  and inactivation variables of the deterministic HH model, each K + channel can be  in one of five different states, and the rates for transition between these states are  given in the following diagram,  4on 3on 2on  no ,-- nl ,-- n2 ,-- n3  , 2, 3, 4,  [n4]  (2)  where [nj] refers to the number of channels which are currently in the state nj.  Here [n4] labels the single open state of a potassium channel, and an,/, are the  voltage-dependent rate-functions in the HH formalism. A similar model is used for  the Na + channel (The Na + kinetic model has 8 states, with only one open state,  see [12] for details).  The potassium and sodium membrane conductances are given by,  ilK(V, t) -' q'K [n4] gNa(V, t) = 3'V. [mahx] (a)  where ')'K and 3';v. are the conductances of an ion channel for the K + and Na + re-  spectively. We take the conductance of a single channel to be 20pS [14] for both the  180 E. Schneidman, I. Segev and N. Tishby  K + and Na + channel types i Each of the ion channels will thus respond stochas-  tically by closing or opening its 'gates' according to the kinetic model, fluctuating  around the average expected behavior. Figure I demonstrates the effect of the ion  A B  time [sec]  1.5 1.6 1.7 1.8 1.9 2  time [sec]  C ,oo, D 1.2  o  o  S 10 15 20 S 10 15 20  DC input {i.A/cm 2} DC input [14A/cm 2}  Figure 1: Reliability of firing patterns in a model of an isopotential Hodgkin-Huxley  membrane patch in response to different current inputs. (A) Injecting a slowly changing  current input (low-pass Gaussian white noise with a mean r/= 8 laA/cm 2, and standard  deviation a = 1 laA/cm 2 which was convolved with an 'alpha-function' with a time constant  T = 3 msec, top frame), results in high 'jitter' in the timing of the spikes (raster plots  of spike responses, bottom frame). (B) The same patch was again stimulated repeatedly,  with a highly fluctuating stimulus (r/= 8 laA/cm 2, er -- 7 laA/cm 2 and - = 3 msec, top  frame) The 'jitter' in spike timing is significantly smaller in B than in A (i.e. increased  reliability for the fluctuating current input). Patch area used was 200/am 2 , with 3,600 K +  channels and 12,000Na + channels. (Compare to Fig. 1 in see [3]). (C) Average firing  rate in response to DC current input of both the HH and the stochastic HH model. (D)  Coecient of variation of the inter spike interval of the SHH model in response to DC  inputs, giving values which are comparable to those observed in real neurons  channel stochasticity, showing the response of a 200/m 2 SHH isopotential mem-  brane patch (with the 'standard' SHH channel densities) to repeated presentation  of suprathreshold current input. When the same slowly varying input is repeatedly  presented (Fig. 1A), the spike trains are very different from each other, i.e., spike  firing time is unreliable. On the other hand, when the input is highly fluctuat-  ing (Fig. lB), the reliability of the spike timing is relatively high. The stochastic  model thus replicates the input-dependent reliability and precision of spike trains  observed in pyramidal cortical neurons [3]. As for cortical neurons, the Repeatability  and Precision of the spike trains of the stochastic model (defined in [3]) are strongly  correlated with the fluctuations in the current input and may get to sub-millisecond  precision [12]. The f-I curve of the stochastic model (Fig. 1C) and the coefficient of  variation (CV) of the inter-spike intervals (ISI) distribution for DC inputs (Fig. 1D)  are both similar to the behavior of cortical neurons in vivo [18], in clear contrast to  the deterministic model 2   The number of channels is thus the ratio between the total conductance of a single type  of ion channels and the single channel conductance, and so the 'standard' SHH densities  will be 60 Na + and 18 Na + channels per/am '.  2Although the total number of channels in the model is very large, the microscopic level  ion channel noise has a macroscopic effect on the spike train reliability, since the number  Capacity and Robustness of Stochastic Neuron Models 181  3 The Information Capacity of the SHH Neuron  Expanding the Repeatability and Precision measures [3], we turn to quantify how  much information the neuron model encodes about the stimuli it receives. We thus  present the model with a set of 'representative' input current traces, and the amount  of information that the respective spike trains encode is calculated.  Following Mainen and Sejnowski [3], we use a set of input current traces which imi-  tate the synaptic current that reaches the soma from the dendritic tree. We convolve  a Gaussian white noise trace (with a mean current r/and standard deviation a) with  an alpha function (with a ra = 3 msec). Six different mean current values are used  (r/= 0, 2, 4, 6, 8, 10 IA/cm 2) , and five different std values (a = 1,3, 5, 7, 9 iA/cm2),  yielding a set of 30 input current traces (each is 10 seconds long). This set of inputs  is representative of the wide variety of current traces that neurons might encounter  under in vivo conditions in the sense that the average firing rates for this set of  inputs which range between 2 - 70 Hz (not shown).  We present these input traces to the model, and calculate the amount of information  that the resulting spike trains convey about each input, following [6, 19]. Each  input is presented repeatedly and the resulting spike trains are discretized in Ar  bins, using a sliding 'window' of size T along the discretized sequence. Each train  of spikes is thus transformed into a sequence of K-letter 'words' (K = T/At),  consisting of O's (no spike) and 1's (spike). We estimate P(W), the probability of  the word W to appear in the spike trains, and then compute the entropy rate of its  total word distribution,  Htott = - E P(W) log 2 P(W)  w  bits/word (4)  which measures the capacity of information that the neuron spike trains hold [20, 6,  19]. We then examine the set of words that the neuron model used at a particular  time t over all the repeated presentations of the stimulus, and estimate P(WIt),  the time-dependent word probability distribution. At each time t we calculate the  time-dependent entropy rate, and then take the average of these entropies  - (- P(Wlt) log2 P(Wlt)),  w  bits/word (5)  where {...It denotes the average over all times t. Hoise is the noise entropy rate,  which measures how much of the fine structure of the spike trains of the neuron is  just noise. After performing the calculation for each of the inputs, using different  word sizes 3, we estimate the limit of the total entropy and noise entropy rates at  T -- c, where the entropies converge to their real values (see [19] for details) .  Figure 2A shows the total entropy rate of the responses to the set of stimuli, ranging  from 10 to 170 bits/sec. The total entropy rate is correlated with the firing rates of  the neuron (not shown). The noise entropy rate however, depends in a different way  on the input parameters: Figure 2B shows the noise entropy rate of the responses  to the set of stimuli, which may get up to 100 bits/sec. Specifically, for inputs with  high mean current values and low fluctuation amplitude, many of the spikes are  of ion channels which are open near the spike firing threshold is rather small [12]. The  fluctuations in this small number of open channels near firing threshold give rise to the  input-dependent reliability of the spike timing.  athe bin size r = 2 msec has been set to be small enough to keep the fine tem-  poral structure of the spike train within the word sizes used, yet large enough to avoid  undersampling problems  182 E. Schneidman, I. Segev and N. I)hby  just noise, even if the mean firing rate is high. The difference between the neuron's  entropy rate (the total capacity of information of the neuron's spike train) and the  noise entropy rate, is exactly the average rate of information that the neuron's spike  trains encode about the input, l(stimuIus, spike train) = Htotat - Hnoise [20, 6],  this is shown in Figure 2C. The information rate is more sensitive to the size of  5 5  1'1 [p..A/cm 2] 0 0  [ij.A/cm 2]  B  -.100-  Z 10  5  n IF .A/cm2]  100 D    D  5  0 0  [pA/cm 2]  1'1 [p.A/cm 2] 0 0  [p.A/cm 2]  150  100  50  0  .5  .5  .5  Figure 2: Information capacity of the SHH model. (A) The total spike train entropy  rate of the SHH model as a function of r h the current input mean, and c, the standard  deviation (see text for details). Error bar values of this surface as well as for the other  frames range between I - 6% (not shown). (B) Noise entropy rate as a function of the  current input parameters. (C) The information rate about the stimulus in the spike trains,  as a function of the input parameters, calculated by subtracting noise entropy from the  total entropy (note the change in grayscale in C and D). (D) Information per spike as a  function of the input parameters, which is calculated by normalizing the results shown in  C by the average firing rate of the responses to each of the inputs.  fluctuations in the input than to the mean value of the current trace (as expected,  from the reliability and precision of spike timing observed in vitro [3] and in vivo  [6] as well as in simulations [12]). The dependence of the neural code on the input  parameters is better reflected when calculating the average amount of information  per spike that the model gives for each of the inputs (Fig. 2D) (see for comparison  the values for the Fly's HI neuron [6]).  4  The effect of Changing the Neuron Parameters on the  Information Capacity  Increasing the density of ion channels in the membrane compared to the 'standard'  SHH densities, while keeping the ratio between the K + and Na + channels fixed,  only diminishes the amount of information that the neuron encodes about any of  the inputs in the set. However, the change is rather small: Doubling the channel  density decreases the amount of information by 5 - 25% (Fig. 3A), depending on  the specific input. Decreasing the channel densities of both types, results in en-  coding more information about certain stimuli and less about others. Figure 3B  shows that having half the channel densities would result with in 10% changes in  the information in both directions. Thus, the information rates conveyed by the s-  tochastic model are robust to changes in the ion channel density. Similar robustness  (not shown) has been observed for changes in the membrane area (keeping channel  Capacity and Robustness of Stochastic Neuron Models 183  density fixed) and in the temperature (which effects the channel kinetics). However,  A .2  .2  .1  0.8;  10  13 [p.A/cm 2]  c  o o  5  o [p.A/cm 2]  0.8  10 O I 10  5 5 0.5 5 5  ..rl [HA/cm 2] 00 o' [j.I.A/cm 2] 13 [HA/cm 2] 0 0 o [pA/cm 2]  0.8  0.4  0.3  0.2  0.1  0  Figure 3: The effect of changing the ion channel densities on the information capacity.  (A) The ratio of the information rate of the SHH model with twice the density of the  'standard' SHH densities divided by the information rate of the mode with 'standard'  SHH densities. (B) As in A, only for the SHH model with half the 'standard' densities.  (C) The ratio of the info rate of the SHH model with twice as many Na + channels, divided  by the info rate of the standard SHH Na + channel density, where the K + channel density  remains untouched (note the change in graycale in C and D). (D) As in C, only for the  SHH model with the number of Na + channels reduced by half.  changing the density of the Na + channels alone has a larger impact on the amount  of information that the neuron conveys about the stimuli. Increasing Na + channel  density by a factor of two results in less information about most of the stimuli, and  a gain in a few others (Fig. 3C). However, reducing the number of Na + channels by  half results in drastic loss of information for all of the inputs (Fig. 3D).  15 Discussion  We have shown that the amount of information that the stochastic HH model en-  codes about its current input is highly correlated with the amplitude of fluctuations  in the input and less so with the mean value of the input. The stochastic HH  model, which incorporates ion channel noise, closely replicates the input-dependent  reliability and precision of spike trains observed in cortical neurons. The informa-  tion rates and information per spike are also similar to those of real neurons. As  in other biological systems (e.g., [21]), we demonstrate robustness of macroscopic  performance to changes in the cellular properties - the information coding rates of  the SHH model are robust to changes in the ion channels densities as well as in  the area of the excitable membrane patch and in the temperature (kinetics) of the  channel dynamics. However, the information coding rates are rather sensitive to  changes in the ratio between the densities of different ion channel types, suggests  that the ratio between the density of the K + channels and the Na + channels in the  'standard' SHH model may be optimal in terms of the information capacity. This  may have important implications on the nature of the neural code under adaptation  and learning. We suggest that these notions of optimality and robustness may be  a key biophysical principle of the operation of real neurons. Further investigation-  s should take into account the activity-dependent nature of the channels and the  184 E. Schneidman, I. Segev and N. Ilshby  neuron [15, 16] and the notion of local learning rules which could modify neuronal  and suggest local learning rules as in [22].  Acknowledgements  This research was supported by a grant from the Ministry of Science, Israel.  References  [1] Rieke F., Warland D., de Ruyter van Steveninck R., and Bialek W. Spike: Exploring  the Neural Code. MIT Press, 1997.  [2] Shadlen M. and Newsome W. Noise, neural codes and cortical organization. Curt.  Opin. Neurobiol., 4:569-579, 1994.  [3] Mainen Z. and Sejnowski T. Reliability of spike timing in neocortical neurons. Science,  268:1503-1508, 1995.  [4] Nowak L., Sanches-Vives M., and McCormick D. Influence of low and high frequency  inputs on spike timing in visual cortical neurons. Cerebral Cortex, 7:487-501, 1997.  [5] Bair W. and Koch C. Temporal precision of spike trains in extrastriate cortex of the  behaving macaque monkey. Neural Comp., 8:1185-1202, 1996.  [6] de Ruyter van Steveninck R., Lewen G., Strong S., Koberle R., and Bialek W. Re-  producibility and variability in neural spike trains. Science, 275:1805-1808, 1997.  [7] Reich D., Victor J., Knight B., Ozaki T., and Kaplan E. Response variability and  timing precision of neuronal spike trains in vivo. J. Neurophysiol., 77:2836:2841, 1997.  [8] Hodgkin A. and Huxley A. A quantitative description of membrane current and its  application to conduction and excitation in nerve. J. Physiol., 117:500-544, 1952.  [9] Fitzhugh R. A kinetic model of the conductance changes in nerve membrane. J. Cell.  Comp. Physiol., 66:111-118, 1965.  [10] DeFelice L. Introduction to Membrane Noise. Perseus Books, 1981.  [11] Skaugen E. and Walloe L. Firing behavior in a stochastic nerve membrane model  based upon the Hodgkin-Huxley equations. Acta Physiol. Scand., 107:343-363, 1979.  [12] Schneidman E., Freedman B., and Segev I. Ion channel stochasticity may be critical in  determining the reliability and precision of spike timing. Neural Comp., 10:1679-1704,  1998.  [13] White J., Klink R., Alonso A., and Kay A. Noise from voltage-gated channels may  influence neuronal dynamics in the entorhinal cortex. J Neurophysiol, 80:262-9, 1998.  [14] Hille B. Ionic Channels of Excitable Membrane. Sinauer Associates, 2nd ed., 1992.  [15] Marder E., Abbott L., Turrigiano G., Liu Z., and Golowasch J. Memory from the  dynamics of intrinsic membrane currents. Proc. Natl. Acad. Sci., 93:13481-6, 1996.  [16] Toib A., Lyakhov V., and Marom S. Interaction between duration of activity and rate  of recovery from slow inactivation in mammalian brain Na + channels. J Neurosci.,  18:1893-1903, 1998.  [17] Strassberg A. and DeFelice L. Limits of the HH formalism: Effects of single channel  kinetics on transmembrane voltage dynamics. Neural Comp., 5:843-856, 1993.  [18] Softky W. and Koch C. The highly irregular firing of cortical cells is inconsistent with  temporal integration of random EPSPs. J. Neurosci., 13:334-350, 1993.  [19] Strong S., Koberle R., de Ruyter van Steveninck R., and Bialek W. Entropy and  information in neural spike trains. Phys. Rev. Left., 80:197-200, 1998.  [20] Cover T.M. and Thomas J.A. Elements of Information Theory. Wiley, 1991.  [21] Barkai N. and Leibler S. Robustness in simple biochemical networks. Nature, 387:913-  917, 1997.  [22] Stemruler M. and Koch C. How voltage-dependent conductances can adapt to maxi-  mize the information encoded by neuronal firing rate. Nat. Neurosci., 2:521-7, 1999.  
Training Data Selection  for Optimal Generalization  in Trigonometric Polynomial Networks  Masashi Sugiyama*and Hidemitsu Ogawa  Department of Computer Science, Tokyo Institute of Technology,  2-12-1, O-okayama, Meguro-ku, Tokyo, 152-8552, Japan.  sugi@cs. titech. ac.jp  Abstract  In this paper, we consider the problem of active learning in trigonomet-  ric polynomial networks and give a necessary and sufficient condition of  sample points to provide the optimal generalization capability. By ana-  lyzing the condition from the functional analytic point of view, we clarify  the mechanism of achieving the optimal generalization capability. We  also show that a set of training examples satisfying the condition does  not only provide the optimal generalization but also reduces the compu-  tational complexity and memory required for the calculation of learning  results. Finally, examples of sample points satisfying the condition are  given and computer simulations are performed to demonstrate the effec-  tiveness of the proposed active learning method.  1 Introduction  Supervised learning is obtaining an underlying rule from training examples, and can  be formulated as a function approximation problem. If sample points are actively  designed, then learning can be performed more efficiently. In this paper, we discuss  the problem of designing sample points, referred to as active learning, for optimal  generalization.  Active learning is classified into two categories depending on the optimality. One  is global optimal, where a set of all training examples is optimal (e.g. Fedorov  [3]). The other is greedy optimal, where the next training example to sample is  optimal in each step (e.g. MacKay [5], Cohn [2], Fukumizu [4], and Sugiyama and  Ogawa [10]). In this paper, we focus on the global optimal case and give a new ac-  tive learning method in trigonometric polynomial networks. The proposed method  does not employ any approximations in its derivation, so that it provides exactly  the optimal generalization capability. Moreover, the proposed method reduces the  computational complexity and memory required for the calculation of learning re-  sults. Finally, the effectiveness of the proposed method is demonstrated through  computer simulations.  * http://ogawa-www.cs.titech.ac.jp/~sugi.  Training Data Selection for Optimal Generalization 625  2 Formulation of supervised learning  In this section, the supervised learning problem is formulated from the functional  analytic point of view (see Ogawa [7]). Then, our learning criterion and model are  described.  2.1 Supervised learning as an inverse problem  Let us consider the problem of obtaining the optimal approximation to a target  function f(x) of L variables from a set of M training examples. The training  examples are made up of sample points Xm E T), where T) is a subset of the L-  dimensional Euclidean space R , and corresponding sample values Ym  C:  n M  {(Xm, Ym) l Ym---- f(Xm) + m}m=l, (1)  where Ym is degraded by zero-mean additive noise nm. Let n and y be M-  dimensional vectors whose m-th elements are nm and Ym, respectively. y is called  a sample value vector. In this paper, the target function f(x) is assumed to be-  long to a reproducing kernel Hilbert space H (Aronszajn [1]). If H is unknown,  then it can be estimated by model selection methods (e.g. Sugiyama and Ogawa  [9]). Let K(-, .) be the reproducing kernel of H. If a function Pm(X) is defined  as Pm(X) = K(x, Xm), then the value of f at a sample point Xm is expressed as  f(Xm) = (f, )m), where (., .) stands for the inner product. For this reason, )m is  called a sampling function. Let A be an operator defined as  M  A =  (era  m), (2)  m=l  where em is the m-th vector of the so-called standard basis in C M and (.  r)  stands for the Neumann-Schatten product 1. A is called a sampling operator. Then,  the relationship between f and y can be expressed as  y=Af +n.  Let us denote a mapping from y to a learning result f0 by X:  fo = Xy,  (4)  where X is called a learning operator. Then, the supervised learning problem is  reformulated as an inverse problem of obtaining X providing the best approximation  f0 to f under a certain learning criterion.  2.2 Learning criterion and model  As mentioned above, function approximation is performed on the basis of a learning  criterion. Our purpose of learning is to minimize the generalization error of the  learning result f0 measured by  JG = Enllf0 - fl[ 2, (5)  where En denotes the ensemble average over noise. In this paper, we adopt projec-  tion learning as our learning criterion. Let A*, T(A*), and P(A*) be the adjoint  operator of A, the range of A*, and the orthogonal projection operator onto T(A*),  respectively. Then, projection learning is defined as follows.  XFor any fixed g in a Hilbert space Hx and any fixed f in a Hilbert space H2, the  Neumann-Schatten product (f  ) is an operator from Hx to H2 defined by using any  h C nx as (f  )h = (h,g)f.  626 M. Sugiyama and H. Ogawa  Definition 1 (Projection learning) (Ogawa [6]) An operator X is called the  projection learning operator if X minimizes the functional Jp IX] = En II Xn 112 under  the constraint XA = P(A*).  It is well-known that Eq.(5) can be decomposed into the bias and variance:  JG = [IP(A.)f- f[[2 + EnllXnll 2. (6)  Eq.(6) implies that the projection learning criterion reduces the bias to a certain  level and minimizes the variance.  Let us consider the following function space.  Definition 2 (Trigonometric polynomial space) Let x = ((1),(2),...,(L))T.  For i _< 1 _< L, let Nl be a positive integer and Z)l = [-r, r]. Then, a function  space H is called a trigonometric polynomial space of order (N1, N2,-.., N) if H  is spanned by  exp(int (0) (7)  /=1 nl=-N1 ,n2------N2,"',nL------NL  defined on D1 x D2 x ... x D, and the inner product in H is defined as  (f,g)- (2r)  .-. f(x)g(x)d(1)d(2)...d (). (8)  The dimension/ of a trigonometric polynomial space of order (N1, N2,..., N) is  / ---- rIL=i(2Ni q- 1), and the reproducing kernel of this space is expressed as  where  K((),() ') = {  L  K(x, x') = H (9)  /=1  -(t)') / . (t)-(t)'  sin (2N+1)((0 _ '/ran  2 2  2Nt+l  if (t)  0)', (10)  if () -- (t)'.  3 Active learning in trigonometric polynomial space  The problem of active learning is to find a set M  {Xm}m= 1 of sample points providing  the optimal generalization capability. In this section, we give the optimal solution  to the active learning problem in the trigonometric polynomial space.  Let A t be the Moore-Penrose generalized inverse  of A. Then, the following propo-  sition holds.  Proposition 1 If the noise covariance matrix Q is given as Q = a2 with a 2 > O,  then the projection learning operator X is expressed as X = A t.  Note that the sampling operator A is uniquely determined by M  {Xm}m= 1 (see Eq.(2)).  From Eq. (6), the bias of a learning result f0 becomes zero for all f in H if and only  if iV(A) = {0}, where iV(.) stands for the null space of an operator. For this reason,  2An operator X is called the Moore-Penrose generalized inverse of an operator A if X  satisfies AXA = A, XAX = X, (AX)* = AX, and (XA)* = XA.  Training Data Selection for Optimal Generalization 62 7  C M  Figure 1' Mechanism of noise suppression by Theorem 1. If a set M  (Xm)m=l of sample  points satisfies A*A - MI, then XAf -- f, [[Xnl[[ - 1  11111, and = O.  we consider the case where a set M  {Xm}m=l of sample points satisfies Af(A) = {0).  In this case, Eq.(6) is reduced to  aG = Enlln*nll 2 ,  which is equivalent to the noise variance in H. Consequently, the problem of active  x M  learning becomes the problem of finding a set { m}m=l of sample points minimizing  Eq.(11) under the constraint Af(A) = {0}.  First, we derive a condition for optimal generalization in terms of the sampling  operator A.  Theorem 1 Assume that the noise covariance matrix Q is given as Q = a2I with  cr 2 > O. Then, JG in Eq. (11) is minimized under the constraint iV(A) = {0} if and  only if  A*A=M, (12)  where I denotes the identity operator on H. In this case, the minimum value of J  is a2tz/M, where tz is the dimension of H.  Eq.(12) implies that i M  {)m}m=l forms a pseudo orthonormal basis (Ogawa [8])  in H, which is an extension of orthonormal bases. The following lemma gives  interpretation of Theorem 1.  Lemma 1  x M  When a set { m}m=l of sample points satisfies Eq.(12), it holds that  XAf = f for all f E H, (13)  IlAfl[ = v/-llfll for all f e H, (14)  1 '(A),  IlXull - 11ull for u  (15)  0 for u  T4(A)  .  1  Eqs.(14) and (15) imply that A becomes an isometry and vX becomes a  partial isometry with the initial space T(A), respectively. Let us decompose the  noise n as n = nl q- n2, where nl G T(A) and n2  T(A) . Then, the sample value  vector y is rewritten as y = Af + nl q- n2. It follows from Eq.(13) that the signal  component Af is transformed into the original function f by X. From Eq.(15), X  suppresses the magnitude of noise nl in T(A) by 1 and completely removes the  628 M. Sugiyama and H. Ogawa  Xl X2 23 X4 X5 X6  --71' C 2.__E.  71'  M  (a) Theorem 2  I X4 X5  , x x 2  (b) Theorem3  Figure 2: Two examples of sample points such that Condition (12) holds (p = 3  and M = 6).  noise n2 in 7(A) -L. This analysis is summarized in Fig.1. Note that Theorem 1 and  its interpretation are valid for all Hilbert spaces such that K(x, x) is a constant for  any x.  In Theorem 1, we have given a necessary and sufficient condition to minimize JG  in terms of the sampling operator A. Now we give two examples of sample points  M  {Xm}m= 1 such that Condition (12) holds. From here on, we focus on the case when  the dimension L of the input x is 1 for simplicity. However, the following results  can be easily scaled to the case when L > 1.  Theorem 2 Let M _> I,  constant such that -r <  determined as  then Eq. (12) holds.  where I is the dimension of H. Let c be an arbitrary  2,r M  c _< -r + . If a set {Xm}m= 1 of sample points is  1),  (16)  Theorem 3 Let M = kl where k is a positive integer.  constant such that -r < c < -r + 2  determined as  2r  Xm -- C + r, where  then Eq. (12) holds.  Let c be an arbitrary  x M  If a set { m}m=l of sample points is  r = m- 1 (mod/),  (17)  Theorem 2 means that M sample points are fixed to 2r/M intervals in the domain  [-r, r] and sample values are gathered once at each point (see Fig.2 (a)). In con-  trast, Theorem 3 means that / sample points are fixed to 2r// intervals in the  domain and sample values are gathered k times at each point (see Fig.2 (b)).  Now, we discuss calculation methods of the projection learning result fo(x). Let hm  be the m-th column vector of the M-dimensional matrix (AA*) t. Then, for general  sample points, the projection learning result fo(x) can be calculated as  M  fo(x) --  (y, hm}m(X). (18)  m=l  When we use the optimal sample points satisfying Condition (12), the following  theorems hold.  Theorem 4  lated as  When Eq. (12) holds, the projection learning result fo(x) can be calcu-  M  fo(X) ---- M  YmCm(X). (19)  m----1  Training Data Selection for Optimal Generalization 629  Theorem 5 When sample points are determined following Theorem 3, the projec-  tion learning result fo(x) can be calculated as  i ' i k  So(x)- where y-;= (20)  p=l q=l  In Eq.(18), the coefficient of )m(X) is obtained by the inner product (y, hm). In  contrast, it is replaced with Ym/M in Eq. (19), which implies that the Moore-Penrose  generalized inverse of AA* is not required for calculating fo(x). This property  is quite useful when the number M of training examples is very large since the  calculation of the Moore-Penrose generalized inverse of high dimensional matrices  is sometimes unstable. In Eq.(20), the number of basis functions is reduced to/  and the coefficient of bp(X) is obtained by pp/U, where pp is the mean sample values  at Xp.  For general sample points, the computational complexity and memory required for  calculating fo(x) by Eq.(18) are both O(M2). In contrast, Theorem 4 states that  if a set of sample points satisfies Eq.(12), then both the computational complexity  and memory are reduced to O(M). Hence, Theorem 1 and Theorem 4 do not only  provide the optimal generalization but also reduce the computational complexity  and memory. Moreover, if we determine sample points following Theorem 3 and  calculate the learning result f0 (x) by Theorem 5, then the computational complexity  and memory are reduced to O(/). This is extremely efficient since/ does not depend  on the number M of training examples. The above results are shown in Tab.1.  4 Simulations  In this section, the effectiveness of the proposed active learning method is demon-  strated through computer simulations.  Let H be a trigonometric polynomial space of order 100, and the noise covariance  matrix Q be Q - I. Let us consider the following three sampling schemes.  (A) Optimal sampling: Training examples are gathered following Theorem 3.  (B) Experimental design: Eq.(2) in Cohn [2] is adopted as the active learning  criterion. The value of this criterion is evaluated by 30 reference points. The  sampling location is determined by multi-point-search with 3 candidates.  (C) Passive learning: Training examples are given unilaterally.  Fig.3 shows the relation between the number of training examples and the gener-  alization error. The horizontal and vertical axes display the number of training  examples and the generalization error JG measured by Eq.(5), respectively. The  solid line shows the sampling scheme (A). The dashed and dotted lines denote the  averages of 10 trials of the sampling schemes (B) and (C), respectively. When the  number of training examples is 201, the generalization error of the sampling scheme  (A) is 1 while the generalization errors of the sampling schemes (B) and (C) are  3.18 x 104 and 8.75 x 104, respectively. This graph illustrates that the proposed sam-  pling scheme gives much better generalization capability than the sampling schemes  (B) and (C) especially when the number of training examples is not so large.  5 Conclusion  We proposed a new active learning method in the trigonometric polynomial space.  The proposed method provides exactly the optimal generalization capability and  630 M. Sugiyama and H. Ogawa  Table 1: Computational complexity  and memory required for projection . .  Optimal sampling   l "' "'" ....  i- 2 .,. : ......  learning.  Computational  Calculation Complexity  methods and  Memory  Eq.(18) O(M )  Theorem 4 O(M)  Theorem 5  O(u)  300 400 500 600  M = k/ where/ is the dimension The number of training examples  of H and k is a positive integer. Figure 3: Relation between the number of  training examples and the generalization er-  ror.  at the same time, it reduces the computational complexity and memory required  for the calculation of learning results. The mechanism of achieving the optimal  generalization was clarified from the functional analytic point of view.  References  [1] N. Aronszajn. Theory of reproducing kernels. Transactions on American Mathemat-  ical Society, 68:337-404, 1950.  [2] D. Cohn. Neural network exploration using optimal experiment design. In J. Cowan  et al. (Eds.), Advances in Neural Information Processing Systems 6, pp. 679-686.  Morgan-Kaufmann Publishers Inc., San Mateo, CA, 1994.  [3] V. V. Fedorov. Theory of Optimal Experiments. Academic Press, New York, 1972.  [4] K. Fukumizu. Active learning in multilayer perceptrons. In D. Touretzky et al. (Eds.),  Advances in Neural Information Processing Systems 8, pp. 295-301. The MIT Press,  Cambridge, 1996.  [5] D. MacKay. Information-based objective functions for active data selection. Neural  Computation, 4(4):590-604, 1992.  [6] H. Ogawa. Projection filter regularization of ill-conditioned problem. In Proceedings  of SPIE, 808, Inverse Problems in Optics, pp. 189-196, 1987.  [7] H. Ogawa. Neural network learning, generalization and over-learning. In Proceedings  of the ICIIPS'9, International Conference on Intelligent Information Processing   System, vol. 2, pp. 1-6, Beijing, China, 1992.  [8] H. Ogawa. Theory of pseudo biorthogonal bases and its application. In Research  Institute for Mathematical Science, RIMS Kokyuroku, 1067, Reproducing Kernels  and their Applications, pp. 24-38, 1998.  [9] M. Sugiyama and H. Ogawa. Functional analytic approach to model selection--  Subspace information criterion. In Proceedings of 1999 Workshop on Information-  Based Induction Sciences (IBIS'99), pp. 93-98, Syuzenji, Shizuoka, Japan, 1999  (Its complete version is available at ftp://ftp.cs.titech.ac.jp/pub/TR/99/TR99-  0009.ps.gz).  [10] M. Sugiyama and H. Ogawa. Incremental active learning in consideration of bias,  Technical Report of IEICE, NC99-56, pp. 15-22, 1999 (Its complete version is avail-  able at ftp://ftp.cs.titech.ac.jp/pub/TR/99/TR99-OO10.ps.gz).  
Managing Uncertainty in Cue Combination  Zhiyong Yang  Department of Neurobiolog Box 3209  Duke University Medical Center  Durham, NC 27710  zhyyan@duke. edu  Richard S. Zemel  Department of Psychology  University of Arizona  Tucson, AZ 85721  zemel@u. arizona. edu  Abstract  We develop a hierarchical generative model to study cue combi-  nation. The model maps a global shape parameter to local cue-  specific parameters, which in turn generate an intensity image.  Inferring shape from images is achieved by inverting this model.  Inference produces a probability distribution at each level; using  distributions rather than a single value of underlying variables at  each stage preserves information about the validity of each local  cue for the given image. This allows the model, unlike standard  combination models, to adaptively weight each cue based on gen-  eral cue reliability and specific image context. We describe the  results of a cue combination psychophysics experiment we con-  ducted that allows a direct comparison with the model. The model  provides a good fit to our data and a natural account for some in-  teresting aspects of cue combination.  Understanding cue combination is a fundamental step in developing computa-  tional models of visual perception, because many aspects of perception naturally  involve multiple cues, such as binocular stereo, motion, texture, and shading. It is  often formulated as a problem of inferring or estimating some relevant parameter,  e.g., depth, shape, position, by combining estimates from individual cues.  An important finding of psychophysical studies of cue combination is that cues  vary in the degree to which they are used in different visual environments. Weights  assigned to estimates derived from a particular cue seem to reflect its estimated  reliability in the current scene and viewing conditions. For example, motion  and stereo are weighted approximately equally at near distances, but motion is  weighted more at far distances, presumably due to distance limits on binocular  disparity. s Experiments have also found these weightings sensitive to image ma-  nipulations; if a cue is weakened, such as by adding noise, then the uncontami-  nated cue is utilized more in making depth judgments. 9 A recent study 2 has shown  that observers can adjust the weighting they assign to a cue based on its relative  utility for a particular task. From these and other experiments, we can identify two  types of information that determine relative cue weightings: (1) cue reliability: its  relative utility in the context of the task and general viewing conditions; and (2)  region informativeness: cue information available locally in a given image.  A central question in computational models of cue combination then concerns how  these forms of uncertainty can be combined. We propose a hierarchical generative  870 Z. Yang and R. S. Zemel  model. Generative models have a rich history in cue combination, as they underlie  models of Bayesian perception that have been developed in this area?, 5The nov-  elty in the generative model proposed here lies in its hierarchical nature and use  of distributions throughout, which allows for both context-dependent and image-  specific uncertainty to be combined in a principled manner.  Our aims in this paper are dual: to develop a combination model that incorporates  cue reliability and region informativeness (estimated across and within images),  and to use this model to account for data and provide predictions for psychophys-  ical experiments. Another motivation for the approach here stems from our recent  probabilistic framework,  which posits that every step of processing entails the  representation of an entire probability distribution, rather than just a single value  of the relevant underlying variable(s). Here we use separate local probability dis-  tributions for each cue estimated directly from an image. Combination then entails  transforming representations and integrating distributions across both space and  cues, taking across- and within-image uncertainty into account.  1 IMAGE GENERATION  In this paper we study the case of combining shading and texture. Standard shape-  from-shading models exclude texture, ,8 while standard shape-from-texture mod-  els exclude shading. ? Experimental results and computational arguments have  supported a strong interaction between these cues,  but no model accounting for  this interaction has yet been worked out.  The shape used in our experiments is a simple surface:  Z = B(1- z), I'1 <= 1, lYl <= I (1)  where Z is the height from the zy plane. B is the only shape parameter.  Our image formation model is a hierarchical generative model (see Figure 1). The  top layer contains the global parameter B. The second layer contains local shad-  ing and texture parameters S, T = {Si, T/}, where i indexes image regions. The  generation of local cues from a global parameter is intended to allow local uncer-  tainties to be introduced separately into the cues. This models specific conditions  in realistic images, such as shading uncertainty due to shadows or specularities,  and texture uncertainty when prior assumptions such as isotropicity are violated. 4  Here we introduce uncertainty by adding independent local noise to the underly-  ing shape parameter; this manipulation is less realistic but easier to control.  Local Shading ({S}) Local Texture ({T})  Image (I)  Figure 1: Left: The generative model of image formation. Right: Two sample  images generated by the image formation procedure. B = 1.4 in both. Left: r =  0.05, rt = 0. Right: r = 0, rt = 0.05.  The local cues are sampled from Gaussian distributions: p(SilB) = A/'(f(B); r,);  iv(T[B) = ;V (g(B); rt). f (B), g(B) describe how the local cue parameters depend  Managing Uncertainty in Cue Combination 8 71  on the shape parameter B, while rr and rrt represent the degree of noise in each  cue. In this paper, to simplify the generation process we set f(B) = g(B) = B.  From {$i } and { }, two surfaces are generated; these are essentially two separate  noisy local versions of B. The intensity image combines these surfaces. A set  of same-intensity texsels sampled from a uniform distribution are mapped onto  the texture surface, and then projected onto the image plane under orthogonal  projection. The intensity of surface pixels not contained within these texsels are  determined generated from the shading surface using Lambertjan shading. Each  image is composed of 10 x 10 non-overlapping regions, and contains 400 x 400  pixels. Figure 1 shows two images generated by this procedure.  2 COMBINATION MODEL  We create a combination, or recognition model by inverting the generative model  of Figure 1 to infer the shape parameter B from the image. An important aspect of  the combination model is the use of distributions to represent parameter estimates  at each stage. This preserves uncertainty information at each level, and allows it to  play a role in subsequent inference.  The overall goal of combination is to infer an estimate of B given some image I. We  derive our main inference equation using a Bayesian integration over distributions:  ?(BI) = f ?(BIS, T)?(S, TI0dSdT (2)  P(S, TI) ~ I-I (3)  i  P(BlS, T) -- P(B)P(S, TIB)//P(B)P(S, TIB)d* ~ P(SIB)P(TIB) (4)  i  To simplify the two components we have assumed that the prior over B is uniform,  and that the S, T am conditionally independent given B, and given the image. This  third assumption is dubious but is not essential in the model, as discussed below.  We now consider these two components in turn.  2.1 Obtaining local cue-specific representations from an image  One component in the inference equation, P(S, TII ), describes local cue-  dependent information in the particular image I. We first define intermediate  representations S, T that are dependent on shading and texture cues, respectively.  The shading representation is the curvature of a horizontal section: S = f(B) =  2B(1 + 4z2B2) -a/2. The texture representation is the cosine of the surface slant:  T = g(B) = (1 + 4z2B2) -/. Note that these S, T variables do not match those  used in the generative model; ideally we could have used these cue-dependent  variables, but generating images from them proved difficult.  Some image pre-processing must take place in order to estimate values and un-  certainties for these particular local variables. The approach we adopt involves a  simple statistical matching procedure, similar to k-nearest neighbors, applied to  local image patches. After applying Gaussian smoothing and band-pass filtering  to the image, two representations of each patch am obtained using separate shad-  ing and texture filters. For shading, image patches are represented by forming a  histogram of --; for texture, the patch is represented by the mean and standard  deviation of the amplitude of Gabor filter responses at 4 scales and orientations.  This representation of a shading patch is then compared to a database of similar  872 Z gang and R. S. Zemel  patch representations. Entries in the shading database are formed by first select-  ing a particular value of B and try, generating an image patch, and applying the  appropriate filters. Thus S = fiB) and the noise level rr are known for each entry,  allowing an estimate of these variables for the new patch to be formed as a linear  combination of the entries with similar representations. An analogous procedure,  utilizing a separate database, allows 7' and an uncertainty estimate to be derived  for texture. Both databases have 60 different b, rr pairs, and 10 samples of each pair.  Based on this procedure we obtain for each image patch mean values M[, M/t and  uncertainty values V?, V/t for Si,Ti. These determine P(I[S),P(I[T), which are  approximated as Gaussians. Taking into account the Gaussian priors for $i,  P(S, 1I) = P(IlS,)P($, ) .., exp(-'--(S- M[)')exp(---(S- M)') (5)   V, t  P(T1I) = P(IITi)P(Ti ) .-,exp(--(T- M[)2)exp(---(T - M) 2) (6)  Note that the independence assumption of Equation 3 is not necessary, as the  matching procedure could use a single database indexed by both the shading and  texture representations of a patch.  2.2 Transforming and combining cue-specific local representations  The other component of the inference equation describes the relationship between  the intermediate, cue-specific representations S, T and the shape parameter B:  P(SIB) .., exp(- V V, t  I(B)) 2) ; P(TIB),-, -  exp(--(T #(B)) 2) (7)  The two parameters V , V t in this equation describe the uncertainty in the relation-  ship between the intermediate parameters S, T and B; they are invariant across  space. These two, along with the parameters of the priors--M, M, V, V0t--are  the free parameters of this model. Note that this combination model neatly ac-  counts for both types of cue validity we identified: the variance in P(SIB) de-  scribes the general uncertainty of a given cue, while the local variance in P(SII)  describes the finage-specific uncertainty of the cue.  Combining Equations 3-7, and completing the integral in Equation 2, we have:  P(BII)'"exp[- E. af(B)2+a2g(B)2-2aaf(B)-2a4g(B)] (8)  vt(v,'+W) W(v?+vo') W(v:t+v;) vff ' '  - (v, M, +v,; Mo)  cq _ v,:+W+W, a2 = W+v?+Vo,, aa = v,:+vl+vg , a4 = vff+v,,+Vo  . Ap-  proximating P(B[I) as a Gaussian, we obtain the mean t/and std. deviation :  0 log(-P(BII)) 0 2 (- log P(B 10) lu]_ (9)  OB lu = 0 ; E = [ 0 2B  Thus our model infers from any image a mean t/and variance y:2 for B as non-  linear combinations of the cue estimates, taking into account the various forms of  uncertainty.  3 A CUE COMBINATION PSYCHOPHYSICS EXPERIMENT  We have conducted psychophysical experiments using stimuli generated by the  procedure described above. In each experimental trial, a stimulus image and four  Managing Uncertainty in Cue Combination 873  views of a mesh surface are displayed side-by-side on a computer screen. The  subject's task is to manipulate the curvature of the mesh to match the stimulus.  The final shape of the mesh surface describes the subject's estimate of the shape  parameter B on that trial. The subject's variance is computed across repeated trials  with an identical stimulus. In a given block of trials, the stimulus may contain  only shading information (no texture elements), only texture information uniform  shading), or both. The local cue noise (trs, trt) is zero in some blocks, non-zero in  others. The primary experimental findings (see Figure 2) are:   Shape from shading alone produces underestimates of B. Shape from tex-  ture alone also leads to underestimation, but to a lesser degree.   Shape from both cues leads to almost perfect estimation, with smaller vari-  ance than shape from either cue alone. Thus cue enhancement more accu-  rate and robust judgements for stimuli containing multiple cues than just  individual cues--applies to this paradigm.   The variance of a subject's estimation increases with B.   Noise in either shading or texture systematically biases the estimation  from the true values: the greater the noise level, the greater the bias.   Shape from both cues is more robust against noise than shape from either  cue alone, providing evidence of another form of cue enhancement.  0.5  1 2  Stimulus  1 2  Stimulus   t.s  0.5l'  1 2  Stimulus  2  1.8  1.6  .1.4  LU  1.2  1.6  1.4  1.2  1  .,'  1 1.5 2 1 1.5 2  Stimulus Stimulus  Figure 2: Means and standard errors are shown for the shape matching exper-  iment, for different values of B, under different stimulus conditions. TOP: No  noise in local shape parameters. Left: Shape from shading alone. Middle: Shape  from texture alone. Right: Shape from shading and texture. BOTTOM: Shape from  shading and texture. Left: r, = 0.05, trt = 0. Right: r, = 0, trt = 0.05.  4 MODELING RESULTS  The model was trained using a subset of data from these experiments. The error  criteria was mean relative error (MITE) between the model outputs (t/, I2) and  874 Z. ang and R. S. Zemel  B o' trt data (///P.) model (/,//P.)  1.4 0.10 0 1.18/0.072 1.20/0.06  1.6 0.10 0 1.34/0.075 1.35/0.063  1.4 0.05 0 1.32/0.042 1.4 / 0.067  1.6 0.05 0 1.52/0.049 1.46/0.069  1.2 0 0.05 1.20/0.052 1.14/0.056  1.4 0 0.05 1.36/0.062 1.30/0.063  Table 1: Data versus model predictions on images outside the training class. The  first column of means and variances am from the experimental data, the second  column from the model.  experimental data (subject mean and variance on the same image). The six free  parameters of the model were described as the sum of third order polynomials of  local $, T and the noise levels. Gradient descent was used to train the model.  The model was trained and tested on three different subsets of the experimental  data. When trained on data in which only B varied, the model output accurately  predicts unseen experimental data of the same type. When the data varied in B  and tr8 or err, the model outputs agree very well with subject data (M_RE ,,,, 5 -  8%). When trained on data where all three variables vary, the model fits the data  masonably well (M_RE ,-, 8-13%). For a model of the first type, Figure 3 compares  model predictions to data from within the same set, while Table 1 shows model  outputs and subject responses for test examples from outside the training class.  1.6  1.5  1.4  1.3  1.2  1.1  1.2 1.3 1.4 1.5 1.6  Stimulus  Figure 3: Model performance on data in which ', = 0, err = 0.10. Upper line:  perfect estimation. Lower line: experimental data. Dashed line: model prediction.  The model accounts for some important aspects of cue combination. Trained  model parameters reveal that the texture prior is considerably weaker than the  shading prior, and texture has a more reliable relationship with B. Consequently,  at equal noise levels texture outweighs shading in the combination model. These  factors account for the degree of underestimation found in each single-cue experi-  ment, and the greater accuracy (i.e., enhancement) with combined-cues. Our stud-  ies also reveal a novel form of cue interaction: for some image patches, esp. at  high curvature and noise levels, shading information becomes harmful, i.e., cur-  vature estimation becomes less reliable when shading information is taken into  account. Note that this differs from cue veto, in that texture does not veto shading.  Finally, the primary contribution of our model lies in its ability to predict the effect  of continuous within-image variation in cue reliability on combination. Figure 4  shows how the estimation becomes more accurate and less variable with increas-  Managing Uncertainty in Cue Combination 875  ing certainty in shading information. Standard cue combination models cannot  produce similar behavior, as they do not estimate within-image cue reliabilities.  0.9   0.85  '5   0.8   0.75  ._  "' 0.7  .- B=1.4  /'  -_ - - 5=1.6  O 5=1.8  0  0.1  0.09  0.08  0.07  0.06  0.05  0  0.65  0 0.5 1 1.5 0.5 1 1.5  Figure 4: Mean (left) and variance (right) of model output as a function of V/, for  different values of B. Here rr = 0.15, rrt = 0, all model parameters held constant.  5 CONCLUSION  We have proposed a hierarchical generative model to study cue combination. In-  ferring parameters from images is achieved by inverting this model. Inference pro-  duces probability distributions at each level: a set of local distributions, separately  representing each cue, are combined to form a distribution over a relevant scene  variable. The model naturally handles variations in cue reliability, which depend  both on spatially local image context and general cue characteristics. This form of  representation, incorporating image-specific cue utilities, makes this model more  powerful than standard combination models. The model provides a good fit to  our psychophysics results on shading and texture combination and an account for  several aspects of cue combination; it also provides predictions for how varying  noise levels, both within and across images, will effect combination.  We are extending this work in a number of directions. We are conducting exper-  iments to obtain local shape estimates from subjects. We are considering better  ways to extract local representations and distributions over them directly from an  image, and methods of handling natural outliers such as shadows and occlusion.  References  [1] Horn, B. K. E (1977).Understanding image intensities. AI 8, 201-231.  [2] Jacobs, R. A. & Fine I. (1999). Experience-dependent integration of texture and motion cues to depth. Vis. Res.,  39, 4062-4075.  [3] Johnston, E. B., Cureming, B. G., & Landy, M. S. (1994). Integration of depth modules: Stereopsis and texture.  V/s. Res. 34, 2259-2275.  [4] Knill, D.C. (1998). Surface orientation from texture: ideal observers, generic observers and the information  content of texture cues. Vis. Res. 38, 1655-1682.  [5] Knill, D.C., Kersten, D., & Mamassian P. (1996). Implications of a Bayesian formulation of visual information  for processing for psychophysics. In Perception as Bayesian Inference, D.C. Knill and W. Richards (Eds.), 239-286,  Cambridge Univ Press.  [6] Landy, M. S., Maloney, L. T., Johnston, E. B., & Young, M. J. (1995). Measurement and modeling of depth cue  combination: In defense of weak fusion. Vis. Res. 35, 389-412.  [7] Malik, J. & Rosenholtz, R. (1997). Computing local surface orientation and shape from texture for curved sur-  faces. IJCV 23,149-168.  [8] Pentland, A. (1984). Local shading analysis. IEEE PAMI, 6, 170-187.  [9] Young, M.J., Landy, Mq., & Maloney, L.T. (1993). A perturbation analysis of depth perception from combina-  tions of texture and motion cues. Vis. Res. 33, 2685-2696.  [10] Yuille, A. & Bulthoff, H. H. (1996). Bayesian decision theory and psychophysics. In Perception as Bayesian Infer-  ence, D.C. Knill and W. Richards (Eds.), 123-16_1, Cambridge Univ Press.  [11] Zemel, R. S., Dayan, P., & Pouget, A. (1998). Probabilistic interpretation of population codes. Neural Computa-  tion, 403-430.  PART VIII  APPLICATIONS  
Approximate inference algorithms for two-layer  Bayesian networks  Andrew Y. Ng  Computer Science Division  UC Berkeley  Berkeley, CA 94720  ang @ cs. berkeley. edu  Michael I. Jordan  Computer Science Division and  Department of Statistics  UC Berkeley  Berkeley, CA 94720  jordan @ cs. berkeley. edu  Abstract  We present a class of approximate inference algorithms for graphical  models of the QMR-DT type. We give convergence rates for these al-  gorithms and for the Jaakkola and Jordan (1999) algorithm, and verify  these theoretical predictions empirically. We also present empirical re-  sults on the difficult QMR-DT network problem, obtaining performance  of the new algorithms roughly comparable to the Jaakkola and Jordan  algorithm.  1 Introduction  The graphical models formalism provides an appealing framework for the design and anal-  ysis of network-based learning and inference systems. The formalism endows graphs with  a joint probability distribution and interprets most queries of interest as marginal or con-  ditional probabilities under this joint. For a fixed model one is generally interested in the  conditional probability of an output given an input (for prediction), or an input conditional  on the output (for diagnosis or control). During learning the focus is usually on the like-  lihood (a marginal probability), on the conditional probability of unobserved nodes given  observed nodes (e.g., for an EM or gradient-based algorithm), or on the conditional proba-  bility of the parameters given the observed data (in a Bayesian setting).  In all of these cases the key computational operation is that of marginalization. There are  several methods available for computing marginal probabilities in graphical models, most  of which involve some form of message-passing on the graph. Exact methods, while viable  in many interesting cases (involving sparse graphs), are infeasible in the dense graphs that  we consider in the current paper. A number of approximation methods have evolved to treat  such cases; these include search-based methods, loopy propagation, stochastic sampling,  and variational methods.  Variational methods, the focus of the current paper, have been applied successfully to a  number of large-scale inference problems. In particular, Jaakkola and Jordan (1999) de-  veloped a variational inference method for the QMR-DT network, a benchmark network  involving over 4,000 nodes (see below). The variational method provided accurate ap-  proximation to posterior probabilities within a second of computer time. For this difficult  534 A. Y. Ng and M. I. dordan  inference problem exact methods are entirely infeasible (see below), loopy propagation  does not converge to correct posteriors (Murphy, Weiss, & Jordan, 1999), and stochastic  sampling methods are slow and unreliable (Jaakkola & Jordan, 1999).  A significant step forward in the understanding of variational inference was made by Kearns  and Saul (1998), who used large deviation techniques to analyze the convergence rate of  a simplified variational inference algorithm. Imposing conditions on the magnitude of the  weights in the network, they established a O(v/log N/N) rate of convergence for the error  of their algorithm, where N is the fan-in.  In the current paper we utilize techniques similar to those of Kearns and Saul to derive a  new set of variational inference algorithms with rates that are faster than O(v/log N/N).  Our techniques also allow us to analyze the convergence rate of the Jaakkola and Jordan  (1999) algorithm. We test these algorithms on an idealized problem and verify that our  analysis correctly predicts their rates of convergence. We then apply these algorithms to  the difficult the QMR-DT network problem.  2 Background  2.1 The QMR-DT network  The QMR-DT (Quick Medical Reference, Decision-Theoretic) network is a bipartite graph  with approximately 600 top-level nodes di representing diseases and approximately 4000  lower-level nodes f5 representing findings (observed symptoms). All nodes are binary-  valued. Each disease is given a prior probability P(di -- 1), obtained from archival data,  and each finding is parameterized as a "noisy-OR" model:  where ri is the set of parent diseases for finding fi, and where the parameters 0ij are  obtained from assessments by medical experts (see Shwe, et al., 1991).  Letting zi = Oio + YjE Oid, we have the following expression for the likelihoodl:  i----1 i=1 jl  (1)  where the sum is a sum across the approximately 26 configurations of the diseases. Note  that the second product, a product over the negative findings, factorizes across the diseases  ds; these factors can be absorbed into the priors P(d) and have no significant effect on the  complexity of inference. It is the positive findings which couple the diseases and prevent  the sum from being distributed across the product.  Generic exact algorithms such as the junction tree algorithm scale exponentially in the  size of the maximal clique in a moralized, triangulated graph. Jaakkola and Jordan (1999)  found cliques of more than 150 nodes in QMR-DT; this rules out the junction tree algo-  rithm. Heckerman (1989) discovered a factorization specific to QMR-DT that reduces the  complexity substantially; however the resulting algorithm still scales exponentially in the  number of positive findings and is only feasible for a small subset of the benchmark cases.  In this expression, the factors P(dj) are the probabilities associated with the (parent-less) disease  nodes, the factors (1 - e -z ) are the probabilities of the (child) finding nodes that are observed to be  in their positive state, and the factors e- z are the probabilities of the negative findings. The resulting  product is the joint probability P(f, d), which is marginalized to obtain the likelihood P(f).  Approximate Inference Algorithms for Two-Layer Bayesian Networks 535  2.2 The Jaakkola and Jordan (J J) algorithm  Jaakkola and Jordan (1999) proposed a variational algorithm for approximate inference in  the QMR-DT setting. Briefly, their approach is to make use of the following variational  inequality:  I - e -zl <_ e 'izi--ci ,  where ci is a deterministic function of )q. This inequality holds for arbitrary values of  the free "variational parameter" Ai. Substituting these variational upper bounds for the  probabilities of positive findings in Eq. (1), one obtains a factorizable upper bound on the  likelihood. Because of the factorizability, the sum across diseases can be distributed across  the joint probability, yielding a product of sums rather than a sum of products. One then  minimizes the resulting expression with respect to the variational parameters to obtain the  tightest possible variational bound.  2.3 The Kearns and Saul (KS) algorithm  A simplified variational algorithm was proposed by Kearns and Saul (1998), whose main  goal was the theoretical analysis of the rates of convergence for variational algorithms. In  their approach, the local conditional probability for the finding fi is approximated by its  value at a point a small distance i above or below (depending on whether upper or lower  bounds are desired) the mean input E[zi]. This yields a variational algorithm in which the  values i are the variational parameters to be optimized. Under the assumption that the  weights Oij are bounded in magnitude by r/N, where r is a constant and N is the number  of parent ("disease") nodes, Kearns and Saul showed that the error in likelihood for their  algorithm converges at a rate of O(x/log N/N).  3 Algorithms based on local expansions  Inspired by Kearns and Saul (1998), we describe the design of approximation algorithms  for QMR-DT obtained by expansions around the mean input to the finding nodes. Rather  than using point approximations as in the Kearns-Saul (KS) algorithm, we make use of  Taylor expansions. (See also Plefka (1982), and Barber and van de Laar (1999) for other  perturbational techniques.)  Consider a generalized QMR-DT architecture in which the noisy-OR model is replaced by a  general function b(z): 11 --> [0, 1] having uniformly bounded derivatives, i.e., Ib(i) (z)l <  B i. Define F(z1,... ,ZK) -- H/K=i ((zi)) fl H/K=i (1- )(zi)) 1-fl so that the likelihood  can be written as  P(f) = E{z,}[F(zl,...,ZK)]. (2)  N  Also define/i = E[zi] = Oio + Ej=i OoP(dJ = 1).  A simple mean-field-like approximation can be obtained by evaluating F at the mean values  /i:  P(f)  F(i,... ,K). (3)  We refer to this approximation as "MF(0)."  Expanding the function F to second order, and defining ci -- zi - Ii, we have:  K I K K  P(f) = E{,,} F()+ E Fil()cixq- .1 E E Fili2()ili2 q-  t. i:1 i1=1 i=1  1 K K K __>  - E E E Zili2i$( *)ili2i$ (4)  i1=1 i2=1 ia=l  536 A. Y. Ng and M. I. dordan  where the subscripts on F represent derivatives. Dropping the remainder term and bringing  the expectation inside, we have the "MF(2)" approximation:  1 K K  P(f)  F() +    Fili2()E[ili2]  i1=1 i2-1  More generally, we obtain a "MF(i)" approximation by carrying out a Taylor expansion to  i-th order.  3.1 Analysis  In this section, we give two theorems establishing convergence rates for the MF(i) family  of algorithms and for the Jaakkola and Jordan algorithm. As in Kearns and Saul (1998),  our results are obtained under the assumption that the weights are of magnitude at most  O(1/N) (recall that N is the number of disease nodes). For large N, this assumption of  "weak interactions" implies that each zi will be close to its mean value with high probability  (by the law of large numbers), and thereby gives justification to the use of local expansions  for the probabilities of the findings.  Due to space constraints, the detailed proofs of the theorems given in this section are de-  ferred to the long version of this paper, and we will instead only sketch the intuitions for  the proofs here.  Theorem 1 Let K (the number offindings) be fixed, and suppose IOijl _<  for all i, j for  some fixed constant r. Then the absolute error of the MF( k ) approximation is 0 ( 1  N(/+1)/2 )  1  for k odd and 0 (N(/2+1) ) for k even.  Proof intuition. First consider the case of odd k. Since IOijl < r  _ N, the quantity ei = zi -  Ii = Y',j Oij(dj - E[dj]) is like an average of N random variables, and hence has standard  deviation on the order I/v/fl. Since MF(k) matches F up to the k-th order derivatives, we  find that when we take a Taylor expansion of MF(k)'s error, the leading non-zero term is the  _k+l Now because ei has standard  k + 1-st order term, which contains quantities such as i   deviation on the order 1/v/if, it is unsurprising that E[e +1] is on the order l/N( +1)/2,  which gives the error of MF(k) for odd k.  For k even, the leading non-zero term in the Taylor expansion of the error is a k + 1-st order  _+1 But if we think of ei as converging (via a central limit  term with quantities such as i   theorem effect) to a symmetric distribution, then since symmetric distributions have small  odd central moments, E[e/+1] would be small. This means that for k even, we may look to  the order k + 2 term for the error, which leads to MF(k) having the the same big-O error as  MF(k + 1). Note this is also consistent with how MF(0) and MF(1) always give the same  estimates and hence have the same absolute error. []  A theorem may also be proved for the convergence rate of the Jaakkola and Jordan (JJ)  algorithm. For simplicity, we state it here only for noisy-OR networks. 2 A closely related  result also holds for sigmoid networks with suitably modified assumptions; see the full  paper.  Theorem 2 Let K be fixed, and suppose b(z) = 1-e -z is the noisy-OR function. Suppose  further that 0 < Oij  k for all i, j for some fixed constant r, and that Ii _> ]gmin for all  i, for some fixed ]gmin > 0. Then the absolute error of the JJ approximation is 0 (- ).  2Note in any case that JJ can be applied only when b is log-concave, such as in noisy-OR networks  (where incidentally all weights are non-negative).  Approximate Inference Algorithms for Two-Layer Bayesian Networks 537  The condition of some/gmi n lowerbounding the/zi's ensures that the findings are not too  unlikely; for it to hold, it is sufficient that there be bias ("leak") nodes in the network with  weights bounded away from zero.  Proof intuition. Neglecting negative findings, (which as discussed do not need to be han-  dled variationally,) this result is proved for a "simplified" version of the JJ algorithm, that  always chooses the variational parameters so that for each i, the exponential upperbound  on P(zi) is tangent to ;b at zi -/zi. (The "normal" version of JJ can have error no worse  than this simplified one.) Taking a Taylor expansion again of the approximation's error, we  find that since the upperbound has matched zeroth and first derivatives with F, the error is  2 As discussed in the MF(k) proof outline,  a second order term with quantities such as e i .  this quantity has expectation on the order I/N, and hence JJ's error is O(1/N). []  To summarize our results in the most useful cases, we find that MF(0) has a convergence  rate of O(1/N), both MF(2) and MF(3) have rates of O(1/N2), and JJ has a convergence  rate of O(1/N).  4 Simulation results  4.1 Artificial networks  We carried out a set of simulations that were intended to verify the theoretical results pre-  sented in the previous section. We used bipartite noisy-OR networks, with full connectivity  between layers and with the weights Oij chosen uniformly in (0, 2IN). The number N of  top-level ("disease") nodes ranged from 10 to 1000. Priors on the disease nodes were  chosen uniformly in (0, 1).  The results are shown in Figure 1 for one and five positive findings (similar results where  obtained for additional positive findings).  10   i;o o0o  #diseases  lO 6  lff a  10 100 1000  #diseases  Figure 1: Absolute error in likelihood (averaged over many randomly generated networks) as a func-  tion of the number of disease nodes for various algorithms. The short-dashed lines are the KS upper  and lower bounds (these curves overlap in the left panel), the long-dashed line is the JJ algorithm and  the solid lines are MF(0), MF(2) and MF(3) (the latter two curves overlap in the right panel).  The results are entirely consistent with the theoretical analysis, showing nearly exactly the  expected slopes of -1/2, -1 and -2 on a loglog plot. 3 Moreover, the asymptotic results are  3The anomalous behavior of the KS lower bound in the second panel is due to the fact that the  algorithm generally finds a vacuous lower bound of 0 in this case, which yields an error which is  essentially constant as a function of the number of diseases.  538 A. Y. Ng and M. I. Jordan  also predictive of overall performance: the MF(2) and MF(3) algorithms perform best in  all cases, MF(0) and JJ are roughly equivalent, and KS is the least accurate.  4.2 QMR-DT network  We now present results for the QMR-DT network, in particular for the four benchmark  CPC cases studied by Jaakkola and Jordan (1999). These cases all have fewer than 20  positive findings; thus it is possible to run the Heckerman (1989) "Quickscore" algorithm  to obtain the true likelihood.  Case 16  Case 32  1 2 3 4 5 6 7 8 9 10 I 2 3 4 5 6 7  #Exactly treated findings #Exactly treated findings  Figure 2: Results for CPC cases 16 and 32, for different numbers of exactly treated findings. The  horizontal line is the true likelihood, the dashed line is JJ's estimate, and the lower solid line is  MF(3)'s estimate.  10 -;;  10 -H  10 -  Case 34  Case 46  2 4 6 8 1 12 2 4 6 8 1  #Exactly treated findings #Exactly treated findings  Figure 3: Results for CPC cases 34 and 46. Same legend as above.  In Jaakkola and Jordan (1999), a hybrid methodology was proposed in which only a portion  of the findings were treated approximately; exact methods were used to treat the remaining  findings. Using this hybrid methodology, Figures 2 and 3 show the results of running JJ  and MF(3) on these four cases. 4  4These experiments were run using a version of the JJ algorithm that optimizes the variational  parameters just once without any findings treated exactly, and then uses these fixed values of the  parameters thereafter. The order in which findings are chosen to be treated exactly is based on JJ's  estimates, as described in Jaakkola and Jordan (1999). Missing points in the graphs for cases 16 and  Approximate Inference Algorithms for Two-Layer Bayesian Networks 539  The results show the MF algorithm yielding results that are comparable with the JJ algo-  rithm.  5 Conclusions and extension to multilayer networks  This paper has presented a class of approximate inference algorithms for graphical models  of the QMR-DT type, supplied a theoretical analysis of convergence rates, verified the rates  empirically, and presented promising empirical results for the difficult QMR-DT problem.  Although the focus of this paper has been two-layer networks, the MF(k) family of al-  gorithms can also be extended to multilayer networks. For example, consider a 3-layer  network with nodes bi being parents of nodes di being parents of nodes fi. To approximate  Pr[f] using (say) MF(2), we first write Pr[f] as an expectation of a function (F) of the  zi's, and approximate this function via a second-order Taylor expansion. To calculate the  expectation of the Taylor approximation, we need to calculate terms in the expansion such  as E[di], E[didj] and E[d,2. ]. When di had no parents, these quantities were easily derived in  terms of the disease prior probabilities. Now, they instead depend on the joint distribution  of di and dj, which we use our two-layer version of MF(k), applied to the first two (bi and  di) layers of the network, to approximate. It is important future work to carefully study the  performance of this algorithm in the multilayer setting.  Acknowledgments  We wish to acknowledge the helpful advice of Tommi Jaakkola, Michael Kearns, Kevin  Murphy, and Larry Saul.  References  [1] Barber, D., & van de Laar, P. (1999) Variational cumulant expansions for intractable distributions.  Journal of Artificial Intelligence Research, 10, 435-455.  [2] Heckerman, D. (1989). A tractable inference algorithm for diagnosing multiple diseases. In  Proceedings of the Fifth Conference on Uncertainty in Artificial Intelligence.  [3] Jaakkola, T. S., & Jordan, M. I. (1999). Variational probabilistic inference and the QMR-DT  network. Journal of Artificial Intelligence Research, 10, 291-322.  [4] Jordan, M. I., Ghahramani, Z., Jaakkola, T. S., & Saul, L. K. (1998). An introduction to variational  methods for graphical models. In Learning in Graphical Models. Cambridge: MIT Press.  [5] Keams, M. J., & Saul, L. K. (1998). Large deviation methods for approximate probabilistic infer-  ence, with rates of convergence. In G. F. Cooper & S. Moral (Eds.), Proceedings of the Fourteenth  Conference on Uncertainty in Artificial Intelligence. San Mateo, CA: Morgan Kaufmann.  [6] Murphy, K. P., Weiss, Y., & Jordan, M. I. (1999). Loopy belief propagation for approximate  inference: An empirical study. In Proceedings of the Fifteenth Conference on Uncertainty in Artificial  Intelligence.  [7] Plefka, T. (1982). Convergence condition of the TAP equation for the infinite-ranged Ising spin  glass model. In J. Phys. A: Math. Gen., 5(6).  [8] Shwe, M., Middleton, B., Heckerman, D., Henrion, M., Horvitz, E., Lehman. n, H., & Cooper, G.  (1991). Probabilistic diagnosis using a reformulation of the INTERNIST-1/QMR knowledge base I.  The probabilistic model and inference algorithms. Methods oflnformation in Medicine, 30, 241-255.  34 correspond to runs where our implementation of the Quickscore algorithm encountered numerical  problems.  
A generative model for attractor dynamics  Richard S. Zemel  Department of Psychology  University of Arizona  Tucson, AZ 85721  zemel@u. arizona. edu  Michael C. Mozer  Department of Computer Science  University of Colorado  Boulder, CO 80309-0430  mozer@colorado. edu  Abstract  Attractor networks, which map an input space to a discrete out-  put space, are useful for pattern completion. However, designing  a net to have a given set of attractors is notoriously tricky; training  procedures are CPU intensive and often produce spurious attrac-  tors and ill-conditioned attractor basins. These difficulties occur  because each connection in the network participates in the encod-  ing of multiple attractors. We describe an alternative formulation  of attractor networks in which the encoding of knowledge is local,  not distributed. Although localist attractor networks have similar  dynamics to their distributed counterparts, they are much easier  to work with and interpret. We propose a statistical formulation of  localist attractor net dynamics, which yields a convergence proof  and a mathematical interpretation of model parameters.  Attractor networks map an input space, usually continuous, to a sparse output  space composed of a discrete set of alternatives. Attractor networks have a long  history in neural network research.  Attractor networks are often used for pattern completion, which involves filling in  missing, noisy; or incorrect features in an input pattern. The initial state of the  attractor net is typically determined by the input pattern. Over time, the state is  drawn to one of a predefined set of statesrathe attractors. Attractor net dynam-  ics can be described by a state trajectory (Figure la). An attractor net is generally  implemented by a set of visible units whose activity represents the instantaneous  state, and optionally, a set of hidden units that assist in the computation. Attractor  dynamics arise from interactions among the units. In most formulations of attrac-  tor nets, e,3 the dynamics can be characterized by gradient descent in an energy  landscape, allowing one to partition the output space into attractor basins. Instead  of homogeneous attractor basins, it is often desirable to sculpt basins that depend  on the recent history of the network and the arrangement of attractors in the space.  In psychological models of human cognition, for example, priming is fundamental:  after the model visits an attractor, it should be faster to fall into the same attractor  in the near future, i.e., the attractor basin should be broadened. l, 6  Another property of attractor nets is key to explaining behavioral data in psycho-  logical and neurobiological models: the gang effect, in which the strength of an  attractor is influenced by other attractors in its neighborhood. Figure lb illustrates  the gang effect: the proximity of the two rightmost attractors creates a deeper at-  tractor basin, so that if the input starts at the origin it will get pulled to the right.  Generaave Model for ,4ttractor Dynamics 81  Figure 1: (a) A two-dimensional space can be carved into three regions (dashed  lines) by an attractor net. The dynamics of the net cause an input pattern (the X)  to be mapped to one of the attractors (the O's). The solid line shows the tempo-  ral trajectory of the network state. (b) the actual energy landscape for a localist  attractor net as a function of ,, when the input is fixed at the origin and there are  three attractors, w = ((-1, 0), (1, 0), (1,-.4)), with a uniform prior. The shapes of  attractor basins are influenced by the proximity of attractors to one another (the  gang effect). The origin of the space (depicted by a point) is equidistant from the  attractor on the left and the attractor on the upper right, yet the origid dearly lies  in the basin of the right attractors.  This effect is an emergent property of the distribution of attractors, and is the basis  for interesting dynamics; it produces the mutuall reinforcing or inhibitory influ-  Y  ence of similar items in domains such as semantics, 9 memory, ' 2 and olfaction. 4  Training an attractor net is notoriously tricky. Training procedures are CPU inten-  sive and often produce spurious attractors and ill-conditioned attractor basins. 5.    Indeed, we are aware of no existing procedure that can robustly translate an arbi-  trary specification of an attractor landscape into a set of weights. These difficulties  are due to the fact that each connection partialpates in the specification of multiple  attractors; thus, knowledge in the net is distributed over connections.  We describe an alternative attractor network model in which knowledge is local-  ized, hence the name localist attractor network. The model has many virtues, includ-  ing: a trivial procedure for wiring up the architechn given an attractor landscape;  eliminating spurious attractors; achieving gang effects; providing a dear mathe-  matical interpretation of the model parameters, which clarifies how the parameters  control the qualitative behavior of the model (e.g., the magnitude of gang effects);  and proofs of convergence and stability.  A localist attractor net consists of a set of n state units and m attractor units. Pa-  rameters associated with an attractor unit i encode the location of the attractor,  denoted wi, and its "pull"or strength, denoted rri, which influence the shape of  the attractor basin. Its activity at time t, qi (t), reflects the normalized distance from  the attractor center to the current state, y(t), weighted by the attractor strength:  qi(t) = ti#(y(t),wi,(t)) (1)  y'j rd#(y(t), wj, r(t))  g(y,w,r) = exp(-ly-wl2/2r 2) (2)  Thus, the attractors form a layer of normalized radial-basis-function units.  The input to the net, , serves as the initial value of the state, and thereafter the  state is pulled toward attractors in proportion to their activity. A straightforward  82 R. S. Zemel and M. C. Mozer  expression of this behavior is:  y(t + 1) = a(t) + (1 - a(t)) y qi(t)wi.  i  (3)  where a(1) = 1 on the first update and a(t) = 0 fort > 1. More generally, however,  one might want to gradually reduce a over time, allowing for a persistent effect of  the external input on the asymptotic state. The variables tr(t) and a(t) are not free  parameters of the model, but can be derived from the formalism we present below.  The localist attractor net is motivated by a generative model of the input based on  the attractor distribution, and the network dynamics corresponds to a search for  a maximum likelihood interpretation of the observation. In the following section,  we derive this result, and then present simulation studies of the architecture.  1 A MAXIMUM LIKELIHOOD FORMULATION  The starting point for the statistical formulation of a localist attractor network is  a mixture of Gaussians model. A standard mixture of Gaussians consists of m  Gaussian density functions in n dimensions. Each Gaussian is parameterized by  a mean, a covariance matrix, and a mixture coefficient. The mixture model is gen-  erative, i.e., it is considered to have produced a set of observations. Each obser-  vation is generated by selecting a Gaussian based on the mixture coefficients and  then stochastically selecting a point from the corresponding density function. The  model parameters are adjusted to maximize the likelihood of a set of observations.  The Expectation-Maximization (EM) algorithm provides an efficient procedure for  estimating the parameters.The Expectation step calculates the posterior probabil-  ity qi of each Gaussian for each observation, and the Maximization step calculates  the new parameters based on the previous values and the set of qi.  The mixture of Gaussians model can provide an interpretation for a localist attrac-  tor network, in an unorthodox way. Each Gaussian corresponds to an attractor, and  an observation corresponds to the state. Now, however, instead of fixing the ob-  servation and adjusting the Gaussians, we fix the Gaussians and adjust the obser-  vation. If there is a single observation, and a = 0 and all Gaussians have uniform  spread tr, then Equation 1 corresponds to the Expectation step, and Equation 3 to  the Maximization step in this unusual mixture model.  Unfortunately, this simple characterization of the localist attractor network does  not produce the desired behavior. Many situations produce partial solutions, in  which the observation does not end up at an attractor. For example, if two unidi-  mensional Gaussians overlap significantly, the most likely value for the observa-  tion is midway between them rather than at the mean of either Gaussian.  We therefore extend this mixture-of-Gaussians formulation to better characterize  the localist attractor network. As in the simple model, each of the m attractors is  a Gaussian generator, the mean of which is a location in the n-dimensional state  space. The input to the net, , is considered to have been generated by a stochastic  selection of one of the attractors, followed by the addition of zero-mean Gaussian  noise with variance specified by the attractor. Given a particular observation ,  the an attractor's posterior probability is the normalized Gaussian probability of  , weighted by its mixing proportion. This posterior distribution for the attractors  corresponds to a distribution in state space that is a weighted sum of Gaussians.  We then consider the attractor network as encoding this distribution over states  implied by the attractor posterior probabilities. At any one time, however, the  attractor network can only represent a single position in state space, rather than  ,4 Generative Model for ,4ttractor Dynamics 83  the entire distribution over states. This restriction is appropriate when the state is  an n-dimensional point represented by the pattern of activity over n state units.  To accommodate this restriction, we change the standard mixture of Gaussians  generative model by interjecting an intermediate level between the attractors and  the observation. The first generative level consists of the discrete attractors, the  second is the state space, and the third is the observation. Each observation is  considered to have been generated by moving down this hierarchy:  1. select an attractor x = i from the set of attractors  2. select a state (i.e., a pattern of activity across the state units) based on the  preferred location of that attractor: y = wi + A/'y  3. select an observation z = yG +  The observation z produced by a particular state y depends on the generative  weight matrix G. In the networks we consider here, the observation and state  spaces are identical, so G is the identity matrix, but the formulation allows for z  to lie in some other space. A/'y and ;V'z describe the zero-mean, spherical Gaussian  noise introduced at the two levels, with deviations try and try, respectively.  In comparison with the 2-level Gaussian mixture model described above, this 3-  level model is more complicated but more standard: the observation  is pre-  served as stable data, and rather than the model manipulating the data here it  can be viewed as iteratively manipulating an internal representation that fits the  observation and attractor structure. The attractor dynamics correspond to an iter-  ative search through state space to find the most likely single state that: (a) was  generated by the mixture of Gaussian attractors, and (b) in turn generated the ob-  servation.  Under this model, one could fit an observation  by finding the posterior distribu-  tion over the hidden states (X and Y) given the observation:  p(X = i, Y = ylZ = ) = P(1Y, i)p(Y, i) = p(ly)trip(yli) (4)  P() fy P([Y) '4 'ip(y[i)dy  where the conditional distributions are Gaussian: p(Y = yl X = i) - (y[wi, try)  and p([Y - y) = 6(1Y, trz). Evaluating the distribution in Equation 4 is  tractable, because the palition function is a sum of a set of Gaussian integrals.  Due to the restriction that the network cannot represent the entire distribution, we  do not directly evaluate this distribution but instead adopt a mean-field approach,  in which we approximate the posterior by another distribution Q(X, Y[). Based  on this approximation, the network dynamics can be seen as minimizing an objec-  tive function that describes an upper bound on the negative log probability of the  observation given the model and mean-field parameters.  In this approach, one can choose any form of Q to estimate the posterior distri-  bution, but a better estimate allows the network to approach a maximum like-  lihood solution? We select a simple posterior: (X, Y) = qid(Y - '), where  qi -- (X - i) iS the responsibility assigned to attractor i, and : is the estimate of  the state that accounts for the observation. The delta function over Y is motivated  by the restriction that the explanation of an input consists of a single state.  Given this posterior distribution, the objective for the network is to minimize the  free energy F, described here for a particular input example :  Q(X=i,Y=y) dy  F(q,$,[) = E Q(X=i'Y=y)lnp(,X=i,Y=y )  - q, ln - lnp([$,)- q, lnp(S,]i)  7ri  i i  84 R. S. Zemel and M. C. Mozer  where rci is the prior probability (mixture coefficient) associated with attractor i.  These priors are parameters of the generative model, as are rv, rz, and w.  F(q, Srl)=Eqilnq-d-i + l-Srl'+v qilSr-wi['+nln(rvrz ) (5)  i i .  Given an observation, a good set of mean-field parameters can be determined by  alternating between updating the generative parameters and the mean-field pa-  rameters. The update procedure is guaranteed to converge to a m'mimum of F, as  long as the updates are done asynchronously and each update minimizes F with  respect to a parameter. 8 The update equations for the mean-field parameters are:  2  - (6)  qi - j wjp(lj) (7)  In our simulations, we hold most of the parameters of the generative model con-  stant, such as the priors r, the weights w, and the generative noise in the observa-  tion, az. The only aspect that changes is the generative noise in the state, r v, which  is a single parameter shared by all attractors:  2_ 1  % - 2 '" qil: - wi (8)  i  The updates of Equations 6-8 can be in any order. We typically initialize the state  , to  at time 0, and then cyclically update the qi, O'y, then ,.  This generarive model avoids the problem of spurious attractors described above  for the standard Gaussian mixture model. Intuition into how the model avoids  spurious attractors can be gained by inspecting the update equations. These equa-  tions effectively tie together two processes: moving , closer to some wi than the  others, and increasing the corresponding responsibility qi. As these two processes  evolve together, they act to descrease the noise ry, which accentuates the pull of  the attractor. Thus stable points that do not correspond to the attractors are rare.  2 SIMULATION STUDIES  To create an attractor net, we specify the parameters (rri, wi) associated with the  attractors based on the desired structure of the energy landscape (e.g., Figure lb).  The only remaining free parameter, r, plays an important role in determining  how responsive the system is to the external input.  We have conducted several simulation studies to explore properties of localist at-  tractor networks. Systematic investigations with a 200-dimensional state space and  200 attractors, randomly placed at comers of the 200-D hypercube, have demon-  strated that spurious responses are exceedingly rare unless more than 85% of an  input's features are distorted (Figure 2), and that manipulating parameters such  as noise and prior probabilities has the predicted effects. We have also conducted  studies of localist attractor networks in the domain of visual images of faces. These  simulations have shown that gang effects arise when there is structure among the  attractors. For example, when the attractor set consists of a single view of several  different faces, and multiple views of one face, then an input that is a morphed  face--a linear combination of one of the single-view faces and one view of the gang  face---will end up in the gang attractor even when the initial weighting assigned  to the gang face was less than 40%.  A Generative Model for Attractor Dynamics 85  o  / I '"7  85 90 95 100  % Missing features  95 100  % Missing features  Figure 2: The input must be severely corrupted before the net makes spurious (final  state not at an attractor) or adulterous (final state at a neighbor of the generating  attractor) responses. (a) The percentage of spurious responses increases as r, is  increased. (b) The percentage of adulterous responses increases as rz is decreased.  To test the architecture on a larger, structured problem, we modeled the domain  of three-letter English words. The idea is to use the attractor network as a content  addressable memory which might, for example, be queried to retrieve a word with  P in the third position and any letter but A in the second position, a word such as  HIP. The attractors consist of the 423 three-letter English words, from ACE to ZOO.  The state space of the attractor network has one dimension for each of the 26 letters  of the English alphabet in each of the 3 positions, for a total of 78 dimensions. We  can refer to a given dimension by the letter and position it encodes, e.g., Pa denotes  the dimension corresponding to the letter P in the third position of the word. The  attractors are at the comers of a [-1, +1] TM hypercube. The attractor for a word such  as HIP is located at the state having value -1 on all dimensions except for Hi, 12,  and Pa which have value +1. The external input specifies a state that constrains  the solution. For example, one might specify "P in the third position"by setting  the external input to +1 on dimension Pa and to -1 on dimensions aa, for all letters  a other than E One might specify the absence of a constraint in a particular letter  position, p, by setting the external input to 0 on dimensions ap, for all letters a.  The network's task is to settle on a state corresponding to one of the words,  given soft constraints on the letters. The interactive-activation model of word  perception 7 performs a similar computation, and our implementation exhibits the  key qualitative properties of their model. If the external input specifies a word,  of course the attractor net will select that word. Interesting queries are those in  which the external input underconstrains or overconstrains the solution. We il-  lustrate with one example of the network's behavior, in which the external input  specifies D, E2, and Ga. Because DEG is a nonword, no attractor exists for that  state. The closest attractors share two letters with DEG, e.g., PEG, BEG, DEN, and  DOG. Figure 3 shows the effect of gangs on the selection of a response, BEG.  3 CONCLUSION  Localist attractor networks offer an attractive altemative to standard attractor net-  works, in that their dynamics are easy to control and adapt. We described a sta-  tistical formulation of a type of localist attractor, and showed that it provides a  Lyapunov function for the system as well as a mathematical interpretation for the  network parameters. The dynamics of this system are derived not from intuitive  arguments but from this formal mathematical model. Simulation studies show  that the architecture achieves gang effects, and spurious attractors are rare. This  approach is ineffident if the attractors have compositional structure, but for many  applications of pattern recognition or associative memory, the number of items  86 R. S. Zemel and M. C. Mozer  Iteration 1  .:-'{ &:!: -i::;-:-: ,;j!i '..:a.,-: -:E? , x:.-f, L},-Z} .i;{; I1= .!E" i'...Z-i'. ::{.; i : '- ':-.. '.,  ": -"-:' ;' .;. "  Idon 2  Ion 3  8  Idon 4  Idon 5  8  Figure 3: Simulation of the 3-letter word attractor network, queried with DEG.  Each frame shows the relative activity of attractor units at various points in pro-  cessing. Activity in each frame is normalized such that the most active unit is  printed in black ink; the lighter the ink color, the less active the unit. Only attractor  units sharing at least one letter with DEG are shown. The selection, BEG, is a prod-  uct of a gang effect. The gangs in this example are formed by words sharing two  letters. The most common word beginnings are PE- (7 instances) and DI- (6); the  most common word endings are -AG (10) and -ET (10); the most common first-last  pairings are B-G (5) and D-G (3). One of these gangs supports B1, two support E2,  and three support Ga, hence BEG is selected.  being stored is small. The approach is especially useful in cases where attractor  locations are known, and the key focus of the network is the mutual influence of  the attractors, as in many cognitive modelling studies.  References  [1] Becker, S., Moscovitch, IvL, Behrmann, M., & Joordens, S. (1997).Long-term semantic primin A computational  account and empirical evidence. Journal of Experimental Psychology: Learning, Memory, & Cognition, 23(5), 1059-  1082.  [2] Golden, IL (1988). Probabilistic characterization of neural model computations. In D. Z. Anderson (Ed.), Neural  Infvnnation Processing Systems (pp. 310-316).American Institute of Physics.  [3] Hopfield, J. J. (1982). Neural networks and physical systems with emergent collective computational abilities.  Proc__!_ings of the National Academy of Sciences, 79, 2554-2558.  [4] Kay, L.IvL, Lancaslea; L.R., & Freeman W.J. (1996). R_eafference and attractors in the olfactory system during  odor recognition. Int J Neural Systems, 7(4), 489-95.  [5] Mathis, D. (1997). A computational theory of consciousness in cognition. Unpublished Doctoral Dissertation. Boul-  der, CO: Department of Computer Science, Univemity of Colorado.  [6] Mathis, D., & Mozer, M. C. (1996). Conscious and unconscious perception: A computational theory. In G. Cot-  trell (Ed.), Procer'dings of the Eighteenth Annual Conference of the Cognitive Sc/ence Soc/ety (pp. 324-328). Erlbaum.  [7] McClelland, J. L. & Rumelhart, D. E. (1981). An interactive activation model of context effects in letter percep-  tion: Part I. An account of basic findings. PsychologicalReview, 88, 375-407.  [8] Neal, IL IvL & Hinton, G. E. (1998). A view of the EM algorithm that justifies incremental, sparse, and other  variants. In IvL I. Jordan (Ed.), Lmrning in GraphicaIModels. Kluwer Academic Press.  [9] McRae, K., de Sa, V. R., & Seldenberg, IvL S. (1997) On the nature and scope of featural repreooentations of word  meaning. Journal of Experimental Psychology: General, 126(2), 99-130.  [10] Redish, A.D. & Touretzky, D. S. (1998). The role of the hippocampus in solving the Morris water maze. Neural  Computation, 10(1), 73-111.  [11] Rodrigues, N. C., & Fontanari, J. E (1997). Multivalley structure of attractor neural networks. Journal of Physics  A (Mathematicaland General), 30, 7945-7951.  [12] Samsonovich, A. & McNaughton, B. L. (1997) Pathintegration and cognitive mapping in a continuous attractor  neural network model. Journal of Neuroscience, 17(15),5900-5920.  [13] Saul, L.I<., Jaakkola, T., & Jordan, IvLI. (1996). Mean field theory for sigmoid belief networks. Journal of AI  Research, 4, 61-76.  PART II  NEUROSCIENCE  
Predictive Approaches For Choosing  Hyperparameters in Gaussian Processes  S. Sundararajan  Computer Science and Automation  Indian Institute of Science  Bangalore 560 012, India  sundarcsa. iisc. ernet. in  S. Sathiya Keerthi  Mechanical and Production Engg.  National University of Singapore  10 Kentridge Crescent, Singapore 119260  mpesskguppy. rope. nus. edu. sg  Abstract  Gaussian Processes are powerful regression models specified by  parametrized mean and covariance functions. Standard approaches  to estimate these parameters (known by the name Hyperparam-  eters) are Maximum Likelihood (ML) and Maximum APosterior  (MAP) approaches. In this paper, we propose and investigate pre-  dictive approaches, namely, maximization of Geisser's Surrogate  Predictive Probability (GPP) and minimization of mean square er-  ror with respect to GPP (referred to as Geisser's Predictive mean  square Error (GPE)) to estimate the hyperparameters. We also  derive results for the standard Cross-Validation (CV) error and  make a comparison. These approaches are tested on a number of  problems and experimental results show that these approaches are  strongly competitive to existing approaches.  I Introduction  Gaussian Processes (GPs) are powerful regression models that have gained popular-  ity recently, though they have appeared in different forms in the literature for years.  They can be used for classification also; see MacKay (1997), Rasmussen (1996) and  Williams and Rasmussen (1996). Here, we restrict ourselves to regression problems.  Neal (1996) showed that a large class of neural network models converge to a Gaus-  sian Process prior over functions in the limit of an infinite number of hidden units.  Although GPs can be created using infinite networks, often GPs are specified di-  rectly using parametric forms for the mean and covariance functions (Williams and  Rasmussen (1996)). We assume that the process is zero mean. Let Zv = {Xv, yv}  whereXv = {x(i)' i = 1,...,N}andyv = {y(i)' i = 1,...,N}. Here, y(i)  represents the output corresponding to the input vector x(i). Then, the Gaussian  prior over the functions is given by  p(yNlXN,) -- exp(-Yv(lYv) (1)  where v is the covariance matrix with (i,j) th element  and (.; ) denotes the parametrized covariance function. Now, assuming that the  632 $. $undararajan and $. S. Keerthi  observed output tv is modeled as tv = yv + eN and eN is zero mean multivariate  Gaussian with covariance matrix a2Iv and is independent of yv, we get  p(tNIXN,0) : xP(--tTNC ltN)  (2)lcNl (2)  where Cv = v +a2Iv. Therefore, [Cv]i,j = [v]i,j +a6i,j, where 6i,j = 1 when  i = j and zero otherwise. Note that 0 = (0,a ) is the new set of hyperparameters.  Then, the predictive distribution of the output y(N + 1) for a test case x(N + 1) is  also Gaussian with mean and variance  9(N + 1) r -1  = kN+lC N tN (3)  and  ay(N+) = bN+ - k/+ClkN+ (4)  C(x(N + 1), x(N + 1); I) and kv+l is an N x 1 vector with i th  where bN+ =  element given by C(x(N + 1),x(i);0). Now, we need to specify the covariance  function C(.; 0). Williams and Rasmussen (1996) found the following covariance  function to work well in practice.  (5)  M I M  (x(i),x(j); O) = ao + al 2 xp(i)xp(j) .- voexp(- y. wp (xp(i) - xp(j)) )  p----1 p----1  where xp(i) is the pth component of i tn input vector x(i). The wp are the Auto-  matic Relevance Determination (ARD) parameters. Note that C(x(i),x(j); O) =  (x(i),x(j);O) + aSi,j. Also, all the parameters are positive and it is conve-  nient to use logarithmic scale. Hence, 0 is given by log(ao, al, v0, Wl,..., wM, a2).  Then, the question is: how do we handle 0 ? More sophisticated techniques  like Hybrid Monte Carlo (HMC) methods (Rasmussen (1996) and Neal (1997))  are available which can numerically integrate over the hyperparameters to make  predictions. Alternately, we can estimate 0 from the training data. We restrict  to the latter approach here. In the classical approach, 0 is assumed to be de-  terministic but unknown and the estimate is found by maximizing the likelihood  argmax  (2). That is, OML = 0 p(tvIXv, 0). In the Bayesian approach, 0 is  assumed to be random and a prior p(O) is specified. Then, the MAP estimate  argmax  OMp is obtained as OMp = 0 p(tlX, O)p(O) with the motivation that  the the predictive distribution p(y(N + 1)lx(N + 1), Zv) can be approximated as  p(y(N + 1)lx(N + 1), ZN, 04). With this background, in this paper we propose  and investigate different predictive approaches to estimate the hyperparameters  from the training data.  2 Predictive approaches for choosing hyperparameters  Geisser (1975) proposed Predictive Sample Reuse (PSR) methodology that can be  applied for both model selection and parameter estimation problems. The basic  idea is to define a partition scheme P(N,n,r) such that P()_. = tZ . o   v-, Z ) is  ita partition belonging to a set F of partitions with Z_n, Z/  representing the  N - n retained and n omitted data sets respectively. Then, the unknown  is esti-  mated (or a model Mj is chosen among a set of models indexed by j = 1,..., J)  by means of optimizing a predictive measure that measures the predictive perfor-  mance on the omitted observations X/  by using the retained observations Z_n  averaged over the partitions (i  F). In the special case of n = 1, we have the  leave one out strategy. Note that this approach was independently presented in the  Predictive Approaches for Choosing Hyperparameters in Gaussian Processes 633  name of cross-validation (CV) by Stone (1974). The well known examples are the  standard CV error and negative of average predictive likelihood. Geisser and Eddy  N  Z?,  (1979) proposed to maximize rIi=lp(t()[x(i), Mj) (known as Geisser's surro-  gate Predictive Probability (GPP)) by synthesizing Bayesian and PSR methodology  in the context of (parametrized) model selection. Here, we propose to maximize  rI=l p(t(i)lx(i), z?, 0) to estimate 0, where Z? is obtained from ZN by removing  the i th sample. Note that p(t(i)]x(i), Z?, 0) is nothing but the predictive distribu-  tion p(y(i)[x(i), Z?, 0) evaluated at y(i) - t(i). Also, we introduce the notion of  i E/N=1 E((y(i) - t(i)) 2)  Geisser's Predictive mean square Error (GPE) defined as   (where the expectation operation is defined with respect to p(y(i)lx(i), Z?, 0)) and  propose to estimate 0 by minimizing GPE.  2.1 Expressions for GPP and its gradient  The objective function corresponding to GPP is given by  N  1  G(o) - log(p(t(i)lx(i), (6)  i----1  From (S) and (4) we get  i k (t(i) - + (7)  G(O) --  i----1 Y )  2cr2,i,  where (i) [c?]r[C?]-lt? and   -- fly(i)  is an N - 1 x N - 1 matrix obtained from Cv by removing the i th column and  i t row. Similarly, [() and c i) are obtained from tv and ci (i.e., i  column of  Cv) respectively by removing the i  element. Then, G() and is gradien can be  computed ecienly using the following result.  N  1  1  2N  log fly(i) -]-  log  i=1  ['(i)]T[g'(i)]--lr'(i) Here, C( )  = eli -- ['i J ["NJ 'i '  Theorem 1  by  The objective function G(O) under the Gaussian Process model is given  G(O) - 1 i.l q(i) I N 1 2  2N c-it 27 ylogeii + log r (8)  '---- i1  where ii denotes the ita diagonal entry of C x and qv (i) denotes the i th element  of qv = cltN  ItS gradient is given by  OG(O) = I /1 (1 + q-(i))(sj'--!) +  OOj 2N .__ eli Cii  - __ OCN cl tN  where sj,i - c/Tc----' rj - -C 1 o0  -- O0 i ,  denotes the i  column of the matrix C 1 .  1 v (r(i)]    qv(i), ii / (9)  i----1  and qv = cltN . Here, i  Thus, using (8) and (9) we can compute the GPP and its gradient. We will give  meaningful interpretation to the different terms shortly.  2.2 Expressions for CV function and its gradient  We define the CV function as  N  1  H(O) =   (t(i) - (i))   i=1  (10)  634 $. $undararajan and $. $. Keerthi  where ?(i) is the mean of the conditional predictive distribution as given above.  Now, using the following result we can compute H(O) efficiently.  Theorem 2 The CV function H(O) under the Gaussian model is given by  N  1 (qN(i)) 2 (11)  i=1  and its gradient is given by  OH(O) 1  ooj N  il (qv(i)rj(i) qv(i)) (sj,i)  where 8j,i, rj, qlv(i) and ii are as defined in theorem 1.  2.3 Expressions for GPE and its gradient  The GPE function is defined as  1  =  N  N  /(t(i) - y(i)) 2 p(y(i)lx(i),z(),o) dy(i)  i=1  (13)  which can be readily simplified to  N N  fly(i)  i=1 i=1  (14)  On comparing (14) with (10), we see that while CV error minimizes the deviation  from the predictive mean, GPE takes predictive variance also into account. Now,  the gradient can be written as  0(7(0) OH(O) 1 N [lh2rg.TOC N  aOj = OOj +  E \ii] ' O0j ei (15)  i----1  where we have used the results a 2 = i Oil OCv  OCq  __  y(i) ii' 00 -- eiT-i e i and o0 --  oC .  -C 1 o0 C 1 Here ei denotes the i th column vector of the identity matrix IN.  2.4 Interpretations  More insight can be obtained from reparametrizing the covariance function as fol-  lows.  M I M  C(x(i),x(j); O) = 0.2 (0_ 1 E xp(i)xp(j)+oexp(-  p=l p=l  (16)  where ao - a 2 ao, al - a 2 al, vo = a 2 Vo. Let us define P(x(i),x(j);O) :  - - ', where Ci,, i,j  l-C(x(i),x(j); 0). Then rr 1 -- 0 '2 Cr 1. Therefore, ci,j -   denote the (i, j)th element of the matrices C31 and pl respectively. From theorem  2 (see (10) and (11)) we have t(i)- (i)  (8) as  I q(i)  (7(0) = 2N0. 2 Z P-i  i1  Then, we can rewrite  N  1 logii + log2r0.  2N  (17)  Predictive Approaches for Choosing Hyperparameters in Gaussian Processes 635  Here, v = pltv and, i, ii denote, respectively, the i th column and i t' diagonal  entry of the matrix pl. Now, by setting the derivative of (17) with respect to a 2  to zero, we can infer the noise level as  I N  6. 2 _  y (i) (18)  i=1 Pii  Similarly, the CV error (10) can be rewritten as  1 v qv(i) (19)  i=1 Pii  Note that H(I) is dependent only on the ratio of the hyperparameters (i.e., on  a0, al, v0) apart from the ARD parameters. Therefore, we cannot infer the noise  level uniquely. However, we can estimate the ARD parameters and the ratios  a0, al, 0. Once we have estimated these parameters, then we can use (18) to es-  timate the noise level. Next, we note that the noise level preferred by the GPE  criterion is zero. To see this, first let us rewrite (14) under reparametrization as  I N k 1  a(o) = :2 +  Pii  ii  '----  (20)  Since qv(i) and ii are independent of a 2, it follows that the GPE prefers zero as  the noise level, which is not true. Therefore, this approach can be applied when,  either the noise level is known or a good estimate of it is available.  3 Simulation results  We carried out simulation on four data sets. We considered MacKay's robot arm  problem and its modified version introduced by Neal (1996). We used the same  data set as MacKay (2-inputs and 2-outputs), with 200 examples in the training  set and 200 in the test set. This data set is referred to as 'data set 1' in Table  1. Next, to evaluate the ability of the predictive approaches in estimating the  ARD parameters, we carried out simulation on the robot arm data with 6 inputs  (Neal's version), denoted as 'data set 2' in Table 1. This data set was generated by  adding four further inputs, two of which were copies of the two inputs corrupted  by additive zero mean Gaussian noise of standard deviation 0.02 and two further  irrelevant Gaussian noise inputs with zero mean and unit variance (Williams and  Rasmussen (1996)). The performance measures chosen were average of Test Set  Error (normalized by true noise level of 0.0025) and average of negative logarithm of  predictive probability (NLPP) (computed from Gaussian density function with (3)  and (4)). Friedman's [1] data sets 1 and 2 were based on the problem of predicting  impedance and phase respectively from four parameters of an electrical circuit.  Training sets of three different sizes (50, 100, 200) and with a signal-to-noise ratio  of about 3:1 were replicated 100 times and for each training set (at each sample  size N), scaled integral squared error (ISE = fv(Y(x)-(x))2dx  varo y(x)  and NLPP were  computed using 5000 data points randomly generated from a uniform distribution  over D (Friedman (1991)). In the case of GPE (denoted as G in the tables), we  used the noise level estimate generated from Gaussian distribution with mean NL,  (true noise level) and standard deviation 0.03 NL.. In the case of CV, we estimated  the hyperparameters in the reparametrized form and estimated the noise level using  (18). In the case of MAP (denoted as MP in the tables), we used the same prior  636 S. Sundararajan andS. S. Keerthi  Table 1: Results on robot arm data sets. Average of normalized test set error (TSE)  and negative logarithm of predictive probability (NLPP) for various methods.  Data Set: 1 Data Set: 2  TSE NLPP TSE NLPP  ML 1.126 -1.512 1.131 -1.512  MP 1.131 -1.511 1.181 -1.489  Gp 1.115 -1.524 1.116 -1.516  CV 1.112 -1.518 1.146 -1.514  G 1.111 -1.524 1.112 -1.524  Table 2: Results on Friedman's data sets. Average of scaled integral squared error  and negative logarithm of predictive probability (given in brackets) for different  training sample sizes and various methods.  Data Set: 1 Data Set: 2  N = 50 N = 100 N = 200 N = 50 N = 100 N = 200  ML 0.43(7.24) 0.19(6.71) 0.10(6.49) 0.26(1.05) 0.16(0.82) 0.11(0.68)  MP 0.42(7.18) 0.22(6.78) 0.12(6.56) 0.25(1.01) 0.16(0.82) 0.11(0.69)  Gp 0.47(7.29) 0.20(6.65) 0.10(6.44) 0.33(1.25) o.20(0.86) 0.12(0.70)  CV 0.55(7.27) 0.22(6.67) 0.10(6.44) 0.42(1.36) 0.21(0.91) 0.13(0.70)  es 0.35(7.10) 0.15(6.60) 0.08(6.37) 0.28(1.20) 0.18(0.85) o.12(o.63)  given in Rasmussen (1996). The GPP approach is denoted as G in the tables. For  all these methods, conjugate gradient (CG) algorithm (Rasmussen (1996)) was used  to optimize the hyperparameters. The termination criterion (relative function error)  with a tolerance of 10 -7 was used, but with a constraint on the maximum number  of CG iterations set to 100. In the case of robot arm data sets, the algorithm  was run with ten different initial conditions and the best solution (chosen from  respective best objective function value) is reported. The optimization was carried  out separately for the two outputs and the results reported are the average TSE,  NLPP. In the case of Friedman's data sets, the optimization algorithm was run  with three different initial conditions and the best solution was picked up. When  N = 200, the optimization algorithm was run with only one initial condition. For  all the data sets, both the inputs and outputs were normalized to zero mean and  unit variance.  From Table 1, we see that the performances (both TSE and NLPP) of the predic-  tive approaches are better than ML and MAP approaches for both the data sets.  In the case of data set 2, we observed that like ML and MAP methods, all the  predictive approaches rightly identified the irrelevant inputs. The performance of  GPE approach is the best on the robot arm data and demonstrates the usefulness  of this approach when a good noise level estimate is available. In the case of Fried-  man's data set I (see Table 2), the important observation is that the performances  (both ISE and NLPP) of GPP, CV approaches are relatively poor at low sample size  (N = 50) and improve very well as N increases. Note that the performances of the  predictive approaches are better compared to the ML and MAP methods starting  from N = 100 onwards (see NLPP). Again, GPE gives the best performance and  the performance at low sample size (N = 50) is also quite good. In the case of  Friedman's data set 2, the ML and MAP approaches perform better compared to  the predictive approaches except GPE. The performances of GPP and CV improve  Predictive Approaches for Choosing Hyperparameters in Gaussian Processes 63 7  as N increases and are very close to the ML and MAP methods when N - 200.  Next, it is clear that the MAP method gives the best performance at low sample  size. This behavior, we believe, is because the prior plays an important role and  hence is very useful. Also, note that unlike data set 1, the performance of GPE is  inferior to ML and MAP approaches at low sample sizes and improves over these  approaches (see NLPP) as N increases. This suggests that the knowledge of the  noise level alone is not the only issue. The basic issue we think is that the predictive  approaches estimate the predictive performance of a given model from the training  samples. Clearly, the quality of the estimate will become better as N increases.  Also, knowing the noise level improves the quality of the estimate.  4 Discussion  Simulation results indicate that the size N required to get good estimates of predic-  tive performance will be dependent on the problem. When N is sufficiently large, we  find that the predictive approaches perform better than ML and MAP approaches.  The sufficient number of samples can be as low as 100 as evident from our results  on Friedman's data set 1. Also, MAP approach is the best, when N is very low.  As one would expect, the performances of ML and MAP approaches are nearly  same as .iV increases. The comparison with the existing approaches indicate that  the predictive approaches developed here are strongly competitive. The overall cost  for computing the function and the gradient (for all three predictive approaches)  is O(MN3). The cost for making prediction is same as the one required for ML  and MAP methods. The proofs of the results and detailed simulation results will  be presented in another paper (Sundararajan and Keerthi, 1999).  References  Friedman, J.H., (1991) Multivariate Adaptive Regression Splines, Ann. of Stat., 19, 1-141.  Geisser, S., (1975) The Predictive Sample Reuse Method with Applications, Journal of  the American Statistical Association, 70, 320-328.  Geisser, S., and Eddy, W.F, (1979) A Predictive Approach to Model Selection, Journal  of the American Statistical Association, 74, 153-160.  MacKay, D.J.C. (1997) Gaussian Processes - A replacement .for neural networks ?, Avail-  able in Postscript via URL http://www. woI. ra.phy. cam. ac.uk/mackay/.  Neal, R.M. (1996) Bayesian Learning .for Neural Networks, New York: Springer-Verlag.  Neal, R.M. (1997) Monte Carlo Implementation of Gaussian Process Models for Bayesian  Regression and Classification. Tech. Rep. No. 9702, Dept. of Statistics, University of  Toronto.  Rasmussen, C. (1996) Evaluation of Gaussian Processes and other Methods .for Non-Linear  Regression, Ph.D. Thesis, Dept. of Computer Science, University of Toronto.  Stone, M. (1974) Cross-Validatory Choice and Assessment of Statistical Predictions (with  discussion), Journal of Royal Statistical Society, ser. B, 36, 111-147.  Sundararajan, S., and Keerthi, S.S. (1999) Predictive Approaches for Choosing Hy-  perparameters in Gaussian Processes, submitted to Neural Computation, available at:  http : // guppy. mpe. nus. edu. sg/~mpes sk/gp /gp.htmI.  Williams, C.K.I., and Rasmussen, C.E. (1996) Gaussian Processes for Regression. In  Advances in Neural Information Processing Systems 8, ed. by D.S.Touretzky, M.C.Mozer,  and M.E.Hasselmo. MIT Press.  
Inference for the Generalization Error  Claude Nadeau  CIRANO  2020, University,  Montreal, Qc, Canada, H3A 2A5  j cnadeauOaltavista. net  Yoshua Bengio  CIRANO and Dept. IRO  Universitd de Montrdal  Montreal, Qc, Canada, H3C 3J7  bengioyOiro. umontreal. ca  Abstract  In order to to compare learning algorithms, experimental results reported  in the machine learning litterature often use statistical tests of signifi-  cance. Unfortunately, most of these tests do not take into account the  variability due to the choice of training set. We perform a theoretical  investigation of the variance of the cross-validation estimate of the gen-  eralization error that takes into account the variability due to the choice  of training sets. This allows us to propose two new ways to estimate  this variance. We show, via simulations, that these new statistics perform  well relative to the statistics considered by Dietterich (Dietterich, 1998).  1 Introduction  When applying a learning algorithm (or comparing several algorithms), one is typically  interested in estimating its generalization error. Its point estimation is rather trivial through  cross-validation. Providing a variance estimate of that estimation, so that hypothesis test-  ing and/or confidence intervals are possible, is more difficult, especially, as pointed out in  (Hinton et al., 1995), if one wants to take into account the variability due to the choice of  the training sets (Breiman, 1996). A notable effort in that direction is Dietterich's work (Di-  etterich, 1998). Careful investigation of the variance to be estimated allows us to provide  new variance estimates, which turn out to perform well.  Let us first lay out the framework in which we shall work. We assume that data are avail-  able in the form Z = {Z,..., Zn}. For example, in the case of supervised learning,  Zi = (Xi, Yi)  Z C_ R p+q, where p and q denote the dimensions of the Xi's (inputs)  and the Y/'s (outputs). We also assume that the Zi's are independent with Zi ,',, P(Z).  Let (D; Z), where D represents a subset of size nl < n taken from Z, be a function  ,Z TM x ,Z -->  For instance, this function could be the loss incurred by the decision  that a learning algorithm trained on D makes on a new example Z. We are interested in  estimating ntz -- E[(Z; Zn+)] where Zn+ "' P(Z) is independent of Z. Subscript  n stands for the size of the training set (Z here). The above expectation is taken over Z  and Zn+, meaning that we are interested in the performance of an algorithm rather than  the performance of the specific decision function it yields on the data at hand. According to  Dietterich's taxonomy (Dietterich, 1998), we deal with problems of type 5 through 8, (eval-  uating learning algorithms) rather then type 1 through 4 (evaluating decision functions). We  call n/z the generalization error even though it can also represent an error difference:   Generalization error  We may take  (D; Z) = (D; (X, Y)) = Q(F(D)(X), Y), (1)  308 C. Nadeau and Y. Bengio  where F(D) (F(D) : lRv  lRq) is the decision function obtained when training an  algorithm on D, and Q is a loss function measuring the inaccuracy of a decision. For  instance, we could have Q(0, y) = I[0  y], where I[ ] i the indicator function, for  classification problems and Q(O, y) -II - u II 2, where is II' I is the Euclidean norm, for  "regression" problems. In that case ,it is what most people call the generalization error.   Comparison of generalization errors  Sometimes, we are not interested in the performance of algorithms per se, but instead in  how two algorithms compare with each other. In that case we may want to consider  (D;Z) = (D;(X,Y)) =Q(FA(D)(X),Y) -Q(FB(D)(X),Y), (2)  where FA(D) and FB(D) are decision functions obtained when training two algorithms  (A and B) on D, and Q is a loss function. In this case ,/z would be a difference of  generalization errors as outlined in the previous example.  The generalization error is often estimated via some form of cross-validation. Since there  are various versions of the latter, we lay out the specific form we use in this paper.   Let Sj be a random set of nx distinct integers from {1,...,n}(n < n). Here nx  represents the size of the training set and we shall let n2 - n - nx be the size of the test  set.   Let $x,... $j be independent such random sets, and let $ = {1,..., n} \ $j denote the  complement of Sj.   Let Z$ s = {Zli 6 $j} be the training set obtained by subsampling Z according to the  random index set Sj. The corresponding test set is Z$ = {Zili  $ }.   Let L(j, i) = (Z$s; Zi). According to (1), this could be the error an algorithm trained  on the training set Z$ makes on example Zi. According to (2), this could be the difference  of such errors for two different algorithms.   Let j = K Y'k=x L(j, i) where if,..., i3c are randomly and independently drawn  from $. Here we draw K examples from the test set Z$ with replacement and compute  the average error committed. The notation does not convey the fact that j depends on K,  n and n2.  ^ -- Y'4$? L(j, i) denote what bj becomes as K increases   Let ttf = limx-}oo j = = ,  without bounds. Indeed, when sampling infinitely often from Zs, each Zi (i  S) is  x , yielding the usual "average test egor". The use of K is  chosen with relative frequency   just a mathematical device to make the test exmples sampled independently from S.  Then the cross-validation estimate of the generalization egor consider in this paper is  n2gK 1    nJ =  j'  =1  We note that this an unbiased estimator of n = E[E(Z  , Zn+)] (not the same as n).  This paper is about the estimation of the vance of n=   m a  We first study theoretically  this vmiance in section 2, leading to two new vmiance estimators developped in section 3.  Stion 4 shows p of a simulation study we peffomed to see how the proposed statistics  behave compared to statistics already in use.  2 Analysis of Vat[  n/J J  Here we study Vat[ n r,c This is important to understand why some inference proce-  n tJ J'  dures about m tt presently in use are inadequate, as we shall underline in section 4. This  investigation also enables us to develop estimators of Vat[ n/j ] in section 3. Before we  proceed, we state the following useful lemma, proved in (Nadeau and Bengio, 1999).  Inference for the Generalization Error 309  Lemma 1 Let U1,..., Uk be random variables with common mean fi, common variance  6 and Cov[Ui, Uj] = % Vi  j. Let rr =  be the correlation between Ui and Uj (i  j).  Let  = k -1 I I  Ei=l Ui a S = 1  - i=i (Ui - 0): be the sample mean and sample  variance respectively. Then E[Sb] = 6 - 7 and Var[O] = 7 + (-) - 6 ( + )  To study Vat[ n;',K1  m J  we need to define the following covariances.   Let a0 = ao(nl) = Var[L(j, i)] when i is randomly drawn from S.   Let o- = rr(n,n2) = Cov[L(j,i),L(j,i')] for i and i' randomly and indepen-  dently drawn from $.   Let rr2 = a2(n,n2) = C[L(j,i),L(j',i')], with j  j', i and i' randomly and  independently drawn from S and S, respectively.  * Letaa = aa(n) = C[L(j,i),L(j,i')] fori, i'  S and/ i'. is is not the  same as a. In fact, it may be shown that  a = Cov[L(j,i) L(j,i')] ao (n2- 1)a a0  , = + = as + (3)  2 2  Let us look at the mean and viance of  and  . Concerning expectations, we  obviously have E[] = n and thus E[ = . From Lemma 1, we have  o- which implies  Var[fiF] = Vat[ lim fi2] = lim Var[fij] =  It can also be shown that Cov[fi2, fly] = a2, j  f, and therefore (using Lemma 1)  mJ l = 2 + j = 2 + K --  j (4)  We shall often encounter a0, a, a2, aa in the future, so some knowlge about those quan-  tities is valuable. Here's what we can say about them.  Proposition 1 For given n and n2, we have 0  a2'  al  ao and 0  Proof See (Nadeau and Bengio, 1999).  A natural question about the estimator n2 ?,K is how hi, n2, K and J affect its viance.  Proposition 2 The variance of n r,K is non-increasing in J, K and  nJ  Proof See (Nadeau and Bengio, 1999).  Clearly, increasing K leads to smaller variance because the noise introduced by sampling  with replacement from the test set disappears when this is done over and over again. Also,  averaging over many train/test (increasing J) improves the estimation of m/. Finally, all  things equal elsewhere (r fixed among other things), the larger the size of the test sets, the  better the estimation of m/.  ^K  The behavior of Vat[ n2j ] with respect to rl is unclear, but we conjecture that in most  n  situations it should decrease in n. Our argument goes like this. The variability in n2 fi  comes from two sources: sampling decision rules (training process) and sampling testing  examples. Holding r2, J and K fixed freezes the second source of variation as it solely de-  pends on those three quantities, not r. The problem to solve becomes: how does n affect  the first source of variation? It is not unreasonable to say that the decision function yielded  by a learning algorithm is less variable when the training set is large. We conclude that the  first source of variation, and thus the total variation (that is Vat[ n2r, Kll is decreasing in  nJ  r. We advocate the use of the estimator  310 C. Nadeau and Y. Bengio  as it is easier to compute and has smaller variance than  Var[  nllj ] = lim Vat[ n2f ] = 0'2 +  K-- oo  where p =  = Cr[, ].  3 Estimation of Vat[  1 'J (J' '1, '2 held constant).  0'1-0'2 (l-p) (6)  j =0' p+  '  We are interested in estimating n2,,2 Vat[ 2b] where ,:b is as defined in (5). We  nl 'J - 1 1  provide two different estimators of Vat[ n ^ oo  m/J ]' The first is simple but may have a positive  or negative bias for the actual variance. The second is meant to be conservative, that is,  if our conjecture of the previous section is correct, its expected value exceeds the actual  variance.  1st Method: Corrected Resampled t-Test. Let us recall that  5 2 be the sample variance of the 's. According to Lemma 1,  E[a 2] = (1 - p) = .... p + =  so that ( + l)2 is an unbiased estimator of Vat[  n2hoo _ 1 J  - Let  Vg/'[ n2 h]  = 1 p , (7)  7+l-p  The only problem is  that p = p(n n2) = a2(m,n2) the correlation between the / s, is unknown and  difficult to estimate. We use a naive surrogate for p as follows. Let us recall that  la ? i (Zs' Zi). For the purpose of building our estimator, let us make the  ^ = ,  approximation that (Z&; Zi) depends only on Zi and n. Then it is not hard to show (see  (Nadeau and Bengio, 1999)) that the correlation between the O,s becomes ' There-  n+n2 '  fore our first estimator of Vat[  nlJ ] 1--po] wherepo = po(nl,n2) = n+n2'  ('} ) "2 ;'1 accord-  n_z 52 This will tend to overestimate or underestimate Var[  that is + n   ing to whether Po > p or Po < p. Note that this first method basically does not require  any more computations than that already performed to estimate generalization error by  cross-validation.  2nd Method: Conservative Z. Our second method aims at overestimating Vat[  which will lead to conservative inference, that is tests of hypothesis with actual size less  than the nominal size. This is important because techniques currently in use have the  opposite defect, that is they tend to be liberal (tests with actual size exceeding the nominal  size), which is typically regarded as less desirable than conservative tests.  Estimating n,,.2 unbiasedly is not trivial as hinted above. However we may estimate  nl J  unbiasedly n2 0'. = Vat[ n  c n n 52  n i njlwheren = [J--n2 <n.Let n j be the unbiased  estimator, developed below, of the above variance. We argued in the previous section that  Vat[ nm Vat[ n2;',oo] Therefore n2-2  n[ tj ] > n *j . n[ oj will tend to overestimate n2rr2 that is  -- nl "J'  E[ n22] n20'2 n2rr2   rl "J'  n2 0.2  Here's how we may estimate n  without bias. For simplicity, assume that n is even.  We have to randomly split our data Z into two distinct data sets, D and D, of size   each. Let () be the statistic of interest ( n2 c c  n t  computed on D. This involves, among  other things, drawing J train/test subsets from D. Let ) be the statistic computed on  D. Then () and ) are independent since D and D  independent data sets, so  that (() ()+) )2 = )2  2 +  2  ((1) - 1) is an unbiased estimate  of 2 a. This splitting process may be repeat M times. This yields Dm and D, with  InJbrence for the Generalization Error 311  Dm tO D = Z[ , D, Cl D = 0 for m = 1,..., M. Each split yields a pair (fi(m), bm))  that is such that  ^ ^ c 2 n2 _2  5 (/(m) -/(m)) is unbiased for n[ oj. This allows us to use the following  n2  unbiased estimator of [ o-.'  M  '5-, = 1  n[ 2M E ((m) -- m)) 2- (8)  m=l  Note that, according to Lemma 1 Vat[ n22  ^c 2  : -- (m)) ](/' 1._)  ' n al  Var[((m) + with  r -- Corr[(b(i) ^c 2  -/(i)) , ((J) - j))2] for i  j. Simulations suggest that r is usually  close to 0, so that the above variance decreases roughly like -- for M up to 20, say. The  second method is therefore a bit more computation intensive, since requires to perform  cross-validation M times, but it is expected to be conservative.  4 Simulation study  We consider five different test statistics for the hypothesis H0  m P = p0. The first three  are methods already in use in the machine learning community, the last two are the new  methods we put forward. They all have the following form  rejectHoif ]   > c. (9)  Table 1 describes what they are  We performed a simulation study to inves-  tigate the size (probability of rejecting the null hypothesis when it is true) and  the power (probability of rejecting the null hypothesis when it is false) of the  five test statistics shown in Table 1. We consider the problem of estimating gen-  eralization errors in the Letter Recognition classification problem (available from  www. ics. uci. edu/pub/machine-learnin-databases). The learning algo-  rithms are  1. Classification tree  We used the function tree in Splus version 4.5 for Windows. The default argu-  ments were used and no pruning was performed. The function predict with option  type="class" was used to retrieve the decision function of the tree: FA (Z$) (X).  Here the classification loss function LA(j, i) = I[FA(Z$ s)(Xi)  Y/] is equal  to 1 whenever this algorithm misclassifies example i when the training set is  otherwise it is 0.  2. First nearest neighbor  We apply the first nearest neighbor rule with a distorted distance metric to pull  down the performance of this algorithm to the level of the classification tree (as  in (Dietterich, 1998)). We have LB (j, i) equal to 1 whenever this algorithm mis-  classifies example i when the training set is $j; otherwise it is 0.  In addition to inference about the generalization errors n ]ZA and m B associated with  those two algorithms, we also consider inference about nIZA_B -- nIZA -- nIZB --  E[LA_8(j,i)] where LA_8(j,i) = LA(j,i) -- Ls(j,i).  We sample, without replacement, 300 examples from the 20000 examples available in the  Letter Recognition data base. Repeating this 500 times, we obtain 500 sets of data of the  form {Z1,..., Za00}. Once a data set Z a = {Z,... Za00} has been generated, we may  When comparing two classifiers, (Nadeau and Bengio, 1999) show that the t-test is closely re-  lated to McNemar's test described in (Dietterich, 1998). The 5 x 2 cv procedure was developed in  (Dietterich, 1998) with solely the comparison of classifiers in mind but may trivially be extended to  other problems as shown in (Nadeau and Bengio, 1999).  312 C. Nadeau and Y. Bengio  t-test (McNemar) n2 $V(L(1,i)) tn2_l,l_a/2 > 1  nl  resampl t    - 2  7a tJ-l,l-a/2 1 + J > 1  nl  n/2   Dietterich's 5 x 2 cv n/2l see (Diettefich, 1998) t5,1_a/2  n2 2  1' conservative Z -2  -  2 " < 1  n n J Zl-a/2  2: corn resampled t n + ts_x,x_./ +  Table 1: Description of five test statistics in relation to the rejection criteria shown in (9).  Z v and tk,v refer to the quarttile p of the N(0, 1) and Student tk distribution respectively.  2 is as defined above (7) and SV(L(1, i)) is the sample variance of the L(1, i)'s involved  in ,2 . The ratio (which comes from proper application of Lemma 1, except for  Dietterich's 5 x 2 cv and the Conservative Z) indicates if a test will tend to be conservative  (ratio less than 1) or liberal (ratio greater than 1).  perform hypothesis testing based on the statistics shown in Table 1. A difficulty arises  however. For a given n (n - 300 here), those methods don't aim at inference for the same  generalization error. For instance, Dietterich's 5 x 2 cv test aims at n/2[, while the others  2. for  aim at m P where nl would usually be different for different methods (e.g.  9. for the resampled t test statistic, for instance). In order  the t test statistic, and nl = -  to compare the different techniques, for a given n, we shall always aim at n/2P, i.e. use  = n2 f with J > 1, normal usage would call for  n However, for statistics involving n  nl .  n Therefore, for those statistics, we  nl to be 5 or l0 times larger than n2, not nl = n2 = .  n n/10 c  also use nl =  and n2 =  so that  = 5. To obtain ./2 s we simply throw out 40%  of the data. For the conservative Z, we do the variance calculation as we would normally  n2 2 n/10.2  do (n2 =  for instance) to obtain ./2_.2os = 2./5 s' However, in the numerator we  compute both f and -2 o  ./2 s ./2 s instead of  = ---2 P, as explained above.  -/106.2  Note that the rationale that led to the conservative Z statistics is maintained, that is 2n/5  overestimates both Var[ nn512] and Var[ nnS;]  E [ n/16'2] n/lr'ml  2,/5 s] > Vat[ >  -- n/2 IJ J  Vat[ n/2  n/21J 1'  Figure 1 shows the estimated power of different statistics when we are interested in/z4 and  /4-. We estimate powers by computing the proportion of rejections of H0. We see that  tests based on the t-test or resampled t-test are liberal, they reject the null hypothesis with  probability greater than the prescribed a = 0.1, when the null hypothesis is true. The other  tests appear to have sizes that are either not significantly larger the 10% or barely so. Note  that Dietterich's 5 x 2cv is not very powerful (note that its curve has the lowest power on  the extreme values of rnu0). To make a fair comparison of power between two curves, one  should mentally align the size (bottom of the curve) of these two curves. Indeed, even the  resampled t-test and the conservative Z that throw out 40% of the data are more powerful.  That is of course due to the fact that the 5 x 2 cv method uses J = 1 instead of J = 15.  This is just a glimpse of a much larger simulation study. When studying the corrected  resampled t-test and the conservative Z in their natural habitat (nl "  = 5-6 and n2 = 00)' we  see that they are usually either right on the money in term of size, or slightly conservative.  Their powers appear equivalent. The simulations were performed with J up to 25 and M  up to 20. We found that taking J greater than 15 did not improve much the power of the  In./brence for the Generalization Error 313  Figure 1: Powers of the tests about H0  /A = /0 (left panel) and Ho ' A-B = 0  (right panel) at level a - 0.1 for varying/0. The dotted vertical lines correspond to the  95% confidence interval for the actual/A or IA-B, therefore that is where the actual size  of the tests may be read. The solid horizontal line displays the nominal size of the tests,  i.e. 10%. Estimated probabilities of rejection laying above the dotted horizontal line are  significatively greater than 10% (at significance level 5%). Solid curves either correspond  to the resampled t-test or the corrected resampled t-test. The resampled t-test is the one that  has ridiculously high size. Curves with circled points are the versions of the ordinary and  corrected resampled t-test and conservative Z with 40% of the data thrown away. Where it  matters J = 15, M = 10 were used.  statistics. Taking M = 20 instead of M = 10 does not lead to any noticeable difference  in the distribution of the conservative Z. Taking M = 5 makes the statistic slightly less  conservative. See (Nadeau and Bengio, 1999) for further details.  5 Conclusion  This paper addresses a very important practical issue in the empirical validation of new  machine learning algorithms: how to decide whether one algorithm is significantly better  than another one. We argue that it is important to take into account the variability due to  the choice of training set. (Dietterich, 1998) had already proposed a statistic for this pur-  pose. We have constructed two new variance estimates of the cross-validation estimator  of the generalization error. These enable one to construct tests of hypothesis and confi-  dence intervals that are seldom liberal. Furthermore, tests based on these have powers that  are unmatched by any known techniques with comparable size. One of them (corrected  resampled t-test) can be computed without any additional cost to the usual K-fold cross-  validation estimates. The other one (conservative Z) requires M times more computation,  where we found sufficiently good values of M to be between 5 and 10.  References  Breiman, L. (1996). Heuristics of instability and stabilization in model selection. Annals of Statistics,  24 (6):2350-2383.  Dietterich, T. (1998). Approximate statistical tests for comparing supervised classification learning  algorithms. Neural Computation, 10 (7): 1895-1924.  Hinton, G., Neal, R., Tibshirani, R., and DELVE team members (1995). Assessing learning proce-  dures using DELVE. Technical report, University of Toronto, Department of Computer Science.  Nadeau, C. and Bengio, Y. (1999). Inference for the generalisation error. Technical Report in prepa-  ration, CIRANO.  
Bayesian Reconstruction of 3D Human Motion  from Single-Camera Video  Nicholas R. Howe  Department of Computer Science  Cornell University  Ithaca, NY 14850  nihowe @ cs. cornell. edu  Michael E. Leventon  Artificial Intelligence Lab  Massachusetts Institute of Technology  Cambridge, MA 02139  leventon @ ai. mit. edu  William T. Freeman  MERL - a Mitsubishi Electric Research Lab  201 Broadway  Cambridge, MA 02139  fre erna n @ me rl. com  Abstract  The three-dimensional motion of humans is underdetermined when the  observation is limited to a single camera, due to the inherent 3D ambi-  guity of 2D video. We present a system that reconstructs the 3D motion  of human subjects from single-camera video, relying on prior knowledge  about human motion, learned from training data, to resolve those am-  biguities. After initialization in 2D, the tracking and 3D reconstruction  is automatic; we show results for several video sequences. The results  show the power of treating 3D body tracking as an inference problem.  1 Introduction  We seek to capture the 3D motions of humans from video sequences. The potential appli-  cations are broad, including industrial computer graphics, virtual reality, and improved  human-computer interaction. Recent research attention has focused on unencumbered  tracking techniques that don't require attaching markers to the subject's body [4, 5], see  [ 12] for a survey. Typically, these methods require simultaneous views from multiple cam-  eras o  Motion capture from a single camera is important for several reasons. First, though under-  determined, it is a problem people can solve easily, as anyone viewing a dancer in a movie  can confirm. Single camera shots are the most convenient to obtain, and, of course, apply  to the world's film and video archives. It is an appealing computer vision problem that  emphasizes inference as much as measurement.  This problem has received less attention than motion capture from multiple cameras.  Goncalves et.al. rely on perspective effects to track only a single arm, and thus need not  deal with complicated models, shadows, or self-occlusion [7]. Bregler & Malik develop a  body tracking system that may apply to a single camera, but performance in that domain is  Bayesian Reconstruction of 3D Human Motion from Single-Camera Video 821  not clear; most of the examples use multiple cameras [4]. Wachter & Nagel use an iterated  extended Kalman filter, although their body model is limited in degrees of freedom [12].  Brand [3] uses an learning-based approach, although with representational expressiveness  restricted by the number of HMM states. An earlier version of the work reported here [ 10]  required manual intervention for the 2D tracking.  This paper presents our system for single-camera motion capture, a learning-based ap-  proach, relying on prior information learned from a labeled training set. The system tracks  joints and body parts as they move in the 2D video, then combines the tracking informa-  tion with the prior model of human motion to form a best estimate of the body's motion in  3D. Our reconstruction method can work with incomplete information, because the prior  model allows spurious and distracting information to be discarded. The 3D estimate pro-  vides feedback to influence the 2D tracking process to favor more likely poses.  The 2D tracking and 3D reconstruction modules are discussed in Sections 3 and 4, respec-  tively. Section 4 describes the system operation and presents performance results. Finally,  Section 5 concludes with possible improvements.  2 2D Tracking  The 2D tracker processes a video stream to determine the motion of body parts in the image  plane over time. The tracking algorithm used is based on one presented by Ju et. al. [9],  and performs a task similar to one described by Morris & Rehg [ 11 ]. Fourteen body parts  are modeled as planar patches, whose positions are controlled by 34 parameters. Tracking  consists of optimizing the parameter values in each frame so as to minimize the mismatch  between the image data and a projection of the body part maps. The 2D parameter values  for the first frame must be initialized by hand, by overlaying a model onto the 2D image of  the first frame.  We extend Ju et. al.'s tracking algorithm in several ways. We track the entire body, and  build a model of each body part that is a weighted average of several preceding frames, not  just the most recent one. This helps eliminate tracking errors due to momentary glitches  that last for a frame or two.  We account for self-occlusions through the use of support maps [4, 1]. It is essential to  address this problem, as limbs and other body parts will often partly or wholly obscure one  another. For the single-camera case, there are no alternate views to be relied upon when a  body part cannot be seen.  The 2D tracker returns the coordinates of each limb in each successive frame. These in turn  yield the positions of joints and other control points needed to perform 3D reconstruction.  3 3D Reconstruction  3D reconstruction from 2D tracking data is underdetermined. At each frame, the algorithm  receives the positions in two dimensions of 20 tracked body points, and must to infer the  correct depth of each point. We rely on a training set of 3D human motions to determine  which reconstructions are plausible. Most candidate projections are unnatural motions,  if not anatomically impossible, and can be eliminated on this basis. We adopt a Bayesian  framework, and use the training data to compute prior probabilities of different 3D motions.  We model plausible motions as a mixture of Gaussian probabilities in a high-dimensional  space. Motion capture data gathered in a professional studio provide the training data:  frame-by-frame 3D coordinates for 20 tracked body points at 20-30 frames per second. We  want to model the probabilities of human motions of some short duration, long enough be  822 N. R. Howe, M. E. Leventon and W. T. Freeman  informative, but short enough to characterize pt'obabilistically from our training data. We  assembled the data into short motion elements we called snippets of 1 1 successive frames,  about a third of a second. We represent each snippet from the training data as a large  column vector of the 3D positions of each tracked body point in each frame of the snippet.  We then use those data to build a mixture-of-Gaussians probability density model [2]. For  computational efficiency, we used a clustering approach to approximate the fitting of an EM  algorithm. We use ]c-means clustering to divide the snippets into m groups, each of which  will be modeled by a Gaussian probability cloud. For each cluster, the matrix Mj is formed,  where the columns of Mj are the nj individual motion snippets after subtracting the mean  /j. The singular value decomposition (SVD) gives Mj = UjSjVj T, where $j contains  the singular values along the diagonal, and Uj contains the basis vectors. (We truncate  the SVD to include only the 50 largest singular values.) The cluster can be modeled by  - 1US2Ur  a multidimensional Gaussian with covariance Aj - -   j . The prior probability  of a snippet : over all the models is a sum of the Gaussian probabilities weighted by the  probability of each model.  m  P() = E kwje- (-m)rA-(-m)  j=l  (1)  Here k is a normalization constant, and rj is the a priori probability of model j, computed  as the fraction of snippets in the knowledge base that were originally placed in cluster j.  Given this approximately derived mixture-of-factors model [6], we can compute the prior  probability of any snippet.  To estimate the data term (likelihood) in Bayes' law, we assume that the 2D observations  include some Gaussian noise with variance a. Combined with the prior, the expression for  the probability of a given snippet :E' given an observation 37 becomes  (2)  In this equation, Ro,8,e(g) is a rendering function which maps a 3D snippet ginto the image  coordinate system, performing scaling s, rotation about the vertical axis 0, and image-plane  translation 5. We use the EM algorithm to find the probabilities of each Gaussian in the  mixture and the corresponding snippet g that maximizes the probability given the observa-  tions [6]. This allows the conversion of eleven frames of 2D tracking measurements into  the most probable corresponding 3D snippet. In cases where the 2D tracking is poor, the  reconstruction may be improved by matching only the more reliable points in the likelihood  term of Equation 2. This adds a second noise process to explain the outlier data points in  the likelihood term.  To perform the full 3D reconstruction, the system first divides the 2D tracking data into  snippets, which provides the 7 values of Eq. 2, then finds the best (MAP) 3D snippet  for each of the 2D observations. The 3D snippets are stitched together, using a weighted  interpolation for frames where two snippets overlap. The result is a Bayesian estimate of  the subject's motion in three dimensions.  4 Performance  The system as a whole will track and successfully 3D reconstruct simple, short video clips  with no human intervention, apart from 2D pose initialization. It is not currently reliable  enough to track difficult footage for significant lengths of time. However, analysis of short  clips demonstrates that the system can successfully reconstruct 3D motion from ambiguous  Bayesian Reconstruction of 3D Human Motion from Single-Camera ldeo 823  2D video. We evaluate the two stages of the algorithm independently at first, and then  consider their operation as a system.  4.1 Performance of the 3D reconstruction  The 3D reconstruction stage is the heart of the system. To our knowledge, no similar  2D to 3D reconstruction technique relying on prior information has been published. ([3],  developed simultaneously, also uses an inference-based approach). Our tests show that the  module can restore deleted depth information that looks realistic and is close to the ground  truth, at least when the knowledge base contains some examples of similar motions. This  makes the 3D reconstruction stage itself an important result, which can easily be applied in  conjunction with other tracking technologies.  To test the reconstruction with known ground truth, we held back some of the training  data for testing. We artificially provided perfect 2D marker position data,/7 in Eq. 2, and  tested the 3D reconstruction stage in isolation. After removing depth information from the  test sequence, the sequence is reconstructed as if it had come from the 2D tracker. Se-  quences produced in this manner look very much like the original. They show some rigid  motion error along the line of sight. An analysis of the uncertainty in the posterior prob-  ability predicts high uncertainty for the body motion mode of rigid motion parallel to the  orthographic projection [ 10]. This slipping can be corrected by enforcing ground-contact  constraints. Figure 1 shows a reconstructed running sequence corrected for.rigid motion  error and superimposed on the original. The missing depth information is reconstructed  well, although it sometimes lags or anticipates the true motion slightly. Quantitatively, this  error is a relatively small effect. After subtracting rigid motion error, the mean residual  3D errors in limb position are the same order of magnitude as the small frame-to frame  changes in those positions.  Figure 1: Original and reconstructed running sequences superimposed (frames 1, 7, 14,  and 21).  4.2 Performance of the 2D tracker  The 2D tracker performs well under constant illumination, providing quite accurate results  from frame to frame. The main problem it faces is the slow accumulation of error. On  longer sequences, the errors can build up to the point where the module is no longer tracking  the body parts it was intended to track. The problem is worsened by low contrast, occlusion  and lighting changes. More careful body modeling [5], lighting models, and modeling of  the background may address these issues. The sequences we used for testing were several  seconds long and had fairly good contrast. Although adequate to demonstrate the operation  of our system, the 2D tracker contains the most open research issues.  4.3 Overall system performance  Three example reconstructions are given, showing a range of different tracking situations.  The first is a reconstruction of a stationary figure waving one arm, with most of the motion  824 N. R. Howe, M. E. Leventon and I. T. Freeman  in the image plane. The second shows a figure bringing both arms together towards the  camera, resulting in a significant amount of foreshortening. The third is a reconstruction of  a figure walking sideways, and includes significant self-occlusion  Figure 2: First clipand its reconstruction (frames 1, 25, 50, and 75).  The first video is the easiest to track because there is little or no occlusion and change in  lighting. The reconstruction is good, capturing the stance and motion of the arm. There  is some rigid motion error, which is corrected through ground friction constraints. The  knees are slightly bent; this may be because the subject in the video has different body  proportions than those represented in the training database.  . .  -.?  Figure 3: Second clip and its reconstruction (IYames 1,25, 50, and 75).  The second video shows a figure bringing its arms together towards the camera. The only  indication of this is in the foreshortening of the limbs, yet the 3D reconstruction correctly  captures this in the right arm. (Lighting changes and contrast problems cause the 2D tracker  to lose the left arm partway through, confusing the reconstruction of that limb, but the right  arm is tracked accurately throughout.)  The third video shows a figure walking to the right in the image plane. This clip is the  hardest for the 2D tracker, due to repeated and prolonged occlusion of some body parts.  The tracker loses the left arm after 15 frames due to severe occlusion, yet the remaining  tracking information is still sufficient to perform an adequate reconstruction. At about  frame 45, the left leg has crossed behind the right several times and is lost, at which point  the reconstruction quality begins to degrade. The key to a more reliable reconstruction on  this sequence is better tracking.  Bayesian Reconstruction of 3D Human Motion from Single-Camera Video 825  Figure 4: Third clip and its reconstruction (frames 6, 16, 26, and 36).  5 Conclusion  We have demonstrated a system that tracks human figures in short video sequences and  reconstructs their motion in three dimensions. The tracking is unassisted, although 2D pose  initialization is required. The system uses prior information learned from training data to  resolve the inherent ambiguity in going from two to three dimensions, an essential step  when working with a single-camera video source. To achieve this end, the system relies  on prior knowledge, extracted from examples of human motion. Such a learning-based  approach could be combined with more sophisticated measurement-based approaches to  the tracking problem [12, 8, 4].  References  [1] J. R. Bergen, P. Anandan, K. J. Hanna, and R. Hingorani. Hierarchical model-based  motion estimation. In European Conference on Computer lsion, pages 237-252,  1992.  [2] C. M. Bishop. Neural networks for pattern recognition. Oxford, 1995.  [3] M. Brand. Shadow puppetry. In Proc. 7th Intl. Conf. on Computer ISsion, pages  1237-1244. IEEE, 1999.  [4] C. Bregler and J. Malik. Tracking people with twists and exponential maps. In IEEE  Computer Society Conference on Computer Vision and Pattern Recognition, Santa  Barbera, 1998.  [5] D. M. Gavrila and L. S. Davis. 3d model-based tracking of humans in action: A  multi-view approach. In IEEE Computer Society Conference on Computer ISsion  and Pattern Recognition, San Francisco, 1996.  [6] Z. Ghahramani and G. E. Hinton. The EM algorithm for mixtures of factor analyzers.  Technical report, Department of Computer Science, University of Toronto, May 21  1996. (revised Feb. 27, 1997).  [7] L. Goncalves, E. Di Bernardo, E. Ursella, and P. Perona. Monocular tracking of the  human arm in 3D. In Proceedings of the Third International Conference on Computer  ISsion, 1995.  [8] M. Isard and A. Blake. Condensation - conditional density propagation for visual  tracking. International Journal of Computer ISsion, 29(1):5-28, 1998.  [9] S. X. Ju, M. J. Black, and Y. Yacoob. Cardboard people: A parameterized model of  articulated image motion. In 2nd International Conference on Automatic Face and  Gesture Recognition, 1996.  826 N. R. Howe, M. E. Leventon and W. T. Freeman  [10] M. E. Leventon and W. T. Freeman. Bayesian estimation of 3-d human motion from  an image sequence. Technical Report TR98-06, Mitsubishi Electric Research Lab,  1998.  [11] D. D. Morris and J. Rehg. Singularity analysis for articulated object tracking. In  IEEE Computer Societ 3' Conference on Computer Vision and Pattern Recognition,  Santa Barbera, 1998.  [12] S. Wachter and H.-H. Nagel. Tracking of persons in monocular image sequences. In  Nonrigid and Articulated Motion Workshop, 1997.  
Kirchoff Law Markov Fields for Analog  Circuit Design  Richard M. Golden *  RMG Consulting Inc.  2000 Fresno Road, Plano, Texas 75074  RMG CONS UL T@A OL. COM,  www. neural-network. corn  Abstract  Three contributions to developing an algorithm for assisting engi-  neers in designing analog circuits are provided in this paper. First,  a method for representing highly nonlinear and non-continuous  analog circuits using Kirchoff current law potential functions within  the context of a Markov field is described. Second, a relatively effi-  cient algorithm for optimizing the Markov field objective function  is briefly described and the convergence proof is briefly sketched.  And third, empirical results illustrating the strengths and limita-  tions of the approach are provided within the context of a JFET  transistor design problem. The proposed algorithm generated a set  of circuit components for the JFET circuit model that accurately  generated the desired characteristic curves.  1 Analog circuit design using Markov random fields  1.1 Markov random field models  A Markov random field (MRF) is a generalization of the concept of a Markov chain.  In a Markov field one begins with a set of random variables and a neighborhood re-  lation which is represented by a graph. Each random variable will be assumed in  this paper to be a discrete random variable which takes on one of a finite number  of possible values. Each node of the graph indexs a specific random variable. A  link from the jth node to the ith node indicates that the conditional probability  distribution of the ith random variable in the field is functionally dependent upon  the jth random variable. That is, random variable j is a neighbor of random vari-  able i. The only restriction upon the definition of a Markov field (i.e., the positivity  condition) is that the probability of every realization of the field is strictly posi-  tive. The essential idea behind Markov field design is that one specifies a potential  (energy) function for every clique in the neighborhood graph such that the subset  of random variables associated with that clique obtain their optimal values when  that clique's potential function obtains its minimal value (for reviews see [1]-[2]).  Associate Professor at University of Texas at Dallas (www.utdallas. edu/,golden)  908 R. M. Golden  Markov random field models provide a convenient mechanism for probabilistically  representing and optimally combining combinations of local constraints.  1.2 Analog circuit design using SPICE  In some mixed signal ASIC (Application Specific Integrated Circuit) design prob-  lems, most of the circuit design specifications are well known but the introduction  of a single constraint (e.g., an increase in substrate noise) could result in a major  redesign of an entire circuit. The industry standard tool for aiding engineers in  solving analog circuit design problems is SPICE which is a software environment  for simulation of large scale electronic circuits. SPICE does have special optimiza-  tion options for fitting circuit parameters to desired input-output characteristics  but typically such constraints are too weak for SPICE to solve analog circuit de-  sign problems with large numbers of free parameters (see [3] for an introduction to  SPICE). Another difficulty with using SPICE is that it does not provide a global  confidence factor for indicating its confidence in a generated design or local confi-  dence factors for determining the locations of "weak points" in the automatically  generated circuit design solution.  1.3 Markov field approaches to analog circuit design  In this paper, an approach for solving real-world analog circuit design problems us-  ing an appropriately constructed Markov random is proposed which will be referred  to as MRFSPICE. Not only are desired input-output characteristics directly incor-  porated into the construction of the potential functions for the Markov field but  additional constraints based upon Kirchoff's current law are directly incorporated  into the field. This approach thus differs from the classic SPICE methodology be-  cause Kirchoff current law constraints are explicitly incorporated into an objective  function which is minimized by the "optimal design". This approach also differs  from previous Markov field approaches (i.e., the "Harmony" neural network model  [4] and the "Brain-State-in-a-Box" neural network model [5]) designed to qualita-  tively model human understanding of electronic circuit behavior since those ap-  proaches used pair-wise correlational (quadratic) potential functions as opposed to  the highly nonlinear potential functions that will be used in the approach described  in this paper.  1.4 Key contributions  This paper thus makes three important contributions to the application of Markov  random fields to the analog circuit design problem. First, a method for represent-  ing highly nonlinear and non-continuous analog circuits using Kirchoff current law  potential functions within the context of a Markov field is described. Second, a  relatively efficient algorithm for optimizing the Markov field objective function is  briefly described and the convergence proof is briefly sketched. And third, empirical  results illustrating the strengths and limitations of the approach is provided within  the context of a JFET transistor design problem.  2 Modeling assumptions and algorithms  2.1 Probabilistic modeling assumptions  A given circuit circuit design problem consists of a number of design decision vari-  ables. Denote those design decision variables by the discrete random variables  Kirchoff Law Markov Fields for Analog Circuit Design 909  ,...,d. Let the MRF be denoted by the set  = [,...,d] SO that a realiza-  tion of i is the d-dimensional real vector x. A realization of i is referred to as a  circuit design solution.  Let the joint (global) probability mass function for  be denoted by PG. It is  assumed that Pa (x) > Pa (Y) if and only if the circuit design solution x is preferred  to the circuit design solution y. Thus, pG(x) specifies a type of probabilistic fuzzy  measure [1].  For example, the random variable :i might refer to a design decision concerning  the choice of a particular value for a capacitor C4. From previous experience,  it is expected that the value of C4 may be usually constrained without serious  difficulties to one of ten possible values:  0.1tF, 0.2tF, 0.3tF, 0.4tF, 0.5tF, 0.6tF, 0.7pF, 0.8tF, 0.9tF, or ltF.  Thus, ki = 10 in this example. By limiting the choice of C4 to a small number of  finite values, this permits the introduction of design expertise hints directly into the  problem formulation without making strong committments to the ultimate choice of  the value of capacitor C4. Other examples of design decision variable Values include:  resistor values, inductor values, transistor types, diode types, or even fundamentally  different circuit topologies.  The problem that is now considered will be to assign design preference probabilities  in a meaningful way to alternative design solutions. The strategy for doing this will  be based upon constructing Pa with the property that if Pa (x) > Pa (Y), then circuit  design solution x exhibits the requisite operating characteristics with respect to a  set of M "test circuits" more effectively than circuit design solution y. An optimal  analog circuit design solution x* then may be defined as a global maximum of Pa.  The specific details of this strategy for constructing pa are now discussed by first  carefully defining the concept of a "test circuit".  Let  = {0, 1, 2,...,m} be a finite set of integers (i.e., the unique "terminals" in  the test circuit) which index a set of rn complex numbers, v0, v, v2,..., vm which  will be referred to as voltages. The magnitude of vk indicates the voltage magnitude  while the angle of vk indicates the voltage phase shift. By convention the ground  voltage, vo, is always assigned the value of 0. Let d   x  (i.e., an ordered pair  of elements in ). A circuit component current source is defined with respect to  by a complex-valued function ia,b whose value is typically functionally dependent  upon v and Vb but may also be functionally dependent upon other voltages and  circuit component current sources associated with  For example, a "resistor" circuit component current source would be modeled by  choosing ia,b -' (Vb -- va)/R where R is the resistance in ohms of some resistor, Vb is  the voltage observed on one terminal of the resistor, and va is the voltage observed  on the other terminal of the resistor. The quantity ia,b is the current flowing through  the resistor from terminal a to terminal b. Similarly, a "capacitor" circuit component  current source would be modeled by choosing i,,b -- (Vb -- Va) / [2rj f] where j = vfL'-f  and f is the frequency in Hz of the test circuit. A "frequency specific voltage  controlled current source" circuit component current source may be modeled by  making ia,b functionally dependent upon some subset of voltages in the test circuit.  See [6] for additional details regarding the use of complex arithmetic for analog  circuit analysis and design.  An important design constraint is that Kirchoff's current law should be satisfied  at every voltage node. Kirchoff's current law states that the sum of the currents  entering a voltage node must be equal to zero [6]. We will now show how this  physical law can be directly embodied as a system of nonlinear constraints on the  910 R. M. Golden  behavior of the MRF.  We say that the kth voltage node in test circuit q is clamped if the voltage vk is  known. For example, node k in circuit q might be directly grounded, node k might  be directly connected to a grounded voltage source, or the voltage at node k, vk,  might be a desired known target voltage.  If voltage node k in test circuit q is clamped, then Kirchoff's current law at voltage  node k in circuit q is simply assumed to be satisfied which, in turn, implies that the  voltage potential function (I, q, - O.  Now suppose that voltage node k in test circuit q is not clamped. This means  that the voltage at node k must be estimated. If there are no controlled current  sources in the test circuit (i.e., only passive devices), then the values of the voltages  at the unclamped nodes in the circuit can be calculated by solving a system of  linear equations where the current choice of circuit component values are treated  as constants. In the more general case where controlled current sources exist in the  test circuit, then an approximate iterative gradient descent algorithm (such as the  algorithm used by SPICE) is used to obtain improved estimates of the voltages of  the unclamped nodes. The iterative algorithm is always run for a fixed number of  iterations.  Now the value of q,n must be computed. The current entering node k via arc  j in test circuit q is denoted by the two-dimensional real vector Ij whose first  component is the real part of the complex current and whose second component is  the imaginary part.  The average current entering node k in test circuit q is given by the formula:  i I = (1/n) - II, j-  Design circuit components (e.g., resistors, capacitors, diodes, etc.) which minimize  i will satisfy Kirchoff's current law at node k in test circuit q. However, the  measure i is an not entirely adequate indicator of the degree to which Kirchoff's  current law is satisfied since i may be small in magnitude not necessarily because  Kirchoff's current law is satisfied but simply because all currents entering node k are  small in magnitude. To compensate for this problem, a normalized current signal  magnitude to current signal variability ratio is minimized at node k in test circuit  q. This ratio decreases in magnitude if i has a magnitude which is small relative  to the magnitude of individual currents entering node k in test circuit q.  The voltage potential function, q,i, for voltage node k in test circuit q is now  formally defined as follows. Let  qk,q '- (1/n:) -'.(II,. ql(I q )T.  j=l  Let A1,..., A, be those eigenvalues of Qn,q whose values are strictly greater than  some small positive number e. Let ei be the eigenvector associated with eigenvalue  Ai. Define  Qk-q = Z(1/A.)eje'.  j-----1  Thus, if Qk,q has all positive eigenvalues, then Qk,q is simply the matrix inverse of  Q-1 Using this notation, the voltage potential function for the unclamped voltage  k,q'  Kirchoff Law Markov Fields for Analog Circuit Design 911  node k in test circuit q may be expressed by the formula:  -q T -1-q  q,t` = [It, ] Q It`.  Now define the global probability or "global preference" of a particular design con-  figuration by the formula:  pG(x) = (1/Z)exp(-U(x)) (1)  where U = (l/N)y.q '-k q,t` and where N is the total number of voltage nodes  across all test circuits. The most preferred (i.e., "most probable") design are the  design circuit components that maximize PG. Note that probabilities have been  assigned such that circuit configurations which are less consistent with Kirchoff's  current law are considered "less probable" (i.e., "less preferred").  Because the normalization constant Z in (1) is computationally intractable to com-  pute, it is helpful to define the easily computable circuit confidence factor, CCF,  given by the formula: CCF(x) = exp(-U(x)) = ZpG(x). Note that the global  probability p is directly proportional to the CCF. Since U is always non-negative  and complete satisfaction of Kirchoff's current laws corresponds to the case where  U = 0, it follows that CCF(x) has a lower bound of 0 (indicating "no subjective  confidence" in the design solution x) and an upper bound of I (indicating "absolute  subjective confidence" in the design solution x).  In addition, local conditional probabilities of the form  Pi = P(;i -' Xi[Xl,''',Xi--i,Xi+i, ''',xd)  can be computed using the formula:  p(xx,..., xi-, xi, x+,..., xd)  =  Such local conditional probabilities are helpful for explicitly computing the proba-  bility or "preference" for selecting one design circuit component value given a subset  of other design component values have been accepted. Remember that probability  (i.e., "preference") is essentially a measure of the degree to which the chosen de-  sign components and pre-specified operating characteristic voltage versus frequency  curves of the circuit satisfy Kirchoff's current laws.  2.2 MRFSPICE algorithm  The MRFSPICE algorithm is a combination of the Metropolis and Besag's ICM  (Iterated Conditional Modes) algorithms [1]-[2]. The stochastic Metropolis algo-  rithm (with temperature parameter set equal to one) is used to sample from p(x).  As each design solution is generated, the CCF for that design solution is computed  and the design solution with the best CCF is kept as an initial design solution guess  x0. Next, the deterministic ICM algorithm is then initialized with x0 and the ICM  algorithm is applied until an equilibrium point is reached.  A simulated annealing method involving decreasing the temperature parameter ac-  cording to a logarithmic cooling schedule in Step 1 through Step 5 could easily be  used to guarantee convergence in distribution to a uniform distribution over the  global maxima of pG (i.e., convergence to an optimal solution) [1]-[2]. However, for  the test problems considered thus far, equally effective results have been obtained by  using the above fast heuristic algorithm which is guaranteed to converge to a local  maximum as opposed to a global maximum. It is proposed that in situations where  the convergence rate is slow or the local maximum generated by MRFSPICE is a  912 R. M. Golden  poor design solution with low CCF, that appropriate local conditional probabilites  be computed and provided as feedback to a human design engineer. The human  design engineer can then make direct alterations to the sample space of pc (i.e., the  domain of CCF) in order to appropriately simply the search space. Finally, the  ICM algorithm can be easily viewed as an artificial neural network algorithm and  in fact is a generalization of the classic Hopfield (1982) model as noted in [1].  EDTP.-T  zqot  EGT'T  Figure 1: As external input voltage generator EGTEST and external supply voltage  EDTEST are varied, current IllTEST flowing through external resistor RTEST is  measured.  3 JFET design problem  In this design problem, specific combinations of free parameters for a macroequiva-  lent JFET transistor model were selected on the basis of a given set of characteristic  curves specifying how the drain to source current of the JFET varied as a function of  the gate voltage and drain voltage at OHz and 1MHz. Specifically, a JFET transis  tot model was simulated using the classic Shichman and Hodges (1968) large-signal  n-channel JFET model as described by Vladimirescu [3] (pp. 96-100). The circuit  diagram of this transistor model is shown in Figure 1. The only components in  the circuit diaam which are not part of the JFET transistor model are the exter-  nal voltage generators EDTEST and EGTEST, and external resistor RTEST. The  specific functions which describe how IDIQGD1, CDIQGD1, RDIQGD1, IDIQGS1,  CDIQGS1, RDIQGS1, CGDQ1, and CGSQ1 change as a function of EGTEST and  the current IRTEST (which flows through RTEST) are too long and complex to be  Kirchoff Law Markov Fields for Analog Circuit Design 913  presented here (for more details see [3] pp. 96-100).  Five design decision variables were defined. The first design decision variable,  XDIQGS1, specified a set of parameter values for the large signal gate to source  diode model portion of the JFET model. There were 20 possible choices for the  value of XDIQGS1. Similarly, the second design decision variable, XDIQGD1, had  20 possible values and specified a set of parameter values for the large signal gate to  drain diode model portion of the JFET model. The third design decision variable  was XQ1 which also had 20 possible values were each value specified a set of choices  for JFET-type specific parameters. The fourth and fifth design decision variables  were the resistors RSQ1 and RSD1 each of which could take on one of 15 possible  values.  The results of the JFET design problem are shown in Table 1. The phase angle  for IRTEST at 1MHz was specified to be approximately 10 degrees, while the  observed phase angle for IRTEST ranged from 7 to 9 degrees. The computing time  was approximately 2- 4 hours using unoptimized prototype MATLAB code on a  200 MHZ Pentium Processor. The close agreement between the desired and actual  results suggests further research in this area would be highly rewarding.  Table 1: Evaluation of MRFSPICE-generated JFET design  'EGTEST EDTEST IRTEST @ DC (ma) IRTEST @ 1MHZ (ma)  (desired/actual. ) (desired/actual)  0 1.5 1.47/1.50 1.19/1.21  0 2.0 1.96/1.99 1.60/1.62  0 3.0 2.94/2.99 2.43/2.43  -0.5 1.5 1.47/1.50 1.07/1.11  -0.5 2.0 1.96 / 2.00 1.49 / 1.52  -0.5 3.0 2.95/2.99 2.34/2.35  1.0 1.5 1.48/1.50 0.96/1.02  1.0 2.0 1.97/2.00 1.39 / 1.44  1.0 3.0 2.96 / 3.00 2.27/2.29  Acknowledgments  This research was funded by Texas Instruments Inc. through the direct efforts  of Kerry Hanson. Both Kerry Hanson and Ralph Golden provided numerous key  insights and knowledge substantially improving this project's quality.  References  [1] Golden, R. M. (1996) Mathematical methods .for neural network analysis and design.  Cambridge: MIT Press.  [2] Winkler, G. (1995) Image analysis, random fields, and dynamic Monte Carlo methods:  A mathematical introduction. New York: Springer-Verlag.  [3] Vladimirescu, A. (1994) The SPICE book. New York: Wiley.  [4] Smolensky, P. (1986). Information processing in dynamical systems: Foundations of  Harmony theory. In D. E. Rumelhart and J. L. McClelland (eds.), Parallel distributed  processing. Volume 1: Foundations, pp. 194-281. Cambridge: MIT Press.  [5] Anderson, J. A. (1995). An introduction to neural networks. Cambridge: MIT Press.  [6] Skilling, H. (1959) Electrical engineering circuits. New York: Wiley.  
Learning to Parse Images  Geoffrey E. Hinton and Zoubin Ghahramani  Gatsby Computational Neuroscience Unit  University College London  London, United Kingdom WCIN 3AR  (hinton, zoubin) gatsby. ucl. ac. uk  Yee Whye Teh  Department of Computer Science  University of Toronto  Toronto, Ontario, Canada M5S 3G4  ywteh cs. utoronto. ca  Abstract  We describe a class of probabilistic models that we call credibility  networks. Using parse trees as internal representations of images,  credibility networks are able to perform segmentation and recog-  nition simultaneously, removing the need for ad hoc segmentation  heuristics. Promising results in the problem of segmenting hand-  written digits were obtained.  I Introduction  The task of recognition has been the main focus of attention of statistical pattern  recognition for the past 40 years. The paradigm problem is to classify an object from  a vector of features extracted from the image. With the advent of backpropagation  [1], the choice of features and the choice of weights to put on these features became  part of a single, overall optimization and impressive performance was obtained for  restricted but important tasks such as handwritten character identification [2].  A significant weakness of many current recognition systems is their reliance on a  separate preprocessing stage that segments one object out of a scene and approx-  imately normalizes it. Systems in which segmentation precedes recognition suffer  from the fact that the segmenter does not know the shape of the object it is seg-  menting so it cannot use shape information to help it. Also, by segmenting an  image, we remove the object to be recognized from the context in which it arises.  Although this helps in removing the clutter present in the rest of the image, it  might also reduce the ability to recognize an object correctly because the context  in which an object arises gives a great deal of information about the nature of the  object. Finally, each object can be described in terms of its parts, which can also  be viewed as objects in their own right. This raises the question of how fine-grained  the segmentations should be. In the words of David Marr: "Is a nose an object?  Is a head one? ... What about a man on a horseback?" [3].  464 G. E. Hinton, Z. Ghahramani and Y. W. Teh  The successes of structural linguistics inspired an alternative approach to pattern  recognition in which the paradigm problem was to parse an image using a hierarchi-  cal grammar of scenes and objects. Within linguistics, the structural approach was  seen as an advance over earlier statistical approaches and for many years linguist-  s eschewed probabilities, even though it had been known since the 1970's that a  version of the EM algorithm could be used to fit stochastic context free grammars.  Structural pattern recognition inherited the linguists aversion to probabilities and  as a result it never worked very well for real data. With the advent of graphical  models it has become clear that structure and probabilities can coexist. Moreover,  the "explaining away" phenomenon that is central to inference in directed acyclic  graphical models is exactly what is needed for performing inferences about possible  segmentations of an image.  In this paper we describe an image interpretation system which combines segmenta-  tion and recognition into the same inference process. The central idea is the use of  parse trees of images. Graphical models called credibility networks which describe  the joint distribution over the latent variables and over the possible parse trees  are used. In section 2 we describe some current statistical models of image inter-  pretation. In section 3 we develop credibility networks and in section 4 we derive  useful learning and inference rules for binary credibility networks. In section 5 we  demonstrate that binary credibility networks are useful in solving the problem of  classifying and segmenting binary handwritten digits. Finally in section 6 we end  with a discussion and directions for future research.  2 Related work  Neal [4] introduced generative models composed of multiple layers of stochastic l-  ogistic units connected in a directed acyclic graph. In general, as each unit has  multiple parents, it is intractable to compute the posterior distribution over hidden  variables when certain variables are observed. However, Neal showed that Gibbs  sampling can be used effectively for inference [4]. Efficient methods of approximat-  ing the posterior distribution were introduced later [5, 6, 7] and these approaches  were shown to yield good density models for binary images of handwritten digits  [8]. The problem with these models which make them inappropriate for modeling  images is that they fail to respect the 'single-parent' constraint: in the correct  interpretation of an image of opaque objects each object-part belongs to at most  one object - images need parse trees, not parse DAGs.  Multiscale models [9] are interesting generative models for images that use a fixed  tree structure. Nodes high up in the tree control large blocks of the image while  bottom level leaves correspond to individual pixels. Because a tree structure is used,  it is easy to compute the exact posterior distribution over the latent (non-terminal)  nodes given an image. As a result, the approach has worked much better than  Markov random fields which generally involve an intractable partition function. A  disadvantage is that there are serious block boundary artifacts, though overlapping  trees can be used to smooth the transition from one block to another [10]. A more  serious disadvantage is that the tree cannot possibly correspond to a parse tree  because it is the same for every image.  Zemel, Mozer and Hinton [11] proposed a neural network model in which the ac-  tivities of neurons are used to represent the instantiation parameters of objects or  their parts, i.e. the viewpoint-dependent coordinate transformation between an ob-  ject's intrinsic coordinate system and the image coordinate system. The weights on  connections are then used to represent the viewpoint-invariant relationship between  the instantiation parameters of a whole, rigid object and the instantiation parame-  Learning to Parse Images 465  ters of its parts. This model captures viewpoint invariance nicely and corresponds  to the way viewpoint effects are handled in computer graphics, but there was no  good inference procedure for hierarchical models and no systematic way of sharing  modules that recognize parts of objects among multiple competing object models.  Simard et al [12] noted that small changes in object instantiati0n parameters result  in approximately linear changes in (real-valued) pixel intensities. These can be  captured successfully by linear models. To model larger changes, many locally linear  models can be pieced together. Hinton, Dayan and Revow [13] proposed a mixture  of factor analyzers for this. Tipping and Bishop have recently shown how to make  this approach much more computationally efficient [14]. To make the approach  really efficient, however, it is necessary to have multiple levels of factor analyzers  and to allow an analyzer at one level to be shared by several competing analyzers  at the next level up. Deciding which subset of the analyzers at one level should be  controlled by one analyzer at the level above is equivalent to image segmentation or  the construction of part of a parse tree and the literature on linear models contains  no proposals on how to achieve this.  3 A new approach to image interpretation  We developed a class of graphical models called credibility networks in which the  possible interpretations of an image are parse trees, with nodes representing object-  parts and containing latent variables. Given a DAG, the possible parse trees of an  image are constrained to be individual or collections of trees where each unit satisfies  the single-parent constraint, with the leaves being the pixels of an image. Credibility  networks describe a joint distribution over the latent variables and possible tree  structures. The EM algorithm [15] can be used to fit credibility networks to data.  Let i  I be a node in the graph. There are three random variables associated with  i. The first is a multinomial variate hi = (ij)jpa(i) which describes the parent of  i from among the potential parents pa(i):  ij={ if parent of i is j, (1)  if parent of i is not j.  The second is a binary variate si which determines whether the object i 1 is present  (si = 1) or not (si = 0). The third is the latent variables xi that describe the  pose and deformation of the object. Let A = {)i: i  I),S = (si: i  I) and  X = :i e I).  Each connection j - i has three parameters also. The first, cij is an unnormalized  prior probability that j is i's parent given that object j is present. The actual prior  probability is  cijsj (2)  7riJ ---- Ekpa(i) ikSk  We assume there is always a unit I  pa(i) such that s - 1. This acts as a default  parent when no other potential parent is present and makes sure the denominator in  (2) is never 0. The second parameter, Pij, is the conditional probability that object  i is present given that j is i's parent (Aij -- 1). The third parameter tij characterizes  the distribution of xi given Aij - I and xj. Let 0 = (cij,pij, tij : i  ,j  pa(i)).  Using Bayes' rule the joint distribution over A, S and X given 0 is p(A, S, XI ) -  p(A, SIO)p(XIA , S,O). Note that A and S together define a parse tree for the im-  age. Given the parse tree the distribution over latent variables p(XIA , $, ) can be  Technically this should be the object represented by node i.  466 G. E. Hinton, Z. Ghahrarnani and Y. 144. Teh  efficiently inferred from the image. The actual form of p(XIA , S, 19) is unimportant.  The joint distribution over A and $ is  P(A'SIO) = rI II (rijPiSJ (1 -PiJ)l-*')  iI jpa(i)  (3)  4 Binary credibility networks  The simulation results in section 5 are based on a simplified version of credibility  networks in which the latent variables X are ignored. Notice that we can sum out  A from the joint distribution (3), so that  P($10) = H E rijpiSJ (1 -- ,l-s,  Pij 1  iI jpa(i)  (4)  Using Bayes' rule and dividing (3) by (4), we have  (cijsjpiSj(l_pij)l_s,  P(A[$,0) - H H Ekpa(i) CikSkp}(1 - Pik) 1-s'  iI jpa(i)  (5)  s, (1 -Pij) 1-s' . We can view vij as the unnormalized posterior proba-  Let rij ---- ijPij  bility that j is i's parent given that object j is present. The actual posterior is the  fraction in (5):  rqs (6)  03ij -- Ekepa(i) rikSk  Given some observations O C S, let 7-/ = S \ O be the hidden variables.  approximate the posterior distribution for 7-/using a factored distribution  We  s (1 - ri) 1-si  = H  (7)  The variational free energy, r(Q, 0) = EQ[-logP(Sl O) + log Q(S)] is  r(Q'0) -- ii (EQ[ log E cijsj-log E cijsjPiSj(1-Pij)l-s']) +   jpa(i) jpa(i)  E(ailogai+(1-ai)log(1-ai))  iI  (8)  The negative of the free energy _r is a lower bound on the log likelihood of gen-  erating the observations (9. The variational EM algorithm improves this bound by  iteratively improving _r with respect to Q (E-step) and to 0 (M-step). Let ch(i)  be the possible children of i. The inference rules can be derived from (8):  ai = sigmoid  (9)  Let D be the training set and Qa be the mean field approximation to the posterior  distribution over 7-/given the training data (observation) d ED. Then the learning  Learning to Parse Images 467  Figure 1' Sample images from the test set. The classes of the two digits in each  image in a row are given to the left.  rules are  O- E:(Q,O)  Y Er2a [wij - rij] (10)  co log cij ,te r>  few EQd[js]  ij   dD ZQa[iJ] (11)  For an efficient implementation of credibility networks using mean field approxima-  tions, we still need to evaluate terms of the form E[logx] and Ell/x] where x is a  weighted sum of binary random variates. In our implementation we used the sim-  plest approximations: E[logx] m log E[x] and Ell/x]  liE[x]. Although bied  the implementation works well enough in general.  5 Segmenting handwritten digits  Hinton and Revow [16] used a mixture of factor analyzers model to segment and  estimate the pose of digit strings. When the digits do not overlap, the model was  able to identify the digits present and segment the image easily. The hard cases  are those in which two or more digits overlap significantly. To assess the ability  of credibility networks at segmenting handwritten digits, we used superpositions of  digits at exactly the same location. This problem is much harder than segmenting  digit strings in which digits partially overlap.  The data used is a set of 4400 images of single digits from the classes 2, 3, 4 and  5 derived from the CEDAR CDROM 1 database [17]. Each image has size 16x16.  The size of the credibility network is 256-64-4. The 64 middle layer units are meant  to encode low level features, while each of the 4 top level units are meant to encode  a digit class. We used 700 images of single digits from each class to train the  network. So it was not trained to segment images. During training we clamped at  1 the activation of the top layer unit corresponding to the class of the digit in the  current image while fixing the rest at 0.  After training, the network was first tested on the 1600 images of single digits  not in the training set. The predicted class of each image was taken to be the  468 G. E. Hinton, Z. Ghahramani and Y. . Teh  h)  Figure 2: Segmentations of pairs of digits. (To make comparisons easier we show  the overlapping image in both columns of a)-l).)  class corresponding to the top layer unit with the highest activation. The error  rate was 5.5%. We then showed the network 120 images of two overlapping digits  from distinct classes. There were 20 images per combination of two classes. Some  examples are given in Figure 1. The predicted classes of the two digits are chosen  to be the corresponding classes of the 2 top layer units with the highest activations.  A human subject (namely the third author) was tested on the same test set. The  network achieved an error rate of 21.7% while the author erred on 19.2% of the  images.  We can in fact produce a segmentation of each image into an image for each class  present. Recall that given the values of $ the posterior probability of unit j being  pixel i's parent is cij. Then the posterior probability of pixel i belonging to digit  class k is Ej EQ[COijCjk].  This gives a simple way to segment the image. Figure 2 shows a number of segmen-  tations. Note that for each pixel, the sum of the probabilities of the pixel belonging  to each digit class is 1. To make the picture clearer, a white pixel means a proba-  bility of _ .1 of belonging to a class, while black means _ .6 probability, and the  intensity of a gray pixel describes the size of the probability if it is between .1 and  .6. Figures 2a) to 2f) shows successful segmentations, while Figure 2g) to 21) shows  unsuccessful segmentations.  6 Discussion  Using parse trees as the internal representations of images, credibility networks  avoid the usual problems associated with a bottom-up approach to image interpre-  tation. Segmentation can be carried out in a statistically sound manner, removing  the need for hand crafted ad hoc segmentation heuristics. The granularity problem  for segmentation is also resolved since credibility networks use parse trees as inter-  nal representations of images. The parse trees describe the segmentations of the  image at every level of granularity, from individual pixels to the whole image.  We plan to develop and implement credibility networks in which each latent variable  xi is a multivariate Gaussian, so that a node can represent the position, orientation  and scale of a 2 or 3D object, and the conditional probability models on the links can  represent the relationship between a moderately deformable object and its parts.  Learning to Parse Images 469  Acknowledgments  We thank Chris Williams, Stuart Russell and Phil Dawid for helpful discussions  and NSERC and ITRC for funding.  References  [1] D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning internal representa-  tions by error propagation. In D. E. Rumelhart, J. L. McClelland, and the PDP  Research Group, editors, Parallel Distributed Processing: Explorations in The Mi-  crostructure of Cognition. Volume i: Foundations. The MIT Press, 1986.  [2] Y. Le Cun, B. Boser, J. S. Denker, S. Solla, R. E. Howard, and L. D. Jackel.  Back-propagation applied to handwritten zip code recognition. Neural Computation,  1(4):541-551, 1989.  [3] D. Marr. Vision: A Computational Investigation into the Human Representation  and Processing of Visual Information. W. H. Freeman and company, San Francisco,  1980.  [4] R. M. Neal. Connectionist learning of belief networks. Artificial Intelligence, 56:71-  113, 1992.  [5] P. Dayan, G. E. Hinton, R. M. Neal, and R. S. Zemel. Helmholtz machines. Neural  Computation, 7:1022-1037, 1995.  [6] G. E. Hinton, P. Dayan, B. J. Frey, and R. M. Neal. The wake-sleep algorithm for  self-organizing neural networks. Science, 268:1158-1161, 1995.  [7] L. K. Saul and M. I. Jordan. Attractor dynamics in feedforward neural networks.  Submitted for publication.  [8] B. J. Frey, G. E. Hinton, and P. Dayan. Does the wake-sleep algorithm produce good  density estimators? In D. Touretzky, M. Mozer, and M. Hasselmo, editors, Advances  in Neural Information Processing Systems, volume 8. The MIT Press, 1995.  [9] M. R. Luettgen and A. S. Willsky. Likelihood calculation for a class of multiscale  stochastic models, with application to texture discrimination. IEEE Transactions on  Image Processing, 4(2):194-207, 1995.  [10] W.W. Irving, P. W. Fieguth, and A. S. Willsky. An overlapping tree approach to mul-  tiscale stochastic modeling and estimation. IEEE Transactions on Image Processing,  1995.  [11] R. S. Zemel, M. C. Mozer, and G. E. Hinton. TRAFFIC: Recognizing objects us-  ing hierarchical reference frame transformations. In Advances in Neural Information  Processing Systems, volume 2. Morgan Kaufmann Publishers, San Mateo CA, 1990.  [12] P. Simard, Y. Le Cun, and J. Denker. Ecient pattern recognition using a new  transformation distance. In S. Hanson, J. Cowan, and L. Giles, editors, Advances  in Neural Information Processing Systems, volume 5. Morgan Kaufmann Publishers,  San Mateo CA, 1992.  [13] G. E. Hinton, P. Dayan, and M. Revow. Modeling the manifolds of images of hand-  written digits. IEEE Transactions on Neural Networks, 8:65-74, 1997.  [14] M. E. Tipping and C. M. Bishop. Mixtures of probabilistic principal component anal-  ysis. Technical Report NCRG/97/003, Aston University, Department of Computer  Science and Applied Mathematics, 1997.  [15] A.P. Dempster, N.M. Laird, and D.B. Rubin. Maximum likelihood from incomplete  data via the EM algorithm. Journal of the Royal Statistical Society B, 39:1-38, 1977.  [16] G. E. Hinton and M. Revow. Using mixtures of factor analyzers for segmentation and  pose estimation, 1997.  [17] J. J. Hull. A database for handwritten text recognition research. IEEE Transactions  on Pattern Analysis and Machine Intelligence, 16(5):550-554, 1994.  
The Nonnegative Boltzmann Machine  Oliver B. Downs  Hopfield Group  Schultz Building  Princeton University  Princeton, NJ 08544  obdowns@princeton. edu  David J.C. MacKay  Cavendish Laboratory  Madingley Road  Cambridge, CB3 0HE  United Kingdom  mackay@mrao. cam. ac. uk  Daniel D. Lee  Bell Laboratories  Lucent Technologies  700 Mountain Ave.  Murray Hill, NJ 07974  ddlee@bell- labs. com  Abstract  The nonnegative Boltzmann machine (NNBM) is a recurrent neural net-  work model that can describe multimodal nonnegative data. Application  of maximum likelihood estimation to this model gives a learning rule that  is analogous to the binary Boltzmann machine. We examine the utility of  the mean field approximation for the NNBM, and describe how Monte  Carlo sampling techniques can be used to learn its parameters. Reflec-  tive slice sampling is particularly well-suited for this distribution, and  can efficiently be implemented to sample the distribution. We illustrate  learning of the NNBM on a translationally invariant distribution, as well  as on a generative model for images of human faces.  Introduction  The multivariate Gaussian is the most elementary distribution used to model generic da-  ta. It represents the maximum entropy distribution under the constraint that the mean and  covariance matrix of the distribution match that of the data. For the case of binary data,  the maximum entropy distribution that matches the first and second order statistics of the  data is given by the Boltzmann machine [1]. The probability of a particular state in the  Boltzmann machine is given by the exponential form:  P({si: +1}): exp - Z siAidsJ + bisi  (1)  ij '  Interpreting Eq. 1 as a neural network, the parameters Aid represent symmetric, recurrent  weights between the different units in the network, and bi represent local biases. Unfortu-  nately, these parameters are not simply related to the observed mean and covariance of the  The Nonnegative Boltzmann Machine 429  (a) :" - .... ...  50-  40-  3O  20  10-  O  0  2  3  1 2 3 4 5 54 1 2 3 4 5  (b)  5  4  3  2  1  Oo  Figure 1' a) Probability density and b) shaded contour plot of a two dimensional competi-  tive NNBM distribution. The energy function E(:v) for this distribution contains a saddle  point and two local minima, which generates the observed multimodal distribution.  data as they are for the normal Gaussian. Instead, they need to be adapted using an iterative  learning rule that involves difficult sampling from the binary distribution [2].  The Boltzmann machine can also be generalized to continuous and nonnegative variables.  In this case, the maximum entropy distribution for nonnegative data with known first and  second order statistics is described by a distribution previously called the "rectified Gaus-  sian" distribution [3]:  P(z) = exp [-E(z)] ifzi > 0 Vi,  if any :i < 0, (2)  where the energy fimction E(:c) and normalization constant Z are:  2  z = az exp[-E(z)]. (4)  >o  The properties of this nonnegative Boltzmann machine (NNBM) distribution differ quite  substantially from that of the normal Gaussian. In particular, the presence of the nonnega-  tivity constraints allows the distribution to have multiple modes. For example, Fig. 1 shows  a two-dimensional NNBM distribution with two separate maxima located against the rec-  tifying axes. Such a multimodal distribution would be poorly modelled by a single normal  Gaussian.  In this submission, we discuss how a multimodal NNBM distribution can be learned from  nonnegative data. We show the limitations of mean field approximations for this distribu-  tion, and illustrate how recent developments in efficient sampling techniques for continuous  belief networks can be used to tune the weights of the network [4]. Specific examples of  learning are demonstrated on a translationally invariant distribution, as well as on a gener-  ative model for face images.  Maximum Likelihood  The learning rule for the NNBM can be derived by maximizing the log likelihood of the  observed data under Eq. 2. Given a set of nonnegative vectors {u}, where tt = 1..M  430 O. B. Downs, D. J. MacKay and D. D. Lee  indexes the different examples, the log likelihood is:  M M  1 1  L= M ZlgP(:u) - M ZE(x-'U)-lgZ' (5)  tt=l /=1  Taking the derivatives ofEq. 5 with respect to the parameters A and b gives:  0L  = <xixj>f- <xixj>c (6)  OAij  OL  = (xi)c - (xi)f, (7)  where the subscript "c" denotes a "clamped" average over the data, and the subscript "f"  denotes a "free" average over the NNBM distribution:  M  1  (f(x))c = M y f(:u) (8)  (f(x))f : (9)  These derivatives are used to define a gradient ascent learning rule for the NNBM that is  similar to that of the binary Boltzmann machine. The contrast between the clamped and  free covariance matrix is used to update the iteractions A, while the difference between the  clamped and free means is used to update the local biases b.  Mean field approximation  The major difficulty with this learning algorithm lies in evaluating the averages {XiXj) f  and (Xi)f. Because it is analytically intractable to calculate these free averages exactly,  approximations are necessary for learning. Mean field approximations have previously  been proposed as a deterministic alternative for learning in the binary Boltzmann machine,  although there have been contrasting views on their validity [5, 6]. Here, we investigate the  utility of mean field theory for approximating the NNBM distribution.  The mean field equations are derived by approximating the NNBM distribution in Eq. 2  with the factorized form:  1 1 (x) *  II II -  Q(3C) '-- Qri(3Ci) --' . T i  i i  where the different marginal densities Q(xi) are characterized by the means ri with a fixed  constant '7. The product of ?-distributions is the natural factorizable distribution for non-  negative random variables.  The optimal mean field parameters ri are determined by minimizing the Kullback-Leibler  divergence between the NNBM distribution and the factorized distribution:  D:L(QIIP) = dxQ(x)log [p-j = (E(x))(x) + logZ- H(Q). (11)  Finding the minimum of Eq. 11 by setting its derivatives with respect to the mean field  parameters ri to zero gives the simple mean field equations:  [ 1]  Aii-i = (? + 1) bi - Aijrj + -- (12)   -i  The Nonnegative Boltzmann Machine 431  y  (a)  P(x)  xi+ 1 x i x  (b)  Figure 2: a) Slice sampling in one dimension. Given the current sample point, a:i, a height  t E [0, P(a:)] is randomly chosen. This defines a slice (:c  StP(:c ) > t) in which a  new :Ci+z is chosen. b) For a multidimensional slice S, the new point a:i+z is chosen using  ballistic dynamics with specular reflections off the interior boundaries of the slice.  These equations can then be solved self-consistently for ri. The "free" statistics of the  NNBM are then replaced by their statistics under the factorized distribution Q (a:):  <Xi)f  Ti, <XiXj)f  [( + 1) 2 q- (")/ q- 1)(ij]TiTj.  (13)  The fidelity of this approximation is determined by how well the factorized distribution  Q(z) models the NNBM distribution. Unfortunately, for distributions such as the one  shown in Fig. 3, the mean field approximation is quite different from that of the true mul-  timodal NNBM distribution. This suggests that the naive mean field approximation is i-  nadequate for learning in the NNBM, and in fact attempts to use this approximation fail  to learn the examples given in following sections. However, the mean field approximation  can still be used to initialize the parameters to reasonable values before using the sampling  techniques that are described below.  Monte-Carlo sampling  A more direct approach to calculating the "free" averages in Eq. 6-7 is to numerically ap-  proximate them. This can be accomplished by using Monte Carlo sampling to generate a  representative set of points that sufficiently approximate the statistics of the continuous dis-  tribution. In particular, Markov chain Monte-Carlo methods employ an iterative stochastic  dynamics whose equilibrium distribution converges to that of the desired distribution [4].  For the binary Boltzmann machine, such sampling dynamics involves random "spin flips"  which change the value of a single binary component. Unfortunately, these single compo-  nent dynamics are easily caught in local energy minima, and can converge very slowly for  large systems. This makes sampling the binary distribution very difficult, and more spe-  cialized computational techniques such as simulated annealing, cluster updates, etc., have  been developed to try to circumvent this problem.  For the NNBM, the use of continuous variables makes it possible to investigate different  stochastic dynamics in order to more efficiently sample the distribution. We first experi-  mented with Gibbs sampling with ordered overrelaxation [7], but found that the required  inversion of the error function was too computationally expensive. Instead, the recently  developed method of slice sampling [8] seems particularly well-suited for implementation  in the NNBM.  The basic idea of the slice sampling algorithm is shown in Fig. 2. Given a sample point  :ci, a random t  [0, O_P(:Ci)] is first uniformly chosen. Then a slice S is defined as the  connected set of points (a:  S I aP(a:) > t), and the new point a:i+z  S is chosen  432 O. B. Downs, D. J. MacKay and D. D. Lee  'i, ' ' (a)  iO 1 2 3 4 5  00 1 2 3 4 5  Figure 3' Contours of the two-dimensional competitive NNBM distribution overlaid by a)  3' - 1 mean field approximation and b) 500 reflected slice samples.  randomly from this slice. The distribution of z,, for large n can be shown to converge  to the desired density P(x). Now, for the NNBM, solving the boundary points along a  particular direction in a given slice is quite simple, since it only involves solving the roots  of a quadratic equation. In order to efficiently choose a new point within a particular slice,  reflective "billiard ball" dynamics are used. A random initial velocity is chosen, and the  new point is evolved by travelling a certain distance from the current point while specularly  reflecting from the boundaries of the slice. Intuitively, the reversibility of these reflections  allows the dynamics to satisfy detailed balance.  In Fig. 3, the mean field approximation and reflective slice sampling are used to mod-  el the two-dimensional competitive NNBM distribution. The poor fit of the mean field  approximation is apparent from the unimodality of the factorized density, while the sam-  ple points from the reflective slice sampling algorithm are more representative of the un-  derlying NNBM distribution. For higher dimensional data, the mean field approximation  becomes progressively worse. It is therefore necessary to implement the numerical slice  sampling algorithm in order to accurately approximate the NNBM distribution.  Translationally invariant model  Ben-Yishai et al. have proposed a model for orientation tuning in primary visual cortex that  can be interpreted as a cooperative NNBM distribution [9]. In the absence of visual input,  the firing rates of N cortical neurons are described as minimizing the energy function E(z)  with parameters:  Aij  bi  1 e cos(2X li - Jl) (14)  5ij q- i i '  = 1  This distribution was used to test the NNBM learning algorithm. First, a large set of N =  25 dimensional nonnegative mining vectors were generated by sampling the distribution  with fi - 50 and e = 4. Using these samples as mining data, the A and b parameters were  learned from a unimodal initialization by evolving the mining vectors using reflective slice  sampling, and these evolved vectors were used to calculate the "free" averages in Eq. 6-7.  The A and b estimates were then updated, and this procedure was iterated until the evolved  averages matched that of the training data. The learned A and b parameters were then found  to almost exactly match the original form in Eq. 14. Some representative samples from the  learned NNBM distribution are shown in Fig. 4.  The Nonnegative Boltzmann Machine 433  4 I  3  2  1  5 10 15 20 25 5 10 15 20 25  2  1  0 o  ,1  O0 5 10 15 20 25 O0 5 10 15 20 25  Figure 4: Representative samples taken from a NNBM after training to learn a translation-  ally invariant cooperative distribution with/ - 50 and e = 4.  b)  Figure 5: a) Morphing of a face image by successive sampling from the learned NNBM  distribution. b) Samples generated from a normal Gaussian.  Generative model for faces  We have also used the NNBM to learn a generative model for images of human faces. The  NNBM is used to model the correlations in the coefficients of the nonnegative matrix fac-  torization (NMF) of the face images [ 10]. NMF reduces the dimensionality of nonnegative  data by decomposing the face images into parts correponding to eyes, noses, ears, etc. S-  ince the different parts are coactivated in reconstructing a face, the activations of these parts  contain significant correlations that need to be captured by a generative model. Here we  briefly demonstrate how the NNBM is able to learn these correlations.  Sampling from the NNBM stochastically generates coefficients which can graphically be  displayed as face images. Fig. 5 shows some representative face images as the reflective  slice sampling dynamics evolves the coefficients. Also displayed in the figure are the anal-  ogous images generated if a normal Gaussian is used to model the correlations instead. It  is clear that the nonnegativity constraints and multimodal nature of the NNBM results in  samples which are cleaner and more distinct as faces.  434 O. B. Downs, D. J. MacKay and D. D. Lee  Discussion  Here we have introduced the NNBM as a recurrent neural network model that is able to  describe multimodal nonnegative data. Its application is made practical by the efficiency  of the slice sampling Monte Carlo method. The learning algorithm incorporates numerical  sampling from the NNBM distribution and is able to learn from observations of nonneg-  ative data. We have demonstrated the application of NNBM learning to a cooperative,  translationally invariant distribution, as well as to real data from images of human faces.  Extensions to the present work include incorporating hidden units into the recurrent net-  work. The addition of hidden units implies modelling certain higher order statistics in the  data, and requires calculating averages over these hidden units. We anticipate the marginal  distribution over these units to be most commonly unimodal, and hence mean field theory  should be valid for approximating these averages.  Another possible extension involves generalizing the NNBM to model continuous data  confined within a certain range, i.e. 0 < X i  1. In this situation, slice sampling techniques  would also be used to efficiently generate representative samples. In any case, we hope that  this work stimulates more research into using these types of recurrent neural networks to  model complex, multimodal data.  Acknowledgements  The authors acknowledge useful discussion with John Hopfield, Sebastian Seung, Nicholas  Socci, and Gayle Wittenberg, and are indebted to Haim Sompolinsky for pointing out the  maximum entropy interpretation of the Boltzmann machine. This work was funded by Bell  Laboratories, Lucent Technologies.  O.B. Downs is grateful for the moral support, and open ears and minds of Beth Brittle,  Gunther Lenz, and Sandra Scheitz.  References  [1] Hinton, GE & Sejnowski, TJ (1983). Optimal perceptual learning. IEEE Conference on Com-  puter Vision and Pattern Recognition, Washington, DC, 448-453.  [2] Ackley, DH, Hinton, GE, & Sejnowski, TJ (1985). A learning algorithm for Boltzmann ma-  chines. Cognitive Science 9, 147-169.  [3] Socci, ND, Lee, DD, and Seung, HS (1998). The rectified Gaussian distribution. Advances in  Neural Information Processing Systems 10, 350-356.  [4] MacKay, DJC (1998). Introduction to Monte Carlo Methods. Learning in Graphical Models.  Kluwer Academic Press, NATO Science Series, 175-204.  [5] Galland, CC (1993). The limitations of deterministic Boltzmann machine learning. Network 4,  355-380.  [6] Kappen, HJ & Rodriguez, FB (1997). Mean field approach to learning in Boltzmann machines.  Pattern Recognition in Practice V), Amsterdam.  [7] Neal, RM (1995). Suppressing random walks in Markov chain Monte Carlo using ordered  overrelaxation. Technical Report 9508, Dept. of Statistics, University of Toronto.  [8] Neal, RM (1997). Markov chain Monte Carlo methods based on "slicing" the density function.  Technical Report 9722, Dept. of Statistics, University of Toronto.  [9] Ben-Yishai, R, Bar-Or, RL, & Sompolinsky, H (1995). Theory of orientation tuning in visual  cortex. Proc. Nat. Acad. Sci. USA 92, 3844-3848.  [10] Lee, DD, and Seung, HS (1999) Learning the parts of objects by non-negative matrix factor-  ization. Nature 401,788-791.  
Evolving Learnable Languages  Bradley Tonkes  Dept of Comp. Sci. and Elec. Engineering  University of Queensland  Queensland, 4072  Australia  btonkes @csee. uq. edu. au  Alan Blair  Department of Computer Science  University of Melbourne  Parkville, Victoria, 3052  Australia  blair@cs.mu. oz. au  Janet Wiles  Dept of Comp. Sci. and Elec. Engineering  School of Psychology  University of Queensland  Queensland, 4072  Australia  janetw@csee. uq. edu. au  Abstract  Recent theories suggest that language acquisition is assisted by the  evolution of languages towards forms that are easily learnable. In  this paper, we evolve combinatorial languages which can be learned  by a recurrent neural network quickly and from relatively few ex-  amples. Additionally, we evolve languages for generalization in  different "worlds", and for generalization from specific examples.  We find that languages can be evolved to facilitate different forms  of impressive generalization for a minimally biased, general pur-  pose learner. The results provide empirical support for the theory  that the language itself, as well as the language environment of a  learner, plays a substantial role in learning: that there is far more  to language acquisition than the language acquisition device.  I Introduction: Factors in language learnability  In exploring issues of language learnability, the special abilities of humans to learn  complex languages have been much emphasized, with one dominant theory based  on innate, domain-specific learning mechanisms specifically tuned to learning hu-  man languages. It has been argued that without strong constraints on the learning  mechanism, the complex syntax of language could not be learned from the sparse  data that a child observes [1]. More recent theories challenge this claim and em-  phasize the interaction between learner and environment [2]. In addition to these  two theories is the proposal that rather than "language-savvy infants", languages  themselves adapt to human learners, and the ones that survive are "infant-friendly  languages" [3-5]. To date, relatively few empirical studies have explored how such  adaptation of language facilitates learning. Hare and Elman [6] demonstrated that  Evolving Learnable Languages 67  classes of past tense forms could evolve over simulated generations in response to  changes in the frequency of verbs, using neural networks. Kirby [7] showed, using  a symbolic system, how compositional languages are more likely to emerge when  learning is constrained to a limited set of examples. Batali [8] has evolved recurrent  networks that communicate simple structured concepts.  Our argument is not that humans are general purpose learners. Rather, current  research questions require exploring the nature and extent of biases that learners  bring to language learning, and the ways in which languages exploit those biases  [2]. Previous theories suggesting that many aspects of language were unlearnable  without strong biases are gradually breaking down as new aspects of language are  shown to be learnable with much weaker biases. Studies include the investigation  of how languages may exploit biases as subtle as attention and memory limitations  in children [9]. A complementary study has shown that general purpose learners  can evolve biases in the form of initial starting weights that facilitate the learning  of a family of recursive languages [10].  In this paper we present an empirical paradigm for continuing the exploration of fac-  tors that contribute to language learnability. The paradigm we propose necessitates  the evolution of languages comprising recursive sentences over symbolic strings --  languages whose sentences cannot be conveyed without combinatorial composition  of symbols drawn from a finite alphabet. The paradigm is not based on any specific  natural language, but rather, it is the simplest task we could find to illustrate the  point that languages with compositional structure can be evolved to be learnable  from few sentences. The simplicity of the communication task allows us to analyze  the language and its generalizability, and highlight the nature of the generalization  properties.  We start with the evolution of a recursive language that can be learned easily from  five sentences by a minimally biased learner. We then address issues of robust  learning of evolved languages, showing that different languages support generaliza-  tion in different ways. We also address a factor to which scant regard has been  paid, namely that languages may evolve not just to their learners, but also to be  easily generalizable from a specific set of concepts. It seems almost axiomatic that  learning paradigms should sample randomly from the training domain. It may be  that human languages are not learnable from random sentences, but are easily gen-  eralizable from just those examples that a child is likely to be exposed to in its  environment. In the third series of simulations, we test whether a language can  adapt to be learnable from a core set of concepts.  2 A paradigm for exploring language learnability  We consider a simple language task in which two recurrent neural networks try to  communicate a "concept" represented by a point in the unit interval, [0, 1] over a  symbolic channel. An encoder network sends a sequence of symbols (thresholded  outputs) for each concept, which a decoder network receives and processes back into  a concept (the framework is described in greater detail in [11]). For communication  to be successful, the decoder's output should approximate the encoder's input for  all concepts.  The architecture for the encoder is a recurrent network with one input unit and  five output units, and with recurrent connections from both the output and hidden  units back to the hidden units. The encoder produces a sequence of up to five  symbols (states of the output units) taken from E = {A, ..., J}, followed by the $  symbol, for each concept taken from [0, 1]. To encode a value x  [0, 1], the network  68 B. Tonkes, A. Blair and J. Wiles   C A E A  B C B A ICAIB[ A I E  E E  /A'xA IAIIIII AIA A/IX  BCAEA EC[BABBABBABABOABCBCA  II IIII ITI I I I I I I I I ITI I I I I I ITI I I I I I  CE B A   C BC E E A C BC A  CAEBAECBAEBCABAECBAEBC  Figure 1: Hierarchical decomposition of the language produced by an encoder, with  the first symbols produced appearing near the root of the tree. The ordering of  leaves in the tree represent the input space, smaller inputs being encoded by those  sentences on the left. The examples used to train the best decoder found during  evolution are highlighted. The decoder must generalize to all other branches. In  order to learn the task, the decoder must generalize systematically to novel states  in the tree, including generalizing to symbols in different positions in the sequence.  (Figure 2 shows the sequence of states of a successful decoder.)  is presented with a sequence of inputs (x, 0, 0, ..). At each step, the output units  of the network assume one of eleven states: all zero if no output is greater than  0.5 (denoted by $); or the saturation of the two highest activations at 1.0 and the  remainder at 0.0 (denoted by A = [1,1, 0, 0, 0] through J = [0, 0, 0, 1, 1]). If the zero  output is produced, propagation is halted. Otherwise propagation continues for up  to five steps, after which the output units assume the zero ($) state.  The decoder is a recurrent network with 5 input units and a single output, and a  recurrent hidden layer. Former work [11] has shown that due to conflicting con-  straints of the encoder and decoder, it is easier for the decoder to process strings  which are in the reverse order to those produced by the encoder. Consequently,  the input to the decoder is taken to be the reverse of the output from the decoder,  except for $, which remains the last symbol. (For clarity, strings are written in  the order produced by the encoder.) Each input pattern presented to the decoder  matches the output of the encoder -- either two units are active, or none are. The  network is trained with backpropagation through time to produce the desired value,  x, on presentation of the final symbol in the sequence ($).  A simple hill-climbing evolutionary strategy with a two-stage evaluation function  is used to evolve an initially random encoder into one which produces a language  which a random decoder can learn easily from few examples. The evaluation of an  encoder, mutated from the current "champion" by the addition of Gaussian noise  to the weights, is performed against two criteria. (1) The mutated network must  produce a greater variety of sequences over the range of inputs; and (2) a decoder  with initially small random weights, trained on the mutated encoder's output, must  yield lower sum-squared error across the entire range of inputs than the champion.  Each mutant encoder is paired with a single decoder with initially random weight-  s. If the mutant encoder-decoder pair is more successful than the champion, the  mutant becomes champion and the process is repeated. Since the encoder's input  space is continuous and impossible to examine in its entirety, the input range is  approximated with 100 uniformly distributed examples from 0.00 to 0.99. The final  output from the hill-climber is the language generated by the best encoder found.  Evolving Learnable Languages 69  2.1 Evolving an easily learnable language  Humans learn from sparse data. In the first series of simulations we test whether  a compositional language can be evolved that learners can reliably and effectively  learn from only five examples. From just five training examples, it seems unrea-  sonable to expect that any decoder would learn the task. The task is intentionally  hard in that a language is restricted to sequences of discrete symbols with which  it must describe a continuous space. Note that simple linear interpolation is not  possible due to the symbolic alphabet of the languages. Recursive solutions are  possible but are unable to be learned by an unbiased learner. The decoder is a  minimally-biased learner and as the simulations showed, performed much better  than arguments based on learnability theory would predict.  Ten languages were evolved with the hill-climbing algorithm (outlined above) for  10000 generations. x For each language, 100 new random decoders were trained  under the same conditions as during evolution (five examples, 400 epochs). All ten  runs used encoders and decoders with five hidden units.  All of the evolved languages were learnable by some decoders (minimum 20, max-  imum 72, mean 48). A learner is said to have effectively learned the language if  its sum-squared-error across the 100 points in the space is less than 1.0. 2 Encoders  employed on average 36 sentences (minimum 21, maximum 60) to communicate  the 100 points. The 5 training examples for each decoder were sampled randomly  from [0, 1] and hence some decoders faced very difficult generalization tasks. The  difficulty of the task is demonstrated by the language analyzed in Figures I and 2.  The evolved languages all contained similar compositional structure to that of the  language described in Figures 1 and 2. The inherent biases of the decoder, although  minimal, are clearly sufficient for learning the compositional structure.  3 Evolving languages for particular generalization  The first series of simulations demonstrate that we can find languages for which a  minimally biased learner can generalize from few examples. In the next simulations  we consider whether languages can be evolved to facilitate specific forms of general-  ization in their users. Section 2.1 considered the case where the decoder's required  output was the same as the encoder's input. This setup yields the approximation  to the line y = x in Figure 2. The compositional structure of the evolved languages  allows the decoder to generalize to unseen regions of the space. In the following  series of simulations we consider the relationship between the structure of a lan-  guage and the way in which the decoder is required to generalize. This association  is studied by altering the desired relationship between the encoder's input (x) and  the decoder's output (y).  Two sets of ten languages were evolved, one set requiring y = x (identity, as in  section 2.1), the other using a function resembling a series of five steps at random  heights: y = r(L5xJ); r = (0.3746, 0.5753, 0.8102, 0.7272, 0.4527) (random step) 3.  All conditions were as for section 2, with the exception that 10 training examples  were used and the hill-climber ran for 1000 generations. On completion of evolution,  100 decoders were trained on the 20 final languages under both conditions above as  One generation represents the creation of a more variable, mutated encoder and the  subsequent training of a decoder.  2A language is said to be reliably learnable when at least 50% of random decoders are  able to effectively learn it within 400 epochs.  3 L5xj provides an index into the array r, based on the magnitude of x.  70 B. Tonkes, A. Blair and J. Ve2les  0.5  0  -0.5  0.1 0.2 0.3 0,4 0.5 0.6 07 08 0.9  0 ! 0.2 0.3 04 05 0.6 0.7 0.8 09  01 02 03 0.4 0.5 0.6 07 0.8 09  (a)  (c)  o  o  -o5  o o, o., o, o., (b)  O5  0  -O5  o o, o o= o., o. o, o., o., o  1.5  0.5  0  -O5  Figure 2: Decoder output after seeing the first n symbols in the message, for n = 1  (a) to n = 6 (f) (from the language in Figure 1). The X-axis is the encoder's input,  the Y-axis is the decoder's output at that point in the sequence. The five points  that the decoder was trained on are shown as crosses in each graph. After the first  symbol (A, B, C, E or $), the decoder outputs one of five values (a); after the  second symbol, more outputs are possible (b). Subsequent symbols in each string  specify finer gradations in the output. Note that the output is not constructed  monotonically, with each symbol providing a closer approximation to the target  function, but rather recursively, only approximating the linear target at the final  position in each sequence. Structure inherent in the sequences allows the system to  generalize to parts of the space it has never seen. Note that the generalization is not  based on interpolation between symbol values, but rather on their compositional  structure.  well as two others, a sine function and a cubic function.  The results show that languages can be evolved to enhance generalization prefer-  entially for one "world" over another. On average, the languages performed far  better when tested in the world in which they were evolved than in other worlds.  Languages evolved for the identity mapping were on average learned by 64% of  decoders trained on the identity task compared with just 5% in the random step  case. Languages evolved for the random step task were learned by 60% of decoders  trained on the random step task but only 24% when trained on the identity task.  Decoders generally performed poorly on the cubic function, and no decoder learned  the sine task from either set of evolved languages. The second series of simulation-  s show that the manner in which the decoder generalizes is not restricted to the  task of section 2.1. Rather, the languages evolve to facilitate generalization by the  decoder in different ways, aided by its minimal biases.  Evolving Learnable Languages 71  4 Generalization from core concepts  In the former simulations, randomly selected concepts were used to train decoders.  In some cases a pathological distribution of points made learning extremely difficult.  In contrast, it seems likely that human children learn language based on a common  set of semantically-constrained core concepts ("Mom", "I want milk", "no", etc).  For the third series of simulations, we tested whether selecting a fortuitous set of  training concepts could have a positive affect on the success of an evolved language.  The simulations with alternative generalization functions (section 3) indicated that  decoders had difficulty generalizing to the sine function. Even when encoders were  evolved specifically on the sine task, in the best of 10 systems only 13 of 100 random  decoders successfully learned.  We evolved a new language on a specifically chosen set of 10 points for generalization  to the sine function. One hundred decoders were then trained on the resulting  language using either the same set of 10 points, or a random set. Of the networks  trained on the fixed set, 92 learned the tasked, compared with 5 networks trained  on the random sets. That a language evolves to communicate a restricted set of  concepts is not particularly unusual. But what this simulation shows is the more  surprising result that a language can evolve to generalize from specific core concepts  to a whole rccursive language in a particular way (in this case, a sine function).  5 Discussion  The first series of simulations show that a compositional language can be learned  from five strings by an recurrent network. Generalization performance included  correct decoding of novel branches and symbols in novel positions (Figure 1). The  second series of simulations highlight how a language can be evolved to facilitate  different forms of generalization in the decoder. The final simulation demonstrates  that languages can also be tailored to generalize from a specific set of examples.  The three series of simulations modify the language environment of the decoder in  three different ways: (1) the relationship between utterances and meaning; (2) the  type of generalization required from the decoder; and (3) the particular utterances  and meanings to which a learner is exposed. In each case, the language environment  of the learner was sculpted to exploit the minimal biases present in the learner.  While taking an approach similar to [10] of giving the learner an additional bias  in the form of initial weights was also likely to have been effective, the purpose  of the simulations was to investigate how strongly external factors could assist in  simplifying learning.  6  Conclusions  "The key to understanding language learnability does not lie in  the richly social context of language training, nor in the incredi-  bly prescient guesses of young language learners; rather, it lies in  a process that seems otherwise far remote from the microcosm of  toddlers and caretakers -- language change. Although the rate of  social evolutionary change in learning structure appears unchang-  ing compared to the time it takes a child to develop language a-  bilities, this process is crucial to understanding how the child can  learn a language that on the surface appears impossibly complex  and poorly taught." [3, p115].  72 B. Tonkes, A. Blair and J. Wiles  In this paper we studied ways in which languages can adapt to their learners. By  running simulations of a language evolution process, we contribute additional com-  ponents to the list of aspects of language that can be learned by minimally-biased,  general-purpose learners, namely that recursive structure can be learned from few  examples, that languages can evolve to facilitate generalization in a particular way,  and that they can evolve to be easily learnable from common sentences. In al-  l the simulations in this paper, enhancement of language learnability is achieved  through changes to the learner's environment without resorting to adding biases in  the language acquisition device.  Acknowledgements  This work was supported by an APA to Bradley Tonkes, a UQ Postdoctoral Fel-  lowship to Alan Blair and an ARC grant to Janet Wiles.  References  [1] N. Chomsky. Language and Mind. Harcourt, Brace, New York, 1968.  [2] J. L. Elman, E. A. Bates, M. H. Johnson, A. Karmiloff-Smith, D. Parisi, and K. Plun-  keft. Rethinking Innateness: A Connectionist Perspective on Development. MIT  Press, Boston, 1996.  [3] T. W. Deacon. The Symbolic Species: The Co-Evolution of Language and the Brain.  W. W. Norton and Company, New York, 1997.  [4] S. Kirby. Fitness and the selective adaptation of language. In J. Hurford, C. Knight,  and M. Studdert-Kennedy, editors, Approaches to the Evolution of Language. Cam-  bridge University Press, Cambridge, 1998.  [5] M. H. Christiansen. Language as an organism -- implications for the evolution and  acquisition of language. Unpublished manuscript, February 1995.  [6] M. Hare and J. L. Elman. Learning and morphological change. Cognition, 56:61-98,  1995.  [7] S. Kirby. Syntax without natural selection: How compositionality emerges from  vocabulary in a population of learners. In C. Knight, J. Hurford, and M. Studdert-  Kennedy, editors, The Evolutionary Emergence of Language: Social function and the  origins of linguistic form. Cambridge University Press, Cambridge, 1999.  [8] J. Batall. Computational simulations of the emergence of grammar. In J. Hurford,  C. Knight, and M. Studdert-Kennedy, editors, Approaches to the Evolution of Lan-  guage, pages 405-426. Cambridge University Press, Cambridge, 1998.  [9] J. L. Elman. Learning and development in neural networks: The importance of  staxting small. Cognition, 48:71-99, 1993.  [10] J. Batall. Innate biases and critical periods: Combining evolution and learning in the  acquisition of syntax. In R. Brooks and P. Maes, editors, Proceedings of the Fourth  Artificial Life Workshop, pages 160-171. MIT Press, 1994.  [11] B. Tonkes, A. Blair, and J. Wiles. A paradox of neural encoders and decoders, or,  why don't we talk backwards? In B. McKay, X. Yao, C. S. Newton, J. -H. Kim,  and T. Furuhashi, editors, Simulated Evolution and Learning, volume 1585 of Lecture  Notes in Artificial Intelligence. Springer, 1999.  
An MEG Study of Response Latency and  Variability in the Human Visual System  During a VisualsMotor Integration Task  Akaysha C. Tang  Dept. of Psychology  University of New Mexico  Albuquerque, NM 87131  akaysha@unm. edu  Barak A. Pearlmutter  Dept. of Computer Science  University of New Mexico  Albuquerque, NM 87131  bap@cs.unm. edu  Tim A. Hely  Santa Fe Institute  1399 Hyde Park Road  Santa Fe, NM 87501  timhely@santafe. edu  Michael Zibulevsky  Dept. of Computer Science  University of New Mexico  Albuquerque, NM 87131  michael@cs. unto. edu  Michael P. Weisend  VA Medical Center  1501 San Pedro SE  Albuquerque, NM 87108  mweisend@unm. edu  Abstract  Human reaction times during sensory-motor tasks vary consider-  ably. To begin to understand how this variability arises, we exam-  ined neuronal populational response time variability at early versus  late visual processing stages. The conventional view is that pre-  cise temporal information is gradually lost as information is passed  through a layered network of mean-rate "units." We tested in hu-  mans whether neuronal populations at different processing stages  behave like mean-rate "units". A blind source separation algorithm  was applied to MEG signals from sensory-motor integration tasks.  Response time latency and variability for multiple visual sources  were estimated by detecting single-trial stimulus-locked events for  each source. In two subjects tested on four visual reaction time  tasks, we reliably identified sources belonging to early and late vi-  sual processing stages. The standard deviation of response latency  was smaller for early rather than late processing stages. This sup-  ports the hypothesis that human populational response time vari-  ability increases from early to late visual processing stages.  I Introduction  In many situations, precise timing of a motor output is essential for successful task  completion. Somehow the reliability in the output timing is related to the reliability  of the underlying neural systems associated with different stages of processing. Re-  cent literature from animal studies suggests that individual neurons from different  brain regions and different species can be surprising reliable [1, 2, 5, 7-9, 14, 17, 18],  186 A. C. Tang, B. A. Pearlmutter, T. A. Hely, M. Zibulevsky and M. P Weisend  on the order of a few milliseconds. Due to the low spatial resolution of electroen-  cephalography (EEG) and the requirement of signal averaging due to noisiness of  magnetoencephalography (MEG), in vivo measurement of human populational re-  sponse time variability from different processing stages has not been available.  In four visual reaction time (RT) tasks, we estimated neuronal response time vari-  ability at different visual processing stages using MEG. One major obstacle that has  prevented the analysis of response timing variability using MEG before is the rela-  tive weakness of the brain's magnetic signals (100fT) compared to noise in a shielded  environment (magnetized lung contaminants: 106fT; abdominal currents 105fT; car-  diogram and oculogram: 104fT; epileptic and spontaneous activity: 10afT) and in  the sensors (10fT) [13]. Consequently, neuronal responses evoked during cognitive  tasks often require signal averaging across many trials, making analysis of single-  trial response times unfeasible.  Recently, Bell-Sejnowski Infomax [1995] and Fast ICA [10] algorithms have been  used successfully to isolate and remove major artifacts from EEG and MEG data  [11, 15, 20]. These methods greatly increase the effective signal-to-noise ratio and  make single-trial analysis of EEG data feasible [12]. Here, we applied a Second-  Order Blind Identification algorithm (SOBI) [4] (another blind source separation, or  BSS, algorithm) to MEG data to find out whether populational response variability  changes from early to late visual processing stages.  2 Methods  2.1 Experimental Design  Two volunteer normal subjects (females, right handed) with normal or corrected-  to-normal visual acuity and binocular vision participated in four different visual RT  tasks. Subjects gave informed consent prior to the experimental procedure. During  each task we recorded continuous MEG signals at a 300Hz sampling rate with a  band-pass filter of 1-100Hz using a 122 channel Neuromag-122.  In all four tasks, the subject was presented with a pair of abstract color patterns,  one in the left and the other in the right visual field. One of the two patterns was a  target pattern. The subject pressed either a left or right mouse button to indicate on  which side the target pattern was presented. When a correct response was given, a  low or high frequency tone was presented binaurally following respectively a correct  or wrong response. The definition of the target pattern varied in the four tasks and  was used to control task difficulty which ranged from easy (task 1) to more difficult  (task 4) with increasing RTs. (The specific differences among the four tasks are not  important for the analysis which follows and are not discussed further.)  In this study we focus on the one element that all tasks have in common, i.e. ac-  tivation of multiple visual areas along the visual pathways. Our goal is to identify  visual neuronal sources activated in all four visual RT tasks and to measure and  compare response time variability between neuronal sources associated with early  and later visual processing stages. Specifically, we test the hypothesis that popula-  tional neuronal response times increase from early to later visual processing stages.  2.2 Source Separation Using SOBI  In MEG, magnetic activity from different neuronal populations is observed by many  sensors arranged around the subject's head. Each sensor responds to a mixture of  the signals emitted by multiple sources. We used the Second-Order Blind Identi-  MEG Study of Response Latency and Variability 187  fication algorithm (SOBI) [4] (a BSS algorithm) to simultaneously separate neu-  romagnetic responses from different neuronal populations associated with different  stages of visual processing. Responses from different neuronal populations will be  referred to as source responses and the neuronal populations that give rise to these  responses will be referred to as neuronal sources or simply sources. These neu-  tonal sources often, but not always, consist of a spatially contiguous population of  neurons. BSS separates the measured sensor signals into maximally independent  components, each having its own spatial map. Previously we have shown that some  of these BSS separated components correspond to noise sources, and many others  correspond to neuronal sources [19].  To establish the identity of the components, we analyzed both temporal and spa-  tial properties of the BSS separated components. Their temporal properties are  displayed using MEG images, similar to the ERP images described by [12] but  without smoothing across trials. These MEG images show stimulus or response  locked responses across many trials in a map, from which response latencies across  all displayed trials can be observed with a glance. The spatial properties of the sep-  arated components are displayed using a field map that shows the sensor projection  of a given component. The intensity at each point on the field map indicates how  strongly this component influences the sensor at this location.  The correspondence between the separated components and neuronal populational  responses at different visual processing stages were established by considering both  spatial and temporal properties of the separated components [19]. For example,  a component was identified as an early visual neuronal source if and only if (1)  the field pattern, or the sensor projection, of the separated component showed  a focal response over the occipital lobe, and (2) the ERP image showed visual  stimulus locked responses with latencies shorter than all other visual components  and falling within the range of early visual responses reported in studies using  other methods. Only those components consistent both spatially and temporally  with known neurophysiology and neuroanatomy were identified as neuronal sources.  2.3 Single Event Detection and Response Latency Estimation  For all established visual components we calculated the single-trial response latency  as follows. First, a detection window was defined using the stimulus-triggered av-  erage (STA). The beginning of the detection window was defined by the time at  which the STA first exceeded the range of baseline fluctuation. Baseline fluctuation  was estimated from the time of stimulus onset for approximately 50ms (the visual  response occurred no earlier than 60ms after stimulus onset.) The detection win-  dow ended when the STA first returned to the same level as when the detection  window began. The detection threshold was determined using a control window  with the same width as the detection window, but immediately preceding the de-  tection window. The threshold was adjusted until no more than five false detections  occurred within the control window for each ninety trials. We estimated RTs using  the leading edge of the response, rather than the time of the peak as this is more  robust against noise.  3 Results  In both subjects across all four visual RT tasks, SOBI generated components that  corresponded to neuronal populational responses associated with early and late  stages of visual processing. In both subjects, we identified a single component with  a sensor projection at the occipital lobe whose latency was the shortest among all  188 A. C. Tang, B. A. Pearlmutter, T. A. Hely, M. Zibulevsky and M. P. Weisend  task  1  2  3  4  early source late source  1  2  3  4  Figure 1: MEG images and field maps for an early and a late source from each  task, for subject 1 (top) and subject 2 (bottom). MEG image pixels are brightness-  coded source strength. Each row of a bitmap is one trial, running 1170ms from left  to right. Vertical bars mark stimulus onset, and 333ms of pre-stimulus activity is  shown. Each panel contains 90 trials. Field map brightness indicates the strength  with which a source activates each of the 61 sensor pairs.  visual stimulus locked components within task and subject (Fig. 1 left). We iden-  tified multiple components that had sensor projections either at occipital-parietal,  occipital-temporal, or temporal lobes, and whose response latencies are longer than  early-stage components within task and subject (Fig. 1 right).  Fig. 2a shows examples of detected single-trial responses for one early and one late  visual component (left: early; right: late) from one task. To minimize false positives,  the detection threshold was set high (allowing 5 false detections out of 90 trials)  at the expense of a low detection rate (15%-67%.) When Gaussian filters were  applied to the raw separated data, the detection rates were increased to 22-91%  (similar results hold but not shown). Fig. 2b shows such detected response time  histograms superimposed on the stimulus triggered average using raw separated  data. One early (top row) and two late visual components (middle and bottom  rows) are plotted for each of the four experiments in subject one. The histogram  width is smallest for early visual components (short mean response latency) and  larger for late visual components (longer latency.)  We computed the standard deviation of component response times as a measure of  response variability. Fig. 2c shows the response variability as a function of mean  response latency for subject one. Early components (solid boxes, shorter mean  latency) have smaller variability (height of the boxes) while late components (dashed  boxes, longer mean latency) have larger variability (height of the boxes). Multiple  MEG Study of Response Latency and Variability 189  lme (ms)   20  I I I I i I o  o 1oo o 1oo  Tim (ms)  1Or  Time (ms)  o  True (ms)  Time (n') Time  0 100 200 300 400 0 100 200 300 400  True (ms) True (ms)  Time  o  Time (m)  I  33me (m)  100 150 200  latency (ms)  Figure 2: (a, left) Response onset was estimated for each trial via threshold crossing  within a window of eligibility. (b, top right) The stimulus-locked averages for a  number of sources overlaid on histograms of response onset times. (c, bottom  right) Scatter plot of visual components from all experiments on subject 1 showing  the standard deviation of the latency (y axis) versus the mean latency (x axis),  with the error bars in each direction indicating one standard error in the respective  measurement. Lines connect sources from each task.  visual components from each task are connected by a line. Four tasks were shown  here. There is a general trend of increasing standard deviation of response times  as a function of early-to-late processing stages (increasing mean latency from left  to right). For the early visual components the standard deviation ranges from  6.6+0.63ms to 13.4+1.23ms, and for the late visual components, from 9.9+0.86ms  to 38.85:3.73ms (t = 3.565, p = 0.005.)  4 Discussion  By applying SOBI to MEG data from four visual RT tasks, we separated compo-  nents corresponding to neuronal populational responses associated with early and  190 A. C. Tang, B. A. Pearlmutter, T. A. Hely, M. Zibulevs19, and M. P Weisend  later stage visual processing in both subjects across all tasks. We performed single-  trial RT detection on these early- and late-stage components and estimated both  the mean and stdev of their response latency. We found that variability of the  populational response latency increased from early to late processing stages.  These results contrast with single neuron recordings obtained previously. In early  and late visual processing stages, the rise time of mean firing rate in single units  remained constant, suggesting an invariance in RT variability [16]. Characterizing  the precise relationship between single neuron and populational response reliability  is difficult without careful simulations or simultaneous single unit and MEG record-  ing. However, some major differences exist between the two types of studies. While  MEG is more likely to sample a larger neuronal population, single unit studies are  more likely to be selective to those neurons that are already highly reliable in their  responses to stimulus presentation. It is possible that the most reliable neurons at  both the early and late processing stages are equally reliable while large differences  exist between the early and late stages for the low reliability neurons.  Previously, ICA algorithms have been used successfully to separate out various noise  and neuronal sources in MEG data [19, 20]. Here we show that SOBI can also be  used to separate different neuronal sources, particularly those associated with dif-  ferent processing stages. The SOBI algorithm assumes that the components are  independent across multiple time scales and attempts to minimize the temporal  correlation at these time scales. Although neuronal sources at different stages of  processing are not completely independent as assumed in SOBI's derivation, BSS  algorithms of this sort are quite robust even when the underlying assumptions are  not fully met [6], i.e. the goodness of the separation is not significantly affected.  The ultimate reality check should come from satisfying physiological and anatom-  ical constraints derived from prior knowledge of the neural system under study.  This was carried out for our analysis. Firstly, the average response latencies of the  separated components fell within the range of latencies reported in MEG studies us-  ing conventional source modeling methods. Secondly, the spatial patterns of sensor  responses to these separated components are consistent with the known functional  anatomy of the visual system.  We have attempted to rule out many confounding factors. Our observed results  cannot be accounted for by a higher signal to noise ratio in the early visual re-  sponses. The increase in measured onset response time variability from early to  late visual processing stages was actually accompanied by an slightly lower signal-  to-noise ratio among the early components. The number of events detected for the  later components were also slightly greater than the earlier components. The higher  signal-to-noise ratio at later components should reduce noise-induced variability in  the later components, which would bias against the hypothesis that later visual re-  sponses have greater response time variability. We also found that response duration  and detection window size cannot account for the observed differential variabilities.  Later visual responses also had gentler onset slopes (as measured by the stimulus-  triggered average). Sensor noise unavoidably introduces noise into the response  onset detection process. We cannot rule out the possibility that the interaction of  the noise with the response onset profiles might give rise to the observed differ-  ential variabilities. Similarly, we cannot rule out the possibility that even greater  control of the experimental situation, such as better fixation and more effective  head restraints, would differentially reduce the observed variabilities. In general, all  measured variabilities can only be upper bounds, subject to downward revision as  improved instrumentation and experiments become available. It is with this cau-  tion in mind that we conclude that response time variability of neuronal populations  increases from early to late processing stages in the human visual system.  MEG Study of Response Latency and Variability 191  Acknowledgments  This research was supported by NSF CAREER award 97-02-311, and by the Na-  tional Foundation for Functional Brain Imaging.  References  [1] M. Abeles, H. Bergman, E. Margalit, and E Vaadia. Spatiotemporal firing patterns  in the frontal cortex of behaving monkeys. J. Neurophys., 70:1629-1638, 1993.  [2] W. Bair and C. Koch. Temporal precision of spike trains in extrastriate cortex of the  behaving macaque monkey. Neural Computation, 8(6):1184-1202, 1996.  [3] A. J. Bell and T. J. Sejnowski. An information-maximization approach to blind  separation and blind deconvolution. Neural Computation, 7(6):1129-1159, 1995.  [4] A. Belouchrani, K. A. Meraim, J.-F. Cardoso, and E. Moulines. Second-order blind  separation of correlated sources. In Proc. Int. Conf. on Digital Sig. Proc., pages  346-351, Cyprus, 1993.  [5] M. J. Berry, W. K. Waxland, and M. Meister. The structure and precision of retinal  spike trains. Proc. Natl. Acad. Sci. USA, 94:5411-5416, 1997.  [6] J.-F. Caxdoso. Blind signal separation: statistical principles. Proceedings of the IEEE,  9(10):2009-2025, October 1998.  [7] R.R. de Ruyter van Steveninck, G. D. Lewen, S. P. Strong, R. Kobetie, and W. Bialek.  Reproducibility and variability in neural spike trains. Science, 275:1805-1808, 1997.  [8] R. C. deChaxms and M. M. Merzenich. Primary cortical representation of sounds by  the coordination of action-potential timing. Nature, 381:610-3, 1996.  [9] M. Gur, A. Beylin, and D. M. Snodderly. Response variability of neurons in primary  visual cortex (V1) of alert monkeys. J. Neurosci., 17(8):2914-2920, 1997.  [10] A. Hyvaxinen and E. Oja. A fast fixed-point algorithm for independent component  analysis. Neural Computation, 9(7), October 1997.  [11] T.-P. Jung, C. Humphties, T.-W. Lee, M. J. McKeown, V. Iragui, S. Makeig, and T. J.  Sejnowski. Removing electroencephalographic artifacts by blind source separation.  Psychophysiology, 1999. In Press.  [12] T.-P. Jung, S. Makeig, M. Westerfield, J. Townsend, E. Courchesne, and T. J. Se-  jnowski. Analyzing and visualizing single-trial event-related potentials. In Advances  in Neural Information Processing Systems 11, pages 118-124. MIT Press, 1999.  [13] J. D. Lewine and W. W. Orrison, II. Magnetoencephalography and magnetic source  imaging. In Functional Brain Imaging, pages 369-417. Mosby, St. Louis, 1995.  [14] Z. F. Mainen and T. J. Sejnowski. Reliability of spike timing in neocortical neurons.  Science, 268:1503-1506, 1995.  [15] S. Makeig, T.-P. Jung, A. J. Bell, D. Ghahremani, and T. J. Sejnowski. Blind sepa-  ration of auditory event-related brain responses into independent components. Proc.  Nat. Acad. Sci., 94:10979-84, 1997.  [16] P. Marsalek, C. Koch, and J. Maunsell. On the relationship between synaptic input  and spike output jitter in individual neurons. Proc. Natl. Acad. Sci., 94:735-40, 1997.  [17] D. S. Reich, J. D. Victor, B. W. Knight, and T. Ozaki. Response vaxiability and  timing precision of neuronal spike trains in vivo. J. Neurophys., 77:2836-2841, 1997.  [18] A. C. Tang, A.M. Barrels, and T. J. Sejnowksi. Effects of cholinergic modulation on  responses of neocortical neurons to fluctuating inputs. Cereb. Cortex, 7:502-9, 1997.  [19] A. C. Tang, B. A. Pearlmutter, M. Zibulevsky, and R. Loring. Response time vari-  ability in the human sensory and motor systems. In Computational Neuroscience,  1999. To appear as a special issue of Neurocomputing.  [20] R. Vigfixio, V. Jousm/ki, M. H/m'fil/inen, R. Hari, and E. Oja. Independent com-  ponent analysis for identification of artifacts in magnetoencephalographic recordings.  In Advances in Neural Information Processing Systems 10. MIT Press, 1998.  
Neural Computation with Winner-Take-All as  the only Nonlinear Operation  Wolfgang Maass  Institute for Theoretical Computer Science  Technische Universitit Graz  A-8010 Graz, Austria  email: maassigi.tu-graz.ac.at  http://www. cis.tu-graz.ac.at/igi/maass  Abstract  Everybody "knows" that neural networks need more than a single layer  of nonlinear units to compute interesting functions. We show that this is  false if one employs winner-take-all as nonlinear unit:  Any boolean function can be computed by a single k-winner-take-  all unit applied to weighted sums of the input variables.  Any continuous function can be approximated arbitrarily well by  a single soft winner-take-all unit applied to weighted sums of the  input variables.  Only positive weights are needed in these (linear) weighted sums.  This may be of interest from the point of view of neurophysiology,  since only 15% of the synapses in the cortex are inhibitory. In addi-  tion it is widely believed that there are special microcircuits in the  cortex that compute winner-take-all.  Our results support the view that winner-take-all is a very useful  basic computational unit in Neural VLSI:  [] it is wellknown that winner-take-all of n input variables can  be computed very efficiently with 2n transistors (and a to-  tal wire length and area that is linear in n) in analog VLSI  [Lazzaro et al., 1989]  [] we show that winner-take-all is not just useful for special pur-  pose computations, but may serve as the only nonlinear unit for  neural circuits with universal computational power  [] we show that any multi-layer perceptron needs quadratically in  n many gates to compute winner-take-all for n input variables,  hence winner-take-all provides a substantially more powerful  computational unit than a perceptron (at about the same cost  of implementation in analog VLSI).  Complete proofs and further details to these results can be found in  [Maass, 2000].  294 W. Maass  1 Introduction  Computational models that involve competitive stages have so far been neglected in com-  putational complexity theory, although they are widely used in computational brain models,  artificial neural networks, and analog VLSI. The circuit of [Lazzaro et al., 1989] computes  an approximate version of winner-take-all on n inputs with just 2n transistors and wires  of length O(n), with lateral inhibition implemented by adding currents on a single wire of  length O(n). Numerous other efficient implementations of winner-take-all in analog VLSI  have subsequently been produced. Among them are circuits based on silicon spiking neu-  rons ([Meador and Hylander, 1994], [Indiveri, 1999]) and circuits that emulate attention in  artificial sensory processing ([Horiuchi et al., 1997], [Indiveri, 1999]). Preceding analytical  results on winner-take-all circuits can be found in [Grossberg, 1973] and [Brown, 1991].  We will analyze in section 4 the computational power of the most basic competitive compu-  tational operation: winner-take-all (= 1-WTA,). In section 2 we will discuss the somewhat  more complex operation k-winner-take-all (k-WTAn), which has also been implemented  in analog VLSI [Urahama and Nagao, 1995]. Section 3 is devoted to soft winner-take-all,  which has been implemented by [Indiveri, 1999] in analog VLSI via temporal coding of  the output.  Our results shows that winner-take-all is a surprisingly powerful computational module  in comparison with threshold gates (= McCulloch-Pitts neurons) and sigmoidal gates.  Our theoretical analysis also provides answers to two basic questions that have been  raised by neurophysiologists in view of the well-known asymmetry between excitatory  and inhibitory connections in cortical circuits: how much computational power of neural  networks is lost if only positive weights are employed in weighted linear sums, and how  much learning capability is lost if only the positive weights are subject to plasticity.  2 Restructuring Neural Circuits with Digital Output  We investigate in this section the computational power of a k-winner-take-all gate comput-  ing the function k - WTA, : 11 ' - {0, 1} '  Xl x2  bl b2  k - WTAn  b,  {0,1}  with  bi - 1 ++ xi is among the k largest of the inputs Xl,... , X n.  [precisely: bi = I - xj > xi holds for at most k - i indices j]  Neural Computation with Vnner-Take-All 295  Theorem 1. Any two-layer feedforward circuit C (with m analog or binary input  variables and one binary output variable) consisting of threshoM gates (=percep-  trons) can be simulated by a circuit W consisting of a single k-winner-take-all gate  k-WTAn I applied to weighted sums of the input variables with positive weights. This holds  for all digital inputs, and for analog inputs except for some set S c_ 1t m of inputs that has  measure O.  In particular, any boolean function  f ' {O, 1}m -+ {0,1}  can be computed by a single k-winner-take-all gate applied to positive weighted sums of  the input bits.  Remarks  If (7 has polynomial size and integer weights, whose size is bounded by a polyno-  mial in m, then the number of linear gates S in W can be bounded by a polynomial  in m, and all weights in the simulating circuit W are natural numbers whose size  is bounded by a polynomial in m.  The exception set of measure 0 in this result is a union of finitely many hyper-  planes in 11 m. One can easily show that this exception set S of measure 0 in  Theorem 1 is necessary.  Any circuit that has the structure of W can be converted back into a 2-layer thresh-  old circuit, with a number of gates that is quadratic in the number of weighted  sums (=linear gates) in W. This relies on the construction in section 4.  Proof of Theorem 1: Since the outputs of the gates on the hidden layer of C are from  {0, 1}, we can assume without loss of generality that the weights cti,... , ct, of the out-  put gate G of C are from {- 1, 1 } (see for example [Siu et al., 1995] for details; one first  observes that it suffices to use integer weights for threshold gates with binary inputs, one  can then normalize these weights to values in {- 1, 1 } by duplicating gates on the hidden  layer of C). Thus for any circuit input z E l " we have C(z) = 1 ,  ctj Gj (_z) >_ t3,  j=l  where G1,... , Gn are the threshold gates on the hidden layer of C, ctl,... , ct, are from  {-1, 1}, and 0 is the threshold of the output gate G. In order to eliminate the negative  weights in G we replace each gate Gj for which c U = - 1 by another threshold gate j so  that  (z) = 1 - G (z_) for all z E 1 " except on some hyperplane. 2 We set Oj := Gj  for all j  { 1,..., n} with cj = 1. Then we have for all z  1 " , except for z from some  exception set S consisting of up to n hyperplanes,  EctGJ(Z) = ESJ(z) -[{j  {1,... ,n} 'c U = -1}[.  j=l j=l  HenceC(z) = 1   8j(z) >_ k  j=l  for all z  11 m - S, for some suitable/c E N.  Let w{,... , wJ  11 be the weights and O )  11 be the threshold of gate j, j = 1,... , n.  l of which we only use its last output bit  2We exploit here that -, ii wizi _> 0 4g i=i (-wi)zi > -O for arbitrary wi, zi, 0 E R .  296 W. Maass  Z1 Zm  G Gn  G  u  Gi,... ,Gn are arbitrary threshold gates, G  is a threshold gate with weights from {- 1,1 }  Z1 Zm W  '1 n+ 1  b  and back  S1,... , Sn+l are linear gates (with positive  weights only, which are sums of absolute val-  ues of weights from the gates GI,.   , Gn)  Thus O(z_)= I   and  E Iwlzi- E Iwlzi >- OJ. Hence with  i:w >o i:w <o  i:w <o - i:wf>o  forj = 1,... ,n  j=l i:w >0  we have for every j E {1,... ,n} and every z E IR "   Sn+l _> s   Iilz-  Ilz _> o  (_)= 1.  i:w>O i:w<O  This implies that the (n + 1)st output bn+ of the k-winner-take-all gate k-WTA,+i for  Neural Computation with Winner-Take-All 297  k := n - c + 1 applied to S1,... , $n+1 satisfies  bn+l - 1  I{J E {1,... ,n + 1}: $,+1 >_ $j}l > k + 1  q= ]{j e {1,... ,n} ' Sn+l >_ S5}1 _> [c  4= C(z) = 1.  Note that all the coefficients in the sums S1,..., Sn+ are positive.  3 Restructuring Neural Circuits with Analog Output  In order to approximate arbitrary continuous functions with values in [0, 1] by circuits that  have a similar structure as those in the preceding section, we consider here a variation of a  winner-take-all gate that outputs analog numbers between 0 and 1, whose values depend on  the rank of the corresponding input in the linear order of all the n input numbers. One may  argue that such gate is no longer a "winner-take-all" gate, but in agreement with common  terminology we refer to it as a soft winner-take-all gate. Such gate computes a function  from 11 n into [0, 1]"  soft winner-take-all  /'1 /'2  [0,1]  whose ith output/'i  [0, 1] is roughly proportional to the rank of xi among the numbers  Xl,  .. , Xn. More precisely: for some parameter T E N we set  I{J e {1,... ,n}' xi > xj}l   /'i -- ,  T  rounded to 0 or I if this value is outside [0, 1]. Hence this gate focuses on those  inputs xi whose rank among the n input numbers x,... ,Xn belongs to the set  {," "   + 1,..., min{n, T +  }}. These ranks are linearly scaled into [0, 1]. 3  Theorem 2. Circuits consisting of a single soft winner-take-all gate (of which we only use  its first output r 0 applied to positive weighted sums of the input variables are universal  approximators for arbitrary continuous functions from 11 m into [0, 1].   3It is shown in [Maass, 2000] that actually any continuous monotone scaling into [0, 1] can be  used instead.  298 W. Maass  A circuit of the type considered in Theorem 2 (with a soft winner-take-all gate applied to  n positive weighted sums S,... , S,) has a very simple geometrical interpretation: Over  each point z of the input "plane" 11 ' we consider the relative heights of the n hyperplanes  H,... , H, defined by the n positive weighted sums S1,... , S,. The circuit output de-  pends only on how many of the other hyperplanes H2,... , H, are above H at this point z.  4 A Lower Bound Result for Winner-Take-All  One can easily see that any k-WTA gate with n inputs can be computed by a 2-layer thresh-  old circuit consisting of () + n threshold gates:  ?  xi    Xl X i ,, Xj X n  () threshold gates  n threshold gates  Hence the following result provides an optimal lower bound.  Theorem 3. Any feedforward threshold circuit (=multi-layer perceptron) that computes  1-WTA for n inputs needs to have at least () + n gates. I  5 Conclusions  The lower bound result of Theorem 3 shows that the computational power of winner-take-  all is quite large, even if compared with the arguably most powerful gate commonly studied  in circuit complexity theory: the threshold gate (also referred to a McCulloch-Pitts neuron  or perceptron).  Neural Computation with Winner-Take-All 299  It is well known ([Minsky and Papert, 1969]) that a single threshold gate is not able to  compute certain important functions, whereas circuits of moderate (i.e., polynomial) size  consisting of two layers of threshold gates with polynomial size integer weights have re-  markable computational power (see [Siu et al., 1995]). We have shown in Theorem 1 that  any such 2-layer (i.e., 1 hidden layer) circuit can be simulated by a single k-winner-take-all  gate, applied to polynomially many weighted sums with positive integer weights of poly-  nomial size.  We have also analyzed the computational power of soft winner-take-all gates in the context  of analog computation. It is shown in Theorem 2 that a single soft winner-take-all gate  may serve as the only nonlinearity in a class of circuits that have universal computational  power in the sense that they can approximate any continuous functions.  Furthermore our novel universal approximators require only positive linear operations be-  sides soft winner-take-all, thereby showing that in principle no computational power is lost  if in a biological neural system inhibition is used exclusively for unspecific lateral inhibi-  tion, and no adaptive flexibility is lost if synaptic plasticity (i.e., "learning") is restricted to  excitatory synapses.  Our somewhat surprising results regarding the computational power and universality of  winner-take-all point to further opportunities for low-power analog VLSI chips, since  winner-take-all can be implemented very efficiently in this technology.  References  [Brown, 1991] Brown, T. X. (1991). Neural NetworkDesign for Switching Network Con-  trol.. Ph.-D.-Thesis, CALTECH.  [Grossberg, 1973] Grossberg, S. (1973). Contour enhancement, short term memory, and  constancies in reverberating neural networks. Studies in Applied Mathematics, vol. 52,  217-257.  [Horiuchi et al., 1997] Horiuchi, T. K., Morris, T. G., Koch, C., DeWeerth, S. P. (1997).  Analog VLSI circuits for attention-based visual tracking. Advances in Neural Informa-  tion Processing Systems, vol. 9, 706-712.  [Indiveri, 1999] Indiveri, G. (1999). Modeling selective attention using a neuromorphic  analog VLSI device, submitted for publication.  [Lazzaro et al., 1989] Lazzaro, J., Ryckebusch, S., Mahowald, M. A., Mead, C. A. (1989).  Winner-take-all networks of O (rt) complexity. Advances in Neural Information Process-  ing Systems, vol. I, Morgan Kaufmann (San Mateo), 703-71 I.  [Maass, 2000] Maass, W. (2000). On the computational power of winner-take-all, Neural  Computation, in press.  [Meador and Hylander, 1994] Meador, J. L., and Hylander, P. D. (1994). Pulse coded  winner-take-all networks. In: Silicon Implementation of Pulse Coded Neural Networks,  Zaghloul, M. E., Meador, J., and Newcomb, R. W., eds., Kluwer Academic Publishers  (Boston), 79-99.  [Minsky and Papert, 1969] Minsky, M. C., Papert, S. A. (1969). Perceptrons, MIT Press  (Cambridge).  [Siu et al., 1995] Siu, K.-Y., Roychowdhury, V., Kailath, T. (1995). Discrete Neural Com-  putation: A Theoretical Foundation. Prentice Hall (Englewood Cliffs, N J, USA).  [Urahama and Nagao, 1995] Urahama, K., and Nagao, T. (1995). k-winner-take-all circuit  with O( N) complexity. IEEE Trans. on Neural Networks, vol.6, 776-778.  
Policy Search via Density Estimation  Andrew Y. Ng  Computer Science Division  U.C. Berkeley  Berkeley, CA 94720  ang @ cs. berkeley. edu  Ronald Parr  Computer Science Dept.  Stanford University  Stanford, CA 94305  parr @ cs. stanford. edu  Daphne Koller  Computer Science Dept.  Stanford University  Stanford, CA 94305  kolle r@ cs. stanfo rd. edu  Abstract  We propose a new approach to the problem of searching a space of  stochastic controllers for a Markov decision process (MDP) or a partially  observable Markov decision process (POMDP). Following several other  authors, our approach is based on searching in parameterized families  of policies (for example, via gradient descent) to optimize solution qual-  ity. However, rather than trying to estimate the values and derivatives  of a policy directly, we do so indirectly using estimates for the proba-  bility densities that the policy induces on states at the different points  in time. This enables our algorithms to exploit the many techniques for  efficient and robust approximate density propagation in stochastic sys-  tems. We show how our techniques can be applied both to deterministic  propagation schemes (where the MDP's dynamics are given explicitly in  compact form,) and to stochastic propagation schemes (where we have  access only to a generative model, or simulator, of the MDP). We present  empirical results for both of these variants on complex problems.  1 Introduction  In recent years, there has been growing interest in algorithms for approximate planning  in (exponentially or even infinitely) large Markov decision processes (MDPs) and par-  tially observable MDPs (POMDPs). For such large domains, the value and Q-functions  are sometimes complicated and difficult to approximate, even though there may be simple,  compactly representable policies which perform very well. This observation has led to par-  ticular interest in direct policy search methods (e.g., [9, 8, 1]), which attempt to choose a  good policy from some restricted class II of policies. In our setting, II = {ro: 0 C I ' } is  a class of policies smoothly parameterized by 0 C 11 m . If the value of ro is differentiable  in 0, then gradient ascent methods may be used to find a locally optimal 'o. However,  estimating values of ro (and the associated gradient) is often far from trivial. One simple  method for estimating ro's value involves executing one or more Monte Carlo trajectories  using ro, and then taking the average empirical return; cleverer algorithms executing sin-  gle trajectories also allow gradient estimates [9, 1]. These methods have become a standard  approach to policy search, and sometimes work fairly well.  In this paper, we propose a somewhat different approach to this value/gradient estimation  problem. Rather than estimating these quantities directly, we estimate the probability den-  sity over the states of the system induced by ro at different points in time. These time slice  Policy Search via Density Estimation 1023  densities completely determine the value of the policy 7to. While density estimation is not  an easy problem, we can utilize existing approaches to density propagation [3, 5], which al-  low users to specify prior knowledge about the densities, and which have also been shown,  both theoretically and empirically, to provide robust estimates for time slice densities. We  show how direct policy search can be implemented using this approach in two very differ-  ent settings of the planning problem: In the first, we have access to an explicit model of the  system dynamics, allowing us to provide an explicit algebraic operator that implements the  approximate density propagation process. In the second, we have access only to a genera-  tive model of the dynamics (which allows us only to sample from, but does not provide an  explicit representation of, next-state distributions). We show how both of our techniques  can be combined with gradient ascent in order to perform policy search, a somewhat subtle  argument in the case of the sampling-based approach. We also present empirical results for  both variants in complex domains.  2 Problem description  A Markov Decision Process (MDP) is a tuple (S, so, A,/, P) where:  S is a (possibly  infinite) set of states; so C $ is a start state; A is a finite set of actions; / is a reward  function trl  S  [0,/,n:]; P is a transition model P  S x A  As, such that  P( s I s, a) gives the probability of landing in state s  upon taking action a in state s.  A stochastic policy is a map 7r  $  AA, where 7r(a I s) is the probability of taking action  a in state s. There are many ways of defining a policy 7r's "quality" or value. For a horizon  T and discount factor 3`, the finite horizon discounted value function VT,. [7r] is defined by  V0,7171-]($ ) -- .R($); Vt+1,7171-]($) -- ]($) q- 3` Ea 71'(a I $) Es, P(s' l s,a),,[w](s').  For an infinite state space (here and below), the summation is replaced by an integral. We  can now define several optimality criteria. The finite horizon total reward with horizon  T is Vy[7r] = Vy, l[7r](so). The infinite horizon discounted reward with discount 3' <  1 is V.[7r] = limr_, VT,,[7r](so). The infinite horizon average reward is V,a[7r ] =  limT_ 1   VT, 1 [7r] (So), where we assume that the limit exists.  Fix an optimality criterion V. Our goal is to find a policy that has a high value. As dis-  cussed, we assume we have a restricted set II of policies, and wish to select a good 7r C II.  We assume that II - {7to ] 0  11 m } is a set of policies parameterized by 0  11 m, and  that 7to (a I s) is continuously differentiable in 0 for each s, a. As a very simple example,  we may have a one-dimensional state, two-action MDP with "sigmoidal" 7to, such that the  probability of choosing action ao at state x is 7ro(ao [ x) = 1/(1 + exp(-0 - 02x)).  Note that this framework also encompasses cases where our family II consists of policies  that depend only on certain aspects of the state. In particular, in POMDPs, we can restrict  attention to policies that depend only on the observables. This restriction results in a sub-  class of stochastic memory-free policies. By introducing artificial "memory bits" into the  process state, we can also define stochastic limited-memory policies. [6]  Each 0 has a value V[O] = V[7ro], as specified above. To find the best policy in II, we can  search for the 0 that maximizes V[O]. If we can compute or approximate V[O], there are  many algorithms that can be used to find a local maximum. Some, such as NeMer-Mead  simplex search (not to be confused with the simplex algorithm for linear programs), require  only the ability to evaluate the function being optimized at any point. If we can compute  or estimate V[O]'s gradient with respect to 0, we can also use a variety of (deterministic or  stochastic) gradient ascent methods.  We write rewards as R(s) rather than R(s, a), and assume a single start state rather than an  initial-state distribution, only to simplify exposition; these and several other minor extensions are  trivial.  1024 A. Y. Ng, R. Parr and D. Koller  3 Densities and value functions  Most optimization algorithms require some method for computing V[O] for any 0 (and  sometimes also its gradient). In many real-life MDPs, however, doing so exactly is com-  pletely infeasible, due to the large or even infinite number of states. Here, we will consider  an approach to estimating these quantities, based on a density-based reformulation of the  value function expression. A policy r induces a probability distribution over the states at  each time t. Letting (o) be the initial distribution (giving probability 1 to so), we define  the time slice distributions via the recurrence:  (1)  It is easy to verify that the standard notions of value defined earlier can reformulated in  terms of (t). e.g., Vr,,[r](so) r  , -- Y'-t=0  R), where  is the dot-product operation  (equivalently, the expectation of R with respect to (t)). Somewhat more subtly, for the  case of infinite horizon average reward, we have that V, vg [r] = ()  R, where () is  the limiting distribution of (1), if one exists.  This reformulation gives us an alternative approach to evaluating the value of a policy ro'  we first compute the time slice densities (t) (or ()), and then use them to compute the  value. Unfortunately, that modification, by itself, does not resolve the difficulty. Repre-  senting and computing probability densities over large or infinite spaces is often no easier  than representing and computing value functions. However, several results [3, 5] indicate  that representing and computing high-quality approximate densities may often be quite  feasible. The general approach is an approximate density propagation algorithm, using  time-slice distributions in some restricted family  For example, in continuous spaces, E  might be the set of multivariate Gaussians.  The approximate propagation algorithm modifies equation (1) to maintain the time-slice  densities in E. More precisely, for a policy ro, we can view (1) as defining an operator  (I)[0] that takes one distribution in As and returns another. For our current policy roo,  we can rewrite (1) as: (t+) = (i)[0o]((t)). In most cases, E will not be closed under  (I); approximate density propagation algorithms use some alternative operator , with the  properties that, for  6 =' (a) () is also in E, and (b) () is (hopefully)close to (1)(6).  We use ,[0] to denote the approximation to (I)[0], and ((t) to denote (,[0])(t)(()). If  , is selected carefully, it is often the case that ((t) is close to (t). Indeed, a standard  contraction analysis for stochastic processes can be used to show:  Proposition 1 Assume that for all t, I1(I)( ('9) - '(((*))ll -< - Then there exists some  constant/X such that for all t,  In some cases, ,X might be arbitrarily small, in which case the proposition is meaningless.  However, there are many systems where/X is reasonable (and independent of e) [3]. Fur-  thermore, empirical results also show that approximate density propagation can often track  the exact time slice distributions quite accurately.  Approximate tracking can now be applied to our planning task. Given an optimality crite-  rion V expressed with (t) s, we define an approximation 1 to it by replacing each (t) with  ((t), e.g., T,h,[71-](8O) T  -- Y-t=0 3/t (t) ' t. Accuracy guarantees on approximate tracking  induce comparable guarantees on the value approximation; from this, guarantees on the  ^  performance of a policy r b found by optimizing V are also possible:  Proposition 2 Assume that, for all t, we have that - c*)111 Then for each fixed  T, %' IVT,.[r](so)- T,,-),[71-1($o)1--- O(().  Policy Search via Density Estimation 1025  Proposition3 Let 0* -- argmaxo VIOl and  = argmaxo 1[0]. /f maxo IV[0] -  01011 _< e, then V[0*] - V[O] _< 2e.  4 Differentiating approximate densities  In this section we discuss two very different techniques for maintaining an approximate  density (t) using an approximate propagation operator , and show when and how they  can be combined with gradient ascent to perform policy search. In general, we will assume  that E is a family of distributions parameterized by  C 11  . For example, if E is the set  of d-dimensional multivariate Gaussians with diagonal covariance matrices,  would be a  2d-dimensional vector, specifying the mean vector and the covariance matrix's diagonal.  Now, consider the task of doing gradient ascent over the space of policies, using some  optimality criterion 1, say lrT,7[O ]. Differentiating it relative to 0, we get 701rr,710 ] :  r t (*) . R. To avoid introducing new notation, we also use (t) to denote the as-  Et=O t dO  sociated vector of parameters  c I  . These parameters are a function of O. Hence, the  internal gradient term is represented by an g x m Jacobian matrix, with entries representing  the derivative of a parameter i relative to a parameter Oj. This gradient can be computed  using a simple recurrence, based on the chain rule for derivatives:  --(Oo). (2)  The first summand (an  x m Jacobian) is the derivative of the transition operator  relative  to the policy parameters 0. The second is a product of two terms: the derivative of ,  relative to the distribution parameters, and the result of the previous step in the recurrence.  4.1 Deterministic density propagation  Consider a transition operator  (for simplicity, we omit the dependence on 0). The idea in  this approach is to try to get  () to be as close as possible to  (), subject to the constraint  ^ ^  that ()  E. Specifically, we define a projection operator F that takes a distribution b  not in E, and returns a distribution in E which is closest (in some sense) to b. We then  define () = F(()). In order to ensure that gradient descent applies in this setting,  we need only ensure that F and  are differentiable functions. Clearly, there are many  instantiations of this idea for which this assumption holds. We provide two examples.  Consider a continuous-state process with nonlinear dynamics, where  is a mixture of  conditional linear Gaussians. We can define E to be the set of multivariate Gaussians.  The operator F takes a distribution (a mixture of gaussians) b and computes its mean  and covariance matrix. This can be easily computed from 's parameters using simple  differentiable algebraic operations.  A very different example is the algorithm of [3] for approximate density propagation in  dynamic Bayesian networks (DBNs). A DBN is a structured representation of a stochastic  process, that exploits conditional independence properties of the distribution to allow com-  pact representation. In a DBN, the state space is defined as a set of possible assignments  a: to a set of random variables X,... , Xn. The transition model P(a? [ a:) is described  using a Bayesian network fragment over the nodes {X,..., X,, XI,..., X}. A node  Xi represents X? ) and X represents X? +). The nodes Xi in the network are forced  to be roots (i.e., have no parents), and are not associated with conditional probability dis-  tributions. Each node X is associated with a conditional probability distribution (CPD),  which specifies P(X [ Parents(X)). The transition probability P(Xt[ X) is defined as  1026 A. Y. Ng, R. Parr and D. Koller  I-[i P(X I Parents(X)). DBNs support a compact representation of complex transition  models in MDPs [2]. We can extend the DBN to encode the behavior of an MDP with a  stochastic policy r by introducing a new random variable A representing the action taken  at the current time. The parents of A will be those variables in the state on which the action  is allowed to depend. The CPD of A (which may be compactly represented with function  approximation) is the distribution over actions defined by r for the different contexts.  In discrete DBNs, the number of states grows exponentially with the number of state vari-  ables, making an explicit representation of a joint distribution impractical. The algorithm  of [3] defines E to be a set of distributions defined compactly as a set of marginals over  smaller clusters of variables. In the simplest example, E is the set of distributions where  X,... , X, are independent. The parameters  defining a distribution in E are the param-  eters of n multinomials. The projection operator F simply marginalizes distributions onto  the individual variables, and is differentiable. One useful corollary of [3]'s analysis is that  the decay rate of a structured ' over E can often be much higher than the decay rate of   , so that multiple applications of  can converge very rapidly to a stationary distribution;  this property is very useful when approximating p(m) to optimize relative to V, v9.  4.2 Stochastic density propagation  In many settings, the assumption that we have direct access to  is too strong. A weaker  assumption is that we have access to a generative model -- a black box from which we  can generate samples with the appropriate distribution; i.e., for any s, a, we can generate  samples s t from P(s' I s, a). In this case, we use a different approximation scheme,  based 6n [5]. The operator , is a stochastic operator. It takes the distribution , and  generates some number of random state samples si from it. Then, for each si and each  action a, we generate a sample s' i from the transition distribution P(. I si, a). This sample  (si, ai, si) is then assigned a weight wi = ro(ai I si), to compensate for the fact that not  all actions would have been selected by ro with equal probability. The resulting set of N  samples si weighted by the wis is given as input to a statistical density estimator, which  uses it to estimate a new density . We assume that the density estimation procedure is a  differentiable function of the weights, often a reasonable assumption.  Clearly, this , can be used to compute (t) for any t, and thereby approximate ro's value.  However, the gradient computation for  is far from trivial. In particular, to compute the  derivative 0'/0, we must consider ,'s behavior for some perturbed ?) other than the  one (say, (o t)) to which it was applied originally. In this case, an entirely different set of  samples would probably have been generated, possibly leading to a very different density.  It is hard to see how one could differentiate the result of this perturbation. We propose an  alternative solution based on importance sampling. Rather than change the samples, we  modify their weights to reflect the change in the probability that they would be generated.  Specifically, when fitting t+), we now define a sample (si, ai, s:)'s weight to be  w,(?,o) -- Is,) (3)  We can now compute "s derivatives at (0o, (o t)) with respect to any of its parameters, as  required in (2). Let ( be the vector of parameters (0, ). Using the chain rule, we have  o,i,[o]() o4[o1()  The first term is the derivative of the estimated density relative to the sample weights (an   x N matrix). The second is the derivative of the weights relative to the parameter vector  (an N x (m + ) Jacobian), which can easily be computed from (3).  Policy Search via Density Estimation 1027  0.42  0.4  038  0.36  (' 0.32  0.28  026  024  0  Actual avg. cost  Estimatedavg. cost  50 100 150 200 250 300 350 400 450  #Function evaluations  (a) (b)  Figure 1: Driving task: (a) DBN model; (b) policy-search/optimization results (with 1 s.e.)  5 Experimental results  We tested our approach in two very different domains. The first is an average-reward  DBN-MDP problem (shown in Figure 1 (a)), where the task is to find a policy for changing  lanes when driving on a moderately busy two-lane highway with a slow lane and a fast  lane. The model is based on the BAT DBN of [4], the result of a separate effort to build a  good model of driver behavior. For simplicity, we assume that the car's speed is controlled  automatically, so we are concerned only with choosing the Lateral Action - change lane or  drive straight. The observables are shown in the figure: LClr and RClr are the clearance to  the next car in each lane (close, medium or far). The agent pays a cost of 1 for each step  it is "blocked" by (meaning driving close to) the car to its front; it pays a penalty of 0.2  per step for staying in the fast lane. Policies are specified by action probabilities for the 18  possible observation combinations. Since this is a reasonably small number of parameters,  we used the simplex search algorithm described earlier to optimize 1 [0].  The process mixed quite quickly, so (2o) was a fairly good approximation to ().  used  a fully factored representation of the joint distribution except for a single cluster over the  three observables. Evaluations are averages of 300 Monte Carlo trials of 400 steps each.  Figure l(b) shows the estimated and actual average rewards, as the policy parameters are  evolved over time. The algorithm improved quickly, converging to a very natural policy  with the car generally staying in the slow lane, and switching to the fast lane only when  necessary to overtake.  In our second experiment, we used the bicycle simulator of [7]. There are 9 actions cor-  responding to leaning leftYcenter/right and applying negative/zero/positive torque to the  handlebar; the six-dimensional state used in [7] includes variables for the bicycle's tilt an-  gle and orientation, and the handlebar's angle. If the bicycle tilt exceeds 7r/15, it falls over  and enters an absorbing state. We used policy search over the following space: we selected  twelve (simple, manually chosen but not fine-tuned) features of each state; actions were  chosen with a softmax -- the probability of taking action ai is exp (a: .wi) / Y']j exp (a: .w j).  As the problem only comes with a generative model of the complicated, nonlinear, noisy  bicycle dynamics, we used the stochastic density propagation version of our algorithm,  with (stochastic) gradient ascent. Each distribution in E was a mixture of a singleton point  consisting of the absorbing-state, and of a 6-D multivariate Gaussian.  1028 A. Y. Ng, R. Parr and D. Koller  The first task in this domain was to balance reliably on the bicycle. Using a horizon of  T -- 200, discount 3' -- 0.995, and 600 si samples per density propagation step, this was  quickly achieved. Next, trying to learn to ride to a goal 2 10m in radius and 1000m away,  it also succeeded in finding policies that do so reliably. Formal evaluation is difficult, but  this is a sufficiently hard problem that even finding a solution can be considered a success.  There was also some slight parameter sensitivity (and the best results were obtained only  with (o) picked/fit with some care, using in part data from earlier and less successful trials,  to be "representative" of a fairly good rider's state distribution,) but using this algorithm,  we were able to obtain solutions with median riding distances under 1.1 km to the goal. This  is significantly better than the results of [7] (obtained in the learning rather than planning  setting, and using a value-function approximation solution), which reported much larger  riding distances to the goal of about 7km, and a single "best-ever" trial of about 1.7km.  6 Conclusions  We have presented two new variants of algorithms for performing direct policy search in  the deterministic and stochastic density propagation settings. Our empirical results have  also shown these methods working well on two large problems.  Acknowledgements. We warmly thank Kevin Murphy for use of and help with his Bayes  Net Toolbox, and Jette Rand10v and Preben Alstr0m for use of their bicycle simulator. A.  Ng is supported by a Berkeley Fellowship. The work of D. Koller and R. Parr is sup-  ported by the ARO-MURI program "Integrated Approach to Intelligent Systems", DARPA  contract DACA76-93-C-0025 under subcontract to IET, Inc., ONR contract N66001-97-C-  8554 under DARPA's HPKB program, the Sloan Foundation, and the Powell Foundation.  References  [1] L. Baird and A.W. Moore. Gradient descent for general Reinforcement Learning. In NIPS 11,  1999.  [2] C. Boutilier, T. Dean, and S. Hanks. Decision theoretic planning: Structural assumptions and  computational leverage. J. Artificial Intelligence Research, 1999.  [3] X. Boyen and D. Koller. Tractable inference for complex stochastic processes. In Proc. UAI,  pages 33-42, 1998.  [4] J. Forbes, T Huang, K. Kanazawa, and S.J. Russell. The BATmobile: Towards a Bayesian  automated taxi. In Proc. IJCAI, 1995.  [5] D. Koller and R. Fratkina. Using learning for approximation in stochastic processes. In Proc.  ICML, pages 287-295, 1998.  [6] N. Meuleau, L. Peshkin, K-E. Kim, and L.P. Kaelbling. Learning finite-state controllers for  partially observable environments. In Proc. UAI 15, 1999.  [7] J. Rand10v and P. Alstr0m. Learning to drive a bicycle using reinforcement learning and shaping.  In Proc. ICML, 1998.  [8] J.K. Williams and S. Singh. Experiments with an algorithm which learns stochastic memoryless  policies for POMDPs. In NIPS 11, 1999.  [9] R.J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement  learning. Machine Learning, 8:229-256, 1992.  2For these experiments, we found learning could be accomplished faster with the simulator's  integration delta-time constant tripled for training. This and "shaping" reinforcements (chosen to  reward progress made towards the goal) were both used, and training was with the bike "infinitely  distant" from the goal. For this and the balancing experiments, sampling from the fallen/absorbing-  state portion of the distributions (t) is obviously inefficient use of samples, so all samples were  drawn from the non-absorbing state portion (i.e. the Gaussian, also with its tails corresponding to tilt  angles greater than -/15 truncated), and weighted accordingly relative to the absorbing-state portion.  
Churn Reduction in the Wireless Industry  Michael C. Mozer *+, Richard Wolniewicz*, David B. Grimes *+,  Eric Johnson*, Howard Kaushansky*  * Athene Software + Department of Computer Science  2060 Broadway, Suite 300 University of Colorado  BouMer, CO 80302 Boulder, CO 80309-0430  Abstract  Competition in the wireless telecommunications industry is rampant. To main-  tain profitability, wireless carriers must control churn, the loss of subscribers  who switch from one carder to another. We explore statistical techniques for  chum prediction and, based on these predictions, an optimal policy for identify-  ing customers to whom incentives should be offered to increase retention. Our  experiments are based on a data base of nearly 47,000 U.S. domestic subscrib-  ers, and includes information about their usage, billing, credit, application, and  complaint history. We show that under a wide variety of assumptions concerning  the cost of intervention and the retention rate resulting from intervention, chum  prediction and remediation can yield significant savings to a carrier. We also  show the importance of a data representation crafted by domain experts.  Competition in the wireless telecommunications industry is rampant. As many as seven  competing carders operate in each market. The industry is extremely dynamic, with new  services, technologies, and carriers constantly altering the landscape. Carriers announce  new rates and incentives weekly, hoping to entice new subscribers and to lure subscribers  away from the competition. The extent of rivalry is reflected in the deluge of advertise-  ments for wireless service in the daily newspaper and other mass media.  The United States had 69 million wireless subscribers in 1998, roughly 25% of the  population. Some markets are further developed; for example, the subscription rate in Fin-  land is 53%. Industry forecasts are for a U.S. penetration rate of 48% by 2003. Although  there is significant room for growth in most markets, the industry growth rate is declining  and competition is rising. Consequently, it has become crucial for wireless carriers to con-  trol churn--the loss of customers who switch from one carrier to another. At present,  domestic monthly chum rates are 2-3% of the customer base. At an average cost of $400  to acquire a subscriber, churn cost the industry nearly $6.3 billion in 1998; the total annual  loss rose to nearly $9.6 billion when lost monthly revenue from subscriber cancellations is  considered (Luna, 1998). It costs roughly five times as much to sign on a new subscriber  as to retain an existing one. Consequently, for a carrier with 1.5 million subscribers, reduc-  ing the monthly churn' rate from 2% to 1% would yield an increase in annual earnings of at  least $54 million, and an increase in shareholder value of approximately $150 million.  (Estimates are even higher when lost monthly revenue is considered; see Fowlkes, Madan,  Andrew, & Jensen, 1999; Luna, 1998.)  The goal of our research is to evaluate the benefits of predicting churn using tech-  niques from statistical machine learning. We designed models that predict the probability  936 M. C. Mozer, R. Wolniewicz, D. B. Grimes, E. Johnson and H. Kaushansky  of a subscriber churning within a short time window, and we evaluated how well these pre-  dictions could be used for decision making by estimating potential cost savings to the  wireless carrier under a variety of assumptions concerning subscriber behavior.  1 THE FRAMEWORK  Figure 1 shows a framework for churn prediction and profitability maximization.  Data from a subscriber--on which we elaborate in the next section--is fed into three com-  ponents which estimate: the likelihood that the subscriber will churn, the profitability  (expected monthly revenue) of the subscriber, and the subscriber's credit risk. Profitability  and credit risk determine how valuable the subscriber is to the carrier, and hence influ-  ences how much the carrier should be willing to spend to retain the subscriber. Based on  the predictions of subscriber behavior, a decision making component determines an inter-  vention strategy--whether a subscriber should be contacted, and if so, what incentives  should be offered to appease them. We adopt a decision-theoretic approach which aims to  maximize the expected profit to the carrier.  In the present work, we focus on chum prediction and utilize simple measures of  subscriber profitability and credit risk. However, current modeling efforts are directed at  more intelligent models of profitability and credit risk.  2 DATA SET  The subscriber data used for our experiments was provided by a major wireless car-  rier. The carder does not want to be identified, as churn rates are confidential. The carrier  provided a data base of 46,744 primarily business subscribers, all of whom had multiple  services. (Each service corresponds to a cellular telephone or to some other service, such  as voice messaging or beeper capability.) All subscribers were from the same region of the  United States, about 20% in major metropolitan areas and 80% more geographically dis-  tributed. The total revenue for all subscribers in the data base was $14 million in October  1998. The average revenue per subscriber was $234. We focused on multi-service sub-  scribers, because they provide significantly more revenue than do typical single-service  subscribers.  When subscribers are on extended contracts, churn prediction is relatively easy: it  seldom occurs during the contract period, and often occurs when the contract comes to an  end. Consequently, all subscribers in our data base were month-to-month, requiring the  use of more subtle features than contract termination date to anticipate churn.  The subscriber data was extracted from the time interval October through December,  1998. Based on these data, the task was to predict whether a subscriber would churn in  January or February 1999. The carder provided their internal definition of churn, which  was based on the closing of all services held by a subscriber. From this definition, 2,876 of  the subscribers active in October through December churned--6.2% of the data base.  subscriber__  '! subscriber  churn  prediction  . subscriber  profitability  estimation   subscriber  credit risk  estimation  decision  making   intervention  strategy  FIGURE 1. The framework for churn prediction and profitability maximization  Churn Reduction in the Wireless Industry 93 7  2.1 INPUT FEATURES  Ultimately, churn occurs because subscribers are dissatisfied with the price or quality of  service, usually as compared to a competing carrier. The main reasons for subscriber dis-  satisfaction vary by region and over time. Table 1 lists important factors that influence  subscriber satisfaction, as well as the relative importance of the factors (J. D. Power and  Associates, 1998). In the third column, we list the type of information required for deter-  mining whether a particular factor is likely to be influencing a subscriber. We categorize  the types of information as follows.  Network. Call detail records (date, time, duration, and location of all calls), dropped  calls (calls lost due to lack of coverage or available bandwidth), and quality of ser-  vice data (interference, poor coverage).  Billing. Financial information appearing on a subscriber's bill (monthly fee, addi-  tional charges for roaming and additional minutes beyond monthly prepaid limit).  Customer Service. Calls to the customer service department and their resolutions.  Application for Service. Information from the initial application for service, includ-  ing contract details, rate plan, handset type, and credit report.  Market. Details of rate plans offered by carrier and its competitors, recent entry of  competitors into market, advertising campaigns, etc.  Demographics. Geographic and population data of a given region.  A subset of these information sources were used in the present study. Most notably, we did  not utilize market information, because the study was conducted over a fairly short time  interval during which the market did not change significantly. More important, the market  forces were fairly uniform in the various geographic regions from which our subscribers  were selected. Also, we were unable to obtain information about the subscriber equipment  (age and type of handset used).  The information sources listed above were distributed over three distinct data bases  maintained by the carrier. The data bases contained thousands of fields, from which we  identified 134 variables associated with each subscriber which we conjectured might be  linked to churn. The variables included: subscriber location, credit classification, customer  classification (e.g., corporate versus retail), number of active services of various types,  beginning and termination dates of various services, avenue through which services were  activated, monthly charges and usage, number, dates and nature of customer service calls,  number of calls made, and number of abnormally terminated calls.  2.2 DATA REPRESENTATION  As all statisticians and artificial intelligence researchers appreciate, representation is key.  A significant portion of our effort involved working with domain experts in the wireless  telecommunications industry to develop a representation of the data that highlights and  makes explicit those features which--in the expert's judgement--were highly related to  churn. To evaluate the benefit of carefully constructing the representation, we performed  TABLE 1. Factors influencing subscriber satisfaction  Factor Importance Nature of data required for prediction  call quality 21% network  pricing options 18% market, billing  corporate capability 17% market, customer service  customer service 17% customer service  credibility / customer communications 10% market, customer service  "roaming / coverage 7% network  handset 4% application  billing 3% billing  cost of roaming 3% market, billing  938 M. C. Mozer, R. Wolniewicz, D. B. Grimes, E. Johnson and H. Kaushansky  studies using both naive and a sophisticated representations.  The naive representation mapped the 134 variables to a vector of 148 elements in a  straightforward manner. Numerical variables, such as the length of time a subscriber had  been with the carrier, were translated to an element of the representational vector which  was linearly related to the variable value. We imposed lower and upper limits on the vari-  ables, so as to suppress irrelevant variation and so as not to mask relevant variation by too  large a dynamic range; vector elements were restricted to lie between -4 and +4 standard  deviations of the variable. One-of-n discrete variables, such as credit classification, were  translated into an n-dimensional subvector with one nonzero element.  The sophisticated representation incorporated the domain knowledge of our experts  to produce a 73-element vector encoding attributes of the subscriber. This representation  collapsed across some of the variables which, in the judgement of the experts, could be  lumped together (e.g., different types of calls to the customer service department), and  expanded on others (e.g., translating the scalar length-of-time-with-carrier to a multidi-  mensional basis-function representation, where the receptive-field centers of the basis  functions were suggested by the domain experts), and performed transformations of other  variables (e.g., ratios of two variables, or time-series regression parameters).  3 PREDICTORS  The task is to predict the probability of churn from the vector encoding attributes of the  subscriber. We compared the churn-prediction performance of two classes of models: logit  regression and a nonlinear neural network with a single hidden layer and weight decay  (Bishop, 1995). The neural network model class was parameterized by the number of units  in the hidden layer and the weight decay coefficient. We originally anticipated that we  would require some model selection procedure, but it turned out that the results were  remarkably insensitive to the choice of the two neural network parameters; weight decay  up to a point seemed to have little effect, and beyond that point it was harmful, and varying  the number of hidden units from 5 to 40 yielded nearly identical performance. We likely  were not in a situation where overfitting was an issue, due to the large quantity of data  available; hence increasing the model complexity (either by increasing the number of hid-  den units or decreasing weight decay) had little cost.  Rather than selecting a single neural network model, we averaged the predictions of  an ensemble of models which varied in the two model parameters. The average was uni-  formly weighted.  4 METHODOLOGY  We constructed four predictors by combining each of the two model classes (logit regres-  sion and neural network) with each of the two subscriber representations (naive and  sophisticated). For each predictor, we performed a ten-fold cross validation study, utilizing  the same splits across predictors. In each split of the data, the ratio of churn to no chum  examples in the training and validation sets was the same as in the overall data set.  For the neural net models, the input variables were centered by subtracting the means  and scaled by dividing by their standard deviation. Input values were restricted to lie in the  range [-4, +4]. Networks were trained until they reached a local minimum in error.  5 RESULTS AND DISCUSSION  5.1 CHURN PREDICTION  For each of the four predictors, we obtain a predicted probability of churn for each sub-  scriber in the data set by merging the test sets from the ten data splits. Because decision  making ultimately requires a "chum" or "no churn" prediction, the continuous probability  measure must be thresholded to obtain a discrete predicted outcome.  Chum Reduction in the Wireless Industry 939  For a given threshold, we determine the proportion of churners who are correctly  identified as churners (the hit rate), and the proportion of nonchurners who are correctly  identified as nonchurners (the rejection rate). Plotting the hit rate against the rejection rate  for various thresholds, we obtain an ROC curve (Green & Swets, 1966). In Figure 2, the  closer a curve comes to the upper right comer of the graph--100% correct prediction of  churn and 100% correct prediction of nonchurn the better is the predictor at discriminat-  ing churn from nonchum. The dotted diagonal line indicates no discriminability: If a pre-  dictor randomly classifies x% of cases as chum, it is expected to obtain a hit rate of x%  and a rejection rate of (100-x)%.  As the Figure indicates, discriminability is clearly higher for the sophisticated repre-  sentation than for the naive representation. Further, for the sophisticated representation at  least, the nonlinear neural net outperforms the logit regression. It appears that the neural  net can better exploit nonlinear structure in the sophisticated representation than in the  naive representation, perhaps due to the basis-function representation of key variables.  Although the four predictors appear to yield similar curves, they produce large differences  in estimated cost savings. We describe how we estimate cost savings next.  5.2 DECISION MAKING  Based on a subscriber's predicted chum probability, we must decide whether to offer the  subscriber some incentive to remain with the carrier, which will presumably reduce the  likelihood of churn. The incentive will be offered to any subscriber whose churn probabil-  ity is above a certain threshold. The threshold will be selected to maximize the expected  cost savings to the carrier; we will refer to this as the optimal decision-making policy.  The cost savings will depend not only on the discriminative ability of the predictor,  but also on: the cost to the carrier of providing the incentive, denoted C i (the cost to the  carrier may be much lower than the value to the subscriber, e.g., when air time is offered);  the time horizon over which the incentive has an effect on the subscriber's behavior; the  reduction in probability that the subscriber will leave within the time horizon as a result of  the incentive, Pi; and the lost-revenue cost that results when a subscriber chums, C I.  lOO  90   80  0   ,-, 70  - 80   0  ._ 40  - 30  0  - 20  10  neural net / sophisticated  logit regression/sophisticated  neural net / naive  logit regression / naive  o  o 1 o 20 30 40 50 60 70 80 90 1 oo  % churn identified (hit rate)  FIGURE 2. Test-set performance for the four predictors. Each curve shows, for various  thresholds, the ability of a predictor to correctly identify churn (x axis) and nonchurn (y axis).  The more bowed a curve, the better able a predictor is at discriminating churn from  nonchurn.  940 M. C. Mozer, R. Wolniewicz, D. B. Grimes, E. Johnson and H. Kaushansky  We assume a time horizon of six months. We also assume that the lost revenue as a  result of churn is the average subscriber bill over the time horizon, along with a fixed cost  of $500 to acquire a replacement subscriber. (This acquisition cost is higher than the typi-  cal cost we stated earlier because subscribers in this data base are high valued, and often  must be replaced with multiple low-value subscribers to achieve the same revenue.) To  estimate cost savings, the parameters C i, Pi, and C 1 are combined with four statistics  obtained from a predictor:  N(pL, aL): number of subscribers who are predicted to leave (churn) and who actu-  ally leave barring intervention  N(pS, aL): number of subscribers who are predicted to stay (nonchurn) and who  actually leave barring intervention  N(pL, aS): number of subscribers who are predicted to leave and who actually stay  N(pS, aS): number of subscribers who are predicted to stay and who actually stay  Given these statistics, the net cost to the carrier of performing no intervention is:  net(no intervention) = [ N(pL, aL) + N(pS, aL) ] C!  This equation says that whether or not churn is predicted, the subscriber will leave, and the  cost per subscriber will be C I. The net cost of providing an incentive to all subscribers  whom are predicted to chum can also be estimated:  net(incentive) = [ N(pL, aL) + N(pL, aS) ] C i + [ Pi N(pL, aL) + N(pS, aL) l C!  This equation says that the cost of offering the incentive, C i, is incurred for all subscribers  for who are predicted to churn, but the lost revenue cost will decrease by a fraction Pi for  the subscribers who are correctly predicted to churn. The savings to the carrier as a result  of offering incentives based on the churn predictor is then  savings per chumable subscriber =  [ net(no intervention) - net(incentive) ] / [ N(pL, aL) + N(pS, aL) ]  The contour plots in Figure 3 show expected savings per churnable subscriber, for a  range of values of C i, Pi, and C 1, based on the optimal policy and the sophisticated neural-  net predictor. Each plot assumes a different subscriber retention rate (= 1-Pi) given inter-  vention. The "25% retention rate" graph supposes that 25% of the churning subscribers  who are offered an incentive will decide to remain with the carder over the time horizon of  six months. For each plot, the cost of intervention (Ci) is varied along the x-axis, and the  average monthly bill is varied along the y-axis. (The average monthly bill is converted to  lost revenue, C!, by computing the total bill within the time horizon and adding the sub-  scriber acquisition cost.) The shading of a region in the plot indicates the expected savings  assuming the specified retention rate is achieved by offering the incentive. The grey-level  bar to the right of each plot translates the shading into dollar savings per subscriber who  will churn barring intervention. Because the cost of the incentive is factored into the sav-  ings estimate, the estimate is actually the net return to the carder.  The white region in the lower right portion of each graph is the region in which no  cost savings will be obtained. As the graphs clearly show, if the cost of the incentive  needed to achieve a certain retention rate is low and the cost of lost revenue is high, signif-  icant per-subscriber savings can be obtained.  As one might suspect in examining the plots, what's important for determining per-  subscriber savings is the ratio of the incentive cost to the average monthly bill. The plots  clearly show that for a wide range of assumptions concerning the average monthly bill,  incentive cost, and retention rate, a significant cost savings is realized.  The plots assume that all subscribers identified by the predictor can be contacted and  offered the incentive. If only some fraction F of all subscribers are contacted, then the esti-  mated savings indicated by the plot should be multiplied by F.  To pin down a likely scenario, it is reasonable to assume that 50% of subscribers can  be contacted, 35% of whom will be retained by offering an incentive that costs the carrier  Chum Reduction in the Wireless Industry 941  250  v200  '150  E100  50  25% retention rate  50 100 150  intervention cost ($)  40O  350  300  250  200  150  100  50  200  250  :>'150  o  E100  50  35% retention rate  50 100 150 200  intervention cost ($)  6OO  00  [00  3OO  2OO  100  25O  o  E100  50  50% retention rate  9O0  800 250  700 g 20O  600 --  500 ' 150  400 c  o  300  100  200   50  100  0  75% retention rate  0 /  50 100 150 200 50 100 150 200  intervention cost ($) intervention cost ($)  1400  1200  1000  80O  60O  400  200  FIGURE 3. Expected savings to the carrier per churnable subscriber, under a variety of  assumptions concerning intervention cost, average monthly bill of subscriber, and retention  rate that will be achieved by offering an incentive to a churnable subscriber.  $75, and in our data base, the average monthly bill is $234. Under this scenario, the  expected savings ...above and beyond recovering the incentive cost--to the carder is $93  based on the sophisticated neural net predictor. In contrast, the expected savings is only  $47 based on the naive neural net predictor, and $81 based on the sophisticated logistic  regression model. As we originally conjectured, both the nonlinearity of the neural net and  the bias provided by the sophisticated representation are adding value to the predictions.  Our ongoing research involves extending these initial results in a several directions.  First, we have confirmed our positive results with data from a different time window, and  for test data from a later time window than the training data (as would be necessary in  real-world usage). Second, we have further tuned and augmented our sophisticated repre-  sentation to obtain higher prediction accuracy, and are now awaiting additional data to  ensure the result replicates. Third, we are applying a variety of techniques, including sen-  sitivity analysis and committee and boosting techniques, to further improve prediction  accuracy. And fourth, we have begun to explore the consequences of iterating the decision  making process and evaluating savings over an extended time period. Regardless of these  directions for future work, the results presented here show the promise of data mining in  the domain of wireless telecommunications. As is often the case for decision-making sys-  tems, the predictor need not be a perfect discriminator to realize significant savings.  6 REFERENCES  Bishop, C. (1995). Neural networks for pattern recognition. New York: Oxford University Press.  Fowlkes, A. J., Madan, A., Andrew, J., and Jensen, C.(1999). The effect of chum on value: An  industry advisory.  Green, D. M., & Swets, J. A. (1966). Signal detection theory andpsychophysics. New York: Wiley.  Luna, L. (1998). Chum is epidemic. Radio Communications Report, December 14, 1998.  Power, J. D., & Associates (1998). 1998 Residential Wireless Customer Satisfaction Survey. Septem-  ber 22, 1998.  
Large Margin DAGs for  Multiclass Classification  John C. Platt  Microsoft Research  1 Microsoft Way  Redmond, WA 98052  jplatt @ microsoft. corn  Nello Cristianini  Dept. of Engineering Mathematics  University of Bristol  Bristol, BS8 1TR - UK  hello. c ristian in i @ b risto l. ac. uk  John Shawe-Taylor  Department of Computer Science  Royal Holloway College - University of London  EGHAM, Surrey, TW20 0EX - UK  j. shawe-taylor@dcs. rhbnc. ac. uk  Abstract  We present a new learning architecture: the Decision Directed Acyclic  Graph (DDAG), which is used to combine many two-class classifiers  into a multiclass classifier. For an N-class problem, the DDAG con-  tains N(N - 1)/2 classifiers, one for each pair of classes. We present a  VC analysis of the case when the node classifiers are hyperplanes; the re-  sulting bound on the test error depends on N and on the margin achieved  at the nodes, but not on the dimension of the space. This motivates an  algorithm, DAGSVM, which operates in a kernel-induced feature space  and uses two-class maximal margin hyperplanes at each decision-node  of the DDAG. The DAGSVM is substantially faster to train and evalu-  ate than either the standard algorithm or Max Wins, while maintaining  comparable accuracy to both of these algorithms.  1 Introduction  The problem of multiclass classification, especially for systems like SVMs, doesn't present  an easy solution. It is generally simpler to construct classifier theory and algorithms for two  mutually-exclusive classes than for N mutually-exclusive classes. We believe constructing  N-class SVMs is still an unsolved research pr. oblem.  The standard method for N-class SVMs [ 10] is to construct N SVMs. The ith SVM will be  trained with all of the examples in the ith class with positive labels, and all other examples  with negative labels. We refer to SVMs trained in this way as l ov-r SVMs (short for one-  versus-rest). The final output of the N 1-v-r SVMs is the class that corresponds to the SVM  with the highest output value. Unfortunately, there is no bound on the generalization error  for the 1-v-r SVM, and the training time of the standard method scales linearly with N.  Another method for constructing N-class classifiers from SVMs is derived from previous  research into combining two-class classifiers. Knerr [5] suggested constructing all possible  two-class classifiers from a training set of N classes, each classifier being trained on only  548 J. C. Platt, N. Cristianini and J. Shawe-Taylor  two out of N classes. There would thus be K = N(N - 1)/2 classifiers. When applied to  SVMs, we refer to this as l-v-1 SVMs (short for one-versus-one).  Knerr suggested combining these two-class classifiers with an "AND" gate [5]. Fried-  man [4] suggested a Max Wins algorithm: each l-v-1 classifier casts one vote for its pre-  ferred class, and the final result is the class with the most votes. Friedman shows cir-  cumstances in which this algorithm is Bayes optimal. Krefel [6] applies the Max Wins  algorithm to Support Vector Machines with excellent results.  A significant disadvantage of the l-v-1 approach, however, is that, unless the individual  classifiers are carefully regularized (as in SVMs), the overall N-class classifier system will  tend to overfit. The "AND" combination method and the Max Wins combination method  do not have bounds on the generalization error. Finally, the size of the l-v-1 classifier may  grow superlinearly with N, and hence, may be slow to evaluate on large problems.  In Section 2, we introduce a new multiclass learning architecture, called the Decision Di-  rected Acyclic Graph (DDAG). The DDAG contains N(N - 1)/2 nodes, each with an  associated l-v-1 classifier. In Section 3, we present a VC analysis of DDAGs whose clas-  sifiers are hyperplanes, showing that the margins achieved at the decision nodes and the  size of the graph both affect their performance, while the dimensionality of the input space  does not. The VC analysis indicates that building large margin DAGs in high-dimensional  feature spaces can yield good generalization performance. Using such bound as a guide,  in Section 4, we introduce a novel algorithm for multiclass classification based on placing  l-v-1 SVMs into nodes of a DDAG. This algorithm, called DAGSVM, is efficient to train  and evaluate. Empirical evidence of this efficiency is shown in Section 5.  2 Decision DAGs  A Directed Acyclic Graph (DAG) is a graph whose edges have an orientation and no cycles.  A Rooted DAG has a unique node such that it is the only node which has no arcs pointing  into it. A Rooted Binary DAG has nodes which have either 0 or 2 arcs leaving them.  We will use Rooted Binary DAGs in order to define a class of functions to be used in  classification tasks. The class of functions computed by Rooted Binary DAGs is formally  defined as follows.  Definition 1 Decision DAGs (DDAGs). Given a space X and a set of boolean functions  f. = {f: X --+ {0, 1}}, the class DDAG(f.) of Decision DAGs on N classes over .7' are  functions which can be implemented using a rooted binary DAG with N leaves labeled by  the classes where each of the K = N(N - 1)/2 internal nodes is labeled with an element  off'. The nodes are arranged in a triangle with the single root node at the top, two nodes  in the second layer and so on until the final layer of N leaves. The i-th node in layer j < N  is connected to the i-th and (i + 1)-st node in the (j + 1)-st layer.  To evaluate a particular DDAG G on input a: E X, starting at the root node, the binary  function at a node is evaluated. The node is then exited via the left edge, if the binary  function is zero; or the right edge, if the binary function is one. The next node's binary  function is then evaluated. The value of the decision function D(a:) is the value associated  with the final leaf node (see Figure l(a)). The path taken through the DDAG is known  as the evaluation path. The input a: reaches a node of the graph, if that node is on the  evaluation path for a:. We refer to the decision node distinguishing classes i and j as the  /j-node. Assuming that the number of a leaf is'its class, this node is the i-th node in the  (N - j + i)-th layer provided i < j. Similarly the j-nodes are those nodes involving class  j, that is, the internal nodes on the two diagonals containing the leaf labeled by j.  The DDAG is equivalent to operating on a list, where each node eliminates one class from  the list. The list is initialized with a list of all classes. A test point is evaluated against the  decision node that corresponds to the first and last elements of the list. If the node prefers  Large Margin DAGs for Multiclass Classification 549  4 3 2  2 .4 4  .2_ 22 :4'4 test points on this I   _2 side of hyperplane   cannot be in class 1  3  1 1 vs 4 SVM  1  1 1 1  1 1 1 test points on this  side of hyperptane  cannot be n class 4  (a) (b)  Figure 1' (a) The decision DAG for finding the best class out of four classes. The equivalent  list state for each node is shown next to that node. (b) A diagram of the input space of a  four-class problem. A l-v-1 SVM can only exclude one class from consideration.  one of the two classes, the other class is eliminated from the list, and the DDAG proceeds  to test the first and last elements of the new list. The DDAG terminates when only one  class remains in the list. Thus, for a problem with N classes, N - 1 decision nodes will be  evaluated in order to derive an answer.  The current state of the list is the total state of the system. Therefore, since a list state  is reachable in more than one possible path through the system, the decision graph the  algorithm traverses is a DAG, not simply a tree.  Decision DAGs naturally generalize the class of Decision Trees, allowing for a more ef-  ficient representation of redundancies and repetitions that can occur in different branches  of the tree, by allowing the merging of different decision paths. The class of functions  implemented is the same as that of Generalized Decision Trees [ 1], but this particular rep-  resentation presents both computational and learning-theoretical advantages.  3 Analysis of Generalization  In this paper we study DDAGs where the node-classifiers are hyperplanes. We define a  Perceptron DDAG to be a DDAG with a perceptron at every node. Let w be the (unit)  weight vector correctly splitting the i and j classes at the/j-node with threshold 0. We  define the margin of the/j-node to be ? = rninc(:)=i,j {[(w, a:} - 01}, where c(a:) is the  class associated to training example a:. Note that, in this definition, we only take into  account examples with class labels equal to i or j.  Theorem 1 Suppose we are able to classify a random ra sample of labeled examples using  a Perceptron DDAG on N classes containing K decision nodes with margins ?i at node i,  then we can bound the generalization error with probability greater than 1 - 6 to be less  than  130R2 (D' log(4ern)log(4rn) + log 2(2 )K),  m  where D/ K 1  -- 5-i= , and t is the radius of a ball containing the distribution's support.  Proof: see Appendix []  550 J. C. Platt, N. Cristianini and J. Shawe-Taylor  Theorem 1 implies that we can control the capacity of DDAGs by enlarging their margin.  Note that, in some situations, this bound may be pessimistic: the DDAG partitions the  input space into polytopic regions, each of which is mapped to a leaf node and assigned  to a specific class. Intuitively, the only margins that should matter are the ones relative to  the boundaries of the cell where a given training point is assigned, whereas the bound in  Theorem 1 depends on all the margins in the graph.  By the above observations, we would expect that a DDAG whose j-node margins are large  would be accurate at identifying class j, even when other nodes do not have large margins.  Theorem 2 substantiates this by showing that the appropriate bound depends only on the  j-node margins, but first we introduce the notation, ej (G) = P{x: (x in class j and x is  misclassified by G) or x is misclassified as class j by G}.  Theorem 2 Suppose we are able to correctly distinguish class j from the other classes in  a random m-sample with a DDAG G over N classes containing K decision nodes with  margins  ( 2(2m)V-)  ej(G) ( 130R' D  log(4em) log(4m) + log  where D  ij-nodes -- ' and R is the radius of a ball containing the support of the  distribution.  Proof: follows exactly Lemma 4 and Theorem 1, but is omitted. C3  4 The DAGSVM algorithm  Based on the previous analysis, we propose a new algorithm, called the Directed Acyclic  Graph SVM (DAGSVM) algorithm, which combines the results of l-v-1 SVMs. We will  show that this combination method is efficient to train and evaluate.  The analysis of Section 3 indicates that maximizing the margin of all of the nodes in a  DDAG will minimize a bound on the generalization error. This bound is also independent  of input dimensionality. Therefore, we will create a DDAG whose nodes are maximum  margin classifiers over a kernel-induced feature space. Such a DDAG is obtained by train-  ing each/j-node only on the subset of training points labeled by i or j. The final class  decision is derived by using the DDAG architecture, described in Section 2.  The DAGSVM separates the individual classes with large margin. It is safe to discard the  losing class at each l-v- 1 decision because, for the hard margin case, all of the examples  of the losing class are far away from the decision surface (see Figure 1 (b)).  For the DAGSVM, the choice of the class order in the list (or DDAG) is arbitrary. The ex-  periments in Section 5 simply use a list of classes in the natural numerical (or alphabetical)  order. Limited experimentation with re-ordering the list did not yield significant changes  in accuracy performance.  The DAGSVM algorithm is superior to other multiclass SVM algorithms in both training  and evaluation time. Empirically, SVM training is observed to scale super-linearly with the  training set size m [7], according to a power law: T = crn', where "r  2 for algorithms  based on the decomposition method, with some proportionality constant c. For the standard  1-v-r multiclass SVM training algorithm, the entire training set is used to create all N  classifiers. Hence the training time for 1 -v-r is  T1 -v-r '- cNm '  ( 1 )  Assuming that the classes have the same number of examples, training each l-v-1 SVM  only requires 2m/N training examples. Thus, training K l-v-1 SVMs would require  T-v- = c  2'- cN'-'m '. (2)  Large Margin DAGs for Multiclass Classification 551  For a typical case, where 3' = 2, the amount of time required to train all of the l-v- 1 SVMs  is independent of N, and is only twice that of training a single 1-v-r SVM. Using l-v-1  SVMs with a combination algorithm is thus preferred for training time.  5 Empirical Comparisons and Conclusions  The DAGSVM algorithm was evaluated on three different test sets: the USPS handwritten  digit data set [10], the UCI Letter data set [2], and the UCI Covertype data set [2]. The  USPS digit data consists of 10 classes (0-9), whose inputs are pixels of a scaled input  image. There are 7291 training examples and 2007 test examples. The UCI Letter data  consists of 26 classes (A-Z), whose inputs are measured statistics of printed font glyphs.  We used the first 16000 examples for training, and the last 4000 for testing. All inputs of  the UCI Letter data set were scaled to lie in [-1, 1]. The UCI Covertype data consists of  7 classes of trees, where the inputs are terrain features. There are 11340 training examples  and 565893 test examples. All of the continuous inputs for Covertype were scaled to have  zero mean and unit variance. Discrete inputs were represented as a 1-of-n code.  On each data set, we trained N 1-v-r SVMs and K l-v-1 SVMs, using SMO [7], with  soft margins. We combined the l-v-1 SVMs both with the Max Wins algorithm and with  DAGSVM. The choice of kernel and of the regularizing parameter C was determined via  performance on a validation set. The validation performance was measured by training  on 70% of the training set and testing the combination algorithm on 30% of the training  set (except for Covertype, where the UCI validation set was used). The best kernel was  selected from a set of polynomial kernels (from degree 1 through 6), both homogeneous and  inhomogeneous; and Gaussian kernels, with various a. The Gaussian kernel was always  found to be best.  a C' Error Kernel Training CPU Classifier Size  Rate (%) Evaluations Time (sec) (Kparameters)  USPS  1-v-r 3.58 100 4.7 2936 3532 760  Max Wins 5.06 100 4.5 1877 307 487  DAGSVM 5.06 100 4.4 819 307 487  Neural Net [10] 5.9  UCI Letter  1-v-r 0.447 100 2.2 8183 1764 148  Max Wins 0.632 100 2.4 7357 441 160  DAGSVM 0.447 10 2.2 3834 792 223  Neural Net 4.3  UCI Covertype  1-v-r 1 10 30.2 7366 4210 105  Max Wins 1 10 29.0 7238 1305 107  DAGSYM 1 10 29.2 4390 1305 107  Neural Net [2] 30  Table 1: Experimental Results  Table 1 shows the results of the experiments. The optimal parameters for all three multi-  class SVM algorithms are very similar for both data sets. Also, the error rates are similar  for all three algorithms for both data sets. Neither 1-v-r nor Max Wins is statistically sig-  nificantly better than DAGSVM using McNemar's test [3] at a 0.05 significance level for  USPS or UCI Letter. For UCI Covertype, Max Wins is slightly better than either of the  other SVM-based algorithms. The results for a neural network trained on the same data  sets are shown for a baseline accuracy comparison.  The three algorithms distinguish themselves in training time, evaluation time, and classifier  size. The number of kernel evaluations is a good indication of evaluation time. For l-v-  552 J. C. Platt, N. Cristianini and J. Shawe-Taylor  r and Max Wins, the number of kernel evaluations is the total number of unique support  vectors for all SVMs. For the DAGSVM, the number of kernel evaluations is the number  of unique support vectors averaged over the evaluation paths through the DDAG taken by  the test set. As can be seen in Table 1, Max Wins is faster than 1-v-r SVMs, due to shared  support vectors between the l-v-1 classifiers. The DAGSVM has the fastest evaluation.  The DAGSVM is between a factor of 1.6 and 2.3 times faster to evaluate than Max Wins.  The DAGSVM algorithm is also substantially faster to train than the standard 1-v-r SVM  algorithm: a factor of 2.2 and 11.5 times faster for these two data sets. The Max Wins  algorithm shares a similar training speed advantage.  Because the SVM basis functions are drawn from a limited set, they can be shared across  classifiers for a great savings in classifier size. The number of parameters for DAGSVM  (and Max Wins) is comparable to the number of parameters for 1-v-r SVM, even though  there are N(N - 1)/2 classifiers, rather than N.  In summary, we have created a Decision DAG architecture, which is amenable to a VC-  style bound of generalization error. Using this bound, we created the DAGSVM algorithm,  which places a two-class SVM at every node of the DDAG. The DAGSVM algorithm  was tested versus the standard 1-v-r multiclass SVM algorithm, and Friedman's Max Wins  combination algorithm. The DAGSVM algorithm yields comparable accuracy and memory  usage to the other two algorithms, but yields substantial improvements in both training and  evaluation time.  6 Appendix: Proof of Main Theorem  Definition 2 Let f' be a set of real valued functions. We say that a set of points X is ?-  shattered by f' relative to r - (rx)xeX, if there are real numbers rx, indexed by x E X,  such that for all binary vectors b indexed by X, there is a function fb  f' satisfying  (2bx - 1)lb(x) _> (2bx - 1)rx + 3'- The fat shattering dimension, fatr, of the set f' is a  function from the positive real numbers to the integers which maps a value 3' to the size of  the largest 7-shattered set, if the set is finite, or maps to infinity otherwise.  As a relevant example, consider the class &in = {x - {w,x ) - 0: Ilwll = 1). we quote  the following result from [1].  Theorem 3 Let f'lin be restricted  origin. Then  fatY'li n  to points in a ball of n dimensions of radius R about the  (3') _< min{R2/3'2, n + 1}.  We will bound generalization with a technique that closely resembles the technique used  in [1] to study Perceptron Decision Trees. We will now give a lemma and a theorem: the  lemma bounds the probability over a double sample that the first half has zero error and the  second error greater than an appropriate e. We assume that the DDAG on N classes has  K = N(N - 1)/2 nodes and we denote fatYli n (3') by fat(3').  Lemma 4 Let G be a DDAG on N classes with K -- N(N - 1)/2 decision nodes with  margins 3', 3'2,..., 3'K at the decision nodes satisfying ki = fat(3'i/8), where fat is con-  tinuous from the right. Then the following bound holds, p2m{xy: 3 a graph G : G  which separates classes i and j at the ij-node for all x in x, a fraction of points mis-  (D 10g(8m) + log 2____) and  classified in y > e(m,K, 5).} < 5 where e(m,K,) =   D ---- EK=i k i log(4em/ki).  Proof The proof of Lemma 4 is omitted for space reasons, but is formally analogous to the  proof of Lemma 4.4 in [8], and can easily be reconstructed from it. t2  Large Margin DAGs for Multiclass Classification 553  Lemma 4 applies to a particular DDAG with a specified margin 7i at each node. In practice,  we observe these quantities after generating the DDAG. Hence, to obtain a bound that can  be applied in practice, we must bound the probabilities uniformly over all of the possible  margins that can arise. We can now give the proof for Theorem 1.  Proof of Main Theorem: We must bound the probabilities over different margins. We first  use a standard result due to Vapnik [9, page 168] to bound the probability of error in terms  of the probability of the discrepancy between the performance on two halves of a double  sample. Then we combine this result with Lemma 4. We must consider all possible patterns  of ki's over the decision nodes. The largest allowed value of ki is m, and so, for fixed K,  we can bound the number of possibilities by m K. Hence, there are m K of applications of  Lemma 4 for a fixed N. Since K = N(N - 1)/2, we can let 6k = (5/m K, so that the sum  m  5-t= (St = (5. Choosing  6 65/2 D' log(4em) log(4m) + log (3)  e m,K,-- =   in the applications of Lemma 4 ensures that the probability of any of the statements failing  to hold is less than (5/2. Note that we have replaced the constant 82 = 64 by 65 in order  to ensure the continuity from the right required for the application of Lemma 4 and have  upper bounded log(4em/ki) by log(4em). Applying Vapnik's Lemma [9, page 168] in  each case, the probability that the statement of the theorem fails to hold is less than (5. [21  More details on this style of proof, omitted in this paper for space constraints, can be  found in [1].  References  [1] K. Bennett, N. Cristianini, J. Shawe-Taylor, and D. Wu. Enlarging the margin in perceptron  decision trees. Machine Learning (submitted). http://lara.enm.bris.ac.uk/cig/pubs/ML-PDTps.  [2] C. Blake, E. Keogh, and C. Merz. UCI repository of machine learning databases. Dept. of  information and computer sciences, University of California, Irvine, 1998.  http://www. ics.uci.edu/mlearn/MLRepository. html.  [3] T.G. Dietterich. Approximate statistical tests for comparing supervised classification learning  algorithms. Neural Computation, 10:1895-1924, 1998.  [4] J. H. Friedman. Another approach to polychotomous classification. Technical report, Stanford  Department of Statistics, 1996. http://www-stat.stanford.edu/reports/friedman/poly. ps.Z.  [5] S. Knerr, L. Personnaz, and G. Dreyfus. Single-layer learning revisited: A stepwise procedure  for building and training a neural network. In Fogelman-Soulie and Herault, editors,  Neurocomputing: Algorithms, Architectures and Applications, NATO ASI. Springer, 1990.  [6] U. Kre13el. Pairwise classification and support vector machines. In B. Sch61kopf, C. J. C.  Burges, and A. J. Smola, editors, Advances in Kernel Methods: Support Vector Learning,  pages 255-268. MIT Press, Cambridge, MA, 1999.  [7] J. Platt. Fast training of support vector machines using sequential minimal optimization. In  B. Sch61kopf, C. J. C. Burges, and A. J. Smola, editors, Advances in Kernel Methods --  Support Vector Learning, pages 185-208. MIT Press, Cambridge, MA, 1999.  [8] J. Shawe-Taylor and N. Cristianini. Data dependent structural risk minimization for perceptron  decision trees. In M. Jordan, M. Keams, and S. Solla, editors, Advances in Neural Information  Processing Systems, volume 10, pages 336-342. MIT Press, 1999.  [9] V. Vapnik. Estimation of Dependences Based on Empirical Data [in Russian]. Nauka,  Moscow, 1979. (English translation: Springer Verlag, New York, 1982).  [10] V. Vapnik. Statistical Learning Theory. Wiley, New York, 1998.  
Bayesian Network Induction via Local  Neighborhoods  Dimitris Margaritis  Department of Computer Science  Carnegie Mellon University  Pittsburgh, PA 15213  D.M a rg a ri tis @ cs. cmu. edu  Sebastian Thrun  Department of Computer Science  Carnegie Mellon University  Pittsburgh, PA 15213  S. Thrun @ cs. cmu. edu  Abstract  In recent years, Bayesian networks have become highly successful tool for di-  agnosis, analysis, and decision making in real-world domains. We present an  efficient algorithm for learning Bayes networks from data. Our approach con-  structs Bayesian networks by first identifying each node's Markov blankets, then  connecting nodes in a maximally consistent way. In contrast to the majority of  work, which typically uses hill-climbing approaches that may produce dense and  causally incorrect nets, our approach yields much more compact causal networks  by heeding independencies in the data. Compact causal networks facilitate fast in-  ference and are also easier to understand. We prove that under mild assumptions,  our approach requires time polynomial in the size of the data and the number of  nodes. A randomized variant, also presented here, yields comparable results at  much higher speeds.  1 Introduction  A great number of scientific fields today benefit from being able to automatically estimate  the probability of certain quantities of interest that may be difficult or expensive to observe  directly. For example, a doctor may be interested in estimating the probability of heart  disease from indications of high blood pressure and other directly measurable quantities.  A computer vision system may benefit from a probability distribution of buildings based  on indicators of horizontal and vertical straight lines. Probability densities proliferate the  sciences today and advances in its estimation are likely to have a wide impact on many  different fields.  Bayesian networks are a succinct and efficient way to represent a joint probability distri-  bution among a set of variables. As such, they have been applied to fields such as those  mentioned [Herskovits90][Agosta88]. Besides their ability for density estimation, their  semantics lend them to what is sometimes loosely referred to as causal discovery, namely  directional relationships among quantities involved. It has been widely accepted that the  most parsimonious representation for a Bayesian net is one that closely represents the causal  independence relationships that may exist. For these reasons, there has been great interest  in automatically inducing the structure of Bayesian nets automatically from data, preferably  also preserving the independence relationships in the process.  Two research approaches have emerged. The first employs independence properties of  the underlying network that produced the data in order to discover parts of its structure.  This approach is mainly exemplified by the SGS and PC algorithms in [Spirtes93], as well  506 D. Margaritis and S. Thrun  Figure 1: On the left, an example of a Markov blanket of variable X is shown. The members of  the blanket are shown shaded. On the right, an example reconstruction of a 5 x 5 rectangular net of  branching factor 3 by the algorithm presented in this paper using 20000 samples. Indicated by dotted  lines are 3 directionality errors.  as for restricted classes such as trees [Chow68] and polytrees [Rebane87]. The second  approach is concerned more with data prediction, disregarding independencies in the data.  It is typically identified with a greedy hill-climbing or best-first beam search in the space  of legal structures, employing as a scoring function a form of data likelihood, sometimes  penalized for network complexity. The result is a local maximum score network structure  for representing the data, and is one of the more popular techniques used today.  This paper presents an approach that belongs in the first category. It addresses the two main  shortcomings of the prior work which, we believe, are preventing its use from becoming  more widespread. These two disadvantages are: exponential execution times, and proneness  to errors in dependence tests used. The former problem is addressed in this paper in two  ways. One is by identifying the local neighborhood of each variable in the Bayesian net  as a preprocessing step, in order to facilitate the recovery of the local structure around  each variable in polynomial time under the assumption of bounded neighborhood size. The  second, randomized version goes one step further, employing a user-specified number of  randomized tests (constant or logarithmic) in order to ascertain the same result with high  probability. The second disadvantage of this research approach, namely proneness to errors,  is also addressed by the randomized version, by using multiple data sets (if available) and  Bayesian accumulation of evidence.  2 The Grow-Shrink Markov Blanket Algorithm  The concept of the Markov blanket of a variable or a set of variables is central to this paper.  The concept itself is not new. For example, see [Pearl88]. It is surprising, however, how  little attention it has attracted for all its being a fundamental property of a Bayesian net.  What is new in this paper is the introduction of the explicit use of this idea to effectively  limit unnecessary computation, as well as a simple algorithm to compute it. The definition  of a Markov blanket is as follows: denoting V as the set of variables and X(XS Y as the  conditional dependence of X and Y given the set S, the Markov blanket B ) C_ V of  X C V is any set of variables such that for any Y C V - BL(X) - {X}, X BL(x) Y'  In other words, BL(X) completely shields variable X from any other variable in V. The  notion of a minimal Markov blanket, called a Markov boundary, is also introduced in  [Pearl88] and its uniqueness shown under certain conditions. The Markov boundary is  not unique in certain pathological situations, such as the equality of two variables. In  our following discussion we will assume that the conditions necessary for its existence and  uniqueness are satisfied and we will identify the Markov blanket with the Markov boundary,  using the notation B (X) for the blanket of variable X from now on. It is also illuminating  to mention that, in the Bayesian net framework, the Markov blanket of a node X is easily  identifiable from the graph: it consists of all parents, children and parents of children of  X. An example Markov blanket is shown in Fig. 1. Note that any of these nodes, say Y, is  dependent with X given B(X) - {Y}.  Bayesian Network Induction via Local Neighborhoods 507  1. S - 13.  2. While 3 Y C V - {X} such that Y +-'>S X, do S -- S U {Y}.  3. While 3 Y C S such that Y OS-{Y} X, do S <-- S - {Y}.  4. B(X) -- S.  [Growing phase]  [Shrinking phase]  Figure 2: The basic Markov blanket algorithm.  The algorithm for the recovery of the Markov blanket of X is shown in Fig. 2. The idea  behind step 2 is simple: as long as the Markov blanket property of X is violated (ie. there  exists a variable in V that is dependent on X), we add it to the current set S until there are  no more such variables. In this process however, there may be some variables that were  added to S that were really outside the blanket. Such variables would have been rendered  independent from X at a later point when "intervening" nodes of the underlying Bayesian  net were added to S. This observation necessitates step 3, which identifies and removes  those variables. The algorithm is efficient, requiring only O (n) conditional tests, making  its running time O(n IDI), where n = IVI and D is the set of examples. For a detailed  derivation of this bound as well as a formal proof of correctness, see [Margaritis99]. In  practice one may try to minimize the number of tests in step 3 by heuristically ordering the  variables in the loop of step 2, for example by ascending mutual information or probability  of dependence between X and Y (as computed using the X 2 test, see section 5).  3 Grow-Shrink (GS) Algorithm for Bayesian Net Induction  The recovery of the local structure around each node is greatly facilitated by the knowledge  of the nodes' Markov blankets. What would normally be a daunting task of employ-  ing dependence tests conditioned on an exponential number of subsets of large sets of  variables--even though most of their members may be irrelevant--can now be focused on  the Markov blankets of the nodes involved, making structure discovery much faster and  more reliable. We present below the plain version of the GS algorithm that utilizes blanket  information for inducing the structure of a Bayesian net. At a later point of this paper, we  will present a robust, randomized version that has the potential of being faster and more  reliable, as well as being able to operate in an "anytime" manner.  In the following N(X) represents the direct neighbors of X.  [ Compute Markov Blankets ]  For all X  V, compute the Markov blanket B (X).  [ Compute Graph Structure ]  For all X  V and Y  B (X), determine Y to be a direct neighbor of X if X and  Y are dependent given S for all S C_ T, where T is the smaller of B(X) - {Y} and  B(Y)- {X}.  [ Orient Edges ]  For all X E V and Y C N(X), orient Y --> X if there exists a variable Z   N(X) - N(Y) - {Y} such that Y and Z are dependent given S U {X} for all S C_ U,  where U is the smaller of B(Y) - {Z} and B(Z) - {Y}.  [ Remove Cycles ]  Do the following while there exist cycles in the graph:  1. Compute the set of edges C = {X --> Y such that X --> Y is part of a cycle}.  2. Remove the edge in C that is part of the greatest number of cycles, and put it in  R.  508 D. Margaritis and $. Thrun  [ Reverse Edges ]  Insert each edge from R in the graph, reversed.  [ Propagate Directions ]  For all X C V and Y C N(X) such that neither Y --> X nor X - Y, execute the  following rule until it no longer applies: If there exists a directed path from X to Y,  orient X --> Y.  In the algorithm description above, step 2 determines which of the members of the blanket  of each node are actually direct neighbors (parents and children). Assuming, without loss of  generality, that B (X) - {Y} is the smaller set, if any of the tests are successful in separating  (making independent) X from Y, the algorithm determines that there is no direct connection  between them. That would happen when the conditioning set S includes all parents of X  and no common children of X and Y. It is interesting to note that the motivation behind  selecting the smaller set to condition on stems not only from computational efficiency  but from reliability as well: a conditioning set S causes the data set to be split into 2181  partitions; smaller conditioning sets cause the data set to be split into larger partitions and  make dependence tests more reliable.  Step 3 exploits the fact that two variables that have a common descendant become dependent  when conditioning on a set that includes any such descendant. Since the direct neighbors  of X and Y are known from step 2, we can determine whether a direct neighbor Y is a  parent of X if there exists another node Z (which, coincidentally, is also a parent) such  that any attempt to separate Y and Z by conditioning on a subset of the blanket of Y that  includes X, fails (assuming that B (Y) is smaller than B (Z)). If the directionality is indeed  Y - X  Z, there should be no such subset since, by conditioning on X, a permanent  dependency path between Y and Z is created. This would not be the case if Y were a child  of X.  It is straightforward to show that the algorithm requires O(n 2 + rib22 b) conditional inde-  pendence tests, where b = maxx(IB(X)l). Under the assumption that b is bounded by a  constant, this algorithm is O (n 2) in the number of conditional independence tests. It is  worthwhile to note that the time to compute a conditional independence test by a pass over  the data set D is O(n ID I) and not O(21Vl). An analysis and a formal proof of correctness  of the algorithm is presented in [Margaritis99].  Discussion  The main advantage of the algorithm comes through the use of Markov blankets to restrict  the size of the conditioning sets. The Markov blankets may be usually wrong in the side  of including too many nodes because they are represented by a disjunction of tests for  all values of the conditioning set, on the same data. This emphasizes the importance of  the "direct neighbors" step which removes nodes that were incorrectly added during the  Markov blanket computation step by admitting variables whose dependence was shown  high confidence in a large number of different tests.  It is also possible that an edge direction is wrongly determined during step 3 due to non-  representative or noisy data. This may lead to directed cycles in the resulting graph. It is  therefore necessary to remove those cycles by identifying the minimum set of edges than  need to be reversed for all cycles to disappear. This problem is closely related [Margaritis99]  to the Minimum Feedback Arc Set problem, which is concerned with identifying a minimum  set of edges that need to be removed from a graph that possibly contains directed cycles,  in order for all such cycles to disappear. Unfortunately, this problem is NP-complete in its  generality [Jtinger85]. We introduce here a reasonable heuristic for its solution that is based  on the number of cycles that an edge that is part of a cycle is involved in.  Not all edge directions can be determined during the last two steps. For example, nodes with  a single parent or multi-parent nodes (called colliders) whose parents are directly connected  do not apply to step 3, and steps 4 and 5 are only concerned with already directed edges.  Step 6 attempts to ameliorate that, through orienting edges in a way that does not introduce  Bayesian Network Induction via Local Neighborhoods 509  a cycle, if the reverse direction necessarily does. It is not obvious that, for example, if the  direction X  Y produces a cycle in an otherwise acyclic graph, the opposite direction  Y --+ X will not also. However, this is the case. For the proof of this, see [Margaritis99].  The algorithm is similar to the SGS algorithm presented in [Spirtes93], but differs in a  number of ways. Its main difference lies in the use of Markov blankets to dramatically  improve performance (in many cases where the bounded blanket size assumptions hold).  Its structure is similar to SGS, and the stability (frequently referred to as robustness in the  following discussion) arguments presented in [Spirtes93] apply. Increased reliability stems  from the use of smaller conditioning sets, leading to greater number of examples per test.  The PC algorithm, also in [Spirtes93], differs from the GS algorithm in that it involves  linear probing for a separator set, which makes it unnecessarily inefficient.  4 Randomized Version of the GS Algorithm  The GS algorithm, as presented above, is appropriate for situations where the maximum  Markov blanket of each of a set of variables is small. While it is reasonable to assume  that in many real-life problems where high-level variables are involved this may be the  case, other problems such as Bayesian image retrieval in computer vision, may employ  finer representations. In these cases the variables used may depend in a direct manner on  many others. For example, we may choose to use variables to characterize local texture in  different parts of an image. If the resolution of the mapping from textures to variables is  increasingly fine, direct dependencies among those variables may be plentiful and therefore  the maximum Markov blanket size may be significant.  Another problem that has plagued independence-test based algorithms for Bayesian net  structure induction in general is that their decisions are based on a single or a few tests  ("hard" decisions), making them prone to errors due to noise in the data. This also applies  to the the GS algorithm. It would therefore be advantageous to employ multiple tests before  deciding on a direct neighbor or the direction of an edge.  The randomized version of the GS algorithm addresses these two problems. Both of  them are tackled through randomized testing and Bayesian evidence accumulation. The  problem of exponential running times in the maximum blanket size of steps 2 and 3 of the  plain algorithm is overcome by replacing them by a series of tests, whose number may be  specified by the user, with the members of the conditioning set chosen randomly from the  smallest blanket of the two variables. Each such test provides evidence for or against the  direct connection between the two variables, appropriately weighted by the probability that  circumstances causing that event occur or not, and due to the fact that connectedness is the  conjunction of more elementary events.  This version of the algorithm is not shown here in detail due to space restrictions. Its  operation follows closely the one of the plain GS version. The main difference lies in the  usage of Bayesian updating of the posterior probability of a direct link (or a dependence  through a collider) between a pair of variables X and Y using conditional dependence tests  that take into account independent evidence. The posterior probability pi of a link between  X and Y after executing i dependence tests dj, j = 1,..., i is  Pi- 1 di  Pi =  pi-ldi q- (1 - Pi-1)(G q- 1 - di)  where G -- G(X, Y) = 1 - ()lTI is a factor that takes values in the interval [0, 1) and  can be interpreted as the "(un)importance" of the truth of each test di, while T is the  smaller of B(X) - {Y} and B(Y) - {X}. We can use this accumulated evidence to guide  our decisions to the hypothesis that we feel most confident about. Besides being able to  do that in a timely manner due to the user-specified number of tests, we also note how  this approach also addresses the robustness problem mentioned above through the use of  multiple weighted tests, and leaving for the end the "hard" decisions that involve a threshold  (ie. comparing the posterior probability with a threshold, which in our case is -}).  510 D. Margaritis and $. Thrun  150  125  100  75  50  25  0  0  0.00015  0001  5e-05  0  0  Edge errors versus number of samples  Plain GBN   Randomized GSBN -------  Hill-Climbing, score data likelihood ,-. -  Hill-Climbing, score: BIC -- - --  KL-divergence versus number of samples  Plain GSiN  Randomized GSBN .......  Hill-Climbing, score' data likelihood .- ,,---  Hill-Climbing, score: BIC ---a---  4000 8000 12000 16000 20000  Number of samples  Direction errors versus number of samples  Plain GBN  Randomized GSBN ---,---  Hill-Climbing, score: data likelihood  Hill-Climbing, score BIC  8000 12000 0 4000 8000 12000 18000 20000  Number of samples Number of samples  Figure 3: Results for a 5 x 5 rectangular net with branching factor 2 (in both directions, blanket size  8) as a function of the number of samples. On the top, KL-divergence is depicted for the plain GS,  randomized GS, and hill-climbing algorithms. On the bottom, the percentage of edge and direction  errors are shown. Note that certain edge error rates for the hill-climbing algorithm exceed 100%.  5 Results  Throughout the algorithms presented in this paper we employ standard chi-square (X 2) con-  ditional dependence tests (as is done also in [Spirtes93]) in order to compare the histograms  (X) and (X I Y). The X 2 test gives us a probability of the error of assuming that the  two variables are dependent when in fact they are not (type II error of a dependence test),  from which we can easily derive the probability that X and Y are dependent. There is an  implicit confidence threshold r involved in each dependence test, indicating how certain  we wish to be about the correctness of the test without unduly rejecting dependent pairs,  something that is always possible in reality due to the presence of noise. In all experiments  we used r = 0.95, which corresponds to a 95% confidence test.  We test the effectiveness of the algorithms through the following procedure: we generate a  random rectangular net of specified dimensions and up/down branching factor. A number  of examples are drawn from that net using logic sampling and they are used as input to  the algorithm under test. The resulting nets can be compared with the original ones along  dimensions of KL-divergence and difference in edges and edge directionality. The KL-  divergence was estimated using a Monte Carlo procedure. An example reconstruction was  shown in the beginning of the paper, Fig. 1.  Fig. 3 shows how the KL-divergence between the original and the reconstructed net as well  as edge omissions/false additions/reversals as a function of number of samples used. It  demonstrates two facts. First, that typical KL-divergence for both GS and hill-climbing  algorithms is low (with hill-climbing slightly lower), which shows good performance for  applications where prediction is of prime concern. Second, the number of incorrect edges  and the errors in the directionality of the edges present is much higher for the hill-climbing  algorithm, making it unsuitable for accurate Bayesian net reconstruction.  Fig. 4 shows the effects of increasing the Markov blanket through an increasing branching  factor. As expected, we see a dramatic (exponential) increase in execution time of the plain  Bayesian Network Induction via Local Neighborhoods 511  lOO  90  80  70  60  50  40  30  20  lO  o  2  Edge / Direction Errors versus Branching Factor  Edge errors, plain GSBN ,  Edge errors, randomized GSBN ...........  Direction errors, plain GSBN .....  Direction errors, randomized GSBN  3 4 5  Branching Factor  22000  20000  18000  16000  14000  12000  10000  8000  6000  4000  2000  0  Execution Time versus Branching Factor  ' Plair GSBN ,  2 3 4 5  Branching Factor  Figure 4: Results for a 5 x 5 rectangular net from which 10000 samples were generated and used  for reconstruction, versus increasing branching factor. On the left, errors are slowly increasing as  expected, but comparable for the plain and randomized versions of the GS algorithm. On the right,  corresponding execution times are shown.  GS algorithm, though only a mild increase of the randomized version. The latter uses 200  (constant) conditional tests per decision, and its execution time increase can be attributed  to the (quadratic) increase in the number of decisions. Note that the error percentages  between the plain and the randomized version remain relatively close. The number of  direction errors for the GS algorithm actually decreases due to the larger number of parents  for each node (more "V" structures), which allows a greater number of opportunities to  recover the directionality of an edge (using an increased number of tests).  6 Discussion  In this paper we presented an efficient algorithm for computing the Markov blanket of  a node and then used it in the two versions of the GS algorithm (plain and randomized)  by exploiting the properties of the Markov blanket to facilitate fast reconstruction of the  local neighborhood around each node, under assumptions of bounded neighborhood size.  We also presented a randomized variant that has the advantages of faster execution speeds  and added reconstruction robustness due to multiple tests and Bayesian accumulation of  evidence. Simulation results demonstrate the reconstruction accuracy advantages of the  algorithms presented here over hill-climbing methods. Additional results also show that  the randomized version has a dramatical execution speed benefit over the plain one in  cases where the assumption of bounded neighborhood does not hold, without significantly  affecting the reconstruction error rate.  References  [Chow68]  [Herskovits90]  [Spirtes93]  [Pearl88]  [Rebane87]  [Verma90]  [Agosta88]  [Cheng97]  [Margaritis99]  [JQnger85]  C.K. Chow and C.N. Liu. Approximating discrete probability distributions with  dependence trees. IEEE Transactions on Information Theory, 14, 1968.  E.H. Herskovits and G.F. Cooper. Kutat6: An entropy-driven system for construc-  tion of probabilistic expert systems from databases. UAI-90.  P. Spirtes, C. Glymour, and R. Scheines. Causation, Prediction, and Search,  Springer, 1993.  J. Pearl. Probabilistic Reasoning in Intelligent Systems, Morgan Kaufmann, 1988.  G. Rebane and J. Pearl. The recovery of causal poly-trees from statistical data.  UAI-87.  T.S. Verma, and J. Pearl. Equivalence and Synthesis of Causal Models. UAI-90.  J.M. Agosta. The structure of Bayes networks for visual recognition. UAI-88.  J. Cheng, D.A. Bell, W. Liu, An algorithm for Bayesian network construction from  data. AI and Statistics, 1997.  D. Margaritis, S. Thrun, Bayesian Network Induction via Local Neighborhoods.  TR CMU-CS-99-134, forthcoming.  M. JQnger, Polyhedral combinatorics and the acyclic subdigraph problem, Helder-  mann, 1985.  
Population Decoding Based on  an Unfaithful Model  S. Wu, H. Nakahara, N. Murata and S. Amari  RIKEN Brain Science Institute  Hirosawa 2-1, Wako-shi, Saitama, Japan  {phwusi, hiro, mura, amari} @brain.riken.go.jp  Abstract  We study a population decoding paradigm in which the maximum likeli-  hood inference is based on an unfaithful decoding model (UMLI). This  is usually the case for neural population decoding because the encoding  process of the brain is not exactly known, or because a simplified de-  coding model is preferred for saving computational cost. We consider  an unfaithful decoding model which neglects the pair-wise correlation  between neuronal activities, and prove that UMLI is asymptotically effi-  cient when the neuronal correlation is uniform or of limited-range. The  performance of UMLI is compared with that of the maximum likelihood  inference based on a faithful model and that of the center of mass de-  coding method. It turns out that UMLI has advantages of decreasing  the computational complexity remarkablely and maintaining a high-level  decoding accuracy at the same time. The effect of correlation on the  decoding accuracy is also discussed.  1 Introduction  Population coding is a method to encode and decode stimuli in a distributed way by us-  ing the joint activities of a number of neurons (e.g. Georgopoulos et al., 1986; Paradiso,  1988; Seung and Sompolinsky, 1993). Recently, there has been an expanded interest in  understanding the population decoding methods, which particularly include the maximum  likelihood inference (MLI), the center of mass (COM), the complex estimator (CE) and the  optimal linear estimator (OLE) [see (Pouget et al., 1998; Salinas and Abbott, 1994) and the  references therein]. Among them, MLI has an advantage of having small decoding error  (asymptotic efficiency), but may suffers from the expense of computational complexity.  Let us consider a population of N neurons coding a variable c. The encoding process  of the population code is described by a conditional probability q(rlx ) (Anderson, 1994;  Zemel et al., 1998), where the components of the vector r = {ri) for i = 1,-.-, N are  the firing rates of neurons. We study the following MLI estimator given by the value of  c that maximizes the log likelihood lnp(rlc), where p(rlc ) is the decoding model which  might be different from the encoding model q(rlc ). So far, when people study MLI in a  population code, it normally (or implicitly) assumes that p(rlc ) is equal to the encoding  model q(rlc ). This requires that the estimator has full knowledge of the encoding process.  Taking account of the complexity of the information process in the brain, it is more natural  Population Decoding Based on an Unfaithful Model 193  to assume p(r[x)  q(rlx). Another reason for choosing this is for saving computational  cost. Therefore, a decoding paradigm in which the assumed decoding model is different  from the encoding one needs to be studied. In the context of statistical theory, this is called  estimation based on an unfaithful or a misspecified model. Hereafter, we call the decoding  paradigm of using MLI based on an unfaithful model, UMLI, to distinguish from that of  MLI based on the faithful model, which is called FMLI. The unfaithful model studied in  this paper is the one which neglects the pair-wise correlation between neural activities. It  turns out that UMLI has attracting properties of decreasing the computational cost of FMLI  remarkablely and at the same time maintaining a high-level decoding accuracy.  2 The Population Decoding Paradigm of UMLI  2.1 An Unfaithful Decoding Model of Neglecting the Neuronal Correlation  Let us consider a pair-wise correlated neural response model in which the neuron activities  are assumed to be multivariate Gaussian  1 exp[ 1  q(rlx) = V/(27ra2)N det(A) --2a E A l(ri - fi(x))(D - fj(x))], (1)  i,j  where fi(:c) is the tuning function. In the present study, we will only consider the radial  symmetry tuning function.  Two different correlation structures are considered. One is the uniform correlation model  (Johnson, 1980; Abbott and Dayan, 1999), with the covariance matrix  A 0 =60+c(1-60), (2)  where the parameter c (with - 1 < c < 1) determines the strength of correlation.  The other correlation structure is of limited-range (Johnson, 1980; Snippe and Koenderink,  1992; Abbott and Dayan, 1999), with the covariance matrix  Aij = b li-jl, (3)  where the parameter b (with 0 < b < 1) determines the range of correlation. This structure  has translational invariance in the sense that Aij = Akt, if  The unfaithful decoding model, treated in the present study, is the one which neglects the  correlation in the encoding process but keeps the tuning functions unchanged, that is,  1 exp[ 1 E(ri_ fi(z))2] ' (4)  p(rlx) = V/(27rrr2) N -2a i  2.2 The decoding error of UMLI and FMLI  The decoding error of UMLI has been studied in the statistical theory (Akahira and  Takeuchi, 1981; Murata et al., 1994). Here we generalize it to the population cod-  ing. For convenience, some notations are introduced. Vf(r, x) denotes df(r,c)/dc.  Eq[f(r,x)] and Vq[f(r,x)] denote, respectively, the mean value and the variance of  f(r, x) with respect to the distribution q(rl:c ). Given an observation of the population  activity r*, the UMLI estimate : is the value of :c that maximizes the log likelihood  Lp(r*,x) = lnp(r*lx ).  Denote by Xop t the value of x satisfying Eq[VLp(r, Xopt) ] = 0. For the faithful model  where p: q, Xop t = x. Hence, (Xop t - x) is the error due to the unfaithful setting,  whereas (: - Xopt) is the error due to sampling fluctuations. For the unfaithful model (4),  194 S. Wu, H. Nakahara, N. Murata and S. Amari  since Eq[VLp(r, xopt)]: O, ]i[fi(x) - fi(xopt)]f'(Xopt): O. Hence, Xop t: x and  UMLI gives an unbiased estimator in the present cases.  Let us consider the expansion of VLp (r*, ) at x,  VLp(r*,) _ VLp(r*,x) + VVLp(r*,x) ( - x).  Since VLv(r* , ) = O,  1  VVL(*,) (- ) _ ---  (5)  1  N 7Lp (r*, x), (6)  where N is the number of neurons. Only the large N limit is considered in the present  study.  Let us analyze the properties of the two random variables VVLp(r*,x) and  VLp(r*, z). We consider first the uniform correlation model.  For the uniform correlation structure, we can write  ri - fi(x) + (i q- T]), (7)  where r/and {ei}, for i = 1,..., N, are independent random variables having zero mean  and variance c and 1 - c, respectively. r/is the common noise for all neurons, representing  the uniform character of the correlation.  By using the expression (7), we get  1 1  VLp(r*,x) = No  1  X77Lp(r*,x) =  (8)  (9)  Without loss of generality, we assume that the distribution of the preferred stimuli is uni-  form. For the radial symmetry tuning functions,  ]i f/(x) and  Y]i f'(x) approaches  zero when N is large. Therefore, the correlation contributions (the terms of r/) in the above  two equations can be neglected. UMLI performs in this case as if the neuronal signals are  uncorrelated.  Thus, by the weak law of large numbers,  1  VVLp(r*,x)  where Qp _= Eq[VVLp(r,x)].  1  i  = N ' (10)  According to the central limit theorem, VLp (r*, x)/N converges to a Gaussian distribution  1--C  VLp(r*,x)  N(O, /20. 2 E  Gp  = N(0, -ff), (11)  where N(0, t 2) denoting the Gaussian distribution having zero mean and variance t, and  Op =  Population Decoding Based on an Unfaithful Model 195  Combining the results of eqs.(6), (10) and (11), we obtain the decoding error of UMLI,  (: - X)UML I ,-, N(0, Q;2Gv) ,  (1 -c)a 2  = N(0,  f?/-5 )' (12)  In the similar way, the decoding error of FMLI is obtained,  (-X)FL I  V(0,  = V(0, ( - c)  y.i f(x)2 ) , (13)  which has the same form as that of UMLI except that Qq and G are now defined with  respect to the faithful decoding model, i.e., p(rlz ) = q(rlz ). To get eq.(13), the condition  Y'i f(z) = 0 is used. Interestingly, UMLI and FMLI have the same decoding error. This  is because the uniform correlation effect is actually neglected in both UMLI and FMLI.  Note that in FMLI, (q -- (]q -- Vq[VLq(rlx)] is the Fisher information. (-2Gq is the  Cram(r-Rao bound, which is the optimal accuracy for an unbiased estimator to achieve.  Eq.(13) shows that FMLI is asymptotically efficient. For an unfaithful decoding model,  Qp and Gp are usually different from the Fisher information. We call Q;2Gp the gen-  eralized Cram(r-Rao bound, and UMLI quasi-asymptotically efficient if its decoding error  approaches Q-2Gp asymptotically. Eq.(12) shows that UMLI is quasi-asymptotic efficient.  In the above, we have proved the asymptotic efficiency of FMLI and UMLI when the neu-  ronal correlation is uniform. The result relies on the radial symmetry of the tuning function  and the uniform character of the correlation, which make it possible to cancel the corre-  lation contributions from different neurons. For general tuning functions and correlation  structures, the asymptotic efficiency of UMLI and FMLI may not hold. This is because the  law of large numbers (eq.(10)) and the central limit theorem (eq.(11)) are not in general  applicable.  We note that for the limited-range correlation model, since the correlation is translational  invariant and its strength decreases quickly with the dissimilarity in the neurons' preferred  stimuli, the correlation effect in the decoding of FMLI and UMLI becomes negligible when  N is large. This ensures that the law of large numbers and the central limit theorem hold  in the large N limit. Therefore, UMLI and FMLI are asymptotically efficient. This is  confirmed in the simulation in Sec.3.  When UMLI and FMLI are asymptotic efficient, their decoding errors in the large N limit  can be calculated according to the Cram6r-Rao bound and the generalized Cram(r-Rao  bound, respectively, which are  ((:b - x)2)UMLi  or2 Yij Aijfi(x)fj(x)  0-2  , (14)  (15)  3 Performance Comparison  The performance of UMLI is compared with that of FMLI and of the center of mass de-  coding method (COM). The neural population model we consider is a regular array of N  neurons (Baldi and Heiligenberg, 1988; Snippe, 1996) with the preferred stimuli uniformly  distributed in the range [-D, D], that is, ci = -D + 2iD/(N + 1), for i: 1,..., N. The  comparison is done at the stimulus x = 0.  196 S. Wu, H. Nakahara, N. Murata andS. Amari  COM is a simple decoding method without using any information of the encoding process,  whose estimate is the averaged value of the neurons' preferred stimuli weighted by the  responses (Georgopoulos et al., 1982; Snippe, 1996), i.e.,  : ._ i rii (16)  Ei  The shortcoming of COM is a large decoding erron  For the population model we consider, the decoding error of COM is calculated to be  a2 ']4j Aijcicj  - x) )COM " [Y'i fi(x)]  '  (17)  where the condition 5'i fi(x)ci = 0 is used, due to the regularity of the distribution of the  preferred stimuli.  The tuning function is Gaussian, which has the form  fi(x) = exp[- (x - ci)2],  2a 2  (18)  where the parameter a is the tuning width.  We note that the Gaussian response model does not give zero probability for negative firing  rates. To make it more reliable, we set ri = 0 when fi(x) < 0.11 (I z - cil > 3a), which  means that only those neurons which are active enough contribute to the decoding. It is easy  to see that this cut-off does not effect much the results of UMLI and FMLI, due to their  nature of decoding by using the derivative of the tuning functions. Whereas, the decoding  error of COM will be greatly enlarged without cut-off.  For the tuning width a, there are N = Int[6a/d - 1] neurons involved in the decoding  process, where d is the difference in the preferred stimuli between two consecutive neurons  and the function Int[-] denotes the integer part of the argument.  In all experiment settings, the parameters are chosen as a = I and a = 0.1. The decoding  errors of the three methods are compared for different values of N when the correlation  strength is fixed (c = 0.5 for the uniform correlation case and b = 0.5 for the limited-range  correlation case), or different values of the correlation strength when N is fixed to be 50.  Fig. 1 compares the decoding errors of the three methods for the uniform correlation model.  It shows that UMLI has the same decoding error as that of FMLI, and a lower error than that  of COM. The uniform correlation improves the decoding accuracies of the three methods  (Fig. lb).  In Fig.2, the simulation results for the decoding errors of FMLI and UMLI in the limited-  range correlation model are compared with those obtained by using the Cramr-Rao bound  and the generalized Cramr-Rao bound, respectively. It shows that the two results agree  very well when the number of neurons, N, is large, which means that FMLI and UMLI  are asymptotic efficient as we analyzed. In the simulation, the standard gradient descent  method is used to maximize the log likelihood, and the initial guess for the stimulus is  chosen as the preferred stimulus of the most active neuron. The CPU time of UMLI is  around 1/5 of that of FMLI. UMLI reduces the computational cost of FMLI significantly.  Fig.3 compares the decoding errors of the three methods for the limited-range correlation  model. It shows that UMLI has a lower decoding error than that of COM. Interestingly,  UMLI has a comparable performance with that of FMLI for the whole range of correlation.  The limited-range correlation degrades the decoding accuracies of the three methods when  the strength is small and improves the accuracies when the strength is large (Fig.3b).  Population Decoding Based on an Unfaithful Model 197  0015 r 0015 1  ,  '  -- FMLI, UMLI FULl, UMLI  ..... COM ..... COM   oo10 r- c 00o  '"..  .,-   oo , oo -...  N C  (a)  (b)  Figure 1' Comparing the decoding errors of UMLI, FMLI and COM for the uniform cor-  relation model.  0015 0015 r-  ' - CRB, b=0.5 -- GCRB, b=0.5 j  , = , SMR, b=0.5  [ = , SMR, b=0.5  --- CRB, b=0.8 --- GCRB, b=0.8  7. .---suR b--os '  .---suR, b--o.8 ,  ,'n ''.. ,'n ,   0005 - t' 0005  0000 0(300 J  20 40 60 80 100  N  (b)  (a)  Figure 2: Comparing the simulation results of the decoding errors of UMLI and FMLI in  the limited-range correlation model with those obtained by using the Cram(r-Rao bound  and the generalized Cram(r-Rao bound, respectively. CRB denotes the Cram(r-Rao bound,  GCRB the generalized Cramr-Rao bound, and SMR the simulation result. In the simula-  tion, 10 sets of data is generated, each of which is averaged over 1000 trials. (a) FMLI; (b)  UMLI.  4 Discussions and Conclusions  We have studied a population decoding paradigm in which MLI is based on an unfaithful  model. This is motivated by the facts that the encoding process of the brain is not exactly  known by the estimator. As an example, we consider an unfaithful decoding model which  neglects the pair-wise correlation between neuronal activities. Two different correlation  structures are considered, namely, the uniform and the limited-range correlations. The per-  formance of UMLI is compared with that of FMLI and COM. It turns out that UMLI has a  lower decoding error than that of COM. Compared with FMLI, UMLI has comparable per-  formance whereas with much less computational cost. It is our future work to understand  the biological implication of UMLI.  As a by-product of the calculation, we also illustrate the effect of correlation on the decod-  ing accuracies. It turns out that the correlation, depending on its form, can either improve  or degrade the decoding accuracy. This observation agrees with the analysis of Abbott  and Dayan (Abbott and Dayan, 1999), which is done with respect to the optimal decoding  accuracy, i.e., the Cram6r-Rao bound.  198 S. Wu, H. Nakahara, N. Murata andS. Amari  0020 r  (a)  = = FMLI  *-- UMLI /"  o 03  - .... COM / ""   0 02 L /' ,  00 02 o4 06 08 10  b  (b)  Figure 3: Comparing the decoding errors of UMLI, FMLI and COM for the limited-range  correlation model.  Acknowledgment  We thank the three anonymous reviewers for their valuable comments and insight sugges-  tion. S. Wu acknowledges helpful discussions with Danmei Chen.  References  L. F. Abbott and P. Dayan. 1999. The effect of correlated variability on the accuracy of a population  code. Neural Computation, 11:91-101.  M. Akahira and K. Takeuchi. 1981. Asymptotic efficiency of statistical estimators: concepts and  high order asymptotic efficiency. In Lecture Notes in Statistics 7.  C. H. Anderson. 1994. Basic elements of biological computational systems. International Journal  of Modern Physics C, 5:135-137.  P. Baldi and W. Heiligenberg. 1988. How sensory maps could enhance resolution through ordered  arrangements of broadly tuned receivers. Biol. Cybern., 59:313-318.  A. P. Georgopoulos, J. F. Kalaska, R. Caminiti, and J. T. Massey. 1982. On the relations between  the direction of two-dimensional arm movements and cell discharge in primate motor cortex. J.  Neurosci., 2:1527-1537.  K. O. Johnson. 1980. Sensory discrimination: neural processes preceding discrimination decision.  J. Neurophy., 43:1793-1815.  M. Murata, S. Yoshizawa, and S. Amari. 1994. Network information criterion-determining the  number of hidden units for an artificial neural network model. IEEE. Trans. Neural Networks, 5:865-  872.  A. Pouget, K. Zhang, S. Deneve, and P. E. Latham. 1998. Statistically efficient estimation using  population coding. Neural Computation, 10:373-401.  E. Salinas and L. F. Abbott. 1994. Vector reconstruction from firing rates. Journal of Computational  Neuroscience, 1:89-107.  H. P. Snippe and J. J. Koenderink. 1992. Information in channel-coded systems: correlated receivers.  Biological Cybernetics, 67:183-190.  H. P. Snippe. 1996. Parameter extraction from population codes: a critical assessment. Neural  Computation, 8:511-529.  R. S. Zemel, P. Dayan, and A. Pouget. 1998. Population interpolation of population codes. Neural  Computation, 10:403-430.  
Nonlinear Discriminant Analysis using  Kernel Functions  Volker Roth & Volker Steinhage  University of Bonn, Institut of Computer Science III  RSmerstrasse 164, D-53117 Bonn, Germany  { roth, steinhag} @ cs. uni-bonn. de  Abstract  Fishers linear discriminant analysis (LDA) is a classical multivari-  ate technique both for dimension reduction and classification. The  data vectors are transformed into a low dimensional subspace such  that the class centroids are spread out as much as possible. In  this subspace LDA works as a simple prototype classifier with lin-  ear decision boundaries. However, in many applications the linear  boundaries do not adequately separate the classes. We present a  nonlinear generalization of discriminant analysis that uses the ker-  nel trick of representing dot products by kernel functions. The pre-  sented algorithm allows a simple formulation of the EM-algorithm  in terms of kernel functions which leads to a unique concept for un-  supervised mixture analysis, supervised discriminant analysis and  semi-supervised discriminant analysis with partially unlabelled ob-  servations in feature spaces.  I Introduction  Classical linear discriminant analysis (LDA) projects N data vectors that belong to  c different classes into a (c- 1)-dimensionai space in such way that the ratio of be-  tween group scatter $$ and within group scatter Sw is maximized [1]. LDA formally  consists of an eigenvalue decomposition of S  SB leading to the so called canonical  vatlares which contain the whole class specific information in a (c- 1)-dimensional  subspace. The canonical variates can be ordered by decreasing eigenvalue size in-  dicating that the first variates contain the major part of the information. As a  consequence, this procedure allows low dimensional representations and therefore  a visualization of the data. Besides from interpreting LDA only as a technique for  dimensionality reduction, it can also be seen as a multi-class classification method:  the set of linear discriminant functions define a partition of the projected space into  regions that are identified with class membership. A new observation x is assigned  to the class with centroid closest to x in the projected space.  To overcome the limitation of only linear decision functions some attempts have  been made to incorporate nonlinearity into the classical algorithm. HASTIE et al.  [2] introduced the so called model of Flexible Discriminant Analysis: LDA is refor-  mulated in the framework of linear regression estimation and a generalization of this  method is given by using nonlinear regression techniques. The proposed regression  techniques implement the idea of using nonlinear mappings to transform the input  data into a new space in which again a linear regression is performed. In real world  Nonlinear Discriminant Analysis Using Kernel Functions 569  applications this approach has to deal with numerical problems due to the dimen-  sional explosion resulting from nonlinear mappings. In the recent years approaches  that avoid such explicit mappings by using kernel functions have become popular.  The main idea is to construct algorithms that only afford dot products of pattern  vectors which can be computed efficiently in high-dimensional spaces. Examples of  this type of algorithms are the Support Vector Machine [3] and Kernel Principal  Component Analysis [4].  In this paper we show that it is possible to formulate classical linear regression  and therefore also linear discriminant analysis exclusively in terms of dot products.  Therefore, kernel methods can be used to construct a nonlinear variant of dis-  criminant analysis. We call this technique Kernel Discriminant Analysis (KDA).  Contrary to a similar approach that has been published recently [5], our algorithm is  a real multi-class classifier and inherits from classical LDA the convenient property  of data ,isualization.  2 Review of Linear Discriminant Analysis  Under the assumption of the data being centered (i.e. Y-i xi = 0) the scatter ma-  trices $s and Sw are defined by  c 1 nj (x?)(x) T  SB: Ej=I -. El,m=l ) ) (1)  c . 1 . x? ) ) 1 n  - Z --Z (2)  where nj is the number of patterns x? ) that belong to class j.  LDA chooses a transformation matrix V that maximizes the objective function  Ivrssvl  J(V) = iVTSwV I . (3)  The columns of an optimal V are the generalized eigenvectors that correspond to  the nonzero eigenvalues in Ssvi = hi Swvi.  In [6] and [7] we have shown, that the standard LDA algorithm can be restat-  ed exclusively in terms of dot products of input vectors. The final equation is an  eigenvalue equation in terms of dot product matrices which are of size N x N. Since  the solution of high-dimensional generalized eigenvalue equations may cause numer-  ical problems (N may be large in real world applications), we present an improved  algorithm that reformulates discriminant analysis as a regression problem. More-  over, this version allows a simple implementation of the EM-algorithm in feature  spaces.  3 Linear regression analysis  In this section we give a brief review of linear regression analysis which we use as  "building block" for LDA. The task of linear regression analysis is to approximate  the regression function by a linear function  r(x) = E(YlA' = x) m c + x T/. (4)  on the basis of a sample (y,x),... ,(yN,XN). Let now y denote the vector  (y,...,yN) T and X denote the data matrix which rows are the input vectors.  Using a quadratic loss function, the optimal parameters c and / are chosen to  minimize the average squared residual  = Ilu-  1 + x11 +  IN denotes a N-vector of ones, [2 denotes a ridge-type penalty matrix [2 = el which  penalizes the coefficients of/. Assuming the data being centered, i.e y//v= xi = O,  the parameters of the regression function are given by:  c = X -1 yN  i= Yi =: Uy, / = (XrX + eI) -Xry. (6)  570 V. Roth and V. Steinhage  4 LDA by optimal scoring  In this section the LDA problem is linked to linear regression using the framework  of penalized optimal scoring. We give an overview over the detailed derivation in  [2] and [8]. Considering again the problem with c classes and N data vectors,  the class-memberships are represented by a categorical response variable  with  c levels. It is useful to code the n responses in terms of the indicator matrix  Z: Zi,5 = 1, if the i-th data vector belongs to class j, and 0 otherwise. The point  of optimal scoring is to turn categorical variables into quantitative ones by assigning  scores to classes: the score vector 0 assigns the real number 0 5 to the j-th level of  . The vector Z then represents a vector of scored training data and is regressed  onto the data matrix X. The simultaneous estimation of scores and regression  coefficients constitutes the optimal scoring problem: minimize the criterion  ASR(O, fi) = N-l[llZO - Still 4. (7)  under the constraint -[[Z0[[ 2 = 1. According to (6), for a given score 0 the  minimizing fi is given by  rios = ( xrx + fi)- x rzo, (8)  and the partially minimized criterion becomes:  minASR(,fi) - 1- N-iT zT M(f)Z,  (9)  where M(fl) = X(XTX + fl)-X T denotes the regularized hat or smoother matrix.  Minimizing of (9) under the constraint [[ZO[[  = 1 can be performed by the  following procedure:  1. Choose an initial matrix O0 satisfying the constraint N-10oZTZOo = I  and set O) = ZOo  2. Run a multi-response regression of  onto X: ) = M(f)O) = XB,  where B is the matrix of regression coefficients.  3. Eigenanalyze o)T)) to obtain the optimal scores, and update the matrix of  regression coefficients: B* = BW, with W being the matrix ofeigenvectors.  It can be shown, that the final matrix B* is, up to a diagonal scale matrix, equivalent  to the matrix of LDA-vectors, see [8].  5 Ridge regression using only dot products  The penalty matrix  in (5) assures that the penalized d x d covariance matrix  ,, - xTx 4- eI is a symmetric nonsingular matrix. Therefore, it has d eigenvectors  ai with accomplished positive eigenvalues ?i such that the following equations hold:  -- ---- --aia i (10)  j--1 i----1 i  The first equation implies that the first l leading eigenvectors ai with eigenvalues  ?i  e have an expansion in terms of the input vectors. Note that l is the number  of nonzero eigenvalues of the unpenalized covariance matrix xTX. Together with  (6), it follows for the general case, when the dimensionality d may extend l, that fi  can be written as the sum of two terms: an expansion in terms of the vectors xi  with coefficients ai and a similar expansion in terms of the remaining eigenvectors:  N d d  Z OiXi 4- Z jaj -- XToI 4- Z jaj, (11)  fi "-- i----1 j----l+l j----l+l  with e = (a ... an) T. However, the last term can be dropped, since every eigen-  vector aj, j = l + 1,..., d is orthogonal to every vector xi and does not influence  the value of the regression function (4).  The problem of penalized linear regression can therefore be stated as minimizing  Nonlinear Discriminant Analysis Using Kernel Functions 571  = [ Ilu - I +  A stationary vector c is determined by  a = (XX r +  (12)  (13)  Let now the dot product matrix K be defined by Kij -- :riT:rj and let for a given test  point (a:t) the dot product vector kt be defined by kt = Xxt. With this notation  the regression function of a test point (xt) reads  (,)  + [ ( + )-  = y. (14)  This equation requires only dot products and we can apply the kernel trick. The  final equation (14), up to the constant term u, h also been found by SUDERS et  al., [9]. They restated ridge regression in dual variables and optimized the resulting  criterion function with a lagrange multiplier technique. Note that our derivation,  which is a direct generalization of the standard linear regression formalism, leads in  a natural way to a cls of more general regression functions including the constant  term.  6 LDA using only dot products  Setting/ = xTc as in (11) and using the notation of section 5, for a given score  0 the optimal vector c is given by:  Otos -' (XX T q- -)-I zo. (15)  Analogous to (9), the partially minimized criterion becomes:  minASR(O,a) = 1- N-Or Zr g7I(fi)ZO, (16)  ot  with  /f/(fl) = XX'(XX T + )- = K(K + eI) -  1  To minimize (16) under the constraint llzoll 2 - 1 the procedure described in  section 4 can be used when M(fl) is substituted by /f/(fl). The matrix Y which  rows are the input vectors projected onto the column vectors of B* is given by:  Y -- XB* = K(K + eI) -1ZOoW. (17)  Note that again the dot product matrix K is all that is needed to calculate Y.  7 The kernel trick  The main idea of constructing nonlinear algorithms is to apply the linear methods  not in the space of observations but in a feature space F that is related to the former  by a nonlinear mapping b: //v _> F, x --> b(x).  Assuming that the mapped data are centered in F, i.e. Yi c)(xi) = 0, the present-  ed algorithms remain formally unchanged if the dot product matrix K is computed  in F: Kij = (qb(xi)  qb(xj)). As shown in [4], this assumption can be dropped by  1 n  writing ) instead of the mapping b: p(xi) := qb(xi) -  Yi= qb(xi).  Computation of dot products in feature spaces can be done efficiently by using k-  ernel functions k(xi, xj) [3]: For some choices of k there exists a mapping b into  some feature space F such that k acts as a dot product in F. Among possible  kernel functions there are e.g. Radial Basis Function (RBF) kernels of the form  k(m, u) -- exp(-I[m - ull 2/c).  8 The EM-algorithm in feature spaces  LDA can be derived as the maximum likelihood method for normal populations  with different means and common covariance matrix E (see [11]). Coding the class  membership of the observations in the matrix Z as in section 4, LDA maximizes  the (complete data) log-likelihood function  572 14. Roth and 14. Steinhage  This concept can be generalized for the case that only the group membership of  Nc < N observations is known ([14], p.679): the EM-algorithm provides a conve-  nient method for maximizing the likelihood function with missing data:  E-step: set Pki = Prob(xi E class k)  fZik, if the class membership of xi has been observed  Pi = ) v.c?!_ , otherwise, cfi(xi) (x exp[-1/2(xi - k)T'.-l(xi -- k)]  M-step: set  N N  i  1  rk -- -- ),Pki, Ilk -' Nrk PkiXi,  N i--1 i--1  c N  E =   YPki (xi - !a)(xi - !ak) T  k=l i=1  The idea behind this approach is that even an unclassified observation can be used  for estimation if it is given a proper weight according to its posterior probability  for class membership. The M-step can be seen as weighted mean and covariance  maximum likelihood estimates in a weighted and augmented problem: we augment  the data by replicating the N observations c times, with the 1-th such replication  having observation weights Pti. The maximization of the likelihood function can be  achieved via a weighted and augmented LDA. It turns out that it is not necessary to  explicitly replicate the observations and run a standard LDA: the optimal scoring  version of LDA described in section 4 allows an implicit solution of the augmented  problem that still uses only N observations. Instead of using a response indicator  matrix Z, one uses a blurred response Matrix , whose rows consist of the current  class probabilities for each observation. At each M-step this  is used in a multiple  linear regression followed by an eigen-decomposition. A detailed derivation is given  in [11]. Since we have shown that the optimal scoring problem can be solved in fea-  ture spaces using kernel functions this is also the case for the whole EM-aigorithm:  the E-step requires only differences in Mahalonobis distances which are supplied by  KDA.  After iterated application of the E- and M-step an observation is classified to the  class k with highest probability p. This leads to a unique framework for pure  mixture analysis (Nc = 0), pure discriminant analysis (Nc = N) and the semi-  supervised models of discriminant analysis with partially unclassified observations  (0 < Nc < N) in feature spaces.  9 Experiments  Waveform data: We illustrate KDA on a popular simulated example, taken from  [10], p.49-55 and used in [2, 11]. It is a three class problem with 21 variables. The  learning set consisted of 100 observations per class. The test set was of size 1000.  The results are given in table 1.  Table 1: Results for waveform data. The values are averages over 10 simulations.  The 4 entries above the line are taken from [11]. QDA: quadratic discriminant  analysis, FDA: flexible discriminant analysis, MDA: mixture discriminant analysis.  Technique Training Error [%] Test Error [%]  LDA  QDA  FDA (best model parameters)  MDA (best model parameters)  KDA (RBF kernel, a = 2, e = 1.5)  12.1(0.6) 19.1(0.6)  3.9(0.4) 20.5(0.6)  10.0(0.6) 19.1(0.6)  13.9(0.5) 15.5(0.5)  10.7(0.6) 14.1(0.7)  Nonlinear Discriminant Analysis Using Kernel Functions 573  The Bayes risk for the problem is about 14% [10]. KDA outperforms the other  nonlinear versions of discriminant analysis and reaches the Bayes rate within the  error bounds, indicating that one cannot expect significant further improvement  using other classifiers. Figure 1 demonstrates the data visualization property of  KDA. Since for a 3 class problem the dimensionality of the projected space equals  2, the data can be visualized without any loss of information. In the left plot one  can see the projected learn data and the class centroids, the right plot shows the  test data and again the class centroids of the learning set.  Figure 1: Data visualization with KDA. Left: learn set, right: test set  To demonstrate the effect of using unlabeled data for classification we repeated  the experiment with waveform data using only 20 labeled observations per class.  We compared the the classification results on a test set of size 300 using only the  labeled data (error rate E) with the results of the EM-model which considers  the test data as incomplete measurements during an iterative maximization of the  likelihood function (error rate E2). Using a RBF kernel ( = 250), we obtained the  following mean error rates over 20 simulations: E = 30.5(3.6)%, E2 = 17.1(2.7)%.  The classification performance could be drastically improved when including the  unlabelled data into the learning process.  Object recognition: We tested KDA on the MPI Chair Database . It consists of  89 regular spaced views form the upper viewing hemisphere of 25 different classes  of chairs as a training set and 100 random views of each class as a test set. The  available images are downscaled to 16 x 16 pixels. We did not use the additional  4 edge detection patterns for each view. Classification results for several classifiers  are given in table 2.  Table 2: Test error rates (%). Support Vector Machine, Multi Layer Perceptron,  Oriented Filter, taken from [12].  SVM MLP I 201.F 0 KDA, RBFkernel KDApoly. kernel  2.0 7.2 1.9 2.1  For a comparison of the computational performance we also trained the SVM-light  implementation (V 2.0) on the data, [13]. In this experiment with 25 classes the  KDA algorithm showed to be significantly faster than the SVM: using the RBF-  kernel, KDA was 3 times faster, with the polynomial kernel KDA was 20 times  faster than $VM-light.  10 Discussion  In this paper we present a nonlinear version of classical linear discriminant analysis.  The main idea is to map the input vectors into a high- or even infinite dimensional  feature space and to apply LDA in this enlarged space. Restating LDA in a way that  only dot products of input vectors are needed makes it possible to use kernel repre-  sentations of dot products. This overcomes numerical problems in high-dimensional  The database is available via ftp://ftp.mpik-tueb.mpg.de/pub/chair_dataset/  574 V. Roth and E Steinhage  feature spaces. We studied the classification performance of the KDA classifier on  simulated waveform data and on the MPI chair database that has been widely used  for benchmarking in the literature. For medium size problems, especially if the  number of classes is high, the KDA algorithm showed to be significantly faster than  a SVM while leading to the same classification performance. From classical LDA  the presented algorithm inherits the convenient property of data visualization, since  it allows low dimensional views of the data vectors. This makes an intuitive inter-  pretation possible, which is helpful in many practical applications. The presented  KDA algorithm can be used as the maximization step in an EM algorithm in feature  spaces. This allows to include unlabeled observation into the learning process which  can improve classification results. Studying the performance of KDA for other clas-  siftcation problems as well as a theoretical comparison of the optimization criteria  used in the KDA- and SVM-algorithm will be subject of future work.  Acknowledgements  This work was supported by Deutsche Forschungsgemeinschaft, DFG. We heavily  profitted from discussions with Armin B. Cremers, John Held and Lothat Hermes.  References  [1] R. Duda and P. Hart, Pattern Classification and Scene Analysis. Wiley &: Sons, 1973.  [2] T. Hastie, R. Tibshirani, and A. Buja, "Flexible discriminant analysis by optimal  scoring," JASA, vol. 89, pp. 1255-1270, 1994.  [3] V. N. Vapnik, Statistical learning theory. Wiley &: Sons, 1998.  [4] B. SchSlkopf, A. Smola, and K.-R. Muller, "Nonlinear component analysis as a kernel  eigenvalue problem," Neural Computation, vol. 10, no. 5, pp. 1299-1319, 1998.  [5] S. Mika, G. R/itsch, J. Weston, B. SchSlkopf, and K.-R. Milllet, "Fisher discrimi-  nant analysis with kernels," in Neural Networks for Signal Processing IX (Y.-H. Hu,  J. Larsen, E. Wilson, and S. Douglas, eds.), pp. 41-48, IEEE, 1999.  [6] V. Roth and V. Steinhage, "Nonlinear discriminant analysis using kernel functions,"  Tech. Rep. IAI-TR-99-7, Department of Computer Science III, Bonn University, 1999.  [7] V. Roth, A. Pogoda, V. Steinhage, and S. SchrSder, "Pattern recognition combining  feature- and pixel-based classification within a real world application," in Muster-  erkennung 1999 (W. FSrstner, J. Buhmann, A. Faber, and P. Faber, eds.), Informatik  aktuell, pp. 120-129, 21. DAGM Symposium, Bonn, Springer, 1999.  [8] T. Hastie, A. Buja, and R. Tibshirani, "Penalized discriminant analysis," ArmStat,  vol. 23, pp. 73-102, 1995.  [9] S. Saunders, A. Gammermann, and V. Vovk, "Ridge regression learning algorithm in  dual variables," tech. rep., Royal Holloway, University of London, 1998.  [10] L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone, Classification and Re-  gression Trees. Monterey, CA: Wadsworth and Brooks/Cole, 1984.  [11] T. Hastie and R. Tibshirani, "Discriminant analysis by gaussian mixtures," JRSSB,  vol. 58, pp. 158-176, 1996.  [12] B. SchSlkopf, Support Vector Learning. PhD thesis, 1997. R. Oldenbourg Verlag,  Munich.  [13] T. Joachims, "Making large-scale svm learning practical," in Advances in Kernel  Methods - Support Vector Learning (B. SchSlkopf, C. Burges, and A. Smola, eds.),  MIT Press, 1999.  [14] B. Flury, A First Course in Multivariate Statistics. Springer, 1997.  
An Oscillatory Correlation Framework for  Computational Auditory Scene Analysis  Guy J. Brown  Department of Computer Science  University of Sheffield  Regent Court, 211 Portobello Street,  Sheffield S 1 4DP, UK  Email: g. brown @dcs. shefac. uk  DeLiang L. Wang  Department of Computer and Information  Science and Centre for Cognitive Science  The Ohio State University  Columbus, OH 43210-1277, USA  Email: dwang @ cis. ohio-state. edu  Abstract  A neural model is described which uses oscillatory correlation to  segregate speech from interfering sound sources. The core of the model  is a two-layer neural oscillator network. A sound stream is represented  by a synchronized population of oscillators, and different streams are  represented by desynchronized oscillator populations. The model has  been evaluated using a corpus of speech mixed with interfering sounds,  and produces an improvement in signal-to-noise ratio for every mixture.  1 Introduction  Speech is seldom heard in isolation: usually, it is mixed with other environmental sounds.  Hence, the auditory system must parse the acoustic mixture reaching the ears in order to  retrieve a description of each sound source, a process termed auditory scene analysis  (ASA) [2]. Conceptually, ASA may be regarded as a two-stage process. The first stage  (which we term 'segmentation') decomposes the acoustic stimulus into a collection of  sensory elements. In the second stage ('grouping'), elements that are likely to have arisen  from the same environmental event are combined into a perceptual structure called a  stream. Streams may be further interpreted by higher-level cognitive processes.  Recently, there has been a growing interest in the development of computational systems  that mimic ASA [4], [1], [5]. Such computational auditory scene analysis (CASA)  systems are inspired by auditory function but do not model it closely; rather, they employ  symbolic search or high-level inference engines. Although the performance of these  systems is encouraging, they are no match for the abilities of a human listener; also, they  tend to be complex and computationally intensive. In short, CASA currently remains an  unsolved problem for real-time applications such as automatic speech recognition.  Given that human listeners can segregate concurrent sounds with apparent ease,  computational systems that are more closely modelled on the neurobiological mechanisms  of hearing may offer a performance advantage over existing CASA systems. This  observation - together with a desire to understand the neurobiological basis of ASA - has  led some investigators to propose neural network models of ASA. Most recently, Brown  and Wang [3] have given an account of concurrent vowel separation based on oscillatory  correlation. In this framework, oscillators that represent a perceptual stream are  synchronized (phase locked with zero phase lag), and are desynchronized from oscillators  that represent different streams [8]. Evidence for the oscillatory correlation theory comes  from neurobiological studies which report synchronised oscillations in the auditory, visual  and olfactory cortices (see [10] for a review).  748 G. J. Brown and D. L. Wang  In this paper, we propose a neural network model that uses oscillatory correlation as the  underlying neural mechanism for ASA; streams are formed by synchronizing oscillators  in a two-dimensional time-frequency network. The model is evaluated on a task that  involves the separation of two time-varying sounds. It therefore extends our previous  study [3], which only considered the segregation of vowel sounds with static spectra.  2 Model description  The input to the model consists of a mixture of speech and an interfering sound source,  sampled at a rate of 16 kHz with 16 bit resolution. This input signal is processed in four  stages described below (see [10] for a detailed account).  2.1 Peripheral auditory processing  Peripheral auditory frequency selectivity is modelled using a bank of 128 gammatone  filters with center frequencies equally distributed on the equivalent rectangular bandwidth  (ERB) scale between 80 Hz and 5 kHz [1]. Subsequently, the output of each filter is  processed by a model of inner hair cell function. The output of the hair cell model is a  probabilistic representation of auditory nerve firing activity.  2.2 Mid-level auditory representations  Mechanisms similar to those underlying pitch perception can contribute to the perceptual  separation of sounds that have different fundamental frequencies (F0s) [3]. Accordingly,  the second stage of the model extracts periodicity information from the simulated auditory  nerve firing patterns. This is achieved by computing a running autocorrelation of the  auditory nerve activity in each channel, forming a representation known as a correlogram  [1], [5]. At time step j, the autocorrelation A(i},'c) for channel i with time lag 'c is given by:  K-I  A(i'i') = E r(i'i-k)r(i'j-k-)w(k) (1)  k=0  Here, r is the output of the hair cell model and w is a rectangular window of width K time  steps. We use K = 320, corresponding to a window width of 20 ms. The autocorrelation lag  'c is computed in L steps of the sampling period between 0 and L-l; we use L = 201,  corresponding to a maximum delay of 12.5 ms. Equation (1) is computed for M time  frames, taken at 10 ms intervals (i.e., at intervals of 160 steps of the time index j).  For periodic sounds, a characteristic 'spine' appears in the correlogram which is centered  on the lag corresponding to the stimulus period (Figure 1A). This pitch-related structure  can be emphasized by forming a 'pooled' correlogram s(j,'c), which exhibits a prominent  peak at the delay corresponding to perceived pitch:  N  s(j, x) =  A(i, j, x) (2)  i=1  It is also possible to extract harmonics and formants from the correlogram, since  frequency channels that are excited by the same acoustic component share a similar  pattern of periodicity. Bands of coherent periodicity can be identified by cross-correlating  adjacent correlogram channels; regions of high correlation indicate a harmonic or formant  [1]. The cross-correlation C(i/) between channels i and i+1 at time framej is defined as:  L-1  1  C(i,j) =  E (i,j,x)(i+ l,j,x) (I<i<N-1) (3)  Here, (i, j, 'c) is the autocorrelation function of (1) which has been normalized to have  zero mean and unity variance. A typical cross-correlation function is shown in Figure 1A.  Oscillatory Correlation for CASA 749  2.3 Neural oscillator network: overview  Segmentation and grouping take place within a two-layer oscillator network (Figure lB).  The basic unit of the network is a single oscillator, which is defined as a reciprocally  connected excitatory variable x and inhibitory variable y [7]. Since each layer of the  network takes the form of a time-frequency grid, we index each oscillator according to its  frequency channel (i) and time frame (j):  3  ij = 3xij - xij + 2 - Yij + lij + Sij + f) (4a)  )ij = (y(1 + tanh(xij/[))-Yij) (4b)  Here, Iij represents external input to the oscillator, Sij denotes the coupling from other  oscillators in the network, ,  and  are parameters, and p is the amplitude of a Gaussian  noise term. If coupling and noise are ignored and I O. is held constant, (4) defines a  relaxation oscillator with two time scales. The x-nullcline, i.e. i: = 0, is a cubic function  and the y-nullcline is a sigmoid function. If I O > 0, the two nullclines intersect only at a  point along the middle branch of the cubic with  chosen small. In this case, the oscillator  exhibits a stable limit cycle for small values of , and is referred to as enabled. The limit  cycle alternates between silent and active phases of near steady-state behaviour.  Compared to motion within each phase, the alternation between phases takes place  rapidly, and is referred to as jumping. If li_. < 0, the two nullclines intersect at a stable fixed  point. In this case, no oscillation occurs. ence, oscillations in (4) are stimulus-dependent.  2.4 Neural oscillator network: segment layer  In the first layer of the network, segments are formed - blocks of synchronised oscillators  that trace the evolution of an acoustic component through time and frequency. The first  layer is a two-dimensional time-frequency grid of oscillators with a global inhibitor (see  Figure lB). The coupling term Sij in (4a) is defined as  Sij = Z Wij,tlS(Xt l - Ox) - WzS(z - Oz) (5)  kl  N(i, i)  r > wi  where H is the Heaviside function (i.e., H(x) = 1 fox _ 0, and zero other se), Wij, icl is the  connection weight from an oscillator (id) to an oscillator (k,l) and N(id) is the four nearest  neighbors of (id). The threshold 0 x is chosen so that an oscillator has no influence on its  5000-  2741-  1457-  729-  315-  80-  110  B  Grouping  Layer  Segment  Layer  2'.5 5'.0 7'.5 1(.0 li.5 Global  Autocorrelation Lag (ms) Inhibitor  Figure 1: A. Correlogram of a mixture of speech and trill telephone, taken 450 ms after the  start of the stimulus. The pooled correlogram is shown in the bottom panel, and the cross-  correlation function is shown on the right. B. Structure of the two-layer oscillator network.  750 G. J. Brown and D. L. Wang  neighbors unless it is in the active phase. The weight of neighboring connections along the  time axis is uniformly set to 1. The connection weight between an oscillator (ij) and its  vertical neighbor (i+lj) is set to 1 if C(i) exceeds a threshold 0c; otherwise it is set to 0.  W z is the weight of inhibition from the global inhibitor z, defined as   = ooo- z (6)  where oo = 1 if xij > 0 z for at least one oscillator (id), and oo = 0 otherwise. Hence 0 z is a  threshold. If o = 1, z -->1.  Small segments may form which do not correspond to perceptually significant acoustic  components. In order to remove these noisy fragments, we introduce a lateral potential Pij  for oscillator (ij), defined as [11 ]'  tgiJ = (1-Pij)H[ E H(Xkl-Ox)-Op]-Pij (7)  kl  Np(i, j)  Here, Op is a threshold. Np(id) is called the potential neighborhood of (ij), which is chosen  to be (ij-1) and (i,/+1). If both neighbors of (ij) are active, Pij approaches 1 on a fast time  scale; otherwise, Pij relaxes to 0 on a slow time scale determined by .  The lateral potential plays its role by gating the input to an oscillator. More specifically,  we replace (4a) with  3  2i/ = 3xij - xij + 2 --Yij + lijH(Pij - O) + Sij + p (4a')  With Pij initialized to 1, it follows that Pij will drop below the threshold 0 unless the  oscillator (ij) receives excitation from its entire potential neighborhood. Given our choice  of neighborhood in (5), this implies that a segment must extend for at least three  consecutive time frames. Oscillators that are stimulated but cannot maintain a high  potential are relegated to a discontiguous 'background' of noisy activity.  An oscillator (ij) is stimulated if its corresponding input lij > 0. Oscillators are stimulated  only if the energy in their corresponding correlogram channel exceeds a threshold 0 a. It is  evident from (1) that the energy in a correlogram channel i at time j corresponds to  A(ij,0); thus we set Iij = 0.2 if A(i},0) > O a, and I = -5 otherwise.  Figure 2A shows the segmentation of a mixture of speech and trill telephone. The network  was simulated by the LEGION algorithm [8], producing 94 segments (each represented by  a distinct gray level) plus the background (shown in black). For convenience we show all  segments together in Figure 2A, but each actually arises during a unique time interval.  1457  1457-  729 ' 729-  315  315-  80, 80  Time (seconds) Time (seconds)  Figure 2: A. Segments formed by the first layer of the network for a mixture of speech and  trill telephone. B. Categorization of segments according to F0. Gray pixels represent the set  P, and white pixels represent regions that do not agree with the F0.  Oscillatory Correlation for CASA 751  2.5 Neural oscillator network: grouping layer  The second layer is a two-dimensional network of laterally coupled oscillators without  global inhibition. Oscillators in this layer are stimulated if the corresponding oscillator in  the first layer is stimulated and does not form part of the background. Initially, all  oscillators have the same phase, implying that all segments from the first layer are  allocated to the same stream This initialization is consistent with psychophysical  evidence suggesting that perceptual fusion is the default state of auditory organisation [2].  In the second layer, an oscillator has the same form as in (4), except that xij is changed to:  'i/ = 3x(i -- Xij + 2 -- Yij + I0'[1 + gH(pij - 0)] + Sij + Q (4a")  Here, It is a small positive parameter; this implies that an oscillator with a high lateral  potential gets a slightly higher external input We choose N(i,j) and 0n so that oscillators   / 1   which correspond to the longest segment from the first layer are the first to jump to the  active phase. The longest segment is identified by using the mechanism described in [9].  The coupling term in (4a") consists of two types of coupling:  s 0 = s 0 + s 0 (8)  Here, S. . represents mutual excitation between oscillators within each segment We set  tJ   S/ = 4 if the active oscillators from the same segment occupy more than half of the  lehgth of the segment; otherwise S/ = 0.1 if there is at least one active oscillator from the  same segment.  The coupling term S. denotes vertical connections between oscillators corresponding to  different frequency cannels and different segments, but within the same time frame. At  each time frame, an F0 is estimated from the pooled correlogram (2) and this is used to  classify frequency channels into two categories: a set of channels, P, that are consistent  with the F0, and a set of channels that are not (Figure 2B). Given the delay m at which the  largest peak occurs in the pooled correlogram, for each channel i at time frame j, i  P if  A(i, i, Xm)/A( i, J, O) > 0 d (9)  Since A(ij,0) is the energy in correlogram channel i at time j, (9) amounts to classification  on the basis of an energy threshold We use 0 a = 0.95. The delay m can be found by using  a winner-take-all network, although for simplicity we currently apply a maximum selector.  2741  2741-  1457  1457-  729 729'  315  315'  8O 80'  oo Time (seconds) 115 1)'.o Time (seconds)  Figure 3: A. Snapshot showing the activity of the second layer shortly after the start of  simulation. Active oscillators (white pixels) correspond to the speech stream. B. Another  snapshot, taken shortly after A. Active oscillators correspond to the telephone stream.  752 G. J.. Brown and D. L. Wang  The F0 classification process operates on channels, rather than segments. As a result,  channels within the same segment at a particular time frame may be allocated to different  F0 categories. Since segments cannot be decomposed, we enforce a rule that all channels  of the same frame within each segment must belong to the same F0 category as that of the  majority of channels. After this conformational step, vertical connections are formed such  that, at each time frame, two oscillators of different segments have mutual excitatory links  if the two corresponding channels belong to the same F0 category; otherwise they have  mutual inhibitory links. S. v. is set to -0.5 if (ij) receives an input from its inhibitory links;  similarly, S is set to 0.5  (ij) receives an input from its vertical excitatory links.  At present, our model has no mechanism for grouping segments that do not overlap in  time. Accordingly, we limit operation of the second layer to the time span of the longest  segment. After forming lateral connections and trimming by the longest segment, the  network is numerically solved using the singular limit method [6].  Figure 3 shows the response of the second layer to the mixture of speech and trill  telephone. The figure shows two snapshots of the second layer, where a white pixel  indicates an active oscillator and a black pixel indicates a silent oscillator. The network  quickly forms two synchronous blocks, which desynchronize from each other. Figure 3A  shows a snapshot taken when the oscillator block (stream) corresponding to the segregated  speech is in the active phase; Figure 3B shows a subsequent snapshot when the oscillator  block corresponding to the trill telephone is in the active phase. Hence, the activity in this  layer of the network embodies the result of ASA; the components of an acoustic mixture  have been separated using F0 information and represented by oscillatory correlation.  2.6 Resynthesis  The last stage of the model is a resynthesis path. Phase-corrected output from the  gammatone filterbank is divided into 20 ms sections, overlapping by 10 ms and windowed  with a raised cosine. A weighting is then applied to each section, which is unity if the  corresponding oscillator is in its active phase, and zero otherwise. The weighted filter  outputs are summed across all channels to yield a resynthesized waveform.  A 70 B oo  60  50  40-  30  20  I0-  0  -10  NO N1 N2 N3 N4 N5 N6 N7 N8 N9  Intmsion type  90  80  70  6O  5O  40  30  20  I0  0  NO N1  N2 N3 N4 N5 N6 N7 N8 N9  Intrusion type  Figure 4: A. SNR before (black bar) and after (grey bar) separation by the model. Results  are shown for voiced speech mixed with ten intrusions (NO = 1 kHz tone; N1 = random  noise; N2 = noise bursts; N3 = 'cocktail party' noise; N4 = rock music; N5 = siren; N6 =  trill telephone; N7 = female speech; N8 = male speech; N9 = female speech). B.  Percentage of speech energy recovered from each mixture after separation by the model.  Oscillatory Correlation for CASA 753  3 Evaluation  The model has been evaluated using 100 mixtures of speech and noise [4]. The mixtures  are obtained by adding the waveforms of ten voiced utterances to each of ten intrusive  sounds. Since separate speech and noise waveforms are available, a signal-to-noise ratio  (SNR) can be computed for each mixture. Also, the SNR can be estimated after processing  by the model using separated speech and noise waveforms from the resynthesis path [10].  The SNR before and after separation by the model is shown in Figure 4A, averaged across  the ten utterances for each noise condition. Dramatic improvements in SNR are obtained  when the interfering noise is narrowband (1 kHz tone and siren); such intrusions tend to be  represented as a single segment, which can be segregated very effectively from the speech  source. Informal listening tests suggest that the intelligibility of the resynthesized speech  is good. Also, we have quantified the percentage of speech energy that is recovered by the  segregation process: typically, this is between 55% and 80% (Figure 4B).  4 Discussion  A significant feature of the model proposed here is that each stage has a neurobiological  foundation. The peripheral auditory model is based upon the gammatone filter, which is  derived from physiological measurement of auditory nerve impulse responses. Similarly,  our mid-level auditory representations are consistent with the physiology of the higher  auditory system [1]. Overall, the model is based on a framework - oscillatory correlation  - which is supported by recent neurophysiological findings.  The neural oscillator network performs ASA in a distributed manner; each oscillator  behaves autonomously and in parallel with the other oscillators. Although there are issues  regarding real-time implementation of the model that must be resolved [10], there is a real  possibility that the oscillator network can be implemented in analog VLSI. This feature is  very attractive, since the high speed and compact size of analog VLSI will be needed if  CASA is to provide an effective front-end for automatic speech recognition systems.  References  [1] Brown, G. J. & Cooke, M. (1994) Computational auditory scene analysis. Computer Speech and  Language, 8, pp. 297-336.  [2] Bregman, A. S. (1990) Auditory scene analysis. Cambridge MA: MIT Press.  [3] Brown, G. J. & Wang, D. L. (1997) Modelling the perceptual segregation of double vowels with  a network of neural oscillators. Neural Networks, 10, pp. 1547-1558.  [4] Cooke, M. (1993) Modelling auditory processing and organization. Cambridge U.K.:  Cambridge University Press.  [5] Ellis, D. P. W. (1996) Prediction-driven computational auditory scene analysis. Ph.D.  Dissertation, MIT Department of Electrical Engineering and Computer Science.  [6] Linsay, P.S. & Wang, D. L. (1998) Fast numerical integration of relaxation oscillator networks  based on singular limit solutions. IEEE Transactions on Neural Networks, 9, pp. 523-532.  [7] Terman, D. & Wang, D. L. (1995) Global competition and local cooperation in a network of  neural oscillators, Physica D, 81, pp. 148-176.  [8] Wang, D. L. (1996) Primitive auditory segregation based on oscillatory correlation, Cognitive  Science, 20, pp. 409-456.  [9] Wang, D. L. (1999) Object selection based on oscillatory correlation, Neural Networks, 12, pp.  579-592.  [ 10] Wang, D. L. & Brown, G. J. (1999) Separation of speech from interfering sounds based on  oscillatory correlation. IEEE Transactions on Neural Networks, 10, pp. 684-697.  [ 11] Wang, D. L. & Terman, D. (1997) Image segmentation based on oscillatory correlation, Neural  Computation, 9, pp. 805-836 (for errata see Neural Computation, 9, pp. 1623-1626, 1997).  
Reconstruction of Sequential Data with  Probabilistic Models and Continuity Constraints  Miguel . Carreira-Perpifi.4n  Dept. of Computer Science, University of Sheffield, UK  miguel @ dcs. shef ac. uk  Abstract  We consider the problem of reconstructing a temporal discrete sequence  of multidimensional real vectors when part of the data is missing, under  the assumption that the sequence was generated by a continuous pro-  cess. A particular case of this problem is multivariate regression, which  is very difficult when the underlying mapping is one-to-many. We pro-  pose an algorithm based on a joint probability model of the variables  of interest, implemented using a nonlinear latent variable model. Each  point in the sequence is potentially reconstructed as any of the modes  of the conditional distribution of the missing variables given the present  variables (computed using an exhaustive mode search in a Gaussian mix-  ture). Mode selection is determined by a dynamic programming search  that minimises a geometric measure of the reconstructed sequence, de-  rived from continuity constraints. We illustrate the algorithm with a toy  example and apply it to a real-world inverse problem, the acoustic-to-  articulatory mapping. The results show that the algorithm outperforms  conditional mean imputation and multilayer perceptrons.  1 Definition of the problem  Consider a mobile point following a continuous trajectory in a subset of I D. Imagine  that it is possible to obtain a finite number of measurements of the position of the point.  Suppose that these measurements are corrupted by noise and that sometimes part of, or all,  the variables are missing. The problem considered here is to reconstruct the sequence from  the part of it which is observed. In the particular case where the present variables and the  missing ones are the same for every point, the problem is one of multivariate regression.  If the pattern of missing variables is more general, the problem is one of missing data  reconstruction.  Consider the problem of regression. If the present variables uniquely identify the missing  ones at every point of the data set, the problem can be adequately solved by a universal  function approximator, such as a multilayer perceptron. In a probabilistic framework, the  conditional mean of the missing variables given the present ones will minimise the average  squared reconstruction error [3]. However, if the underlying mapping is one-to-many, there  will be regions in the space for which the present variables do not identify uniquely the  missing ones. In this case, the conditional mean mapping will fail, since it will give a  compromise value--an average of the correct ones. Inverse problems, where the inverse  Probabilistic Sequential Data Reconstruca'on 415  of a mapping is one-to-many, are of this type. They include the acoustic-to-articulatory  mapping in speech [ 1 5], where different vocal tract shapes may produce the same acoustic  signal, or the robot arm problem [2], where different configurations of the joint angles may  place the hand in the same position.  In some situations, data reconstruction is a means to some other objective, such as classifi-  cation or inference. Here, we deal solely with data reconstruction of temporally continuous  sequences according to the squared error. Our algorithm does not apply for data sets that  either lack continuity (e.g. discrete variables) or have lost it (e.g. due to undersampling or  shuffling).  We follow a statistical learning approach: we attempt to reconstruct the sequence by learn-  ing the mapping from a training set drawn from the probability distribution of the data,  rather than by solving a physical model of the system. Our algorithm can be described  briefly as follows. First, a joint density model of the data is learned in an unsupervised way  from a sample of the data  . Then, pointwise reconstruction is achieved by computing all  the modes of the conditional distribution of the missing variables given the present ones  at the current point. In principle, any of these modes is potentially a plausible reconstruc-  tion. When reconstructing a sequence, we repeat this mode search for every point in the  sequence, and then find the combination of modes that minimises a geometric sequence  measure, using dynamic programming. The sequence measure is derived from local conti-  nuity constraints, e.g. the curve length.  The algorithm is detailed in 2 to 4. We illustrate it with a 2D toy problem in 5 and apply  it to an acoustic-to-articulatory-like problem in 6. 7 discusses the results and compares  the approach with previous work.  Our notation is as follows. We represent the observed variables in vector form as t =  N  (t,..., tD) E I D. A data set (possibly a temporal sequence) is represented as {t,},=.  Groups of variables are represented by sets of indices 27, ,7 E {1,..., D}, so that if 27 -  {1, 7, 3}, then tz = (tt7t3).  2 Joint generafive modelling using latent variables  Our starting point is a joint probability model of the observed variables p(t). From it, we  can compute conditional distributions of the form p(t71tz) and, by picking representative  points, derive a (multivalued) mapping tz --> t,7. Thus, contrarily to other approaches,  e.g. [6], we adopt multiple pointwise imputation. In 4 we show how to obtain a single  reconstructed sequence of points.  Although density estimation requires more parameters than mapping approximation, it has  a fundamental advantage [6]: the density model represents the relation between any vari-  ables, which allows to choose any missing/present variable combination. A mapping ap-  proximator treats asymmetrically some variables as inputs (present) and the rest as outputs  (missing) and can't easily deal with other relations.  The existence of functional relationships (even one-to-many) between the observed vari-  ables indicates that the data must span a low-dimensional manifold in the data space. This  suggests the use of latent variable models for modelling the joint density. However, it is  possible to use other kinds of density models.  In latent variable modelling the assumption is that the observed high-dimensional data t  is generated from an underlying low-dimensional process defined by a small number L  of latent variables x : (ar,...,a:) [1]. The latent variables are mapped by a fixed  1 In our examples we only use complete training data (i.e., with no missing data), but it is perfectly  possible to estimate a probability model with incomplete training data by using an EM algorithm [6].  416 M. .i. Carreira-Perpifidn  transformation into a D-dimensional data space and noise is added there. A particular  model is specified by three parametric elements: a prior distribution in latent space p(x), a  smooth mapping f from latent space to data space and a noise model in data space p(t[x).  Marginalising the joint probability density function p(t, x) over the latent space gives the   {t}= l, a pa-  distribution in data space, p(t) Given an observed sample in data space /v  rameter estimate can be found by maximising the log-likelihood, typically using an EM  algorithm. We consider the following latent variable models, both of which allow easy  computation of conditional distributions of the form p(tj[tz):  Factor analysis [1], in which the mapping is linear, the prior in latent space is unit Gaus-  sian and the noise model is diagonal Gaussian. The density in data space is then  Gaussian with a constrained covariance matrix. We use it as a baseline for com-  parison with more sophisticated models.  The generafive topographic mapping (GTM) [4] is a nonlinear latent variable model,  where the mapping is a generalised linear model, the prior in latent space is dis-  crete uniform and the noise model is isotropic Gaussian. The density in data space  is then a constrained mixture of isotropic Gaussians.  In latent variable models that sample the latent space prior distribution (like GTM), the  mixture centroids in data space (associated to the latent space samples) are not trainable  parameters. We can then improve the density model at a higher computational cost with no  generalisation loss by increasing the number of mixture components. Note that the number  of components required will depend exponentially on the intrinsic dimensionality of the  data (ideally coincident with that of the latent space, L) and not on the observed one, D.  3 Exhaustive mode finding  Given a conditional distribution p(tj[tz), we consider all its modes as plausible predic-  tions for tj. This requires an exhaustive mode search in the space of tj. For Gaussian  mixtures, we do this by using a maximisation algorithm starting from each centroid 2, such  as a fixed-point iteration or gradient ascent combined with quadratic optimisation [5]. In  the particular case where all variables are missing, rather than performing a mode search,  we return as predictions all the component centroids. It is also possible to obtain error  bars at each mode by locally approximating the density function by a normal distribution.  However, if the dimensionality of tj is high, the error bars become very wide due to the  curse of the dimensionality.  An advantage of multiple pointwise imputation is the easy incorporation of extra constraints  on the missing variables. Such constraints might include keeping only those modes that lie  in an interval dependent on the present variables [8] or discarding low-probability (spuri-  ous) modes--which speeds up the reconstruction algorithm and may make it more robust.  A faster way to generate representative points of p(t cr[tz) is simply to draw a fixed number  of samples from it--which may also give robustness to poor density models. However, in  practice this resulted in a higher reconstruction error.  4 Continuity constraints and dynamic programming (DP) search  Application of the exhaustive mode search to the conditional distribution at every point  of the sequence produces one or more candidate reconstructions per point. To select a  2Actually, given a value of tz, most centroids have negligible posterior probability and can be  removed from the mixture with practically no loss of accuracy. Thus, a large number of mixture  components may be used without deteriorating excessively the computational efficiency.  Probabilistic Sequential Data Reconstruction 417  -6  trajectory  factor an.  --- mean  ---- dpmod  Average squared reconstruction error  Missing Factor MLP" GTM  pattern analysis mean dpmode cmode  t2 3.8902 0.2046 0.2044 0.2168 0.2168  t  4.3226 2.5126 2.4224 0.0522 0.0522  t  or t2 4.2020 -- 1.2963 0.1305 0.1305  10% 1.0983 -- 0.3970 0.0253 0.025 I  50% 6.2914 -- 4.6530 0.1176 0.0771  90% 21.4942 -- 20.7877 2.2261 0.0643  aThe MLP cannot be applied to varying patterns of  missing data.  -6 -4 -2 0 2 4 6  Table 1: Trajectory reconstruction for a 2D problem. The table gives the average squared  reconstruction error when t2 is missing (row 1), tl is missing (row 2), exactly one variable  per point is missing at random (row 3) or a percentage of the values are missing at random  (rows 4-6). The graph shows the reconstructed trajectory when t is missing: factor anal-  ysis (straight, dotted line), mean (thick, dashed), alpmode (superimposed on the trajectory).  single reconstructed sequence, we define a local continuity constraint: consecutive points  in time should also lie nearby in data space. That is, if 6 is some suitable distance in 1I D ,  8(tn, tn+l ) should be small. Then we define a global geometric measure  for a sequence  N N def N-- 1  {tn}.= as  ({t. ) = 6 . take as so  }n=l (t,, 6 the distance,  Y'],= 1 t,+l) We Euclidean   becomes simply the length of the sequence (considered as a polygonal line). Finding the  sequence of modes with minimal  is efficiently achieved by dynamic programming.  5 Results with a toy problem  To illustrate the algorithm, we generated a 2D data set from the curve (tl, t2) = (z, z +  3 sin(z)) for z E [-27r, 27r], with normal isotropic noise (standard deviation 0.2) added.  Thus, the mapping tl --> t2 is one-to-one but the inverse one, t2 -+ tl, is multivalued.  One-dimensional factor analysis (6 parameters) and GTM models (21 parameters) were  estimated from a 1000-point sample, as well as two 48-hidden-unit multilayer perceptrons  (98 parameters), one for each mapping. For GTM we tried several strategies to select points  from the conditional distribution: mean (the conditional mean), alpmode (the mode selected  by dynamic programming) and cmode (the closest mode to the actual value of the missing  variable). The cmode, unknown in practice, is used here to compute a lower bound on  the performance of any mode-based strategy. Other strategies, such as picking the global  mode, a random mode or using a local (greedy) search instead of dynamic programming,  gave worse results than the dpmode.  Table 1 shows the results for reconstructing a 100-point trajectory. The nonlinear nature of  the problem causes factor analysis to break down in all cases. For the one-to-one mapping  case (t2 missing) all the other methods perform well and recover the original trajectory,  with mean attaining the lowest error, as predicted by the theory 3. For the one-to-many case  (tl missing, see fig.), both the MLP and the mean are unable to track more than one branch  of the mapping, but the alpmode still recovers the original mapping. For random missing  3A combined strategy could retain the optimality of the mean in the one-to-one case and the  advantage of the modes in the one-to-many case, by choosing the conditional mean (rather than the  mode) when the conditional distribution is unimodal, and all the modes otherwise.  418 M. .. Carreira-Perpihdn  Missing Factor GTM  pattern analysis mean dpmode cmode  PLP 0.9165 0.6217 0.6250 0.4587  EPG 3.7177 2.3729 2.0613 1.0538  10% 0.2046 0.0947 0.0903 0.0841  50% 1.1285 0.7540 0.6527 0.6023  blocks 0.1950 0.1669 0.1005 0.0925  Table 2: Average squared reconstruction error for an utterance. The last row corresponds  to a missing pattern of square blocks totalling 10% of the utterance.  patterns 4, the dpmode is able to cope well with high amounts of missing data.  The consistently low error of the cmode shows that the modes contain important infor-  mation about the possible options to predict the missing values. The performance of the  dpmode, close to that of the cmode even for large amounts of missing data, shows that  application of the continuity constraint allows to recover that information.  6 Results with real speech data  We report a preliminary experiment using acoustic and electropalatographic (EPG) data 5  for the utterance "Put your hat on the hatrack and your coat in the cupboard" (speaker FG)  from the ACCOR database [10]. 12th-order perceptual linear prediction coefficients [7]  plus the log-energy were computed at 200 Hz from its acoustic waveform. The EPG data  consisted of 62-bit frames sampled at 200 Hz, which we consider as 62-dimensional vectors  of real numbers. No further preprocessing of the data was carried out. Thus, the resulting  sequence consisted of over 600 75-dimensional real vectors. We constructed a training set  by picking, in random order, 80% of these vectors. The whole utterance was used for the  reconstruction test.  We trained two density models: a 9-dimensional factor analysis (825 parameters) and a  two-dimensional 6 GTM (3676 parameters) with a 20 x 20 grid (resulting in a mixture of  400 isotropic Gaussians in the 75-dimensional data space). Table 2 confirms again that the  linear method (factor analysis) fares worst (despite its use of a latent space of dimension  L: 9). The dpmode attains almost always a lower error than the conditional mean, with up  to a 40% improvement (the larger the higher the amount of missing data). When a shuffled  version of the utterance (thus having lost its continuity) was reconstructed, the error of the  dpmode was consistently higher than that of the mean, indicating that the application of the  continuity constraint was responsible for the error decrease.  7 Discussion  Using a joint probability model allows flexible construction of predictive distributions for  the missing data: varying patterns of missing data and multiple pointwise imputations are  possible, as opposed to standard function approximators. We have shown that the modes of  the conditional distribution of the missing variables given the present ones are potentially  4Note that the nature of the missing pattern (missing at random, missing completely at random,  etc. [9]) does not matter for reconstruction--although it does for estimation.  5An EPG datum is the (binary) contact pattern between the tongue and the palate at selected  locations in the latter. Note that it is an incomplete articulatory representation of speech.  6A latent space of 2 dimensions is clearly too low for this data, but the computational complexity  of GTM prevents the use of a higher one. Still, its nonlinear character compensates partly for this.  Probabilistic Sequential Data Reconstruction 419  plausible reconstructions of the missing values, and that the application of local continuity  constraints--when they hold--can help to recover the actually plausible ones.  Previous work The key aspects of our approach are the use of a joint density model  (learnt in an unsupervised way), the exhaustive mode search, the definition of a geometric  trajectory measure derived from continuity constraints and its implementation by dynamic  programming. Several of these ideas have been applied earlier in the literature, which we  review briefly.  The use of the joint density model for prediction is the basis of the statistical technique  of multiple imputation [9]. Here, several versions of the complete data set are generated  from the appropriate conditional distributions, analysed by standard complete-data methods  and the results combined to produce inferences that incorporate missing-data uncertainty.  Ghahramani and Jordan [6] also proposed the use of the joint density model to generate a  single estimate of the missing variables and applied it to a classification problem.  Conditional distributions have been approximated by MLPs rather than by density estima-  tion [ 16], but this lacks flexibility to varying patterns of missing data and requires an extra  model of the input variables distribution (unless assumed uniform).  Rohwer and van der Rest [12] introduce a cost function with a description length interpre-  tation whose minimum is approximated by the densest mode of a distribution. A neural  network trained with this cost function can learn one branch of a multivariate mapping, but  is unable to select other branches which may be correct at a given time.  Continuity constraints implemented via dynamic programming have been used for the  acoustic-to-articulatory mapping problem [15]. Reasonable results (better than using an  MLP to approximate the mapping) can be obtained using a large codebook of acoustic and  articulatory vectors. Rahim et al. [ 11 ] achieve similar quality with much less computa-  tional requirements using an assembly of MLPs, each one trained in a different area of the  acoustic-articulatory space, to locally approximate the mapping. However, clustering the  space is heuristic (with no guarantee that the mapping is one-to-one in each region) and  training the assembly is difficult. It also lacks flexibility to varying missingness patterns.  A number of trajectory measures have been used in the robot arm problem literature [2] and  minimised by dynamic programming, such as the energy, torque, acceleration, jerk, etc.  Temporal modelling It is important to remark that our approach does not attempt to  model the temporal evolution of the system. The joint probability model is estimated stat-  ically. The temporal aspect of the data appears indirectly and a posteriori through the ap-  plication of the continuity constraints to select a trajectory 7. In this respect, our approach  differs from that of dynamical systems or from models based in Markovian assumptions,  such as hidden Markov models or other trajectory models [13, 14]. However, the fact that  the duration or speed of the trajectory plays no role in the algorithm may make it invariant  to time warping (e.g. robust to fast/slow speech styles).  Choice of density model The fact that the modes are a key aspect of our approach make  it sensitive to the density model. With finite mixtures, spurious modes can appear as ripple  superimposed on the density function in regions where the mixture components are sparsely  distributed and have little interaction. Such modes can lead the DP search to a wrong  trajectory. Possible solutions are to improve the density model (perhaps by increasing the  number of components, see 2, or by regularisation), to smooth the conditional distribution  or to look for bumps (regions of high probability mass) instead of modes.  7However, the method may be derived by assuming a distribution over the whole sequence with a  normal, Markovian dependence between adjacent frames.  420 M... Carreira-Perpidn  Computational cost The DP search has complexity O(NM2), where M is an average  of the number of modes per sequence point and N the number of points in the sequence. In  our experiments M is usually small and the DP search is fast even for long sequences. The  bottleneck of the reconstruction part of the algorithm is obtaining the modes of the condi-  tional distribution for every point in the sequence when there are many missing variables.  Further work We envisage more thorough experiments using data from the Wisconsin  X-ray microbeam database and comparing with recurrent MLPs or an MLP committee,  which may be more suitable for multivalued mappings. Extensions of our algorithm in-  clude different geometric measures (e.g. curvature-based rather than length-based), differ-  ent strategies for multiple pointwise imputation (e.g. bump searching) or multidimensional  constraints (e.g. temporal and spatial). Other practical applications include audiovisual  mappings for speech, hippocampal place cell reconstruction and wind vector retrieval from  scatterometer data.  Acknowledgments  We thank Steve Renals for useful conversations and for comments about this paper.  References  [ 1] D.J. Bartholomew. Latent Variable Models and Factor Analysis. Charles Griffin & Company  Ltd., London, 1987.  [2] N. Bernstein. The Coordination and Regulation of Movements. Pergamon, Oxford, 1967.  [3] C. M. Bishop. Neural Networks for Pattern Recognition. Oxford University Press, 1995.  [4] C. M. Bishop, M. Svens6n, and C. K. I. Williams. GTM: The generative topographic mapping.  Neural Computation, 10(1):215-234, Jan. 1998.  [5] M. . Carreira-Perpifin. Mode-finding in Gaussian mixtures. Technical Report CS-99-03,  Dept. of Computer Science, University of Sheffield, UK, Mar. 1999. Available online at  http://www. dcs. shef. ac. uk/~miguel/papers/c s-99-03. html.  [6] Z. Ghahramani and M. I. Jordan. Supervised learning from incomplete data via an EM ap-  proach. In NIPS6, pages 120-127, 1994.  [7] H. Hermansky. Perceptual linear predictive (PLP) analysis of speech. J. Acoustic Soc. Amer.,  87(4): 1738-1752, Apr. 1990.  [8] L. Josifovski, M. Cooke, P. Green, and A. Vizinho. State based imputation of missing data for  robust speech recognition and speech enhancement. In Proc. Eurospeech 99, pages 2837-2840,  1999.  [9] R. J. A. Little and D. B. Rubin. Statistical Analysis with Missing Data. John Wiley & Sons,  New York, London, Sydney, 1987.  [10] A. Marchal and W. J. Hardcastle. ACCOR: Instrumentation and database for the cross-language  study of coarticulation. Language and Speech, 36(2, 3): 137-153, 1993.  [ 11 ] M. G. Rahim, C. C. Goodyear, W. B. Kleijn, J. Schroeter, and M. M. Sondhi. On the use of  neural networks in articulatory speech synthesis. J. Acoustic Soc. Amer., 93(2): 1109-1121, Feb.  1993.  [ 12] R. Rohwer and J. C. van der Rest. Minimum description length, regularization, and multimodal  data. Neural Computation, 8(3):595-609, Apr. 1996.  [13] S. Roweis. Constrained hidden Markov models. In NIPS 12 (this volume), 2000.  [14] L. K. Saul and M. G. Rahim. Markov processes on curves for automatic speech recognition. In  NIPS 11, pages 751-757, 1999.  [15] J. Schroeter and M. M. Sondhi. Techniques for estimating vocal-tract shapes from the speech  signal. IEEE Trans. Speech and Audio Process., 2(1): 133-150, Jan. 1994.  [16] V. Tresp, R. Neuneier, and S. Ahmad. Efficient methods for dealing with missing data in  supervised learning. In NIPS 7, pages 689-696, 1995.  
Model Selection for Support Vector Machines  Olivier Chapelle*,t, Vladimir Vapnik*  * AT&T Research Labs, Red Bank, NJ  t LIP6, Paris, France  { chapelle, vlad} @research. att. com  Abstract  New functionals for parameter (model) selection of Support Vector Ma-  chines are introduced based on the concepts of the span of support vec-  tors and rescaling of the feature space. It is shown that using these func-  tionals, one can both predict the best choice of parameters of the model  and the relative quality of performance for any value of parameter.  1 Introduction  Support Vector Machines (SVMs) implement the following idea: they map input vectors  into a high dimensional feature space, where a maximal margin hyperplane is constructed  [6]. It was shown that when training data are separable, the error rate for SVMs can be  characterized by  h: R2/M 2, (1)  where/i is the radius of the smallest sphere containing the training data and M is the mar-  gin (the distance between the hyperplane and the closest training vector in feature space).  This functional estimates the VC dimension of hyperplanes separating data with a given  margin M.  To perform the mapping and to calculate/i and M in the SVM technique, one uses a  positive definite kernel K(x, x') which specifies an inner product in feature space. An  example of such a kernel is the Radial Basis Function (RBF),  K(x, x') - e -IIx-x'112/22  This kernel has a free parameter a and more generally, most kernels require some param-  eters to be set. When treating noisy data with SVMs, another parameter, penalizing the  training errors, also needs to be set. The problem of choosing the values of these parame-  ters which minimize the expectation of test error is called the model selection problem.  It was shown that the parameter of the kernel that minimizes functional (1) provides a good  choice for the model: the minimum for this functional coincides with the minimum of the  test error [ 1 ]. However, the shapes of these curves can be different.  In this article we introduce refined functionals that not only specify the best choice of  parameters (both the parameter of the kernel and the parameter penalizing training error),  but also produce curves which better reflect the actual error rate.  Model Selection for Support Vector Machines 231  The paper is organized as follows. Section 2 describes the basics of SVMs, section 3  introduces a new functional based on the concept of the span of support vectors, section 4  considers the idea of rescaling data in feature space and section 5 discusses experiments of  model selection with these functionals.  2 Support Vector Learning  We introduce some standard notation for SVMs; for a complete description, see [6]. Let  (xi, yi)<i< be a set of training examples, xi 6 11 r which belong to a class labeled by  yi 6 {-1, 1 }. The decision function given by a SVM is   , (2)  0  where the coefficients a i are obtained by maximizing the following functional   W(a) = E ai-3 E aiajYiYjK(xi,xj)  i:1 i,j=l  (3)  under constraints  Eaiyi = 0 andO _< ai _< C i = 1,...,.  i=1  (7 is a constant which controls the tradeoff between the complexity of the decision function  and the number of training examples misclassified. SVM are linear maximal margin clas-  sifiers in a high-dimensional feature space where the data are mapped through a non-linear  function ,I,(x) such that ,I,(xi). ,I,(xj) - K(xi,xj).  The points xi with ai > 0 are called support vectors. We distinguish between those with  0 < ai < (7 and those with ai = (7. We call them respectively support vectors of the first  and second category.  3 Prediction using the span of support vectors  The results introduced in this section are based on the leave-one-out cross-validation esti-  mate. This procedure is usually used to estimate the probability of test error of a learning  algorithm.  3.1 The leave-one-out procedure  The leave-one-out procedure consists of removing from the training data one element, con-  structing the decision rule on the basis of the remaining training data and then testing the  removed element. In this fashion one tests all  elements of the training data (using g dif-  ferent decision rules). Let us denote the number of errors in the leave-one-out procedure  by E(Xl, y, ..., x, y). It is known [6] that the the leave-one-out procedure gives an al-  most unbiased estimate of the probability of test error: the expectation of test error for the  machine trained on  - 1 examples is equal to the expectation of (x, y, ..., x, y).  We now provide an analysis of the number of errors made by the leave-one-out procedure.  For this purpose, we introduce a new concept, called the span of support vectors [7].  232 O. Chapelle and V. N. Vapnik  3.2 Span of support vectors  Since the results presented in this section do not depend on the feature space, we will  consider without any loss of generality, linear SVMs, i.e. K(xi, x/) = xi  xj.  Suppose that c  = (a, ..., a) is the solution of the optimization problem (3).  For any fixed support vector xp we define the set Ap as constrained linear combinations of  the support vectors of the first category (xi)ip   Ap = )  )iXi, )i = 1, 0 _< ot i q-yiypOtp) i _< C . (4)  {i4p/ o<,, <c)  i=1, ip  Note that hi can be less than 0.  We also define the quantity $p, which we call the span of the support vector xp as the  minimum distance between xp and this set (see figure 1)  $p2 = d2(xp,Ap) = min (x - x) 2.  xA,  (5)  Figure 1' Three support vectors with al = a2 = a3/2. The set Ai is the semi-opened  dashed line.  It was shown in [7] that the set A v is not empty and that S v = d(xp, Ap) _< Dsv, where  Dsv is the diameter of the smallest sphere containing the support vectors.  Intuitively, the smaller $ -- d(xp, A) is, the less likely the leave-one-out procedure is to  make an error on the vector xp. Formally, the following theorem holds   Theorem 1 [7] If in the leave-one-out procedure a support vector xp corresponding to  0 < a < C is recognized incorrectly, then the following inequality holds  o> 1  ap _ $p max(D, 1/x/-)'  This theorem implies that in the separable case (C = ), the number of errors  made by the leave-one-out procedure is bounded as follows  (x,y,...,xe,ye) <_  maxv$pD = maxv$vD/M 2, because av  = 1/M 2 [6]. This is already an  Ep Op  improvement compared to functional (1), since $p _< Dsv. But depending on the geome-  try of the support vectors the value of the span $p can be much less than the diameter Dsv  of the support vectors and can even be equal to zero.  We can go further under the assumption that the set of support vectors does not change  during the leave-one-out procedure, which leads us to the following theorem   Model Selection for Support Vector Machines 233  Theorem 2 If the sets of support vectors of first and second categories remain the same  during the leave-one-out procedure, then for any support vector Xp, the following equality  holds:  yp[f(xp) fp(xp)] o 2  where fo and fP are the decision function (2) given by the SVM trained respectively on the  whole training set and after the point xp has been removed.  The proof of the theorem follows the one of Theorem 1 in [7].  The assumption that the set of support vectors does not change during the leave-one-out  procedure is obviously not satisfied in most cases. Nevertheless, the proportion of points  which violate this assumption is usually small compared to the number of support vec-  tors. In this case, Theorem 2 provides a good approximation of the result of the leave-one  procedure, as pointed out by the experiments (see Section 5.1, figure 2).  As already noticed in [ 1], the larger ap is, the more "important" in the decision function the  support vector xp is. Thus, it is not surprising that removing a point xp causes a change in  the decision function proportional to its Lagrange multiplier ap. The same kind of result as  Theorem 2 has also been derived in [2], where for SVMs without threshold, the following  inequality has been derived' yp(f(xp) - fP(xp)) _< avK(xp,xp). The span $p takes  into account the geometry of the support vectors in order to get a precise notion of how  "important" is a given point.  The previous theorem enables us to compute the number of errors made by the leave-one-  out procedure:  Corollary 1 Under the assumption of Theorem 2, the test error prediction given by the  leave-one-out procedure is  l(x Yl, x,ye) lCard{p/ o 2  = ap$; > ypf(xp)} (6)  t ----   ... __  Note that points which are not support vectors are correctly classified by the leave-one-out  procedure. Therefore tt defines the number of errors of the leave-one-out procedure on the  entire training set.  Under the assumption in Theorem 2, the box constraints in the definition of Ap (4) can  be removed. Moreover, if we consider only hyperplanes passing through the origin, the  constraint  hi = 1 can also be removed. Therefore, under those assumptions, the com-  putation of the span $p is an unconstrained minimization of a quadratic form and can be  done analytically. For support vectors of the first category, this leads to the closed form  2 --1  S -- 1/(Ksv)pp, where Ksv is the matrix of dot products between support vectors of  the first category. A similar result has also been obtained in [3].  In Section 5, we use the span-rule (6) for model selection in both separable and non-  separable cases.  4 Rescaling  As we already mentioned, functional (1) bounds the VC dimension of a linear margin clas-  sifier. This bound is tight when the data almost "fills" the surface of the sphere enclosing  the training data, but when the data lie on a flat ellipsoid, this bound is poor since the radius  of the sphere takes into account only the components with the largest deviations. The idea  we present here is to make a rescaling of our data in feature space such that the radius of the  sphere stays constant but the margin increases, and then apply this bound to our rescaled  data and hyperplane.  234 O. Chapelle and V. N. Vapnik  Let us first consider linear SVMs, i.e. without any mapping in a high dimensional space.  The rescaling can be achieved by computing the covariance matrix of our data and rescaling  according to its eigenvalues. Suppose our data are centered and let (qol,..., qo,) be the  normalized eigenvectors of the covariance matrix of our data. We can then compute the  smallest enclosing box containing our data, centered at the origin and whose edges are  parallels to (qol,..., qo,). This box is an approximation of the smallest enclosing ellipsoid.  The length of the edge in the direction qo; is/; = max/Ixi  qo; I. The rescaling consists  of the following diagonal transformation:  D: x > Dx =  k  Let us consider i = D-1xi and  = Dw. The decision function is not changed under  this transformation since   i - w  xi and the data i fill a box of side length 1. Thus,  in functional (1), we replace/{2 by 1 and 1/M 2 by 2. Since we rescaled our data in a  box, we actually estimated the radius of the enclosing ball using the oo-norm instead of  the classical 2-norm. Further theoretical works needs to be done to justify this change of  norm.  In the non-linear case, note that even if we map our data in a high dimensional feature space,  they lie in the linear subspace spanned by these data. Thus, if the number of training data   is not too large, we can work in this subspace of dimension at most . For this purpose, one  can use the tools of kernel PCA [5]: if A is the matrix of normalized eigenvectors of the  Gram matrix Kij = K(xi, xj) and (,i) the eigenvalues, the dot product xi. qo is replaced  by x/A and w. qo becomes x/i AikYiOti' ThUS, we can still achieve the diagonal  transformation A and finally functional (1) becomes  E A miax Ak (E AikYa)2'  k i  5 Experiments  To check these new methods, we performed two series of experiments. One concerns the  choice of a, the width of the RBF kernel, on a linearly separable database, the postal  database. This dataset consists of 7291 handwritten digit of size 16x16 with a test set  of 2007 examples. Following [4], we split the training set in 23 subsets of 317 training  examples. Our task consists of separating digit 0 to 4 from 5 to 9. Error bars in figures 2a  and 3 are standard deviations over the 23 trials. In another experiment, we try to choose  the optimal value of C in a noisy database, the breast-cancer database 1. The dataset has  been split randomly 100 times into a training set containing 200 examples and a test set  containing 77 examples.  Section 5.1 describes experiments of model selection using the span-rule (6), both in the  separable case and in the non-separable one, while Section 5.2 shows VC bounds for model  selection in the separable case both with and without rescaling.  5.1 Model selection using the span-rule  In this section, we use the prediction of test error derived from the span-rule (6) for model  selection. Figure 2a shows the test error and the prediction given by the span for differ-  ent values of the width cr of the RBF kernel on the postal database. Figure 2b plots the  same functions for different values of C on the breast-cancer database. We can see that  the method predicts the correct value of the minimum. Moreover, the prediction is very  accurate and the curves are almost identical.  Available from http: //horn. first. Imd. de/,-raetsch/data/breast-cancer  Model Selection for Support Vector Machines 235  40  35  30  25  2c  I.u  15  1(2  - - -' Test err)r  T -- Span pred ct on  ',,  Log sigma  (a) choice of cr in the postal database  36  34  32  30  28  I.u  26  24  2O  TTT.  I  '  Log C  '--- Tst erro) j  -- Span pred ction   +  10 12  (b) choice of C' in the breast-cancer database  Figure 2: Test error and its prediction using the span-rule (6).  The computation of the span-rule (6) involves computing the span $ (5) for every support  vector. Note, however, that we are interested in the inequality $v 2 < /vf(xv)/a, rather  than the exact value of the span $v. Thus, while minimizing Sv = d(xv, Av), if we find a  point x*  Av such that d(xv,x*) 2 <_ /vf(xv)/ct, we can stop the minimization because  this point will be correctly classified by the leave-one-out procedure.  It turned out in the experiments that the time required to compute the span was not pro-  hibitive, since it is was about the same than the training time.  There is a noteworthy extension in the application of the span concept. If we denote by  0 one hyperparameter of the kernel and if the derivative OK(x,,x) is computable, then it  o0  is possible to compute analytically o E a$-I(x) which is the derivative of an upper  00 '  bound of the number of errors made by the leave-one-out procedure (see Theorem 2). This  provides us a more powerful technique in model selection. Indeed, our initial approach  was to choose the value of the width cr of the RBF kernel according to the minimum of  the span-rule. In our case, there was only hyperparamter so it was possible to try different  values of a. But, if we have several hyperparameters, for example one cr per component,  - E ("  K(x, x') = e ' , it is not possible to do an exhaustive search on all the possible  values of of the hyperparameters. Nevertheless, the previous remark enables us to find their  optimal value by a classical gradient descent approach.  Preliminary results seem to show that using this approach with the previously mentioned  kernel improve the test error significantely.  5.2 VC dimension with rescaling  In this section, we perform model selection on the postal database using functional (1) and  its rescaled version. Figure 3a shows the values of the classical bound 2/M2 for different  values of a. This bound predicts the correct value for the minimum, but does not reflect the  actual test error. This is easily understandable since for large values of or, the data in input  space tend to be mapped in a very flat ellipsoid in feature space, a fact which is not taken  into account [4]. Figure 3b shows that by performing a rescaling of our data, we manage  to have a much tighter bound and this curve reflects the actual test error, given in figure 2a.  236 O. Chapelle and V. N. Vapnik  18000  16000  14000  12000  10000  .ooo I  6000[  4000[  [ ' VC Dirensi_[  I  - -2 0 2 4  Log sigma  (a) without rescaling  120  100  80  ._  7o 60  40  20   '1 V( Dimension with rscalincj  01  Log sigma  (b) with rescaling  Figure 3: Bound on the VC dimension for different values of or on the postal database. The  shape of the curve with rescaling is very similar to the test error on figure 2.  6 Conclusion  In this paper, we introduced two new techniques of model selection for SVMs. One is based  on the span, the other is based on rescaling of the data in feature space. We demonstrated  that using these techniques, one can both predict optimal values for the parameters of the  model and evaluate relative performances for different values of the parameters. These  functionals can also lead to new learning techniques as they establish that generalization  ability is not only due to margin.  Acknowledgments  The authors would like to thank Jason Weston and Patrick Haffner for helpfull discussions  and comments.  References  [l] C.J.C. Burges. A tutorial on support vector machines for pattern recognition. Data Mining and  Knowledge Discovery, 2(2): 121-167, 1998.  [2] T S. Jaakkola and D. Haussler. Probabilistic kernel regression models. In Proceedings of the  1999 Conference on AI and Statistics, 1999.  [3] M. Opper and O. Winther. Gaussian process classification and SVM: Mean field results and  leave-one-out estimator. In Advances in Large Margin Classifiers. MIT Press, 1999. to appear.  [4] B. SchOlkopf, J. Shawe-Taylor, A. J. Smola, and R. C. Williamson. Kernel-dependent Support  Vector error bounds. In Ninth International Conference on Artificial Neural Networks, pp. 304 -  309  [5] B. SchOlkopf, A. Smola, and K.-R. MQller. Kernel principal component analysis. In Artifi-  cial Neural Networks -- ICANN'97, pages 583 - 588, Berlin, 1997. Springer Lecture Notes in  Computer Science, Vol. 1327.  [6] V. Vapnik. Statistical Learning Theory. Wiley, New York, 1998.  [7] V. Vapnik and O. Chapelle. Bounds on error expectation for SVM. Neural Computation, 1999.  Submitted.  
Learning sparse codes with a  mixture-of-Gaussians prior  Bruno A. Olshausen  Department of Psychology and  Center for Neuroscience, UC Davis  1544 Newton Ct.  Davis, CA 95616  baolshausenucdavis. edu  K. Jarrod Millman  Center for Neuroscience, UC Davis  1544 Newton Ct.  Davis, CA 95616  kjmillmanucdavis. edu  Abstract  We describe a method for learning an overcomplete set of basis  functions for the purpose of modeling sparse structure in images.  The sparsity of the basis function coefficients is modeled with a  mixture-of-Gaussians distribution. One Gaussian captures non-  active coefficients with a small-variance distribution centered at  zero, while one or more other Gaussians capture active coefficients  with a large-variance distribution. We show that when the prior is  in such a form, there exist efficient methods for learning the basis  functions as well as the parameters of the prior. The performance  of the algorithm is demonstrated on a number of test cases and  also on natural images. The basis functions learned on natural  images are similar to those obtained with other methods, but the  sparse form of the coefficient distribution is much better described.  Also, since the parameters of the prior are adapted to the data, no  assumption about sparse structure in the images need be made a  priori, rather it is learned from the data.  I Introduction  The general problem we address here is that of learning a set of basis functions  for representing natural images efficiently. Previous work using a variety of opti-  mization schemes has established that the basis functions which best code natural  images in terms of sparse, independent components resemble a Gabor-like wavelet  basis in which the basis functions are spatially localized, oriented and bandpass in  spatial-frequency [1, 2, 3, 4]. In order to tile the joint space of position, orienta-  tion, and spatial-frequency in a manner that yields useful image representations,  it has also been advocated that the basis set be overcomplete [5], where the num-  ber of basis functions exceeds the dimensionality of the images being coded. A  major challenge in learning overcomplete bases, though, comes from the fact that  the posterior distribution over the coefficients must be sampled during learning.  When the posterior is sharply peaked, as it is when a sparse prior is imposed, then  conventional sampling methods become especially cumbersome.  842 B. A. Olshausen and K. J. Millman  One approach to dealing with the problems associated with overcomplete codes and  sparse priors is suggested by the form of the resulting posterior distribution over the  coefficients averaged over many images. Shown below is the posterior distribution of  one of the coefficients in a 4 x's overcomplete representation. The sparse prior that  was imposed in learning was a Cauchy distribution and is overlaid (dashed line). It  would seem that the coefficients do not fit this imposed prior very well, and instead  want to occupy one of two states: an inactive state in which the coefficient is set  nearly to zero, and an active state in which the coefficient takes on some significant  non-zero value along a continuum. This suggests that the appropriate choice of  prior is one that is capable of capturing these two discrete states.  10"  -4 -2 0 2 4  coeffx:tenl vaJue  Figure 1: Posterior distribution of coefficients with Cauchy prior overlaid.  Our approach to modeling this form of sparse structure uses a mixture-of-Gaussians  prior over the coefficients. A set of binary or ternary state variables determine  whether the coefficient is in the active or inactive state, and then the coefficient  distribution is Gaussian distributed with a variance and mean that depends on  the state variable. An important advantage of this approach, with regard to the  sampling problems mentioned above, is that the use of Gaussian distributions allows  an analytical solution for integrating over the posterior distribution for a given  setting of the state variables. The only sampling that needs to be done then is over  the binary or ternary state variables. We show here that this problem is a tractable  one. This approach differs from that taken previously by Attias [6] in that we do  not use variational methods to approximate the posterior, but rather we rely on  sampling to adequately characterize the posterior distribution over the coefficients.  2 Mixture-of-Gaussians model  An image, I(x, y), is modeled as a linear superposition of basis functions, qbi(x, y),  with coefficients ai, plus Gaussian noise v(x,y):  I(x,y) = E ai q3i(x,y) + v(x,y) (1)  i  In what follows this will be expressed in vector-matrix notation as I - (I, a + v.  The prior probability distribution over the coefficients is factoffal, with the distri-  bution over each coefficient ai modeled as a mixture-of-Gaussians distribution with  either two or three Gaussians (fig. 2). A set of binary or ternary state variables si  then determine which Gaussian is used to describe the coefficients.  The total prior over both sets of variables, a and s, is of the form  P(a, s) = H P(ailsi)P(si) (2)  i  Learning Sparse Codes with a Mixture-of-Gaussians Prior 843  Two Gaussians  (binary state variables)  Three Gaussians  (ternary state variables)  si=-I  P(ai)  si--O  si=l   a i  Figure 2: Mixture-of-Gaussians prior.  where P(si) determines the probability of being in the active or inactive states, and  P(ailsi) is a Gaussian distribution whose mean and variance is determined by the  current state si.  The total image probability is then given by  (3)  where  P(ila, O) _ 1 __ ii_al =  - --e , (4)  P(als, O) 1 _ 1_. (a_/(s))t Aa(s) (a--/(s))  = --e  (5)  ZA(s)  r(10) -  -' '   -  (6)  ZA  and the parameters O include )v, I,, Aa(s), p(s), and As. Aa(s) is a diagonal in-  verse covariance matrix with elements Aa(s)ii = )a (si). (The notations Aa(s) and  p(s) are used here to explicitly reflect the dependence of the means and variances  of the ai on si.) As is also diagonal (for now) with elements Asii = )s. The model  is illustrated graphically in figure 3.  ternary)  Figure 3: Image model.  844 B. A. Olshausen and K. J. Millman  3 Learning  The objective function for learning the parameters of the model is the average  log-likelihood:  C = (logP(II0)) (7)  Maximizing this objective will minimize the lower bound on coding length.  Learning is accomplished via gradient ascent on the objective, 12. The learning rules  for the parameters As, Aa(s), it(s) and (I, are given by:  = 1 (8)  2  0i  _ 1 [(*(Si -- U)}p(slI,O )  (*(s - u) (K(u) - 2a(u)m(u) + u(u)))r(li,0)] (0)  0i  m(u)  Ore(u)  = - - (10)  0i  0  where u takes on values 0,1 (binary) or -1,0,1 (ternary) and K(s) = H-(s) +  &(s) &(s) r. (& and H are defined in eqs. 15 and 16 in the next section.) Note  that in these expressions we have dropped the outer brackets averaging over images  simply to reduce clutter.  Thus, for each image we must sample from the posterior P(slI, 0) in order to collect  the appropriate statistics needed for learning. These statistics must be accumulated  over many different images, and then the parameters are updated according to the  rules above. Note that this approach differs from that of Atti [6] in that we do  not attempt to sum over all states, s, or to use the variational approximation to  approximate the posterior. Instead, we are effectively summing only over those  states that are most probable according to the posterior. We conjecture that this  scheme will work in practice because the posterior has significant probability only  for a small fraction of states s, and so it can be well-characterized by a relatively  small number of samples. Next we present an efficient method for Gibbs sampling  from the posterior.  4 Sampling and inference  In order to sample from the posterior P(s]I, 0), we first cast it in Boltzmann form:  P(s[I, 0) o e -E(s) (12)  where  E(s) = - log P(s, I[0) = - logP(s10 ) / P(Ila , 0)P(a[s, O)da  Learning Sparse Codes with a Mixture-of-Gaussians Prior 845  and  1  : $T As s + log ZAa(s) + Eals(&, s) +  log det H(s) + const.  Eals(a, s)  (13)  VVaEals(a, s ) = NTI  q- Aa(s)  I )T  _ AN II - 'I'al 2 + (a- p(s) Aa(s) (a- p(s)) (14)  = 2  = argmin Eals(a, s) (15)  a  = (16)  Gibbs-sampling on P(s[I, 0) can be performed by flipping state variables si accord-  ing to  P(si 4- s ) =  P(si 4- s ) =  I (binary) (17)  lq_eAE(sis a)  1  l+eA!(,i,)[l+e_a!(,i,) ] (ternary) (18)  Where s a = s-7 in the binary case, and s a and s  are the two alternative states in  the ternary case. AE(si 4- s ) denotes the change in E(s) due to changing si to  s  and is given by:  1  AE(si 4-s a) --   A,,(si) + Asi As, + log(1 + AA,,J/i) +  log ha, (s (a))  ^2 _ 2tiAv i _ JiiAv ]  Aalai 1 + AA, Jii + A(iVi)  (19)  where Asi = s  - si, AA, = A(s ) - A,(si), J = H -I, and vi = Aai($i)i($i).  Note that all computations for considering a change of state are local and involve  only terms with index i. Thus, deciding whether or not to change state can be  computed quickly. However, if a change of state is accepted, then we must update  J. Using the Sherman-Morrison formula, this can be kept to an O(N 2) computation:  J 4- J- 1 + A.a J J jT (20)  As long as accepted state changes are rare (which we have found to be the case for  sparse distributions), then Gibbs sampling may be performed quickly and efficiently.  In addition, H and J are generally very sparse matrices, so as the system is scaled  up the number of elements of a that are affected by a flip of si will be relatively  few.  In order to code images under this model, a single state of the coefficients must be  chosen for a given image. We use for this purpose the MAP estimator:  & = argmaxP(alI,.,O ) (21)  . = argmaxP(slI, 0 ) (22)  Maximizing the posterior distribution over s is accomplished by assigning a tem-  perature,  P(slI, 0) cre  (23)  and gradually lowering it until there are no more state changes.  846 B. .4. Olshausen and K. d. Mllman  5 Results  5.1 Test cases  We first trained the algorithm on a number of test cases containing known forms  of both sparse and non-sparse (bi-modal) structure, using both critically sampled  (complete) and 2x's overcomplete basis sets. The training sets consisted of 6x6  pixel image patches that were created by a sparse superposition of basis functions  (36 or 72) with P([sil = 1) = 0.2, ai(0) = 1000, and a(1) = 10. The results of  these test cases confirm that the algorithm is capable of correctly extracting both  sparse and non-sparse structure from data, and they are not shown here for lack of  space.  5.2 Natural images  We trained the algorithm on 8x8 image patches extracted from pre-whitened natural  images. In all cases, the basis functions were initialized to random functions (white  noise) and the prior was initialized to be Gaussian (both Gaussians of roughly equal  variance). Shown in figure 4a, b are the results for a set of 128 basis functions (2 x's  overcomplete) in the two-Gausian case. In the three-Gaussian case, the prior was  initialized to be platykurtic (all three Gaussians of equal variance but offset at  three different positions). Thus, in this case the sparse form of the prior emerged  completely from the data. The resulting priors for two of the coefficients are shown  in figure 4c, with the posterior distribution averaged over many images overlaid. For  some of the coefficients the posterior distribution matches the mixture-of-Gaussians  prior well, but for others the tails appear more Laplacian in form. Also, it appears  that the extra complexity offered by having three Gaussians is not utilized: Both  Gaussians move to the center position and have about the same mean. When a  non-sparse, bimodal prior is imposed, the basis function solution does not become  localized, oriented, and bandpass as it does with sparse priors.  5.3 Coding efficiency  We evaluated the coding efficiency by quantizing the coefficients to different levels  and calculating the total coefficient entropy as a function of the distortion intro-  duced by quantization. This was done for basis sets containing 48, 64, 96, and  128 basis functions. At high SNR's the overcomplete basis sets yield better coding  efficiency, despite the fact that there are more coefficients to code. However, the  point at which this occurs appears to be well beyond the point where errors are no  longer perceptually noticeable (around 14 dB).  6 Conclusions  We have shown here that both the prior and basis functions of our image model  can be adapted to natural images. Without sparseness being imposed, the model  both seeks distributions that are sparse and learns the appropriate basis functions  for this distribution. Our conjecture that a small number of samples allows the  posterior to be sufficiently characterized appears to hold. In all cases here, aver-  ages were collected over 40 Gibbs sweeps, with 10 sweeps for initialization. The  algorithm proved capable of extracting the structure in challenging datasets in high  dimensional spaces.  The overcomplete image codes have the lowest coding cost at high SNR levels, but  at levels that appear higher than is practically useful. On the other hand, the  Learning Sparse Codes with a Mixture-of-Gaussians Prior 847  SNR  Figure 4: An overcomplete set of 128 basis functions (a) and priors (b, vertical axis  is log-probability) learned from natural images. c, Two of the priors learned from  a three-Gaussian mixture using 64 basis functions, with the posterior distribution  averaged over many coefficients overlaid. d, Rate distortion curve comparing the  coding efficiency of different learned basis sets.  sum of marginal entropies likely underestimates the true entropy of the coefficients  considerably, as there are certainly statistical dependencies among the coefficients.  So it may still be the case that the overcomplete bases will show a win at lower  SNR's when these dependencies are included in the model (through the coupling  term As).  Acknowledgments  This work was supported by NIH grant R29-MH057921.  References  [1] Olshausen BA, Field DJ (1997) "Sparse coding with an overcomplete basis set: A  strategy employed by VI?" Vision Research, 37: 3311-3325.  [2] Bell A J, Sejnowski TJ (1997) "The independent components of natural images are  edge filters," Vision Research, 37: 3327-3338.  [3] van Hateten JH, van der Schaaff A (1997) "Independent component filters of natural  images compared with simple cells in primary visual cortex," Proc. Royal Soc. Lond.  B, 265: 359-366.  [4] Lewicki MS, Olshausen BA (1999) "A probabilistic framework for the adaptation and  comparison of image codes," JOSA A, 16(7): 1587-1601.  [5] Simoncelli EP, Freeman WT, Adelson EH, Heeger DJ (1992) "Shiftable multiscale  transforms," IEEE Transactions on Information Theory, 38(2): 587-607.  [6] Attias H (1999) "Independent factor analysis," Neural Computation, 11: 803-852.  
Statistical Dynamics of Batch Learning  S. Li and K. Y. Michael Wong  Department of Physics, Hong Kong University of Science and Technology  Clear Water Bay, Kowloon, Hong Kong  phlisong, phkywong) @ust. hk  Abstract  An important issue in neural computing concerns the description of  learning dynamics with macroscopic dynamical variables. Recen-  t progress on on-line learning only addresses the often unrealistic  case of an infinite training set. We introduce a new framework to  model batch learning of restricted sets of examples, widely applica-  ble to any learning cost function, and fully taking into account the  temporal correlations introduced by the recycling of the examples.  For illustration we analyze the effects of weight decay and early  stopping during the learning of teacher-generated examples.  I Introduction  The dynamics of learning in neural computing is a complex multi-variate process.  The interest on the macroscopic level is thus to describe the process with macro-  scopic dynamical variables. Recently, much progress has been made on modeling  the dynamics of on-line learning, in which an independent example is generated for  each learning step [1, 2]. Since statistical correlations among the examples can be  ignored, the dynamics can be simply described by instantaneous dynamical vari-  ables.  However, most studies on on-line learning focus on the ideal case in which the net-  work has access to an almost infinite training set, whereas in many applications,  the collection of training examples may be costly. A restricted set of examples  introduces extra temporal correlations during learning, and the dynamics is much  more complicated. Early studies briefly considered the dynamics of Adaline learn-  ing [3, 4, 5], and has recently been extended to linear percepttons learning nonlinear  rules [6, 7]. Recent attempts, using the dynamical replica theory, have been made  to study the learning of restricted sets of examples, but so far exact results are pub-  lished for simple learning rules such as Hebbian learning, beyond which appropriate  approximations are needed [8].  In this paper, we introduce a new framework to model batch learning of restricted  sets of examples, widely applicable to any learning rule which minimizes an arbi-  trary cost function by gradient descent. It fully takes into account the temporal  correlations during learning, and is therefore exact for large networks.  Statistical Dynamics of Batch Learning 287  2 Formulation  Consider the single layer perceptron with N >> I input nodes { } connecting to a  single output node by the weights {Jj}. For convenience we assume that the inputs  j are Gaussian variables with mean 0 and variance 1, and the output state $ is a  function f(x) of the activation x at the output node, i.e.  S=f(x); x=f-'. (1)  The network is assigned to "learn" p -_- aN examples which map inputs {?} to the  outputs {S} (p = 1,... ,p). S are the outputs generated by a teacher perceptron  {Bj }, namely  S = f(y,); y =/. '". (2)  Batch learning by gradient descent is achieved by adjusting the weights {Jj} itera-  tively so that a certain cost function in terms of the student and teacher activations  {x) and {y) is minimized. Hence we consider a general cost function  E- -Eg(x,y). (3)  The precise functional form of g(x, y) depends on the adopted learning algorithm.  For the case of binary outputs, f(x) -- sgnx. Early studies on the learning dynamics  considered Adaline learning [3, 4, 5], where g(x,y) -- -(S- x)2/2 with S - sgny.  For recent studies on Hebbian learning [8], g(x, y) -- xS.  To ensure that the perceptton is regularized after learning, it is customary to intro-  duce a weight decay term. Furthermore, to avoid the system being trapped in local  minima, noise is often added in the dynamics. Hence the gradient descent dynamics  is given by  dJj(t) 1  dt =  Eg'(x(t),y) - AJj(t) + ]j(t), (4)  where, here and below, g'(x,y) and g"(x,y) respectively represent the first and  second partial derivatives of g(x, y) with respect to x. A is the weight decay strength,  and ]j (t) is the noise term at temperature T with  2T  and = 5kS(t- s).  (5)  3 The Cavity Method  Our theory is the dynamical version of the cavity method [9, 10, 11]. It uses a  self-consistency argument to consider what happens when a new example is added  to a training set. The central quantity in this method is the cavity activation, which  is the activation of a new example for a perceptron trained without that example.  Since the original network has no information about the new example, the cavity  activation is stochastic. Specifically, denoting the new example by the label 0, its  cavity activation at time t is  ho(t) = J(t). (6)  For large N and independently generated examples, ho(t) is a Gaussian variable.  Its covariance is given by the correlation function C(t, s) of the weights at times t  and s, that is,  /ho(t)o(s)) = f(t). f(s) -- C(t,s),  288 S. Li and K. Y.. M. Wong  where ? and  are assumed to be independent for j  k. The distribution is  further specified by the teacher-student correlation R(t), given by  (ho(t)yo} - f(t) . t -- R(t).  (8)  Now suppose the perceptron incorporates the new example at the batch-mode learn-  ing step at time s. Then the activation of this new example at a subsequent time  t > s will no longer be a random variable. Furthermore, the activations of the  original p examples at time t will also be adjusted from (x(t)) to (x,(t)) because  of the newcomer, which will in turn affect the evolution of the activation of example  0, giving rise to the so-called Onsager reaction effects. This makes the dynamics  complex, but fortunately for large p  N, we can assume that the adjustment from  o (t) is small, and perturbative analysis can be applied.  x(t) to x,  Suppose the weights of the original and new perceptron at time t are {Jj(t)} and  {J(t)} respectively. Then a perturbation of (4) yields  d  I , o  q- A (Jj(t)- Jj(t)) - g (xo(t),yo)  1  - (k(t) - A(t))  + N Eg"(x"(t)'  J .  k  (9)  The first term on the right hand side describes the primary effects of adding example  0 to the training set, and is the driving term for the difference between the two  perceptrons. The second term describes the secondary effects due to the changes  to the original examples caused by the added example, and is referred to as the  Onsager reaction term. One should note the difference between the cavity and  generic activations of the added example. The former is denoted by ho(t) and  corresponds to the activation in the perceptron {Jj (t)}, whereas the latter, denoted  by xo (t) and corresponding to the activation in the perceptron {Jj (t)}, is the one  used in calculating the gradient in the driving term of (9). Since their notations  are sufficiently distinct, we have omitted the superscript 0 in xo(t), which appears  (t)  in the background examples x .  The equation can be solved by the Green's function technique, yielding  f (1, O)  Jj(t)- Sj(t)- E dsGjk(t,s) go(s)k ,  k  (10)  where g)(s) = g'(xo (s), yo) and Gin (t, s) is the weight Green's function satisfying  1 /  = - - t s),  Gjn(t,s) G()(t s)6 +   dt'G()(t ' "  (11)  G () (t - s) -- O(t - s)exp(-A(t - s)) is the bare Green's function, and 0 is the  step function. The weight Green's function describes how the effects of example 0  propagates from weight Jn at learning time s to weight Jj at a subsequent time t,  including both primary and secondary effects. Hence all the temporal correlations  have been taken into account.  For large N, the equation can be solved by a diagrammatic approach similar to [5].  The weight Green's function is self-averaging over the distribution of examples and  is diagonal, i.e. limN-+oo Gj(t, s) = G(t, s)5j, where  G(t,s): G''(t- s)q-or / dt I / dt2''(t- tl)(g(tl)D(tl,t2))(t2,s ). (12)  Statistical Dynamics of Batch Learning 289  D(t, s) is the example Green's function given by  Vu(t,s) = 6(t- s) + f dt'G(t,t')g(t')Du(t',s). (13)  This allows us to express the generic activations of the examples in terms of their  cavity counterparts. Multiplying both sides of (10) and summing over j, we get  xo(t) - no(t) = / dsG(t,s)g(s). (14)  This equation is interpreted as follows. At time t, the generic activation xo(t)  deviates from its cavity counterpart because its gradient term g(s) was present  in the batch learning step at previous times s. This gradient term propagates its  influence from time s to t via the Green's function G(t, s). Statistically, this equation  enables us to express the activation distribution in terms of the cavity activation  distribution, thereby getting a macroscopic description of the dynamics.  To solve for the Green's functions and the activation distributions, we further need  the fluctuation-response relation derived by linear response theory,  C(t,s) a/dt'G()(t t"' ''t"x f  = - );g. ) .s)) + 2T dt'G()(t - t')G(s,t'). (15)  Finally, the teacher-student correlation is given by  R(t) a f dt'G()(t t') ' t'  = - (g,()y,). (16)  4 A Solvable Case  The cavity method can be applied to the dynamics of learning with an arbitrary cost  function. When it is applied to the Hebb rule, it yields results identical to the exact  results in [8]. Here we present the results for the Adaline rule to illustrate features  of learning dynamics derivable from the study. This is a common learning rule and  bears resemblance with the more common back-propagation rule. Theoretically, its  dynamics is particularly convenient for analysis since g"(x) = -1, rendering the  weight Green's function time translation invariant, i.e. G(t, s) = G(t - s). In this  case, the dynamics can be solved by Laplace transform.  To monitor the progress of learning, we are interested in three performance mea-  sures: (a) Training error et, which is the probability of error for the training ex-  amples. It is given by et = (O(-xsgny)), where the average is taken over the  joint distribution p(x, y) of the training set. (b) Test error Crest, which is the prob-  ability of error when the inputs ' of the training examples are corrupted by an  additive Gaussian noise of variance A 2. This is a relevant performance measure  when the perceptron is applied to process data which are the corrupted versions of  the training data. It is given by Crest = (H(xsgny/ACxf,t))). When A 2 = 0,  the test error reduces to the training error. (c) Generalization error %, which is the  probability of error for an arbitrary input j when the teacher and student outputs  are compared. It is given by eg = arccos[R(t)/Cxf , t)]/r.  Figure l(a) shows the evolution of the generalization error at T - 0. When the  weight decay strength varies, the steady-state generalization error is minimized at  the optimum  ,opt ----  -- 1, (17)  290 $. Li and K. Y.. M. Wong  which is independent of a. It is interesting to note that in the cases of the linear  perceptton, the optimal weight decay strength is also independent of a and only  determined by the output noise and unlearnability of the examples [5, 7]. Simi-  larly, here the student is only provided the coarse-grained version of the teacher's  activation in the form of binary bits.  For/  ,opt, the generalization error is a non-monotonic function in learning time.  Hence the dynamics is plagued by overtraining, and it is desirable to introduce early  stopping to improve the perceptron performance. Similar behavior is observed in  linear percepttons [5, 6, 7].  To verify the theoretical predictions, simulations were done with N - 500 and using  50 samples for averaging. As shown in Fig. l(a), the agreement is excellent.  Figure l(b) compares the generalization errors at the steady-state and the early  stopping point. It shows that early stopping improves the performance for   'opt,  which becomes near-optimal when compared with the best result at  =/opt. Hence  early stopping can speed up the learning process without significant sacrifice in the  generalization ability. However, it cannot outperform the optimal result at steady-  state. This agrees with a recent empirical observation that a careful control of the  weight decay may be better than early stopping in optimizing generalization [12].  0.40 0.38  0.36  0.32  0.28  0.24  0.20  c-0.5 3.=10  3.=k,  a=l.2 3.=10  '.' ............... 3.01  res  0.36 L  0.34  0.32  0.30  0.28  0.26  0.24  te,  a--0.5  ' u '   ' 0.22  ............  J  2 4 6 8 10 12 0.0 0.5 1.0 1.5  time t weight decay .  Figure 1: (a) The evolution of the generalization error at T - 0 for a -- 0.5, 1.2  and different weight decay strengths A. Theory: solid line, simulation: symbols.  (b) Comparing the generalization error at the steady state (oc) and at the early  stopping point (res) for a -- 0.5, 1.2 and T - 0.  In the search for optimal learning algorithms, an important consideration is the  environment in which the performance is tested. Besides the generalization per-  formance, there are applications in which the test examples have inputs correlated  with the training examples. Hence we are interested in the evolution of the test  error for a given additive Gaussian noise A in the inputs. Figure 2(a) shows, again,  that there is an optimal weight decay parameter/opt which minimizes the test error.  Furthermore, when the weight decay is weak, early stopping is desirable.  Figure 2(b) shows the value of the optimal weight decay as a function of the input  noise variance A 2. To the lowest order approximation, 'opt O( h 2 for sufficiently  large A 2. The dependence of ,opt on input noise is rather general since it also holds  in the case of random examples [13]. In the limit of small A 2, 'opt vanishes as  A 2 for a < 1, whereas ,opt approaches a nonzero constant for a > 1. Hence for  Statistical Dynamics of Batch Learning 291  c < 1, weight decay is not necessary when the training error is optimized, but when  the perceptton is applied to process increasingly noisy data, weight decay becomes  more and more important in performance enhancement.  Figure 2(b) also shows the phase line /kot(A 2) below which overtraining occurs.  Again, to the lowest order approximation, Aot cr A 2 for sufficiently large A 2. How-  ever, unlike the case of generalization error, the line for the onset of overtraining  does not coincide exactly with the line of optimal weight decay. In particular, for  an intermediate range of input noise, the optimal line lies in the region of over-  training, so that the optimal performance can only be attained by tuning both the  weight decay strength and learning time. However, at least in the present case,  computational results show that the improvement is marginal.  0.30 F 20  0.28 '-  0.28 ' k=3.6  0.24 r- ' 10  0.22 ,L  0.20  0 1 2 3 4 5  Time  1 2 3 4  1  Figure 2: (a) The evolution of the test error for A 2 = 3, T = 0 and different weight  decay strengths A ()opt ' 1.5, 3.6 for a = 0.5, 1.2 respectively). (b) The lines of  the optimal weight decay and the onset of overtraining for a - 5. Inset: The same  data with Aot - ,opt (magnified) versus A 2.  5 Conclusion  Based on the cavity method, we have introduced a new framework for modeling the  dynamics of learning, which is applicable to any learning cost function, making it  a versatile theory. It takes into full account the temporal correlations generated by  the use of a restricted set of examples, which is more realistic in many situations  than theories of on-line learning with an infinite training set.  While the Adaline rule is solvable by the cavity method, it is still a relatively  simple model approachable by more direct methods. Hence the justification of the  method as a general framework for learning dynamics hinges on its applicability to  less trivial cases. In general, g"(t') in (13) is not a constant and Du(t,s) has to  be expanded as a series. The '3;namical equations can then be considered as the  starting point of a perturbation theory, and results in various limits can be derived,  e.g. the limits of small a, large a, large A, or the asymptotic limit. Another area for  the useful application of the cavity method is the case of batch learning with very  large learning steps. Since it has been shown recently that such learning converges  in a few steps [6], the dynamical equations remain simple enough for a meaningful  study. Preliminary results along this direction are promising and will be reported  elsewhere.  292 S. Li and K. Y. M. Wong  An alternative general theory for learning dynamics, the dynamical replica theo-  ry, has recently been developed [8]. It yields exact results for Hebbian learning,  and approximate results for more non-trivial cases. Based on certain self-averaging  assumptions, the theory is able to approximate the dynamics by the evolution of  single-time functions, at the expense of having to solve a set of saddle point equa-  tions in the replica formalism at every learning instant. On the other hand, our  theory retains the functions G(t, s) and C(t, s) with double arguments, but devel-  ops naturally from the stochastic nature of the cavity activations. Contrary to a  suggestion [14], the cavity method can also be applied to the on-line learning with  restricted sets of examples. It is hoped that by adhering to an exact formalism,  the cavity method can provide more fundamental insights when the studies are  extended to more sophisticated multilayer networks of practical importance.  The method enables us to study the effects of weight decay and early stopping. It  shows that the optimal strength of weight decay is determined by the imprecision  in the examples, or the level of input noise in anticipated applications. For weaker  weight decay, the generalization performance can be made near-optimal by early  stopping. Furthermore, depending on the performance measure, optimality may  only be attained by a combination of weight decay and early stopping. Though  the performance improvement is marginal in the present case, the question remains  open in the more general context.  We consider the present work as the beginning of an in-depth study of learning  dynamics. Many interesting and challenging issues remain to be explored.  Acknowledgments  We thank A. C. C. Coolen and D. Saad for fruitful discussions during NIPS. This  work was supported by the grant HKUST6130/97P from the Research Grant Coun-  cil of Hong Kong.  References  [1] D. Saad and S. Solla, Phys. Rev. Left. 74, 4337 (1995).  [2] D. Saad and M. Rattray, Phys. Rev. Left. 79, 2578 (1997).  [3] J. Hertz, A. Krogh and G.I. Thorbergssen, J. Phys. A 22, 2133 (1989).  [4] M. Opper, Europhys. Left. 8, 389 (1989).  [5] A. Krogh and J. A. Hertz, J. Phys. A 25, 1135 (1992).  [6] S. BSs and M. Opper, J. Phys. A 31, 4835 (1998).  [7] S. BSs, Phys. Rev. E 58, 833 (1998).  [8] A. C. C. Coolen and D. Saad, in On-line Learning in Neural Networks, ed. D. Saad  (Cambridge University Press, Cambridge, 1998).  [9] M. Mzard, G. Parisi and M. Virasoro, Spin Glass Theory and Beyond (World Sci-  entific, Singapore) (1987).  [10] K. Y. M. Wong, Europhys. Left. 30, 245 (1995).  [11] K. Y. M. Wong, Advances in Neural Information Processing Systems 9, 302 (1997).  [12] L.K. Hansen, J. Larsen and T. Fog, IEEE Int. Conf. on Acoustics, Speech, and Signal  Processing 4, 3205 (1997).  [13] Y. W. Tong, K. Y. M. Wong and S. Li, to appear in Proc. of IJCNN'99 (1999).  [14] A. C. C. Coolen and D. Saad, Preprint KCL-MTH-99-33 (1999).  
Support Vector Method for Novelty Detection  Bernhard Schilkopf*, Robert Williamson,  Alex Smola, John Shawe-Taylor t, John Platt*  * Microsoft Research Ltd., 1 Guildhall Street, Cambridge, UK   Department of Engineering, Australian National University, Canberra 0200  t Royal Holloway, University of London, Egham, UK  * Microsoft, 1 Microsoft Way, Redmond, WA, USA  bsc/jplatt@microsoft.com, Bob. Williamson/Alex. Smola@anu.edu.au, john@dcs.rhbnc.ac.uk  Abstract  Suppose you are given some dataset drawn from an underlying probabil-  ity distribution P and you want to estimate a "simple" subset $ of input  space such that the probability that a test point drawn from P lies outside  of $ equals some a priori specified v between 0 and 1.  We propose a method to approach this problem by trying to estimate a  function f which is positive on $ and negative on the complement. The  functional form of f is given by a kernel expansion in terms of a poten-  tially small subset of the training data; it is regularized by controlling the  length of the weight vector in an associated feature space. We provide a  theoretical analysis of the statistical performance of our algorithm.  The algorithm is a natural extension of the support vector algorithm to  the case of unlabelled data.  1 INTRODUCTION  During recent years, a new set of kernel techniques for supervised learning has been devel-  oped [8]. Specifically, support vector (SV) algorithms for pattern recognition, regression  estimation and solution of inverse problems have received considerable attention. There  have been a few attempts to transfer the idea of using kernels to compute inner products  in feature spaces to the domain of unsupervised learning. The problems in that domain  are, however, less precisely specified. Generally, they can be characterized as estimating  functions of the data which tell you something interesting about the underlying distribu-  tions. For instance, kernel PCA can be characterized as computing functions which on the  training data produce unit variance outputs while having minimum norm in feature space  [4]. Another kernel-based unsupervised learning technique, regularized principal mani-  folds [6], computes functions which give a mapping onto a lower-dimensional manifold  minimizing a regularized quantization error. Clustering algorithms are further examples of  unsupervised learning techniques which can be kernelized [4].  An extreme point of view is that unsupervised learning is about estimating densities.  Clearly, knowledge of the density of P would then allow us to solve whatever problem  can be solved on the basis of the data. The present work addresses an easier problem: it  Support Vector Method for Novelty Detection 583  proposes an algorithm which computes a binary function which is supposed to capture re-  gions in input space where the probability density lives (its support), i.e. a function such  that most of the data will live in the region where the function is nonzero [5]. In doing so,  it is in line with Vapnik's principle never to solve a problem which is more general than the  one we actually need to solve. Moreover, it is applicable also in cases where the density  of the data's distribution is not even well-defined, e.g. if there are singular components.  Part of the motivation for the present work was the paper [ 1 ]. It turns out that there is a  considerable amount of prior work in the statistical literature; for a discussion, cf. the full  version of the present paper [3].  2 ALGORITHMS  We first introduce terminology and notation conventions. We consider training data  x,..., x 6 X, where  6 N is the number of observations, and X is some set. For  simplicity, we think of it as a compact subset of 11 N . Let  be a feature map X --> F,  i.e. a map into a dot product space F such that the dot product in the image of  can be  computed by evaluating some simple kernel [8]  k(x,y) = (rI,(x). rI,(y)),  (1)  such as the Gaussian kernel  k(x, y) = e -tlx-yl12/c. (2)  Indices i and j are understood to range over 1,...,  (in compact notation: i, j E []).  Bold face greek letters denote -dimensional vectors whose components are labelled using  normal face typeset.  In the remainder of this section, we shall develop an algorithm which returns a function  f that takes the value + 1 in a "small" region capturing most of the data points, and -1  elsewhere. Our strategy is to map the data into the feature space corresponding to the  kernel, and to separate them from the origin with maximum margin. For a new point x, the  value f(x) is determined by evaluating which side of the hyperplane it falls on, in feature  space. Via the freedom to utilize different types of kernel functions, this simple geometric  picture corresponds to a variety of nonlinear estimators in input space.  To separate the data set from the origin, we solve the following quadratic program:  l llwll2 q_ 1  min  7 -i i - P (3)  w6 F, 6lt t,p6R  subject to (w. (xi)) _> p- (i, (i > 0. (4)  Here, v E (0, 1) is a parameter whose meaning will become clear later. Since nonzero slack  variables i are penalized in the objective function, we can expect that if w and p solve this  problem, then the decision function f(x) -- sgn((w  rI,(x)) - p) will be positive for most  examples xi contained in the training set, while the SV type regularization term ][w[I will  still be small. The actual trade-off between these two goals is controlled by v. Deriving the  dual problem, and using (1), the solution can be shown to have an SV expansion  f(x)=sgn(i oik(xi,x)-p) (5)  (patterns xi with nonzero ai are called SVs), where the coefficients are found as the solu-  tion of the dual problem:  min 1 1  Ot  E OziOzjk(xi' xj ) subjectto0_<ai_<, EOzi----1. (6)  ij i  584 B. SchOlkopf, R. C. V'lliamson, .4. d. Smola, d. Shawe-Taylor and d.C. Platt  This problem can be solved with standard QP routines. It does, however, possess features  that sets it apart from generic QPs, most notably the simplicity of the constraints. This can  be exploited by applying a variant of SMO developed for this purpose [3].  The offset p can be recovered by exploiting that for any cti which is not at the upper or  lower bound, the corresponding pattern xi satisfies p = (w. (xi)) = y'j ctjk(x, xi).  Note that if v approaches 0, the upper boundaries on the Lagrange multipliers tend to infin-  ity, i.e. the second inequality constraint in (6) becomes void. The problem then resembles  the corresponding hard margin algorithm, since the penalization of errors becomes infinite,  as can be seen from the primal objective function (3). It can be shown that if the data set  is separable from the origin, then this algorithm will find the unique supporting hyperplane  with the properties that it separates all data from the origin, and its distance to the origin is  maximal among all such hyperplanes [3]. If, on the other hand, v approaches 1, then the  constraints alone only allow one solution, that where all cti are at the upper bound 1/(v).  In this case, for kernels with integral 1, such as normalized versions of (2), the decision  function corresponds to a thresholded Parzen windows estimator.  To conclude this section, we note that one can also use balls to describe the data in feature  space, close in spirit to the algorithms of [2], with hard boundaries, and [7], with "soft  margins." For certain classes of kernels, such as Gaussian RBF ones, the corresponding  algorithm can be shown to be equivalent to the above one [3].  3 THEORY  In this section, we show that the parameter v characterizes the fractions of SVs and outliers  (Proposition 1). Following that, we state a robustness result for the soft margin (Proposition  2) and error bounds (Theorem 5). Further results and proofs are reported in the full version  of the present paper [3]. We will use italic letters to denote the feature space images of the  corresponding patterns in input space, i.e. a:i := (xi).  Proposition 1 Assume the solution of(4) satisfies p  O. The following statements hold:  (i) u is an upper bound on the fraction of outliers.  (ii) u is a lower bound on the fraction of SVs.  (iii) Suppose the data were generated independently from a distribution P(x) which does  not contain discrete components. Suppose, moreover, that the kernel is analytic and non-  constant. With probability 1, asymptotically, v equals both the fraction of SVs and the  fraction of outliers.  The proof is based on the constraints of the dual problem, using the fact that outliers must  have Lagrange multipliers at the upper bound.  Proposition 2 Local movements of outliers parallel to w do not change the hyperplane.  We now move on to the subject of generalization. Our goal is to bound the probability  that a novel point drawn from the same underlying distribution lies outside of the estimated  region by a certain margin. We start by introducing a common tool for measuring the  capacity of a class  of functions that map 2C to 5L  Definition 3 Let (X, d) be a pseudo-metric space,  let A be a subset of X and e > O. A  set B C_ X is an e-cover for A if, for every a  A, there exists b  B such that d(a, b) _< e.  The e-covering number of A, 3fa(e, A), is the minimal cardinality of an e-cover for A (if  there is no such finite cover then it is defined to be c).  I i.e. with a distance function that differs from a metric in that it is only semidefinite  Support Vector Method for Novelty Detection 585  The idea is that B should be finite but approximate all of A with respect to the pseudometric  d. We will use the l distance over a finite sample X = (a:l,..., a:t) for the pseudo-  metric in the space of functions, dx(f,g) = maxi[t] If(xi) - g(xi)l. Let f(e, if, ) =  suPx6xt fax (e, if). Below, logarithms are to base 2.  Theorem 4 Consider any distribution P on 3J and any 0  ll Suppose Xl,..., X are  generated i.i.d. from P. Then with probability 1 - (5 over such an g-sample, if we find  f  IT such that f(xi) >_ 0 + '7for all i  [],  P{z 'f(z) < 0- '7} _< (k + log ),  where  We now consider the possibility that for a small number of points f(xi) fails to exceed  0 + '7. This corresponds to having a non-zero slack variable i in the algorithm, where we  take 0 + '7 = p/llwll and use the class of linear functions in feature space in the application  of the theorem. There are well-known bounds for the log covering numbers of this class.  Let f be a real valued function on a space 3J. Fix 0 E 1tL For a: E 2C, define  d(x, f, '7) = max{0, 0 + '7- f(x)}.  Similarly for a training sequence X, we define D(X, f, '7) = 5'zex d(z, f, '7).  Theorem 5 Fix 0  ll Consider a fixed but unknown probability distribution P on the  input space 3J and a class of real valued functions IT with range [a, b]. Then with probability  1 - (5 over randomly drawn training sequences c of size , for all '7 > 0 and any f  IT,  P {a:: f(a:) < 0 - '7 and c  X} _< (k + log ),  64(b-a)v(x,f,,) *7 '1 ae(b-a) 2  where k= [logXf('7/2, IT, 2)+ .2 log ( log ( .e  The theorem bounds the probability of a new point falling in the region for which f(z)  has value less than 0 - '7, this being the complement of the estimate for the support of the  distribution. The choice of'7 gives a trade-off between the size of the region over which the  bound holds (increasing '7 increases the size of the region) and the size of the probability  with which it holds (increasing '7 decreases the size of the log covering numbers).  The result shows that we can bound the probability of points falling outside the region of  estimated support by a quantity involving the ratio of the log covering numbers (which can  be bounded by the fat shattering dimension at scale proportional to '7) and the number of  training examples, plus a factor involving the 1-norm of the slack variables. It is stronger  than related results given by [ 1 ], since their bound involves the square root of the ratio of  the Pollard dimension (the fat shattering dimension when '7 tends to 0) and the number of  training examples.  The output of the algorithm described in Sec. 2 is a function f(z) = Yi aik(zi, z) which  is greater than or equal to p - i on example xi. Though non-linear in the input space, this  function is in fact linear in the feature space defined by the kernel k. At the same time the  2-norm of the weight vector is given by B - arx/-7'-, and so we can apply the theorem  with the function class IT being those linear functions in the feature space with 2-norm  bounded by B. If we assume that 0 is fixed, then '7 = p - 0, hence the support of the  distribution is the set {z  f(z) >_ 0 - '7 = 20 - p}, and the bound gives the probability of  a randomly generated point falling outside this set, in terms of the log covering numbers of  the function class IT and the sum of the slack variables i. Since the log covering numbers  586 B. SchOlkopf, R. C. }qlliamson, .,t. d. Smola, d. Shawe-Taylor and d.C. Platt  at scale 7/2 of the class ff can be bounded by O( R,--  log 2 ) this gives a bound in terms  of the 2-norm of the weight vector.  Ideally, one would like to allow 0 to be chosen after the value of p has been determined,  perhaps as a fixed fraction of that value. This could be obtained by another level of struc-  tural risk minimisation over the possible values of p or at least a mesh of some possible  values. This result is beyond the scope of the current preliminary paper, but the form of the  result would be similar to Theorem 5, with larger constants and log factors.  Whilst it is premature to give specific theoretical recommendations for practical use yet,  one thing is clear from the above bound. To generalize to novel data, the decision function  to be used should employ a threshold r/ p, where r/< 1 (this corresponds to a nonzero  4 EXPERIMENTS  We apply the method to artificial and real-world data. Figure 1 displays 2-D toy examples,  and shows how the parameter settings influence the solution.  Next, we describe an experiment on the USPS dataset of handwritten digits. The database  contains 9298 digit images of size 16 x 16 = 256; the last 2007 constitute the test set. We  trained the algorithm, using a Gaussian kernel (2) of width c = 0.5  256 (a common value  for SVM classifiers on that data set, cf. [2]), on the test set and used it to identify outliers  it is folklore in the community that the USPS test set contains a number of patterns  which are hard or impossible to classify, due to segmentation errors or mislabelling. In the  experiment, we augmented the input patterns by ten extra dimensions corresponding to the  class labels of the digits. The rationale for this is that if we disregarded the labels, there  would be no hope to identify mislabelled patterns as outliers. Fig. 2 shows the 20 worst  outliers for the USPS test set. Note that the algorithm indeed extracts patterns which are  very hard to assign to their respective classes. In the experiment, which took 36 seconds on  a Pentium II running at 450 MHz, we used a v value of 5%.  ,, width c  frac. SVs/OLs  margin P/llwll  0.5, 0.5 I 0.5, 0.5 0.1, 0.5  0.54, 0.43 0.59, 0.47 0.24, 0.03  0.84 0.70 0.62  0.5, 0.1  0.65, 0.38  0.48  Figure 1: First two pictures: A single-class SVM applied to two toy problems; t,, -- c -- 0.5,  domain: [-1, 1] 2. Note how in both cases, at least a fraction of t/of all examples is in the  estimated region (cf. table). The large value of t/causes the additional data points in the  upper left corner to have almost no influence on the decision function. For smaller values of  t/, such as 0.1 (third picture), the points cannot be ignored anymore. Alternatively, one can  force the algorithm to take these 'outliers' into account by changing the kernel width (2):  in the fourth picture, using c - 0.1, t,, -- 0.5, the data is effectively analyzed on a different  length scale which leads the algorithm to consider the outhers as meaningful points.  Support Vector Method for Novelty Detection 587  -513 9 -507 1 -458 0 -377 1 -282 7 -216 2 -200 3 -186 9 -179 5 -162 0  -1533-1436-1286-1230-1177-93 5-78 0-58 7-52 6-48 3  Figure 2: Outliers identified by the proposed algorithm, ranked by the negative output of  the SVM (the argument of the sgn in the decision function). The outputs (for convenience  in units of 10 -s) are written underneath each image in italics, the (alleged) class labels are  given in bold face. Note that most of the examples are "difficult" in that they are either  atypical or even mislabelled.  5 DISCUSSION  One could view the present work as an attempt to provide an algorithm which is in line  with Vapnik's principle never to solve a problem which is more general than the one that  one is actually interested in. E.g., in situations where one is only interested in detecting  novelty, it is not always necessary to estimate a full density model of the data. Indeed,  density estimation is more difficult than what we are doing, in several respects.  Mathematically speaking, a density will only exist if the underlying probability measure  possesses an absolutely continuous distribution function. The general problem of estimat-  ing the measure for a large class of sets, say the sets measureable in Borel's sense, is not  solvable (for a discussion, see e.g. [8]). Therefore we need to restrict ourselves to making  a statement about the measure of some sets. Given a small class of sets, the simplest esti-  mator accomplishing this task is the empirical measure, which simply looks at how many  training points fall into the region of interest. Our algorithm does the opposite. It starts  with the number of training points that are supposed to fall into the region, and then esti-  mates a region with the desired property. Often, them will be many such regions -- the  solution becomes unique only by applying a regularizer, which in our case enforces that  the region be small in a feature space associated to the kernel. This, of course, implies, that  the measure of smallness in this sense depends on the kernel used, in a way that is no dif-  ferent to any other method that regularizes in a feature space. A similar problem, however,  appears in density estimation already when done in input space. Let p denote a density on  2C. If we perform a (nonlinear) coordinate transformation in the input domain 2C, then the  density values will change; loosely speaking, what remains constant is p(a:)  da:, while  is transformed, too. When directly estimating the probability measure of regions, we are  not faced with this problem, as the regions automatically change accordingly.  An attractive property of the measure of smallness that we chose to use is that it can also  be placed in the context of regularization theory, leading to an interpretation of the solution  as maximally smooth in a sense which depends on the specific kernel used [3].  The main inspiration for our approach stems from the earliest work of Vapnik and collab-  orators. They proposed an algorithm for characterizing a set of unlabelled data points by  separating it from the origin using a hyperplane [9]. However, they quickly moved on to  two-class classification problems, both in terms of algorithms and in the theoretical devel-  opment of statistical learning theory which originated in those days. From an algorithmic  point of view, we can identify two shortcomings of the original approach which may have  caused research in this direction to stop for more than three decades. Firstly, the original  588 B. SchOlkopf, R. C. Vfi'lliamson, A. J. Smola, J. Shawe-Taylor and J. C. Platt  algorithm in was limited to linear decision rules in input space, secondly, there was no way  of dealing with outliers. In conjunction, these restrictions are indeed severe -- a generic  dataset need not be separable from the origin by a hyperplane in input space. The two mod-  ifications that we have incorporated dispose of these shortcomings. Firstly, the kernel trick  allows for a much larger class of functions by nonlinearly mapping into a high-dimensional  feature space, and thereby increases the chances of separability from the origin. In partic-  ular, using a Gaussian kemel (2), such a separation exists for any data set x,..., xt: to  see this, note that k(xi, xj) > 0 for all i, j, thus all dot products are positive, implying that  all mapped patterns lie inside the same orthant. Moreover, since k(xi, xi) = i for all i,  they have unit length. Hence they are separable from the origin. The second modification  allows for the possibility of outliers. We have incorporated this 'softness' of the decision  rule using the v-trick and thus obtained a direct handle on the fraction of outliers.  We believe that our approach, proposing a concrete algorithm with well-behaved compu-  tational complexity (convex quadratic programming) for a problem that so far has mainly  been studied from a theoretical point of view has abundant practical applications. To turn  the algorithm into an easy-to-use black-box method for practicioners, questions like the  selection of kernel parameters (such as the width of a Gaussian kernel) have to be tackled.  It is our expectation that the theoretical results which we have briefly outlined in this paper  will provide a foundation for this formidable task.  Acknowledgement. Part of this work was supported by the ARC and the DFG (# Ja  379/9-1), and done while BS was at the Australian National University and GMD FIRST.  AS is supported by a grant of the Deutsche Forschungsgemeinschaft (Sm 62/1-1). Thanks  to S. Ben-David, C. Bishop, C. Schn6rr, and M. Tipping for helpful discussions.  References  [1] S. Ben-David and M. Lindenbaum. Learning distributions by their density levels: A  paradigm for learning without a teacher. Journal of Computer and System Sciences,  55:171-182, 1997.  [2] B. Sch61kopf, C. Burges, and V. Vapnik. Extracting support data for a given task. In  U. M. Fayyad and R. Uthurusamy, editors, Proceedings, First International Conference  on Knowledge Discovery & Data Mining. AAAI Press, Menlo Park, CA, 1995.  [3] B. Sch61kopf, J. Platt, J. Shawe-Taylor, A.J. Smola, and R.C. Williamson. Estimating  the support of a high-dimensional distribution. TR MSR 99 - 87, Microsoft Research,  Redmond, WA, 1999.  [4] B. Sch61kopf, A. Smola, and K.-R. Mtiller. Kernel principal component analysis. In  B. Sch61kopf, C. Burges, and A. Smola, editors, Advances in Kernel Methods -- Sup-  port Vector Learning. MIT Press, Cambridge, MA, 1999. 327 - 352.  [5] B. Sch61kopf, R. Williamson, A. Smola, and J. Shawe-Taylor. Single-class support  vector machines. In J. Buhmann, W. Maass, H. Ritter, and N. Tishby, editors, Unsu-  pervised Learning, Dagstuhl-Seminar-Report 235, pages 19 -20, 1999.  [6] A. Smola, R. C. Williamson, S. Mika, and B. Sch61kopf. Regularized principal mani-  folds. In Computational Learning Theory: 4th European Conference, volume 1572 of  Lecture Notes in Artificial Intelligence, pages 214 - 229. Springer, 1999.  [7] D.M.J. Tax and R.P.W. Duin. Data domain description by support vectors. In M. Ver-  leysen, editor, Proceedings ESANN, pages 251 -256, Brussels, 1999. D Facto.  [8] V. Vapnik. Statistical Learning Theory. Wiley, New York, 1998.  [9] V. Vapnik and A. Lerner. Pattern recognition using generalized portraits. Avtomatika i  Telemekhanika, 24:774 - 780, 1963.  
An Analog VLSI Model of  Periodicity Extraction  Andr6 van Schaik  Computer Engineering Laboratory  J03, University of Sydney, NSW 2006  Sydney, Australia  andrenee. usyd. edu.au  Abstract  This paper presents an electronic system that extracts the  periodicity of a sound. It uses three analogue VLSI building  blocks: a silicon cochlea, two inner-hair-cell circuits and two  spiking neuron chips. The silicon cochlea consists of a cascade of  filters. Because of the delay between two outputs from the silicon  cochlea, spike trains created at these outputs are synchronous only  for a narrow range of periodicities. In contrast to traditional band-  pass filters, where an increase in' selectivity has to be traded off  against a decrease in response time, the proposed system responds  quickly, independent of selectivity.  I Introduction  The human ear transduces airborne sounds into a neural signal using three stages in  the inner ear's cochlea: (i) the mechanical filtering of the Basilar Membrane (BM),  (ii) the transduction of membrane vibration into neurotransmitter release by the  Inner Hair Cells (IHCs), and (iii) spike generation by the Spiral Ganglion Cells  (SGCs), whose axons form the auditory nerve. The properties of the BM are such  that close to the entrance of the cochlea (the base) the BM is most sensitive to high  frequencies and at the apex the BM responds best to low frequencies. Along the BM  the best-frequency decreases in an exponential manner with distance along the  membrane. For frequencies below a given point's best-frequency the response drops  off gradually, but for frequencies above the best-frequency the response drops off  rapidly (see Fig. lb for examples of such frequency-gain functions).  An Inner Hair Cell senses the local vibration of a section of the Basilar Membrane.  The intracellular voltage of an IHC resembles a half-wave-rectified version of the  local BM vibration, low-pass filtered at 1 kHz. The IHC voltage has therefore lost  it's AC component almost completely for frequencies above about 4 kHz. Well  below this frequency, however, the IHC voltage has a clear temporal structure,  which will be reflected in the spike trains on the auditory nerve.  These spike trains are generated by the spiral ganglion cells. These SGCs spike with  a probability roughly proportional to the instantaneous inner hair cell voltage.  Therefore, for the lower sound frequencies, the spectrum of the input waveform is  not only encoded in the form of an average spiking rate of different fibers along the  An Analog VLSI Model of Periodicity Extraction 739  cochlea (place coding), but also in the periodicity of spiking of the individual  auditory nerve fibers. It has been shown that this periodicity information is a much  more robust cue than the spatial distribution of average firing rates [1]. Some  periodicity information can already be detected at intensities 20 dB below the  intensity needed to obtain a change in average rate. Periodicity information is  retained at intensities in the range of 60-90 dB SPL, for which the average rate of  the majority of the auditory nerve fibers is saturated. Moreover, the positions of the  fibers responding best to a given frequency move with changing sound intensity,  whereas the periodicity information remains constant. Furthermore, the frequency  selectivity of a given fiber's spiking rate is drastically reduced at medium and high  sound intensities. The robustness of periodicity information makes it likely that the  brain actually uses this information.  2 Modelling periodicity extraction  Several models have been proposed that extract periodicity information using the  phase encoding of fibers connected to the same inner hair cell or that use the  synchronicity of firing on auditory nerve fibers connected to different inner hair  cells (see [2] for 4 examples of these models). The simplest of the phase encoding  schemes correlate the output of the cochlea at a given position with a delayed  version of itself. It is easy to see that for pure tones, the comparison sin(2 x f t) =  sin(2  f(t - A)) is only true for frequencies that are a multiple of I/A, i.e., for these  frequencies the signals are in perfect synchrony and thus perfectly correlated. We  can adapt the delay A to each cochlear output, so that 1/A equals the best frequency  of that cochlear output. In this case higher multiples of 1/A will be suppressed due  to the very steep cut-off of the cochlear filters for frequencies above the best  frequency. Each synchronicity detector will then only be sensitive to the best  frequency of the filter to which it is connected. If we code the direct signal and the  delayed signal with two spike trains, with one spike per period at a fixed phase  each, it becomes a very simple operation to detect the synchronicity. A simple  digital AND operator will be enough to detect overlap between two spikes. These  spikes will overlap perfectly when f= I/A, but some overlap will still be present for  frequencies close to l/A, since the spikes have a finite width. The bandwidth of the  AND output can thus be controlled by the spike width.  It is possible to create a silicon implementation of this scheme using an artificial  cochlea, an IHC circuit, and a spiking neuron circuit together with additional  circuits to create the delays. A chip along these lines has been developed by John  Lazzaro [3] and functioned correctly. A disadvantage of this scheme, however, is  the fact that the delay associated with a cochlear output has to be matched to the  inverse of the best frequency of that cochlear output. For a cochlea whose best  frequency changes exponentially with filter number in the cascade from 4 kHz (the  upper range of phase locking on the auditory nerve) to 100 Hz, we will have to  create delays that range from 0.25 ms to 10 ms. In the brain, such a large variation  in delays is unlikely to be provided by an axonal delay circuit because it would  require an excessively large variation in axon length.  A possible solution comes from the observation that the phase of a pure tone of a  given frequency on the basilar membrane increases from base to apex, and the phase  changes rapidly around the best frequency. The silicon cochlea, which is  implemented with a cascade of second-order low-pass filters (Fig. l a), also  functions as a delay line, and each filter adds a delay which corresponds to x/2 at  the cut-off frequency of that filter. If we assume that filter i and filter i-4 have the  same cut-off frequency (which is not the case), the delay between the output of both  filters will correspond to a full period (2x) at the cut-off frequency.  740 A. v. Schaik  0.01  0  phase  (rad)  -0.5  o.1 normalized frequency lO lOO  Silicon Cochlea  Neurons  I AND I  Low CF  d)  Figure 1: a) Part of a silicon cochlea. Each section contains a second-order  low-pass filter and a derivator; b) accumulated gain at output i and i-4; c)  phase curves of the individual stages between output i and output i-4; d)  proposed implementation of the periodicity extraction model.  In reality, the filters along the cochlea will have different cut-off frequencies, as  shown in Fig. 1. Here we show the accumulated gain at the outputs i and i-4 (Fig.  lb), and the delay added by each individual filter between these two outputs (Fig.  1 c) as a function of frequency (normalized to the cut-off frequency of filter i). The  solid vertical line represents this cut-off frequency, and we can see that only filter i  adds a delay of x/2, and the other filters add less. However, if we move the vertical  line to the right (indicated by the dotted vertical line), the delay added by each filter  will increase relatively quickly, and at some frequency slightly higher than the cut-  off frequency of filter i, the sum of the delays will become 2x (dashed line). At this  frequency neither filter i nor filter i-4 has maximum gain, but if the cut-off  frequency of both filters is not too different, the gain will still be high enough for  both filters at the correlator frequency to yield output signals with reasonable  amplitudes.  The improved model can be implemented using building blocks as shown in Fig. 1 d.  Each of these building blocks have previously been presented (refer to [4] for  additional details). The silicon cochlea is used to filter and delay the signal, and has  been adjusted so that the cut-off frequency decreases by one octave every twenty  stages, so that the cut-off frequencies of neighboring filters are almost equal. The  IHC circuit half-wave rectifies the signal in the implementation of Figure l d. The  low-pass filtering of the biological Inner Hair Cell can be ignored for frequencies  below the approximately l kHz cut-off frequency of the cell. Since we limited our  measurements to this range, the low-pass filtering has not been modeled by the  circuit. Two chips containing electronic leaky-integrate-and-fire neurons have been  used to create the two spike trains. In the first series of measurements, each chip  generates exactly one spike per period of the input signal. A final test will set the 32  neurons on each chip to behave more like biological spiral ganglion cells and the  effect on periodicity extraction will be shown. A digital AND gate is used to  compare the output spikes of the two chips, and the spike rate at the output of the  AND gate is the measure of activity used.  3 Test results  The first experiment measures the number of spikes per second at the output of the  AND gate as a function of input frequency, using different cochlear filter  combinations. Twelve filter pairs have been measured, each combining a filter  An Analog VLSI Model of Periodicity Extraction 741  output with the output of a filter four sections earlier in the cascade. The best  frequency of the filter with the lowest best frequency of the pairs ranged from  200 Hz to 880 Hz. The results are shown in Fig. 2a.  1500  750  0  200  400 600  800 1000 1200  frequency (Hz)  1.:  200  frequency (log scale)  2000  Figure 2: a) measured output rate at different cochlear positions, and b)  spike rate normalized to best input frequency, plotted on a log frequency  scale.  The maximum spike rate increases approximately linearly with frequency; this is to  be expected, since we will have approximately one spike per signal period. Further-  more the best response frequencies of the filters sensitive to higher frequencies are  further apart, due to the exponential scaling of the frequencies along the cochlea.  Finally, a given time delay corresponds to a larger phase delay for the higher  frequencies, so that the absolute bandwidth of the coincidence detectors, i.e., the  range of input frequencies to which they respond, is larger. When we normalize the  spike rate and plot the curves on a logarithmic frequency scale, as in Fig. 2b, we see  that the best frequencies of the correlators follow the exponential scaling of the best  frequencies of the cochlear filters, and that the relative bandwidth is fairly constant.  1200 -  spike  rate  1200.  spike  rate  600  a .... 2omv   '  30mY  ; ' f ..... 4o.,v   I  ;   651 744 837 930 1023 1116 1209 1302  input frequency (Hz)  600  ..........  .......................................................... 1 .... 2omvl  ..... 30mY[  40mVj  0 0  558 558 651  744 837 930 1023 1116 1209 1302  carrier frequency (Hz)  Figure 3: Frequency selectivity for different input intensities. a) pure tones  b) AM signals.  Using the same settings as in the previous experiment, the output spike rate of the  system for different input amplitudes has been measured, using the cochlear filter  pair with best frequencies of 710 Hz and 810 Hz. In principle, the amplitude of the  input signal should have no effect on the output of the system, since the system only  uses phase information. However, this is only true if the spikes are always created at  the same phase of the output signal of the cochlear filters, for instance at the peak,  or the zero crossing. Fig. 3 shows however that the resulting filter selectivity shifts  to lower frequencies for higher intensity input signals.  This is a result of the way the spikes are created on the neuron chip. The neurons  have been adjusted to spike once per period, but the phase at which they spike with  respect to the half-wave-rectified waveform depends on the integration time of the  neuron, which is the time needed with a given input current to reach the spike  threshold voltage from the zero resting voltage. This time depends on the amplitude  of the input current, which in turn is proportional to the amplitude of the input  signal. Since the amplitude gain of the two cochlear filters used is not the same, the  amplitude of the current input to the two neuron chips is different. Therefore, they  do not spike at the same phase with respect to their respective input waveforms.  This causes the frequency selectivity of the system to shift to lower frequencies with  increasing intensity. However, this is an artifact of the spike generation used to  742 A. v. Schaik  simplify the system. On the auditory nerve, spikes arrive with a probability roughly  proportional to the half-wave rectified waveform. The most probable phase for a  spike is therefore always at the maximum of the waveform, independent of  intensity. In such a system, the frequency selectivity will therefore be independent  of amplitude. A second advantage of coding (at least half of) the waveform in spike  probability is that it does not assume that the input waveform is sinusoidal. Coding  a waveform with just one spike per period can only code the frequency and phase of  the waveform, but not its shape. A square wave and a sine wave would both yield  the same spike train. We will discuss the "auditory-nerve-like" coding at the end of  this section.  To test the model with a more complex waveform, a 930 Hz sine wave 100%  amplitude-modulated at 200 Hz generated on a computer has been used. The carrier  frequency was varied by playing the whole waveform a certain percentage slower or  faster. Therefore the actual modulation frequency changes with the same factor as  the carrier frequency. The results of this test are shown in Fig. 3b for three different  input amplitudes. Compared to the measurements in Fig. 3a, we see that the filter is  less selective and centered at a higher input frequency. The shift towards a higher  frequency can be explained by the fact that the average amplitude of a half-wave  rectified amplitude modulated signal is lower than in for a half-wave rectified pure  tone with the same maximum amplitude. Furthermore, the amplitude of the positive  half-cycle of the output of the IHC circuit changes from cycle to cycle because of  the amplitude modulation. We have seen that the amplitude of the input signal  changes the frequency for which the two spike trains are synchronous, which means  that the frequency which yields the best response changes from cycle to cycle with a  periodicity equal to the modulation frequency. This introduces a sort of"roaming"  of the frequencies in the input signal, effectively reducing the selectivity of the  filters. Finally, because of the 100% depth of the amplitude modulation, the  amplitude of the input will be too low during some cycles to create a spike, which  therefore reduces the total number of spikes which can coincide.  Fig. 3b shows that this model detects periodicity and not spectral content. The  spectrum of a 930 Hz pure tone 100% amplitude modulated at 200 Hz contains,  apart from a 930 Hz carrier component, components both at 730 Hz and 1130 Hz,  with half the amplitude of the cartier component. When the speed of the waveform  playback is varied so that the carrier frequency is either 765 Hz or 1185 Hz, one of  these spectral side bands will be at 930 Hz, but the system does not respond at these  carrier frequencies. This is explained by the fact that the periodicity of the zero  crossings, and thus of the positive half cycles of the IHC output, is always equal to  the carder frequency.  Traditional band-pass filters with a very high quality factor (Q) can also yield a  narrow pass-band, but their step response takes about 1.5Q cycles at the center  frequency to reach steady state. The periodicity selectivity of the synchronicity  detector shown in Fig. 3a corresponds to a quality factor of 14; a traditional band-  pass filter would take about 21 cycles of the 930Hz input signal to reach 95% of it's  final output value. Fig. 4 shows the temporal aspect of the synchronicity detection  in our system. The top trace in this figure shows the output of the cochlear filter  with the highest best frequency (index i-4 in Fig. 1) and the spikes generated based  on this output. The second trace shows the same for the output of the cochlear filter  with the lower best frequency (index i in Fig. 1). The third trace shows the output of  the AND gate with the above inputs, which are slightly above its best periodicity.  Coincidences are detected at the onset of the tone, even when it is not of the correct  periodicity, but only for the first one or two cycles. The bottom trace shows the  output of the AND gate for an input at best frequency. The system thus responds to  the presence of a pure tone of the correct periodicity after only a few cycles,  independent of the filters selectivity.  An Analog VLSI Model of Periodicity Extraction 743  time (ms) 10  Figure 4: Oscilloscope traces of the temporal aspect of synchronicity  detection. The vertical scale is 20mV per square for the cochlear outputs,  the spikes are 5V in amplitude.  To show this more dramatically, we have reduced the spike width to 10 Its, to obtain  a high periodicity selectivity as shown in Fig. 5a. The bandwidth of this filter is  only 20 Hz at 930 Hz, equivalent to a quality factor of 46.5. A traditional filter with  such a quality factor would only settle 70 cycles after the onset of the signal,  whereas the periodicity detector still settles after the first few cycles, as shown in  Fig. 5b. We can compare this result with the response of a classic RLC band-pass  filter with a 930 Hz center frequency and a quality factor of 46.5 as shown in Fig. 6.  After 18 cycles of the input signal, the output of the band-pass filter has only  reached 65% of its final value. Thresholding the RLC output could signal the  presence of a periodicity faster, but it would then still respond very slowly to the  offset of the tone as the RLC filter will continue ringing after the offset.  800  spike  rat   o  830  4oo  880 930 980 lO3O  input frequency (Hz)  5 lO 15 time (ms) 20  1  gain  05  Figure 5' a) Frequency selectivity with a 10ts spike width. b) Cochlear  output (top, 40 mV scale) and coincidences (bottom) for a signal at best  frequency.  0    I  830 880 930 980 1030  input frequency (Hz}  0 5 10 15 time (ms) 20  -65%  Figure 6: Simulated response of the RLC band-pass filter. a) frequency  selectivity, b) transient response (scale units are 40 mV).  In the previous experiments we simplified the model to use one spike per period in  order to understand the principle behind the periodicity detection. However, we  have seen that this implementation leads to a shift in best periodicity with changing  amplitude, because the phase at which the 'single neuron' spikes changes with  intensity. Now, we will change the settings to be more realistic, so that each of the  32 neurons cannot spike at every period, and we will reduce the output gain of the  IHC circuit so that the neurons receive less signal current, and thus have a lower  input SNR. The resulting spike distribution is a better simulation of the spike  distribution on the auditory nerve. This is shown in Fig. 7 for a group of 32 neurons  stimulated by and IHC circuit connected to a single cochlear output. The bottom  trace shows the sum of spikes over the 32 neurons on an arbitrary scale. When we  744 A. v. Schaik  use this spike distribution and repeat the pure-tone detection experiment of Fig. 3a  at different input intensities, we obtain the curve of Fig. 7b. Indeed, in this case, the  best periodicity does not change; the curves are remarkably independent of input  intensity. However, the selectivity curve is about twice as wide at the base as the  ones in Fig. 3a, but the slopes of the selectivity curve rise and fall much more  gradually. This means that we can easily increase the selectivity of these curves by  setting a higher threshold, e.g., discarding spike rates below 70 spikes per second.  Because of the steep slopes in Fig. 3a such an operation would hardly increase the  selectivity for that case.  5 1o 15 time (ms) 20  140  spike  rate  70  0  558  .,/ .... 20mV  30mV  , ..... 40mY  651 744 837 930 1023 1116 1209 1302  Input frequency (Hz)  Figure 7: a) Cochlear output (top) and population average of the auditory  nerve spikes (bottom); b) periodicity selectivity with auditory nerve like  spike distribution.  4 Conclusions  In this paper we have presented a neural system for periodicity detection  implemented with three analogue VLSI building blocks. The system uses the delay  between the outputs at two points along the cochlea and synchronicity of the spike  trains created from these cochlear outputs to detect the periodicity of the input  signal. An especially useful property of the cochlea is that the delay between two  points a fixed distance apart corresponds to a full period at a frequency that scales in  the same way as the best frequency along the cochlea, i.e., decreases exponentially.  If we always create spikes at the same phase of the output signal at each filter, or  simply have the highest spiking probability for the maximum instantaneous  amplitude of the output signal, then both outputs will only have synchronous spikes  for a Certain periodicity, and we can easily detect this synchronicity with  coincidence detectors. This system offers a way to obtain very selective filters using  spikes. Even though they react to a very narrow range of periodicities, these filters  are able to react after only a few periods. Furthermore, the range of periodicities it  responds to can be made independent of input intensity, which is not the case with  the cochlear output itself. This clearly demonstrates the advantages of using spikes  in the detection of periodicity.  Acknowledgements  The author thanks Eric Fragnire, Eric Vittoz and the Swiss NSF for their support.  References  [1] Evans, "Functional anatomy of the auditory system," in Barlow and Mollon (editors),  The Senses, Cambridge University Press, pp. 251-306, 1982.  [2] Seneft, Shamma, Deng, & Ghitza, Journal of Phonetics, Vol. 16, pp. 55-123, 1988.  [3] Lazzaro, "A silicon model of an auditory neural representation of spectral shape."IEEE  Journal of Solid-State Circuits, Vol. 26, No. 5, pp. 772-777, 1991.  [4] van Schaik, "An Analogue VLSI Model of Periodicity Extraction in the Human Auditory  System," to appear in Analog Integrated Circuits and Signal Processing, Kluwer, 2000.  PART VI  SPEECH HANDWRITING AND SIGNAL  PROCESSING  
Monte Carlo POMDPs  Sebastian Thrun  School of Computer Science  Carnegie Mellon University  Pittsburgh, PA 15213  Abstract  We present a Monte Carlo algorithm for learning to act in partially observable  Markov decision processes (POMDPs) with real-valued state and action spaces.  Our approach uses importance sampling for representing beliefs, and Monte Carlo  approximation for belief propagation. A reinforcement learning algorithm, value  iteration, is employed to learn value functions over belief states. Finally, a sample-  based version of nearest neighbor is used to generalize across states. Initial  empirical results suggest that our approach works well in practical applications.  1 Introduction  POMDPs address the problem of acting optimally in partially observable dynamic environ-  ment [6]. In POMDPs, a learner interacts with a stochastic environment whose state is only  partially observable. Actions change the state of the environment and lead to numerical  penalties/rewards, which may be observed with an unknown temporal delay. The learner's  goal is to devise a policy for action selection that maximizes the reward. Obviously, the  POMDP framework embraces a large range of practical problems.  Past work has predominately studied POMDPs in discrete worlds [ 1 ]. Discrete worlds have  the advantage that distributions over states (so-called "belief states") can be represented  exactly, using one parameter per state. The optimal value function (for finite planning  horizon) has been shown to be convex and piecewise linear [10, 14], which makes it  possible to derive exact solutions for discrete POMDPs.  Here we are interested in POMDPs with continuous state and action spaces, paying tribute  to the fact that a large number of real-world problems are continuous in nature. In general,  such POMDPs are not solvable exactly, and little is known about special cases that can be  solved. This paper proposes an approximate approach, the MC-POMDP algorithm, which  can accommodate real-valued spaces and models. The central idea is to use Monte Carlo  sampling for belief representation and propagation. Reinforcement learning in belief space  is employed to learn value functions, using a sample-based version of nearest neighbor  for generalization. Empirical results illustrate that our approach finds to close-to-optimal  solutions efficiently.  2 Monte Carlo POMDPs  2.1 Preliminaries  POMDPs address the problem of selection actions in stationary, partially observable, con-  trollable Markov chains. To establish the basic vocabulary, let us define:   State. At any point in time, the world is in a specific state, denoted by :c.  Monte Carlo POMDPs 1065   Action. The agent can execute actions, denoted a.   Observation. Through its sensors, the agent can observe a (noisy) projection of the  world's state. We use o to denote observations.   Reward. Additionally, the agent receives rewards/penalties, denoted R  . To  simplify the notation, we assume that the reward is part of the observation. More  specifically, we will use R(o) to denote the function that "extracts" the reward from  the observation.  Throughout this paper, we use the subscript t to refer to a specific point in time (e.g., st  refers to the state at time t).  POMDPs are characterized by three probability distributions:  1. The initial distribution, '(x) :-- Pr(zo), specifies the initial distribution of states at  time t -- 0.  2. The next state distribution,/_t(z I a,) := Pt(oct - vc [ at_l '- a, vct_l --  describes the likelihood that action a, when executed at state , leads to state .  3. The perceptual distribution, v(o [ ) := Pt(or = o[ct - ), describes the likeli-  hood of observing o when the world is in state  A history is a sequence of states and observations. For simplicity, we assume that actions  and observations are alternated. We use dt to denote the history leading up to time t:  dt :- {ot, at-l,Ot-l, at-2,...,ao, oo} (1)  The fundamental problem in POMDPs is to devise a policy for action selection that maxi-  mizes reward. A policy, denoted  cr  d > a (2)  is a mapping from histories to actions. Assuming that actions are chosen by a policy or,  each policy induces an expected cumulative (and possibly discounted by a discount factor  ? _< 1) reward, defined as  : s (3)  Here E[ ] denotes the mathematical expectation. The POMDP problem is, thus, to find a  policy or* that maximizes J", i.e.,  or* -- argmax J" (4)  o'  2.2 Belief States  To avoid the difficulty of learning a function with unbounded input (the history can be  arbitrarily long), it is common practice to map histories into belief states, and learn a  mapping from belief states to actions instead [10].  Formally, a belief state (denoted 0) is a probability distribution over states conditioned on  past actions and observations:  Ot- Pr(xt I dt) - Pr(xt I ot,at_l,...,oo) (5)  Belief are computed incrementally, using knowledge of the POMDP's defining distributions  ', p, and ,. Initially  00 : r  For t > O, we obtain  Ot+l = Pr(xt+l l ot+l,at,...,oo)  = o Pr(ot+l  = o Pr(ot+l  = o Pr(Ot+l  (6)  (7)  (8)  o0) -r(xt+l I at,..., o0)  zt+) f Pr(zt+l l at,...,oo, zt) Pr(zt l at,...,oo) dzt (9)  ct+) j Pr(oet+l l at,ct) Ot dzt (10)  1066 $. Thrun  0.2  lllllllllNIllllllllllllllllllllll IIIIlll II  II I I  2 4 6 8 10 12  2 4 $ 8 10 12  Figure 1: Sampling: (a) Likelihood-weighted sampling and (b) importance sampling. At the bottom  of each graph, samples are shown that approximate the function f shown at the top. The height of  the samples illustrates their importance factors.  Here a denotes a constant normalizer. The derivations of (8) and (10) follow directly from  the fact that the environment is a stationary Markov chain, for which future states and  observations are conditionally independent from past ones given knowledge of the state.  Equation (9) is obtained using the theorem of total probability.  Armed with the notion of belief states, the policy is now a mapping from belief states  (instead of histories) to actions:  o':0 >a (11)  The legitimacy of conditioning a on 0, instead of d, follows directly from the fact that the  environment is Markov, which implies that 0 is all one needs to know about the past to  make optimal decisions.  2.3 Sample Representations  Thus far, we intentionally left open how belief states 0 are represented. In prior work, state  spaces have been discrete. In discrete worlds, beliefs can be represented by a collection  of probabilities (one for each state), hence, beliefs can be represented exactly. Here were  are interested in real-valued state spaces. In general, probability distributions over real-  valued spaces possess infinitely many dimensions, hence cannot be represented on a digital  computer.  The key idea is to represent belief states by sets of (weighted) samples drawn from the  belief distribution. Figure 1 illustrates two popular schemes for sample-based approxima-  tion: likelihood-weighted sampling, in which samples (shown at the bottom of Figure la)  are drawn directly from the target distribution (labeled f in Figure la), and importance  sampling, where samples are drawn from some other distribution, such as the curve labeled  # in Figure lb. In the latter case, samples z are annotated by a numerical importance factor  f(z) (12)  p(x) = g(x)  to account for the difference in the sampling distribution, g, and the target distribution f  (the height of the bars in Figure 1 b illustrates the importance factors). Importance sampling  requires that f > 0 - g > 0, which will be the case throughout this paper. Obviously, both  sampling methods generate approximations only. Under mild assumptions, they converge   with N denoting the sample set size [16].  to the target distribution at a rate of ,  In the context of POMDPs, the use of sample-based representations gives rise to the  following algorithm for approximate belief propagation (c.f., Equation (10)):  Algorithm particle_filter(Or, at, Or+  ):  Ot+ = 0  do N times:  draw random state zt from Ot  Monte Carlo POMDPs 1067  sample xt+, according to(xt+, [ at, xt)  set importance factor p( z t + l ) -- v ( ot + , l z t + , )  add (zt+,,p(zt+,)) toot+,  normalize all p(zt+,)  Or+, so that -p(zt+,) = 1  return Ot + ,  This algorithm converges to (10) for arbitrary models /, ,, and r and arbitrary belief  distributions 0, defined over discrete, continuous, or mixed continuous-discrete state and  action spaces. It has, with minor modifications, been proposed under names like particle  filters [13], condensation algorithm [5], survival of the fittest [8], and, in the context of  robotics, Monte Carlo localization [4].  2.4 Projection  In conventional planning, the result of applying an action at at a state zt is a distribution  Pr(zt+,, Rt+, I at, zt) over states zt+, and rewards Rt+, at the next time step. This  operation is called projection. In POMDPs, the state zt is unknown. Instead, one has to  compute the result of applying action at to a belief state Or. The result is a distribution  Pt(Or+,, Rt+, I at, Or) over belief states Or+, and rewards Rt+,. Since belief states them-  selves are distributions, the result of a projection in POMDPs is, technically, a distribution  over distributions.  The projection algorithm is derived as follows. Using total probability, we obtain:  Pr(0t+,,/t+, l at,Or) - Pr(Ot+,,Rt+, l at,dr) (13)  ,Pr(Ot+,,tt+, I ot+,,at,dt) Pt(or+, l at,dr) dot+, (14)  (,) (**)  The term (,) has already been derived in the previous section (c.f., Equation (10)), under  the observation that the reward/t+, is trivially computed from the observation  The second term, (**), is obtained by integrating out the unknown variables, zt+, and zt,  and by once again exploiting the Markov property:  Pr(ot+t l at,dt) -' / Pr(ot+, I Xt+l) Pr(xt+, [ at,dt) dzt+, (15)  = / Pt(or+, I zt+,) / Pr(zt+, I zt,at) Pr(zt Idt)dzt dzt+(i16)  This leads to the following approximate algorithm for projecting belief state. In the spirit  of this paper, our approach uses Monte Carlo integration instead of exact integration. It  represents distributions (and distributions over distributions) by samples drawn from such  distributions.  Algorithm partide_projecfion(0t, at):  Ot = 0  do N times:  draw random state zt from Ot  sample a next state zt+ 1 according to p(zt+, I at,  sample an observation or+, according to '(Ot+l I  compute Ot + , =particle_filter(Or, at, Ot + l)  add {Ot+,,(ot+,)> toot  return Ot  The result of this algorithm, Or, is a sample set of belief states Or+, and rewards  drawn from the desired distribution Pv(Ot+l, Rt+l [ Or, at). As N - c, Ot converges  with probability 1 to the true posterior [16].  1068 S. Thrun  2.5 Learning Value Functions  Following the rich literature on reinforcement learning [7, 15], our approach solves the  POMDP problem by value iteration in belief space. More specifically, our approach  recursively learns a value function Q over belief states and action, by backing up values  from subsequent belief states:  Q(Ot,at) < E [/l:(ot_l_l) q- 7maaxQ(0t+l, 5)] (18)  Leaving open (for a moment) how Q is represented, it is easy to be seen how the algorithm  particle_projection can be applied to compute a Monte Carlo approximation of the right  hand-side expression: Given a belief state Ot and an action at, particle_projection computes  a sample of ](ot+l ) and Or+l, from which the expected value on the right hand side of (18)  can be approximated.  It has been shown [2] that if both sides of (18) are equal, the greedy policy  o'Q(0) = argmaxQ(0, o,) (19)  a  is optimal, i.e., or* -- crq?. Furthermore, it has been shown (for the discrete case!) that  repetitive application of (18) leads to an optimal value function and, thus, to the optimal  policy [17, 3].  Our approach essentially performs model-based reinforcement learning in belief space  using approximate sample-based representations. This makes it possible to apply a rich  bag of tricks found in the literature on MDPs. In our experiments below, we use on-  line reinforcement learning with counter-based exploration and experience replay [9] to  determine the order in which belief states are updated.  2.6 Nearest Neighbor  We now return to the issue how to represent Q. Since we are operating in real-valued  spaces, some sort of function approximation method is called for. However, recall that  Q accepts a probability distribution (a sample set) as an input. This makes most existing  function approximators (e.g., neural networks) inapplicable.  In our current implementation, nearest neighbor [11] is applied to represent Q. More  specifically, our algorithm maintains a set of sample sets 0 (belief states) annotated by an  action a and a Q-value Q(O, a). When a new belief state 0 r is encountered, its Q-value is  obtained by finding the k nearest neighbors in the database, and linearly averaging their  Q-values. If there aren't sufficiently many neighbors (within a pre-specified maximum  distance), 0' is added to the database; hence, the database grows over time.  Our approach uses KL divergence (relative entropy) as a distance function . Technically,  the KL-divergence between two continuous distributions is well-defined. When applied  to sample sets, however, it cannot be computed. Hence, when evaluating the distance be-  tween two different sample sets, our approach maps them into continuous-valued densities  using Gaussian kernels, and uses Monte Carlo sampling to approximate the KL divergence  between them. This algorithm is fairly generic an extension of nearest neighbors to func-  tion approximation in density space, where densities are represented by samples. Space  limitations preclude us from providing further detail (see [11, 12]).  3 Experimental Results  Preliminary results have been obtained in a world shown in two domains, one synthetic and  one using a simulator of a RWI B21 robot.  In the synthetic environment (Figure 2a), the agents starts at the lower left corner. Its  objective is to reach "heaven" which is either at the upper left corner or the lower right  Stdctly speaking, KL divergence is not a distance metric, but this is ignored here.  Monte Carlo POMDPs 1069  (a) (b![  :I  Figure 2: (a) The environment, schematically. (b) Average performance (reward) as a function of  training episodes. The black graph corresponds to the smaller environment (25 steps min), the grey  graph to the larger environment (50 steps min). (c) Same results, plotted as a function of number of  backups (in thousands).  comer. The opposite location is "hell." The agent does not know the location of heaven,  but it can ask a "priest" who is located in the upper right comer. Thus, an optimal solution  requires the agent to go first to the priest, and then head to heaven. The state space contains  a real-valued (coordinates of the agent) and discrete (location of heaven) component. Both  are unobservable: In addition to not knowing the location of heaven, the agent also cannot  sense its (real-valued) coordinates. 5% random motion noise is injected at each move.  When an agent hits a boundary, it is penalized, but it is also told which boundary it hit  (which makes it possible to infer its coordinates along one axis). However, notice that the  initial coordinates of the agent are known.  The optimal solution takes approximately 25 steps; thus, a successful POMDP planner must  be capable of looking 25 steps ahead. We will use the term "successful policy" to refer  to a policy that always leads to heaven, even if the path is suboptimal. For a policy to be  successful, the agent must have learned to first move to the priest (information gathering),  and then proceed to the right target location.  Figures 2b&c show performance results, averaged over 13 experiments. The solid (black)  curve in both diagrams plots the average cumulative reward J as a function of the number  of training episodes (Figure 2b), and as a function of the number of backups (Figure 2c).  A successful policy was consistently found after 17 episodes (or 6,150 backups), in all  13 experiments. In our current implementation, 6,150 backups require approximately 29  minutes on a Pentium PC. In some experiments, a successful policy was identified in 6  episodes (less than 1,500 backups or 7 minutes). After a successful policy is found, further  learning gradually optimizes the path. To investigate scaling, we doubled the size of the  environment (quadrupling the size of the state space), making the optimal solution 50 steps  long. The results are depicted by the gray curves in Figures 2b&c. Here a successful  policy is consistently found after 33 episodes (10,250 backups, 58 minutes). In some runs,  a successful policy is identified after only 14 episodes.  We also applied MC-POMDPs to a robotic locate-and-retrieve task. Here a robot (Figure 3a)  is to find and grasp an object somewhere in its vicinity (at floor or table height). The robot's  task is to grasp the object using its gripper. It is rewarded for successfully grasping the  object, and penalized for unsuccessful grasps or for moving too far away from the object.  The state space is continuous in a: and y coordinates, and discrete in the object's height.  The robot uses a mono-camera system for object detection; hence, viewing the object from  a single location is insufficient for its 3D localization. Moreover, initially the object might  not be in sight of the robot's camera, so that the robot must look around first. In our  simulation, we assume 30% general detection error (false-positive and false-negative), with  additional Gaussian noise if the object is detected correctly. The robot's actions include  tums (by a variable angle), translations (by a variable distance), and grasps (at one of two  legal heights). Robot control is erroneous with a variance of 20% (in :c-y-space) and 5% (in  rotational space). Typical belief states range from uniformly distributed sample sets (initial  belief) to samples narrowly focused on a specific a:-t-z location.  1070 S. Thrun  (a)  Figure 3: Find and fetch task:  (c)  % success  iteration  (a) The mobile robot with gripper and camera, holding the target  object (experiments are carded out in simulation!), (b) three successful runs (trajectory projected into  2D), and (c) success rate as a function of number of planning steps.  Figure 3c shows the rate of successful grasps as a function of iterations (actions). While  initially, the robot fails to grasp the object, after approximately 4,000 iterations its perfor-  mance surpasses 80%. Here the planning time is in the order of 2 hours. However, the robot  fails to reach 100%. This is in part because certain initial configurations make it impossible  to succeed (e.g., when the object is too close to the maximum allowed distance), in part  because the robot occasionally misses the object by a few centimeters. Figure 3b depicts  three successful example trajectories. In all three, the robot initially searches the object,  then moves towards it and grasps it successfully.  4 Discussion  We have presented a Monte Carlo approach for learning how to act in partially observable  Markov decision processes (POMDPs). Our approach represents all belief distributions  using samples drawn from these distributions. Reinforcement learning in belief space is  applied to learn optimal policies, using a sample-based version of nearest neighbor for  generalization. Backups are performed using Monte Carlo sampling. Initial experimental  results demonstrate that our approach is applicable to real-valued domains, and that it yields  good performance results in environments that are--by POMDP standards--relatively large.  References  [1] AAAI Fall symposium on POMDPs. 1998. See http://www.cs.duke.edu/mlittman/talks/  pomdp-symposium. html  [2] R.E. Bellman. Dynamic Programming. Princeton University Press, 1957.  [3] P. Dayan and T. J. Sejnowski. TD(,X) converges with probability 1. 1993.  [4] D. Fox, W. Burgard, E Dellaert, and S. Thrun. Monte carlo localization: Efficient position estimation for mobile robots.  AAAI-99.  [5] M. Isard and A. Bake. Cndensatin: cnditina density prpagatinfr visua tracking. nternatinalJurnal f Crnputer  Vision, 1998.  [6] L.P. Kaelbling, M.L. Littman, and A.R. Cassandra. Planning and acting in partially observable stochastic domains. Submitted  for publication, 1997.  [7] L.P. Kaelbling, M.L. Littman, and A.W. Moore. Reinforcement learning: A survey. JAIR, 4, 1996.  [8] K. Kanazawa, D. Koller, and S.J. Russell. Stochastic simulation algorithms for dynamic probabilistic networks. UAI-95.  [9] L.-J. Lin. Self-improving reactive agents based on reinforcement learning, planning and teaching. Machine Learning, 8,  1992.  [10] M.L. Littman, A.R. Cassandra, and L.P. Kaelbling. Learning policies for partially observable environments: Scaling up.  ICML-95.  [11] A.W. Moore, C.G. Atkeson, and S.A. Schaal. Locally weighted learning for control. AIReview, 11, 1997.  [12] D. Ormoneit and S. Sen. Kernel-based reinforcementlearning. TR 1999-8, Statistics, Stanford University, 1999.  [ 13] M. Pitt and N. Shephard. Filtering via simulation: auxiliary particle filter. Journal of the American Statistical Association,  1999.  [14] E. Sondik. The Optimal Control of Partially Observable Markov Processes. PhD thesis, Stanford, 1971.  [15] R.S. Sutton and A.G. Barto. Reinforcement Learning: An Introduction. MIT Press, 1998.  [16] M.A. Tanner. Tools for Statistical Inference. Springer Verlag, 1993.  [17] C.J.C.H. Watkins. Learning from Delayed Rewards. PhD thesis, King's College, Cambridge, 1989.  
Reinforcement Learning Using Approximate  Belief States  Andr6s Rodriguez *  Artificial Intelligence Center  SRI International  333 Ravenswood Avenue, Menlo Park, CA 94025  rodriguez @ ai. sri. com  Ronald Parr, Daphne Ko!!er  Computer Science Department  Stanford University  Stanford, CA 94305  {parr, kolle r} @ cs. s tan ford. edu  Abstract  The problem of developing good policies for partially observable Markov  decision problems (POMDPs) remains one of the most challenging ar-  eas of research in stochastic planning. One line of research in this area  involves the use of reinforcement learning with belief states, probabil-  ity distributions over the underlying model states. This is a promis-  ing method for small problems, but its application is limited by the in-  tractability of computing or representing a full belief state for large prob-  lems. Recent work shows that, in many settings, we can maintain an  approximate belief state, which is fairly close to the true belief state. In  particular, great success has been shown with approximate belief states  that marginalize out correlations between state variables. In this paper,  we investigate two methods of full belief state reinforcement learning and  one novel method for reinforcement learning using factored approximate  belief states. We compare the performance of these algorithms on several  well-known problem from the literature. Our results demonstrate the im-  portance of approximate belief state representations for large problems.  1 Introduction  The Markov Decision Processes (MDP) framework [2] is a good way of mathematically  formalizing a large class of sequential decision problems involving an agent that is inter-  acting with an environment. Generally, an MDP is defined in such a way that the agent has  complete knowledge of the underlying state of the environment. While this formulation  poses very challenging research problems, it is still a very optimistic modeling assumption  that is rarely realized in the real world. Most of the time, an agent must face uncertainty  or incompleteness in the information available to it. An extension of this formalism that  generalizes MDPs to deal with this uncertainty is given by partially observable Markov  Decision Processes (POMDPs) [1, 11] which are the focus of this paper.  Solving a POMDP means finding an optimal behavior policy r*, that maps from the agent's  available knowledge of the environment, its belief state, to actions. This is usually done  through a function, V, that assigns values to belief states. In the fully observable (MDP)  *The work presented in this paper was done while the first author was at Stanford University.  Reinforcement Learning Using Approximate Belief States 103 7  case, a value function can be computed efficiently for reasonably sized domains. The  situation is somewhat different for POMDPs, where finding the optimal policy is PSPACE-  hard in the number of underlying states [6]. To date, the best known exact algorithms to  solve POMDPs are taxed by problems with a few dozen states [5].  There are several general approaches to approximating POMDP value functions using rein-  forcement learning methods and space does not permit a full review of them. The approach  upon which we focus is the use of a belief state as a probability distribution over underlying  model states. This is in contrast to methods that manipulate augmented state descriptions  with finite memory [9, 12] and methods that work directly with observations [8].  The main advantage of a probability distribution is that it summarizes all of the informa-  tion necessary to make optimal decisions [1]. The main disadvantages are that a model  is required to compute a belief state, and that the task of representing and updating belief  states in large problems is itself very difficult. In this paper, we do not address the problem  of obtaining a model; our focus is on the the most effective way of using a model. Even  with a known model, reinforcement learning techniques can be quite competitive with ex-  act methods for solving POMDPs [10]. Hence, we focus on extending the model-based  reinforcement learning approach to larger problems through the use of approximate belief  states. There are risks to such an approach: inaccuracies introduced by belief state approx-  imation could give an agent a hopelessly inaccurate perception of its relationship to the  environment.  Recent work [4], however, presents an approximate tracking approach, and provides theo-  retical guarantees that the result of this process cannot stray too far from the exact belief  state. In this approach, rather than maintaining an exact belief state, which is infeasible  in most realistically large problems, we maintain an approximate belief state, usually from  some restricted class of distributions. As the approximate belief state is updated (due to  actions and observations), it is continuously projected back down into this restricted class.  Specifically, we use decomposed belief states, where certain correlations between state  variables are ignored.  In this paper we present empirical results comparing three approaches to belief state rein-  forcement learning. The most direct approach is the use of a neural network with one input  for each element of the full belief state. The second is the SPOVA method [10], which  uses a function approximator designed for POMDPs and the third is the use of a neural net-  work with an approximate belief state as input. We present results for several well-known  problems in the POMDP literature, demonstrating that while belief state approximation is  ill-suited for some problems, it is an effective means of attacking large problems.  2 Basic Framework and Algorithms  A POMDP is defined as a tuple < $, A, O, 7", 7, (.9 > of three sets and three functions.  $ is a set of states, A is a set of actions and O is a set of observations. The transition  function 7": $ x A --> II(S) specifies how the actions affect the state of the world. It can  be viewed as T(si, a, s j) = P(sj I a, si), the probability that the agent reaches state s if it  currently is in state si and takes action a. The reward function 7: $ x A -->  determines  the immediate reward received by the agent The observation model (.9: S x A -> II(O)  determines what the agent perceives, depending on the environment state and the action  taken. O(s, a, o) = P(ola , s) is the probability that the agent observes o when it is in state  s, having taken the action a.  1038 A. Rodriguez, R. Parr andD. Koller  2.1 POMDP belief states  A belief state, b, is defined as a probability distribution over all states s 6 S, where b(s),  represents probability that the environment is in state s. After taking action a and observing  o, the belief state is updated using Bayes rule:  o(8', a, o) Es,s a,  = e' I = Zs,so(sj,,o) 7-(8,,  The size of an exact belief state is equal to the number of states in the model. For large  problems, maintaining and manipulating an exact belief state can be problematic even if  the the transition model has a compact representation [4]. For example, suppose the state  space is described via a set of random variables X = {X, ... , X,}, where each Xi takes  on values in some finite domain Val(Xi), a particular s defines a value a:i  Val(Xi) for  each variable Xi. The full belief state representation will be exponential in n. We use the  approximation method analyzed by Boyen and Koller [4], where the variables are parti-  tioned into a set of disjoint clusters C... Ce and belief functions, b... be are maintained  over the variables in each cluster. At each time step, we compute the exact belief state, then  compute the individual belief functions by marginalizing out inter-cluster correlations. For  some assignment, ci, to variables in Ci, we obtain bi(ci) = Eycl P(i, Y). An approxi-  mation of the original, full belief state is then reconstructed as b(s) e  : I'-[i----'1 bi(ci)'  By representing the belief state as a product of marginal probabilities, we are projecting  the belief state into a reduced space. While a full belief state representation for n state  variables would be exponential in n, the size of decomposed belief state representation is  exponential in the size of the largest cluster and additive in the number of clusters. For pro-  cesses that mix rapidly enough, the errors introduced by approximation will stay bounded  over time [4]. As discussed by Boyen and Koller [4], this type of decomposed belief state  is particularly suitable for processes that can themselves be factored and represented as a  dynamic Bayesian network [3]. In such cases we can avoid ever representing an exponen-  tially sized belief state. However, the approach is fully general, and can be applied in any  setting where the state is defined as an assignment of values to some set of state variables.  2.2 Value functions and policies for POMDPs  If one thinks of a POMDP as an MDP defined over belief states, then the well-known fixed  point equations for MDPs still hold. Specifically,  oO  where 7 is the discount factor and b  (defined above) is the next belief state. The optimal  policy is determined by the maximizing action for each belief state. In principle, we could  use Q-learning or value iteration directly to solve POMDPs. The main difficulty lies in the  fact that there are uncountably many belief states, making a tabular representation of the  value function impossible.  Exact methods for POMDPs use the fact that finite horizon value functions are piecewise-  linear and convex [11], ensuring a finite representation. While finite, this representation  can grow exponentially with the horizon, making exact approaches impractical in most set-  tings. Function approximation is an attractive alternative to exact methods. We implement  function approximation using a set of parameterized Q-functions, where Q, (b, W,) is the  reward-to-go for taking action a in belief state b. A value function is reconstructed from the  Q-functions as V(b) = max (Q (b, W)), and the update rule for W when a transition  Reinforcement Learning Using Approximate Belief States 1039  from state b to b' under action a with reward R is:  /xw. = .(.v(o') + ,- - w.))Vwa w.)  2.3 Function approximation architectures  We consider two types of function approximators. The first is a two-layer feedforward  neural network with sigmoidal internal units and a linear outermost layer. We used one  network for each Q function. For full belief state reinforcement learning, we used networks  with [$[ inputs (one for each component of the belief state) and [X/ hidden nodes. For  approximate belief state reinforcement learning, we used networks with one input for each  assignment to the variables in each cluster. If we had two clusters, for example, each with  3 binary variables, then our Q networks would each have 23 + 23 = 16 inputs. We kept the  number of hidden nodes for each network as the square root of the number of inputs.  Our second function approximator is SPOVA [ 10], which is a soft max function designed  to exploit the piecewise-linear structure of POMDP value functions. A SPOVA Q function  maintains a set of weight vectors w,... wai , and is evaluated as:  In practice, a small value of k (usually 1.2) is adopted at the start of learning, making the  function very smooth. This is increased during learning until SPOVA closely approximates  a PWLC function of b (usually k = 8). We maintained one SPOVA Q function for each  action and assigned [x/ vectors to each function. This gave O([.AII$1 ]V/) parameters  to both SPOVA and the full belief state neural network.  3 Empirical Results  We present results on several problems from the POMDP literature and present an extension  to a known machine repair problem that is designed to highlight the effects of approximate  belief states. Our results are presented in the form of performance graphs, where the value  of the current policy is obtained by taking a snapshot of the value function and measuring  the discounted sum of reward obtained by the resulting policy in simulation. We use "NN"  to refer to the neural network trained reinforcement learner trained with the full belief state  and the term "Decomposed NN" to refer to the neural network trained with an approxi-  mate belief which is decomposed as a product of marginals. We used a simple exploration  strategy, starting with a 0.1 probability of acting randomly, which decreased linearly to  0.01.  Due to space limitations, we are not able to describe each model in detail. However, we  used publicly available model description files from [5]. Table 3.4 shows the running times  of the different methods. These are generally much lower than what would be required to  solve these problems using exact methods.  3.1 Grid Worlds  We begin by considering two grid worlds, a 4 x 3 world from [ 10] and a 60-state world from  [7]. The 4 x 3 world contains only 11 states and does not have a natural decomposition  into state variables, so we compared SPOVA only with the full belief state neural network.  See http://www.cs.brown.edu/research/ai/pomdp/index.html. Note that this file format specifies  a starting distribution for each problem and our results are reported with respect to this starting dis-  tribution.  1040 A. Rodriguez, R. Parr and D. Koller  Figure 1' a) 3 x 4 Grid World, b) 60-state maze  The experimental results, which are averaged over 25 training runs and 100 simulations per  policy snapshot, are presented in Figure 1 a. They show that SPOVA learns faster than the  neural network, but that the network does eventually catch up.  The 60-state robot navigation problem [7] was amenable to a decomposed belief state ap-  proximation since its underlying state space comes from the product of 15 robot positions  and 4 robot orientations. We decomposed the belief state with two clusters, one containing  a position state variable and the other containing an orientation state variable. Figure lb  shows results in which SPOVA again dominates. The decomposed NN has trouble with this  problem because the effects of position and orientation on the value function are not easily  decoupled, i.e., the effect of orientation on value is highly state-dependent. This meant that  the decomposed NN was forced to learn a much more complicated function of its inputs  than the function learned by the network using the full belief state.  3.2 Aircraft Identification  Aircraft identification is another problem studied in Cassandra's thesis. It includes sensing  actions for identifying incoming aircraft and actions for attacking threatening aircraft. At-  tacks against friendly aircraft are penalized, as are failures to intercept hostile aircraft. This  is a challenging problem because there is tension in deciding between the various sensors.  :' Better sensors tend to make the base more visible to hostile aircraft, while more stealthy  sensors are less accurate. The sensors give information about both the aircraft's type and  distance from the base.  The state space of this problem is comprised of three main components. aircraft  type -- either the aircraft is a friend or it is a foe; dis tance -- how far the aircraft  is currently from the base discretized into an adjustable number, d, of distinct distances;  visibility -- a measure of how visible the base is to the approaching aircraft, which  is discretized into 5 levels.  We chose d = 10, gaving this problem 104 states. The problem has a natural decomposition  into state variables for aircraft type, distance and base visibility. The results for the three  algorithms are shown in Figure 2(a). This is the first problem where we start to see an  advantage from decomposing the belief state. For the decomposed NN, we used three  separate clusters, one for each variable, which meant that the network had only 17 inputs.  Not only did the simpler network learn faster, but it learned a better policy overall. We  believe that this illustrates an important point: even though SPOVA and the full belief state  neural network may be more expressive than the decomposed NN, the decomposed NN  is able to search the space of functions it can represent much more efficiently due to the  reduced number of parameters.  Reinforcement Learning Using Approximate Belief States 1041  o  NN .......  Figure 2: a) Aircraft Identification, b) Machine Maintenance  3.3 Machine Maintenance  Our last problem was the machine maintenance problem from Cassandra's database. The  problem assumes that there is a machine with a certain number of components. The quality  of the parts produced by the machine is determined by the condition of the components.  Each component can be in one of four conditions: good . the component is in good  condition; fair -- the component has some amount of wear, and would benefit from  some maintenance; bad -- the part is very worn and could use repairs; broken -- the  part is broken and must be replaced. The status of the components is observable only if the  machine is completely disassembled.  Figure 2(b) shows performance results for this problem for the 4 component version of this  problem. At 256 states, it was at the maximum size for which a full belief state approach  was manageable. However, the belief state for this problem decomposes naturally into  clusters describing the status of each machine, creating a decomposed belief state with  just four components. The graph shows the dominance of this this simple decomposition  approach. We believe that this problem clearly demonstrates the advantage of belief state  decomposition: The decomposed NN learns a function of 16 inputs in fraction of the time  it takes for the full net or SPOVA to learn a lower-quality function of 256 inputs.  3.4 Running Times  The table below shows the running times for the different problems presented above. These  are generally much less than what would be required to solve these problems exactly. The  full NN and SPOVA are roughly comparable, but the decomposed neural network is con-  siderably faster. We did not exploit any problem structure in our approximate belief state  computation, so the time spent computing belief states is actually larger for the decom-  posed NN. The savings comes from the the reduction in the number of parameters used,  which reduced the number of partial derivatives computed. We expect the savings to be  significantly more substantial for processes represented in a factored way [3], as the ap-  proximate belief state propagation algorithm can also take advantage of this additional  structure.  4 Concluding Remarks  We have a proposed a new approach to belief state reinforcement learning through the use  of approximate belief states. Using well-known examples from the POMDP literature, we  have compared approximate belief state reinforcement learning with two other methods  1042 A. Rodriguez, R. Parr andD. Koller  Problem SPOVA NN  Decomposed NN  3x4 19.1 s 13.0 s  Hallway 32.8 min 47.1 min 3.2 min  Aircraft ID 38.3 min 49.9 min 4.4 min  Machine M. 2.5 h 2.6 h 4.7 min  Table 1: Run times (in seconds, minutes or hours) for the different algorithms  that use exact belief states. Our results demonstrate that, while approximate belief states  may not be ideal for tightly coupled problem features, such as the position and orientation  of a robot, they are a natural and effective means of addressing some large problems. Even  for the medium-sized problems we showed here, approximate beJief state reinforcement  learning can outperform full belief state reinforcement learning using fewer trials and much  less CPU time. For many problems, exact belief state methods will simply be impractical  and approximate belief states will provide a tractable alternative.  Acknowledgements  This work was supported by the ARO under the MURI program "Integrated Approach to  Intelligent Systems," by ONR contract N66001-97-C-8554 under DARPA's HPKB pro-  gram, and by the generosity of the Powell Foundation and the Sloan Foundation.  References  [10]  [11]  [12]  [1] K. J. Astrom. Optimal control of Markov decision processes with incomplete state  estimation. J. Math. Anal. Applic., 10:174-205, 1965.  [2] R.E. Bellman. Dynamic Programming. Princeton University Press, 1957.  [3] C. Boutilier, T. Dean, and S. Hanks. Decision theoretic planning: Structural assump-  tions and computational leverage. Journal of Artificial Intelligence Research, 1999.  [4] X. Boyen and D. Koller. Tractable inference for complex stochastic processes. In  Proc. UAI, 1998.  [5] A. Cassandra. Exact and approximate Algorithms for partially observable Markov  Decision Problems. PhD thesis, Computer Science Dept., Brown Univ., 1998.  [6] M. Littman. Algorithms for Sequential Decision Making. PhD thesis, Computer  Science Dept., Brown Univ., 1996.  [7] M. Littman, A. Cassandra, and L.P. Kaelbling. Learning policies for partially observ-  able environments: Scaling up. In Proc. ICML, pages 362-370, 1996.  [8] J. Loch and S. Singh. Using eligibility traces to find the best memoryless policy in  partially observable markov decision processes. In Proc. ICML. Morgan Kaufmann,  1998.  [9] Andrew R. McCallurn. Overcoming incomplete perception with utile distinction  memory. In Proc. ICML, pages 190-196, 1993.  Ronald Parr and Stuart Russell. Approximating optimal policies for partially observ-  able stochastic domains. In Proc. IJCAI, 1995.  R. D. Smallwood and E. J. Sondik. The optimal control of partially observable  Markov processes over a finite horizon. Operations Research, 21:1071-1088, 1973.  M. Wiering and J. Schmidhuber. HQ-learning: Discovering Markovian subgoals for  non-Markovian reinforcement learning. Technical report, Istituo Dalle Molle di Studi  sull'Intelligenza Artificiale, 1996.  
An Environment Model for Nonstationary  Reinforcement Learning  Samuel P.M. Choi Dit-Yan Yeung Nevin L. Zhang  pmchoics. ust. hk dyyeung cs. ust. hk lzhang cs. ust. hk  Department of Computer Science, Hong Kong University of Science and Technology  Clear Water Bay, Kowloon, Hong Kong  Abstract  Reinforcement learning in nonstationary environments is generally  regarded as an important and yet difficult problem. This paper  partially addresses the problem by formalizing a subclass of nonsta-  tionary environments. The environment model, called hidden-mode  Markov decision process (HM-MDP), assumes that environmental  changes are always confined to a small number of hidden modes.  A mode basically indexes a Markov decision process (MDP) and  evolves with time according to a Markov chain. While HM-MDP  is a special case of partially observable Markov decision processes  (POMDP), modeling an HM-MDP environment via the more gen-  eral POMDP model unnecessarily increases the problem complex-  ity. A variant of the Baum-Welch algorithm is developed for model  learning requiring less data and time.  I Introduction  Reinforcement Learning (RL) [7] is a learning paradigm based upon the framework  of Markov decision process (MDP). Traditional RL research assumes that environ-  ment dynamics (i.e., MDP parameters) are always fixed (i.e., stationary). This  assumption, however, is not realistic in many real-world applications. In elevator  control [3], for instance, the passenger arrival and departure rates can vary signifi-  canfly over one day, and should not be modeled by a fixed MDP.  Nonetheless, RL in nonstationary environments is regarded as a difficult problem.  In fact, it is an impossible task if there is no regularity in the ways environment  dynamics change. Hence, some degree of regularity must be assumed. Typically,  nonstationary environments are presummed to change slowly enough such that on-  line RL algorithms can be employed to keep track the changes. The online approach  is memoryless in the sense that even if the environment ever revert to the previously  learned dynamics, learning must still need to be started all over again.  988 S. P M. Choi, D.-Y. Yeung and N. L. Zhang  1.1 Our Proposed Model  This paper proposes a formal model [1] for the nonstationary environments that  repeats their dynamics in certain ways. Our model is inspired by the observations  from the real-world nonstationary tasks with the following properties:  Property 1. Environmental changes are confined to a small number of modes,  which are stationary environments with distinct dynamics. The environment is in  exactly one of these modes at any given time. This concept of modes seems to be  applicable to many real-world tasks. In an elevator control problem, for example,  the system might operate in a morning-rush-hour mode, an evening-rush-hour mode  and a non-rush-hour mode. One can also imagine similar modes for other control  tasks, such as traffic control and dynamic channel allocation [6].  Property 2. Unlike states, modes cannot be directly observed; the current mode  can only be estimated according to the past state transitions. It is analogous to the  elevator control example in that the passenger arrival rate and pattern can only be  inferred through the occurrence of pick-up and drop-off requests.  Property 3. Mode transitions are stochastic events and are independent of the  control system's responses. In the elevator control problem, for instance, the events  that change the current mode of the environment could be an emergency meeting  in the administrative office, or a tea break for the staff on the 10th floor. Obviously,  the elevator's response has no control over the occurrence of these events.  Property 4. Mode transitions are relatively infrequent. In other words, a mode  is more likely to retain for some time before switching to another one. If we consider  the emergency meeting example, employees on different floors take time to arrive at  the administrative office, and thus would generate a similar traffic pattern (drop-off  requests on the same floor) for some period of time.  Property 5. The number of states is often substantially larger than the number  of modes. This is a common property for many real-world applications. In the  elevator example, the state space comprises all possible combinations of elevator  positions, pick-up and drop-off requests, and certainly would be huge. On the other  hand, the mode space could be small. For instance, an elevator control system can  simply have the three modes as described above to approximate the reality.  Based on these properties, an environment model is proposed by introducing a  mode variable to capture environmental changes. Each mode specifies an MDP  and hence completely determines the current state transition function and reward  function (property 1). A mode, however, is not directly observable (property 2),  and evolves with time according to a Markov process (property 3). The model  is therefore called hidden-mode model. Note that our model does not impose any  constraint to satisfy properties 4 and 5. In other words, the hidden-mode model  can work for environments without these two properties. Nevertheless, as will be  shown later, these properties can improve learning in practice.  1.2 Related Work  Our hidden-mode model is related to a nonstationary model proposed by Dayan and  Sejnowski [4]. Although our model is more restrictive in terms of representational  power, it involves much fewer parameters and is thus easier to learn. Besides, other  than the number of possible modes, we do not assume any other knowledge about  An Environment Model for Nonstationary Reinforcement Learning 989  the way environment dynamics change. Dayan and Sejnowski, on the other hand,  assume that one knows precisely how the environment dynamics change.  The hidden-mode model can also be viewed as a special case of the hidden-state  model, or partially observable Markov decision process (POMDP). As will be shown  later, a hidden-mode model can always be represented by a hidden-state model  through state augmentation. Nevertheless, modeling a hidden-mode environment  via a hidden-state model will unnecessarily increase the problem complexity. In this  paper, the conversion from the former to the latter is also briefly discussed.  1.3 Our Focus  There are two approaches for RL. Model-based RL first acquires an environment  model and then, from which, an optimal policy is derived. Model-free RL, on the  contrary, learns an optimal policy directly through its interaction with the envi-  ronment. This paper is concerned with the first part of the model-based approach,  i.e., how a hidden-mode model can be learned from experience. We will address the  policy learning problem in a separate paper.  2 Hidden-Mode Markov Decision Processes  This section presents our hidden-mode model. Basically, a hidden-mode model is  defined as a finite set of MDPs that share the same state space and action space, with  possibly different transition functions and reward functions. The MDPs correspond  to different modes in which a system operates. States are completely observable  and their transitions are governed by an MDP. In contrast, modes are not directly  observable and their transitions are controlled by a Markov chain. We refer to such  a process as a hidden-mode Markov decision process (HM-MDP). An example of  HM-MDP is shown in Figure l(a).    Time  , ,es Mode  Q'"fft ff? Actloll  ....... State  (a) A 3-mode, 4-state,  1-action HM-MDP  (b) The evolution of an HM-MDP. The arcs indicate  dependencies between the variables  Figure 1: An HM-MDP  Formally, an HM-MDP is an 8-tuple (Q, S, A, X, Y, R, H, 2), where Q, S and A  represent the sets of modes, states and actions respectively; the mode transition  function X maps mode m to n with a fixed probability Zm,; the state transition  function Y defines transition probability, ym(S, a, s), from state s to s  given mode  m and action a; the stochastic reward function R returns rewards with mean value  rm(s,a); II and  denote the prior probabilities of the modes and of the states  respectively. The evolution of modes and states over time is depicted in Figure 1 (b).  990 S. P M. Choi, D.-Y. Yeung and N. L. Zhang  HM-MDP is a subclass of POMDP. In other words, the former can be reformulated  as a special case of the latter. Specifically, one may take an ordered pair of any  mode and observable state in the HM-MDP as a hidden state in the POMDP, and  any observable state of the former as an observation of the latter. Suppose the  observable states s and s  are in modes rn and n respectively. These two HM-  MDP states together with their corresponding modes form two hidden states (ra, s)  and (n, s ) for its POMDP counterpart. The transition probability from (ra, s) to  (n, s ) is then simply the mode transition probability Xm, multiplied by the state  transition probability ym(s,a, s). For an M-mode, N-state, K-action HM-MDP,  the equivalent POMDP thus has N observations and MN hidden states. Since  most state transition probabilities are collapsed into mode transition probabilities  through parameter sharing, the number of parameters in an HM-MDP (N2MK +  M 2) is much less than that of its corresponding POMDP (M2N2K).  3 Learning a Hidden-Mode Model  There are now two ways to learn a hidden-mode model. One may learn either an  HM-MDP, or an equivalent POMDP instead. POMDP models can be learned via  a variant of the Baum-Welch algorithm [2]. This POMDP Baum-Welch algorithm  requires O(MNT) time and O(M2N2K) storage for learning an M-mode, N-  state, K-action HM-MDP, given T data items.  A similar idea can be applied to the learning of an HM-MDP. Intuitively, one  can estimate the model parameters based on the expected counts of the mode  transitions, computed by a set of auxiliary variables. The major difference from the  original algorithm is that consecutive state transitions, rather than the observations,  are considered. Additional effort is thus needed for handling the boundary cases.  This HM-MDP Baum-Welch algorithm is described in Figure 2.  4 Empirical Studies  This section empirically examines the POMDP Baum-Welch  and HM-MDP Baum-  Welch algorithms. Experiments based on various randomly generated models and  some real-world environments were conducted. The results are quite consistent.  For illustration, a simple traffic control problem is presented. In this problem,  one direction of a two-way traffic is blocked, and cars from two different directions  (left and right) are forced to share the remaining road. To coordinate the traffic,  two traffic lights equipped with sensors are set. The system then has two possible  actions: either to signal cars from the left or cars from the right to pass. For  simplicity, we assume discrete time steps and uniform speed of the cars.  The system has 8 possible states; they correspond to the combinations of whether  there are cars waiting on the left and the right directions, and the stop signal position  in the previous time step. There are 3 traffic modes. The first one has cars waiting  on the left and the right directions with probabilities 0.3 and 0.1 respectively. In the  second mode, these probabilities are reversed. For the last one, both probabilities  are 0.3. In addition, the mode transition probability is 0.1. A cost of-1.0 results if  XChrisman's algorithm also attempts to learn a minimal possible number of states. Our  paper concerns only with learning the model parameters.  An Environment Model for Nonstationary Reinforcement Learning 991  Given a collection of data and an initial model parameter vector  repeat  O=g  Compute forward variables at.  (i) =   a2(i) = ri b yi(s,al,s2)  at+l (j) = ieQ at(i) xij yj(st, at, st+ )  ieQ  VieQ  VieQ  Compute backward variables  3T(i) = 1  ,(i) = o xo y(s,,a,,s,+) +(j)  (i) =   y(s,,,) (j)  ieQ  VieQ  VieQ  Compute auxiliary variables t and  , (i, j) =  ") ' gt (''+) +  )_.j}o r()  7t(i) = Y.jQ t+(i,j)  vi, jeq  i6Q  Compute the new model parameter g.  T  _ -=  (i,J)  Y. - ', (0 5(,,,)5(,,+,0 (,,)   = (i)  1 a=b  a(a,b) = 0 ab  until maxi IOi - Oil < e  Figure 2: HM-MDP Baum-Welch Algorithm  a car waits on either side.  The experiments were run with the same initial model for data sets of various sizes.  The algorithms iterated until the maximum change of the model parameters was  less than a threshold of 0.0001. The experiment was repeated for 20 times with  different random seeds in order to compute the median. Then the learned models  were compared in their POMDP forms using the Kullback-Leibler (KL) distance  [5], and the total CPU running time on a SUN Ultra I workstation was measured.  Figure 3 (a) and (b) report the results.  Generally speaking, both algorithms learn a more accurate environment model as  the data size increases (Figure 3 (a)). This result is expected as both algorithms  are statistically-based, and hence their performance relies largely on the data size.  When the training data size is very small, both algorithms perform poorly. However,  as the data size increases, HM-MDP Baum-Welch improves substantially faster than  POMDP Baum-Welch. It is because an HM-MDP in general consists of fewer free  992 S. P. M. Choi, D.-Y. Yeung and N. L. Zhang  (a) Error in transition function  (b) Required learning time  Figure 3: Empirical results on model learning  parameters than its POMDP counterpart.  HM-MDP Baum-Welch also runs much faster than POMDP Baum-Welch (Figure 3  (b)). It holds in general for the same reason discussed above. Note that compu-  tational time is not necessarily monotonically increasing with the data size. It is  because the total computation depends not only on the data size, but also on the  number of iterations executed. From our experiments, we noticed that the number  of iterations tends to decrease as the data size increases.  Larger models have also been tested. While HM-MDP Baum-Welch is able to learn  models with several hundred states and a few modes, POMDP Baum-Welch was  unable to complete the learning in a reasonable time. Additional experimental  results can be found in [1].  5 Discussions and Future Work  The usefulness of a model depends on the validity of the assumptions made. We  now discuss the assumptions of HM-MDP, and shed some light on its applicability  to real-world nonstationary tasks. Some possible extensions are also discussed.  Modeling a nonstationary environment as a number of distinct MDPs.  MDP is a flexible framework that has been widely adopted in various applications.  Modeling nonstationary environments by distinct MDPs is a natural extension to  those tasks. Comparing to POMDP, our model is more comprehensive: each MDP  naturally describes a mode of the environment. Moreover, this formulation facili-  tates the incorporation of prior knowledge into the model initialization step.  States are directly observable while modes are not. While completely ob-  servable states are helpful to infer the current mode, it is also possible to extend the  mode] to allow partially observable states. In this case, the extended model would  be equivalent in representational power to a POMDP. This could be proved easily  by showing the reformulation of the two models in both directions.  An Environment Model for Nonstationary Reinforcement Learning 993  Mode changes are independent of the agent's responses. This property  may not always hold for all real-world tasks. In some applications, the agent's  actions might affect the state as well as the environment mode. In that case, an  MDP should be used to govern the mode transition process.  Mode transitions are relatively infrequent. This is a property that generally  holds in many applications. Our model, however, is not limited by this condition.  We have tried to apply our model-learning algorithms to problems in which this  property does not hold. We find that our model still outperforms POMDP, although  the required data size is typically larger for both models.  Number of states is substantially larger than the number of modes. This  is the key property that significantly reduces the number of parameters in HM-MDP  compared to that in POMDP. In practice, introduction of a few modes is sufficient  for boosting the system performance. More modes might only help little. Thus a  trade-off between performance and response time must be decided.  There are additional issues that need to be addressed. First, an efficient algorithm  for policy learning is required. Although in principle it can be achieved indirectly  via any POMDP algorithm, a more efficient algorithm based on the model-based  approach is possible. We will address this issue in a separate paper. Next, the  number of modes is currently assumed to be known. We are now investigating how  to remove this limitation. Finally, the exploration-exploitation issue is currently  ignored. In our future work, we will address this important issue and apply our  model to real-world nonstationary tasks.  References  [1]  S. P.M. Choi, D. Y. Yeung, and N. L. Zhang.  processes. In IJCAI 99 Workshop on Neural,  Methods for Sequence Learning, 1999.  Hidden-mode Markov decision  Symbolic, and Reinforcement  [2] L. Chrisman. Reinforcement learning with perceptual aliasing: The perceptual  distinctions approach. In AAAI-9, 1992.  [3]  R. H. Crites and A. G. Barto. Improving elevator performance using reinforce-  ment learning. In D. Touretzky, M. Mozer, and M. Hasselmo, editors, Advances  in Neural Information Processing Systems 8, 1996.  [4] P. Dayan and T. J. Sejnowski. Exploration bonuses and dual control. Machine  Learning, 25(1):5-22, Oct. 1996.  [5] S. Kullback. Information Theory and Statistics. Wiley, New York, NY, USA,  1959.  [6]  S. Singh and D. P. Bertsekas. Reinforcement learning for dynamic channel  allocation in cellular telephone systems. In Advances in Neural Information  Processing Systems 9, 1997.  [7] R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. The  MIT Press, 1998.  
Effects of Spatial and Temporal Contiguity on  the Acquisition of Spatial Information  Thea B. Ghiselli-Crippa and Paul W. Munro  Department of Information Science and Telecommunications  University of Pittsburgh  Pittsburgh, PA 15260  tbgst@ sis.pitt. edu, munro@ sis.pitt. edu  Abstract  Spatial information comes in two forms: direct spatial information (for  example, retinal position) and indirect temporal contiguity information,  since objects encountered sequentially are in general spatially close. The  acquisition of spatial information by a neural network is investigated  here. Given a spatial layout of several objects, networks are trained on a  prediction task. Networks using temporal sequences with no direct spa-  tial information are found to develop internal representations that show  distances correlated with distances in the external layout. The influence  of spatial information is analyzed by providing direct spatial information  to the system during training that is either consistent with the layout or  inconsistent with it. This approach allows examination of the relative  contributions of spatial and temporal contiguity.  1 Introduction  Spatial information is acquired by a process of exploration that is fundamentally tempo-  ral, whether it be on a small scale, such as scanning a picture, or on a larger one, such as  physically navigating through a building, a neighborhood, or a city. Continuous scanning  of an environment causes locations that are spatially close to have a tendency to occur in  temporal proximity to one another. Thus, a temporal associative mechanism (such as a  Hebb rule) can be used in conjunction with continuous exploration to capture the spatial  structure of the environment [ 1]. However, the actual process of building a cognitive map  need not rely solely on temporal associations, since some spatial information is encoded in  the sensory array (position on the retina and proprioceptive feedback). Laboratory studies  show different types of interaction between the relative contributions of temporal and spa-  tial contiguities to the formation of an internal representation of space. While Clayton and  Habibi's [2] series of recognition priming experiments indicates that priming is controlled  only by temporal associations, in the work of McNamara et al. [3] priming in recogni-  tion is observed only when space and time are both contiguous. In addition, Curiel and  Radvansky's [4] work shows that the effects of spatial and temporal contiguity depend on  whether location or identity information is emphasized during learning. Moreover, other  experiments ([3]) also show how the effects clearly depend on the task and can be quite  different if an explicitly spatial task is used (e.g., additive effects in location judgments).  18 T. B. Ghiselli-Crippa and P. W. Munro  labels labels  labels  (A coeff.)  ,  labels labels coordinates labels  coordinates  (B coeff.)  Figure 1: Network architectures: temporal-only network (left); spatio-temporal network  with spatial units part of the input representation (center); spario-temporal network with  spatial units part of the output representation (right).  2 Network architectures  The goal of the work presented in this paper is to study the structure of the internal rep-  resentations that emerge from the integration of temporal and spatial associations. An  encoder-like network architecture is used (see Figure 1), with a set of N input units and a  set of N output units representing N nodes on a 2-dimensional graph. A set of H units is  used for the hidden layer. To include space in the learning process, additional spatial units  are included in the network architecture. These units provide a representation of the spatial  information directly available during the learning/scanning process. In the simulations de-  scribed in this paper, two units are used and are chosen to represent the (x, y) coordinates of  the nodes in the graph. The spatial units can be included as part of the input representation  or as part of the output representation (see Figure 1, center and right panels): both choices  are used in the experiments, to investigate whether the spatial information could better ben-  efit training as an input or as an output [5]. In the second case, the relative contribution of  the spatial information can be directly manipulated by introducing weighting factors in the  cost function being minimized. A two-term cost function is used, with a cross-entropy term  for the N label units and a squared error term for the 2 coordinate units,  I N ]  E = A - E tilog2(ri) + (1 - ti)log2(1 - ri)  i----1  q-Z E(ti-ri) 2 (1)  i=1  ri indicates the actual output of unit i and ti its desired output. The relative influence of  the spatial information is controlled by the coefficients A and B.  3 Learning tasks  The left panel of Figure 2 shows an example of the type of layout used; the effective  layout used in the study consists of N -- 28 nodes. For each node, a set of neighboring  nodes is defined, chosen on the basis of how an observer might scan the layout to learn the  node labels and their (spatial) relationships; in Figure 2, the neighborhood relationships are  represented by lines connecting neighboring nodes. From any node in the layout, the only  allowed transitions are those to a neighbor, thus defining the set of node pairs used to train  the network (66 pairs out of C(28, 2) = 378 possible pairs). In addition, the probability  of occurrence of a particular transition is computed as a function of the distance to the  corresponding neighbor. It is then possible to generate a sequence of visits to the network  nodes, aimed at replicating the scanning process of a human observer studying the layout.  Spatiotemporal Contiguity Effects on Spatial Information Acquisition 19  cup  eraser  coin  cup  gun oin- ' - eraser  c  button  Figure 2: Example of a layout (left) and its permuted version (right). Links represent  allowed transitions. A larger layout of 28 units was used in the simulations.  The basic learning task is similar to the grammar learning task of Servan-Schreiber et al.  [6] and to the neighborhood mapping task described in [1] and is used to associate each of  the N nodes on the graph and its (a:, t) coordinates with the probability distribution of the  transitions to its neighboring nodes. The mapping can be learned directly, by associating  each node with the probability distribution of the transitions to all its neighbors: in this  case, batch learning is used as the method of choice for learning the mapping. On the  other hand, the mapping can be learned indirectly, by associating each node with itself  and one of its neighbors, with online learning being the method of choice in this case;  the neighbor chosen at each iteration is defined by the sequence of visits generated on  the basis of the transition probabilities. Batch learning was chosen because it generally  converges more smoothly and more quickly than online learning and gives qualitatively  similar results. While the task and network architecture described in [1] allowed only  for temporal association learning, in this study both temporal and spatial associations are  learned simultaneously, thanks to the presence of the spatial units. However, the temporal-  only (T-only) case, which has no spatial units, is included in the simulations performed  for this study, to provide a benchmark for the evaluation of the results obtained with the  spatio-temporal (S-T) networks.  The task described above allows the network to learn neighborhood relationships for which  spatial and temporal associations provide consistent information, that is, nodes experienced  contiguously in time (as defined by the sequence) are also contiguous in space (being spa-  tial neighbors). To tease apart the relative contributions of space and time, the task is kept  the same, but the data employed for training the network is modified: the same layout is  used to generate the temporal sequence, but the a:, t coordinates of the nodes are randomly  permuted (see right panel of Figure 2). If the permuted layout is then scanned following the  same sequence of node visits used in the original version, the net effect is that the temporal  associations remain the same, but the spatial associations change so that temporally neigh-  boring nodes can now be spatially close or distant: the spatial associations are no longer  consistent with the temporal associations. As Figure 4 illustrates, the training pairs (filled  circles) all correspond to short distances in the original layout, but can have a distance  anywhere in the allowable range in the permuted layout. Since the temporal and spatial  distances were consistent in the original layout, the original spatial distance can be used  as an indicator of temporal distance and Figure 4 can be interpreted as a plot of temporal  distance vs. spatial distance for the permuted layout.  The simulations described in the following include three experimental conditions: temporal  only (no direct spatial information available); space and time consistent (the spatial coor-  dinates and the temporal sequence are from the same layout); space and time inconsistent  (the spatial coordinates and the temporal sequence are from different layouts).  20 T. B. Ghiselli-Crippa and P. W. Munro  Hidden unit representations are compared using Euclidean distance (cosine and inner prod-  uct measures give consistent results); the internal representation distances are also used to  compute their correlation with Euclidean distances between nodes in the layout (original  and permuted). The correlations increase with the number of hidden units for values of  H between 5 and 10 and then gradually taper off for values greater than 10. The results  presented in the remainder of the paper all pertain to networks trained with H = 20 and  with hidden units using a tanh transfer function; all the results pertaining to S-T networks  refer to networks with 2 spatial output units and cost function coefficients A = 0.1525 and  B = 6.25.  4 Results  Figure 3 provides a combined view of the results from all three experiments. The left panel  illustrates the evolution of the correlation between internal representation distances and  layout (original and permuted) distances. The right panel shows the distributions of the  correlations at the end of training (1000 epochs). The first general result is that, when spa-  tial information is available and consistent with the temporal information (original layout),  the correlation between hidden unit distances and layout distances is consistently better  than the correlation obtained in the case of temporal associations alone. The second gen-  eral result is that, when spatial information is available but not consistent with the temporal  information (permuted layout), the correlation between hidden unit distances and original  layout distances (which represent temporal distances) is similar to that obtained in the case  of temporal associations alone, except for the initial transient. When the correlation is com-  puted with respect to the permuted layout distances, its value peaks early during training  and then decreases rapidly, to reach an asymptotic value well below the other three cases.  This behavior is illustrated in the boxplots in the right panel of Figure 3, which report the  distribution of correlation values at the end of training.  4.1 Temporal-only vs. spatio-temporal  As a first step in this study, the effects of adding spatial information to the basic temporal  associations used to train the network can be examined. Since the learning task is the same  for both the T-only and the S-T networks except for the absence or presence of spatial  information during training, the differences observed can be attributed to the additional  spatial information available to the S-T networks. The higher correlation between internal  representation distances and original layout distances obtained when spatial information is  S and T consistertl  S and T Incorlslstenl  (corr with T distance)  S and T lnconsstent  (corr. wth S distance)  0 200 400 600 800 100{ S and T T-only S and T S and T  consistent inconsistent irtconslstenl  mx'nber o! epochs (corr with T dst ) (corr vath S (list)  Figure 3: Evolution of correlation during training (0 - 1000 epochs) (left). Distributions of  correlations at the end of training (1000 epochs) (right).  Spatiotemporal Contiguity Effects on Spatial Information Acquisition 21  O0 02 04 08 08 10 I 2  orJ_d  dHU = 0.6 + 3.4d T + 0.3d$ - 2.1(dT) 2 + 0.4(d$) 2 - 0.4dTd $  0 0 02  Figure 4: Distances in the original layout  (:r) vs. distances in the permuted layout  (y). The 66 training pairs are identified by  filled circles.  Figure 5: Similarities (Euclidean distances)  between internal representations developed  by a S-T network (after 300 epochs). Figure  4 projects the data points onto the :r, y plane.  available (see Figure 3) is apparent also when the evolution of the internal representations  is examined. As Figure 6 illustrates, the presence of spatial information results in better  generalization for the pattern pairs outside the training set. While the distances between  training pairs are mapped to similar distances in hidden unit space for both the T-only and  the S-T networks, the T-only network tends to cluster the non-training pairs into a narrow  band of distances in hidden unit space. In the case of the S-T network instead, the hidden  unit distances between non-training pairs are spread out over a wider range and tend to  reflect the original layout distances.  4.2 Permuted layout  As described above, with the permuted layout it is possible to decouple the spatial and  temporal contributions and therefore study the effects of each. A comprehensive view of  the results at a particular point during training (300 epochs) is presented in Figure 5, where  the :r, y plane represents temporal distance vs. spatial distance (see also Figure 4) and the z  axis represents the similarity between hidden unit representations. The figure also includes  a quadratic regression surface fitted to the data points. The coefficients in the equation of  the surface provide a quantitative measure of the relative contributions of spatial (ds) and  temporal distances (dr) to the similarity between hidden unit representations  dHv = ko + kldT + k2d$ + k3(dT) 2 + k4(d$) 2 + ksdrd$  (2)  In general, after the transient observed in early training (see Figure 3), the largest and most  significant coefficients are found for dr and (dr) 2, indicating a stronger dependence of  dHU on temporal distance than on spatial distance.  The results illustrated in Figure 5 represent the situation at a particular point during training  (300 epochs). Similar plots can be generated for different points during training, to study  the evolution of the internal representations. A different view of the evolution process is  provided by Figure 7, in which the data points are projected onto the x,z plane (top panel)  and the y,z plane (bottom panel) at four different times during training. In the top panel,  22 T. B. Ghiselli-Crippa and P. W. Munro  00 02 04 06 08 10 12 00 02 04 06 o& 10 1 00 02 04 06 08 10 12 00 02 04 06 08 10 12  00 02 o,i, 06 08 0 12 00 02 04 06 08 10 12 00 02 04 06 08 10 12 00 02 04 06 o 10 12  Figure 6: Internal representation distances vs. original layout distances: S-T network (top)  vs. T-only network (bottom). The training pairs are identified by filled circles. The presence  of spatial information results in better generalization for the pairs outside the training set.  the internal representation distances are plotted as a function of temporal distance (i.e., the  spatial distance from the original layout), while in the bottom panel they are plotted as a  function of spatial distance (from the permuted layout). The higher asymptotic correlation  between internal representation distances and temporal distances, as opposed to spatial  distances (see Figure 3), is apparent also from the examination of the evolutionary plots,  which show an asymptotic behavior with respect to temporal distances (see Figure 7, top  panel) very similar to the T-only case (see Figure 6, bottom panel).  5 Discussion  The first general conclusion that can be drawn from the examination of the results described  in the previous section is that, when the spatial information is available and consistent with  the temporal information (original layout), the similarity structure of the hidden unit rep-  resentations is closer to the structure of the original layout than that obtained by using  temporal associations alone. The second general conclusion is that, when the spatial in-  formation is available but not consistent with the temporal information (permuted layout),  the similarity structure of the hidden unit representations seems to correspond to temporal  more than spatial proximity. Figures 5 and 7 both indicate that temporal associations take  precedence over spatial associations. This result is in agreement with the results described  in [1], showing how temporal associations (plus some high-level constraints) significantly  contribute to the internal representation of global spatial information. However, spatial in-  formation certainly is very beneficial to the (temporal) acquisition of a layout, as proven by  the results obtained with the S-T network vs. the T-only network.  In terms of the model presented in this paper, the results illustrated in Figures 5 and 7 can  be compared with the experimental data reported for recognition priming ([2], [3], [4]),  with distance between internal representations corresponding to reaction time. The results  of our model indicate that distances in both the spatially far and spatially close condition  appear to be consistently shorter for the training pairs (temporally close) than for the non-  training pairs (temporally distant), highlighting a strong temporal effect consistent with the  data reported in [2] and [4] (for spatially far pairs) and in [3] (only for the spatially close  Spatiotemporal Contiguity Effects on Spatial Information Acquisition 23  00 02 04 06 08 10 12 00 02 04 05 08 10 12 00 02 0.4 06 08 10 12 00 02 04 {36 08 10 1  m_d ) )n_cl ) tn_d ) in_d (T)  {30 02 04 06 0S 10 12 O0 0.2 04 06 O! 10 12 0.0 02 04 06 08 10 1 O0 02 04 06 08 1{3 1  m_d ($) m_d ($) m_d (S) n_d (S)  Figure 7: Internal representation distances vs. temporal distances (top) and vs. spatial  distances (bottom) for a S-T network (permuted layout). The training pairs are identified  by filled circles. The asymptotic behavior with respect to temporal distances (top panel) is  similar to the T-only condition. The bottom panel indicates a weak dependence on spatial  distances.  case). For the training pairs (temporally close), slightly shorter distances are obtained for  spatially close pairs vs. spatially far pairs; this result does not provide support for the  experimental data reported in either [3] (strong spatial effect) or [2] (no spatial effect).  For the non-training pairs (temporally distant), long distances are found throughout, with  no strong dependence on spatial distance; this effect is consistent with all the reported  experimental data. Further simulations and statistical analyses are necessary for a more  conclusive comparison with the experimental data.  References  [1] Ghiselli-Crippa, T.B. & Munro, P.W. (1994). Emergence of global structure from local associa-  tions. In J.D. Cowan, G. Tesauro, & J. Alspector (Eds.), Advances in Neural Information Processing  Systems 6, pp. 1101-1108. San Francisco, CA: Morgan Kaufmann.  [2] Clayton, K.N. & Habibi, A. (1991). The contribution of temporal contiguity to the spatial priming  effect. Journal of Experimental Psychology.' Learning, Memory, and Cognition 17:263-271.  [3] McNamara, T.P., Halpin, J.A. & Hardy, J.K. (1992). Spatial and temporal contributions to the  structure of spatial memory. Journal of Experimental Psychology.' Learning, Memory, and Cognition  18:555-564.  [4] Curiel, J.M. & Radvansky, G.A. (1998). Mental organization of maps. Journal t3fExperimental  Psychology: Learning, Memory, and Cognition 24:202-214.  [5] Caruana, R. & de Sa, V.R. (1997). Promoting poor features to supervisors: Some inputs work  better as outputs. In M.C. Mozer, M.I. Jordan, & T. Petsche (Eds.), Advances in Neural Information  Processing Systems 9, pp. 389-395. Cambridge, MA: MIT Press.  [6] Servan-Schreiber, D., Cleeremans, A. & McClelland, J.L. (1989). Learning sequential structure  in simple recurrent networks. In D.S. Touretzky (Ed.), Advances in Neural Information Processing  Systems 1, pp. 643-652. San Mateo, CA: Morgan Kaufmann.  
Dual Estimation and the Unscented  Transformation  Eric A. Wan  ericwan @ ece. ogi. edu  Rudolph van der Merwe  rudmerwe @ ece. ogi. edu  Alex T. Nelson  atnelson @ ece. ogi. edu  Oregon Graduate Institute of Science & Technology  Department of Electrical and Computer Engineering  20000 N.W. Walker Rd., Beaverton, Oregon 97006  Abstract  Dual estimation refers to the problem of simultaneously estimating the  state of a dynamic system and the model which gives rise to the dynam-  ics. Algorithms include expectation-maximization (EM), dual Kalman  filtering, and joint Kalman methods. These methods have recently been  explored in the context of nonlinear modeling, where a neural network  is used as the functional form of the unknown model. Typically, an ex-  tended Kalman filter (EKF) or smoother is used for the part of the al-  gorithm that estimates the clean state given the current estimated model.  An EKF may also be used to estimate the weights of the network. This  paper points out the flaws in using the EKF, and proposes an improve-  ment based on a new approach called the unscented transformation (UT)  [3]. A substantial performance gain is achieved with the same order of  computational complexity as that of the standard EKF. The approach is  illustrated on several dual estimation methods.  1 Introduction  We consider the problem of learning both the hidden states x and parameters w of a  discrete-time nonlinear dynamic system,  Xkq- 1 --- F(xk,vk,w) (1)  = (2)  where y is the only observed signal. The process noise v drives the dynamic system, and  the observation noise is given by n. Note that we are not assuming additivity of the noise  sources.  A number of approaches have been proposed for this problem. The dual EKF algorithm  uses two separate EKFs: one for signal estimation, and one for model estimation. The states  are estimated given the current weights and the weights are estimated given the current  states. In the joint EKF, the state and model parameters are concatenated within a combined  state vector, and a single EKF is used to estimate both quantities simultaneously. The  EM algorithm uses an extended Kalman smoother for the E-step, in which forward and  Dual Estimation and the Unscented Transformaa'on 667  backward passes are made through the data to estimate the signal. The model is updated  during a separate M-step.  For a more thorough treatment and a theoretical basis on how these algorithms relate, see  Nelson [6]. Rather than provide a comprehensive comparison between the different algo-  rithms, the goal of this paper is to point out the assumptions and flaws in the EKF (Sec-  tion 2), and offer a improvement based on the unscented transformation/filter (Section 3).  The unscented filter has recently been proposed as a substitute for the EKF in nonlinear  control problems (known dynamic model) [3]. This paper presents new research on the use  of the UF within the dual estimation framework for both state and weight estimation. In  the case of weight estimation, the UF represents a new efficient "second-order" method for  training neural networks in general.  2 Flaws in the EKF  Assume for now that we know the model (weight parameters) for the dynamic system in  Equations 1 and 2. Given the noisy observation y, a recursive estimation for  can be  expressed in the form,  (optimal prediction of x) + G x [yk - (optimal prediction of y)] (3)  This recursion provides the optimal MMSE estimate for x assuming the prior estimate  and current observation y are Gaussian. We need not assume linearity of the model. The  optimal terms in this recursion are given by  =  G: PxyP;Js,   .; = E[H(c,n)], (4)  where the optimal prediction : is the expectation of a nonlinear function of the random  variables c_ and v_ (similar interpretation for the optimal prediction of y). The  optimal gain term is expressed as a function of posterior covariance matrices (with 2 -  y - .). Note these terms also require taking expectations of a nonlinear function of the  prior state estimates.  The Kalman filter calculates these quantities exactly in the linear case. For nonlinear mod-  els, however, the extended KF approximates these as:    F(/c_,9) G m PxyP:J- r- = H( fi), (5)  y y   where predictions are approximated as simply the function of the prior mean value for es-  timates (no expectation taken). The covariance are determined by linearizing the dynamic  equations (x+ m Axk + Bye, y  CXk + Dn), and then determining the posterior  covariance matrices analytically for the linear system. As such, the EKF can be viewed  as providing "first-order" approximations to the optimal terms (in the sense that expres-  sions are approximated using a first-order Taylor series expansion of the nonlinear terms  around the mean values). While "second-order" versions of the EKF exist, their increased  implementation and computational complexity tend to prohibit their use.  3 The Unscented Transformation/Filter  The unscented transformation (UT) is a method for calculating the statistics of a random  variable which undergoes a nonlinear transformation [3]. Consider propagating a random  variable c (dimension L) through a nonlinear function,  = g(c). Assume c has mean &  and covariance P,. To calculate the statistics of , we form a matrix X of 2L + 1 sigma  vectors A'i, where the first vector (A'o) corresponds to &, and the rest are computed from  the mean (+)plus and (-)minus each column of the matrix square-root of P,. These sigma  668 E. A. Wan, R. v. d. Merwe and A. T. Nelson  vectors are propagated through the nonlinear function, and the mean and covariance for 1  are approximated using a weighted sample mean and covariance,  1 tg('o) + g(rt'i) , (6)  t,  1 n[g(&) - 3][g(&) - 3] T +  [g(Xi) - 3][g(Xi) - B] T (7)  PZ m L + i=  where n is a scaling hcton Note that this method differs substantially from general "sta-  pling" methods (e.g., Monte-C1o methods and pticle filters [1]) which requffe orders  of magnitude more staple poin in an attempt to propagate an accurate (possibly non-  Gaussian) distribution of the state. The UT approximations e accuram to the third order  for Gaussian inputs for all nonlineities. For non-Gaussian inputs, approximations e  accurate to at least the second-order, with the accuracy detemined by the choice of n [3].  A simple exmple is shown in Figure 1 for a 2-dimensional system: the left plots shows  e e mean and coviance propagation using Monte-C1o stapling; the center plots  show e performce of the  (note only 5 sigma points e requffed); the right plo  show e resulB using a lineization approach as would be done in e E. e superior  performance of the UT is clear.  Actual l[sampling)  true mean  UT Linearized (EKF)  sigma points  y = g(xd  1  UT  tme covariance / me?. UT coiariance  transformed sigma points  P = ArPA  I  ATPaA  Figure 1: Example of the UT for mean and covariance propagation.  UT, c) first-order linear (EKF).  a) actual, b)  The unscented filter (UF) [3] is a straightforward extension of the UT to the recursive  estimation in Equation 3, where we set c - , and denote the corresponding sigma matrix  as X(k[k). The UF equations are given on the next page. It is interesting to note that no  explicit calculation of Jacobians or Hessians are necessary to implement this algorithm.  The total number of computations is only order L 2 as compared to L  for the EKF.   4 Application to Dual Estimation  This section shows the use of the UF within several dual estimation approaches. As an ap-  plication domain for comparison, we consider modeling a noisy time-series as a nonlinear  Note that a matrix square-root using the Cholesky factorization is of order La/. However, the  covariance matrices are expressed recursively, and thus the square-root can be computed in only order  L 2 by performing a recursive update to the Cholesky factorization.  Dual Estimation and the Unscented Transformation 669  UF Equations  Wo = ,q( + ,) , w... w2L = 1/2( + )  x(l 1) r[x( 11 1) /  --  -- -- vv j  i:o WZ(klk - 1)  -- x k ]  E=o m[&(l - 1) - r][(l 1) - - r  p = 2  y(l 1) [(1 1) /  -- 2L  y = =o wY(I- 1)  2L  _ -- y ]  P - E=o w[Y(l- 1) ;][Y(I- 1)- - r  2L  = Yk ]  P E=o w[(l - 1) - ][y(l - 1) - - r  k = x + Pxy Pg (Y - )  P P Pxy -1 ]rpT  = - (Py y  autoregression:  x =  (x_, ...x_, w) +   y = x + n, Vk  {1...N}  (8)  The underlying clean signal xk is a nonlinear function of its past M values, driven by  2 The observed data point yk includes the  white Gaussian process noise v with variance a v.  2 The corresponding  additive noise n, which is assumed to be Gaussian with variance a s.  state-space representation for the signal z is given by:  Xk  Xk--1  Xk--M+i.]  = + (9)  /(xk_,w)  o o I  q-  B  Uk-- 1  y=[1 0 -.- 0].x+n (10)  In this context, the dual estimation problem consists of simultaneously estimating the clean  signal xk and the model parameters w from the noisy data y.  4.1 Dual EKF / Dual UF  One dual estimation approach is the dual extended Kalman filter developed in [8, 6]. The  dual EKF requires separate state-space representation for the signal and the weights. A  state-space representation for the weights is generated by considering them to be a station-  ary process with an identity state transition matrix, driven by process noise u:  w = w_ +uk (11)  y = f(x_, w) + v + nk. (12)  The noisy measurement y has been rewritten as an observation on w. This allows the use  of an EKF for weight estimation (representing a "second-order" optimization procedure)  [7]. Two EKFs can now be run simultaneously for signal and weight estimation. At every  time-step, the current estimate of the weights is used in the signal-filter, and the current  estimate of the signal-state is used in the weight-filter.  670 E.A. Wan, R. v. d. Merwe and A. T. Nelson  The dual UF/EKF algorithm is formed by simply replacing the EKF for state-estimation  with the UF while still using an EKF for weight-estimation. In the dual UF algorithm both  state- and weight-estimation are done with the UF. Note that the state-transition is linear in  the weight filter, so the nonlinearity is restricted to the measurement equation. Here, the  UF gives a more exact measurement-update phase of estimation. The use of the UF for  weight estimation in general is discussed in further detail in Section 5.  4.2 Joint EKF / Joint UF  An alternative approach to dual estimation is provided by the joint extended Kalmanfilter  [4, 5]. In this framework the signal-state and weight vector are concatenated into a single,  joint state vector: zk [xT T T  = Wk ] . The estimation of z can be done recursively by writing  the state-space equations for the joint state as:  [F(x_,w_)] rB.v] and yk = [1 0 .-. 0]z +n, (13)  z = [ I.w_ J + L uk j  and running an EKF on the joint state-space to produce simultaneous estimates of the states  x and w. As discussed in [6], the joint EKF provides approximate MAP estimates by  maximizing the joint density of the signal and weights given the noisy data. Again, our ap-  proach in this paper is to use the UF instead of the EKF to provide more accurate estimation  of the state, resulting in the joint UF algorithm.  4.3 EM - Unscented Smoothing  A somewhat different iterative approach to dual estimation is given by the expectation-  maximization (EM) algorithm applied to nonlinear dynamic systems [2]. In each iteration,  the conditional expectation of the signal is computed, given the data and the current es-  timate of the model (E-step). Then the model is found that maximizes a function of this  conditional mean (M-step). For linear models, the M-step can be solved in closed form.  The E-step is computed with a Kalman smoother, which combines the forward-time esti-  mated mean and covariance Gr f Pf of the signal given past data, with the backward-time  predicted mean and covariance (cht',Ph t') given the future data, producing the following  smoothed statistics given all the data:  (p)-i = (p)-i + (p.t,)-  ^ P[(P ) x  X s _b - ^_b f --^f  = - (Pk) xa].  (14)  (15)  When a MLP neural network model is used, the M-step can no longer be computed in  closed-form, and a gradient-based approach is used instead. The resulting algorithm is  usually referred to as generalized EM (GEM) 2. The E-step is typically approximated by  an extended Kalman smoother, wherein a linearization of the model is used for backward  propagation of the state estimates.  We propose improving the E-step of the EM algorithm for nonlinear models by using a  UF instead of an EKF to compute both the forward and backward passes in the Kalman  smoothen Rather than linearize the model for the backward pass, as in [2], a neural network  is trained on the backward dynamics (as well as the forward dynamics). This allows for  a more exact backward estimation phase using the UF, and enables the development of an  unscented smoother (US).  2An exact M-step is possible using RBF networks [2].  Dual Estimation and the Unscented Transformation 671  4.4 Experiments  We present results on two simple time-series to provide a clear illustration of the use of  the UF over the EKF. The first series is the Mackey-Glass chaotic series with additive  WGN (SNR m 3dB). The second time series (also chaotic) comes from an autoregressive  neural network with random weights driven by Gaussian process noise and also corrupted  by additive WGN (SNR  3dB). A standard 5-3-1 MLP with tanh hidden activation  functions and a linear output layer was used in all the filters. The process and measurement  noise variances were assumed to be known.  Results on training and testing data, as well as training curves for the different dual estima-  tion methods are shown below. The quoted numbers are normalized (clean signal variance)  mean-square estimation and prediction errors. The superior performance of the UT based  algorithms (especially the dual UF) are clear. Note also the more stable learning curves  using the UF approaches. These improvements have been found to be consistent and sta-  tistically significant on a number of additional experiments.  Mackey-Glass Train Test Chaotic AR-NN Train Test  Algorithm Est. Pred. Est. Pred.  Dual EKF 0.20 0.50 0.21 0.54  Dual UF/EKF 0.19 0.50 0.19 0.53  Dual UF 0.15 0.45 0.14 0.48  Joint EKF 0.22 0.53 0.22 0.56  Joint UF 0.19 0.50 0.18 0.53  Mackey-Glass  oo  + Dual EK F   Dual UF/EKF  0.8 O Dual UF  A .Joint UF  a Joint EKF  07  nos  1 2 3 4 5 6 7 8  1 11  iteration  Algorithm Est. Pred. Est. Pred.  'Dual EKF 0.32 0.62 0.36 0.69  Dual UF/EKF 0.26 0.58 0.28 0.69  Dual UF 0.23 0.55 0.27 0.63  Joint EKF 0.29 0.58 0.34 0.72  Joint UF 0.25 0.55 0.30 0.67  Chaotic AR-NN  0 55  + Dual EKF  t* Dual UF/EKF  0 $ O Dual U F  a Joint EKF  O3  iteration  The final table below compares smoother performance used for the E-step in the EM algo-  rithm. In this case, the network models are trained on the clean time-series, and then tested  on the noisy data using either the standard Kalman smoother with linearized backward  model (EKS 1), a Kalman smoother with a second nonlinear backward model (EKS2), and  the unscented smoother (US). The forward (F), backward (B), and smoothed (S) estimation  errors are reported. Again the performance benefits of the unscented approach is clear.  Mackey-Glass Norm. MSE  Algorithm F B S  EKS 1 0.20 0.70 0.27  EKS2 0.20 0.31 0.19  US 0.10 0.24 0.08  Chaotic AR-NN Norm. MSE  Algorithm F B S  EKS1 0.35 0.32 0.28  EKS2 0.35 0.22 0.23  US 0.23 0.21 0.16  5 UF Neural Network Training  As part of the dual UF algorithm, we introduced the use of the UF for weight estimation.  The approach can also be seen as a new method for the general problem of training neural  networks (i.e., for regression or classification problems where the input x is observed and  672 E. A. Wan, R. v. d. Merwe and A. T. Nelson  no state-estimation is required). The advantage of the UF over the EKF in this case is not  as obvious, as the state-transition function is linear (See Equation 11). However, as pointed  out earlier, the observation is nonlinear. Effectively, the EKF builds up an approximation  to the expected Hessian by taking outer products of the gradient. The UF, however, may  provide a more accurate estimate through direct approximation of the expectation of the  Hessian. We have performed a number of preliminary experiments on standard benchmark  data. The figure below shows the mean and std. of learning curves (computed over 100  experiments with different initial weights) for the Mackay Robot Arm Mapping dataset.  Note the faster convergence, lower variance, and lower final MSE performance of the UF  weight training. While these results are encouraging, further study is still necessary to fully  contrast differences between UF and EKF weight training.  0.06  0.05  0,03  0.02  0.01  ' Learning Curves'   UF (mean)  -- UF (std)  - - EKF (mean)  - - EKE Istd)  opoch  1'o ;o' 3o do  6 Conclusions  The EKF has been widely accepted as a standard tool in the machine learning community.  In this paper we have presented an alternative to the EKF using the unscented filter. The  UF consistently achieves a better level of accuracy than the EKF at a comparable level  of complexity. We demonstrated this performance gain on a number of dual estimation  methods as well as standard regression modeling.  Acknowledgements  This work was sponsored in part by the NSF under grant IRI-9712346.  References  [1] J. F. G. de Freitas, M. Niranjan, A. H. Gee, and A. Doucet. Sequential Monte Carlo methods  for optimisation of neural network models. Technical Report TR-328, Cambridge University  Engineering Department, Cambridge, England, November 1998.  [2] Z. Ghahramani and S. T. Roweis. Learning nonlinear dynamical systems using an EM algorithm.  In M. J. Kearns, S. A. Solla, and D. A. Cohn, editors, Advances in Neural Information Processing  Systems 11: Proceedings of the 1998 Conference. MIT Press, 1999.  [3] S.J. Julier and J. K. Uhlmann. A New Extension of the Kalman Filter to Nonlinear Systems. In  Proc. of AeroSense: The 11th International Symposium on Aerospace/Defence Sensing, Simula-  tion and Controls, Orlando, Florida., 1997.  [4] R. E. Kopp and R. J. Orford. Linear regression applied to system identification for adaptive  control systems. AIAA J., 1:2300-06, October 1963.  [5] M. B. Matthews and G. S. Moschytz. Neural-network nonlinear adaptive filtering using the  extended Kalman filter algorithm. In INNC, pages 115-8, 1990.  [6] A.T. Nelson. Nonlinear Estimation and Modeling of Noisy Time-Series by Dual Kalman Filter-  ing Methods. PhD thesis, Oregon Graduate Institute, 1999. In preparation.  [7] S. Singhal and L. Wu. Training multilayer perceptrons with the extended Kalman filter. In  Advances in Neural Information Processing Systems 1, pages 133-140, San Mateo, CA, 1989.  Morgan Kauffman.  [8] E.A. Wan and A. T. Nelson. Dual Kalman filtering methods for nonlinear prediction, estimation,  and smoothing. In Advances in Neural Information Processing Systems 9, 1997.  
A Neurodynamical Approach to Visual Attention  Gustavo Deco  Siemens AG  Corporate Technology  Neural Computation, ZT IK 4  Otto-Hahn-Ring 6  81739 Munich, Germany  Gustavo. Decomchp.siemens.de  Josef Zihl  Institute of Psychology  Neuropsychology  Ludwig-Maximilians-University Munich  Leopoldstr. 13  80802 Munich, Germany  Abstract  The psychophysical evidence for "selective attention" originates mainly  from visual search experiments. In this work, we formulate a hierarchi-  cal system of interconnected modules consisting in populations of neu-  rons for modeling the underlying mechanisms involved in selective  visual attention. We demonstrate that our neural system for visual  search works across the visual field in parallel but due to the different  intrinsic dynamics can show the two experimentally observed modes of  visual attention, namely: the serial and the parallel search mode. In  other words, neither explicit model of a focus of attention nor saliencies  maps are used. The focus of attention appears as an emergent property  of the dynamic behavior of the system. The neural population dynamics  are handled in the framework of the mean-field approximation. Conse-  quently, the whole process can be expressed as a system of coupled dif-  ferential equations.  1 Introduction  Traditional theories of human vision considers two functionally distinct stages of visual  processing [1]. The first stage, termed the preattentive stage, implies an unlimited-  capacity system capable of processing the information contained in the entire visual field  in parallel. The second stage is termed the attentive or focal stage, and is characterized by  the serial processing of visual information corresponding to local spatial regions. This  stage of processing is typically associated with a limited-capacity system which allocates  its resources to a single particular location in visual space. The designed psychophysical  experiments for testing this hypothesis consist of visual search tasks. In a visual search  test the subject have to look at the display containing a frame filled with randomly  positioned items in order to seek for an a priori defined target item. All other items in a  frame which are not the target are called distractors. The number of items in a frame is  called the frame size. The relevant variable to be measured is the reaction time as a  function of the frame size. In this context, the Feature Integration Theory, assumes that the  two stage processes operate sequentially [1]. The first early preattentive stage runs in  parallel over the complete visual field extracting single primitive features without  A Neurodynamical Approach to sual Attention II  integrating them. The second attentive stage has been likened to a spotlight. This metaphor  alludes that attention is focally allocated to a local region of the visual field where stimuli  are processed in more detail and passed to higher level of processing, while, in the other  regions not illuminated by the attentional spotlight, no further processing occurs.  Computational models formulated in the framework of feature integration theory require  the existence of a saliency or priority map for registering the potentially interesting areas  of the retinal input, and a gating mechanism for reducing the amount of incoming visual  information, so that limited computational resources in the system are not overloaded. The  priority map serves to represent topographically the relevance of different parts of the  visual field, in order to have a mechanism for guiding the attentional focus on salient  regions of the retinal input. The focused area will be gated, such that only the information  within will be passed further to yet higher levels, concerned with object recognition and  action. The disparity between these two stages of attentional visual processing originated  a vivid experimental disputation. Duncan and Humphreys [2] have postulated a hypothesis  that integrates both attentional modes (parallel and serial) as an instantiates of a common  principle. This principle sustains in both schemes that a selection is made. In the serial  focal scheme, the selection acts on in the space dimension, while in the parallel spread  scheme the selection concentrates in feature dimensions, e.g. color. On the other hand,  Duncan's attentional theory [3] proposed that after a first parallel search a competition is  initiated, which ends up by accepting only one object namely the target. Recently, several  electrophysiological experiments have been performed which seems to support this  hypothesis [4]. Chelazzi et al. [4] measured IT (inferotemporal) neurons in monkeys  observing a display containing a target object (that the monkey has seen previously) and a  distractor. They report a short period during which the neuron's response is enhanced.  After this period the activity level of the neuron remains high if the target is the neuron's  effective stimulus, and decay otherwise. The challenging question is therefore: is really  the linear increasing reaction time observed in some visual search tests due to a serial  mechanism? or is there only parallel processing followed by a dynamical time consuming  latency? In other words, are really priority maps and spotlight paradigm required? or can a  neurodynamical approach explain the observed psychophysical experiments?.  Furthermore, it should be clarified if the feature dimension search is achieved  independently in each feature dimension or is done after integrating the involved feature  dimensions. We study in this paper these questions from a computational perspective. We  formulate a neurodynamical model consisting in interconnected populations of biological  neurons specially designed for visual search tasks. We demonstrate that it is plausible to  build a neural system for visual search, which works across the visual field in parallel but  due to the different intrinsic dynamics can show the two experimentally observed modes  of visual attention, namely: the serial focal and the parallel spread over the space mode. In  other words, neither explicit serial focal search nor saliency maps should be assumed. The  focus of attention is not .included in the system but just results after convergence of the  dynamical behavior of the neural networks. The dynamics of the system can be interpreted  as an intrinsic dynamical routing for binding features if top-down information is available.  Our neurodynamical computational model requires independent competition mechanism  along each feature dimension for explaining the experimental data, implying the necessity  of the independent character of the search in separated and not integrated feature  dimensions. The neural population dynamics are handled in the framework of the mean-  field approximation yielding a system of coupled differential equations.  2 Neurodynamical model  We extend with the present model the approach of Usher and Niebur [5], which is based  on the experimental data of Chelazzi et al. [4], for explaining the results of visual search  experiments. The hierarchical architecture of our system is shown in Figure 1. The input  retina is given as a matrix of visual items. The location of each item at the retina is  12 G. Deco and J. Zihl  specified by two indices ij meaning the position at the row i and the column j. The  dimension of this matrix is SxS, i.e. the frame size is also SxS. The information is  processed at each spatial location in parallel. Different feature maps extract for the item at  each position the local values of the features. In the present work we hypothesize that  selective attention is guided by an independent mechanism which corresponds to the  independent search of each feature. Let us assume that each visual item can be defined by  K features. Each feature k can adopt L(k) values, for example the feature color can have  the values red or green (in this case L(color)--2). For each feature map k exist L(k)  layers of neurons for characterizing the presence of each feature value.  st r  Spiking --  Neuron  Figure 1: Hierarchical architecture of spiking neural modules for visual selective  attention. Solid arrows denote excitatory connections and dotted arrows denote  inhibitory connections  A cell assembly consisting in a population of full connected excitatory integrate-and-fire  spiking neurons (pyramidal cells) is allocated in each layer and for each item location for  encoding the presence of a specific feature value (e.g. color red) at the corresponding  position. This corresponds to a sparse distributed representation. The feature maps are  topographically ordered, i.e. the receptive fields of the neurons belonging to the cell  assembly ij at one of these maps are sensible to the location ij at the retinal input. We  further assume that the cell assemblies in layers corresponding to a feature dimension are  mutually inhibitory. Inhibition is modeled, according to the constraint imposed by Dale's  principle, by a different pool of inhibitory neurons. Each feature dimension has therefore  an independent pool of inhibitory neurons. This accounts for the neurophysiological fact  that the response of V4 neurons sensible to a specific feature value is enhanced and the  activity of the other neurons sensible to other feature values are suppressed. A high level  map consisting also in a topographically ordered excitatory cell assemblies is introduced  for integration of the different feature dimension at each item location, i.e. for binding the  features of each item. These cell assemblies are also mutually inhibited through a common  A Neurodynamical Approach to Visual Attention 13  pool of inhibitory neurons. This layer corresponds to the modeling of IT neurons, which  show location specific enhancement of activity by suppression of the responses of the cell  assemblies associated to other locations. This fact would yield a dynamical formation of a  focus of attention without explicitly assuming any spotlight. Top-down information  consisting in the feature values at each feature dimension of the target item is feed in the  system by including an extra excitatory input to the corresponding feature layers. The  whole system analyzes the information at all locations in parallel. Larger reaction times  correspond to slower dynamical convergence at all levels, i.e. feature map and integration  map levels.  Instead of solving the explicit set of integrate-and-fire neural equations, the Hebbian cell  assemblies adopted representation impels to adopt a dynamic theory whose dependent  variables are the activation levels of the cell populations. Assuming an ergodic behavior  [5] it is possible to derive the dynamic equations for the cell assembly activities level by  utilizing the mean-field approximation [5]. The essential idea consists in characterizing  each cell assembly by means of each activity x, and an input current that is characteristic  for all cells in the population, denoted by I, which satisfies:  1  x = F(I) = (1)  Tr-'rlg(1-)  which is the response function that transforms current into discharge rates for an integrate-  and-fire spiking neuron with deterministic input, time membrane constant ' and absolute  refractory time T r . The system of differential equations describing the dynamics of the  feature maps are:  X-?ijkt(t) = _ Iijkt(t ) + aF(Iijkt(t))  -- bF(Ik(t)) + I 0 + IFijkl + IAkl +   s s  'CvIVk(t) = --I'k(t)+ C ' ' ' F(Iijk(t))  i= l j= lk = I  -dF ( I'k( t ) )  where Iijkt(t ) is the input current for the population with receptive field at location/j of  the feature map k that analysis the value feature l, I?k(t) is the current in the inhibitory  pool bounded to the feature map layers of the feature dimension k. The frame size is S.  The additive Gaussian noise v considered has standard deviation 0.002. The synaptic time  constants were z -- 5 msec for the excitatory populations and %, -- 20 for the inhibitory  pools. The synaptic weights chosen were: a = 0.95, b -- 0.8, c -- 2. and d = 0.1.  i F  .  I 0 -- 0.025 is a diffuse spontaneous background input, ijkt s the sensory nput to the  cells in feature map k sensible to the value l and with receptive fields at the location ij at  the retina. This input characterizes the presence of the respective feature value at the  corresponding position. A value of 0.05 corresponds to the presence of the respective  feature value and a value of 0 to the absence of it. The top-down target information IAkl  was equal 0.005 for the layers which code the target properties and 0 otherwise.  The higher level integrating assemblies are described by following differential equation  system:  14 G. Deco and J. Zihl  = -Io(t)+ 'F(Iij(t))- 'F(It'H(t))  K L(k)  I o +    F(lijkl(t)) +   k=ll=l  S S   PH  ',HI (t) = -IPH(t)+ '   F(IHij(t))  i=lj=l  --F(IPH(t))  where IHij(t) is the input current for the population with receptive field at location ij of  the high level integrating map, IPH(t) is the associated current in the inhibitory pool. The  synaptic time constants were ZH = 5 msec for the excitatory populations and ZPH = 212  for the inhibitry pools. The ,,synaptic weights chosen were:  a = 0.95, b = 0.8, = 1,' = 1. and d = 0.1.  These systems of differential equations were integrated numerically until a convergence  criterion were reached. This criterion were that the neurons in the high level map are  polarized, i.e.    F(Inij(t))  F(iHi.,j.,(t)) i;ima,j;jma, > 0  (S2- 1)  where the index imaxJrnax denotes the cell assembly in the high level map with maximal  activity and the threshold 0 was chosen equal to 0.1. The second in the l.h.s measure the  mean distractor activity. At each feature dimension the fixed point of the dynamic is given  by the activity of cell assemblies at the layers with a common value with the target and  corresponding to items having this value. For example, if the target is red, at the color  map, the activity at the green layer will be suppressed and the cell assemblies  corresponding to red items will be enhanced. At the high-level map, the populations  corresponding to location which are maximally in all feature dimensions activated will be  enhanced by suppressing the others. In other words, the location that shows all feature  dimension equivalent at what top-down is stimulated and required, will be enhanced when  the target is at this location.  3 Simulations of visual search tasks  In this section we present results simulating the visual search experiments involving  feature and conjunction search [1]. Let us define the different kinds of search tasks by  given a pair of numbers m and n, where m is the number of feature dimensions by which  the distractors differ from the target and n is the number of feature dimensions by which  the distractor groups simultaneously differ from the target. In other words, the feature  search corresponds to a 1,1-search; a standard conjunction search corresponds to a 2,1-  search; a triple conjunction search can be a 3,1 or a 3,2-search if the target differs from all  distractor groups in one or in two features respectively. We assume that the items are  defined by three feature dimensions (K = 3, e.g. color, size and position), each one  A Neurodynarnical Approach to Visual Attention 15  having two values (L(k) -- 2 for k -- 1, 2, 3 ). At each size we repeat the experiment  100 times, each time with different randomly generated distractors and target.  T  100  095  0.90  085  080  0 .? 5  070  065  0 0  055  050  0 45  0.40  0.35  0.3 0  _ (a) 3,1- es ar' - i   2':: v - .... -_-: ...... -- 1,l-search  10.00 20 00 311 00 40 00 50.00  Frame size  Figure 2: Search times for feature and conjunction searches obtained utilizing the  presented model.  We plot as result the mean value T of the 100 simulated reaction times (in msec) as a  function of the frame size. In Figure 2, the results obtained for 1,1; 2,1; 3,1 and 3,2-  searches are shown. The slopes of the reaction time vs. frame size curves for all  simulations are absolutely consistent with the existing experimental results[1]. The  experimental work reports that in feature search (1,1) the target is detected in parallel  across the visual field. Furthermore, the slopes corresponding to. standard conjunction  search and triple conjunction search are a linear function of the frame size, where by the  slope of the triple conjunction search is steeper or very flat than in the case of standard  search (2,1) if the target differs from the distractors in one (3,1) or two features (3,2)  respectively. In order to analyze more carefully the dynamical evolution of the system, we  plot in Figure 3 the temporal evolution of the rate activity corresponding to the target and  to the distractors at the high-level integrating map and also separately for each feature  dimension level for a parallel (1,1-search) and a serial (3,1-search) visual tasks. The frame  size used is 25. It is interesting to note that in the case of 1,1-search the convergence time  in all levels are very small and therefore this kind of search appears as a parallel search. In  the case of 3,1-search the latency of the dynamic takes more time and therefore this kind  of search appears as a serial one, in spite that the underlying mechanisms are parallel. In  this case (see Figure 3-c) the large competition present in each feature dimension delays  the convergence of the dynamics at each feature dimension and therefore also at the high-  level map. Note in Figure 3-c the slow suppression of the distractor activity that reflects  the underlying competition.  16 G. Deco and J. Zihl  (High-Level-Map)  Rates  ....  arget-Activity (1, l-search)  450 00  4O0 00  .....  arget-Activity (3,1-search) .  .....  Distractors-Activity (l,l-searc  sooo S ,1-   oo oo .  (a)  Time  Rates  (b) 1,1-search  Target-Activity  -'. .O. istractors-Activit/  Time  Rates (c) 3,1-search  :i, Target-Activity  DtctorActivity '  ime  Figure 3: Acti,ity levels during visual search experiments. (a) High-level-map rates  for target F(I imaxjmax(t)) and mean dstractors-actmty. (b) Feature-level map rates  for target and one distractor activity for 1,1-search. There is one curve for each  feature dimension (i.e. 3 for target and 3 for distractor. (c) the same as (b) but for 3,1-  search.  References  [1] Treisman, A. (1988) Features and objects: The fourteenth Barlett memorial lecture.  The Quarterly Journal of Experimental Psychology, 40A, 201-237.  [2] Duncan, J. and Humphreys, G. (1989) Visual search and stimulus similarity. Psycho-  logical Review, 96, 433-458.  [3] Duncan, J. (1980) The locus of interference in the perception of simultaneous stimuli.  Psychological Review, 87, 272-300.  [4] Chelazzi, L., Miller, E., Duncan, J. and Desimone, R. (1993) A neural basis for visual  search in inferior temporal cortex. Nature (London), 363, 345-347.  [5] Usher, M. and Niebur, E. (1996) Modeling the temporal dynamics of IT neurons in  visual search: A mechanism for top-down selective attention. Journal of Cognitive Neuro-  science, 8, 311-327.  
Variational Inference for Bayesian  Mixtures of Factor Analysers  Zoubin Ghahramani and Matthew J. Beal  Gatsby Computational Neuroscience Unit  University College London  17 Queen Square, London WCIN 3AR, England  {zoubin,m. bealegat sby. ucl. ac. uk  Abstract  We present an algorithm that infers the model structure of a mix-  ture of factor analysers using an efficient and deterministic varia-  tional approximation to full Bayesian integration over model pa-  rameters. This procedure can automatically determine the opti-  mal number of components and the local dimensionality of each  component (i.e. the number of factors in each factor analyser).  Alternatively it can be used to infer posterior distributions over  number of components and dimensionalities. Since all parameters  are integrated out the method is not prone to overfitting. Using a  stochastic procedure for adding components it is possible to per-  form the variational optimisation incrementally and to avoid local  maxima. Results show that the method works very well in practice  and correctly infers the number and dimensionality of nontrivial  synthetic examples.  By importance sampling from the variational approximation we  show how to obtain unbiased estimates of the true evidence, the  exact predictive density, and the KL divergence between the varia-  tional posterior and the true posterior, not only in this model but  for variational approximations in general.  1 Introduction  Factor analysis (FA) is a method for modelling correlations in multidimensional  data. The model assumes that each p-dimensional data vector y was generated by  first linearly transforming a k < p dimensional vector of unobserved independent  zero-mean unit-variance Gaussian sources, x, and then adding a p-dimensional zero-  mean Gaussian noise vector, n, with diagonal covariance matrix : i.e. y - Ax + n.  Integrating out x and n, the marginal density of y is Gaussian with zero mean  and covariance AA r + . The matrix A is known as the factor loading matrix.  Given data with a sample covariance matrix E, factor analysis finds the A and   that optimally fit E in the maximum likelihood sense. Since k < p, a single factor  analyser can be seen as a reduced parametrisation of a full-covariance Gaussian.   Factor analysis and its relationship to principal components analysis (PCA) and mix-  ture models is reviewed in [10].  450 Z. Ghahramani and M. J. Beal  A mixture of factor analysers (MFA) models the density for y as a weighted average  of factor analyser densities  s  P(ylA, = P(sl)P(yls, A s, (1)  s----1  where r is the vector of mixing proportions, s is a discrete indicator variable, and  A s is the factor loading matrix for factor analyser s which includes a mean vector  for y.  By exploiting the factor analysis parameterisation of covariance matrices, a mix-  ture of factor analysers can be used to fit a mixture of Gaussians to correlated high  dimensional data without requiring O(p 2) parameters or undesirable compromises  such as axis-aligned covariance matrices. In an MFA each Gaussian cluster has in-  trinsic dimensionality k (or ks if the dimensions are allowed to vary across clusters).  Consequently, the mixture of factor analysers simultaneously addresses the prob-  lems of clustering and local dimensionality reduction. When  is a multiple of the  identity the model becomes a mixture of probabilistic PCAs. Tractable maximum  likelihood procedure for fitting MFA and MPCA models can be derived from the  Expectation Maximisation algorithm [4, 11].  The maximum likelihood (ML) approach to MFA can easily get caught in local  maxima. 2 Ueda et al. [12] provide an effective deterministic procedure for avoiding  local maxima by considering splitting a factor analyser in one part of space and  merging two in a another part. But splits and merges have to be considered simul-  taneously because the number of factor analysers has to stay the same since adding  a factor analyser is always expected to increase the training likelihood.  A fundamental problem with maximum likelihood approaches is that they fail to  take into account model complexity (i.e. the cost of coding the model parameter-  s). So more complex models are not penalised, which leads to overfitting and the  inability to determine the best model size and structure (or distributions thereof)  without resorting to costly cross-validation procedures. Bayesian approaches over-  come these problems by treating the parameters 0 as unknown random variables  and averaging over the ensemble of models they define:  P() = f ). (2)  P(Y) is the evidence for a data set Y = {y,... ,yN}. Integrating out parameters  penalises models with more degrees of freedom since these models can a priori  model a larger range of data sets. All information inferred from the data about the  parameters is captured by the posterior distribution P(OIY ) rather than the ML  point estimate  While Bayesian theory deals with the problems of overfitting and model selec-  tion/averaging, in practice it is often computationally and analytically intractable to  perform the required integrals. For Gaussian mixture models Markov chain Monte  Carlo (MCMC) methods have been developed to approximate these integrals by  sampling [8, 7]. The main criticism of MCMC methods is that they are slow and  2Technically, the log likelihood is not bounded above if no constraints are put on the  determinant of the component covariances. So the real ML objective for MFA is to find  the highest finite local maximum of the likelihood.  awe sometimes use 0 to refer to the parameters and sometimes to all the unknown  quantities (parameters and hidden variables). Formally the only difference between the two  is that the number of hidden variables grows with N, whereas the number of parameters  usually does not.  Variational Inference for Bayesian Mixtures of Factor Analysers 451  it is usually difficult to assess convergence. Furthermore, the posterior density over  parameters is stored as a set of samples, which can be inefficient.  Another approach to Bayesian integration for Gaussian mixtures [9] is the Laplace  approximation which makes a local Gaussian approximation around a maximum a  posteriori parameter estimate. These approximations are based on large data limits  and can be poor, particularly for small data sets (for which, in principle, the advan-  tages of Bayesian integration over ML are largest). Local Gaussian approximations  are also poorly suited to bounded or positive parameters such as the mixing pro-  portions of the mixture model. Finally, it is difficult to see how this approach can  be applied to online incremental changes to model structure.  In this paper we employ a third approach to Bayesian inference: variational ap-  proximation. We form a lower bound on the log evidence using Jensen's inequality:  f f P(Y,O)  /2 -- in P(Y) = In dO P(Y, O) _> dO Q(O) In Q(O) -  (3)  which we seek to maximise. Maximising 5 r is equivalent to minimising the KL-  divergence between Q(O) and P(OIY), so a tractable Q can be used as an approx-  imation to the intractable posterior. This approach draws its roots from one way  of deriving mean field approximations in physics, and has been used recently for  Bayesian inference [13, 5, 1].  The variational method has several advantages over MCMC and Laplace approxi-  mations. Unlike MCMC, convergence can be assessed easily by monitoring 5 r. The  approximate posterior is encoded efficiently in Q(O). Unlike Laplace approxima-  tions, the form of Q can be tailored to each parameter (in fact the optimal form  of Q for each parameter falls out of the optimisation), the approximation is global,  and Q optimises an objective function. Variational methods are generally fast, 5 r  is guaranteed to increase monotonically and transparently incorporates model com-  plexity. To our knowledge, no one has done a full Bayesian analysis of mixtures of  factor analysers.  Of course, vis-a-vis MCMC, the main disadvantage of variational approximations  is that they are not guaranteed to find the exact posterior in the limit. However,  with a straightforward application of sampling, it is possible to take the result of  the variational optimisation and use it to sample from the exact posterior and exact  predictive density. This is described in section 5.  In the remainder of this paper we first describe the mixture of factor analysers in  more detail (section 2). We then derive the variational approximation (section 3).  We show empirically that the model can infer both the number of components and  their intrinsic dimensionalities, and is not prone to overfitting (section 6). Finally,  we conclude in section 7.  2 The Model  Starting from (1), the evidence for the Bayesian MFA is obtained by averaging the  likelihood under priors for the parameters (which have their own hyperparameters):  P(Y)  (4)  452 Z. Ghahramani and M. . Beal  Here {a, a, b, ) are hyperparameters 4, v are precision parameters (i.e. inverse vari-  ances) for the columns of A. The conditional independence relations between the  variables in this model are shown  tation in Figure 1.  Figure 1: Generative model for  variational Bayesian mixture of  factor analysers. Circles denote  random variables, solid rectangles  denote hyperparameters, and the  dashed rectangle shows the plate  (i.e. repetitions) over the data.  graphically in the usual belief network represen-  While arbitrary choices could be made for the  priors on the first line of (4), choosing priors that  are conjugate to the likelihood terms on the sec-  ond line of (4) greatly simplifies inference and  interpretability. 5 So we choose P(rla ) to be  symmetric Dirichlet, which is conjugate to the  multinomial P(slr ).  The prior for the factor loading matrix plays a  key role in this model. Each component of the  mixture has a Gaussian prior P(A81vs), where  each element of the vector 8 is the precision of  a column of A. If one of these precisions  -> 0,  then the outgoing weights for factor xt will go to  zero, which allows the model to reduce the in-  trinsic dimensionality of x if the data does not  warrant this added dimension. This method of  intrinsic dimensionality reduction has been used  by Bishop [2] for Bayesian PCA, and is closely  related to MacKay and Neal's method for auto-  matic relevance determination (ARD) for inputs  to a neural network [6].  To avoid overfitting it is important to integrate out all parameters whose cardinality  scales with model complexity (i.e. number of components and their dimensionali-  ties). We therefore also integrate out the precisions using Gamma priors, P(tla , b).  3 The Variational Approximation  Applying Jensen's inequality repeatedly to the log evidence (4) we lower bound it  using the following factorisation of the distribution of parameters and hidden vari-  ables: Q (A) Q (r, ) Q (s, x). Given this factorisation several additional factorisations  fall out of the conditional independencies in the model resulting in the variational  objective function:  N S  q- E E Q(sn) [/ dr Q(r)In p(sn[r) / p(xn)  n:l s":l Q($n) q- dxnQ(xn[$ n) In Q(xnlsn)  +/dASQ(AS)/dxnQ(xnlsn)lnP(ynlxn, sn, As,)] (5)  The variational posteriors Q(.), as given in the Appendix, are derived by performing  a free-form extremisation of  w.r.t.Q. It is not difficult to show that these extrema  are indeed maxima of . The optimal posteriors Q are of the same conjugate forms  as the priors. The model hyperparameters which govern the priors can be estimated  in the same fashion (see the Appendix).  4We currently do not integrate out 9, although this can also be done.  5Conjugate priors have the same effect as pseudo-observations.  Variational Inference for Bayesian Mixtures of Factor Analysers 453  4 Birth and Death  When optimising jr, occasionally one finds that for some s: -n Q(s') = 0. These  zero responsibility components are the result of there being insufficient support from  the local data to overcome the dimensional complexity prior on the factor loading  matrices. So components of the mixture die of natural causes when they are no  longer needed. Removing these redundant components increases  Component birth does not happen spontaneously, so we introduce a heuristic.  Whenever .T has stabilised we pick a parent-component stochastically with prob-  ability proportional to e -y' and attempt to split it into two; s is the s-specific  contribution to jr with the last bracketed term in (5) normalised by ',Q(sn).  This works better than both cycling through components and picking them at ran-  dom as it concentrates attempted births on components that are faring poorly. The  parameter distributions of the two Gaussians created from the split are initialised  by partitioning the responsibilities for the data, Q(s'), along a direction sampled  from the parent's distribution. This usually causes  to decrease, so by monitoring  the future progress of  we can reject this attempted birth if  does not recover.  Although it is perfectly possible to start the model with many components and let  them die, it is computationally more efficient to start with one component and allow  it to spawn more when necessary.  5 Exact Predictive Density, True Evidence, and KL  By importance sampling from the variational approximation we can obtain unbiased  estimates of three important quantities: the exact predictive density, the true log  evidence , and the KL divergence between the variational posterior and the true  posterior. Letting 0 = (A, r), we sample Oi ~ Q(O). Each such sample is an instance  of a mixture of factor analysers with predictive density given by (1). We weight  these predictive densities by the importance weights wi = P(Oi, Y)/Q(Oi), which  are easy to evaluate. This results in a mixture of mixtures of factor analysers, and  will converge to the exact predictive density, P(ylY), as long as Q(O) > 0 wherever  P(O[Y)  O. The true log evidence can be similarly estimated by  = ln(w), where  (-) denotes averaging over the importance samples. Finally, the KL divergence is  given by: KL(Q(O)[[P(OIY)) = ln(w) - (ln w).  This procedure has three significant properties. First, the same importance weights  can be used to estimate all three quantities. Second, while importance sampling  can work very poorly in high dimensions for ad hoc proposal distributions, here the  variational optimisation is used in a principled manner to pick Q to be a good ap-  proximation to P and therefore hopefully a good proposal distribution. Third, this  procedure can be applied to any variational approximation. A detailed exposition  can be found in [3].  6 Results  Experiment 1: Discovering the number of components. We tested the  model on synthetic data generated from a mixture of 18 Gaussians with 50 points  per cluster (Figure 2, top left). The variational algorithm has little difficulty finding  the correct number of components and the birth heuristics are successful at avoiding  local maxima. After finding the 18 Gaussians repeated splits are attempted and  rejected. Finding a distribution over number of components using  is also simple.  Experiment 2: The shrinking spiral. We used the dataset of 800 data points  from a shrinking spiral from [12] as another test of how well the algorithm could  454 Z. Ghahramani and M. J. Beal  Figure 2: (top) Exp 1: The frames from left to right are the data, and the 2 S.D. Gaussian  ellipses after 7, 14, 16 and 22 accepted births. (bottom) Exp 2: Shrinking spiral data  and i S.D. Gaussian ellipses after 6, 9, 12, and 17 accepted births. Note that the number  of Gaussians increases from left to right.  -64O0  -660(  -680(  -700(;  -720(;  -740C  -760C  -760C  of points  per cluster  8  8  intrinsic dimensionalities  7 4 3 2 2  I ; II 1 I  I i II 2   I 4 I 2  1 6 3 3 2 2  1 7 4 3 2 2  1 7 4 3 2 2  Figure 3: (left) Exp 2: . as function of iteration for the spiral problem on a typical run.  Drops in . constitute component births. Thick lines are accepted attempts, thin lines are  rejected attempts. (middle) Exp 3: Means of the factor loading matrices. These results  are analogous to those given by Bishop [2] for Bayesian PCA. (right) Exp 3: Table with  learned number of Gaussians and dimensionalities as training set size increases. Boxes  represent model components that capture several of the clusters.  escape local maxima and how robust it was to initial conditions (Figure 2, bottom).  Again local maxima did not pose a problem and the algorithm always found between  12-14 Gaussians regardless of whether it was initialised with 0 or 200. These runs  took about 3-4 minutes on a 500MHz Alpha EV6 processor. A plot of J shows that  most of the compute time is spent on accepted moves (Figure 3, left).  Experiment 3: Discovering the local dimensionalities. We generated a syn-  thetic data set of 300 data points in each of 6 Gaussians with intrinsic dimension-  alities (7 4 3 2 2 1) embedded in 10 dimensions. The variational Bayesian approach  correctly inferred both the number of Gaussians and their intrinsic dimensionalities  (Figure 3, middle). We varied the number of data points and found that as expected  with fewer points the data could not provide evidence for as many components and  intrinsic dimensions (Figure 3, right).  7 Discussion  Search over model structures for MFAs is computationally intractable if each factor  analyser is allowed to have different intrinsic dimensionalities. In this paper we have  shown that the variational Bayesian approach can be used to efficiently infer this  model structure while avoiding overfitting and other deficiencies of ML approaches.  One attraction of our variational method, which can be exploited in other models,  is that once a factorisation of Q is assumed all inference is automatic and exact.  We can also use J to get a distribution over structures if desired. Finally we derive  Varational Inference for Bayesian Mixtures of Factor Analysers 455  a generally applicable importance sampler that gives us unbiased estimates of the  true evidence, the exact predictive density, and the KL divergence between the  variational posterior and the true posterior.  Encouraged by the results on synthetic data, we have applied the Bayesian mixture  of factor analysers to a real-world unsupervised digit classification problem. We  will report the results of these experiments in a separate article.  Appendix: Optimal Q Distributions and Hyperparameters  Q(xls) ~ Af(', E*) Q(Aq)~Af(X,E q'*) Q(yt)~6(a,bt) Q(,r) ~ z)(wu)  I A, '  lnQ(s)=[(wu,)-(w)]+lnl*[+(lnP(ylx,s , ))+c  '*=*X*-y, X= -xQ(s)y'** q'* a=a+ p b=b+ (At 2  n=l . q q=l  N N  E * = <A*Z-A*> +I, E q'* =q- Q(s)<xxZ> +diag<y*>, wu =   where {, 6, } denote Normal, Gamma d Dirichlet distributions respectively, {.> de-  notes expectation under the variational posterior, d (x) is the digaroma function  (x)   lnF(x). Note that the optimal distributions Q(A *) have block diagonal co-  variance structure; even though each A * is a p x q matrix, its coviance only h Oq 2)  pameters. Differentiating  with respect to the pmeters, a and b, of the precision pri-  or we get fixed point equations (a) = {ln y> +ln b and b = a/{y>. Similly the fixed point  for the pameters of the Dirichlet prior is (a) - (/S) + E [(wu,) - (w)]/S = O.  References  [1] H. Attias. Inferring parameters and structure of latent variable models by variational  Bayes. In Proc. 15th Conf. on Uncertainty in Artificial Intelligence, 1999.  [2] C.M. Bishop. Variational PCA. In Proc. Ninth Int. Conf. on Artificial Neural Net-  works. ICANN, 1999.  [3] Z. Ghahramani, H. Attias, and M.J. Beal. Learning model structure. Technical  Report GCNU-TR-1999-006, (in prep.) Gatsby Unit, Univ. College London, 1999.  [4] Z. Ghahramani and G.E. Hinton. The EM algorithm for mixtures of fac-  tor analyzers. Technical Report CRG-TR-96-1 [http://www.gatsby.ucl.ac.uk/  zoubin/papers/tr-96-1. ps. gz], Dept. of Comp. Sci., Univ. of Toronto, 1996.  [5] D.J.C. MacKay. Ensemble learning for hidden Markov models. Technical report,  Cavendish Laboratory, University of Cambridge, 1997.  [6] R.M. Neal. Assessing relevance determination methods using DELVE. In C.M. Bish-  op, editor, Neural Networks and Machine Learning, 97-129. Springer-Verlag, 1998.  [7] C.E. Rasmussen. The infinite gaussian mixture model. In Adv. Neur. Inf. Proc. $ys.  12. MIT Press, 2000.  [8] S. Richardson and P.J. Green. On Bayesian analysis of mixtures with an unknown  number of components. J. Roy. Star. $oc.-$er. B, 59(4):731-758, 1997.  [9] S.J. Roberts, D. Husmeier, I. Rezek, and W. Penny. Bayesian approaches to Gaussian  mixture modeling. IEEE PAMI, 20(11):1133-1142, 1998.  [10] S. T. Roweis and Z. Ghahramani. A unifying review of linear Gaussian models. Neural  Computation, 11(2):305-345, 1999.  [11] M.E. Tipping and C.M. Bishop. Mixtures of probabilistic principal component ana-  lyzers. Neural Computation, 11(2):443-482, 1999.  [12] N. Ueda, R. Nakano, Z. Ghahramani, and G.E. Hinton. SMEM algorithm for mixture  models. In Adv. Neut. Inf. Proc. $ys. 11. MIT Press, 1999.  [13] S. Waterhouse, D.J.C. Mackay, and T. Robinson. Bayesian methods for mixtures of  experts. In Adv. Neur. Inf. Proc. $ys. 7. MIT Press, 1995.  
Maximum entropy discrimination  Tommi Jaakkola  MIT AI Lab  545 Technology Sq.  Cambridge, MA 02139  tommiai. mir. edu  Marina Meila  MIT AI Lab  545 Technology Sq.  Cambridge, MA 02139  mmp ai. mir. edu  Tony Jebara  MIT Media Lab  20 Ames St.  Cambridge, MA 02139  jebaramedia. mir. edu  Abstract  We present a general framework for discriminative estimation based  on the maximum entropy principle and its extensions. All calcula-  tions involve distributions over structures and/or parameters rather  than specific settings and reduce to relative entropy projections.  This holds even when the data is not separable within the chosen  parametric class, in the context of anomaly detection rather than  classification, or when the labels in the training set are uncertain or  incomplete. Support vector machines are naturally subsumed un-  der this class and we provide several extensions. We are also able  to estimate exactly and efficiently discriminative distributions over  tree structures of class-conditional models within this flamework.  Preliminary experimental results are indicative of the potential in  these techniques.  I Introduction  Effective discrimination is essential in many application areas. Employing gener-  ative probability models such as mixture models in this context is attractive but  the criterion (e.g., maximum likelihood) used for parameter/structure estimation  is suboptimal. Support vector machines (SVMs) are, for example, more robust  techniques as they are specifically designed for discrimination [9].  Our approach towards general discriminative training is based on the well known  maximum entropy principle (e.g., [3]). This enables an appropriate training of both  ordinary and structural parameters of the model (cf. [5, 7]). The approach is not  limited to probability models and extends, e.g., SVMs.  2 Maximum entropy classification  Consider a two-class classification problem  where labels y  {-1, 1} are assigned  The extension to a multi-class is straightforward[4]. The formulation also admits an  easy extension to regression problems, analogously to SVMs.  Maximum Entropy Discrimination 471  to examples X  X. Given two generative probability distributions P(XlOv) with  parameters Or, one for each class, the corresponding decision rule follows the sign  of the discriminant function:  v(xlo)  c(xlo) = log + b  (1)  where 0 = {O,O_,b} and b is a bias term, usually expressed as a log-ratio b =  log p/(1 -p). The class-conditional distributions may come from different families  of distributions or the parametric discriminant function could be specified directly  without any reference to models. The parameters 0v may also include the model  structure (see later sections).  The parameters 0 = {0, 0_, b} should be chosen to maximize classification accu-  racy. We consider here the more general problem of finding a distribution P(O)  over parameters and using a convex combination of discriminant functions, i.e.,  f P(O)(XlO)dO in the decision rule. The search for the optimal P(O) can be for-  malized as a maximum entropy (ME) estimation problem. Given a set of training  examples {X,... ,XT} and corresponding labels {y,... ,yT} we find a distribu-  tion P(O) that maximizes the entropy H(P) subject to the classification constraints  f P(O) [Yt (XtlO)] dO _> 7 for all t. Here 7 > 0 specifies a desired classification  margin. The solution is unique (if it exists) since H(P) is concave and the linear  constraints specify a convex region. Note that the preference towards high entropy  distributions (fewer assumptions) applies only within the admissible set of distribu-  tions 7v consistent with the constraints. See [2] for related work.  We will extend this basic idea in a number of ways. The ME formulation assumes,  for example, that the training examples can be separated with the specified mar-  gin. We may also have a reason to prefer some parameter values over others and  would therefore like to incorporate a prior distribution P0(O). Other extensions  and generalizations will be discussed later in the paper.  A more complete formulation is based on the following minimum relative entropy  principle:  Definition 1 Let {Xt, Yt} be the training examples and labels, (X]O) a parametric  discriminant function, and 7 = [7,...,fit] a set of margin variables. Assuming a  prior distribution P0(O,7), we find the discriminative minimum relative entropy  (MRE) distribution P(O, 7) by minimizing D(PIIPo ) subject to  f r(o, 7) [Yt (XtlO) - ?t] dOd7 _ 0  (2)  for all t. Here  = sign ( f P(O) (XlO ) dO ) specifies the decision rule for any  new example X.  The margin constraints and the preference towards large margin solutions are encod-  ed in the prior P0 ("7). Allowing negative margin values with non-zero probabilities  also guarantees that the admissible set 7 > consisting of distributions P(, 7) con-  sistent with the constraints, is never empty. Even when the examples cannot be  separated by any discriminant function in the parametric class (e.g., linear), we  get a valid solution. The miss-classification penalties follow from P0(7) as well.  472 T. Jaakkola, M. Meila and T. debara  a) , c)  .----""'"  j Po   D(P,P 0 )  /  o 05  Figure 1: a) Minimum relative entropy (MRE) projection from the prior distribution  to the admissible set. b) The margin prior Po(%). c) The potential terms in the  MRE formulation (solid line) and in SVMs (dashed line). c = 5 in this case.  Suppose P0(O, 7) = P0(O)P0(7) and Po(7) = 1-It Po(7t), where  Po(7t) = ce -c0-v) for 7t _< 1,  (3)  This is shown in Figure lb. The penalty for margins smaller than 1 - 1/c (the prior  mean of 7t) is given by the relative entropy distance between P(7) and P0(7). This  is similar but not identical to the use of slack variables in support vector machines.  Other choices of the prior are discussed in [4].  The MRE solution can be viewed as a relative entropy projection from the prior  distribution P0(O,7) to the admissible set P. Figure la illustrates this view. From  the point of view of regularization theory, the prior probability P0 specifies the  entropic regularization used in this approach.  Theorem 1 The solution to the MRE problem has the following general form [1]  __1 P0(O, 7) e -], [ y,c(XlO)-v (4)  P(o, 7) =  where is the noalization constant (paition function) and  = (,..., T)  defines a set of non-negative Lagrange multipliers, one for each classification con-  straint.  are set by finding the unique maximum of the following jointly concave  objective function: J() = -logZ()  The solution is sparse, i.e., only a few Lagrange multipliers will be non-zero. This  arises because many of the classification constraints become irrelevant once the  constraints are enforced for a small subset of examples. Sparsity leads to immediate  but weak generalization guarantees expressed in terms of the number of non-zero  Lagrange multipliers [4]. Practical leave-one-out cross-validation estimates can be  also derived.  2.1 Practical realization of the MRE solution  We now turn to finding the MRE solution. To begin with, we note that any disjoint  factorization of the prior P0(O,7), where the corresponding parameters appear in  distinct additive components in ytl(Xt, O) -7t, leads to a disjoint factorization of  the MRE solution P(O, 7). For example, {O \ b, b, 7} provides such a factorization.  As a result of this factorization, the bias term could be eliminated by imposing  additional constraints on the Lagrange multipliers [4]. This is analogous to the  handling of the bias term in support vector machines [9].  We consider now a few specific realizations such as support vector machines and a  class of graphical models.  Maximum Entropy Discrimination 473  2.1.1 Support vector machines  It is well known that the log-likelihood ratio of two Gaussian distributions with equal  covariance matrices yields a linear decision rule. With a few additional assumptions,  the MRE formulation gives support vector machines:  Theorem 2 Assuming (X, O) = oTx -- b and P0(O, 7) = Po(O)Po(b)Po(7) where  Po(O) is N(O,I), Po(b) approaches a non-informative prior, and Po(7) is given by  eq. (3) then the Lagrange multipliers .k are obtained by maximizing J(.k) subject to  0 <_ At <_ c and Y';.t ,ktYt ---- O, where  I (xtTxt,)  J(A) = E[ St + log(1 - At/c)] -  E AtAt, ytyt,  t t,t'  (5)  The only difference between our J(A) and the (dual) optimization problem for  SVMs is the additional potential term log(1 -At/c). This highlights the effect  of the different miss-classification penalties, which in our case come from the MRE  projection. Figure lb shows, however, that the additional potential term does not  always carry a huge effect (for c = 5). Moreover, in the separable case, letting  c -+ c, the two methods coincide. The decision rules are formally identical.  We now consider the case where the discriminant function (X, )) corresponds to  the log-likelihood ratio of two Gaussians with different (and adjustable) covariance  matrices. The parameters ) in this case are both the means and the covariances.  The prior P0()) must be the conjugate Normal-Wishart to obtain closed form  integrals 2 for the partition function, Z. Here, P()i,  a density over means and covariances.  The prior distribution has the form P0 ()x) = Af(mx; m0, V/k) ZW(V; kVo, k) with  parameters (k, m0, V0) that can be specified manually or one may let k -+ 0 to get  a non-informative prior. Integrating over the parameters and the margin, we get  Z = Z v x Z1 x Z-l, where  a N? d/2 ISxl d F( (N, + 1- j)/2)  1-I j= 1  (6)  N1 _A Zt Wt, .'1 __A__ Zt 't' xt, Sl -- Zt wtXtX? - NiX1X1 T. Here, wt is a scalar  weight given by wt = u(yt)+Ytt. For Z_, the weights are set to wt = u(-yt)-Yt,kt;  u(.) is the step function. Given Z, updating  is done by maximizing J(). The  resulting marginal MRE distribution over the parameters (normalized by Z x Z_i)  is a Normal-Wishart distribution itself, P(O) = Af(ml; '1, V1 IN1 ) Z]//(V1; S1, N1 )  with the final A values. Predicting the label for a new example X involves taking  expectations of the discriminant function under a Normal-Wishart. This is  N1  ElD(el) [log P(X101)] = constant - -(X - )Ts{- (X - )  (7)  We thus obtain discriminative quadratic decision boundaries. These extend the  linear boundaries without (explicitly) resorting to kernels. More generally, the  covariance estimation in this framework adaptively modifies the kernel.  2This can be done more generally for conjugate priors in the exponential family.  474 T. Jaakkola, M. Meila and T. Jebara  2.1.2 Graphical models  We consider here graphical models with no hidden variables. The ME (or MRE)  distribution is in this case a distribution over both structures and parameters. Find-  ing the distribution over parameters can be done in closed form for conjugate priors  when the observations are complete. The distribution over structures is, in general,  intractable. A notable exception is a tree model that we discuss in the forthcoming.  A tree graphical model is a graphical model for which the structure is a tree. This  model has the property that its log-likelihood can be expressed as a sum of local  terms [8]  log(X,1o) = + Wv(X,O) (8)  u uvE  The discriminant function consisting of the log-likelihood ratio of a pair of tree  models (depending on the edge sets E, E-l, and parameters 0, 0_) can be also  expressed in this form.  We consider here the ME distribution over tree structures for fixed parameters 3.  The treatment of the general case (i.e. including the parameters) is a direct exten-  sion of this result. The ME distribution over the edge sets E1 and E-1 factorizes  with components  C uv [ t'O4'l)'"Eu hu(Xt,04-1)] H wl (9)  P(Ei) = Z+ - Z+uveZ.  where Z+, h +, W + are functions of the same Lagrange multipliers A. To com-  pletely define the distribution we need to find A that optimize J(A) in Theorem 1;  for classification we also need to compute averages with respect to P(E+i). For  these, it suffices to obtain an expression of the partition function(s) Zx.  P is a discrete distribution over all possible tree structures for n variables (there  are n '-2 trees). However, a remarkable graph theory result, called the Matrix Tree  Theorem [10], enables us to perform all necessary summations in closed form in  polynomial time. On the basis of this result, we find  Theorem3  The normalization constant Z of a distribution of the form (9) is  Z = h.E H Wuv = h. IQ(W)[, where (10)  E uv6E  -Wuv uv (11)  Q,v(W) =  2v,=Wv,v u=v  This shows that summing over the distribution of all trees, when this distribution  factors according to the trees' edges, can be done in closed form by computing the  value of a determinant in time O(n3). Since we obtain a closed form expression,  optimization of the Lagrange multipliers and evaluating the resulting classification  rule are also tractable.  Figure 2a provides a comparison of the discriminative tree approach and a maximum  likelihood tree estimation method on a DNA splice junction problem.  aEach tree relies on a different set of n - 1 pairwise node marginals. In our experiments  the class-conditional pairwise marginals were obtained directly from data.  Maximum Entropy Discrimination 475  o  0.8  Figure 2: ROC curves based on independent test sets. a) Tree estimation: discrim-  inative (solid) and ML (dashed) trees. b) Anomaly detection: MRE (solid) and  Bayes (dashed). c) Partially labeled case: 100% labeled (solid), 10% labeled + 90%  unlabeled (dashed), and 10% labeled + 0% unlabeled training examples (dotted).  3 Extensions  Anomaly detection: In anomaly detection we are given a set of training ex-  amples representing only one class, the "typical" examples. We attempt to cap-  ture regularities among the examples to be able to recognize unlikely members  of this class. Estimating a probability distribution P(X]O) on the basis of the  training set {X,..., Xr} via the ML (or analogous) criterion is not appropriate;  there is no reason to further increase the probability of those examples that are al-  ready well captured by the model. A more relevant measure involves the level sets  X v = {X E : logP(X[O) _> 7} which are used in deciding the class membership  in any case. We estimate the parameters 0 to optimize an appropriate level set.  Definition 2 Given a probability model P(XlO), 0 O, a set of training examples  {X,..., XT}, a set of margin variables 7 = [7,..., 7iT], and a prior distribution  Po(O, 7) we find the MRE distribution P(O, 7) such that minimizes D(PlIPo) subject  to the constraints f P(0, 7) [ log P(XtlO) - 7t ] dOd7 _> 0 for all t= 1,... ,T.  Note that this again a MRE projection whose solution can be obtained as before.  The choice of P0 (7) in P0 (0, 7) = Po (O)Po (7) is not as straightforward as before since  each margin 7t needs to be close to achievable log-probabilities. We can nevertheless  find a reasonable choice by relating the prior mean of 7t to some c-percentile of the  training set log-probabilities generated through ML or other estimation criterion.  Denote the resulting value by l and define the prior Po(7t) as Po(%) = c e  for 7t <_ l. In this case the prior mean of 7t is l, - 1/c.  Figure 2b shows in the context of a simple product distribution that this choice of  prior together with the MRE flamework leads to a real improvement over standard  (Bayesian) approach. We believe, however, that the effect will be more striking  for sophisticated models such as HMMs that may otherwise easily capture spurious  regularities in the data. An extension of this formalism to latent variable models is  provided in [4].  Uncertain or incompletely labeled examples: Examples with uncertain la-  bels are hard to deal with in any (probabilistic or not) discriminative classification  method. Uncertain labels can be, however, handled within the maximum entropy  formalism: let y = {y,...,yT} be a set of binary variables corresponding to the  labels for the training examples. We can define a prior uncertainty over the labels  by specifying P0(y); for simplicity, we can take this to be a product distribution  476 T. Jaakkola, M. Meila and T. Jebara  Po(Y) = I-It Pt,o(yt) where a different level of uncertainty can be assigned to each  example. Consequently, we find the minimum relative entropy projection from the  prior distribution P0(O, 7, Y) = Po(O)Po(7)Po(y) to the admissible set of distribu-  tions (no longer a function of the labels) that are consistent with the constraints:  Evf,, P(O,%y)[ytE(Xt,O)-7t] dOd7 _ 0 for all t = 1,...,T. The MRE  principle differs from transduction [9], provides a soft rather than hard assignment  of unlabeled examples, and is fundamentally driven by large margin classification.  The MRE solution is not, however, often feasible to obtain in practice. We can  nevertheless formulate an efficient mean field approach in this context [4]. Figure  2c demonstrates that even the approximate method is able to reap most of the ben-  efit from unlabeled examples (compare, e.g., [6]). The results are for a DNA splice  junction classification problem. For more details see [4].  4 Discussion  We have presented a general approach to discriminative training of model param-  eters, structures, or parametric discriminant functions. The formalism is based on  the minimum relative entropy principle reducing all calculations to relative entropy  projections. The idea naturally extends beyond standard classification and cov-  ers anomaly detection, classification with partially labeled examples, and feature  selection.  References  [1] Cover and Thomas (1991). Elements of information theory. John Wiley & Sons.  [2] Kivinen J. and Warmuth M. (1999). Boosting as Entropy Projection. Proceed-  ings of the 12th Annual Conference on Computational Learning Theory.  [3] Levin and Tribus (eds.) (1978). The maximum entropy formalism. Proceedings  of the Maximum entropy formalism conference, MIT.  [4] Jaakkola T., Meil& M. and Jebara T. (1999). Maximum entropy discrimination.  MIT AITR-1668, http://www. ai. mir. edu/~tommi/papers. html.  [5] Jaakkola T. and Haussler D. (1998). Exploiting generafive models in discrimi-  native classifiers. NIPS 11.  [6] Joachims, T. (1999). Transductive inference for text classification using support  vector machines. International conference on Machine Learning.  [7] Jebara T. and Pentland A. (1998). Maximum conditional likelihood via bound  maximization and the CEM algorithm. NIPS 11.  [8] Meil& M. and Jordan M. (1998). Estimating dependency structure as a hidden  variable. NIPS 11.  [9] Vapnik V. (1998). Statistical learning theory. John Wiley & Sons.  [10] West D. (1996). Introduction to graph theory. Prentice Hall.  
Generalized Model Selection For Unsupervised  Learning In High Dimensions  Shivakumar Vaithyanathan  IBM Almaden Research Center  650 Harry Road  San Jose, CA 95136  Shiv @ almaden.ibm. com  Byron Dom  IBM Almaden Research Center  650 Harry Road  San Jose, CA 95136  dom @ almaden.ibm.com  Abstract  We describe a Bayesian approach to model selection in unsupervised  learning that determines both the feature set and the number of  clusters. We then evaluate this scheme (based on marginal likelihood)  and one based on cross-validated likelihood. For the Bayesian  scheme we derive a closed-form solution of the marginal likelihood  by assuming appropriate forms of the likelihood function and prior.  Extensive experiments compare these approaches and all results are  verified by comparison against ground truth. In these experiments the  Bayesian scheme using our objective function gave better results than  cross-validation.  1 Introduction  Recent efforts define the model selection problem as one of estimating the number of  clusters[10, 17]. It is easy to see, particularly in applications with large number of  features, that various choices of feature subsets will reveal different structures  underlying the data. It is our contention that this interplay between the feature subset  and the number of clusters is essential to provide appropriate views of the data. We thus  define the problem of model selection in clustering as selecting both the number of  clusters and the feature subset. Towards this end we propose a unified objective  function whose arguments include the both the feature space and number of clusters.  We then describe two approaches to model selection using this objective function. The  first approach is based on a Bayesian scheme using the marginal likelihood for model  selection. The second approach is based on a scheme using cross-validated likelihood.  In section 3 we apply these approaches to document clustering by making assumptions  about the document generation model. Further, for the Bayesian approach we derive a  closed-form solution for the marginal likelihood using this document generation model.  We also describe a heuristic for initial feature selection based on the distributional  clustering of terms. Section 5 describes the experiments and our approach to validate  the proposed models and algorithms. Section 6 reports and discusses the results of our  experiments and finally section 7 provides directions for future work.  Model Selection for Unsupervised Learning in High Dimensions 971  2 Model selection in clustering  Model selection approaches in clustering have primarily concentrated on determining  the number of components/clusters. These attempts include Bayesian approaches  [7,10], MDL approaches [15] and cross-validation techniques [17]. As noticed in [17]  however, the optimal number of clusters is dependent on the feature space in which the  clustering is performed. Related work has been described in [7].  2.1 A generalized model for clustering  Let D be a data-set consisting of "patterns" {d,..,dv}, which we assume to be  represented in some feature space T with dimension M. The particular problem we  address is that of clustering D into groups such that its likelihood described by a  probability model p(Drlf), is maximized, where D r indicates the representation of D  in feature space T and f is the structure of the model, which consists of the number of  clusters, the partitioning of the feature set (explained below) and the assignment of  patterns to clusters. This model is a weighted sum of models {p(Drlf,)l  m}  where  is the set of all parameters associated with f. To define our model we begin  by assuming that the feature space T consists of two sets: U - useful features and N -  noise features. Our feature-selection problem will thus consist of partitioning T (into U  and N ) for a given number of clusters.  Assumption 1 The feature sets represented by U and N are conditionally  independent  P(DrI.Q,) = P(D v I.Q,) P(D t I_Q,) (1)  where D N indicates data represented in the noise feature space and D u indicates data  represented in useful feature space.  Using assumption 1 and assuming that the data is independently drawn, we can rewrite  equation (1) as  P(Drlf,)= lSI p(d I N)  II II p(d: I ) (2)  i=1 /c=-I jDt  where vis the number of patterns in D, p(dy I c) is the probability of d/ given  the parameter vector  and p(d I is the probability of d given the parameter  vector v . Note that while the explicit dependence on f has been removed in this  notation, it is implicit in the number of clusters K and the partition of T into N and U.  2.2 Bayesian approach to model selection  The objective function, represented in equation (2) is not regularized and attempts to  optimize it directly may result in the set N becoming empty - resulting in overfitting.  To overcome this problem we use the marginal likelihood[2].  K  Assumption 2 All parameter vectors are independent.  () =  (is)  I--I  ()  k=-I  where the (...) denotes a Bayesian prior distribution. The marginal likelihood, using  assumption 2, can be written as  [i:Il ] K u [iID 1 ]  p(orlf)  P(dfl v) n(V)dN I-I p(dyl)  s  =  ( )d (3)  972 S. Vaithyanathan and B. Dom  where =v =t are integral limits appropriate to the particular parameter spaces. These  will be omitted to simplify the notation.  3.0 Document clustering  Document clustering algorithms typically start by representing the document as a  "bag-of-words" in which the features can number ~ 104to 105. Ad-hoc dimensionality  reduction techniques such as stop-word removal, frequency based truncations [ 16] and  techniques such as LSI [5] are available. Once the dimensionality has been reduced, the  documents are usually clustered into an arbitrary number of clusters.  3.1 Multinomial models  Several models of text generation have been studied[3]. Our choice is multinomial  models using term counts as the features. This choice introduces another parameter  indicating the probability of the N and U split. This is equivalent to assuming a  generation model where for each document the number of noise and useful terms are  determined by a probability O s and then the terms in a document are "drawn" with a  probability (0 n or 0: ).  3.2 Marginal likelihood / stochastic complexity  To apply our Bayesian objective function we begin by substituting multinomial models  into (3) and simplifying to obtain  P(D IFS)=(tv+tv) y[(Os)tN (1- Os)tU]n(OS)dO s .  t v J  iz>k {ti,uluU}]I[u(O)t""] (O)dO' (4)  where ((.'.'.'}) is the multinomial coefficient, t i,u is the number of occuences of the  feature term u in document i, tis the total number of all useful features (terms) in  document i (t =  ti,u, t, and ti,n are to be interpreted similar to above but for  noise features, ()   n _ (n-), tis the total number of all noise features in all patterns  and tSis the total number of all useful features in all patterns.  To solve (4) we still need a form for the priors {(...)}. The Beta family is conjugate  to the Binomial family [2] and we choose the Dirichlet distribution (multiple Beta) as  the form for both z(0) and z(0v) and the Beta distribution for n(Os). Substituting  these into equation (8) and simplifying yields  P(DI)= F(y+y) F(t+y,)F(t+y)] [ F) Fn+t)]  F(y,)F(y) F(tV+t+y,+y) ' F+t )   r(fi)  F(a) F(aa+lD&l) .  F(a+tS(a)) H F(a)  F(a + v)  F(IDel)  uu  (5)  Model Selection for Unsupervised Learning in High Dimensions 973  where ,8, and au are the hyper-parameters of the Dirichlet prior for noise and useful  features respectively, ,8 =  ,sn, a =  au, a =  akand FO is the "gamma" function.  nN u U /c=- 1  Further, ?,,, ?t, are the hyper parameters of the Beta prior for the split probability, ID,I is  the number of documents in cluster k and tt(, is computed as  t. The results  iDk  reported for our evaluation will be the negative of the log of equation (5), which  (following Rissanen [14]) we refer to as Stochastic Complexity (SC). In our  experiments all values of the hyper-parameters ji ,6[i O'k, Ta and 7t, are set equal to 1  yielding uniform priors.  3.3 Cross-Validated likelihood  To compute the cross validated likelihood using multinomial models we first substitute  the multinomial functional forms, using the MLE found using the training set. This  results in the following equation  Ite.t -. K  P(CV r 112p ) [(')t. (1-')tgl , p(cv? IO N) 1-I 1-I p(cv? I   = ' ' Ok(i) )  p(c) (6)  '= 1 jD t  where 0 s, 0  and Oi) e the MLE of the appropriate parameter vectors. For our  implementation of MCCV, following the suggestion in [17], we have used a 50% split  of the training and test set. For the vCV criterion although a value of v = 10 was  suggested therein, for computational reasons we have used a value of v = 5.  3.4 Feature subset selection algorithm for document clustering  As noted in section 2.1, for a feature-set of size M there are a total of 2 u partitions and  for large M it would be computationally intractable to search through all possible  partitions to find the optimal subset. In this section we propose a heuristic method to  obtain a subset of tokens that are topical (indicative of underlying topics) and can be  used as features in the bag-of-words model to cluster documents.  3.4.1 Distributional clustering for feature subset selection  Identifying content-bearing and topical terms, is an active research area [9]. We are less  concerned with modeling the exact distributions of individual terms as we are with  simply identifying groups of terms that are topical. Distributional clustering (DC),  apparently first proposed by Pereira et al [13], has been used for feature selection in  supervised text classification [1] and clustering images in video sequences [9]. We  hypothesize that function, content-bearing and topical terms have different distributions  over the documents. DC helps reduce the size of the search space for feature selection  from 2 u to 2 c, where C is the number of clusters produced by the DC algorithm.  Following the suggestions in [9], we compute the following histogram for each token.  The first bin consists of the number of documents with zero occurrences of the token,  the second bin is the number of documents consisting of a single occurrence of the  token and the third bin is the number of documents that contain more two or more  occurrences of the term. The histograms are clustered using relative entropy A(. II .) as  974 S. Vaithyanathan and B. Dom  a distance measure. For two terms with probability distributions p(.) and p2(.), this is  given by [4]:  P'(') (7)  A(p(t) II p2(t)) ---  pl(t)log p2(t)  t  We use a k-means-style algorithm in which the histograms are normalized to sum to  one and the sum in equation (7) is taken over the three bins corresponding to counts of  0,1, and >_ 2. During the assignment-to-clusters step of k-means we compute  A(pw II pc) (where pw is the normalized histogram for term w and pc(t) is the centroid  of cluster k) and the term w is assigned to the cluster for which this is minimum [ 13,8].  4.0 Experimental setup  Our evaluation experiments compared the clustering results against human-labeled  ground truth. The corpus used was the AP Reuters Newswire articles from the TREC-6  collection. A total of 8235 documents, from the routing track, existing in 25 classes  were analyzed in our experiments. To simplify matters we disregarded multiple  assignments and retained each document as a member of a single class.  4.1 Mutual information as an evaluation measure of clustering  We verify our models by comparing our clustering results against pre-classified text.  We force all clustering algorithms to produce exactly as many clusters as there are  classes in the pre-classified text and we report the mutual information[4] (MI) between  the cluster labels and pre-classified class labels  5.0 Results and discussions  After tokenizing the documents and discarding terms that appeared in less than 3  documents we were left with 32450 unique terms. We experimented with several  numbers of clusters for DC but report only the best (lowest SC) for lack of space. For  each of these clusters we chose the best of 20 runs corresponding to different random  starting clusters. Each of these sets includes one cluster that consists of high-frequency  words and upon examination were found to contain primarily function words, which we  eliminated from further consideration. The remaining non-function-word clusters were  used as feature sets for the clustering algorithm. Only combinations of feature sets that  produced good results were used for further document clustering runs.  We initialized the EM algorithm using k-means algorithm - other initialization schemes  are discussed in [11]. The feature vectors used in this k-means initialization were  generated using the pivoted normal weighting suggested in [16]. All parameter vectors  0 and O N were estimated using Laplace's Rule of Succession[2]. Table 1 shows the  best results of the SC criterion, the vCV and MCCV using the feature subsets selected  by the different combinations of distributional clusters. The feature subsets are coded as  FSXP where X indicates the number of clusters in the distributional clustering and P  indicates the cluster number(s) used as U. For SC and MI all results reported are  averages over 3 runs of the k-means+EM combination with different initialization fo  k-means. For clarity, the MI numbers reported are normalized such that the theoretical  maximum is 1.0. We also show comparisons against no feature selection (NF) and LSI.  Model Selection for Unsupervised Learning in High Dimensions 975  For LSI, the principal 165 eigenvectors were retained and k-means clustering was  performed in the reduced dimensional space. While determining the number of clusters,  for computational reasons we have limited our evaluation to only the feature subset that  provided us with the highest MI, i.e., FS41-3.  Feature Useful SC vCV MCCV MI  Set Features X 107 X 107 X 107  FS41-3 6,157 2.66 0.61 1.32 0.61  FS52 386 2.8 0.3 0.69 0.51  NF 32,450 2.96 1.25 2.8 0.58  LSI 32450/165 NA NA NA 0.57  Table 1 Comparison Of Results  o.s  Figuro 2  5.3 Discussion  The consistency between the MI and SC (Figure 1) is striking. The monotonic trend is  more apparent at higher SC indicating that bad clusterings are more easily detected by  SC while as the solution improves the differences are more subtle. Note that the best  value of SC and MI coincide. Given the assumptions made in deriving equation (5), this  consistency and is encouraging. The interested reader is referred to [18] for more  details. Figures 2 and 3 indicate that there is certainly a reasonable consistency  between the cross-validated likelihood and the MI although not as striking as the SC.  Note that the MI for the feature sets picked by MCCV and vCV is significantly lower  than that of the best feature-set. Figures 4,5 and 6 show the plots of SC, MCCV and  vCV as the number of clusters is increased. Using SC we see that FS41-3 reveals an  optimal structure around 40 clusters. As with feature selection, both MCCV and vCV  obtain models of lower complexity than SC. Both show an optimum of about 30  clusters. More experiments are required before we draw final conclusions, however, the  full Bayesian approach seems a practical and useful approach for model selection in  document clustering. Our choice of likelihood function and priors provide a  closed-form solution that is computationally tractable and provides meaningful results.  6.0 Conclusions  In this paper we tackled the problem of model structure determination in clustering.  The main contribution of the paper is a Bayesian objective function that treats optimal  model selection as choosing both the number of clusters and the feature subset. An  important aspect of our work is a formal notion that forms a basis for doing feature  selection in unsupervised learning. We then evaluated two approaches for model  selection: one using this objective function and the other based on cross-validation.  976 S. Vaithyanathan and B. Dom  Both approaches performed reasonably well - with the Bayesian scheme outperforming  the cross-validation approaches in feature selection. More experiments using different  parameter settings for the cross-validation schemes and different priors for the Bayesian  scheme should result in better understanding and therefore more powerful applications  of these approaches.  Figure 2  Figure 4  Igure S  Figure 6  References  [1] Baker, D., et al, Distributional Clustering of Words for Text Classification, SIGIR 1998.  [2] Bernardo, J. M. and Smith, A. F. M., Bayesian Theory, Wiley, 1994.  [3] Church, K.W. et al, Poisson Mixtures, Natural Language Engineering, I(12), 1995.  [4] Cover, T.M. and Thomas, J.A. Elements of Information Theory. Wiley-lnterscience, 1991.  [5] Deerwester, S. et al, Indexing by Latent Semantic Analysis,JASIS, 1990.  [6] Dempster, A.et al., Maximum Likelihood from Incomplete Data Via the EM Algorithm,  JRSS, 39,1977.  [7] Hanson,R., et al, Bayesian Classification with Correlation and Inheritance, IJCAI,1991.  [8] Iyengar, G., Clustering images using relative entropy for efficient retrieval, VLBV, 1998.  [9] Katz, S.M., Distribution of content words and phrases in text and language modeling, NLE,  2, 1996.  [10] Kontkanen, P.T. et al, Comparing Bayesian Model Class Selection Criteria by Discrete  Finite Mixtures, ISIS'96 Conference, 1996.  [11] Meila, M., Heckerman, D., An Experimental Comparison of Several Clustering and  Initialization Methods, MSR-TR-98-06.  [12] Nigam, K et al, Learning to Classify Text from Labeled and Unlabeled Documents, AAAI,  1998.  [13] Pereira, F.C.N. et al, Distributional clustering of English words, ACL,1993.  [14] Rissanen, J., Stochastic Complexity in Statistical Inquiry. World\ Scientific, 1989.  [15] Rissanen, J., Ristad E., Unsupervised classification with stochastic complexity." The  US/Japan Conference on the Frontiers of Statistical Modeling, 1992.  [16] Singhal A. et al, Pivoted Document Length Normalization, SIGIR, 1996.  [ 17] Smyth, P., Clustering using Monte Carlo cross-validation, KDD, 1996.  [18] Vaithyanathan, S. and Dom, B. Model Selection in Unsupervised Learning with  Applications to Document Clustering. IBM Research Report RJ- 10137 (95012) Dec. 14, 1998.  
Lower Bounds on the Complexity of  Approximating Continuous Functions by  Sigmoidal Neural Networks  Michael Schmitt  Lehrstuhl Mathematik und Informatik  Fakultt flit Mathematik  Ruhr-Universitt Bochum  D-44780 Bochum, Germany  mschmitt Imi. ruhr-uni-bochum. de  Abstract  We calculate lower bounds on the size of sigmoidal neural networks  that approximate continuous functions. In particular, we show  that for the approximation of polynomials the network size has  to grow as ((log k) U4) where k is the degree of the polynomials.  This bound is valid for any input dimension, i.e. independently of  the number of variables. The result is obtained by introducing a  new method employing upper bounds on the Vapnik-Chervonenkis  dimension for proving lower bounds on the size of networks that  approximate continuous functions.  1 Introduction  Sigmoidal neural networks are known to be universal approximators. This is one of  the theoretical results most frequently cited to justify the use of sigmoidal neural  networks in applications. By this statement one refers to the fact that sigmoidal  neural networks have been shown to be able to approximate any continuous function  arbitrarily well. Numerous results in the literature have established variants of  this universal approximation property by considering distinct function classes to be  approximated by network architectures using different types of neural activation  functions with respect to various approximation criteria, see for instance [1, 2, 3, 5,  6, 11, 12, 14, 15]. (See in particular Scarselli and Tsoi [15] for a recent survey and  further references.)  All these results and many others not referenced here, some of them being construc-  tive, some being merely existence proofs, provide upper bounds for the network size  asserting that good approximation is possible if there are sufficiently many net-  work nodes available. This, however, is only a partial answer to the question that  mainly arises in practical applications: "Given some function, how many network  nodes are needed to approximate it?" Not much attention has been focused on  establishing lower bounds on the network size and, in particular, for the approx-  imation of functions over the reals. As far as the computation of binary-valued  Complexity of Approximating Continuous Functions by Neural Networks 329  functions by sigmoidal networks is concerned (where the output value of a network  is thresholded to yield 0 or 1) there are a few results in this direction. For a spe-  cific Boolean function Koiran [9] showed that networks using the standard sigmoid  a(y) = 1/(1 + e -y) as activation function must have size fl(n /4) where n is the  number of inputs. (When measuring network size we do not count the input nodes  here and in what follows.) Maass [13] established a larger lower bound by construct-  ing a binary-valued function over l n and showing that standard sigmoidal networks  require fl(n) many network nodes for computing this function. The first work on  the complexity of sigmoidal networks for approximating continuous functions is due  to DasGupta and Schnitger [4]. They showed that the standard sigmoid in network  nodes can be replaced by other types of activation functions without increasing the  size of the network by more than a polynomial. This yields indirect lower bounds  for the size of sigmoidal networks in terms of other network types. DasGupta and  Schnitger [4] also claimed the size bound A (/') for sigmoidal networks with d  layers approximating the function sin(Ax).  In this paper we consider the problem of using the standard sigmoid a(y) =  1/(1 + e -y) in neural networks for the approximation of polynomials. We show  that at least fl((log k) x/4) network nodes are required to approximate polynomials  of degree k with small error in the l norm. This bound is valid for arbitrary input  dimension, i.e., it does not depend on the number of variables. (Lower bounds can  also be obtained from the results on binary-valued functions mentioned above by  interpolating the corresponding functions by polynomials. This, however, requires  growing input dimension and does not yield a lower bound in terms of the degree.)  Further, the bound established here holds for networks of any number of layers. As  far as we know this is the first lower bound result for the approximation of polyno-  mials. From the computational point of view this is a very simple class of functions;  they can be computed using the basic operations addition and multiplication only.  Polynomials also play an important role in approximation theory since they are  dense in the class of continuous functions and some approximation results for neu-  ral networks rely on the approximability of polynomials by sigmoidal networks (see,  e.g., [2, 15]).  We obtain the result by introducing a new method that employs upper bounds on  the Vapnik-Chervonenkis dimension of neural networks to establish lower bounds  on the network size. The first use of the Vapnik-Chervonenkis dimension to obtain  a lower bound is due to Koiran [9] who calculated the above-mentioned bound  on the size of sigmoidal networks for a Boolean function. Koiran's method was  further developed and extended by Maass [13] using a similar argument but another  combinatorial dimension. Both papers derived lower bounds for the computation  of binary-valued functions (Koiran [9] for inputs from {0, 1} n, Maass [13] for inputs  from l n). Here, we present a new technique to show that and how lower bounds can  be obtained for networks that approximate continuous functions. It rests on two  fundamental results about the Vapnik-Chervonenkis dimension of neural networks.  On the one hand, we use constructions provided by Koiran and Sontag [10] to build  networks that have large Vapnik-Chervonenkis dimension and consist of gates that  compute certain arithmetic functions. On the other hand, we follow the lines of  reasoning of Karpinski and Macintyre [7] to derive an upper bound for the Vapnik-  Chervonenkis dimension of these networks from the estimates of Khovanskil [8] and  a result due to Warren [16].  In the following section we give the definitions of sigmoidal networks and the Vapnik-  Chervonenkis dimension. Then we present the lower bound result for function  approximation. Finally, we conclude with some discussion and open questions.  330 M. Schmitt  2 Sigmoidal Neural Networks and VC Dimension  We briefly recall the definitions of a sigmoidal neural network and the Vapnik-  Chervonenkis dimension (see, e.g., [7, 10]). We consider feedforward neural networks  which have a certain number of input nodes and one output node. The nodes  which are not input nodes are called computation nodes and associated with each  of them is a real number t, the threshold. Further, each edge is labelled with a  real number w called weight. Computation in the network takes place as follows:  The input values are assigned to the input nodes. Each computation node applies  the standard sigmoid a(y) = 1/(1 + e -y) to the sum wxxx + ... + WrXr -- t where  xx,..., xr are the values computed by the node's predecessors, w,..., wr are the  weights of the corresponding edges, and t is the threshold. The output value of the  network is defined to be the value computed by the output node. As it is common  for approximation results by means of neural networks, we assume that the output  node is a linear gate, i.e., it just outputs the sum wxxx + ... + wrxr - t. (Clearly,  for computing functions on finite sets with output range [0, 1] the output node  may apply the standard sigmoid as well.) Since a is the only sigmoidal function  that we consider here we will refer to such networks as sigmoidal neural networks.  (Sigmoidal functions in general need to satisfy much weaker assumptions than a  does.) The definition naturally generalizes to networks employing other types of  gates that we will make use of (e.g. linear, multiplication, and division gates).  The Vapnik-Chervonenkis dimension is a combinatorial dimension of a function class  and is defined as follows: A dichotomy of a set S C_ I n is a partition of S into two  disjoint subsets (So, Sx) such that So U Sx = S. Given a set . of functions mapping  ll n to {0, 1} and a dichotomy (So, Sx) of $, we say that . induces the dichotomy  (S0, Sx) on S if there is some f  . such that f(So) C_ {0} and f(Sx) C_ {1}.  We say further that  shatters S if  induces all dichotomies on S. The Vapnik-  Chervonenkis (VC) dimension of , denoted VCdim(), is defined as the largest  number m such that there is a set of m elements that is shattered by . We refer  to the VC dimension of a neural network, which is given in terms of a "feedforward  architecture", i.e. a directed acyclic graph, as the VC dimension of the class of  functions obtained by assigning real numbers to all its programmable parameters,  which are in general the weights and thresholds of the network or a subset thereof.  Further, we assume that the output value of the network is thresholded at 1/2 to  obtain binary values.  3 Lower Bounds on Network Size  Before we present the lower bound on the size of sigmoidal networks required for  the approximation of polynomials we first give a brief outline of the proof idea.  We will define a sequence of univariate polynomials (p,,x by means of which  we show how to construct neural architectures Af,consistg of various types of  gates such as linear, multiplication, and division gates, and, in particular, gates  that compute some of the polynomials. Further, this architecture has a single  weight as programmable parameter (all other weights and thresholds are fixed).  We then demonstrate that, assuming the gates computing the polynomials can be  approximated by sigmoidal neural networks sufficiently well, the architecture  can shatter a certain set by assigning suitable values to its programmable weight.  The final step is to reason along the lines of Karpinski and Macintyre [7] to obtain  via Khovanski's estimates [8] and Warren's result [16] an upper bound on the VC  dimension of Afn in terms of the number of its computation nodes. (Note that we  cannot directly apply Theorem 7 of [7] since it does not deal with division gates.)  Comparing this bound with the cardinality of the shattered set we will then be able  Complexity of Approximating Continuous Functions by Neural Networks 331  i  w  w? )  Figure 1: The network A/'n with values k,j,i, 1 assigned to the input nodes  xx, x2, xa, x4 respectively. The weight w is the only programmable parameter of  the network.  to conclude with a lower bound on the number of computation nodes in A/'n and  thus in the networks that approximate the polynomials.  Let the sequence (p,),>_x of polynomials over I be inductively defined by  4x(1 - x)  p(x) = p(p_x(x))  n--l  n>2.  Clearly, this uniquely defines p for every n _> 1 and it can readily be seen that  p has degree 2 n. The main lower bound result is made precise in the following  statement.  Theorem 1 Sigmoidal neural networks that approximate the polynomials (p)_> x  on the interval [0, 1] with error at most O(2 -) in the lo norm must have at least  (n x/4) computation nodes.  Proof. For each n a neural architecture Af can be constructed as follows: The  network has four input nodes xx, x2, xa, x4. Figure I shows the network with input  values assigned to the input nodes in the order x4 = 1, xa = i, x2 = j, xx = k.  There is one weight which we consider as the (only) programmable parameter of  Afn. It is associated with the edge outgoing from input node x4 and is denoted  by w. The computation nodes are partitioned into six levels as indicated by the  boxes in Figure 1. Each level is itself a network. Let us first assume, for the sake of  simplicity, that all computations over real numbers are exact. There are three levels  labeled with l'I, having n + 1 input nodes and one output node each, that compute  so-called projections r: 1 + -> 1 where r(y,..., y, a) = Ya for a  {1,..., n}.  The levels labeled Ps, P2, Px have one input node and n output nodes each. Level  Ps receives the constant I as input and thus the value w which is the parameter of  the network. We define the output values of level P for A - 3, 2, 1 by  w? ) =p,.x-(v) , b-1,...,n  where v denotes the input value to level PA. This value is equal to w for A = 3 and  +x)   ....  , xx+) otherwise. We observe that (x)  , , '"b+ can be calculated from  332 M. Schmitt  w? ') as p,- (w?)). Therefore, the computations of level P can be implemented  using n gates each of them computing the function Pn--  We show now that Afn can shatter a set of cardinality r 3. Let S = {1,..., r} 3. It  has been shown in Lemma 2 of [10] that for each (/x,...,/%)  {0, 1} r the}e exists  some w  [0, 1] such that for q = 1,..., r  pq(w) 6 [0,1/2) if&=0, andpq(w)  (1/2,1] if&=l.  This implies that, for each dichotomy (So, $x) of $ there is some w  [0, 1] such  that for every (i, j, k)  S  pk(Pj.n(Pi.n2(W))) < 1/2 if (i,j,k)  So,  Pk(Pj.n(Pi.n2(W))) > 1/2 if (i,j,k)  Sx .  Note that P(Pj.n(Pi.na (w))) is the value computed by A/'n given input values k, j, i, 1.  Therefore, choosing a suitable value for w, which is the parameter of Afn, the network  can induce any dichotomy on S. In other words, S is shattered by  It has been shown in Lemma 1 of [10] that there is an architecture .A such that  for each e > 0 weights can be chosen for An such that the function fn, computed  by this network satisfies lim_0 fn, (yx,..., Yn, a) = ya. Moreover, this architecture  consists of O(n) computation nodes, which are linear, multiplication, and division  gates. (Note that the size of An does not depend on e.) Therefore, choosing  sufficiently small, we can implement the projections r in A/'n by networks of O(n)  computation nodes such that the resulting network A/'' still shatters $. Now in A/''  we have O(n) computation nodes for implementing the three levels labeled II and  we have in each level P a number of O(n) computation nodes for computing  respectively. Assume now that the computation nodes for Pn- can be replaced  by sigmoidal networks such that on inputs from S and with the parameter values  defined above the resulting network A/'" computes the same functions as A/''. (Note  that the computation nodes for Pn- have no programmable parameters.)  We estimate the size of A/'". According to Theorem 7 of Karpinski and Macintyre  [7] a sigmoidal neural network with I programmable parameters and m computation  nodes has VC dimension O((ml)a). We have to generalize this result slightly before  being able to apply it. It can readily be seen from the proof of Theorem 7 in [7] that  the result also holds if the network additionally contains linear and multiplication  gates. For division gates we can derive the same bound taking into account that for  a gate computing division, say :r/y, we can introduce a defining equality x = z  y  where z is a new variable. (See [7] for how to proceed.) Thus, we have that a  network with I programmable parameters and m computation nodes, which are  linear, multiplication, division, and sigmoidal gates, has VC dimension O((ml)a).  In particular, if m is the number of computation nodes of A/'", the VC dimension  is O(ma). On the other hand, as we have shown above, A/'" can shatter a set  of cardinality r 3. Since there are O(n) sigmoidal networks in A/'" computing the  functions Pn-, and since the number of linear, multiplication, and division gates  is bounded by O(n), for some value of A a single network computing Pn- must  have size at least (v). This yields a lower bound of (r x/4) for the size of a  sigmoidal network computing  Thus far, we have assumed that the polynomials Pn are computed exactly. Sincb  polynomials are continuous functions and since we require them to be calculated  only on a finite set of input values (those resulting from S and from the parameter  values chosen for w to shatter S) an approximation of these polynomials is sufficient.  A straightforward analysis, based on the fact that the output value of the network  has a "tolerance" close to 1/2, shows that if pn is approximated with error O(2 -n)  Complexity of Approximating Continuous Functions by Neural Networks 333  in the Im norm, the resulting network still shatters the set $. This completes the  proof of the theorem. []  The statement of the previous theorem is restricted to the approximation of poly-  nomials on the input domain [0, 1]. However, the result immediately generalizes to  any arbitrary interval in 11. Moreover, it remains valid for multivariate polynomials  of arbitrary input dimension.  Corollary 2 The approximation of polynomials of degree k by sigmoidal neural  networks with approximation error O(1/k) in the loo norm requires networks of size  f((log k)/4). This holds for polynomials over any number of variables.  4 Conclusions and Open Questions  We have established lower bounds on the size of sigmoidal networks for the approx-  imation of continuous functions. In particular, for a concrete class of polynomials  we have calculated a lower bound in terms of the degree of the polynomials. The  main result already holds for the approximation of univariate polynomials. Intu-  itively, approximation of multivariate polynomials seems to become harder when  the dimension increases. Therefore, it would be interesting to have lower bounds  both in terms of the degree and the input dimension.  Further, in our result the approximation error and the degree are coupled. Naturally,  one would expect that the number of nodes has to grow for each fixed function when  the error decreases. At present we do not know of any such lower bound.  We have not aimed at calculating the constants in the bounds. For practical appli-  cations such values are indispensable. Refining our method and using tighter results  it should be straightforward to obtain such numbers. Further, we expect that better  lower bounds can be obtained by considering networks of restricted depth.  To establish the result we have introduced a new method for deriving lower bounds  on network sizes. One of the main arguments is to use the functions to be approxi-  mated to construct networks with large VC dimension. The method seems suitable  to obtain bounds also for the approximation of other types of functions as long as  they are computationally powerful enough.  Moreover, the method could be adapted to obtain lower bounds also for networks  using other activation functions (e.g. more general sigmoidal functions, ridge func-  tions, radial basis functions). This may lead to new separation results for the  approximation capabilities of different types of neural networks. In order for this  to be accomplished, however, an essential requirement is that small upper bounds  can be calculated for the VC dimension of such networks.  Acknowledgments  I thank Hans U. Simon for helpful discussions. This work was supported in part  by the ESPRIT Working Group in Neural and Computational Learning II, Neuro-  COLT2, No. 27150.  References  [1] A. Barron. Universal approximation bounds for superposition of a sigmoidal  function. IEEE Transactions on Information Theory, 39:930-945, 1993.  334 M. Schmitt  [16]  [2] C. K. Chui and X. Li. Approximation by ridge functions and neural networks  with one hidden layer. Journal of Approximation Theory, 70:131-141, 1992.  [3] G. Cybenko. Approximation by superpositions of a sigmoidal function. Math-  ematics of Control, Signals, and Systems, 2:303-314, 1989.  [4] B. DasGupta and G. Schnitger. The power of approximating: A comparison  of activation functions. In C. L. Giles, S. J. Hanson, and J. D. Cowan, editors,  Advances in Neural Information Processing Systems 5, pages 615-622, Morgan  Kaufmann, San Mateo, CA, 1993.  [5] K. Hornik. Approximation capabilities of multilayer feedforward networks.  Neural Networks, 4:251-257, 1991.  [6] K. Hornik, M. Stinchcombe, and H. White. Multilayer feedforward networks  are universal approximators. Neural Networks, 2:359-366, 1989.  [7] M. Karpinski and A. Macintyre. Polynomial bounds for VC dimension of  sigmoidal and general Pfattian neural networks. Journal of Computer and  System Sciences, 54:169-176, 1997.  [8] A. G. Khovanski. Fewnomials, volume 88 of Translations of Mathematical  Monographs. American Mathematical Society, Providence, RI, 1991.  [9] P. Koiran. VC dimension in circuit complexity. In Proceedings of the 11th  Annual IEEE Conference on Computational Complexity CCC'96, pages 81-85,  IEEE Computer Society Press, Los Alamitos, CA, 1996.  [10] P. Koiran and E. D. Sontag. Neural networks with quadratic VC dimension.  Journal of Computer and System Sciences, 54:190-198, 1997.  [11] V. Y. Kreinovich. Arbitrary nonlinearity is sufficient to represent all functions  by neural networks: A theorem. Neural Networks, 4:381-383, 1991.  [12] M. Leshno, V. Y. Lin, A. Pinkus, and $. Schocken. Multilayer feedforward net-  works with a nonpolynomial activation function can approximate any function.  Neural Networks, 6:861-867, 1993.  [13] W. Maass. Noisy spiking neurons with temporal coding have more compu-  tational power than sigmoidal neurons. In M. Mozer, M. I. Jordan, and  T. Petsche, editors, Advances in Neural Information Processing Systems 9,  pages 211-217. MIT Press, Cambridge, MA, 1997.  [14] H. Mhaskar. Neural networks for optimal approximation of smooth and analytic  functions. Neural Computation, 8:164-177, 1996.  [15] F. Scarselli and A. C. Tsoi. Universal approximation using feedforward neural  networks: A survey of some existing methods and some new results. Neural  Networks, 11:15-37, 1998.  H. E. Warren. Lower bounds for approximation by nonlinear manifolds. Trans-  actions of the American Mathematical Society, 133:167-178, 1968.  
Noisy Neural Networks and  Generalizations  Hava T. Siegelmon  Industrial Eng. and Management, Mathematics  Technion- IIT  Haifa 32000, Israel  iehava @ie. technion. ac. il  Alexander Roitershtein  Mathematics  Technion- IIT  Haifa 32000, Israel  roiterst @math. technion. ac. il  Asa Ben-Hur  Industrial Eng. and Management  Technion- IIT  Haifa 32000, Israel  asa @tx. tech nion. a c. il  Abstract  In this paper we define a probabilistic computational model which  generalizes many noisy neural network models, including the recent  work of Maass and Sontag [5]. We identify weak ergodic.ity as the  mechanism responsible for restriction of the computational power  of probabilistic models to definite languages, independent of the  characteristics of the noise: whether it is discrete or analog, or if  it depends on the input or not, and independent of whether the  variables are discrete or continuous. We give examples of weakly  ergodic models including noisy computational systems with noise  depending on the current state and inputs, aggregate models, and  computational systems which update in continuous time.  I Introduction  Noisy neural networks were recently examined, e.g. in. [1, 4, 5]. It was shown in [5]  that Gaussian-like noise reduces the power of analog recurrent neural networks to  the class of definite languages, which are a strict subset of regular languages. Let  E be an arbitrary alphabet. L C E* is called a definite language if for some integer  r any two words coinciding on the last r symbols are either both in L or neither in  L. The ability of a computational system to recognize only definite languages can  be interpreted as saying that the system forgets all its input signals, except for the  most recent ones. This property is reminiscent of human short term memory.  "Definite probabilistic computational models" have their roots in Rabin's pioneer-  ing work on probabilistic automata [9]. He identified a condition on probabilistic  automata with a finite state space which restricts them to definite languages. Paz  [8] generalized Rabin's condition, applying it to automata with a countable state  space, and calling it weak ergodicity [7, 8]. In their ground-breaking paper [5],  336 H. T. Siegelmann, A. Roitershtein and A. Ben-Hur  Maass and Sontag extended the principle leading to definite languages to a finite  interconnection of continuous-valued neurons. They proved that in the presence  of "analog noise" (e.g. Gaussian), recurrent neural networks are limited in their  computational power to definite languages. Under a different noise model, Maass  and Orponen [4] and Casey [1] showed that such neural networks are reduced in  their power to regular languages.  In this paper we generalize the condition of weak ergodicity, making it applica-  ble to numerous probabilistic computational machines. In our general probabilistic  model, the state space can be arbitrary: it is not constrained to be a finite or  infinite set, to be a discrete or non-discrete subset of some Euclidean space, or  even to be a metric or topological space. The input alphabet is arbitrary as well  (e.g., bits, rationals, reals, etc.). The stochasticity is not necessarily defined via a  transition probability function (TPF) as in all the aforementioned probabilistic and  noisy models, but through the more general Markov operators acting on measures.  Our Markov Computational Systems (MCS's) include as special cases Rabin's ac-  tual probabilistic automata with cut-point [9], the quasi-definite automata by Paz  [8], and the noisy analog neural network by Maass and Sontag [5]. Interestingly,  our model also includes: analog dynamical systems and neural models, which have  no underlying deterministic rule but rather update probabilistically by using finite  memory; neural networks with an unbounded number of components; networks of  variable dimension (e.g., "recruiting networks"); hybrid systems that combine dis-  crete and continuous variables; stochastic cellular automata; and stochastic coupled  map lattices.  We prove that all weakly ergodic Markov systems are stable, i.e. are robust with  respect to architectural imprecisions and environmental noise. This property is de-  sirable for both biological and artificial neural networks. This robustness was known  up to now only for the classical discrete probabilistic automata [8, 9]. To enable  practicality and ease in deciding weak ergodicity for given systems, we provide two  conditions on the transition probability functions under which the associated com-  putational system becomes weakly ergodic. One condition is based on a version  of Doeblin's condition [5] while the second is motivated by the theory of scram-  bling matrices [7, 8]. In addition we construct various examples of weakly ergodic  systems which include synchronous or asynchronous computational systems, and  hybrid continuous and discrete time systems.  2 Markov Computational System (MCS)  Instead of describing various types of noisy neural network models or stochastic  dynamical systems we define a general abstract probabilistic model. When dealing  with systems containing inherent elements of uncertainty (e.g., noise) we abandon  the study of individual trajectories in favor of an examination of the flow of state  distributions. The noise models we consider are homogeneous in time, in that they  may depend on the input, but do not depend on time. The dynamics we consider  is defined by operators acting in the space of measures, and are called Markov  operators [6]. In the following we define the concepts which are required for such  an approach.  Let E be an arbitrary alphabet and f2 be an abstract state space. We assume that  a er-algebra B (not necessarily Borel sets) of subsets of f is given, thus (f, B) is a  measurable space. Let us denote by P the set of probability measures on (f,  This set is called a distribution space.  Let  be a space of finite measures on (f, B) with the total variation norm defined  Noisy Neural Networks and Generalizations 337  by  [1111 ----[1(') -- sup/(A)- inf/(A). (1)  AB AB  Denote by  the set of all bounded linear operators acting from  to itself. The  I1' ]]1- norm on  induces a norm IIPI] - supe ]IP/11 in . An operator P    is said to be a Markov operator if for any probability measure/  P, the image P/  is again a probability measure. For a Markov operator, IIPII - 1.  Definition 2.1 A Markov system is a set of Markov operators T = {P,: u  Z}.  With any Markov system T, one can associate a probabilistic computational sys-  tem. If the probability distribution on the initial states is given by the probability  measure/0, then the distribution of states after n computational steps on inputs  w - w0, Wl, ..., w, is defined as in [5, 8]  Pw!o(A) - Pon '...' PoP0!o  (2)  Let .A and 7 be two subset of 7 ) with the property of having a p-gap  dist(A, ) -- inf lip- '1[  P > 0 (3)  The first set is called a set of accepting distributions and the second is called a set  of rejecting distributions. A language L 6 E* is said to be recognized by Markov  computational system A = (, A, 7, E,/0, T) if  w 6 LP!o6A  w q L,Poo7.  This model of language recognition with a gap between accepting and rejecting  spaces agrees with Rabin's model of probabilistic automata with isolated cut-point  [9] and the model of analog probabilistic computation [4, 5].  An example of a Markov system is a system of operators defined by TPF on (, B).  Let Pu (x, A) be the probability of moving from a state x to the set of states A upon  receiving the input signal u  E. The function Pu(x, .) is a probability measure for  all x   and Pu(', A) is a measurable function of x for any A  B. In this case,  Pu!(A) are defined by  P'!(A) =/c P,(x, A)!(dx). (4)  3 Weakly Ergodic MCS  1  Let P 6  be a Markov operator. The real number 7(P) = I -  sup,,e ]lPp -  PYI[ is called the ergodicity coefficient of the Markov operator. We denote  ((P) = I- 7(P). It can be proven that for any two Markov operators P,P2,  ((PP2) _< ((P)J(P2). The ergodicity coefficient was introduced by Dobrushin [2]  for the particular case of Markov operators induced by TPF P(x, A). In this special  case 7(P) = 1 -supr,y sup A [P(x,A)- P(y,A)I.  Weakly ergodic systems were introduced and studied by Paz in the particular case  of a denumerable state space , where Markov operators are represented by infi-  nite dimensional matrices. The following definition makes no assumption on the  associated measurable space.  Definition 3.1 A Markov system {P,, u G Z} is called weakly ergodic if for any  c > 0, there is an integer r -- r(c 0 such that for any w G E >r and any p, v  P,  1  338 H.T. Siegelmann, ,4. Roitershtein and,4. Ben-Hur  An MCS AA is called weakly ergodic if its associated Markov system  is weakly ergodic.  An MCS JM is weakly ergodic if and only if there is an integer r and real number  c < 1, such that IIPw/ - Pwt, 111 _< c for any word w of length r. Our most general  characterization of weak ergodicity is as follows: [11]:  Theorem 1 An abstract MCS .M is weakly ergodic if and only if there exists  a multiplicative operator's norm [l' II** on  equivalent to the norm I1' I[t :-  liP;kill and such that suPuez IlP. II** < * for some number s < 1   sup{ x:xn=0} IlXll ' - '  The next theorem connects the computational power of weakly ergodic MCS's with  the class of definite languages, generalizing the results by Rabin [9], Paz [8, p. 175],  and Maass and Sontag [5].  Theorem 2 Let .M be a weakly ergodic MCS. If a language L can be recognized by  .M, then it is definite.   4 The Stability Theorem of Weakly Ergodic MCS  An important issue for any computational system is whether the machine is robust  with respect to small perturbations of the system's parameters or under some ex-  ternal noise. The stability of language recognition by weakly ergodic MCS's under  perturbations of their Markov operators was previously considered by Rabin [9] and  Paz [7, 8]. We next state a general version of the stability theorem that is applicable  to our wide notion of weakly ergodic systems.  We first define two MCS's Ad and Ad to be similar if they share the same measur-  able space (f,B), alphabet E, and sets 4 and 7, and if they differ only by their  associated Markov operators.  Theorem 3 Let .M and .M be two similar MCS's such that the first is weakly  ergodic. Then there is a > O, such that if liP. - P. 111 <, for all u e r,, then  the second is also weakly ergodic. Moreover, these two MCS's recognize exactly the  same class of languages.   Corollary 3.1 Let .M and .M be two similar MCS's. Suppose that the first is  weakly ergodic. Then there exists/3 > O, such that ifsup.4es IP.(x,A)-P.(x,A)I _<  /3 for all u 6 E, x  f2, the second is also weakly ergodic. Moreover, these two MCS's  recognize exactly the same class of languages.   A mathematically deeper result which implies Theorem 3 was proven in [11]'  Theorem 4 Let .M and .M be two similar MCS's, such that the first is weakly  ergodic and the second is arbitrary. Then, for any c > 0 there exists  > 0 such  that liP. - _< e for all u   implies IlP - _<  for all words w 6 E*.   Theorem 3 follows from Theorem 4. To see this, one can chose any c < p in Theorem  4 and observe that IIPo - Poll _< c < p implies that the word w is accepted or  rejected by Ad in accordance to whether it is accepted or rejected by  Noisy Neural Networks and Generalizations 339  5 Conditions on the Transition Probabilities  This section discusses practical conditions for weakly ergodic MCS's in which the  Markov operators P are induced by transition probability functions as in (4).  Clearly, a simple sufficient condition for an MCS to be weakly ergodic is given  by supper. d(P) _< I - c, for some c > 0.  Maass and Sontag used Doeblin's condition to prove the computational power of  noisy neural networks [5]. Although the networks in [5] constitute a very particular  case of weakly ergodic MCS's, Doeblin's condition is applicable also to our general  model. The following version of Doeblin's condition was given by Doob [3]:  Definition 5.1 [3] Let P(x, A) be a TPF on (q, B). We say that it satisfies Doeblin  condition, D s, if there exists a constant c and a probability measure it on (, B)  such that P"(z,A) >_ cit(A) for any set A  B.   If an MCS .M is weakly ergodic, then all its associated TPF P(z, A), w  Y:. must  satisfy D8 for some n = n(w). Doob has proved [3, p. 197] that if P(x,A) satisfies  Doeblin's condition D0  with constant c, then for any it,,  P, liPit- P'11 <_  (1 - c)]ll t - '11, i.e., d(P) _< 1 - c. This leads us to the following definition.  Definition 5.2 Let .M be an MCS. We say that the space f is small with respect  to AA if there exists an rn > 0 such that all associated TPF Pw(x,A), w  E "  satisfy Doeblin's condition D& uniformly with the same constant c, i.e., Pw (x, A) >_  cit(A), w  Y?.   The following theorem strengthens the result by Maass and Sontag [5].  Theorem 5 Let .M be an MC$. If the space f is small with respect to .M, then  fid is weakly ergodic, and it can recognize only definite languages.   This theorem provides a convenient method for checking weak ergodicity in a given  TPF. The theorem implies that it is sufficient to execute the following simple check:  choose any integer n, and then verify that for every state x and all input strings  w  Y?, the "absolutely continuous" part of all TPF P, w  Y? is uniformly  bounded from below:  Pw ({y: pw(x,y) >_ c for all w  Y?) _> c2, (6)  where pw(x,y) is the density of the absolutely continuous component of Pw(x,.)  with respect to Pw, and cl, c2 are positive numbers.  Most practical systems can be defined by null preserving TPF (including for example  the systems in [5]). For these systems we provide (Theorem 6) a sufficient and neces-  sary condition in terms of density kernels. A TPF Pu(x, A), u 6 E is called null pre-  serving with respect to a probability measure it  P if it has a density with respect  to it i.e., P(x,A)= f,4 p(x,z)it(dz). It is not hard to see, that the property of null  preserving per letter u 6 E implies that all TPF Pw(x, A) of words w  E* are null  preserving as well. In this case ((P)= 1- inf,y frrnin{p(x,z),p(y,z)}it(dz)  and we have:  Theorem 6 Let fid be an MC$ defined by null preserving transition probability  functions Pu, u  E. Then, fid is weakly ergodic if and only if there exists n such  that infwcr.. inf,,y fa rnin{pu(x,z),pu(y,z)}itu(dz) > O.   A similar result was previously established by Paz [7, 8] for the case of a denumerable  state space . This theorem allows to treat examples which are not covered by  340 H. T. Siegelmann, A. Roitershtein and A. Ben-Hur  Theorem 5. For example, suppose that the space f is not small with respect to an  MCS .M, but for some n and any w 6 E n there exists a measure bw on (f, B) with  the property that for any couple of states x, y 6 f  bw ({z: min{po(x,z),pw(y,z)} > Cl}) > cs, (7)  where pw(x,y) is the density of Pw(x,.) with respect to bw, and Cl,C are positive  numbers. This condition may occur even if there is no y such that pu(x, y) <_ Cl for  all x  fl.  6 Examples of Weakly Ergodic Systems  1. The Synchronous Parallel Model  Let (fi, Bi), i = 1, 2, ..., N be a collection of measurable sets. Define fi = 1-Ij  and 15 4 = 1-Ij Bj. Then (fi, B i) are measurable spaces. Define also E =  and T = {Px,,(x,Ai) : (xl,u) 6 Z} be given stochastic kernels. Each set  defines an MCS AAi. We can define an aggregate MCS by setting  B -- rli/3/, s - rli /, R - rli Ri, and  P,(x,A) : H Px,,,(xi,Ai). (8)  i  This describes a model of N noisy computational systems that update in syn-  chronous parallelism. The state of the whole aggregate is a vector of states of the  individual components, and each receives the states of all other components as part  of its input.  Theorem 7 [12] Let A4 be an MC$ defined by.equation (8). It is weakly erg.odic  P' _ - for any u  at least one set of operators T i is such that 5(u,x) < 1 c  and some positive number c.   2. The Asynchronous Parallel Model  In this model, at every step only one component is activated. Suppose that a collec-  tion of N similar MCS's .AAi, i = 1,..., N is given. Consider a probability measure  e = {el,..., es} on the set K - {1,..., N}. Assume that in each computational  step only one MCS is activated. The current state of the whole aggregate is rep-  resented by the state of its active component. Assume also that the probability of  a computational system .AAi to be activated, is time-independent and is given by  Prob(Adi) = ei. The aggregate system is then described by stochastic kernels  N  P,(x,A) : E eiP'i(x'A)' (9)  i=1  Theorem 8 [12] Let AA be an MCS defined by formula (9). It is weakly ergodic if  at least one set of operators 1  { Pu  ),..., { Pu N) is weakly ergodic.   3. Hybrid Weakly Ergodic Systems  We now present a hybrid weakly ergodic computational system consisting of both  continuous and discrete elements. The evolution of the system is governed by a  differential equation, while its input arrives at discrete times. Let f = IR n, and  consider a collection of differential equations  (s) = (x(s)), u 6 r,, s e [0, ). (0)  Noisy Neural Networks and Generalizations 341  Suppose that p, (x) is sufficiently smooth to ensure the existence and uniqueness of  solutions of Equation (10) for s  [0, 1] and for any initial condition.  Consider a computational system which receives an input u(t) at discrete times  to,t,t2 .... In the interval t  [ti,ti+.] the behavior of the system is described by  Equation (10), where s -- t-ti. A random initial condition for the time t, is defined  by  Prob[x,(t)(O)  A] = P(x(t_)(1),A), (11)  where xu (t _  ) (1) is the st ate of the system after previously completed cornput atio ns,  and Pu (x, A), u C Z is a family of stochastic kernels on  x B. This describes a system  which receives inputs in discrete instants of time; the input letters u  Z cause  random perturbations of the st ate x, (t- ) (1) governed by the transition probability  functions Pu(t)(xu(t-), A). In all other times the system is a noise-free continuous  computational system which evolves according to equation (10).  Let  = I n , x0   be a distinguished initial state, and let $ and R be two subsets  of  with the property of having a p-gap: dist($, R) = infes,yeR I[x - YI[ = P ) O.  The first set is called a set of accepting final states and the second is called a  set of rejecting final states. We say that the hybrid computational system ./M =  (,E,xo,Pu,$,R) recognizes L _C E* if for all w = wo...w  E* and the end  letter $  E the following holds: w  L : Prob(x,$(1)  $)   + e, and  1  Theorem 9 [12] Let JM be a hybrid computational system. It is weakly ergodic if  its set of evolution operators T - {Pu ' u  E) is weakly ergodic.   References  [1] Casey, M., The Dynamics of Discrete-Time Computation, With Application to Re-  current Neural Networks and Finite State Machine Extraction, Neural Computation  8, 1135-1178, 1996.  [2] Dobrushin, R. L., Central limit theorem for nonstationary Markov chains I, II.  Theor. Probability Appl. vol. 1, 1956, pp 65-80, 298-383.  [3] Doob J. L., Stochastic Processes. John Wiley and Sons, Inc., 1953.  [4] W. Maass and Orponen, P., On the effect of analog noise in discrete time computation,  Neural Computation, 10(5), 1998, pp. 1071-1095.  [5] W. Maass and Sontag, E., Analog neural nets with Gaussian or other common noise  distribution cannot recognize arbitrary regular languages, Neural Computation, 11,  1999, pp. 771-782.  [6] Neveu J., Mathematical Foundations of the Calculus of Probability. Holden Day, San  Francisco, 1964.  [7] Paz A., Ergodic theorems for infinite probabilistic tables. Ann. Math. Statist. vol.  41, 1970, pp. 539-550.  [8] Paz A., Introduction to Probabilistic Automata. Academic Press, Inc., London, 1971.  [9] Rabin, M., Probabilistic automata, Information and Control, vol 6, 1963, pp. 230-245.  [10] Siegelmann H. T., Neural Networks and Analog Computation: Beyond the Turing  Limit. Birkhauser, Boston, 1999.  [11] Siegelmann H. T. and Roitershtein A., On weakly ergodic computational systems,  1999, submitted.  [12] Siegelmann H. T., Roitershtein A., and Ben-Hur, A., On noisy computational sys-  tems, 1999, Discrete Applied Mathematics, accepted.  
Image representations for facial expression  coding  Marian Stewart Bartlett*  U.C. San Diego  marnisalk. edu  Javier R. Movellan  U.C. San Diego  movellancogsc. ucsd. edu  Paul Ekman  U.C. San Francisco  ekmancompuserve. com  Gianluca Donato  Digital Persona, Redwood City, CA  gianlucaddigit alpersona. com  Joseph C. Hager  Network Information Res., SLC, Utah  j chagerbm. com  Terrence J. Sejnowski  Howard Hughes Medical Institute  The Salk Institute; U.C. San Diego  terrysalk. edu  Abstract  The Facial Action Coding System (FACS) (9) is an objective  method for quantifying facial movement in terms of component  actions. This system is widely used in behavioral investigations  of emotion, cognitive processes, and social interaction. The cod-  ing is presently performed by highly trained human experts. This  paper explores and compares techniques for automatically recog-  nizing facial actions in sequences of images. These methods include  unsupervised learning techniques for finding basis images such as  principal component analysis, independent component analysis and  local feature analysis, and supervised learning techniques such as  Fisher's linear discriminants. These data-driven bases are com-  pared to Gabor wavelets, in which the basis images are predefined.  Best performances were obtained using the Gabor wavelet repre-  sentation and the independent component representation, both of  which achieved 96% accuracy for classifying 12 facial actions. The  ICA representation employs 2 orders of magnitude fewer basis im-  ages than the Gabor representation and takes 90% less CPU time  to compute for new images. The results provide converging support  for using local basis images, high spatial frequencies, and statistical  independence for classifying facial actions.  I Introduction  Facial expressions provide information not only about affective state, but also about  cognitive activity, temperament and personality, truthfulness, and psychopathology.  The Facial Action Coding System (FACS) (9) is the leading method for measur-  ing facial movement in behavioral science. FACS is performed manually by highly  trained human experts. A FACS coder decomposes a facial expression into com-  ponent muscle movements (Figure 1). Ekman and Friesen described 46 distinct  facial movements, and over 7000 distinct combinations of such movements have  * To whom correspondence should be addressed. (UCSD 0523, La Jolla, CA 92093.)  This research was supported by NIH Grant No. 1F32 MH12417-01.  Image Representation.v for Facial Expression Coding 887  been observed in spontaneous behavior. An automated system would make facial  expression measurement more widely accessible as a research tool for behavioral sci-  ence and medicine. Such a system would also have application in human-computer  interaction tools and low bandwidth facial animation coding.  A number of systems have appeared in the computer vision literature for classifying  facial expressions into a few basic categories of emotion, such as happy, sad, or  surprised. While such approaches are important, an objective and detailed measure  of facial activity such as FACS is needed for basic research into facial behavior. In a  system being developed concurrently for automatic facial action coding, Cohn and  colleagues (7) employ feature point tracking of a select set of image points. Tech-  niques employing 2-D image filters have proven to be more effective than feature-  based representations for face image analysis [e.g. (6)]. Here we examine image  analysis techniques that densely analyze graylevel information in the face image.  This work surveys anal compares techniques for face image analysis as applied to  automated FACS encoding.  The analysis focuses on methods for face image repre-  sentation in which image graylevels are described as a linear superposition of basis  images. The techniques were compared on a common image testbed using common  similarity measures and classifiers.  We compared four representations in which the basis images were learned from  the statistics of the face image ensemble. These include unsupervised learning  techniques such as principal component analysis (PCA), and local feature analy-  sis (LFA), which are learned from the second-order dependences among the image  pixels, and independent component analysis (ICA) which is learned from the high-  order dependencies as well. We also examined a representation obtained through  supervised learning on the second-order image statistics, Fisher's linear discrimi-  nants (FLD). Classification performances with these data-driven basis images were  compared to Gabor wavelets, in which the basis images were pre-defined. We ex-  amined properties of optimal basis images, where optimal was defined in terms of  classification.  Generalization to novel faces was evaluated using leave-one-out cross-validation.  Two basic classifiers were employed: nearest neighbor and template matching, where  the templates were the mean feature vectors for each class. Two similarity measures  were employed for each classifier: Euclidean distance and cosine of the angle between  feature vectors.  2  AU 1 Inner brow raiser  AU 2 Outer brow raiser  AU 4 Brow lowerer  Figure 1- a. The facial muscles underlying six of the 46 facial actions. b. Cropped  face images and 5-images for three facial actions (AU's).  iA detailed description of this work appears in (8).  888 M. S. Bartlett, G. Donato, J. R. Movellan, J.. C. Hager, P. Elanan and T. J. Sejnowsla'  2 Image Database  We collected a database of image sequences of subjects performing specified facial  actions. The database consisted of image sequences of subjects performing specified  facial actions. Each sequence began with a neutral expression and ended with a high  magnitude muscle contraction. For this investigation, we used 111 sequences from  20 subjects and attempted to classify 12 actions: 6 upper face actions and 6 lower  face actions. Upper and lower-face actions were analyzed separately since facial  motions in the lower face do not effect the upper face, and vice versa (9).  The face was located in the first frame in each sequence using the centers of the  eyes and mouth. These coordinates were obtained manually by a mouse click. The  coordinates from Frame I were used to register the subsequent frames in the se-  quence. The aspect ratios of the faces were warped so that the eye and mouth  centers coincided across all images. The three coordinates were then used to rotate  the eyes to horizontal, scale, and finally crop a window of 60 x 90 pixels containing  the upper or lower face. To control for variations in lighting, logistic threshold-  ing and luminance scaling was performed (13). Difference images (6-images) were  obtained by subtracting the neutral expression in the first image of each sequence  from the subsequent images in the sequence.  3 Unsupervised learning  3.1 Eigenfaces (PCA)  A number of approaches to face image analysis employ data-driven basis vectors  learned from the statistics of the face image ensemble. Techniques such as eigen-  faces (17) employ principal component analysis, which is an unsupervised learning  method based on the second-order dependencies among the pixels (the pixelwise  covariances). PCA has been applied successfully to recognizing facial identity (17),  and full facial expressions (14).  Here we performed PCA on the dataset of 6-images, where each 6-image comprised  a point in R ' given by the brightness of the n pixels. The PCA basis images  were the eigenvectors of the covariance matrix (see Figure 2a), and the first p  components comprised the representation. Multiple ranges of components were  tested, from p = 10 to p - 200, and performance was also tested excluding the  first 1-3 components. Best performance of 79.3% correct was obtained with the  first 30 principal components, using the Euclidean distance similarity measure and  template matching classifier.  Padgett and Cottrell (14) found that a local PCA representation outperformed  global PCA for classifying full facial expressions of emotion. Following the meth-  ods in (14), a set of local basis images was derived from the principal components  of 15x15 image patches from randomly sampled locations in the 6-images (see  Figure 2d.) The first p principal components comprised a basis set for all image  locations, and the representation was downsampled by a factor of 4. Best perfor-  mance of 73.4% was obtained with components 2-30, using Euclidean distance and  template matching. Unlike the findings in (14), local basis images obtained through  PCA were not more effective than global PCA for facial action coding. A second  local implementation of PCA, in which the principal components were calculated  for fixed 15x 15 image patches also failed to improve over global PCA.  3.2 Local Feature Analysis (LFA)  Penev and Atick (15) recently developed a local, topographic representation based  on second-order image statistics called local feature analysis (LFA). The kernels are  derived from the principal component axes, and consist of a "whitening" step to  equalize the variance of the PCA coefficients, followed by a rotation to pixel space.  Image Representat'ons for Facial Expression Coding 889  bm  Ce  dm  Figure 2: a. First 4 PCA basis images. b. Four ICA basis images. The ICA basis  images are local, spatially opponent, and adaptive. c. Gabor kernels are local,  spatially opponent, and predefined. d. First 4 local PCA basis images.  We begin with the matrix P containing the principal component eigenvectors in  its columns, and Ai are the corresponding eigenvalues. Each row of the matrix K  serves as an element of the LFA image dictionary 2  K = PVP T where V = D- = diag(--/) i = 1,...,p (1)  where Ai are the eigenvalues. The rows of K were found to have spatially local prop-  erties, and are "topographic" in the sense that they are indexed by spatial location  (15). The dimensionality of the LFA representation was reduced by employing an  iterative sparsification algorithm based on multiple linear regression described in  (15).  The LFA representation attained 81.1% correct classification performance. Best  performance was obtained using the first 155 kernels, the cosine similarity measure,  and nearest neighbor classifier. Classification performance using LFA was not sig-  nificantly different from the performance using PCA. Although a face recognition  algorithm based on the principles of LFA outperformed Eigenfaces in the March  1995 FERET competition, the exact algorithm has not been disclosed. Our results  suggest that an aspect of the algorithm other than the LFA representation accounts  for the difference in performance.  3.3 Independent Component Analysis (ICA)  Representations such as Eigenfaces, LFA, and FLD are based on the second-order  dependencies among the pixels, but are insensitive to the high-order dependencies.  High-order dependencies are relationships that cannot be described by a linear pre-  dictor. Independent component analysis (ICA) learns the high-order dependencies  in addition to the second-order dependencies among the pixels.  2An image dictionary is a set of images that decomposes other images, e.g. through  inner product. Here it finds the coefficients for the basis set K-x  890 M. S. Bartlett, G. Donato, J. R. Movellan, J. C. Hager, P Ekman and T. J. Sejnowski  The ICA representation was obtained by performing Bell & Sejnowski's infomax  algorithm (4) (5) on the ensemble of 5-images in the rows of the matrix X. The  images in X were assumed to be a linear mixture of an unknown set of independent  source images which were recovered through ICA. In contrast to PCA, the ICA  source images were local in nature (see Figure 2b). These source images provided  a basis set for the expression images. The coefficients of each image with respect  to the new basis set were obtained from the estimated mixing matrix A  W -,  where W is the ICA weight matrix [see (1), (2)].  Unlike PCA, there is no inherent ordering to the independent components of the  dataset. We therefore selected as an ordering parameter the class discriminability of  each component, defined as the ratio of between-class to within-class variance. Best  performance of 95.5% was obtained with the first 75 components selected by class  discriminability, using the cosine similarity measure, and nearest neighbor classifier.  Independent component analysis gave the best performance among all of the data-  driven image kernels. Class discriminability analysis of a PCA representation was  previously found to have little effect on classification performance with PCA (2).  4 Supervised learning: Fisher's Linear Discriminants (FLD)  A class specific linear projection of a PCA representation of faces was recently  shown to improve identity recognition performance (3). The method employs a  classic pattern recognition technique, Fisher's linear discriminant (FLD), to project  the images into a c- I dimensional subspace in which the c classes are maximally  separated. Best performance was obtained by choosing p - 30 principal components  to first reduce the dimensionality of the data. The data was then projected down  to 5 dimensions via the FLD projection matrix, Wftd. The FLD image dictionary  was thus Wpc, * Wftd. Best performance of 75.7% correct was obtained with the  Euclidean distance similarity measure and template matching classifier.  FLD provided a much more compact representation than PCA. However, unlike the  results obtained by (3) for identity recognition, Fisher's Linear Discriminants did  not improve over basic PCA (Eigenfaces) for facial action classification. The differ-  ence in performance may be due to the low dimensionality of the final representation  here. Class discriminations that are approximately linear in high dimensions may  not be linear when projected down to as few as 5 dimensions.  5 Predefined image kernels: Gabor wavelets  An alternative to the adaptive bases described above are wavelet decompositions  based on predefined families of Gabor kernels. Gabor kernels are 2-D sine waves  modulated by a Gaussian (Figure 2c). Representations employing families of Gabor  filters at multiple spatial scales, orientations, and spatial locations have proven  successful for recognizing facial identity in images (11). Here, the 5-images were  convolved with a family of Gabor kernels bi, defined as  (2)  where /i = ( cosT, ), v?   sin T, fv = 2- r, T, =/.  Following (11), the representation consisted of the amplitudes at 5 frequencies (v =  0 - 4) and 8 orientations (/ - I - 8). Each filter output was downsampled by a  factor q and normalized to unit length. We tested the performance of the system  using q = 1, 4, 16 and found that q = 16 yielded the best generalization rate. Best  performance was obtained with the cosine similarity measure and nearest neighbor  Image Representations for Facial Expression Coding 891  classifier. Classification performance with the Gabor representation was 95.5%.  This performance was significantly higher than all of the data-driven approaches in  the comparison except independent component analysis, with which it tied.  6 Results and Conclusions  PCA Local PCA LFA ICA FLD Gabor  79.3 J:3.9 73.4 J:4.2 81.1 J:3.7 95.5 J:2.0 75.7 J:4.1 95.5 J:2.0  Table 1: Summary of classification performance for 12 facial actions.  We have compared a number of different image analysis methods on a difiqcult  classification problem, the classification of facial actions. Best performances were  obtained with the Gabor and ICA representations, which both achieved 95.5% cor-  rect classification (see Table 1). The performance of these two methods equaled  the agreement level of expert human subjects on these images (94%). Image repre-  sentations derived from the second-order statistics of the dataset (PCA and LFA)  performed in the 80% accuracy range. An image representation derived from super-  vised learning on the second-order statistics (FLD) also did not significantly differ  from PCA. We also obtained evidence that high spatial frequencies are important  for classifying facial actions. Classification with the three highest frequencies of the  Gabor representation (y = 0, 1,2, cycles/face - 15,18,21 cycles/face) was 93% com-  pared to 84% with the three lowest frequencies (y = 2, 3, 4, cycles/face -- 9,12,15).  The two representations that significantly outperformed the others, Gabor and Inde-  pendent Components, employed local basis images, which supports recent findings  that local basis images are important for face image analysis (14) (10) (12). The  local property alone, however, does not account for the good performance of these  two representations, as LFA performed no better than PCA on this classification  task, nor did local implementations of PCA.  In addition to spatial locality, the ICA representation and the Gabor filter repre-  sentation share the property of redundancy reduction, and have relationships to  representations in the visual cortex. The response properties of primary visual cor-  tical cells are closely modeled by a bank of Gabor kernels. Relationships have been  demonstrated between Gabor kernels and independent component analysis. Bell &  Sejnowski (5) found using ICA that the kernels that produced independent outputs  from natural scenes were spatially local, oriented edge kernels, similar to a bank of  Gabor kernels. It has also been shown that Gabor filter outputs of natural images  are at least pairwise independent (16).  The Gabor wavelets and ICA each provide a way to represent face images as a linear  superposition of basis functions. Gabor wavelets employ a set of pre-defined basis  images, whereas ICA learns basis images that are adapted to the data ensemble.  The Gabor wavelets are not specialized to the particular data ensemble, but would  be advantageous when the amount of data is small. The ICA representation has the  advantage of employing two orders of magnitude fewer basis images. This can be  an advantage for classifiers that involve parameter estimation. In addition, the ICA  representation takes 90% less CPU time than the Gabor representation to compute  once the ICA weights are learned, which need only be done once.  In summary, this comparison provided converging support for using local basis  images, high spatial frequencies, and statistical independence for classifying facial  actions. Best performances were obtained with Gabor wavelet decomposition and  independent component analysis. These two representations employ graylevel basis  functions that share properties of spatial locality, independence, and have relation-  ships to the response properties of visual cortical neurons.  An outstanding issue is whether our findings depend on the simple recognition  engines we employed. Would a smarter recognition engine alter the relative per-  892 M. S. Bartlett, G. Donato, J. R. Movellan, J. C. Hager, P Ekman and T. J. Sejnowski  formances? Our preliminary investigations suggest that is not the case. Hidden  Markov models (HMM's) were trained on the PCA, ICA and Gabor representa-  tions. The Gabor representation was reduced to 75 dimensions using PCA before  training the HMM. The HMM improved classification performance with ICA to  96.3%, and it did not change the overall findings, as it gave similar percent im-  provements to the PCA and PCA-reduced Gabor representations over their nearest  neighbor performances. The dimensionality reduction of the Gabor representation,  however, caused its nearest neighbor performance to drop, and the performance  with the HMM was 92.7%. The lower dimensionality of the ICA representation was  an advantage when employing the HMM.  7  [1]  [2]  References  M.S. Bartlett. Face Image Analysis by Unsupervised Learning and Redundancy Re-  duction. PhD thesis, University of California, San Diego, 1998.  M.S. Bartlett, H.M. Lades, and T.J. Sejnowski. Independent component representa-  tions for face recognition. In T. Rogowitz, B. & Pappas, editor, Proceedings of the  SPIE Symposium on Electonic Imaging: Science and Technology; Human Vision and  Electronic Imaging III, volume 3299, pages 528-539, San Jose, CA, 1998. SPIE Press.  [3] P.N. Belhumeur, J.P. Hespanha, and D.J. Kriegman. Eigenfaces vs. fisherfaces:  Recognition using class specific linear projection. IEEE Transations on Pattern Anal-  ysis and Machine Intelligence, 19(7):711-720, 1997.  [4] A.J. Bell and T.J. Sejnowski. An information-maximization approach to blind sepa-  ration and blind deconvolution. Neural Computation, 7(6):1129-1159, 1995.  [5] A.J. Bell and T.J. Sejnowski. The independent components of natural scenes are edge  filters. Vision Research, 37(23):3327-3338, 1997.  [6] R. Brunelli and T. Poggio. Face recognition: Features versus templates. IEEE trans-  actions on pattern analysis and machine intelligence, 15(10):1042-1052, 1993.  [7] J.F. Cohn, A.J. Zlochower, J.J. Lien, Y-T Wu, and T. Kanade. Automated face cod-  ing: A computer-vision based method of facial expression analysis. Psychophysiology,  35(1):35-43, 1999.  [8] G. Donato, M. Bartlett, J. Hager, P. Ekman, and T. Sejnowski. Classifying facial ac-  tions. IEEE Transactions on Pattern Analysis and Machine Intelligence, 21(10):974-  989, 1999.  [9] P. Ekman and W. Friesen. Facial Action Coding System: A Technique for the Mea-  surement of Facial Movement. Consulting Psychologists Press, Palo Alto, CA, 1978.  [10] M.S. Gray, J. Movellan, and T.J. Sejnowski. A comparison of local versus global image  decomposition for visual speechreading. In Proceedings of the ,lth Joint Symposium  on Neural Computation, pages 92-98. Institute for Neural Computation, La Jolla,  CA, 92093-0523, 1997.  [11] M. Lades, J. Vorbriiggen, J. Buhmann, J. Lange, W. Konen, C. yon der Malsburg, and  R. Wfirtz. Distortion invariant object recognition in the dynamic link architecture.  IEEE Transactions on Computers, 42(3):300-311, 1993.  [12] D.D. Lee and S. Seung. Learning the parts of objects by non-negative matrix factor-  ization. Nature, 401:788-791, 1999.  [13] J.R. Movellan. Visual speech recognition with stochastic networks. In G. Tesauro,  D.S. Touretzky, and T. Leen, editors, Advances in Neural Information Processing  Systems, volume 7, pages 851-858. MIT Press, Cambridge, MA, 1995.  [14] C. Padgett and G. Cottrell. Representing face images for emotion classification.  In M. Mozer, M. Jordan, and T. Petsche, editors, Advances in Neural Information  Processing Systems, volume 9, Cambridge, MA, 1997. MIT Press.  [15] P.S. Penev and J.J. Atick. Local feature analysis: a general statistical theory for  object representation. Network: Computation in Neural Systems, 7(3):477-500, 1996.  [16] E. P. Simoncelli. Statistical models for images: Compression, restoration and synthe-  sis. In 31st Asilomar Conference on Signals, Systems and Computers, Pacific Grove,  CA, November 2-5 1997.  [17] M. Turk and A. Pentland. Eigenfaces for recognition. Journal of Cognitive Neuro-  science, 3(1):71-86, 1991.  
The Parallel Problems Server: an Interactive Tool  for Large Scale Machine Learning  Charles Lee Isbell, Jr.  isbell @research.att.com  AT&T Labs  180 Park Avenue Room A255  Florham Park, NJ 07932-0971  Parry Husbands  PJRHusbands@lbl.gov  Lawrence Berkeley National Laboratory/NERSC  1 Cyclotron Road, MS 50F  Berkeley, CA 94720  Abstract  Imagine that you wish to classify data consisting of tens of thousands of ex-  amples residing in a twenty thousand dimensional space. How can one ap-  ply standard machine learning algorithms? We describe the Parallel Prob-  lems Server (PPServer) and MATLAB*P. In tandem they allow users of  networked computers to work transparently on large data sets from within  Matlab. This work is motivated by the desire to bring the many benefits  of scientific computing algorithms and computational power to machine  learning researchers.  We demonstrate the usefulness of the system on a number of tasks. For  example, we perform independent components analysis on very large text  corpora consisting of tens of thousands of documents, making minimal  changes to the original Bell and Sejnowski Matlab source (Bell and Se-  jnowski, 1995). Applying ML techniques to data previously beyond their  reach leads to interesting analyses of both data and algorithms.  1 Introduction  Real-world data sets are extremely large by the standards of the machine learning community.  In text retrieval, for example, we often wish to process collections consisting of tens or  hundreds of thousands of documents and easily as many different words. Naturally, we  would like to apply machine learning techniques to this problem; however, the sheer size of  the data makes this difficult.  This paper describes the Parallel Problems Server (PPServer) and MATLAB*P. The PPServer  is a "linear algebra server" that executes distributed memory algorithms on large data sets.  Together with MATLAB*P, users can manipulate large data sets within Matlab transpar-  ently. This system brings the efficiency and power of highly-optimized parallel computation  to researchers using networked machines but maintain the many benefits of interactive envi-  ronments.  We demonstrate the usefulness of the PPServer on a number of tasks. For example, we  perform independent components analysis on very large text corpora consisting of tens of  thousands of documents with minimal changes to the original Bell and Sejnowski Matlab  source (Bell and Sejnowski, 1995). Applying ML techniques to datasets previously beyond  704 C. L. Isbell, Jr. and P. Husbands  MATLAB 5  Libraries  Computational &  Interface Routines  Packages  Machinel Machinea  Matlab 5 Server Workers Workers  .-'-  .............  ,"" I_ .............  Server variables  Figure 1: Use of the PPServer by Matlab is almost completely transparent. PPServer vari-  ables are tied to the PPServer itself while Matlab maintains handles to the data. Using Mat-  lab's object system, functions using PPServer variables invoke PPServer commands implic-  itly.  their reach, we discover interesting analyses of both data and algorithms.  2 The Parallel Problems Server  The Parallel Problems Server (PPServer) is the foundation of this work. The PPServer is a  realization of a novel client-server model for computation on very large matrices. It is com-  patible with any Unix-like platform supporting the Message Passing Interface (MPI) library  (Gropp, Lusk and Skjellum, 1994). MPI is the standard for multi-processor communication  and is the most portable way for writing parallel code.  The PPServer implements functions for creating and removing distributed matrices, loading  and storing them from/to disk using a portable format, and performing elementary matrix  operations. Matrices are two-dimensional single or double precision arrays created on the  PPServer itself (functions are provided for transferring matrix sections to and from a client).  The PPServer supports both dense and sparse matrices.  The PPServer communicates with clients using a simple request-response protocol. A client  requests an action by issuing a command with the appropriate arguments, the server executes  that command, and then notifies the client that the command is complete.  The PPServer is directly extensible via compiled libraries called packages. The PPServer im-  plements a robust protocol for communicating with packages. Clients (and other packages)  can load and remove packages on-the-fly, as well as execute commands within packages.  Package programmers have direct access to information about the PPServer and its matrices.  Each package represents its own namespace, defining a set of visible function names. This  supports data encapsulation and allows users to hide a subset of functions in one package by  loading another that defines the same function names. Finally, packages support common  parallel idioms (eg applying a function to every element of a matrix), making it easier to add  common functionality.  All but a few PPServer commands are implemented in packages, including basic matrix  operations. Many highly-optimized public libraries have been realized as packages using ap-  propriate wrapper functions. These packages include ScaLAPACK (Blackford et al., 1997),  S3L (Sun's optimized version of ScaLAPACK), PARPACK (Maschhoff and Sorensen, 1996),  and PETSc (PETSc,).  Large Scale Machine Learning Using The Parallel Problems Server 705  1 function H=hilb(n)  2 J = l:n;  3 J = J(ones(n,1),:);  4 I = J';  5 E = ones (n,n);  6 H = E./(I+J-1);  Figure 2: Matlab code for producing Hilbert matrices. When n is influenced by P, each of  the constructors creates a PPServer object instead of a Matlab object.  3 MATLAB*P  By directly using the PPServer's client communication interface, it is possible for other ap-  plications to use the PPServer's functionality. We have implemented a client interface for  Matlab, called MATLAB*P. MATLAB*P is a collection of Matlab 5 objects, Matlab m-files  (Matlab's scripting language) and Matlab MEX programs (Matlab's external language API)  that allows for the transparent integration of Matlab as a front end for the Parallel Problems  Server.  The choice of Matlab was influenced by several factors. It is the de facto standard for sci-  entific computing, enjoying wide use in industry and academia. In the machine learning  community, for example, algorithms are often written as Matlab scripts and made freely  available. In the scientific computing community, algorithms are often first prototyped in  Matlab before being optimized for languages such as Fortran.  We endeavor to make interaction with the PPServer as transparent as possible for the user.  In principle, a typical Matlab user should never have to make explicit calls to the PPServer.  Further, current Matlab programs should not have to be rewritten to take advantage of the  PPServer.  Space does not permit a complete discussion of MATLAB*P (we refer the reader to (Hus-  bands and Isbell, 1999)); however, we will briefly discuss how to use prewritten Matlab  scripts without modification. This is accomplished through the simple but innovative ? nota-  tion.  We use Matlab 5's object oriented features to create PPServer objects automatically. ? is  a special object we introduce in Matlab that acts just like the integer 1. A user typing  a=ones ( 1000*?, 1000 ) or b=rand (1000,1000*?) obtains two 1000-by-1000 ma-  trices distributed in parallel. The reader can guess the use of P here: it indicates that a is  distributed by rows and b by columns.  To a user, a and b are matrices, but within Matlab, they are handles to special distributed  types that exist on the PPServer. Any further references to these variables (e.g. via such  commands as eig, svd, inv, *, +, -) are recognized as a call to the PPServer rather than as a  traditional Matlab command.  Figure 2 shows the code for Matlab's built in function hlb. The call hlb (n) produces  the n x n Hilbert matrix (Hij - 1  -- i+]-  )' When n is influenced by ?, a parallel array results:   J=l: n in line 2 creates the PPServer vector 1, 2,  ., n and places a handle to it in  J. Note that this behavior does not interfere with the semantics of for loops (for  =l:n) as Matlab assigns to  the value of each column of l:n: the numbers  1,2,...,n.   ones (n, 1) in line 3 produces a PPServer matrix.   Emulation of Matlab's indexing functions results in the correct execution of line 3.  706 C L. Isbell, Jr. and P. Husbands   Overloading of ' (the transpose operator) executes line 4 on the PPServer.   In line 5, lg is generated on the PPServer because of the overloading of ones.   Overloading elementary matrix operations makes H a PPServer matrix (line 6).  The Parallel Problems Server and MATLAB*P have been tested extensively on a variety of  platforms. They currently run on Cray supercomputers , clusters of symmetric multiproces-  sors from Sun Microsystems and DEC as well as on clusters of networked Intel PCs. The  PPServer has also been tested with other clients, including Common LISP.  Although computational performance varies depending upon the platform, it is clear that  the system provides distinct computational advantages. Communication overhead (in our  experiments, roughly two milliseconds per PPServer command) is negligible compared to  the computational and space advantage afforded by transparent access to highly-optimized  linear algebra algorithms.  4 Applications in Text Retrieval  In this section we demonstrate the efficacy of the PPServer on real-world machine learning  problems. In particular we explore the use of the PPServer and MATLAB*P in the text  retrieval domain.  The task in text retrieval is to find the subset of a collection of documents relevant to a user's  information request. Standard approaches are based on the Vector Space Model (VSM).  A document is a vector where each dimension is a count of the occurrence of a different  word. A collection of documents is a matrix, D, where each column is a document vector  di. The similarity between two, documents is their inner product, did j. Queries are just like  documents, so the relevance of documents to a query, q, is D ' q.  Typical small collections contain a thousand vectors in a ten thousand dimensional space,  while large collections may contain 500,000 vectors residing in hundreds of thousands of  dimensions. Clearly, well-understood standard machine learning techniques may exhibit un-  predictable behavior under such circumstances, or simply may not scale at all.  Classically, ML-like approaches try to construct a set of linear operators which extract the  underlying "topic" structure of documents. Documents and queries are projected into that  new (usually smaller) space before being compared using the inner product.  The large matrix support in MATLAB*P enables us to use matrix decomposition techniques  for extracting linear operators easily. We have explored several different algorithms(Isbell  and Viola, 1998). Below, we discuss two standard algorithms to demonstrate how the  PPServer allows us to perform interesting analysis on large datasets.  4.1 Latent Semantic Indexing  Latent Semantic Indexing (LSI) (Deerwester et al., 1990) constructs a smaller document ma-  trix by using the Singular Value Decomposition (SVD): D = USV T. U contains the eigen-  vectors of the co-occurrence matrix while the diagonal elements of $ (referred to as singular  values) contain the square roots of their corresponding eigenvalues. The eigenvectors with  the largest eigenvalues capture the axes of largest variation in the data.  LSI projects documents onto the k-dimensional subspace spanned by the first k columns of U  (denoted Uk) so that the documents are now: Vff = $-Uk. Queries are similarly projected.  Thus, the document-query scores for LSI can be obtained with simple Matlab code:   Although there is no Matlab for the Cray, we are still able to use it to "execute" Matlab code .in  parallel.  Large Scale Machine Learning Using The Parallel Problems Server 707  Figure 3: The first 200 singular values of a collection of about 500,000 documents and  200,000 terms, and singular values for half of that collection. Computation for on the full  collection took only 62 minutes using 32 processors on a Cray T3E.  D=dsparse('term-doc'); %D SPARSE  Q=dsparse('queries');  [U,S,V]=svds(D,k); % compute  sc=getlsiscores(U,S,V,Q); % computes  reads a sparse matrix  the k-SVD of D  v* (i/s)*u'*q  The scores that are returned can then be combined with relevance judgements to obtain pre-  cision/recall curves that are displayed in Matlab:  r=dsparse('judgements');  [pr,re]=precisionrecall(sc,r);  plot(re('@'),pr('@'));  In addition to evaluating the performance of various techniques, we can also explore charac-  teristics of the data itself. For example, many implementations of LSI on large collections  use only a subset of the documents for computational reasons. This leads one to question how  the SVD is affected. Figure 3 shows the first singular values for one large collection as well  as for a random half of that collection. It shows that the shape of the curves are remarkably  similar (as they are for the other half). This suggests that we can derive a projection matrix  from just half of the collection. An evaluation of this technique can easily be performed  using our system. Premlinary experiments show nearly identical retrieval performance.  4.2 What are the Independent Components of Documents?  Independent components analysis (ICA)(Bell and Sejnowski, 1995) also recovers linear pro-  jections from data. Unlike LSI, which finds principal components, ICA finds axes that are  statistically independent. ICA's biggest success is probably its application to the blind source  separation or cocktail party problem. In this problem, one observes the output of a number  of microphones. Each microphone is assumed to be recording a linear mixture of a number  of unknown sources. The task is to recover the original sources.  There is a natural embedding of text retrieval within this framework. The words that are  observed are like microphone signals, and underlying "topics" are the source signals that  give rise to them.  Figure 4 shows a typical distribution of words projected along axes found by ICA. 2 Most  words have a value close to zero. The histogram shows only the words large positive or  2 These results are from a collection containing transcripts of White House press releases from 1993.  There are 1585 documents and 18,675 distinct words.  708 C. L. Isbell, Jr. and P. Husbands  africa  apartheid  anc  transition  mandela  -1 -0.75 .O.8 .0.28  continent  elite  ethiopia  saharan  0,28 4.8 0.78  Values  Figure 4: Distribution of words with large magnitude an ICA axis from White House text.  negative values. One group of words is made up of highly-related terms; namely, "africa,"  "apartheid," and "mandela." The other group of words are not directly related, but each co-  occurs with different individual words in the first group. For example, "saharan" and "africa"  occur together many times, but not in the context of apartheid and South Africa; rather, in  documents concerning US policy toward Africa in general. As it so happens, "saharan" acts  as a discriminating word for these subtopics.  As observed in (Isbell and Viola, 1998), it appears that ICA is finding a set of words, $, that  selects for related documents, H, along with another set of words, T, whose elements do  not select for H, but co-occur with elements of $. Intuitively, S selects for documents in a  general subject area, and T removes a specific subset of those documents, leaving a small set  of highly related documents. This suggests a straightforward algorithm to achieve the same  goal directly. This local clustering approach is similar to an unsupervised version of Rocchio  with Query Zoning (Singhal, 1997).  Further analysis of ICA on similar collections reveals other interesting behavior on large  datasets. For example, it is known that ICA will attempt to find an unmixing matrix that is  full rank. This is in conflict with the notion that these collections actually reside in a much  smaller subspace. We have found in our experiments with ICA that some axes are highly  kurtotic while others produce gaussian-like distributions. We conjecture that any axis that  results in a gaussian-like distribution will be split arbitrarily among all "empty" axes. For  all intents and purposes, these axes are uninformative. This provides an automatic noise-  reduction technique for ICA when applied to large datasets.  For the purposes of comparison, Figure 5 illustrates the performance of several algorithms  (including ICA and various clustering techniques) on articles from the Wall Street Journal. 3  5 Discussion  We have shown that MATLAB*P enables portable, high-performance interactive supercom-  puting using the Parallel Problems Server, a powerful mechanism for writing and accessing  optimized algorithms. Further, the client communication protocol makes it possible to im-  plement transparent integration with sufficiently powerful clients, such as Matlab 5.  With such a tool, researchers can now use Matlab as something more than just a way for  prototyping algorithms and working on small problems. MATLAB*P makes it possible to  interactively operate on and visualize large data sets. We have demonstrated this last claim by  using the PPServer system to apply ML techniques to large datasets, allowing for analyses of  both data and algorithms. MATLAB*P has also been used to implement versions of Diverse  Density(Maron, 1998), MIMIC(DeBonet, Isbell and Viola, 1996), and gradient descent.  3The WSJ collection contains 42,652 documents and 89,757 words  Large Scale Machine Learning Using The Parallel Problems Server 709  Figure 5: A comparison of different algorithms on the Wall Street Journal  References  Bell, A. and Sejnowski, T. (1995). An information-maximizaton approach to blind source separation  and blind deconvolution. Neural Computation, 7:1129-1159.  Blackfor d, L. S., Choi, J., Cleary, A., D'Azevedo, E., Demmel, J., Dhilon, I., Dongarra, J., Hammar-  ling, S., Henry, G., Petitet, A., Stanley, K., Walker, D., and Whaley, R. (1997). ScaLAPACK  Users' Guide. http://www. netlib.org/scalapack/slug/scalapack_slug.html.  DeBonet, J., Isbell, C., and Viola, P. (1996). Mimic: Finding optima by estimating probability densities.  In Advances in Neural Information Processing Systems.  Deerwester, S., Dumais, S. T., Landauer, T. K., Furnas, G. W., and Harshman, R. A. (1990). Indexing  by latent semantic analysis. Journal of the Society for Information Science, 41(6):391-407.  Frakes, W. B. and Baeza-Yates, R., editors (1992). Information Retrieval: Data Structures and Algo-  rithms. Prentice-Hall.  Gropp, W., Lusk, E., and Skjellum, A. (1994). Using MPI: Portable Parallel Programming with the  Message-Passing Interface. The MIT Press.  Husbands, P. and Isbell, C. (1999). MITMatlab: A tool for interactive supercomputing. In Proceedings  of the Ninth SIAM Conference on Parallel Processing for Scientific Computing.  Isbell, C. and Viola, P. (1998). Restructuring sparse high dimensional data for effective retrieval. In  Advances in Neural Information Processing Systems.  Kwok, K. L. (1996). A new method of weighting query terms for ad-hoc retrieval. In Proceedings of  the 19th ACM/SIGIR Conference, pages 187-195.  Maron, O. (1998). A framework for multiple-instance learning. In Advances in Neural Information  Processing Systems.  Maschhoff, K. J. and Sorensen, D.C. (1996). A Portable Implementation of ARPACK for Distributed  Memory Parallel Computers. In Preliminary Proceedings of the Copper Mountain Conference  on Iterative Methods.  O'Brien, G. W. (1994). Information management tools for updating an svd-encoded indexing scheme.  Technical Report UT-CS-94-259, University of Tennessee.  PETSc. The Portable, Extensible Toolkit for Scientific Computation.  http://www. mcs.anl.gov/home/group/petsc.html.  PPServer. The Parallel Problems Server Web Page. http://www. ai.mit.edu/projects/ppserver.  Sahami, M., Hearst, M., and Saund, E. (1996). Applying the multiple cause mixture model to text  categorization. In Proceedings of the 13th Internationai Machine Learning Conference.  Singhal, A. (1997). Learning muting queries in a query zone. In Proceedings of the 20th International  Conference on Research and Development in Information Retrieval.  
Bayesian Map Learning in Dynamic  Environments  Kevin P. Murphy  Computer Science Division  University of California  Berkeley, CA 94720-1776  murphyk @cs. berkeley. edu  Abstract  We consider the problem of learning a grid-based map using a robot  with noisy sensors and actuators. We compare two approaches:  online EM, where the map is treated as a fixed parameter, and  Bayesian inference, where the map is a (matrix-valued) random  variable. We show that even on a very simple example, online EM  can get stuck in local minima, which causes the robot to get "lost"  and the resulting map to be useless. By contrast, the Bayesian  approach, by maintaining multiple hypotheses, is much more ro-  bust. We then introduce a method for approximating the Bayesian  solution, called Rao-Blackwellised particle filtering. We show that  this approximation, when coupled with an active learning strategy,  is fast but accurate.  I Introduction  The problem of getting mobile robots to autonomously learn maps of their envi-  ronment has been widely studied (see e.g., [9] for a collection of recent papers).  The basic difficulty is that the robot must know exactly where it is (a problem  called localization), so that it can update the right part of the map. However, to  know where it is, the robot must already have a map: relying on dead-reckoning  alone (i.e., integrating the motor commands) is unreliable because of noise in the  actuators (slippage and drift).  One obvious solution is to use EM, where we alternate between estimating the  location given the map (the E step), and estimating the map given the location  (the M step). Indeed, this approach has been successfully used by several groups  [8, 11, 12]. However, in all of these works, the trajectory of the robot was specified  by hand, and the map was learned off-line. For fully autonomous operation, and to  cope with dynamic environments, the map must be learned online.  We consider two approaches to online learning: online EM, and Bayesian inference,  1016 K. P. Murphy  a b c  Figure 1: (a) The POMDP represented as a graphical model. Lt is the location,  Me(i) is the label of the i'th grid cell, At is the action, and Zt is the observation.  Dotted circles denote variables that EM treats as parameters. (b) A one-dimensional  grid with binary labels (white = 0, black = 1). (c) A two-dimensional grid, with  four labels (closed doors, open doors, walls, and free space).  where we treat the map as a random variable. In Section 3, we show that the  Bayesian approach can lead to much better results than online EM; unfortunately,  it is computationally intractable, so in Section 4, we discuss an approximation based  on Rao-Blackwellised particle filtering.  2 The model  We now precisely define the model that we will use in this paper; it is similar to, but  much simpler than, the occupancy grid model in [12]. The map is defined to be a  grid, where each cell has a label which represents what the robot would see at that  point. More formally, the map at time t is a vector of discrete random variables,  Mr(i)  {1,...,No), where I _< i < NL. Of course, the map is not observed  directly, and nor is the robot's location, Lt  {1,..., NL). What is observed is  Zt  {1,...,No), the label of the cell at the robot's current location, and At   {1,...,NA), the action chosen by the robot just before time t. The conditional  independence assumptions we are making are illustrated in Figure l(a). We start  by considering the very simple one-dimensional grid shown in Figure l(b), where  there are just two actions, move right (-+) and move left (--), and just two labels,  off (0) and on (1). This is sufficiently small that we can perform exact Bayesian  inference. Later, we will generalize to two dimensions.  The prior for the location is a delta function with all its mass on the first (left-most)  cell, independent of A1. The transition model for the location is as follows.  Pr(Lt - j]Lt-1 - i, At --Y) -- 1 - Pa  1  0  ifj=i+l,j(N  if j =i,j  ifj=i-  otherwise  where Pa is the probability of a successful action, i.e., 1 - Pa is the probability that  the robot's wheels slip. There is an analogous equation for the case when At -{--.  Note that it is not possible to pass through the "rightmost" cell; the robot can use  this information to help localize itself.  The prior for the map is a product of the priors for each cell, which are uniform.  (We could model correlation between neighboring cells using a Markov Random  Field, although this is computationally expensive.) The transition mot[el for the  map is a product of the transition models for each cell, which are defined as follows:  Bayesian Map Learning in Dynamic Environments 1 O17  the probability that a 0 becomes a 1 or vice versa is Pc (probability of change), and  hence the probability that the cell label remains the same is 1 -  Finally, the observation model is  Pr(Zt = k[Mt = (ml,... ,mNL),Lt -- i) = { Po if mi = k  I - po otherwise  where Po is the probability of a succesful observation, i.e., 1 - Po is the probability  of a classification error. Another way of writing this, that will be useful later, is to  introduce the dummy deterministic variable, Z, which has the following distribu-  tion: Pr(Z = klMt = (ml,... ,mNL),Lt = i) = 5(k, mi), where 5(a,b) - 1 if a = b  and is 0 otherwise. Thus Z acts just like a multiplexer, selecting out a component  of Mt as determined by the "gate" Lt. The output of the multiplexer is then passed  through a noisy channel, which flips bits with probability 1 -po, to produce Zt.  3 Bayesian learning compared to EM  For simplicity, we assume that the parameters po, Pa and Pc, are all known. (In this  section, we use Po = 0.9, Pa ---- 0.8 and Pc = 0, so the world is somewhat "slippery",  but static in appearance.) The state estimation problem is to compute the belief  state Pr(Lt, Mtlyl:t), where Yt = (Zt, At) is the evidence at time t; this is equiv-  alent to performing online inference in the graphical model shown in Figure l(a).  Unfortunately, even though we have assumed that the components of Mt are a pri-  ori independent, they become correlated by virtue of sharing a common child, Zt.  That is, since the true location of the robot is unknown, all of the cells are possible  causes of the observation, and they "compete" to "explain" the data. Hence all of  the hidden variables become coupled, and the belief state has size O(NL2r).  If the world is static (i.e., Pc = 0), we can treat M as a fixed, but unknown,  parameter; this can then be combined with the noisy sensor model to define an  HMM with the following observation matrix:  B(i,k) deaf Pr(Zt = kilt - i;M) =  Pr(Zt = k[Z = j)5(M(i),j)  We can then learn B using EM, as in [8, 11, 12]. (We assume for now that the HMM  transition matrix is independent of the map, and encodes the known topology of  the grid, i.e., the robot can move to any neighboring cell, no matter what its label  is. We will lift this restriction in the 2D example.  We can formulate an online version of EM as follows. We use fixed-lag smoothing  with a sliding window of length W, and compute the expected sufficient statis-  tics (ESS) for the observation matrix within this window as follows: Or(i, k) =  t  '-r=t-W:Z,=k Lrlt(i)' where Llt(i) = Pr(Lr = ilYx:t). We can compute L using  the forwards-backwards algorithm, using Lt_W_Xlt_ 1 as the prior. (The initial con-  dition is  = r, where r is the (known) prior for L0.) Thus the cost per time step is  O(2WN). In the M step, we normalize each row of Ot + d x Or-x, where 0 < d < 1  is a decay constant, to get the new estimate of B. We need to downweight the  previous ESS since they were computed using out-of-date parameters; in addition,  exponential forgetting allows us to handle dynamic environments. [1] discuss some  variations on this algorithm.  1018 K. P. Murphy  b c d  Figure 2: (a) The full joint posterior on P(Mt[Yl:t). 0 and 255, on the axis into the  page, represent the maps where every cell is off and every cell is on, respectively; the  mode at t - 16 is for map 171, which corresponds to the correct pattern 01010101.  (b-d) Estimated map. Light cells are more likely to contains Os, so the correct  pattern should have light bars in the odd rows. (b) The marginals of the exact  joint. (c) Online EM. (d) Offline EM.  As the window length increases, past locations are allowed to look at more and  more future data, and hence their estimates become more accurate; however, the  space and time requirements increase. Nevertheless, there are occasions when even  the maximum window size (i.e., looking all the way back to - - 0) will perform  poorly, because of the greedy hill-climbing nature of EM. For a simple example of  this, consider the environment shown in Figure l(b). Suppose the robot starts in  cell 1, keeps going right until it comes to the end of the "corridor", and then heads  back "home". Suppose further that there is a single slippage error at t -- 4, so the  actual path and observation sequence of the robot is as follows:  t I 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  Lt I 2 3 4 4 5 6 7 8 7 6 5 4 3 2 1  Zt 0 I 0 I I 0 I 0 I 0 I 0 I 0 I 0  To study the effect of this sequence, we computed Pr(Mt, Ltlyl:t) by applying the  junction tree algorithm to the graphical model in Figure l(a). We then marginalized  out Lt to compute the posterior P(Mt): see Figure 2(a). At t = 1, there are 27  modes, corresponding to all possible bit patterns on the unobserved cells. At each  time step, the robot thinks it is moving one step to the right. Hence at t = 8, the  robot thinks it is in cell 8, and observes 0. When it tries to move right it knows  it will remain in cell 8 (since the robot knows where the boundaries are). Hence at  t = 9, it is almost 70% confident that it is in cell 8. At t = 9, it observes a 1, which  contradicts its previous observation of 0. There are two possible explanations: this  is a sensor error, or there was a motor error. Which of these is more likely depends  on the relative values of the sensor noise, po, and the system noise, Pa. In our  experiments, we found that the motor error hypothesis is much more likely; hence  the mode of the posterior jumps from the wrong map (in which M(5) = 1) to the  right map (in which M(5) = 0). Furthermore, as the robot returns to "familiar  territory", it is able to better localize itself (see Figure 3(a)), and continues to learn  the map even for far-away cells, because they are all correlated (in Figure 2(b), the  entry for cell 8 becomes sharper even as the robot returns to cell 1)  We now compare the Bayesian solution with EM. Online EM with no smoothing  was not able to learn the correct map. Adding smoothing with the maximum  window size of Wt -- t did not improve matters: it is still unable to escape the local  Bayesian Map Learning in Dynamic Environments 1019  a b c  l  Figure 3: Estimated location. Light cells are more likely to contain the robot.  (a) Optimal Bayes solution which marginalizes out the map. (b) Dead-reckoning  solution which ignores the map. Notice how "blurry" it is. (c) Online EM solution  using fixed-lag smoothing with a maximal window length.  minimum in which M(5) = 1, as shown in Figure 2(c). (We tried various values of  the decay rate d, from 0.1 to 0.9, and found that it made little difference.) With the  wrong map, the robot "gets lost" on the return journey: see Figure 3(c). Offline  EM, on the other hand, does very well, as shown in Figure 2(d); although the initial  estimate of location (see Figure 3(b)) is rather diffuse, as it updates the map it can  use the benefit of hindsight to figure out where it must have been.  4 Rao-Blackwellised particle filtering  Although the Bayesian solution exhibits some desirable properties, its running time  is exponential in the size of the environment. In this section, we discuss a sequential  Monte Carlo algorithm called particle filtering (also known as SIR filtering, the  bootstrap filter, the condensation algorithm, survival of the fittest, etc; see [10, 4]  for recent reviews). Particle filtering (PF) has already been successfully applied to  the problem of (global) robot localization [5]. However, in that case, the state space  was only of dimension 3: the unknowns were the position of the robot, (x, y) 6 ]R 2,  and its orientation, 0  [0, 2r]. In our case, the state space is discrete and of  dimension O(1 + NL), since we need to keep track of the map as well as the robot's  location (we ignore orientation in this paper).  Particle filtering can be very inefficient in high-dimensional spaces. The key obser-  vation which makes it tractable in this context is that, if Ll:t were known, then the  posterior on Mt would be factored; hence Mt can be marginalized out analytically,  and we only need to sample Lt. This idea is known in the statistics literature as Rao-  Blackwellisation [10, 4]. In more detail, we will approximate the posterior at time t  using a set of weighted particles, where each particle specifies a trajectory Ll:t, and  the corresponding conditionally factored representation of P(Mt) - rIi P(Mt(i));  we will denote the j'th particle at time t as b? ). Note that we do not need to actu-  ally store the complete trajectories Ll:t: we only need the most recent value of L.  The approach we take is essentially the same as the one used in the conditional lin-  ear Gaussian models of [4, 3], except we replace the Kalman filter update with one  which exploits the conditionally factored representation of P(Mt). In particular,  the algorithm is as follows: For each particle j = 1,..., _Ns, we do the following:  1. Sample r(J) from a proposal distribution, which we discuss below.  2. Update each component of the map separately using r.(J) and z,+t  +1  Pr(M,(_) r.(J) = i,b(J) z,+t) or Pr(z,+xlMt(})(i)) H Pr(M*(i )(k)lM?)(k))  't+l  '  1020 K. P. Murphy  a b c d  Figure 4: (a-b) Results using 50 particles. (c-d) Results using BK.  3. Update the weights:  (j) (J) (J) where  (j) is defined below.  ua/:+l -- /:-t-lW/: ,  We then resample Ns particles from the normalised weights, using Liu's residual   (5) = 1/Ns for all j. We consider two proposal  resampling algorithm [10], and set oat+ 1  distributions. The first is a simple one which just uses the transition model to  predict the new location: Pr(Lt+llb? ), at+i). In this case, the incremental weight  is 't+l oc P(zt+I[L,+i, . The optimal proposal distribution (the one which  minimizes the variance of the importance weights) takes the most recent evidence  into account, and can be shown to have the form Pr(L,+l[b?),a+l,z,+l) with  incremental weight ut+l oc P(z,+l lb?)). Computing this requires marginalizing out  M+i and L,+i, which can be done in O(NL) time (details omitted).  In Figure 4, we show the results of applying the above algorithm to the same problem  as in Section 3; it can be seen that it approximates the exact solution' very closely,  using only 50 particles. The results shown are for a particular random number seed;  other seeds produce qualitatively very similar results, indicating that 50 particles  is in fact sufficient in this case. Obviously, as we increase the number of particles,  the error and variance decrease, but the running time increases (linearly).  The question of how many particles to use is a difficult one: it depends both on  the noise parameters and the structure of the environment (if every cell has a  unique label, localization is easy). Since we are sampling trajectories, the number  of hypotheses, and hence the number of particles needed, grows exponentially with  time. In the above example, the robot was able to localize itself quite accurately  when it reached the end of the corridor, where most hypotheses "died off". In  general, the number of particles will depend on the length of the longest cycle in  the environment, so we will need to use active learning to ensure tractability.  In the dynamic two-dimensional grid world of Figure 1(c), we chose actions so as  to maximize expected discounted reward (using policy iteration), where the reward  for visiting cell i is  H(Lt)(1 - H(Mt(i))) + (1 - H(Lt))H(Mt(i))  where H(.) is the normalized entropy. Hence, if the robot is "lost", so H(Lt)  1,  the robot will try to visit a cell which it is certain about (see [6] for a better  approach); otherwise, it will try to explore uncertain cells. After learning the map,  the robot spends its time visiting each of the doors, to keep its knowledge of their  state (open or closed) up-to-date.  We now briefly consider some alternative approximate inference algorithms. Exam-  ining the graphical structure of our model (see Figure l(a)), we see that it is identical  Bayesian Map Learning in Dynamic Environments 1021  to a Factorial HMM [7] (ignoring the inputs). Unfortunately, we cannot use their  variational approximation, because they assume a conditional Gaussian observa-  tion model, whereas ours is almost deterministic. Another popular ap.proximate  inference algorithm for dynamic Bayes nets (DBNs) is the "BK algorithm" [2, 1].  This entails projecting the joint posterior at time t onto a product-of-marginals  representation  P(Lt, Mt(1),...,Mt(NL)IYl:t) = P(Ltly1:t) II P(Mt(i)lyl:t)  i  and using this as a factored prior for Bayesian updating at time t -]- 1. Given a  factored prior, we can compute a factored posterior in O(N) time by conditioning  on each Lt+, and then averaging. We found that the BK method does very poorly  on this problem (see Figure 4), because it ignores correlation between the cells. Of  course, it is possible to use pairwise or higher order marginals for tightly coupled  sets of variables. Unfortunately, the running time is exponential in the size of the  largest marginal, and in our case, all the Mr(i) variables are coupled.  Acknowledgments  I would like to thank Nando de Freitas for helping me get particle filtering to work,  Sebastian Thrun for an interesting discussion at the conference, and Stuart Russell for  encouraging me to compare to EM. This work was supported by grant number ONR  N00014-97-1-0941.  References  [1] X. Boyen and D. Koller. Approximate learning of dynamic models. In NIPS, 1998.  [2] X. Boyen and D. Koller. Tractable inference for complex stochastic processes. In  UAI, 1998.  [3] R. Chen and S. Liu. Mixture Kalman filters. Submitted, 1999.  [4] A. Doucet, S. Godsill, and C. Andrieu. On sequential Monte Carlo sampling methods  for Bayesian filtering. Statistics and Computing, 1999.  [5] D. Fox, W. Burgard, F. Dellaeft, and S. Thrun. Monte carlo localization: Efficient  position estimation for mobile robots. In AAAI, 1999.  [6] D. Fox, W. Burgard, and S. Thrun. Active Markov localization for mobile robots.  Robotics and Autonomous Systems, 1998.  [7] Z. Ghahramani and M. Jordan. Factorial Hidden Markov Models. Machine Learning,  29:245-273, 1997.  [8] S. Koenig and R. Simmons. Unsupervised learning of probabilistic models for robot  navigation. In ICRA, 1996.  [9] D. Kortenkamp, R. Bonasso, and R. Murphy, editors. Artificial Intelligence and  Mobile Robots: case studies of successful robot systems. MIT Press, 1998.  [10] J. Liu and R. Chen. Sequential monte carlo methods for dynamic systems. JASA,  93:1032-1044, 1998.  [11] H. Shatkay and L. P. Kaelbling. Learning topological maps with weak local odometric  information. In IJCAI, 1997.  [12] S. Thrun, W. Burgard, and D. Fox. A probabilistic approach to concurrent mapping  and localization for mobile robots. Machine Learning, 31:29-53, 1998.  
A Variational Bayesian Framework for  Graphical Models  Hagai Attias  hagai@gatsby. ucl.ac.uk  Gatsby Unit, University College London  17 Queen Square  London WCIN 3AR, U.K.  Abstract  This paper presents a novel practical framework for Bayesian model  averaging and model selection in probabilistic graphical models.  Our approach approximates full posterior distributions over model  parameters and structures, as well as latent variables, in an analyt-  ical manner. These posteriors fall out of a free-form optimization  procedure, which naturally incorporates conjugate priors. Unlike  in large sample approximations, the posteriors are generally non-  Gaussian and no Hessian needs to be computed. Predictive quanti-  ties are obtained analytically. The resulting algorithm generalizes  the standard Expectation Maximization algorithm, and its conver-  gence is guaranteed. We demonstrate that this approach can be  applied to a large class of models in several domains, including  mixture models and source separation.  I Introduction  A standard method to learn a graphical model 1 from data is maximum likelihood  (ML). Given a training dataset, ML estimates a single optimal value for the model  parameters within a fixed graph structure. However, NIL is well known for its ten-  dency to overfit the data. Overfitting becomes more severe for complex models  involving high-dimensional real-world data such as images, speech, and text. An-  other problem is that NIL prefers complex models, since they have more parameters  and fit the data better. Hence, NIL cannot optimize model structure.  The Bayesian framework provides, in principle, a solution to these problems. Rather  than focusing on a single model, a Bayesian considers a whole (finite or infinite) class  of models. For each model, its posterior probability given the dataset is computed.  Predictions for test data are made by averaging the predictions of all the individ-  ual models, weighted by their posteriors. Thus, the Bayesian framework avoids  overfitting by integrating out the parameters. In addition, complex models are  automatically penalized by being assigned a lower posterior probability, therefore  optimal structures can be identified.  Unfortunately, computations in the Bayesian framework are intractable even for  We use the term 'model' to refer collectively to parameters and structure.  210 H. Attias  very simple cases (e.g. factor analysis; see [2]). Most existing approximation meth-  ods fall into two classes [3]: Markov chain Monte Carlo methods and large sample  methods (e.g., Laplace approximation). MCMC methods attempt to achieve exact  results but typically require vast computational resources, and become impractical  for complex models in high data dimensions. Large sample methods are tractable,  but typically make a drastic approximation by modeling the 'posteriors over all  parameters as Normal, even for parameters that are not positive definite (e.g., co-  variance matrices). In addition, they require the computation of the Hessian, which  may become quite intensive.  In this paper I present Variational Bayes (VB), a practical framework for Bayesian  computations in graphical models. VB draws together variational ideas from in-  tractable latent variables models [8] and from Bayesian inference [4,5,9], which, in  turn, draw on the work of [6]. This framework facilitates analytical calculations of  posterior distributions over the hidden variables, parameters and structures. The  posteriors fall out of a free-form optimization procedure which naturally incorpo-  rates conjugate priors, and emerge in standard forms, only one of which is Normal.  They are computed via an iterative algorithm that is closely related to Expectation  Maximization (EM) and whose convergence is guaranteed. No Hessian needs to  be computed. In addition, averaging over models to compute predictive quantities  can be performed analytically. Model selection is done using the posterior over  structure; in particular, the BIC/MDL criteria emerge as a limiting case.  2 General Framework  We restrict our attention in this paper to directed acyclic graphs (DAGs, a.k.a.  Bayesian networks). Let Y = {Yl, ..., yv) denote the visible (data) nodes, where  n - 1,...,N runs over the data instances, and let X = {xx,...,xv) denote the  hidden nodes. Let O denote the parameters, which are simply additional hidden  nodes with their own distributions. A model with a fixed structure m is fully defined  by the joint distribution p(Y, X, 0 Im). In a DAG, this joint factorizes over the  nodes, i.e. p(Y,X I O,m) = IiP(Ui I pai,Oi,m), where ui  Y U X, Pai is the set  of parents of ui, and Oi  0 parametrize the edges directed toward ui. In addition,  we usually assume independent instances, p(Y, X l O, m ) = InP(yn,xn I O, m).  We shall also consider a set of structures m & M, where m controls the number  of hidden nodes and the functional forms of the dependencies p(ui I Pai,0i,m),  including the range of values assumed by each node (e.g., the number of components  in a mixture model). Associated with the set of structures is a structure prior p(m).  Marginal likelihood and posterior over parameters. For a fixed structure m,  we are interested in two quantities. The first is the parameter posterior distribution  p(O I Y, m). The second is the marginal likelihood p(Y Im), also known as the  evidence assigned to structure m by the data. In the following, the reference to m is  usually omitted but is always implied. Both quantities are obtained from the joint  p(Y, X, 0 [m). For models with no hidden nodes the required computations can  often be performed analytically. However, in the presence of hidden nodes, these  quantities become computationally intractable. We shall approximate them using  a variational approach as follows.  Consider the joint posterior p(X, 0 [ Y) over hidden nodes and parameters. Since  it is intractable, consider a variational posterior q(X, 0 [ Y), which is restricted to  the factorized form  q(X, O[ Y) = q(XlY)q(O IF), (1)  where given the data, the parameters and hidden nodes are independent. This  A Variational BaysJan Framework for Graphical Models 211  restriction is the key: It makes q approximate but tractable. Notice that we do  not require complete factorization, as the parameters and hidden nodes may still  be correlated amongst themselves.  We compute q by optimizing a cost function rm[q] defined by  .T'm[q] = / dO q(X)q(O) log  p(Y,X, O)  q(X)q(O)  _ logp(Y I m) ,  (2)  Penalizing complex models. To see that the VB objective function r, penalizes  complexity, it is useful to rewrite it as  .T', = {logP(Y'X l O))x,o - KL[q(O) II p(O)]  q(X)  where the average in the first term on the r.h.s. is taken w.r.t.  term corresponds to the (averaged) likelihood. The second term is the KL distance  between the prior and posterior over the parameters. As the number of parameters  increases, the KL distance follows and consequently reduces r,.  This penalized likelihood interpretation becomes transparent in the large sample  limit N - oe, where the parameter posterior is sharply peaked about the most  probable value O = O0. It can then be shown that the KL penalty reduces to  (I 0 I/2) log N, which is linear in the number of parameters I 0 I of structure m.  rm then corresponds precisely the Bayesian information criterion (BIC) and the  minimum description length criterion (MDL) (see [3]). Thus, these popular model  selection criteria follow as a limiting case of the VB framework.  Free-form optimization and an EM-like algorithm. Rather than assuming a  specific parametric form for the posteriors, we let them fall out of free-form opti-  mization of the VB objective function. This results in an iterative algorithm directly  analogous to ordinary EM. In the E-step, we compute the posterior over the hidden  nodes by solving 5.T'm/Sq(X ) = 0 to get  q(X) or (1ogp(Y, XIO))o , (4)  where the average is taken w.r.t. q(O).  In the M-step, rather than the 'optimal' parameters, we compute the posterior  distribution over the parameters by solving 6m/6q(O) - 0 to get  q(O) or e(lgp(Y'Xl))Xp(O) ,  (5)  where the average is taken w.r.t. q(X).  This is where the concept of conjugate priors becomes useful. Denoting the expo-  nential term on the r.h.s. of (5) by f(O), we choose the prior p(O) from a family of  distributions such that q(O) c f (O)p(O) belongs to that same family. p(O) is then  said to be conjugate to f(O). This procedure allows us to select a prior from a fairly  large family of distributions (which includes non-informative ones as limiting cases)  (3)  q(X, 0). The first  where the inequality holds for an arbitrary q and follows from Jensen's inequality  (see [6]); it becomes an equality when q is the true posterior. Note that q is always  understood to include conditioning on Y as in (1). Since r is bounded from above  by the marginal likelihood, we can obtain the optimal posteriors by maximizing it  w.r.t.q. This can be shown to be equivalent to minimizing the KL distance between  q and the true posterior. Thus, optimizing m produces the best approximation to  the true posterior within the space of distributions satisfying (1), as well as the  tightest lower bound on the true marginal likelihood.  212 H. Attias  and thus not compromise generality, while facilitating mathematical simplicity and  elegance. In particular, learning in the VB framework simply amounts to updat-  ing the hyperparameters, i.e., transforming the prior parameters to the posterior  parameters. We point out that, while the use of conjugate priors is widespread in  statistics, so far they could only be applied to models where all nodes were visible.  Structure posterior. To compute q(m) we exploit Jensen's inequal-  ity once again to define a more general objective function, r[q] =  meMq(m)['m +1ogp(m)/q(m)] _< logp(Y), where now q = q(X [ m,Y)q(O I  m, Y)q(m I Y). After computing r, for each m  M, the structure posterior is  obtained by free-form optimization of r:  q(m) cr e"p(m) . (6)  Hence, prior assumptions about the likelihood of different structures, encoded by  the prior p(m), affect the selection of optimal model structures performed according  to q(m), as they should.  Predictive quantities. The ultimate goal of Bayesian inference is to estimate  predictive quantities, such as a density or regression function. Generally, these  quantities are computed by averaging over all models, weighting each model by  its posterior. In the VB framework, exact model averaging is approximated by  replacing the true posterior p(O I Y) by the variational q(O I Y). In density  estimation, for example, the density assigned to a new data point y is given by  p(ylY) = fdO p(y[ O) q(OlY).  In some situations (e.g. source separation), an estimate of hidden node values x  from new data y may be required. The relevant quantity here is the conditional  p(x I y, Y), from which the most likely value of hidden nodes is extracted. VB  approximates it by p(x [y,Y) cr fdO p(y,x I ) q(OIY ).  3 Variational Bayes Mixture Models  Mixture models have been investigated and analyzed extensively over many years.  However, the well known problems of regularizing against likelihood divergences and  of determining the required number of mixture components are still open. Whereas  in theory the Bayesian approach provides a solution, no satisfactory practical al-  gorithm has emerged from the application of involved sampling techniques (e.g.,  [7]) and approximation methods [3] to this problem. We now present the solution  provided by VB.  We consider models of the form  m  P(Yn I O, m) = -P(Yn I Sn = S, O)p(Sn = S  $--1  (7)  where y, denotes the nth observed data vector, and s, denotes the hidden compo-  nent that generated it. The components are labeled by s = 1, ..., m, with the struc-  ture parameter m denoting the number of components. Whereas our approach can  be applied to arbitrary models, for simplicity we consider here Normal component  distributions, P(Yn I Sn = S, O) = JV'(ls, rs), where/s is the mean and rs the pre-  cision (inverse covariance) matrix. The mixing proportions are p(s = s [ O) = rs.  In hindsight, we use conjugate priors on the parameters 13 = {rs, t s, rs}. The  mixing proportions are jointly Dirichlet, p({rs}) = D(A), the means (conditioned  on the precisions) are Normal, p(l s I rs) = and the precisions are  Wishart, p(rs) = W(v , ,I,0). We find that the parameter posterior for a fixed m  A Variational Baysian Framework for Graphical Models 213  factorizes into q(O) = q({'s})I-is q(8, rs). The posteriors are obtained by the  following iterative algorithm, termed VB-MOG.  E-step. Compute the responsibilities for instance n using (4):  7 ---- q(Sn -- S [ Yn) 0C {s 1/2 e-(y.-p,)Tf',(y.-p,)/2 e-d/2B (8)  --8 '  noting that here X = S and q(S) = rI, q(s,). This expression resembles the re-  sponsibilities in ordinary ML; the differences stem from integrating out the param-  eters. The special quantities in (8) are logs -- (logrs) = P(As)- P(Y]8' As,),  log's = (log [ r8 }) a  -- Ei=i )((b's 2r' 1- i)/2)- log I 'I'8 I +dlog2, and  f'8 -- {r,) = u8,I,2 , where p(x) = dlogF(x)/dx is the digaroma function, and  the averages {-} are taken w.r.t. q(O). The other parameters are described below.  M-step. Compute the parameter posterior in two stages. First, compute the  quantities  N N N  1 n 1  Es = 7 Cs ,  =- 78 y., v8 (0)  28 N 78 , 8 = 5r 8 ,--  where C = (y, - 8)(Y- - 8) T and 8 = NSs. This stage is identical to the  M-step in ordinary EM where it produces the new parameters. In VB, however, the  quantities in (9) only help characterize the new parameter posteriors. These posteri-  ors are functionally identical to the priors but have different parameter values. The  mixing proportions are jointly Dirichlet, q({rs}) = D({8}), the means are Normal,  q(tq IFs) - A/'(ps,/8F8), and the precisions are Wishart, p(Fs) = W(v8, 'bs). The  posterior parameters are updated in the second stage, using the simple rules  a8 = V8 + ao, p = ( + opo)/( +o),  = & +o, (0)  . =  +.o,  = % + o( _ po)(& _ po)r/( + o) + o.  The final values of the posterior parameters form the output of the VB-MOG. We  remark that (a) Whereas no specific assumptions have been made about them, the  parameter posteriors emerge in suitable, non-trivial (and generally non-Normal)  functional forms. (b) The computational overhead of the VB-MOG compared to  EM is minimal. (c) The coverlance of the parameter posterior is O(1/N), and VB-  MOG reduces to EM (regularized by the priors) as N -4 c. (d) VB-MOG has no  divergence problems. (e) Stability is guaranteed by the existence of an objective  function. (f) Finally, the approximate marginal likelihood rm, required to optimize  the number of components via (6), can also be obtained in closed form (omitted).  Predictive Density. Using our posteriors, we can integrate out the parameters  and show that the density assigned by the model to a new data vector y is a mixture  of Student-t distributions,  m  p(y I Y) = Est,(ylps,As), (11)  where component s has cos = 8 + i - d d.o.f., mean P8, covariance As = ((8 +  1)//8co8)I'8, and proportion 8 - As/Y]8' A,,. (11) reduces to  Nonlinear Regression. We may divide each data vector into input and out-  put parts, y - (yi, yO), and use the model to estimate the regression function  o = f(yi) and error spheres. These may be extracted from the conditional  p(y l y*,Y ) = E m ! !   8= w, t,,(y  [ ps, A8), whmh also turns out to be a mixture  of Student-t distributions, with means P'8 being linear, and covariances A[ and mix-  ing proportions w8 nonlinear, in y*, and given in terms of the posterior parameters.  214 H. Attias  Buffalo post office digits  Misclassification rate histogram  1  0.8  0.6  0.4  0.2  0  0  0.05  0.1  Figure 1: VB-MOG applied to handwritten digit recognition.  VB-MOG was applied to the Boston housing dataset (UCI machine learning repos-  itory), where 13 inputs are used to predict the single output, a house's price. 100  random divisions of the N = 506 dataset into 481 training and 25 test points were  used, resulting in an average MSE of 11.9. Whereas ours is not a discriminative  method, it was nevertheless competitive with Breiman's (1994) bagging technique  using regression trees (MSE=11.7). For comparison, EM achieved MSE=14.6.  Classification. Here, a separate parameter posterior is computed for each class c  from a training dataset yc. Test data vector y is then classified according to the  conditional p(c ] y, {yc)), which has a form identical to (11) (with c-dependent  parameters) multiplied by the relative size of yc.  VB-MOG was applied to the Buffalo post office dataset, which contains 1100 exam-  ples for each digit 0 - 9. Each digit is a gray-level 8 x 8 pixel array (see examples  in Fig. 1 (left)). We used 10 random 500-digit batches for training, and a separate  batch of 200 for testing. An average misclassification rate of .018 was obtained  using m = 30 components; EM achieved .025. The misclassification histograms  (VB-solid, EM-dashed) are shown in Fig. 1 (right).  4 VB and Intractable Models: a Blind Separation Example  The discussion so far assumed that a free-form optimization of the VB objective  function is feasible. Unfortunately, for many interesting models, in particular mod-  els where ordinary ML is intractable, this is not the case. For such models, we  modify the VB procedure as follows: (a) Specify a parametric functional form for  the posterior over the hidden nodes q(X), and optimize w.r.t. its parameters, in the  spirit of [8]. (b) Let the parameter posterior q()) fall out of free-form optimization,  as before.  We illustrate this approach in the context of the blind source separation (BSS)  problem (see, e.g., [1]). This problem is described by Yn - Hxn + Un, where Xn is  an unobserved m-dim source vector at instance n, H is an unknown mixing matrix,  and the noise un is Normally distributed with an unknown precision hi. The task is  to construct a source estimate in from the observed d-dim data y. The sources are  independent and non-Normally distributed. Here we assume the high-kurtosis dis-  tribution p(x?) oc cosh-2(x/2), which is appropriate for modeling speech sources.  One important but heretofore unresolved problem in BSS is determining the num-  ber rn of sources from data. Another is to avoid overfitting the mixing matrix. Both  problems, typical to ML algorithms, can be remedied using VB.  It is the non-Normal nature of the sources that renders the source posterior p(X [ Y)  intractable even before a Bayesian treatment. We use a Normal variational posterior  q(X) - IInA/'(Xn I pn,rn) with instance-dependent mean and precision. The  mixing matrix posterior q(H) then emerges as Normal. For simplicity, ,k is optimized  rather than integrated out. The resulting VB-BSS algorithm runs as follows:  4 Variational BaysJan Framework for Graphical Models 215  o  -lOOO  -2000  -3000  -4000  log Pr(m)  source reconstruCtion error  -1  -15  -20  2 4 6 8 10 12 0 5 10  m SNR(dB)  Figure 2: Application of VB to blind source separation algorithm (see text).  E-step. Optimize the variational mean Pn by iterating to convergence, for each n,  the fixed-point equation AfIT(yn --fIPn ) --tanhPn/2 -- C-Pn, where C is the  source covariance conditioned on the data. The variational precision matrix turns  out to be n-independent: I', = ,TA. q- I/2 q- C -1.  M-step. Update the mean and precision of the posterior q(H) (rules omitted).  This algorithm was applied to 11-dim data generated by linearly mixing 5 100msec-  long speech and music signals obtained from commercial CDs. Gaussian noise were  added at different SNR levels. A uniform structure prior p(m) = 1/K for m _ K  was used. The resulting posterior over the number of sources (Fig. 2 (left)) is  peaked at the correct value m -- 5. The sources were then reconstructed from test  data via p(x I Y, Y). The log reconstruction error is plotted vs. SNR in Fig. 2  (right, solid). The ML error (which includes no model averaging) is also shown  (dashed) and is larger, reflecting overfitting.  5 Conclusion  The VB framework is applicable to a large class of graphical models. In fact, it may  be integrated with the junction tree algorithm to produce general inference engines  with minimal overhead compared to ML ones. Dirichlet, Normal and Wishart  posteriors are not special to models treated here but emerge as a general feature.  Current research efforts include applications to multinomial models and to learning  the structure of complex dynamic probabilistic networks.  Acknowledgements  I thank Matt Beal, Peter Dayan, David Mackay, Carl Rasmussen, and especially Zoubin  Ghahramani, for important discussions.  References  [1] Attias, H. (1999). Independent Factor Analysis. Neural Computation 11, 803-851.  [2] Bishop, C.M. (1999). Variational Principal Component Analysis. Proc. 9th ICANN.  [3] Chickering, D.M.  Heckerman, D. (1997). Efficient approximations for the marginal  likelihood of Bayesian networks with hidden variables. Machine Learning 29, 181-212.  [4] Hinton, G.E. & Van Camp, D. (1993). Keeping neural networks simple by minimizing  the description length of the weights. Proc. 6th COLT, 5-13.  [5] Jaakkola, T. & Jordan, M.I. (1997). Bayesian logistic regression: A variational ap-  proach. Statistics and Artificial Intelligence 6 (Smyth, P. & Madigan, D., Eds).  [6] Neal, R.M. & Hinton, G.E. (1998). A view of the EM algorithm that justifies incre-  mental, sparse, and other variants. Learning in Graphical Models, 355-368 (Jordan, M.I.,  Ed). Kluwer Academic Press, Norwell, MA.  [7] Richardson, S. & Green, P.J. (1997). On Bayesian analysis of mixtures with an un-  known number of components. Journal of the Royal Statistical Society B, 59, 731-792.  [8] Saul, L.K., Jaakkola, T., & Jordan, M.I. (1996). Mean field theory of sigmoid belief  networks. Journal of Artificial Intelligence Research 4, 61-76.  [9] Waterhouse, S., Mackay, D., & Robinson, T. (1996). Bayesian methods for mixture of  experts. NIPS-8 (Touretzky, D.S. et al, Eds). MIT Press.  
Algorithms for Independent Components  Analysis and Higher Order Statistics  Daniel D. Lee  Bell Laboratories  Lucent Technologies  Murray Hill, NJ 07974  Uri Rokni and Haim Sompolinsky  Racah Institute of Physics and  Center for Neural Computation  Hebrew University  Jerusalem, 91904, Israel  Abstract  A latent variable generative model with finite noise is used to de-  scribe several different algorithms for Independent Components Anal-  ysis (ICA). In particular, the Fixed Point ICA algorithm is shown to  be equivalent to the Expectation-Maximization algorithm for maximum  likelihood under certain constraints, allowing the conditions for global  convergence to be elucidated. The algorithms can also be explained by  their generic behavior near a singular point where the size of the opti-  mal generarive bases vanishes. An expansion of the likelihood about this  singular point indicates the role of higher order correlations in determin-  ing the features discovered by ICA. The application and convergence of  these algorithms are demonstrated on a simple illustrative example.  Introduction  Independent Components Analysis (ICA) has generated much recent theoretical and prac-  tical interest because of its successes on a number of different signal processing problems.  ICA attempts to decompose the observed data into components that are as statistically in-  dependent from each other as possible, and can be viewed as a nonlinear generalization of  Principal Components Analysis (PCA). Some applications of ICA include blind separation  of audio signals, beamforming of radio sources, and discovery of features in biomedical  traces [ 1 ].  There have also been a number of approaches to deriving algorithms for ICA [2, 3, 4].  Fundamentally, they all consider the problem of recovering independent source signals { '}  from observations {} such that:  M  Xi : E Wijsj' i = 1..N (1)  j:l  Here, Wij is a N x M mixing matrix where the number of sources M is not greater than  the dimcnsionality N of the observations. Thus, the columns of W represent the different  independent features present in the observed data.  Bell and Scjnowski formulated their Infomax algorithm for ICA as maximizing the mutual  information between the data and a nonlinearly transformed version of the data [5]. The  492 D. D. Lee, U. Rokni and H. Sornpolinsky  covariant version of this algorithm uses the natural gradient of the mutual information to  iteratively update the estimate for the demixing matrix W - in terms of the estimated  components s = W- ix [6]:  -1 w -1, (2)  The nonlinearity g(s) differentiates the features learned by the Infomax ICA algorithm  from those found by conventional PCA. Fortunately, the exact form of the nonlinearity  used in Eq. 2 is not crucial for the success of the algorithm, as long as it preserves the  sub-Gaussian or super-Gaussian nature of the sources [7].  Another approach to ICA due to Hyvarinen and Oja was derived from maximizing objective  functions motivated by projection pursuit [8]. Their Fixed Point ICA algorithm attempts  to self-consistently solve for the extremum of a nonlinear objective function. The simplest  formulation considers a single source M = 1 so that the mixing matrix is a single vector  w, constrained to be unit length Iw[ - 1. Assuming the data is first preprocessed and  whitened, the Fixed Point ICA algorithm iteratively updates the estimate of to as follows:  w +-  w  w +- (3)  where g(wTx) is a nonlinear function and AG is a constant given by the integral over the  Gaussian:  I /_ --r?/2  AG =  drle gt(rl). (4)  The Fixed Point algorithm can be extended to an arbitrary number M <_ N of sources by  using Eq. 3 in a serial deflation scheme. Alternatively, the M columns of the mixing matrix  W can be updated simultaneously by orthogonalizing the N x M matrix:  W +- (zg(Wrz) r) - XoW.  (5)  Under the assumption that the observed data match the underlying ICA model, x = Ws, it  has been shown that the Fixed Point algorithm converges locally to the correct solution with  at least quadratic convergence. However, the global convergence of the generic Fixed Point  ICA algorithm is uncertain. This is in contrast to the gradient-based Infomax algorithm  whose convergence is guaranteed as long as a sufficiently small step size is chosen.  In this paper, we first review the latent variable generative model framework for Indepen-  dent Components Analysis. We then consider the generarive model in the presence of finite  noise, and show how the Fixed Point ICA algorithm can be related to an Expectation-  Maximization algorithm for maximum likelihood. This allows us to elucidate the condi-  tions under which the Fixed Point algorithm is guaranteed to globally converge. Assuming  that the data are indeed generated from independent components, we derive the optimal  parameters for convergence. We also investigate how the optimal size of the ICA mixing  matrix varies as a function of the added noise, and demonstrate the presence of a singular  point. By expanding the likelihood about this singular point, the behavior of the ICA algo-  rithms can be related to the higher order statistics present in the data. Finally, we illustrate  the application and convergence of these ICA algorithms on some artificial data.  Generative model  A convenient method for interpreting the different ICA algorithms is in terms of the hidden,  or latent, variable generarive model shown in Fig. I [9, 10]. The hidden variables  ICA Algorithms and Higher Order Statistics 493  M hidden variables  Weights  Noi Wse ...  N visible variables  Figure 1: Generative model for ICA algorithms. s are the hidden variables, cr are additive  Gaussian noise terms, and x = Ws + cr are the visible variables.  correspond to the different independent components and are assumed to have the factorized  non-Gaussian prior probability distribution:  M  ?(s) = H -y(sj). (6)  j=l  Once the hidden variables are instantiated, the visible variables {x,} are generated via a  linear mapping through the generarive weights W:  II exp - (x,- 2 , (7)  i=1 j  where (7 2 is the variance of the Gaussian noise added to the visible variables.  The probability of the data given this model is then calculated by integrating over all pos-  sible values of the hidden variables:  ?(:) = ds ?(s)?(ls): (2./.s ep  (- ws)"(8)  In the limit that the added noise vanishes, a 2  0, it has previously been shown that  maximizing the likelihood of Eq. 8 is equivalent to the Infomax algorithm in Eq. 2 [11].  In the following analysis, we will consider the situation when the viance of the noise is  nonzero, a 2  0.  Expectation-Maximization  We assume that the data has initially been preprocessed and spherized: (XiXj) -- 5ij.  Unfortunately, for finite noise 0 -2 and an arbitrary prior F($j), deriving a learning rule for  W in closed form is analytically intractable. However, it becomes possible to derive a  simple Expectation-Maximization (EM) learning rule under the constraint:  W = Wo, WorWo=I, (9)  which implies that W is orthogonal, and  is the length of the individual columns of W.  Indeed, for data that obeys the ICA model, x = Ws, it can be shown that the optimal W  must satisfy this orthogonality condition. By assuming the constraint in Eq. 9 for arbitrary  data, the posterior distribution P(slx) becomes conveniently factorized:  M [ 1 [(i/vTx)jsj_ 1 2 2 ]  P(slx ) c H exp -F(sj) + -ff  sj] . (10)  j----1  494 D. D. Lee, U. Rokni and H. Sornpolinsky  For the E-step, this factorized form allows the expectation function f ds P(s[x)s =  g(WTx) to be analytically evaluated. This expectation is then used in the M-step to find  the new estimate W':  (xg(Wrx) r) - AsW' = O, (11)  where As is a symmetric matrix of Lagrange multipliers that constrain the new W t to be  orthogonal. Eq. 11 is easily solved by taking the reduced singular value decomposition of  the rectangular matrix:  UDV T -- (xg(WTx)T), (12)  where UTU - VV r = I and D is a diagonal .Mr x .Mr matrix. Then the solution for the  EM estimate of the mixing matrix is given by'  W' = UV T (13)  1  As = - UDU T. (14)  As a specific example, consider the following prior for binary hidden variables: P(s) =  [6(s - 1) + 6(s + 1)]. In this case, the expectation f ds ?(slx)s = tanh(Wrx/cr 2) and  so the EM update rule is given by orthogonalizing the matrix:  W q-- (xtanh(2 WTx)l . (15)  Fixed Point ICA  Besides the presence of the linear term AGW in Eq. 5, the EM update rule looks very much  like that of the Fixed Point ICA algorithm. It turns out that without this linear term, the  convergence of the naive EM algorithm is much slower than that of Eq. 5. Here we show  that it is possible to interpret the role of this linear term in the Fixed Point ICA algorithm  within the framework of this generatire model.  Suppose that the distribution of the observed data 7o (x) is actually a mixture between an  isotropic distribution 7o (z) and a non-isotropic distribution 2:'1 (x):  PD(X) = ozPo(x) q- (1 - o/)P 1 (x). (16)  Because the isotropic part does not break rotational symmetry, it does not affect the choice  of the directions of the learned basis W. Thus, it is more efficient to apply the learning  algorithm to only the non-isotropic portion of the distribution, P (x) oc PD(x) - cPo(x),  rather than to the whole observed distribution PD(X). Applying EM to Pl(x) results in a  correction term arising from the subtracted isotropic distribution. With this correction, the  EM update becomes:  W +- (xg(WTx)) -- c.GW (17)  which is equivalent to the Fixed Point ICA algorithm when o = 1.  Unfortunately, it is not clear how to compute an appropriate value for o to use in fitting data.  Taking a very small value, o << 1, will result in a learning rule that is very similar to the  naive EM update rule. This implies that the algorithm will be guaranteed to monotonically  converge, albeit very slowly, to a local maximum of the likelihood. On the other hand,  choosing a large value, o >> 1, will result in a subtracted probability density P (z) that is  negative everywhere. In this case, the algorithm will converge slowly to a local minimum  of the likelihood. For the Fixed Point algorithm which operates in the intermediate regime,  o m 1, the algorithm is likely to converge most rapidly. However, it is also in this situation  that the subtracted density P (x) could have both positive and negative regions, and the  algorithm is no longer guaranteed to converge.  ICA Algorithms and Higher Order Statistics 495  Noise  Figure 2: Size of the optimal generative bases as a function of the added noise 7 2, showing  2  the singular point behavior around 7 c m 1.  Optimal value of c  In order to determine the optimal value of ct, we make the assumption that the observed  data obeys the ICA model, :c - As. Note that the statistics of the sources in the data need  not match the assumed prior distribution of the sources in the generarive model Eq. 6. With  this assumption, which is not related to the mixture assumption in Eq. 16, it is easy to show  that W = A is a fixed point of the algorithm. By analyzing the behavior of the algorithm  in the vicinity of this fixed point, a simple expression emerges for the change in deviations  from this fixed point, 5W, after a single iteration of Eq. 17:  0'0)) - aXo 6W + O(6W ) (18)  where the averaging here is over the true source distribution, assumed for simplicity to be  identical for all sources. us, the algorithm converges most rapidly if one chooses:  - ,  so that the local convergence is cubic. From Eq. 18 one can show that the condition for the  stability of the fixed point is given by a < ac, where:  0g(s) + g'0)) (20)  c = 2G  Thus, for a = 0, the stability criterion in Eq. 18 is equivalent to (sg(s)) > (g'(s)). For the  cubic nonlinearity g(s) = s a, this implies that the algorithm will find the true independent  features only if the source distribution has positive kurtosis.  Singular point expansion  Let us now consider how the optimal size  of the weights W varies as a function of the  noise parameter cr 2. For very small 7 2 << 1, the weights W are approximately described  by the Infomax algorithm of Eq. 2, and the lengths of the columns should be unity in order  to match the covariance of the data. For large cr 2 >> 1, however, the optimal size of the  weights should be very small because the covariance of the noise is already larger than that  of the data. In fact, for Factor Analysis which is a special case of the generative model  with F(s) = x 2 in Eq. 6, it can be shown that the weights are exactly zero, W 0, for  $ --  or2 > 1.  Thus, the size of the optimal generarive weights W varies with 7 2 as shown qualitatively  2  in Fig. 2. Above a certain critical noise value 7 c m 1, the weights are exactly equal to  496 D. D. Lee, U. Rokni and H. Sornpolinsky  O  O  0.81  0.8  0.79  0.78  0.77  or=0.9 q-l.0  r 0.5  1.5  0 76 ' '  0 5 10 15  Iteration  Figure 3: Convergence of the modified EM algorithm as a function of o. With g(s) -  tanh(s) as the nonlinearity, the likelihood {ln cosh(WTx)) is plotted as a function of the  iteration number. The optimal basis W are plotted on the two-dimensional data distribution  when the likelihood is maximized (top) and minimized (bottom).  zero, W = 0. Only below this critical value do the weights become nonzero. We expand  the likelihood of the generafive model in the vicinity of this singular point. This expansion  is well-behaved because the size of the generative weights W acts as a small perturbative  parameter in this expansion. The log likelihood of the model around this singular value is  = -1Tr [WW T- (1 - a2)I] 2 (21)  4  1  +. Z kurt(sm){xixjxtx) c l/VimWjml/VkmWlm  ijklm  +O(1 -or2) a,  then given by:  L  where kurt(sm) represents the kurtosis of the prior distribution over the hidden variables.  Note that this expansion is valid for any symmetric prior, and differs from other expansions  that assume small deviations from a Gaussian prior [12, 13]. Eq. 21 shows the importance  of the fourth-order cumulant of the observed data in breaking the rotational degeneracy of  the weights W. The generic behavior of ICA is manifest in optimizing the cumulant term  in Eq.21, and again depends crucially on the sign of the kurtosis that is used for the prior.  Example with artificial data  As an illustration of the convergence of the algorithm in Eq. 17, we consider the simple  two-dimensional uniform distribution:  1/12, -V' _< Xl,X2 _ vf (22)  P(Xl, x2) - 0, otherwise  With g(s) = tanh(s) as the nonlinearity, Fig. 3 shows how the overall likelihood con-  verges for different values of the parameter o as the algorithm is iterated. For o _< 1.0,  the algorithm converges to a maximum of the likelihood, with the fastest convergence at  Oopt = 0.9. However, for o > 1.2, the algorithm converges to a minimum of the like-  lihood. At an intermediate value, o -- 1.1, the likelihood does not converge at all, fluc-  tuating wildly between the maximum and minimum likelihood solutions. The maximum  ICA Algorithms and Higher Order Statistics 497  likelihood solution shows the basis vectors in W aligned with the sides of the square distri-  bution, whereas the minimum likelihood solution has the basis aligned with the diagonals.  These solutions can also be understood as maximizing and minimizing the kurtosis terms  in Eq. 21.  Discussion  The utility of the latent variable generative model is demonstrated on deriving algorithms  for ICA. By constraining the generarive weights to be orthogonal, an EM algorithm is  analytically obtained. By interpreting the data to be fitted as a mixture of isotropic and  non-isotropic parts, a simple correction to the EM algorithm is derived. Under certain  conditions, this modified algorithm is equivalent to the Fixed Point ICA algorithm, and  converges much more rapidly than the naive EM algorithm. The optimal parameter for  convergence is derived assuming the data is consistent with the ICA generative model.  There also exists a critical value for the noise parameter in the generarive model, about  which a controlled expansion of the likelihood is possible. This expansion makes clear  the role of higher order statistics in determining the generic behavior of different ICA  algorithms.  We acknowledge the support of Bell Laboratories, Lucent Technologies, the US-Israel Bi-  national Science Foundation, and the Israel Science Foundation. We also thank Hagai  Attias, Simon Haykin, Juha Karhunen, Te-Won Lee, Erkki Oja, Sebastian Seung, Boris  Shraiman, and Oren Shriki for helpful discussions.  References  [1] Haykin, S (1999). Neural networks: a comprehensive foundation. 2nd ed., Prentice-Hall, Upper  Saddle River, NJ.  [2] Jutten, C & Herault, J (1991). Blind separation of sources, part I: An adaptive algorithm based  on neuromimetic architecture. Signal Processing 24, 1-10.  [3] Comon, P (1994). Independent component analysis: a new concept? Signal Processing 36,  287-314.  [4] Roth, Z & Baram, Y (1996). Multidimensional density shaping by sigmoids. IEEE Trans. Neu-  ral Networks 7, 1291-1298.  [5] Bell, AJ & Sejnowski, TJ (1995). An information maximization approach to blind separation  and blind deconvolution. Neural Computation 7, 1129-1159.  [6] Amari, S, Cichocki, A & Yang, H (1996). A new learning algorithm for blind signal separation.  Advances in Neural Information Processing Systems 8, 757-763.  [7] Lee, TW, Girolami, M, & Sejnowski, TJ (1999). Independent component analysis using an  extended infomax algorithm for mixed sub-gaussian and super-gaussian sources. Neural Com-  putation 11,609-633.  [8] Hyvarinen, A & Oja, E (1997). A fast fixed-point algorithm for independent component analy-  sis. Neural Computation 9, 1483-1492.  [9] Hinton, G & Ghahramani, Z (1997). Generative models for discovering sparse distributed rep-  resentations. Philosophical Transactions Royal Socie.ty B 352, 1177-1190.  [10] Attias, H (1998). Independent factor analysis. Neural Computation 11,803-851.  [11] Pearlmutter, B & Parra, L (1996). A context-sensitive generalization of ICA. In ICONIP '96,  151-157.  [12] Nadal, JP & Parga, N (1997). Redundancy reduction and independent component analysis:  conditions on cumulants and adaptive approaches. Neural Computation 9, 1421-1456.  [13] Cardoso, JF (1999). High-order contrasts for independent component analysis. Neural Compu-  tation 11,157-192.  
Bayesian modelling of fMRI time series  Pedro A.d. E R. H0jen-S0rensen, Lars K. Hansen and Carl Edward Rasmussen  Department of Mathematical Modelling, Building 321  Technical University of Denmark  DK-2800 Lyngby, Denmark  phs, lkhansen, carlimm. dtu. dk  Abstract  We present a Hidden Markov Model (HMM) for inferring the hidden  psychological state (or neural activity) during single trial fMRI activa-  tion experiments with blocked task paradigms. Inference is based on  Bayesian methodology, using a combination of analytical and a variety  of Markov Chain Monte Carlo (MCMC) sampling techniques. The ad-  vantage of this method is that detection of short time learning effects be-  tween repeated trials is possible since inference is based only on single  trial experiments.  1 Introduction  Functional magnetic resonance imaging (fMRI) is a non-invasive technique that enables  indirect measures of neuronal activity in the working human brain. The most common  fMRI technique is based on an image contrast induced by temporal shifts in the relative  concentration of oxyhemoglobin and deoxyhemoglobin (BOLD contrast). Since neuronal  activation leads to an increased blood flow, the so-called hemodynamic response, the mea-  sured fMRI signal reflects neuronal activity. Hence, when analyzing the BOLD signal  there are two unknown factors to consider; the task dependent neuronal activation and the  hemodynamic response. Bandettini et al. [1993] analyzed the correlation between a bi-  nary reference function (representing the stimulus/task sequence) and the BOLD signal.  In the following we will also make reference to the binary representation of the task as  the paradigm. Lange and Zeger [1997] discuss a parameterized hemodynamic response  adapted by a least squares procedure. Multivariate strategies have been pursued in [Wors-  ley et al. 1997, Hansen et al. 1999]. Several explorative strategies have been proposed  for finding spatio-temporal activation patterns without explicit reference to the activation  paradigm. McKeown et al. [1998] used independent component analysis and found sev-  eral types of activations including components with "transient task related" response, i.e.,  responses that could not simply be accounted for by the paradigm. The model presented  in this paper draws on the experimental observation that the basic coupling between the  net neural activity and hemodynamic response is roughly linear while the relation between  neuronal response and stimulus/task parameters is often nonlinear [Dale 1997]. We will  represent the neuronal activity (integrated over the voxel and sampling time interval) by a  binary signal while we will represent the hemodynamic response as a linear filter of un-  known form and temporal extent.  Bayesian Modelling of fMRI Time Series 755  2 A Bayesian model of fMRI time series  Let s = {st  t = 0,... , T - 1} be a hidden sequence of binary state variables st  {0, 1}, representing the state of a single voxel over time; the time variable, t, indexes the  sequence of fMRI scans. Hence, st is a binary representation of the neural state. The  hidden sequence is governed by a symmetric first order Hidden Markov Model (HMM)  with transition probability oz = P(St+l = jlSt = j). We expect the activation to mimic  the blocked structure of the experimental paradigm so for this reason we restrict oz to be  larger than one half. The predicted signal (noiseless signal) is given by tt = h. s + 00 + 01 t,  where  denotes the linear convolution and h is the impulse response of a linear system of  order Mr. The dc off-set and linear trend which are typically seen in fMRI time series  are given by 00 and 01, respectively. Finally, it is assumed that the observable is given by  2 The generative model  zt = lt + t, where t is iid. Gaussian noise with variance a n.  considered is therefore given by:  p(stlst-i,oz) - oZCs,,s,_x + (1 - c0(1 - d,,,_i),  (,)  p(zls, an,O, Ms) AC(y, an2I), where y: {Yt}: HO, and z: {zt}.  Furthermore, d,,_x is the usual Kronecker delta and H = [1, , 3'0s, 3'is,   , 3'Ms-is],  where 1 = (1,...1)', =(1,...,T)'/T and % is a /-step shift operator, that is 3'is =  (0,..., 0, so, Sl,..., ST-l-i)'. The linear parameters are collected in 0 = (00,0i,  s  S 2  X 1 X 2  S: ST_  -()  X 3 XT_ 1  The graphical model. The hidden states  Xt = (St-l,St-2,...,St_(Ms_l)) have  been introduced to make the model first or-  der.  3 Analytic integration and Monte Carlo sampling  In this section we introduce priors over the model parameters and show how inference may  be performed. The filter coefficients and noise parameters may be handled analytically,  whereas the remaining parameters are treated using sampling procedures (a combination  of Gibbs and Metropolis sampling). Like in the previous section explicit reference to the  filter order M s may be omitted to ease the notation.  The dc off-set 00 and the linear trend 01 are given (improper) uniform priors. The filter  coefficients are given priors that are uniform on an interval of length  independently for  each coefficient:  p(h[Mf ) = {   for Ihi[<-, O_<i_<Mf-1  otherwise  Assuming that all the values of 0 for which the associated likelihood has non-vanishing  contributions lie inside the box where the prior for 0 has support, we may integrate out the  filter coefficients via a Gaussian integral:  f  P(Zlan'S'Mf): P(ZlO'an'S'Mf)P(OlMI)dO -- M  exp ( z'z- '  2On )'  756 PA. d. E R. Hojen-Sorensen, L. K. Hansen and C. E. Rasmussen  We have here defined the mean filter, ts = (HHs)-iHz and mean predicted signal,  ) = Hs, for given state and filter length. We set the interval-length,/ to be 4 times the  standard deviation of the observed signal z. This is done, since the response from the filter  should be able to model the signal, for which it is thought to need an interval of plus/minus  two standard deviations.  We now proceed to integrate over the noise parameter; using the (improper) non-  informative Jeffreys prior, p(Crn) oc cr -1 , we get a Gamma integral:  f T-2%  p(zls, Mf): p(z[crn,S, MI)p(crn)dcrn = 2  (z,z-  The remaining variables cannot be handled analytically, and will be treated using various  forms of sampling as described in the following sections.  3.1 Gibbs and Metropolis updates of the state sequence  We use a flat prior on the states, p(st = O) = p(st = 1), together with the first order  Markov property for the hidden states and Bayes' rule to get the conditional posterior for  the individual states:  p(st = jls\st,c,M) oc p(st = jlst_l,O)p(st+llSt = j,o)p(zls, Mf).  These probabilities may (in normalized form) be used to implement Gibbs updates for the  hidden state variables, updating one variable at a time and sweeping through all variables.  However, it turns out that there are significant correlations between states which makes it  difficult for the Markov Chain to move around in the hidden state-space using only Gibbs  sampling (where a single state is updated at a time). To improve the situation we also  perform global state updates, consisting of proposing to move the entire state sequence  one step forward or backward (the direction being chosen at random) and accepting the  proposed state using the Metropolis acceptance procedure. The proposed movements are  made using periodic boundary conditions. The Gibbs sweep is computationally involved,  since it requires computation of several matrix expressions for every state-variable.  3.2 Adaptive Rejection Sampling for the transition probability  The likelihood for the transition probability o is derived from the Hidden Markov Model:  p(slc)  T-1  1 aE( ) (1 - a)  H P($tlst-l')--   t=l  T-1  where E(s) = E/=i 6st ,st-x is the number of neighboring states in s with identical values.  The prior on the transition probabilities is uniform, but restricted to be larger than one half,  since we expect the activation to mimic the blocked structure of the experimental paradigm.  It is readily seen that p(o[s) x p(slo ), o E [21-, 1] is log-concave. Hence, we may use  the Adaptive Rejection Sampling algorithm [Gilks and Wild, 1992] to sample from the  distribution for the transition probability.  3.3 Metropolis updates for the filter length  In practical applications using real fMRI data, we do typically not know the necessary  length of the filter. The problem of finding the "right" model order is difficult and has re-  ceived a lot of attention. Here, we let the Markov Chain sample over different filter lengths,  effectively integrating out the filter-length rather than trying to optimize it. Although the  Bayesian Modelling of fMRI Time Series 757  value of Mf determines the dimensionality of the parameter space, we do not need to use  specialized sampling methodology (such as Reversible Jump MCMC [Green, 1995]), since  those parameters are handled analytically in our model. We put a flat (improper) prior on  Mf and propose new filter lengths using a Gaussian proposal centered on the current value,  with a standard deviation of 3 (non-positive proposed orders are rejected). This choice of  the standard deviation only effects the mixing rate of the Markov chain and does not have  any influence on the stationary distribution. The proposed values are accepted using the  Metropolis algorithm, using p(Mfls, y) oc p(yl s, Mf ).  3.4 The posterior mean and uncertainty of the predicted signal  Since 0 has a flat prior the conditional probability for the filter coefficients is proportional  to the likelihood p( zlO , .) and by (,) we get:  D 2  ,  p(Olz, s, an,Mf) ,,,.A/'( sZ, anDD), Ds = (HH)-H' .  The posterior mean of the predicted signal, 9, is then readily computed as:  = = = =  where F -- HD. Here, the average over 0 and an is done analytically, and the average  over the state and filter length is done using Monte Carlo. The uncertainty in the posterior,  can also be estimated partly by analytical averaging, and partly Monte Carlo:  = - -  _ 1 ((z'z ^'^'F F '\ (Fszz'F ' .  - T - M s - 2 - VY)  /,s + ),s - ))'  4 Example: synthetic data  In order to test the model, we first present some results on a synthetic data set. A signal  z of length 100 is generated using a Mf - 10 order filter, and a hidden state sequence s  consisting of two activation bursts (indicated by dotted bars in figure 1 top left). In this  example, the hidden sequence is actually not generated from the generarive model (,);  however, it still exhibits the kind of block structure that we wish to be able to recover.  The model is run for 10000 iterations, which is sufficient to generate 500 approximately  independent samples from the posterior; figure 2 (right) shows the autoocovariance for  Mf as a function of the iteration lag. It is thought that changes in Mf are indicative of  correlation time of the overall system.  The correlation plot for the hidden states (figure 2, left) shows that the state activation onset  correlates strongly with the second onset and negatively with the end of the activation (and  vice versa). This indicates that the Metropolis updates described in section 3.1 may indeed  be effective. Notice also that the very strong correlation among state variables does not  strongly carry over to the predicted signal (figure 1, bottom right).  To verify that the model can reasonably recover the parameters used to generate the data,  posterior samples from some of the model variables are shown in figure 3. For all these  parameters the posterior density is large around the correct values. Notice, that there in the  original model (,) is an indeterminacy in the simultaneous inference of the state sequence  and the filter parameters (but no indeterminacy in the predicted signal); for example, the  same signal is predicted by shifting the state sequence backward in time and introducing  leading zero filter coefficients. However, the Bayesian methodology breaks this symmetry  by penalizing complex models.  758 P. A. d. E R. Hojen-Sorensen, L. K. Hansen and C. E. Rasmussen  N   4  ._c  - 0  o  z  -2 '  0 20 40 60 80 1 O0  Scan number, t  6  4  2  0  *.- 20  O9 80  20 40 60  Scan number, t  -2 , 1 O0  0 20 40 60 80 100 20 40 60 80 100  Scan number, t Scan number, t  lOO  0.8  __0.6  0.4  0.2  0  -0.2  Figure 1: Experiments with synthetic data. Top left, the measured response from a voxel  is plotted for 100 consecutive scans. In the bottom left, the underlying signal is seen in  thin, together with the posterior mean, ) (thick), and two std. dev. error-bars in dotted. Top  right, the posterior probabilities are shown as a grey level, for each scan. The true activated  instances are indicated by the dotted bars and the pseudo MAP estimate of the activation  sequence is given by the crossed bars. Bottom right, shows the posterior uncertainty  The posterior mean and the two standard deviations are plotted in figure 1 bottom left. No-  tice, however, that the distribution of t is not Gaussian, but rather a mixture of Gaussians,  and is not necessarily well characterized by mean and variance alone. In figure 1 (top left),  the distribution of tt is visualized using grey-scale to represent density.  5 Simulations on real fMRI data and discussion  In figure 4 the model has been applied to two measurements in the same voxel in visual  cortex. The fMRI scans were acquired every 330 ms. The experimental paradigm consisted  of 30 scans of rest followed by 30 scans of activation and 40 rest. Visual activation con-  sisted of a flashing (8 Hz) annular checkerboard pattern. The model readily identifies the  activation burst of somewhat longer duration than the visual stimulus and delayed around  2 seconds. The delay is in part caused by the delay in the hemodynamic response.  These results show that the integration procedure works in spite of the very limited data  at hand. In figure 4 (top) the posterior model size suggests that (at least) two competing  models can explain the signal from this trial. One of these models explains the measured  signal as a simple square wave function which seems reasonable by considering the signal.  Conversely, figure 4 (bottom), suggests that the signal from the second trial can not be  explained by a simple model. This too, seems reasonable because of the long signal raise  interval suggested in the signal.  Bayesian Modelling of fMRI Time Series 759  Hidden state variables, s  o  lag  Figure 2: The covariance of the hidden states based on a long run of the model is shown to  the left. Notice, that the states around the front (back) of the activity "bumps" are highly  (anti-) correlated. Right: The auto-covariance for the filter length M/as a function of the  lag time in iterations. The correlation length is about 20, computed as the sum of auto-  covariance coefficients from lag -400 to 400.  Since the posterior distribution of the filter length is very broad it is questionable whether  an optimization based procedure such as maximum likelihood estimation would be able  to make useful inference in this case were data is very limited. Also, it is not obvious  how one may use cross-validation in this setting. One might expect such optimization  based strategies to get trapped in suboptimal solutions. This, of course, remains to be  investigated.  6 Conclusion  We have presented a model for voxel based explorative data analysis of single trial fMRI  signals during blocked task activation studies. The model is founded on the experimental  observation that the basic coupling between the net neural activity and hemodynamic re-  sponse is roughly linear. The preliminary investigation reported here are encouraging in  that the model reliably detects reasonable hidden states from the very noisy fMRI data.  One drawback of this method is that the Gibbs sampling step is computational expensive.  To improve on this step one could make use of the large class of variational/mean field  methods known from the graphical models literature. Finally, current work is in progress  for generalizing the model to multiple voxels, including spatial correlation due to e.g. spill-  over effects.  0.15  0.1  0.05  0.15  0.1  0.05  0  1 1.2  0.15  0.1  0.05  0  .5 2 2.5  DC off-set  -2 -1 0  Trend  Oo  0 5 10  Mf  Figure 3' Posterior distributions of various model parameters. The parameters used to  generate the data are: a = 1.0, DC off-set -- 2, trend = -1 and filter order Mf -- 10.  760 P. A. d. E R. Hojen-Sorensen, L. K. Hansen and C. E. Rasmussen  320  300  280  280  240 i  22O  2O0  180  0  280 ,  260  24O  22O  20O  180  180  20 40 60 80 100  Scan number, t  0 20 40 80 80  Scan number, t  oo  0.1  0.05  12 14 16 18 20  0.1  0.05  0 5 10 15 20  Mf  0.15  0.1  0.05  o  0.15  10 12 14 16  0.1  0.05  0  0  5 10 15 20  Mf  Figure 4: Analysis of two experimental trials of the same voxel in visual cortex. The left  hand plot shows the posterior inferred signal distribution superimposed by the measured  signal. The dotted bar indicates the experimental paradigm and the crossed bar indicates  the pseudo MAP estimate of the neural activity. To the right the posterior noise level and  inferred filter length are displayed.  Acknowledgments  Thanks to Egill Rostrup for providing the fMRI data. This work is funded by the Danish  Research Councils through the Computational Neural Network Center (CONNECT) and  the THOR Center for Neuroinformatics.  References  Bandettini, P. A. (1993). Processing strategies for time-course data sets in functional MRI of the  human brain Magnetic Resonance in Medicine 30, 161-173.  Dale, A.M. and R. L. Bucknet (1997). Selective Averaging of Individual Trials Using fMRI. Neu-  rolmage 5, Abstract S47.  Green, P. J. (1995). Reversible jump Markov chain Monte Carlo computation and Bayesian model  determination. Biometrika 82, 711-732.  Gilks, W. R. and P. Wild (1992). Adaptive rejection sampling for Gibbs sampling. Applied Statis-  tics 41,337-348.  Hansen, L. K. et al. (1999). Generalizable Patterns in Neuroimaging: How Many Principal Compo-  nents? Neurolmage, to appear.  Lange, N. and S. L. Zeger (1997). Non-linear Fourier time series analysis for human brain mapping  by functional magnetic resonance imaging. Journal of the Royal Statistical Society - Series C Applied  Statistics 46, 1-30.  McKeown, M. J. et al. (1998). Spatially independent activity patterns in functional magnetic reso-  nance imaging data during the stroop color-naming task. Proc. Natl. Acad. Sci. USA. 95, 803-810.  Worsley, K. J. et al. (1997). Characterizing the Response of PET and fMRI Data Using Multivariate  Linear Models (MLM). Neurolmage 6, 305-319.  
Greedy importance sampling  Dale Schuurmans  Department of Computer Science  University of Waterloo  dale @ cs.uwaterloo .ca  Abstract  I present a simple variation of importance sampling that explicitly search-  es for important regions in the target distribution. I prove that the tech-  nique yields unbiased estimates, and show empirically it can reduce the  variance of standard Monte Carlo estimators. This is achieved by con-  centrating samples in more significant regions of the sample space.  1 Introduction  It is well known that general inference and learning with graphical models is computa-  tionally hard [ 1] and it is therefore necessary to consider restricted architectures [ 13], or  approximate algorithms to perform these tasks [3, 7]. Among the most convenient and  successful techniques are stochastic methods which are guaranteed to converge to a correct  solution in the limit of large samples [ 10, 11, 12, 15]. These methods can be easily applied  to complex inference problems that overwhelm deterministic approaches.  The family of stochastic inference methods can be grouped into the independent Monte  Carlo methods (importance sampling and rejection sampling [4, 10, 14]) and the dependent  Markov Chain Monte Carlo (MCMC) methods (Gibbs sampling, Metropolis sampling, and  "hybrid" Monte Carlo) [5, 10, 11, 15]. The goal of all these methods is to simulate drawing  a random sample from a target distribution P(z) (generally defined by a Bayesian network  or graphical model) that is difficult to sample from directly.  This paper investigates a simple modification of importance sampling that demonstrates  some advantages over independent and dependent-Markov-chain methods. The idea is to  explicitly search for important regions in a target distribution P when sampling from a  simpler proposal distribution Q. Some MCMC methods, such as Metropolis and "hybrid"  Monte Carlo, attempt to do something like this by biasing a local random search towards  higher probability regions, while preserving the asymptotic "fair sampling" properties of  the exploration [11, 12]. Here I investigate a simple direct approach where one draws  points from a proposal distribution Q but then explicitly searches in P to find points from  significant regions. The main challenge is to maintain correctness (i.e., unbiasedness) of  the resulting procedure, which we achieve by independently sampling search subsequences  and then weighting the sample points so that their expected weight under the proposal  distribution Q matches their true probability under the target P.  Greedy Importance Sampling 597  Importance sampling   Draw z, ..., z, independently from Q.   Weight each point zi by w(zi) =  Q(,)    For a random variable, f, estimate Ev(,) f(x)  by f-  _  "Indirect" importance sampling   Draw x, ...,x, independently from Q.   Weight each point xi by u(xi) = /sP()  Q('i) '   For a random variable, f, estimate Eio(,) f(z)  Figure 1' Regular and "indirect" importance sampling procedures  2 Generalized importance sampling  Many inference problems in graphical models can be cast as determining the expected  value of a random variable of interest, f, given observations drawn according to a target  distribution P. That is, we are interested in computing the expectation E,(x) f(z). Usually  the random variable f is simple, like the indicator of some event, but the distribution P  is generally not in a form that we can sample from efficiently. Importance sampling is a  useful technique for estimating E,(x) f(z) in these cases. The idea is to draw independent  points zx, ..., z, from a simpler "proposal" distribution Q, but then weight these points  by w(z) = P(z)/Q(z) to obtain a "fair" representation of P. Assuming that we can  efficiently evaluate P(z) at each point, the weighted sample can be used to estimate de-  sired expectations (Figure 1). The correctness (i.e., unbiasedness) of this procedure is easy  to establish, since the expected weighted value of f under Q is just Ec2()f(z)w(z) =  Eex [f(x)w(x)] Q(x) = Eex If(z)  q()] Q(x)= -xex f(x)P(z) = Ep()f(z).  is technique can be implemented using "indirect" weights u(z) = P(z)/Q(z) and an  alternative estimator (Figure 1) that only requires us to compute a fixed multiple of P(z).  is preserves asymptotic cogectness because x Ei=x f(zi)u(zi) and x E=x u (zi) con-  verge to Ep() f(z) and  respectively, which yields f  Ep() f(z) (generally [4]). It  will always be possible to apply this extended approach below, but we drop it for now.  Importance stapling is an effective estimation technique when Q approximates P over  most of the domn, but it fails when Q misses high probability regions of P and system-  atically yields staples with sml weights. In this ce, the resulting estimator will have  high viance because the staple will almost always contn unrepresentative points but is  sometimes dominated by a few high weight points. To overcome this problem it is criti-  c to obtn data points from the important regions of P. Our goal is to avoid generating  systematically under-weight staples by explicitly seching for significant regions in the  tget distribution P. To do this, and mntn the unbiasedness of the resulting procedure,  we develop a series of extensions to importance stapling that e each provably cogect.  e first extension is to consider stapling blocks of points instead of just individu points.  Let BbeaptitionofX into finite blocksB, where esB = X, B B' = 0,  and each B is finite. (Note that B can be infinite.) e "block" stapling procedure  (Figure 2) draws independent blocks of points to construct the final staple, but then  weights points by their tget probability P(z) divided by the total block probability  Q(B(z)). For discrete spaces it is easy to verify that this procedure yields unbied esti-  mates, since EQ()[er()f(Xj)W(Xj)] : xeX [xeB(x) f(Xj)W(Xj) Q(x) =  $98 D. Schuurrnans  "Block" importance sampling   Draw z, ..., z, independently from Q.   For zi, recover block Bi = {zi,i, ..., zi,b }.   Create a large sample out of the blocks  1,1  ...  l,bl  32,1  ... 2,b2  '" n,1  ... n,bn    Weight each zi,j by   For a random variable, f, estimate E,(x) f(z)  "Sliding window" importance sampling   Draw z, ..., z, independently from Q.   For zi, recover block Bi, and let zi, =  - Get Zi,1 'S successors xi,1  :i,2  ... :i,rn  by climbing up ra - 1 steps from  - Get predecessors i,--rn+l   ... Xi,--1  :i,O  by climbing down ra - 1 steps from  - Weight w(zi,j)= P(x,j)/-]k=S_,+C2(x,k)   Create final sample from successor points  :CI,1  ... :Cl,rn 1C2,1  .--:C2,rn ... n,1  ...:Cn,rn.   For a random variable, f, estimate E,(,) f(z)  by ] =  E= E_- f(x,,j)w(x,,).  Figure 2: "Block" and "sliding window" importance sampling procedures  Crucially, this argument does not depend on how the partition of X is chosen. In fact, we  could fix any partition, even one that depended on the target distribution P, and still obtain  an unbiased procedure (so long as the partition remains fixed). Intuitively, this works be-  cause blocks are drawn independently from Q and the weighting scheme still produces a  "fair" representation of P. (Note that the results presented in this paper can all be extended  to continuous spaces under mild technical restrictions. However, for the purposes of clarity  we will restrict the technical presentation in this paper to the discrete case.)  The second extension is to allow countably infinite blocks that each have a discrete to-  tal order ..- < zi- < zi < zi+ < .-- defined on their elements. This order could  reflect the relative probability of zi and zj under P, but for now we just consider it  to be an arbitrary discrete order. To cope with blocks of unbounded length, we em-  ploy a "sliding window" sampling procedure that selects a contiguous sub-block of size  m from within a larger selected block (Figure 2). This procedure builds each indepen-  dent subsample by choosing a random point zx from the proposal distribution Q, de-  termining its containing block B(z), and then climbing up m - 1 steps to obtain the  successors zx, z2, ..., z,, and climbing down m - 1 steps to obtain the predecessors  z_,+x, ..., z_x, zo. The successor points (including zx) appear in the final sample, but  the predecessors are only used to determine the weights of the sample points. Weights  are determined by the target probability P(z) divided by the probability that the point  z appears in a random reconstruction under Q. This too yields an unbiased estimator  [ . ] [ t+,- ,()  since E() Ej= f(zj)w(zj) = EeX E=t f(z) Q(zt) :  6Bi=j-m+l  BE B x6 B j=! J  =_+  (e middle line brews the sum into diQoint blocks and then reorders the sum so thin in-  stead of first choosing the stt point zt and then zt's successors zt, ..., Zt+m-X, we first  choose the successor point zj and then the st points zj-+x, ..., z i that could have led  to zj). Note that this derivation does not depend on the pticul block ptition nor on the  pticular discrete orderings, so long as they remEn fixed. is means thin, again, we can  use ptitions and orderings thin explicitly depend on P and still obtain a cogect procedure.  Greedy Importance Sampling 599  "Greedy" importance sampling (I-D)   Draw Zl, .... :r. independently from Q.   For each x, let xi,1 = xi:  - Compute successors xi,1, xi,2, ..., xi,, by taking  ra - 1 size e steps in the direction of increase.  - Compute predecessors xi,-,+l, ..., xi,_, xi,o by  taking ra- 1 size e steps in the direction of decrease.  - If an improper ascent or descent occurs,  truncate paths as shown on the upper right.  Weight w(xi,j) P(xi,j)/ J  - =   Create the final sample from successor points  Igl,1  --. Xl,rn  Ig2,1  ... II2,rn  ... IIn,1  ... n,rn.   For a random variable, f, estimate Ep() f(z)  collision  merge  Figure 3: "Greedy" importance sampling procedure; "colliding" and "merging" paths.  3 Greedy importance sampling: 1-dimensional case  Finally, we apply the sliding window procedure to conduct an explicit search for impor-  tant regions in X. It is well known that the optimal proposal distribution for importance  sampling is just Q* (z) = If(z)P(z)l/xx If(z)P(z)l (which minimizes variance [21).  Here we apply the sliding window procedure using an order structure that is determined by  the objective I f(z)P(z)[. The hope is to obtain reduced variance by sampling independent  blocks of points where each block (by virtue of being constructed via an explicit search) is  likely to contain at least one or two high weight points. That is, by capturing a moderate  size sample of independent high weight points we intuitively expect to outperform standard  methods that are unlikely to observe such points by chance. Our experiments below verify  this intuition (Figure 4).  The main technical issue is maintaining unbiasedness, which is easy to establish in the 1-  dimensional case. In the simple 1-d setting, the "greedy" importance sampling procedure  (Figure 3) first draws an initial point zx from Q and then follows the direction of increas-  ing ]f(z)P(z)l, taking fixed size  steps, until either rn - 1 steps have been taken or we  encounter a critical point. A single "block" in our final sample is comprised of a complete  sequence captured in one ascending search. To weight the sample points we account for all  possible ways each point could appear in a subsample, which, as before, entails climbing  down rn- 1 steps in the descent direction (to calculate the denominators). The unbiasedness  of the procedure then follows directly from the previous section, since greedy importance  sampling is equivalent to sliding window importance sampling in this setting.  The only nontrivial issue is to maintain disjoint search paths. Note that a search path must  terminate whenever it steps from a point z* to a point z** with lower value; this indicates  that a collision has occurred because some other path must reach z* from the "other side"  of the critical point (Figure 3). At a collision, the largest ascent point z* must be allocated  to a single path. A reasonable policy is to allocate z* to the path that has the lowest weight  penultimate point (but the only critical issue is ensuring that it gets assigned to a single  block). By ensuring that the critical point is included in only one of the two distinct search  paths, a practical estimator can be obtained that exhibits no bias (Figure 4).  To test the effectiveness of the greedy approach I conducted several 1-dimensional experi-  ments which varied the relationship between P, Q and the random variable f (Figure 4). In  600 D. Schuurmans  these experiments greedy importance sampling strongly outperformed standard methods,  including regular importance sampling and directly sampling from the target distribution P  (rejection sampling and Metropolis sampling were not competitive). The results not only  verify the unbiasedness of the greedy procedure, but also show that it obtains significantly  smaller variances across a wide range of conditions. Note that the greedy procedure actu-  ally uses rn out of 2rn - 1 points sampled for each block and therefore effectively uses a  double sample. However, Figure 4 shows that the greedy approach often obtains variance  reductions that are far greater than 2 (which corresponds to a standard deviation reduction  of x/-).  4 Multi-dimensional case  Of course, this technique is worthwhile only if it can be applied to multi-dimensional prob-  lems. In principle, it is straightforward to apply the greedy procedure of Section 3 to  multi-dimensional sample spaces. The only new issue is that discrete search paths can now  possibly "merge" as well as "collide"; see Figure 3. (Recall that paths could not merge  in the previous case.) Therefore, instead of decomposing the domain into a collection of  disjoint search paths, the objective If(z)P(z)l now decomposes the domain into a forest  of disjoint search trees. However, the same principle could be used to devise an unbiased  estimator in this case: one could assign a weight to a sample point z that is just its target  probability P(z) divided by the total Q-probability of the subtree of points that lead to z  in fewer than rn steps. This weighting scheme can be shown to yield an unbiased estimator  as before. However, the resulting procedure is impractical because in an N-dimensional  sample space a search tree will typically have a branching factor of Q(N); yielding expo-  nentially large trees. Avoiding the need to exhaustively examine such trees is the critical  issue in applying the greedy approach to multi-dimensional spaces.  The simplest conceivable strategy is just to ignore merge events. Surprisingly, this turns  out to work reasonably well in many circumstances. Note that merges will be a measure  zero event in many continuous domains. In such cases one could hope to ignore merges  and trust that the probability of "double counting" such points would remain near zero.  I conducted simple experiments with a version of greedy importance sampling procedure  that ignored merges. This procedure searched in the gradient ascent direction of the objec-  tive I f(z)p(z)l and heuristically inverted search steps by climbing in the gradient descent  direction. Figures 5 and 6 show that, despite the heuristic nature of this procedure, it nev-  ertheless demonstrates credible performance on simple tasks.  The first experiment is a simple demonstration from [ 12, 10] where the task is to sample  from a bivariate Gaussian distribution P of two highly correlated random variables using a  "weak" proposal distribution Q that is standard normal (depicted by the elliptical and circu-  lar one standard deviation contours in Figure 5 respectively). Greedy importance sampling  once again performs very well (Figure 5); achieving unbiased estimates with lower variance  than standard Monte Carlo estimators, including common MCMC methods.  To conduct a more significant study, I applied the heuristic greedy method to an inference  problem in graphical models: recovering the hidden state sequence from a dynamic proba-  bilistic model, given a sequence of observations. Here I considered a simple Kalman filter  model which had one state variable and one observation variable per time-step, and used  the conditional distributions XtIXt_x ,.., N(zt_x, 0.), Zt[Xt '" N(zt, 0'o 2) and initial dis-  tribution Xx ,--, N(0, 0'2). The problem was to infer the value of the final state variable zt  given the observations zx, z2, ..., zt. Figure 6 again demonstrates that the greedy approach  Greedy Importance Sampling 601  has a strong advantage over standard importance sampling. (In fact, the greedy approach  can be applied to "condensation" [6, 8] to obtain further improvements on this task, but  space bounds preclude a detailed discussion.)  Overall, these preliminary results show that despite the heuristic choices made in this sec-  tion, the greedy strategy still performs well relative to common Monte Carlo estimators,  both in terms of bias and variance (at least on some low and moderate dimension prob-  lems). However, the heuristic nature of this procedure makes it extremely unsatisfying. In  fact, merge points can easily make up a significant fraction of finite domains. It turns out  that a rigorously unbiased and feasible procedure can be obtained as follows. First, take  greedy fixed size steps in axis parallel directions (which ensures the steps can be inverted).  Then, rather than exhaustively explore an entire predecessor tree to calculate the weights of  a sample point, use the well known technique of Knuth [9] to sample a single path from the  root and obtain an unbiased estimate of the total Q-probability of the tree. This procedure  allows one to formulate an asymptotically unbiased estimator that is nevertheless feasible  to implement. It remains important future work to investigate this approach and compare  it to other Monte Carlo estimation methods on large dimensional problems--in particular  hybrid Monte Carlo [ 11, 12]. The current results already suggest that the method could  have benefits.  References  [1]  [21  [31  [41  [51  [61  [71  [81  [91  [lO1  [111  [121  [131  [141  [151  P. Dagum and M. Luby. Approximating probabilistic inference in Bayesian belief networks is  NP-hard. Artiflntell, 60:141-153, 1993.  M. Evans. Chaining via annealing. Ann Statist, 19:382-393, 1991.  B. Frey. Graphical Models for Machine Learning and Digital Communication. MIT Press,  Cambridge, MA, 1998.  J. Geweke. Baysian inference in econometric models using Monte Carlo integration. Econo-  metrica, 57:1317-1339, 1989.  W. Gilks, S. Richardson, and D. Spiegelhalter. Markov chain Monte Carlo in practice. Chapman  and Hall, 1996.  M. Isard and A. Blake. Coutour tracking by stochastic propagation of conditional density. In  ECCV, 1996.  M. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. An introduction to variational methods for  graphical models. In Learning in Graphical Models. Kluwer, 1998.  K. Kanazawa, D. Koller, and S. Russell. Stochastic simulation algorithms for dynamic proba-  bilistic networks. In UAI, 1995.  D. Knuth. Estimating the efficiency of backtracking algorithms. Math. Cornput., 29(129):121-  136, 1975.  D. MacKay. Intro to Monte Carlo methods. In Learning in GraphicalModels. Kluwer, 1998.  R. Neal. Probabilistic inference using Markov chain Monte Carlo methods. 1993.  R. Neal. Bayesian Learning for Neural Networks. Springer, New York, 1996.  J. Peafl. Probabilistic Reasoning in Intelligence Systems. Morgan Kaufmann, 1988.  R. Shacter and M. Peot. Simulation approaches to general probabilistic inference in belief  networks. In Uncertainty in Artificial Intelligence 5. Elsevier, 1990.  M. Tanner. Tools for statistical inference: Methods for exploration of posterior distributions  and likelihood functions. Springer, New York, 1993.  602 D. Schuurmans  mean  stdev  Direc Greed Imprt  0.779 0.781 0.777  0.001 0.001 0.003  0.071 0.038 0.065  Direc Greed Imprt  1.038 1.044 1.032  0.002 0.003 0.008  0.088 0.049 0.475  Direc Greed Imprt.  0.258 0.208 0.209  0.049 0.000 0.001  0.838 0.010 0.095  Direc Greed Imprt  6.024 6.028 6.033  0.001 0.004 0.009  0.069 0.037 0.094  Figure 4: 1-dimensional experiments: 1000 repetitions on estimation samples of size 100.  Problems with varying relationships between P, Q, f and IfPI.  mean  bias  stdev  Direct Greedy Importance Rejection Gibbs Metropolis  0.1884 0.1937 0.1810 0.1506 0.3609 8.3609  0.0022 0.0075 0.0052 0.0356 0.1747 8.1747  0.07 0.1374 0.1762 0.2868 0.5464 22.1212  Figure 5: 2-dimensional experiments: 500 repetitions on estimation samples of size 200.  Pictures depict: direct, greedy importance, regular importance, and Gibbs sampling, show-  ing 1 standard deviation countours (dots are sample points, vertical lines are weights).  mean  bias  stdev  Importance Greedy  5.2269 6.9236  2.7731 1.0764  1.2107 0.1079  Figure 6: A 6-dimensional experiment: 500 repetitions on estimation samples of size 200.  Estimating the value of vet given the observations zx, ..., zt. Pictures depict paths sampled  by regular versus greedy importance sampling.  
Learning Statistically Neutral Tasks  without Expert Guidance  Ton Weijters  Information Technology,  Eindhoven University,  The Netherlands  Antal van den Bosch  ILK,  Tilburg University,  The Netherlands  Eric Postma  Computer Science,  Universiteit Maastricht,  The Netherlands  Abstract  In this paper, we question the necessity of levels of expert-guided  abstraction in learning hard, statistically neutral classification  tasks. We focus on two tasks, date calculation and parity-12, that  are claimed to require intermediate levels of abstraction that must  be defined by a human expert. We challenge this claim by demon-  strating empirically that a single hidden-layer BP-SOM network can  learn both tasks without guidance. Moreover, we analyze the net-  work's solution for the parity-12 task and show that its solution  makes use of an elegant intermediary checksum computation.  i Introduction  Breaking up a complex task into many smaller and simpler subtasks facilitates  its solution. Such task decomposition has proved to be a successful technique in  developing algorithms and in building theories of cognition. In their study and  modeling of the human problem-solving process, Newell and Simon [1] employed  protocol analysis to determine the subtasks human subjects employ in solving a  complex task. Even nowadays, many cognitive scientists take task decomposition,  i.e., the necessity of explicit levels of abstraction, as a fundamental property of  human problem solving. Dennis Norris' [2] modeling study on the problem-solving  capacity of autistic savants is a case in point. In the study, Norris focuses on the  date-calculation task (i.e., to calculate the day of the week a given date fell on),  which some autistic savants have been reported to perform flawlessly [3]. In an  attempt to train a multi-layer neural network on the task, Norris failed to get a  satisfactory level of generalization performance. Only by decomposing the task into  three sub-tasks, and training the separate networks on each of the sub-tasks, the  date-calculation task could be learned. Norris concluded that the date-calculation  task is solvable (learnable) only when it is decomposed into intermediary steps using  human assistance [2].  The date-calculation task is a very hard task for inductive learning algorithms,  because it is a statistically neutral task: all conditional output probabilities on  any input feature have chance values. Solving the task implies decomposing it,  if possible, into subtasks that are not statistically neutral. The only suggested  decomposition of the date-calculation task known to date involves explicit assistance  74 T. Weijters, A. v. d. Bosch and E. Postma  MFN  SOM  - class A elements  - class B elements  - unlabelled element  Figure 1' An example BP-SOM network.  from a human supervisor [2]. This paper challenges the decomposition assumption  by showing that the date-calculation task can be learned in a single step with a  appropriately constrained single hidden-layer neural network. In addition, another  statistically neutral task, called the parity-n task (given an n-length bit string of  1's and O's, calculate whether the number of 1's is even or odd) is investigated.  In an experimental study by Dehaene, Bossini, and Giraux [4], it is claimed that  humans decompose the parity-n task by first counting over the input string, and  then perform the even/odd decision. In our study, parity-12 is shown to be learnable  by a network with a single hidden layer.  2 BP-SOM  Below we give a brief characterization of the functioning of BP-SOM. For details we  refer to [5]. The aim of the BP-SOM learning algorithm is to establish a coopera-  tion between BP learning and SOM learning in order to find adequately constrained  hidden-layer representations for learning classification tasks. To achieve this aim,  the traditional MFN architecture [6] is combined with SOMs [7]: each hidden layer of  the MFN is associated with one SOM (See Figure 1). During training of the weights in  the MEN, the corresponding SOM is trained on the hidden-unit activation patterns.  After a number of training cycles of BP-SOM learning, each SOM develops a two-  dimensional representation, that is translated into classification information, i.e.,  each SOM element is provided with a class label (one of the output classes of the  task). For example, let the BP-SOM network displayed in Figure I be trained on  a classification task which maps instances to either output class A or B. Three  types of elements can be distinguished in the SOM: elements labelled with class A,  elements labelled with class B, and unlabelled elements (no winning class could be  found). The two-dimensional representation of the SOM is used as an addition to  the standard BP learning rule [6]. Classification and reliability information from the  SOMs is included when updating the connection weights of the MFN. The error of  a hidden-layer vector is an accumulation of the error computed by the BP learning  rule, and a SOM-error. The SOM-error is the difference between the hidden-unit  activation vector and the vector of its best-matching element associated with the  same class on the SOM.  An important effect of including SOM information in the error signals is that clusters  of hidden-unit activation vectors of instances associated with the same class tend  to become increasingly similar to each other. On top of this effect, individual  hidden-unit activations tend to become more streamlined, and often end up having  activations near one of a limited number of discrete values.  Learning Statistically Neutral Tasks without Expert Guidance 75  3 The date-calculation task  The first statistically neutral calculation task we consider is the date-calculation  task: determining the day of the week on which a given date fell. (For instance,  October 2, 1997 fell on a Friday.) Solving the task requires an algorithmic approach  that is typically hard for human calculators and requires one or more intermediate  steps. It is generally assumed that the identity of these intermediate steps follows  from the algorithmic solution, although variations exist in the steps as reportedly  used by human experts [2]. We will show that such explicit abstraction is not  needed, after reviewing the case for the necessity of "human assistance" in learning  the task.  3.1 Date calculation with expert-based abstraction  Norris [2] attempted to model autistic savant date calculators using a multi-layer  feedforward network (MFN) and the back-propagation learning rule [6]. He intended  to build a model mimicking the behavior of the autistic savant without the need  either to develop arithmetical skills or to encode explicit knowledge about reg-  ularities in the structure of dates. A standard multilayer network trained with  backpropagation [6] was not able to solve the date-calculation task. Although the  network was able to learn the examples used for training, it did not manage to  generalize to novel date-day combinations. In a second attempt Norris split up the  date-calculation task in three simpler subtasks and networks.  Using the three-stage learning strategy Norris obtained a nearly perfect performance  on the training material and a performance of over 90% on the test material (errors  are almost exclusively made on dates falling in January or February in leap years).  He concludes with the observation that "The only reason that the network was able  to learn so well was because it had some human assistance." [2, p.285]. In addition,  Norris claims that "even if the [backpropagation] net did have the right number of  layers there would be no way for the net to distribute its learning throughout the  net such that each layer learned the appropriate step in computation." [2, p. 290].  3.2 Date calculation without expert-based abstraction  We demonstrate that with the BP-SOM learning rule, a single hidden-layer feedfor-  ward network can become a successful date calculator. Our experiment compares  three types of learning: standard backpropagation learning (BP, [6]), backpropa-  gation learning with weight decay (BPWD, [8]), and BP-SOM learning. Norris used  BP learning in his experiment which leads to overfitting [2] (a considerably lower  generalization accuracy on new material as compared to reproduction accuracy on  training material); BPWD learning was included to avoid overfitting.  The parameter values for BP (including the number of hidden units for each task)  were optimized by performing pilot experiments with BP. The optimal learning-rate  and momentum values were 0.15 and 0.4, respectively. BP, BPWD, and BP-SOM were  trained for a fixed number of cycles rrt = 2000. Early stopping, a common method  to prevent overfitting, was used in all experiments with BP, BPWD, and BP-SOM [9].  In our experiments with BP-SOM, we used the same interval of dates as used by  Norris, i.e., training and test dates ranged from January 1, 1950 to December $1,  1999. We generated two training sets, each consisting of 3,653 randomly selected  instances, i.e., one-fifth of all dates. We also generated two corresponding test sets  and two validation sets (with 1,000 instances each) of new dates within the same  50-year period. In all our experiments, the training set, test set, and validation set  76 T. Weij'ters, A. v. d. Bosch and E. Postma  Table 1: Average generalization performances (plus standard deviation, after '+';  averaged over ten experiments) in terms of incorrectly-processed training and test  instances, of BP, BPWD, and BP-SOM, trained on the date-calculation task and the  parity-12 task.  BP:  incorrect BPWD: % incorrect BP-SOM: % incorrect  Task Train I Test Train I Test Train I Test  date calc. 20.8 +5.4 28.8 +7.8 1.5 + 0.3 8.8 +1.4 2.9 +2.0 3.3 +1.9  parity-12 14.1 +18.8 27.4 +16.4 21.6 +24.2 22.4 +18.3 5.9 +10.2 6.2 +10.7  had empty intersections. We partitioned the input into three fields, representing  the day of the month (31 units), the month (12 units) and the year (50 units). The  output is represented by 7 units, one for each day of the week. The MFN contained  one hidden layer with 12 hidden units for BP, and 25 hidden units for BPWD and  BP-SOM. The SOM of the BP-SOM network contained 12 x 12 elements. Each of the  three learning types was tested on two different data sets. Five runs with different  random weight initializations were performed on each set, yielding ten runs per  learning type. The averaged classification errors on the test material are reported  in Table 1.  From Table I it follows that the average classification error of BP is high: on test  instances BP yields a classification error of 28.8%, while the classification error of  BP on training instances is 20.8%. Compared to the classification error of BP, the  classification errors on both training and test material of BPWD and BP-SOM are  much lower. However, BPWD'S generalization performance on the test material is  considerably worse than its performance on the training material: a clear indication  of overfitting. We note in passing that the results of BPWD contrast with Norris'  [2] claim that BP is unable to learn the date-calculation task when it is not decom-  posed into subtasks. The inclusion of weight decay in BP is sufficient for a good  approximation of the performance results of Norris' decomposed network.  The results in Table I also show that the performance of BP-SOM on test mate-  rial is significantly better than that of BPWD (t(19)=7.39, p(0.001); BP-SOM has  learned the date-calculation task at a level well beyond the average of human date  calculators as reported by Norris [2]. In contrast with Norris' pre-structured net-  work, BP-SOM does not rely on expert-based levels of abstraction for learning the  date-calculation task.  4 The parity-12 task  The parity-n problem, starting from the XOR problem (parity-2), continues to  be a relevant topic on the agenda of many neural network and machine learning  researchers. Its definition is simple (determine whether there is an odd or even  number of 1's in an n-length bit string of 1's and O's), but established state-of-the-art  algorithms such as C4.5 [10] and backpropagation [6] cannot learn it even with small  n, i.e., backpropagation fails with n _ 4 [11]. That is, these algorithms are unable  to generalize from learning instances of a parity-n task to unseen new instances of  the same task. As with date calculation, this is due to the statistical neutrality  of the task. The solution of the problem must lie in having some comprehensive  overview over all input values at an intermediary step before the odd/even decision  is made. Indeed, humans appear to follow this strategy [4].  Learning Statistically Neutral Tasks without Expert Guidance 77  BP BPWD BP-SOM  Figure 2: Graphic representation of a 7 x 7 SOM associated with a BP-trained MFN  (left) and a BPWD-trained MFN (middle), and a 7 x 7 SOM associated with a BP-SOM  network (right), all trained on the parity-12 task.  Analogous to our study of the date-calculation task presented in Section 3, we apply  BP, BPWD, and BP-SOM to the parity-n task. We have selected n to be 12. The  training set contained 1,000 different instances selected at random out of the set of  4,096 possible bit strings. The test set and the validation set contained 100 new  instances each. The hidden layer of the MFN in all three algorithms contained 20  hidden units, and the SOM in BP-SOM contained 7 x 7 elements. The algorithms  were run with 10 different random weight initializations. Table 1 displays the clas-  sification errors on training instances and test instances.  Analysis of the results shows that BP-SOM performs significantly better than BP and  BPWD on test material (t(19)--3.42, p<0.01 and t(19)--2.42, p<0.05, respectively).  (The average error of 6.2% made by BP-SOM stems from a single experiment out  of the ten performing at chance level, and the remaining nine yielding about 1%  error). BP-SOM is able to learn the parity-12 task quite accurately; BP and BPWD  fail relatively, which is consistent with other findings [11].  As an additional analysis, we have investigated the differences in hidden unit activa-  tions after training with the three learning algorithms. To visualize the differences  between the representations developed at the hidden layers of the MFNS trained with  BP, BPWD, and BP-SOM, we also trained SOMs with the hidden layer activities of  the trained BP and BPWD networks. The left part of Figure 2 visualizes the class  labelling of the SOM attached to the BP-trained MFN after training; the middle part  visualizes the SOM of the BPWD-trained MFN, and the right part displays the SOM of  the BP-SOM network after training on the same material. The SOM of the BP-SOM  network is much more organized and clustered than that of the SOMs corresponding  with the BP-trained and BPWD-trained MFNS. The reliability values of the elements  of all three SOMs are represented by the width of the black and white squares. It  can be seen that the overall reliability and the degree of clusteredness of the SOM of  the BP-SOM network is considerably higher than that of the SOM of the BP-trained  and BPWD-trained MFNs.  5 How parity-12 is learned  Given the hardness of the task and the supposed necessity of expert guidance, and  given BP-SOM's success in learning parity-12 in contrast, it is relevant to analyze  what solution was found in the BP-SOM learning process. In this subsection we  provide such an analysis, and show that the trained network performs an elegant  checksum calculation at the hidden layer as the intermediary step.  All elements of SOMs of BP-SOM networks trained on the parity-12 task are either  the prototype for training instances that are all labeled with the same class, or  78 T. Weij'ters, A. v. d. Bosch and E. Postma  Table 2: List of some training instances of the parity-12 task associated with SOM  elements (1,1), (2,4), and (3,3) of a trained BP-SOM network.  SOM (1,1), class=even, reliability 1.0  inl in2 in3 in4 in5 in6 in7 in8 in9 inlO inll in12 checksum  1 1 0 0 0 0 0 0 0 0 0 0 -2  0 0 1 0 0 0 1 0 1 1 0 0 -2  1 1 0 1 0 0 0 1 0 0 0 0 -2  SOM (2,4), class----odd, reliability 1.0  in1 in2 in3 in4 in5 in6 in7 ins in9 in10 in11 in12 checksum  0 I I I I 0 I I 0 I 0 0 -1  I I I 0 I I I 0 I I 0 I -1  I 0 I i 0 I 0 I I 0 I 0 -1  SOM (3,3), class--even, reliability 1.0  in1 in2 in3 in4 in5 in6!in7 in8 in9 in10 in11 in12 checksum  0 0 I I 0 0 I i 0 I 0 I 0  i I I 1 I 0 I 0 0 0 I I 0  I 0 I i I i 0 I I 0 0 I 0  II- I+ + +1- - + + + II  prototype of no instances at all. Non-empty elements (the black and white squares  in the right part of Figure 2) can thus be seen as containers of homogeneously-  labeled subsets of the training set (i.e., fully reliable elements). The first step of our  analysis consists of collecting, after training, for each non-empty SOM element all  training instances clustered at that SOM element. As an illustration, Table 2 lists  some training instances clustered at the SOM elements at coordinates (1,1), (2,4),  and (3,3). At first sight the only common property of instances associated with  the same SOM element is the class to which they belong; e.g., all instances of SOM  element (1,1) are even, all instances of SOM element (2,4) are odd, and all instances  of SOM element (3,3) are again even.  The second step of our analysis focuses on the sign of the weights of the connections  between input and hidden units. Surprisingly, we find that the connections of each  individual input unit to all hidden units have the same sign; each input unit can  therefore be labeled with a sign marker (as displayed at the bottom of Table 2).  This allows the clusteripg on the soM to become interpretable. All weights from  input unit 1, 2, 3, 7, 8, and 9 to all units of the hidden layer are negative, all weights  from input unit 4, 5, 6, 10, 11, and 12 to all units of the hidden layer are positive. At  the hidden layer, this information is gathered as if a checksum is computed; each  SOM element contains instances that add up to an identical checksum. This can  already be seen using only the sign information rather than the specific weights.  For instance, all instances clustered at soM element (1,1) lead to a checksum of  -2 when a sum is taken of the product of all input values with all weight signs.  Analogously, all instances of cluster (2,4) count up to -1 and the instances of cluster  (3,3) to zero. The same regularity is present in the instances of the other SOM  elements.  In sum, the BP-SOM solution to the parity-12 task can be interpreted as to trans-  form it at the hidden layer into the mapping of different, approximately discrete,  checksums to either class 'even' or 'odd'.  Learning Statistically Neutral Tasks without Expert Guidance 79  6 Conclusions  We have performed two learning experiments in which the BP-SaM learning algo-  rithm was trained on the date-calculation task and on the parity-12 task. Both  tasks are hard to learn because they are statistically neutral, but can be learned  adequately and without expert guidance by the BP-SaM learning algorithm. The  effect of the SaN part in BP-SaM (adequately constrained hidden-layer vectors, re-  liable clustering of vectors on the saN, and streamlined hidden-unit activations)  clearly contributes to this success.  From the results of the experiments on the date-calculation task, we conclude that  Norris' claim that, without human assistance, a backpropagation net would never  learn the date-calculation task is inaccurate. While BP with weight decay performs  at Norris' target level of accuracy, BP-SaM performs even better. Apparently BP-  san is able to distribute its learning throughout the net such that the two parts  of the network (from input layer to hidden layer, and from hidden layer to output  layer) perform the mapping with an appropriate intermediary step.  The parity-12 experiment exemplified that such a discovered intermediary step can  be quite elegant; it consists of the computation of a checksum via the connection  weights between the input and hidden layers. Unfortunately, a similar elegant  simplicity was not found in the connection weights and san clustering of the date  calculation task; future research will be aimed at developing more generic analyses  for trained BP-SON networks, so that automatically-discovered intermediary steps  may be made understandably explicit.  References  [1] Newell, A. and Simon, H.A. (1972) Human problem solving. Engelwood Cliffs, N J:  Prentice-Hall.  [2] Norris, D. (1989). How to build a connectionist idiot (savant). Cognition, 35, 277-291.  [3] Hill, A. L. (1975). An investigation of calendar calculating by an idiot savant. Amer-  ican Journal of Psychiatry, 132, 557-560.  [4] Dehaene, P., Bossini, S., and Giraux, P. (1993). The mental representation of parity  and numerical magnitude. Journal of Experimental Psychology: General, 122, 371-  396.  [5] Weijters, A., Van den Bosch, A., Van den Herik, H. J. (1997). Behavioural Aspects of  Combining Backpropagation Learning and Self-organizing Maps. Connection Science,  9, 235-252.  [6] Rumelhart, D. E., Hinton, G. E., and Williams, R. J. (1986). Learning internal rep-  resentations by error propagation. In D. E. Rumelhart and J. L. McClelland (Eds.),  Parallel Distributed Processing: Explorations in the Microstructure of Cognition, vol-  ume 1: Foundations (pp. 318-362). Cambridge, MA: The MIT Press.  [7] Kohonen, T. (1989). $elf-organisation and Associative Memory. Berlin: Springer  Verlag.  [8] Hinton, G. E. (1986). Learning distributed representations of concepts. In Proceedings  of the Eighth Annual Conference of the Cognitive Science Society, 1-12. Hillsdale, N J:  Erlbaum.  [9] Prechelt, L. (1994). Proben1: A set of neural network benchmark problems and bench-  marking rules. Technical Report 24/94, Fakultit fiir Informatik, Universtrait Karl-  sruhe, Germany.  [10] Quinlan, J. R. (1993). C.5: Programs for Machine Learning. San Mateo, CA: Mor-  gan Kaufmann.  [11] Thornton, C. (1996). Parity: the problem that won't go away. In G. McCalla (Ed.),  Proceeding of AI-96, Toronto, Canada (pp. 362-374). Berlin: Springer Verlag.  
Bayesian averaging is well-temperated  Lars Kai Hansen  Department of Mathematical Modelling  Technical University of Denmark B321  DK-2800 Lyngby, Denmark  lkhansen @imm. dtu. dk  Abstract  Bayesian predictions are stochastic just like predictions of any other  inference scheme that generalize from a finite sample. While a sim-  ple variational argument shows that Bayes averaging is generaliza-  tion optimal given that the prior matches the teacher parameter  distribution the situation is less clear if the teacher distribution is  unknown. I define a class of averaging procedures, the temperated  likelihoods, including both Bayes averaging with a uniform prior  and maximum likelihood estimation as special cases. I show that  Bayes is generalization optimal in this family for any teacher dis-  tribution for two learning problems that are analytically tractable:  learning the mean of a Gaussian and asymptotics of smooth learn-  ers.  1 Introduction  Learning is the stochastic process of generalizing from a random finite sample of  data. Often a learning problem has natural quantitative measure of generalization.  If a loss function is defined the natural measure is the generalization error, i.e., the  expected loss on a random sample independent of the training set. Generalizability  is a key topic of learning theory and much progress has been reported. Analytic  results for a broad class of machines can be found in the litterature [8, 12, 9, 10]  describing the asymptotic generalization ability of supervised algorithms that are  continuously parameterized. Asymptotic bounds on generalization for general ma-  chines have been advocated by Vapnik [11]. Generalization results valid for finite  training sets can only be obtained for specific learning machines, see e.g. [5]. A  very rich framework for analysis of generalization for Bayesian averaging and other  schemes is defined in [6].  Averaging has become popular as a tool for improving generalizability of learning  machines. In the context of (time series) forecasting averaging has been investigated  intensely for decades [3]. Neural network ensembles were shown to improve general-  ization by simple voting in [4] and later work has generalized these results to other  types of averaging. Boosting, Bagging, Stacking, and Arcing are recent examples  of averaging procedures based on data resampling that have shown useful see [2]  for a recent review with references. However, Bayesian averaging in particular is  attaining a kind of cult status. Bayesian averaging is indeed provably optimal in a  266 L. K. Hansen  number various ways (admissibility, the likelihood principle etc) [1]. While it fol-  lows by construction that Bayes is generalization optimal if given the correct prior  information, i.e., the teacher parameter distribution, the situation is less clear if  the teacher distribution is unknown. Hence, the pragmatic Bayesians downplay the  role of the prior. Instead the averaging aspect is emphasized and "vague" priors are  invoked. It is important to note that whatever prior is used Bayesian predictions  are stochastic just like predictions of any other inference scheme that generalize  from a finite sample.  In this contribution I analyse two scenarios where averaging can improve gener-  alizability and I show that the vague Bayes average is in fact optimal among the  averaging schemes investigated. Averaging is shown to reduce variance at the cost  of introducing bias, and Bayes happens to implement the optimal bias-variance  trade-off.  2 Bayes and generalization  Consider a model that is smoothly parametrized and whose predictions can be  described in terms of a density function x. Predictions in the model are based on a  given training set: a finite sample D N  -- {xa}a= of the stochastic vector x whose  density - the teacher - is denoted p(xlOo ). In other words the true density is assumed  to be defined by a fixed, but unknown, teacher parameter vector 00. The model,  denoted H, involves the parameter vector 0 and the predictive density is given by  p(xlD, H) - / p(xl O, H)p(OID, Z)dO  (1)  p(OID , H) is the parameter distribution produced in training process. In a maxi-  mum likelihood scenario this distribution is a delta function centered on the most  likely parameters under the model for the given data set. In ensemble averaging  approaches, like boosting bagging or stacking, the distribution is obtained by train-  ing on resampled traning sets. In a Bayesian scenario, the parameter distribution  is the posterior distribution,  p(OID, H ) --- P(DIO, H)P(OIH)  f p( D[O', H)p( O'lH)dO'  (2)  where p(OIH ) is the prior distribution (probability density of parameters if D is  empty). In the sequel we will only consider one model hence we suppress the model  conditioning label H.  The generalization error is the average negative log density (also known as simply  the "log loss" - in some applied statistics works known as the "deviance")  r(DlO0) = / -logp(xlD)p(xlOo)dx, (3)  The expected value of the generalization error for training sets produced by the  given teacher is given by  F(00) = / / -logp(x[D)p(x[Oo)dxp(D[Oo)dD.  (4)  This does not limit us to conventional density estimation; pattern recognition and  many functional approximations problems can be formulated as density estimation prob-  lems as well.  Bayesian Averaging is Well- Temperated 267  Playing the game of "guessing a probability distribution" [6] we not only face a  random training set, we also face a teacher drawn from the teacher distribution  p(00). The teacher averaged generalization must then be defined as  r = f r(Oo)p(eo)deo. (5)  This is the typical generalization error for a random training set from the randomly  chosen teacher - produced by the model H. The generalization error is minimized  by Bayes averaging if the teacher distribution is used as prior. To see this, form the  Lagrangian functional  [q(xlD)]- f f f-logq(xlD)p(x[Oo)dxp(DlOo)dDp(Oo)dOo+A f q(xlD)dx (6)  defined on positive functions q(xID ). The second term is used to ensure that q(xlD )  is a normalized density in x. Now compute the variational derivative to obtain  ( 1 /  5q(xlD ) = q(x[D) p(xlO)p(DlO)P(O)dO + '' (7)  Equating this derivative to zero we recover the predictive distribution of Bayesian  averaging,  p(DlO)p(O)  q(xID ) - p(xlO ) f P(DlO,)p(O,)do, dO, (8)  where we used that A - f p(DlO)p(O)dO is the appropriate normalization constant.  It is easily verified that this is indeed the global minimum of the averaged gener-  alization error. We also note that if the Bayes average is performed with another  prior than the teacher distribution p(0o), we can expect a higher generalization er-  ror. The important question from a Bayesian point of view is then: Are there cases  where averaging with generic priors (e.g. vague or uniform priors) can be shown to  be optimal?  3 Temperated likelihoods  To come closer to a quantative statement about when and why vague Bayes is the  better procedure we will analyse two problems for which some analytical progress is  possible. We will consider a one-parameter family of learning procedures including  both a Bayes and the maximum likelihood procedure,  p(OI,D,H) = f p(DlO')dO" (9)  where / is a positive parameter (plying the role of an inverse temperature). The  family of procedures are all averaging procedures, and/ controls the width of the  average. Vague Bayes (here used synonymously with Bayes with a uniform prior)  is recoved for/ - 1, while the maximum posterior procedure is obtained by cooling  to zero width/ - o.  In this context the generalization design question can be frased as follows: is there  an optimal temperature in the family of the temperated likelihoods?  3.1 Example: 1D normal variates  Let the teacher distribution be given by  1 ( 1 (x_00)2)  p(xlOo ) -  exp 2/r2  (10)  268 L. K. Hansen  The model density is of the same form with 0 unknown and er 2 assumed to be  known. For N examples the posterior (with a uniform prior) is,  p(OlD) = 2--- exp - ,  (11)  with 7 = 1/N Y'o zoo. The temperated likelihood is obtained by raising to the fi'th  power and normalizing,  p(O]D, fi) = 2---- 2 exp  .  (12)  The predictive distribution is found by integrating w.r.t. 0,  p(xlD,) = p(x10)p(01D,)d0 = (7- , (la)  with  =  (1 + 1/N). We note that this distribution is wider for all the averaging  procedures than it is for maximum likelihood (  ), i.e., less variant. Por very  small  the predictive distribution is almost independent of the data set, hence  highly bied.  It is straightforward to compute the generalization error of the predictive distribu-  tion for general . Pirst we compute the generalization error for the specific training  set D,  r(D, fi, Oo) = -logp(x[D, fi)p(x]Oo)dx = log + 2 ((- O)2 +2),  (14)  The average generalization error is then found by averaging w.r.t the sampling  distribution using  (Oo,a2/N).,  f  a2(1 )  F(fi) = F(D, fi)dDp(DlOo) = log + 2  + i , (15)  We first note that the generalization error is independent of the teacher 0 param-  eter, this happened because  is a "location" parameter. The fi-dependency of the  averaged generalization error is depicted in Figure 1. Solving OF(fi)/Ofi = 0 we find  that the optimal fi solves  +1 =a2 +1  fi=l (16)  Note that this result holds for any N and is independent of the teacher parameter.  The Bayes averaging at unit temperature is optimal for any given value of 0, hence,  for any teacher distribution. We may say that the vague Bayes scheme is robust  to the teacher distribution in this ce. Clearly this is a much stronger optimality  than the more general result proven above.  3.2 Bias-variance tradeoff  It is interesting to decompose the generalization error in Eq. 15 in bias and variance  components. We follow Heskes [7] and define the bias error as the generalization  error of the geometric average distribution,  B(fi) --/-log(x)p(xlOo)dx ,  (17)  Bayesian Averaging is Well-Temperated 269  0.7   0.6 GENERALIZATION '  0.5 BAY  0.1  0  I 315 ; 4'5  0 0.5 1 15 2 25 3  TEMPERATURE  A04  V03  Figure l: Bias-variance trade-off as function of the width of the temperated likeli-  hood ensemble (temperature - 1/fi) for N - 1. The bias is computed as the gen-  eralization error of the predictive distribution obtained from the geometric average  distribution w.r.t. training set fluctuations as proposed by Heskes. The predictive  distribution produced by Bayesian averaging corresponds to unit temperature (ver-  tical line) and it achieves the minimal generalization error. Maximum-likelihood  estimation for reference is recovered as the zero width/temperature limit.  with  (z) = Z-l exp (f log[p(z,D)]p(DlOo)dD)   Inserting from Eq. (13), we find  Integrating over the teacher distribution we find,  1  B(fi) = log + --  1 )  24(- Oo) .  (18)  (19)  O-2  24 (20)  The variance error is given by V(fi) = I'(fi) - B(fi),  o- 2  V(fi)- 2No.  (21)  We can now quantify the statements above. By averaging a bias is introduced -the  predictive distribution becomes wider- which decrease the variance contribution  initially so that the generalization error being the sum of the two decreases. At still  higher temperatures the bias becomes too strong and the generalization error start  to increase. The Bayes average at unit temperature is the optimal trade-off within  the given family of procedures.  270 L. K. Hansen  3.3 Asymptotics for smoothly parameterized models  We now go on to show that a similar result also holds for general learning prob-  lems in limit of large data sets. We consider a system parameterized by a finite  dimensional parameter vector 0. For a given large training set and for a smooth  likelihood function, the temperated likelihood is approximately Gaussian centered  at the maximum posterior parameters[13], hence the normalized temperated poste-  rior reads  P(OlfiD, H) = [ fiNA(D'Oz') exp (----50'A(D,Oz,)50) (22)  2r  where 40 = 0- OM, with OM = OM (D) denoting the maximum likelihood solution  for the given training sample. The second derivative or Hessian matrix is given by  i  A(x O) (23)  A(O,O) =  ,  A(x,O) = OOOO' logp(x[O) (24)  The predictive distribution is given by  V(l"  = f p(lO)p(Ol,D)dO (5)  we write p(x[O) = exp(-e(x[O)) and expand e(x[O) around OML to second order, we  find  p(x[O)  p(xlOML)exp (--a(xlO)'50- 50'A(xlO)50 ) . (26)  We are then in position to perform the integration over the posterior to find the  normalized predictive distribution,  p(xl,W ) p(xlOM). INA(D)I exp ( x (xlOM),A(xlOM)a(xlOM))  INA(D)+A(x)I  (7)  Proceeding  above, we compute the generalization error  r(, 00) = f f -ogp(l, D)p(lOo)dp(DlOo)dD (8)  For suciently smooth likelihoods, fluctuations in the maximum likelihood param-  eters will be asymptotic normal, see e.g. [8], and furthermore fluctuations in A(D)  can be neglected, this means that we can approximate,  1  A(x) + A(D)  ( + 1)A0, A0 = A(xlOo)p(xlOo)dx (29)  where A0 is the averaged Fisher information matrix. With these approximations  (valid as N  ) the generalization error can be found,  a i a 1 + v (30)  r(3, 00)  r() +  log 1 +  1 + 3'  with d = dim(0) denoting the dimension of the parameter vector. Like in the 1D  example (Eq. (15)) we find the generalization error is ymptotically independent  of the teacher parameters. It is minimized for  = 1 and we conclude that Bayes  is well-temperated in the asymptotics and that this holds for any teacher distri-  bution. In the Bayes literature this is refered to  the prior is overwhelmed by  data [1]. Decomposing the errors in bi and variance contributions we find similar  results  for in 1D example, Bayes introduces the optimal bias by averaging at unit  temperature.  Bayesian Averaging is Well-Temperated 271  4 Discussion  We have seen two examples of Bayes averaging being optimal, in particular improv-  ing on maximum likelihood estimation. We found that averaging introduces a bias  and reduces variance so that the generalization error (being the sum of bias and  variance) initially decrease. Bayesian averaging at unit temperature is the optimal  width of the averaging distribution. For larger temperatures (widths) the bias is  too strong and the generalization error increases. Both examples were special in the  sense that they lead to generalization errors that are independent of the random  teacher parameter. This is not generic, of course, rather the generic case is that a  mis-specified prior can lead to arbitrary large learning catastrophes.  Acknowledgments  I thank the organizers of the 1999 Max Planck Institute Workshop on Statistical  Physics of Neural Networks Michael Biehl, Wolfgang Kinzel and Ido Kanter, where  this work was initiated. I thank Carl Edward Rasmussen, Jan Larsen, and Manfred  Opper for stimulating discussions on_Bayesian averaging. This work was funded by  the Danish Research Councils through the Computational Neural Network Center  CONNECT and the THOR Center for Neuroinformatics.  References  [1] C.P. Robert: The Bayesian Choice - A Decision-Theoretic Motivation. Springer Texts  in Statistics, Springer Verlag, New York (1994). A. Ohagan: Bayesian Inference.  Kendall's Advanced Theory of Statistics. Vol 2B. The University Press, Cambridge  (1994).  [2] L. Breiman: Using adaptive bagging to debias regressions. Technical Report 547,  Statistics Dept. U.C. Berkeley, (1999).  [3] R.T. Clemen Combining forecast: A review and annotated bibliography. Journal of  Forecasting 5, 559 (1989).  [4] L.K. Hansen and P. Salamon: Neural Network Ensembles. IEEE Transactions on  Pattern Analysis and Machine Intelligence, 12, 993-1001 (1990).  [5] L.K. Hansen: Stochastic Linear Learning: Exact Test and Training Error Averages.  Neural Networks 6, 393-396, (1993)  [6] D. Haussler and M. Opper: Mutual Information, Metric Entropy, and Cumulative  Relative Entropy Risk Annals of Statistics 25 2451-2492 (1997)  [7] T. Heskes: Bias/Variance Decomposition/or Likelihood-Based Estimators. Neural  Computation 10, pp 1425-1433, (1998).  [8] L. Ljung: System Identification: Theory/or the User. Englewood Cliffs, New Jersey:  Prentice-Hall, (1987).  [9] J. Moody: "Note on Generalization, Regularization, and Architecture Selection in  Nonlinear Learning Systems," in B.H. Juang, S.Y. Kung & C.A. Kamm (eds.) Pro-  ceedings of the first IEEE Workshop on Neural Networks/or Signal Processing, Pis-  cataway, New Jersey: IEEE, 1-10, (1991).  [10] N. Murata, S. Yoshizawa & S. Amax/: Network Information Criterion -- Deter-  mining the Number of Hidden Units for an Artificial Neural Network Model. IEEE  Transactions on Neural Networks, vol. 5, no. 6, pp. 865-872, 1994.  [11] V. Vapnik: Estimation of Dependences Based on Empirical Data. Springer-Verlag  New York (1982).  [12] H. White, "Consequences and Detection of Misspecified Nonlinear Regression Mod-  els," Journal of the American Statistical Association, 76(374), 419-433, (1981).  [13] D.J.C MacKay: Bayesian Interpolation, Neural Computation 4, 415-447, (1992).  
